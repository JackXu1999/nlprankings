



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1799–1809
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1165

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1799–1809
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1165

Topical Coherence in LDA-based Models through Induced Segmentation

Hesam Amoualian
Univ. Grenoble Alps, CNRS, Grenoble INP - LIG

hesam.amoualian@imag.fr

Wei Lu
Singapore University of Technology and Design

luwei@sutd.edu.sg

Eric Gaussier
Univ. Grenoble Alps, CNRS, Grenoble INP - LIG

eric.gaussier@imag.fr

Georgios Balikas
Univ. Grenoble Alps, CNRS, Grenoble INP - LIG

georgios.balikas@imag.fr

Massih-Reza Amini
Univ. Grenoble Alps, CNRS, Grenoble INP - LIG

massih-reza.amini@imag.fr

Marianne Clausel
Univ. Grenoble Alps, CNRS, Grenoble INP - LJK

marianne.clausel@imag.fr

Abstract

This paper presents an LDA-based model
that generates topically coherent segments
within documents by jointly segmenting
documents and assigning topics to their
words. The coherence between topics is
ensured through a copula, binding the top-
ics associated to the words of a segment.
In addition, this model relies on both doc-
ument and segment specific topic distribu-
tions so as to capture fine grained differ-
ences in topic assignments. We show that
the proposed model naturally encompasses
other state-of-the-art LDA-based models de-
signed for similar tasks. Furthermore, our
experiments, conducted on six different
publicly available datasets, show the effec-
tiveness of our model in terms of perplexity,
Normalized Pointwise Mutual Information,
which captures the coherence between the
generated topics, and the Micro F1 measure
for text classification.

1 Introduction

Since the seminal works of Hofmann (1999) and
Blei et al. (2003), there have been several develop-
ments in probabilistic topic models. Many exten-
sions have indeed been proposed for different ap-
plications, including ad-hoc information retrieval
(Wei and Croft, 2006), clustering search results
(Zeng et al., 2004) and driving faceted browsing
(Mimno and McCallum, 2007). In most of these
studies, the initial exchangeability assumptions of

PLSA and LDA, stipulating that words within a
document are interdependent, has led to incoherent
topic assignments within semantically meaningful
text units, even though the importance of having
topically coherent phrases is generally admitted
(Griffiths et al., 2005). More recently, (Balikas
et al., 2016b) has shown that binding topics, so as
to obtain more coherent topic assignments, within
such text segments as noun phrases improves the
performance (e.g. in terms of perplexity) of LDA-
based models. The question nevertheless remains
as to which segmentation one should rely on.

Furthermore, text segments can refer to topics
that are barely present in other parts of the doc-
ument. For example, the segment “the Kurdish
regional capital” in the sentence1 “A thousand
protesters took to the main street in Erbil, the Kur-
dish regional capital, to condemn a new law requir-
ing all public demonstrations to have government
permits.” refers to geography in a document that
is mainly devoted to politics. Relying on a single
topic distribution, as done in most previous studies
including (Balikas et al., 2016b), may prevent one
from capturing those segment specific topics.

In this paper, we propose a novel LDA-based
model that automatically segments documents into
topically coherent sequences of words. The coher-
ence between topics is ensured through copulas
(Elidan, 2013) that bind the topics associated to the
words of a segment. In addition, this model relies
on both document and segment specific topic distri-

1This sentence is taken from New York Times news (NYT)
collection described in Section 4.

1799

https://doi.org/10.18653/v1/P17-1165
https://doi.org/10.18653/v1/P17-1165


butions so as to capture fine grained differences in
topic assignments. A simple switching mechanism
is used to select the appropriate distribution (doc-
ument or segment specific) for assigning a topic
to a word. We show that this model naturally en-
compasses other state-of-the-art LDA-based models
proposed to accomplish the same task, and that it
outperforms these models over six publicly avail-
able collections in terms of perplexity, Normalized
Pointwise Mutual Information (NPMI), a measure
used to assess the coherence of topics with docu-
ments, and the Micro F1-measure in a text classifi-
cation context.

2 Related work

Probabilistic Latent Semantic Analysis (PLSA)
proposed by (Hofmann, 1999) is the first proba-
bilistic model that explains the generation of co-
occurrence data using latent random topics and, the
EM algorithm for parameter estimation. The model
was found more flexible and scalable than the La-
tent Semantic Analysis (Deerwester et al., 1990),
which is based on the singular value decomposi-
tion of the document-term matrix, however PLSA
is not a generative model as parameter estimation
should be performed at each addition of new doc-
uments. To overcome this drawback, Blei et al.
(2003) proposed the Latent Dirichlet Allocation
(LDA) by assuming that the latent topics are ran-
dom variables sampled from a Dirichlet distribu-
tion and that the generated words, occurring in a
document, are exchangeable. The interdependence
assumption allows the parameter estimation and
the inference of the LDA model to be carried out
efficiently, but it is not realistic in the sense that
topics assigned to similar words of a text span are
generally incoherent.

Different studies, presented in the following sec-
tions, attempted to remedy this problem and they
can be grouped in two broad families depending
on whether they make use of external knowledge-
based tools or not in order to exhibit text structure
for word-topic assignment.

2.1 Knowledge-based topic assignments

The main assumption behind these models are that
text-spans such as sentences, phrases or segments
are related in their content. Therefore, the inte-
gration of these dependent structures can help to
discover coherent latent topics for words. Different
attempts to combine LDA-based models with sta-

tistical tools to discover document structures have
been successfully proposed, such as the study of
Griffiths et al. (2005) who investigated the effect
of combining a Hidden Markov Model with LDA
to capture long and short distance dependencies.
Similarly, (Boyd-Graber and Blei, 2008; Balikas
et al., 2016a,b) integrated text structure exhibited
by a parser or a chunker in their topic models. In
this line, Du et al. (2013) following (Du et al.,
2010) presented a hierarchical Bayesian model for
unsupervised topic segmentation. This model in-
tegrates a boundary sampling method used in a
Bayesian segmentation model introduced by Purver
et al.(2006) to the topic model. For inference, a
non-parametric Markov Chain inference is used
that splits and merges the segments while a Pitman-
Yor process (Teh, 2006) binds the topics. Recently,
Tamura and Sumita (2016) extended this idea to the
bilingual setting. They assume that documents con-
sist of segments and the topic distribution of each
segment is generated using a Pitman-Yor process
(Teh, 2006).

Though, the topic assignments follow the struc-
ture of the text; these models suffer from the bias
of statistical or linguistic tools they rely on. To
overpass this limitation, other systems integrated
automatically the extraction of text structure, in the
form of phrases, in their process.

2.2 Knowledge-free topic assignments

This type of models extract text-spans using n-
gram counts and word collections and use bigrams
to integrate the order of words as well as to capture
the topical content of a phrase (Lau et al., 2013).
In (Wang et al., 2007), depending on the topic a
particular bigram can be either considered as a sin-
gle token or as two unigrams. Further, Wang et al.
(2009) merged topic models with a unigram model
over sentences that assigns topics to the sentences
instead of the words.

Our proposed approach also does not make use
of external statistical tools to find text segments.
The main difference with the previous knowledge-
free topic model approaches is that the proposed
approach assigns topics to words based on two,
segment-specific and document-specific distribu-
tions selected from a Bernoulli law. Topics within
segments are then constrained using copulas that
bind their distributions. In this way, segmentation
is embedded in the model and it naturally comes
along with the topic assignment.

1800



α

θd

z1 zn

w1 wn

φ βλ

|S|
D

K

. . .

. . .

(a) copLDA

α

θd

z1 zn

w1 wnS

|Sd|

φ βλ

|S|
D

K

. . .

. . .

(b) segLDAcopp=0

α

θd

fn

θd,s,n

p

θs

zn

wnS

|Sd|

φ β

|S|
D

K

(c) segLDAcopλ=0

α

θd

f1 fn

θd,s,1 θd,s,n

p

θs

z1 zn

w1 wnS

|Sd|

φ βλ

|S|
D

K

. . .

. . .

. . .

. . .

(d) segLDAcop

Figure 1: Graphical model for Copula LDA (copLDA), extension of Copula LDA with segmentation
(segLDAcopp=0), LDA with segmentation and topic shift (segLDAcopλ=0) and complete model
(segLDAcop).

3 Joint latent model for topics and
segments

We define here a segment as a topically coherent
sequence of contiguous words. By topically coher-
ent, we mean that, even though words in a segment
can be associated to different topics, these topics
are usually related. This view is in line with the
one expressed in (Balikas et al., 2016b), in which
a latent topic model, referred to as copLDA in the
remainder, includes a binding mechanism between
topics within coherent text spans, defined in their
study as noun phrases (NPs). The relation between
topics is captured through a copula that provides
a joint probability for all the topics used in a seg-
ment. That is, to generate words in a segment, one
first jointly generates all the word specific topics
z via a copula, and then generates each word in
the segment from its word specific topic and the
word-topic distribution φ. Figure 1(a) illustrates
this.

Copulas are particularly useful when mod-
eling dependencies between random variables,
as the joint cumulative distribution function
(CDF) FX1,··· ,Xn of any random vector X =
(X1, · · · , Xn) can be written as a function of its
marginals, according to Sklar’s Theorem (Nelsen,
2006):

FX1,··· ,Xn(x1, · · · , xp) = C(FX1(x1), · · · , FXn(xn))

where C is a copula. For latent topic models, as
discussed in (Amoualian et al., 2016), Frank’s cop-
ula is particularly interesting as (a) it is invariant by
permutations and associative, as are the words and
topics z in each segment due to the exchangeability
assumption, and (b) it relies on a single parameter
(denoted λ here) that controls the strength of de-
pendence between the variables and is thus easy to
implement. In Frank’s copula, when the parameter
λ approaches 0, the variables are independent of
each other, whereas when λ approaches +∞, the
variables take the same value. For further details
on copulas, we refer the reader to (Nelsen, 2006).

One important problem, however, with copLDA
is its reliance on a predefined segmentation. Al-
though the information brought by the segmenta-
tion based on NPs helps to improve topic assign-
ment, it may not be flexible enough to capture all
the possible segments of a text. It is easy to correct
this problem by considering all possible segmen-
tations of a document and by choosing the most
appropriate one at the same time that topics are
assigned to words. This is illustrated in Figure 1(b),
where a segmentation S is chosen from the set Sd
of possible segmentations for a document d, and
where each segment in S are generated in turn. We
refer to the associated model as segLDAcopp=0
for reasons that will become clear later.

Another point to be noted about copLDA (and

1801



segLDAcopp=0) is that the topics used in each
segment come from the same document specific
topic distribution θd. This entails that, in these
models, one cannot differentiate the main topics of
a document from potential segment specific topics
that can explain some parts of it. Indeed, some text
segments can refer to topics that are barely present
in other parts of the document; relying on a single
topic distribution may prevent one from capturing
those segment specific topics.

It is possible to overcome this difficulty by gen-
erating a segment specific topic distribution as il-
lustrated in Figure 1(c) (this model is referred to
as segLDAcopλ=0, again for reasons that will be-
come clear later). However, as some words in a
segment can be associated to the general topics of
a document, we introduce a mechanism to choose,
for each word in a segment, a topic either from
the segment specific topic distribution θs or from
the document specific topic distribution θd (this
mechanism is similar to the one used for routes
and levels in (Paul and Girju, 2010)). The choice
between them is based on the Bernoulli variable f ,
as explained in the generative story given below.

The above developments can be combined in a
single, complete model, illustrated in Figure 1(d)
and detailed below. We will simply refer to this
model as segLDAcop.

3.1 Complete generative model
As in standard LDA based models, with V de-
noting the size of the vocabulary of the collec-
tion and K the number of latent topics, β and
φk, 1 ≤ k ≤ K, are V dimensional vectors, α
and θ (i.e., θd, θs, θd,s,n) are K dimensional vec-
tors, whereas zn takes value in {1, · · · ,K}. Lower
indices are used to denote coordinates of the above
vectors. Lastly, Dir denotes the Dirichlet distri-
bution, Cat the categorical distribution (which is
a multinomial distribution with one draw) and we
omit, as is usual, the generation of the length of
the document. The complete model segLDAcop
is then based on the following generative process:

1. Generate, for each topic k, 1 ≤ k ≤ K, a
distribution over the words: φk ∼ Dir(β);

2. For each document d, 1 ≤ d ≤ D:
(a) Choose a document specific topic distribu-

tion: θd ∼ Dir(α);
(b) Choose a segmentation S of the document

uniformly from the set of all possible

segmentations Sd: P (S) = 1|Sd| ;
(c) For each segment s in S:

(i) Choose a segment specific topic distri-
bution: θs ∼ Dir(α);

(ii) For each position n in s, choose fn ∼
Ber(p) and set:

θd,s,n =

{
θs if fn = 1
θd otherwise

(iii) Choose topics Zs = {z1, . . . , zn}
from Frank’s copula with parameter
λ and marginals Cat(θd,s,n);

(iv) For each position n in s, choose word
wn: wn ∼ Cat(φzn).

As on can note, the generative process relies on a
segmentation uniformly chosen from the set of pos-
sible segmentations (step 2.b) to generate related
topics within each segment (Frank’s copula in step
2.c.(iii)), the distribution underlying each word spe-
cific topic zn being either specific to the segment or
general to the document (steps 2.c.(i) and 2.c.(ii)).
The other steps are similar to the standard LDA
steps.

As in almost all previous studies on LDA, α and
β are considered fixed and symmetric, each coor-
dinate of the vector being equal: α1 = · · · = αK .
The hyperparameters p (∈ [0, 1]) of the Bernoulli
distribution and λ (∈ [0,+∞]) of Frank’s copula re-
spectively regulate the choice between the segment
specific and the document specific topic distribu-
tions and the strength of the dependence between
topics in a segment. As for the other hyperparame-
ters, we consider them fixed here (the values for all
hyperparameters are given in Section 4).

As mentioned before, all the models presented
in Figure 1 are special cases of the complete
model segLDAcop: hence segLDAcopλ=0 is
obtained by dropping the topic dependencies,
which amounts to setting λ to (a value close to)
0, segLDAcopp=0 is obtained by relying only on
the topic distribution obtained for the document,
which amounts to setting p to 0, and the previously
introduced copLDA model is obtained by setting
p to 0, and fixing the segmentation.

3.2 Inference with Gibbs sampling
The parameters of the complete model can be di-
rectly estimated through Gibbs sampling. The
Gibbs updates for the parameters φ and θ are the
same as the ones for standard LDA (Blei et al.,

1802



2003). The parameters fn are directly estimated
through: fn ∼ Ber(p). Lastly, for the variables z,
we follow the same strategy as the one described
in (Balikas et al., 2016b) and based on (Amoualian
et al., 2016), leading to:

P (Zs|Z−s,W,Θ,Φ, λ) = p(Zs|Θ, λ)
∏

n

φznwn

where W denotes the document collection, and
Θ and Φ the sets of all θ and φk, 1 ≤ k ≤ K,
vectors. p(Zs|Θ, λ) is obtained by Frank’s copula
with parameter λ and marginals Cat(θd,s,n). As is
standard in topic models, the notation −s means
excluding the information from s.

From the above equation, one can formulate an
acceptance/rejection algorithm based on the follow-
ing steps: (a) sample Zs from p(Zs|Θ, λ) using
Frank’s copula, and (b) accept the sample with
probability

∏
n φ

zn
wn , where n runs over all the po-

sitions in segment s.

3.3 Efficient segmentation

As topics may change from one sentence to another,
we assume here that segments cannot overlap sen-
tence boundaries. The different segmentations of
a document are thus based on its sentence segmen-
tations. In the remainder, we use L to denote the
maximum length of a segment and g(M ;L) to de-
note the number of segmentations in a sentence
of length M , each segment comprising at most L
words.

Generating all possible segmentations of a sen-
tence and then selecting one at random is not an
efficient process as the number of segments rapidly
grows with the length of the sentence. In practice,
however, one can define an efficient segmentation
on the basis of the following proposition, the proof
of which is given in Appendix A:

Proposition 3.1. Let lsi be the random variable
associated to the length of the segment starting
at position i in a sentence of length M (positions
go from 1 to M and lsi takes value in {1, · · · , L}).
Then P (lsi = l) :=

g(M+1−i−l);L)
g(M+1−i;L) defines a proba-

bility distribution over lsi .

Furthermore, the following process is equivalent
to choosing sentence segmentations uniformly from
the set of possible segmentations.

From pos. 1, repeat till end of sentence:
(a) Generate segment length acc. to P;
(b) Add segment to current segmentation;
(c) Move to position after the segment.

In practice, we thus replace steps 2.b and 2.c
of the generative story by a loop over all sen-
tences, and in each sentence use the process de-
scribed in Prop, 3.1. Furthermore, as described in
Appendix A, the values of g needed to compute
P (lsi = l) can be efficiently computed by recur-
rence.

4 Experiments

We conducted a number of experiments aimed at
studying the impact of simultaneously segment-
ing and assigning topics to words within segments
using the proposed segLDAcop model.

Datasets: We considered six publicly available
datasets derived from Pubmed2 (Tsatsaronis et al.,
2015), Wikipedia (Partalas et al., 2015), Reuters3

and New York Times (NYT)4 (Yao et al., 2016).
The first two collections were considered in (Ba-
likas et al., 2016a), we followed their setup by con-
sidering 3 subsets of Wikipedia with different num-
ber of classes (namely, Wiki0, Wiki1 and Wiki2).
The Reuters dataset comes from Reuters-21578,
Distribution 1.0 as investigated in (Bird et al., 2009)
and the NYT dataset is collected from full text of
New York Times global news, from January 1st to
December 31st, 2011.

These collections were processed following
(Blei et al., 2003) by removing a standard list of 50
stop words, lemmatizing, lowercasing and keeping
only words made of letters. To deal with relatively
homogeneous collections, we also removed doc-
uments that are too long. The statistics of these
datasets, as well as the admissible maximal length
for documents, in terms of the number of words
they contain, can be found in Table 1.

Settings: We compared our mod-
els (segLDAcopp=0, segLDAcopλ=0,
segLDAcop) with three models, namely
the standard LDA model, and two previously
introduced models aiming at binding topics within
segments:

1. LDA: Standard Latent Dirichlet Allocation im-
plemented using collapsed Gibbs sampling in-
ference (Griffiths and Steyvers, 2004)5. Note

2https://github.com/balikasg/
topicModelling/tree/master/data

3https://archive.ics.uci.edu/
ml/datasets/Reuters-21578+Text+
Categorization+Collection

4https://github.com/yao8839836/COT/
tree/master/data

5http://gibbslda.sourceforge.net

1803



Wiki0 Wiki1 Wiki2
# words 32,354 70,954 103,308
– vocabulary size 7,853 12,689 14,715

# docs 1,014 2,138 3,152
– maximal length 100 100 100

# labels 17 42 53
Pubmed Reuters NYT

# words 104,683 192,562 237,046
– vocabulary size 12,779 10,479 17,773

# docs 2,059 6,708 2,564
– maximal length 75 50 200

# labels 50 83 -

Table 1: Dataset statistics.

that there are neither segmentation nor topic
binding mechanisms in this model;

2. senLDA: Sentence LDA, introduced in (Ba-
likas et al., 2016a), which forces all words
within a sentence to be assigned to the same
topic. The segments considered thus corre-
spond to sentences, and the binding between
topics within segments is maximal as all word
specific topics are equal;

3. copLDA: Copula LDA, introduced in (Ba-
likas et al., 2016b) already discussed before,
which relies on two types of segments, namely
NPs (extracted with the nltk.chunk pack-
age (Bird et al., 2009)) and single words. In
addition, a copula is also used to bind topics
within NPs, from the document specific topic
distribution.

Both senLDA and copLDA implementations,
can be found in https://github.com/
balikasg/topicModelling.

In all models α and β play a symmetric role and
are respectively fixed to 1/K, following (Asuncion
et al., 2009). For copula based models, λ is set
to 5, following (Balikas et al., 2016b). As already
discussed, p is set to 0 for segLDAcopp=0; it is
set to 0.5 for segLDAcop so as not to privilege a
priori one topic distribution (document or segment
specific) over the other. For sampling from Frank’s
copula, we relied on the R copula package (Hofert
and Maechler, 2011) 6. We chose L (the maximum
length of a segment) using line search forL ∈ [2, 5]
and used L = 3 in all our experiments. Finally, to
illustrate the behaviors of the different models with
different number of topics, we present here the
results obtained with K = 20 and K = 100.

We now compare the different models along
three main dimensions: perplexity, use of topic

6Our complete code will be available for research pur-
poses.

representations for classification and topic coher-
ence.

4.1 Perplexity

We first randomly split here all the collections, us-
ing 75% of them for training, and 25% for testing.

In order to see how well the models fit the data
and following (Blei et al., 2003), we first evaluated
the methods in terms of perplexity defined as:

Perplexity = exp

(
−∑d∈D

∑
w∈d log

∑K
k=1 θ

d
kφ

k
w∑

d∈D |d|

)
,

where d is a test document from the test set D, and
|d| is the total number of words in d, and K is the
total number of topics. The lower the perplexity is,
the better the model fits the test data. Table 2 shows
perplexities of different methods for K = 20 and
K = 100 topics.

50 100 150 200 250 300 350

1,400

1,600

1,800

2,000

2,200

Iterations

Pe
rp

le
xi

ty

NYT

LDA

senLDA

copLDA

segLDAcopp=0
segLDAcopλ=0
segLDAcop

Figure 2: Perplexity with respect to training itera-
tion on NYT collection (20 topics).

From Table 2, it comes out that the best perform-
ing model in terms of perplexity over all datasets
and for different number of topics is segLDAcop.
Further, segLDAcopλ=0, that uses both document
and segment specific topic distributions, performs
better than segLDAcopp=0, which in turn outper-
forms copLDA, bringing evidence that using all
possible segmentations rather than only NPs unit
extracted using a chunker yields a more flexible
and natural topic assignment.
segLDAcop also converges faster than the

other methods to its minimum as it is shown in
Figure 2, depicting the evolution of perplexity of
different models over the number of iterations on
the NYT collection (a similar behavior is observed
on the other collections).

1804



Models
Wiki0 Wiki1 Wiki2 Pubmed Reuters NYT

20 100 20 100 20 100 20 100 20 100 20 100
LDA 853.7 370.9 1144.6 541.1 1225.2 570.6 1267.8 628.7 210.6 118.8 1600.1 1172.1
senLDA 958.4 420.5 1236.7 675.3 1253.1 625.2 1346.3 674.3 254.3 173.6 1735.9 1215.3
copLDA 753.1 264.3 954.1 411.5 1028.6 420.6 1031.5 483.2 206.3 101.3 1551.5 1063.2
segLDAcopp=0 670.2 235.4 904.2 382.4 975.7 409.2 985.5 459.3 194.2 96.7 1504.2 1033.2
segLDAcopλ=0 655.1 222.1 890.3 370.2 949.2 404.3 971.3 451.2 190.1 91.3 1474.6 1014.3
segLDAcop 621.2 213.5 861.2 358.6 934.7 394.4 960.4 442.1 182.1 87.5 1424.2 992.3

Table 2: Perplexity with respect to different number of topics (20 and 100).

Models
Wiki0 Wiki1 Wiki2 Pubmed Reuters

20 100 20 100 20 100 20 100 20 100
LDA 55.3 63.5 42.4 51.4 41.2 48.7 54.1 63.5 75.5 82.7
senLDA 41.4 53.2 33.5 44.5 36.4 40.9 50.2 62.5 69.4 74.2
copLDA 51.2 62.7 43.4 52.1 40.8 46.5 53.5 63.1 75.2 81.5
segLDAcopp=0 59.1 64.2 44.8 51.2 42.3 50.1 55.4 63.1 76.8 82.5
segLDAcopλ=0 61.1 67.4 46.5 53.8 44.1 52.2 57.1 65.2 79.6 84.4
segLDAcop 62.3 68.4 48.4 55.2 44.8 53.5 59.3 66.5 80.2 85.1

Table 3: MiF score (percent) with respect to different number of topics (20 and 100).

4.2 Topical induced representation for
classification

Some studies compare topic models using extrin-
sic tasks such as document classification. In this
case, it is possible to reduce the dimensionality
of the representation space by using the induced
topics (Blei et al., 2003). In this study, we first ran-
domly splitted the datasets, except NYT that does
not contain class information, into training (75%)
and test (25%) sets. We then applied SVMs with
a linear kernel; the value of the hyperparameter
C was found by cross-validation over the training
set {0.01, 0.1, 1, 10, 100}. For datasets where cer-
tain documents have more than one label (Pubmed,
Reuters), we used the one-versus-all approach for
performing multi-label classification.

In Table 3, we report the Micro F1 (MiF) score of
different models on the test sets. Again, the best re-
sults are obtained with segLDAcop, followed by
segLDAcopλ=0. This shows the importance of re-
lying on both document and segment specific topic
distributions. As conjectured before, our model
is able to captures fine grained topic assignments
within documents. In addition, all models rely-
ing on an inferred segmentation (segLDAcopp=0,
segLDAcopλ=0, segLDAcop) outperform the
models relying on fixed segmentations (sentences
or NPs). This shows the importance of being able
to discover flexible segmentations for assigning
topics within documents.

4.3 Topic coherence
Another common way to evaluate topic models is
by examining how coherent the produced topics

are. Doing this manually is a time consuming pro-
cess and cannot scale. To overcome this limitation
the task of automatically evaluating the coherence
of topics produced by topic models received a lot
of attention (Mimno et al., 2011). It has been found
that scoring the topics using co-occurrence mea-
sures, such as the pointwise mutual information
(PMI) between the top-words of a topic, correlates
well with human judgments (Newman et al., 2010).
For this purpose an external, large corpus is used as
a meta-document where the PMI scores of pairs of
words are estimated using a sliding window. As dis-
cussed above, calculating the co-occurrence mea-
sures requires selecting the top-N words of a topic
and performing the manual or automatic evaluation.
Hence, N is a hyper-parameter to be chosen and its
value can impact the results. Very recently, Lau and
Baldwin (2016) showed that N actually impacts
the quality of the obtained results and, in particu-
lar, the correlation with human judgments. In their
work, they found that aggregating the topic coher-
ence scores over several topic cardinalities leads to
a substantially more stable and robust evaluation.

Following the findings of Lau and Baldwin
(2016) and using (Newman et al., 2010)’s equa-
tion, we present in Figure 3 the topic coherence
scores as measured by the Normalized Pointwise
Mutual Information (NPMI) . Their values are in [-
1,1], where in the limit of -1 two words w1 and w2
never occur together, while in the limit of +1 they
always occur together (complete co-occurrence).
For the reported scores, we aggregate the topic co-
herence scores over three different topic cardinali-
ties: N ∈ {5, 10, 15}. segLDAcop model which

1805



Wiki0 Wiki1 Wiki2 PubmedReuters NYT

4

6

8

10

12

5.1

6

8

9

4.4

7.1

6.5
6.7

9.2

10.4

6.1

8.2

6.2 6.3

9.4

10.1

5.8

8.5

6.3

7.2

10.1

10.9

6.4

8.9

6

6.9

9.9
10.3

6

8.6

6.9

7.6

10.5

11.5

6.8

9.2

N
PM

I(
%

)
LDA senLDA copLDA segLDAcopp=0 segLDAcopλ=0 segLDAcop

Figure 3: Topic coherence (NPMI) score with respect to 100 of topics.

uses copulas and segmentation together, shows
the best score for the given reference meta-data
(Wikipedia) in all of the datasets. It should be
noted that segLDAcopλ=0 which has not cop-
ula binder inside the model has less improvement
against the segLDAcopp=0 which has the cop-
ula. This means using copula has more effect on
the topic coherence than only the segment-specific
topic distribution.

4.4 Visualization

In order to illustrate the results obtained by
segLDAcop, we display in Figure 4 the top 10
most probable words over 5 topics (K = 20) for
the Reuters dataset, for both segLDAcop and
LDA. In segLDAcop, topic 1, the top-ranked
words are mostly relevant to the topic “date” (e.g.,

Topic1
march, fell, rose, january, rise,
year, fall, february, pct, week

fell, mln, year, january, dlrs,
rise, rose, pct, billion, february

Topic2
currency, bank, pct, cut, rate,
day, prime, exchange, interest,
national

billion, prime, day, rate, dlrs,
pct, reserve, federal, fed, bank

Topic3
term, agreement, acquire, buy,
sell, unit, acquisition, corp,
company, sale

term, dlrs, buy, company, sell,
unit, corp, acquisition, sale,
mln

Topic4
approved, american, common,
split, merger, company, board,
stock, share, shareholder

acquire, mln, company, com-
mon, stock, shareholder, share,
corp, merger, dlrs

Topic5
tokyo, life, intent, letter, buy,
insurance, yen, japan, dealer,
dollar

central, european, japan, yen,
ec, dollar, bank, rate, dealer,
market

Figure 4: Top-10 words of segLDAcop (left) vs
LDA (right) for the Reuters (5 out of 20 topics).

Ralph Borsodi was an economics theorist and practical experimenter

interested in ways of living

Figure 5: Topic assignments with segmentation
boundaries using segLDAcop. Colors are topics
(examples from Wiki0 including stopwords with
20 topics).

march, january, year, fall, february, week). How-
ever, a similar topic learned by LDA appears to
involve less such words (year, january, february),
indicating a less coherent topic.

Figure 5 illustrates another aspect of our model,
namely the possibility to detect topically coherent
segments. In particular, as one can note, the sen-
tence is segmented in six parts by our model, the
first one is a NP, Ralph Borsodi where one single
topic is assigned to both words. We observe a sim-
ilar coherence in topic assignments on other NPs
and segments, in which a single topic is used for
the words involved. The data-driven approach we
have adopted here can discover such fine grained
differences, something the approaches based on
fixed segmentations (either based on sentences or
NPs), are less likely to achieve.

5 Discussion

In this paper, we have introduced an LDA-based
model that generates topically coherent segments
within documents by jointly segmenting documents
and assigning topics to their words. The coherence
between topics is ensured through Frank’s copula,
that binds the topics associated to the words of a
segment. In addition, this model relies on both
document and segment specific topic distributions
so as to capture fine grained differences in topic
assignments. We have shown that this model natu-
rally encompasses other state-of-the-art LDA-based
models proposed to accomplish the same task, and
that it outperforms these models over six publicly
available collections in terms of perplexity, Nor-
malized Pointwise Mutual Information (NPMI),
a measure used to assess the coherence of topics
with documents, and the Micro F1-measure in a
text classification context. Our results confirm the
importance of a flexible segmentation as well as a

1806



binding mechanism to produce topically coherent
segments.

As regards complexity, it is true that more com-
plex models, as the one we are considering, are
more prone to underfitting (when data is scarce)
and overfitting than simpler models. This said, the
experimental results on perplexity (in which the
word-topic distributions are fixed) and on classi-
fication (based on the topical induced representa-
tions) suggest that our model neither underfits nor
overfits compared to simpler models. We believe
that this is due to the fact that the main additional
parameters in our model (the segment specific topic
distribution) do not really add complexity as they
are drawn from the same distribution as the stan-
dard document specific topics. Furthermore, the
parameters p and f are simple parameters to choose
between these two distributions.

The comparison with other segmentation meth-
ods is also an important point. While state-of-the-
art supervised segmentation models can be used
before applying the LDA model, we note such a
pipeline approach comes with several limitations.
The approach requires external annotated data to
train the segmentation models, where certain do-
main and language specific information need to be
captured. By contrast, our unsupervised approach
learns both segmentations and topics jointly in a
domain and language independent manner. Fur-
thermore, existing supervised segmentation models
are largely designed for a very different purpose
with strong linguistic motivations, which may not
align well with our main goal in this paper which is
improving topic coherence in topic modeling. Sim-
ilarly, unsupervised approaches, used for example
in the TDT (Topic Detection and Tracking) cam-
paigns or more recently in Du et al. (2013), usually
consider coarse-grained topics, that can encom-
pass several sentences. In contrast, our approach
aims at identifying fine-grained topics associated
with coherent segments that do not overlap sen-
tence boundaries. These considerations, explain
the choice of the baselines retained: they are based
on segments of different granularities (words, NPs,
sentences) that do not overlap sentence boundaries.

In the future, we plan on relying on other infer-
ence approaches, based for example on variational
Bayes known to yield better estimates for perplex-
ity (Asuncion et al., 2009); it is however not certain
that the gain in perplexity one can expect from
the use of variational Bayes approaches will nec-

essarily result in a gain in, say, topic coherence.
Indeed, the impact of the inference approach on
the different usages of latent topic models for text
collections remains to be better understood.

Acknowledgments

We would like to thank the reviewers for their help-
ful comments. Most of this work was done when
Hesam Amoualian was visiting Singapore Univer-
sity of Technology and Design. This work is sup-
ported by MOE Tier 1 grant SUTDT12015008, also
partly supported by the LabEx PERSYVAL-Lab
ANR-11-LABX-0025.

References
Hesam Amoualian, Marianne Clausel, Eric Gaussier,

and Massih-Reza Amini. 2016. Streaming-lda:
A copula-based approach to modeling topic de-
pendencies in document streams. In Proceed-
ings of the 22nd International Conference on
Knowledge Discovery and Data Mining. ACM,
New York, NY, USA, SIGKDD, pages 695–704.
https://doi.org/10.1145/2939672.2939781.

Arthur Asuncion, Max Welling, Padhraic Smyth,
and Yee Whye Teh. 2009. On smoothing
and inference for topic models. In Proceed-
ings of the 25th Conference on Uncertainty
in Artificial Intelligence. AUAI Press, Arling-
ton, Virginia, United States, UAI, pages 27–34.
http://dl.acm.org/citation.cfm?id=1795114.1795118.

Georgios Balikas, Massih-Reza Amini, and Marianne
Clausel. 2016a. On a topic model for sentences. In
Proceedings of the 39th International Conference on
Research and Development in Information Retrieval.
ACM, New York, NY, USA, SIGIR, pages 921–924.
https://doi.org/10.1145/2911451.2914714.

Georgios Balikas, Hesam Amoualian, Marianne
Clausel, Eric Gaussier, and Massih R Amini. 2016b.
Modeling topic dependencies in semantically coher-
ent text spans with copulas. In Proceedings of
the 26th International Conference on Computational
Linguistics: Technical Papers. The COLING 2016
Organizing Committee, Osaka, Japan, COLING,
pages 1767–1776. http://aclweb.org/anthology/C16-
1166.

Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O’Reilly, Beijing. http://www.nltk.org/book/.

David M. Blei, Andrew Y. Ng, and Michael I.
Jordan. 2003. Latent dirichlet allocation.
Journal of Machine Learning 3:993–1022.
http://dl.acm.org/citation.cfm?id=944919.944937.

Jordan Boyd-Graber and David Blei. 2008. Syn-
tactic topic models. In Proceedings of the

1807



21st International Conference on Neural In-
formation Processing Systems. Curran As-
sociates Inc., USA, NIPS, pages 185–192.
http://dl.acm.org/citation.cfm?id=2981780.2981804.

Scott Deerwester, Susan T. Dumais, George W.
Furnas, Thomas K. Landauer, and Richard
Harshman. 1990. Indexing by latent se-
mantic analysis. Journal of the American
Society for Information Science 41(6):391–
407. http://dx.doi.org/10.1002/(SICI)1097-
4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9.

Lan Du, Wray Buntine, and Huidong Jin.
2010. A Segmented Topic Model Based
on the Two-parameter Poisson-Dirichlet Pro-
cess. Journal of Machine learning 81(1):5–19.
https://doi.org/10.1007/s10994-010-5197-4.

Lan Du, Wray Buntine, and Mark Johnson. 2013.
Topic Segmentation with a Structured Topic Model.
In Proceedings of The Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, Human Language Technolo-
gies. HLT-NAACL, pages 190–200. http://dblp.uni-
trier.de/db/conf/naacl/naacl2013.html/DuBJ13.

Gal Elidan. 2013. Copulas in Machine Learning,
Springer Berlin Heidelberg, Berlin, Heidelberg,
pages 39–60. https://doi.org/10.1007/978-3-642-
35407-6_3.

Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Journal of the National
Academy of Sciences 101(suppl 1):5228–5235.
https://doi.org/10.1073/pnas.0307752101.

Thomas L Griffiths, Mark Steyvers, David M Blei,
and Joshua B Tenenbaum. 2005. Integrating
topics and syntax. In L. K. Saul, Y. Weiss,
and L. Bottou, editors, Advances in the Inter-
national Conference on Neural Information Pro-
cessing Systems. MIT Press, NIPS, pages 537–
544. http://papers.nips.cc/paper/2587-integrating-
topics-and-syntax.pdf.

Marius Hofert and Martin Maechler. 2011. Nested
Archimedean Copulas Meet R: The nacopula Pack-
age. Journal of Statistical Software 39(i09):–.
https://doi.org/http://hdl.handle.net/10.

Thomas Hofmann. 1999. Probabilistic latent se-
mantic indexing. In Proceedings of the 22Nd
Annual International Conference on Research
and Development in Information Retrieval. ACM,
New York, NY, USA, SIGIR, pages 50–57.
https://doi.org/10.1145/312624.312649.

Jey Han Lau and Timothy Baldwin. 2016. The
sensitivity of topic coherence evaluation to topic
cardinality. In Proceedings of The Annual Con-
ference of the North American Chapter of the
Association for Computational Linguistics, Hu-
man Language Technologies, San Diego California,
USA, June 12-17, 2016. NAACL, pages 483–487.
http://aclweb.org/anthology/N/N16/N16-1057.pdf.

Jey Han Lau, Timothy Baldwin, and David Newman.
2013. On collocations and topic models. Journal
of ACM Trans. Speech Lang. Process. 10(3):10:1–
10:14. https://doi.org/10.1145/2483969.2483972.

David Mimno and Andrew McCallum. 2007. Orga-
nizing the oca: Learning faceted subjects from a
library of digital books. In Proceedings of the
7th Joint Conference on Digital Libraries. ACM,
New York, NY, USA, JCDL ’07, pages 376–385.
https://doi.org/10.1145/1255175.1255249.

David Mimno, Hanna M. Wallach, Edmund Tal-
ley, Miriam Leenders, and Andrew McCallum.
2011. Optimizing semantic coherence in topic
models. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics,
Stroudsburg, PA, USA, EMNLP, pages 262–272.
http://dl.acm.org/citation.cfm?id=2145432.2145462.

Roger B. Nelsen. 2006. An Introduction to Cop-
ulas (Springer Series in Statistics). Springer-
Verlag New York, Inc., Secaucus, NJ, USA.
http://www.springer.com/gp/book/9780387286594.

David Newman, Jey Han Lau, Karl Grieser, and
Timothy Baldwin. 2010. Automatic evalua-
tion of topic coherence. In Proceedings of
The Annual Conference of the North Ameri-
can Chapter of the Association for Computa-
tional Linguistics, Human Language Technolo-
gies. Association for Computational Linguistics,
Stroudsburg, PA, USA, NAACL, pages 100–108.
http://dl.acm.org/citation.cfm?id=1857999.1858011.

Ioannis Partalas, Aris Kosmopoulos, Nicolas Bask-
iotis, et al. 2015. LSHTC: A Benchmark for
Large-Scale Text Classification. Journal of CoRR
abs/1503.08581. http://arxiv.org/abs/1503.08581.

Michael Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discov-
ering multi-faceted topics. In Proceedings
of the 24th Conference on Artificial Intelli-
gence. AAAI Press, AAAI, pages 545–550.
http://dl.acm.org/citation.cfm?id=2898607.2898695.

Matthew Purver, Thomas L Griffiths, Konrad P. Körd-
ing, and Joshua B. Tenenbaum. 2006. Unsuper-
vised topic modelling for multi-party spoken dis-
course. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics, Stroudsburg, PA, USA, ACL, pages 17–
24. https://doi.org/10.3115/1220175.1220178.

Akihiro Tamura and Eiichiro Sumita. 2016. Bilin-
gual segmented topic model. In Proceed-
ings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics. ACL.
http://aclweb.org/anthology/P/P16/P16-1120.pdf.

1808



Yee Whye Teh. 2006. A hierarchical bayesian lan-
guage model based on pitman-yor processes. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th An-
nual Meeting of the Association for Computational
Linguistics. Association for Computational Linguis-
tics, Stroudsburg, PA, USA, ACL, pages 985–992.
https://doi.org/10.3115/1220175.1220299.

George Tsatsaronis, Georgios Balikas, Prodro-
mos Malakasiotis, et al. 2015. An overview
of the BIOASQ large-scale biomedical seman-
tic indexing and question answering competi-
tion. Journal of BMC Bioinformatics 16(1):138.
https://doi.org/10.1186/s12859-015-0564-6.

Dingding Wang, Shenghuo Zhu, Tao Li, and Yi-
hong Gong. 2009. Multi-document summa-
rization using sentence-based topic models.
In Proceedings of the Conference on Associ-
ation for Computational Linguistics. Associ-
ation for Computational Linguistics, Strouds-
burg, PA, USA, ACL-IJCNLP, pages 297–300.
http://dl.acm.org/citation.cfm?id=1667583.1667675.

Xuerui Wang, Andrew McCallum, and Xing Wei.
2007. Topical n-grams: Phrase and topic discov-
ery, with an application to information retrieval.
In Proceedings of the 7th International Confer-
ence on Data Mining. IEEE Computer Society,
Washington, DC, USA, ICDM, pages 697–702.
https://doi.org/10.1109/ICDM.2007.86.

Xing Wei and W. Bruce Croft. 2006. Lda-based doc-
ument models for ad-hoc retrieval. In Proceedings
of the 29th Annual International Conference on Re-
search and Development in Information Retrieval.
ACM, New York, NY, USA, SIGIR, pages 178–185.
https://doi.org/10.1145/1148170.1148204.

Liang Yao, Yin Zhang, Baogang Wei, Lei Li, Fei
Wu, Peng Zhang, and Yali Bian. 2016. Con-
cept over time: the combination of probabilistic
topic model with wikipedia knowledge. Journal
of Expert Systems with Applications 60:27 – 38.
https://doi.org/10.1016/j.eswa.2016.04.014.

Hua-Jun Zeng, Qi-Cai He, Zheng Chen, Wei-Ying
Ma, and Jinwen Ma. 2004. Learning to clus-
ter web search results. In Proceedings of the
27th Annual International Conference on Research
and Development in Information Retrieval. ACM,
New York, NY, USA, SIGIR, pages 210–217.
https://doi.org/10.1145/1008992.1009030.

A Efficient segmentation

Let us recall the property presented before:

Proposition A.1. Let lsi be the random variable
associated to the length of the segment starting
at position i in a sentence of length M (positions
go from 1 to M and lsi takes value in {1, · · · , L}).

Then P (lsi = l) :=
g(M+1−i−l);L)
g(M+1−i;L) defines a proba-

bility distribution over lsi .

Furthermore, the following process is equivalent
to choosing sentence segmentations uniformly from
the set of possible segmentations.
From pos. 1, repeat till end of sentence:
(a) Generate segment length acc. to P;
(b) Add segment to current segmentation;
(c) Move to position after the segment.

Proof Any segmentation of the sentence of length
M starts with either a segment of length 1, a seg-
ment of length 2, · · · , or a segment of length L.
Thus, g(M ;L) can be defined through the follow-
ing recurrence relation:

g(M ;L) =
L∑

l=1

g(M − l;L) (1)

together with the initial values
g(1;L), g(2;L), · · · , g(L;L), which can be
computed offline (for example, for L = 3, one has:
g(1; 3) = 1, g(2; 3) = 2, g(3; 3) = 4). Note that
g(1;L) = 1 for all L.

Thus:

L∑

l=1

P (lsi = l) =
L∑

l=1

g(M + 1− i− l);L)
g(M + 1− i;L) = 1

due to the recurrence relation on g. This proves the
first part of the proposition.

Using the process described above where seg-
ments are generated one after another according
to P , for a segmentation S, comprising |S| seg-
ments, let us denote by l1, l2, · · · , l|S| the lengths
of each segment and by i1, i2, · · · , i|S| the starting
positions of each segment (with i1 = 1). One has,
as segments are independent of each other:

P (S) =

|S|∏

j=1

P (lsij = lj) =

|S|∏

j=1

g(M + 1− (ij + lj);L)
g(M + 1− ij ;L)

=
g(M − l1;L)
g(M ;L)

g(M − l1 − l2;L)
g(M − l1;L)

· · · = 1
g(M ;L)

as g(1;L) = 1. This concludes the proof of the
proposition. 2

Furthermore, as one can note from Eq. 1, the
various elements needed to compute P (lsi = l) can
be efficiently computed, the time complexity being
equal to O(M). In addition, as the number of dif-
ferent sentence lengths is limited, one can store the
values of g to reuse them during the segmentation
phase.

1809


	Topical Coherence in LDA-based Models through Induced Segmentation

