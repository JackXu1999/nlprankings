



















































Hierarchical Machine Translation With Discontinuous Phrases


Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 228–238,
Lisboa, Portugal, 17-18 September 2015. c©2015 Association for Computational Linguistics.

Hierarchical Machine Translation With Discontinuous Phrases

Miriam Kaeshammer
University of Düsseldorf

Universitätsstraße 1
40225 Düsseldorf, Germany

kaeshammer@phil.uni-duesseldorf.de

Abstract

We present a hierarchical statistical ma-
chine translation system which supports
discontinuous constituents. It is based on
synchronous linear context-free rewriting
systems (SLCFRS), an extension to syn-
chronous context-free grammars in which
synchronized non-terminals span k ≥ 1
continuous blocks on either side of the
bitext. This extension beyond context-
freeness is motivated by certain complex
alignment configurations that are beyond
the alignment capacity of current transla-
tion models and their relatively frequent
occurrence in hand-aligned data. Our
experiments for translating from German
to English demonstrate the feasibility of
training and decoding with more expres-
sive translation models such as SLCFRS
and show a modest improvement over a
context-free baseline.

1 Introduction

In statistical machine translation, phrase-based
translation models with a beam search decoder
(Koehn et al., 2003) and tree-based models with
a CYK decoder represent two prominent types of
approaches. The latter usually employ some form
of synchronous context-free grammar (SCFG).
They can be grouped into so-called hierarchi-
cal phrase-based models that are formally syntax-
based, such as in Chiang (2007), and models
where hierarchical units are somehow linguisti-
cally motivated, e.g. in Zollmann and Venugopal
(2006) and Hoang and Koehn (2010).

The adequacy of all of these models has been
questioned, as the space of alignments that they
generate is limited. Inside-out alignments are be-
yond the alignment capacity of SCFG of rank
2 (henceforth 2-SCFG) and inversion transduc-

(i)

a b c d

b d a c

(ii)

a b

a1 b1 a2 b2

(iii)

a1 b a2

b1 a b2

Figure 1: Complex alignment configurations: (i)
inside-out alignment; (ii) CDTU; (iii) bonbon.
The configurations can also occur upside down.

tion grammar (Wu, 1997), but they can be gener-
ated with phrase-based translation models thanks
to the reordering component of standard de-
coders. Cross-serial discontinuous translation
units (CDTU) (Søgaard and Kuhn, 2009) and bon-
bon configurations (Simard et al., 2005) in con-
trast can neither be generated by a phrase-based
translation system nor by an SCFG-based one. It
is thereby assumed that a translation unit, the tran-
sitive closure of a set of nodes of the bipartite
alignment graph, represents minimal translational
equivalence, and therefore that an adequate trans-
lation grammar formalism should be able to gen-
erate each translation unit separately.

The aforementioned problematic alignment
configurations are schematically depicted in Fig-
ure 1. Alignment (i) is an inside-out alignment; it
is formed by four translation units (a, b, c and d).
CDTUs (ii) and bonbons (iii) each consist of two
intertwined discontinuous translation units.

Several studies have investigated the alignment
capacity of SCFG-based and phrase-based trans-
lation models in different setups (Wellington et
al., 2006; Søgaard and Kuhn, 2009; Søgaard and
Wu, 2009; Søgaard, 2010; Kaeshammer, 2013).
For example, Wellington et al. (2006) find that
inside-out alignments occur in 5% of their manu-
ally aligned English-Chinese sentence pairs. In the
study of Kaeshammer (2013), 9% of the sentence
pairs in a Spanish-French data set and 5.5% of the
sentence pairs in an English-German data set can-
not be generated by a 2-SCFG. In addition, Kaes-

228



hammer and Westburg (2014) qualitatively inves-
tigate the instances of the complex alignment con-
figurations in the same English-German data set
and find that even though some of them are due to
annotation errors, most of them are correctly an-
notated phenomena that one would like to be able
to generate when translating.

To be able to induce the alignment configu-
rations in question, more expressive translation
models and corresponding decoding algorithms
are necessary. For the phrase-based models, Gal-
ley and Manning (2010) propose a translation
model that uses discontinuous phrases and a cor-
responding beam search decoder. For tree-based
models, a grammar formalism beyond the power
of context-free grammar is necessary. Søgaard
(2008) proposes to apply range concatenation
grammar; Kaeshammer (2013) puts forward the
idea of using synchronous linear context-free
rewriting systems (SLCFRS), a direct extension of
SCFG to discontinous constituents. To the best of
our knowledge, neither of the two proposals have
resulted in an actual machine translation system.

With this work, we extend the line of research
proposed in Kaeshammer (2013), and present the
first full tree-based statistical machine translation
system that allows for discontinuous constituents.
It is thus able to produce the complex alignment
configurations in Figure 1. As such, it combines
the advantage of being able to learn and gener-
ate discontinuous phrases with the benefits of tree-
based translation models.

Currently, our system is hierarchical phrase-
based, i.e. it does not make use of linguistically
motivated syntactic annotation. However, it will
be straightforward to transfer methods to inte-
grate linguistic constituency information from the
SCFG-based machine translation literature (such
as Zollmann and Venugopal (2006)) to our ap-
proach. This is particularly interesting, since, in
the monolingual parsing community, approaches
that are able to produce constituency trees with
discontinuous constituents have become increas-
ingly popular (Maier, 2010; van Cranenburgh and
Bod, 2013; Kallmeyer and Maier, 2013). Re-
cently, such parsers have reached a speed with
which it would actually be feasible to parse the
training set of a machine translation system (Ver-
sley, 2014; Maier, 2015; Fernández-González and
Martins, 2015), which is necessary to train syntac-
tically motivated translation grammars.

In this work, we define a translation model
based on SLCFRS, explain the training of a corre-
sponding hierarchical phrase-based grammar, pro-
vide details about a corresponding decoder and re-
sults of experiments for translating from German
to English.

2 Model

Our translation model is a weighted synchronous
LCFRS. Conceptually, this grammar formalism is
very close to synchronous CFG, with the addition
that non-terminals span tuples of strings (instead
of just strings) on either side of the bitext. Just as
SCFGs, an SLCFRS can be used for synchronous
parsing of parallel sentences as well as for trans-
lating monolingual sentences. For the latter, the
source side of the synchronous grammar is used to
parse the input text, thereby generating target side
derivations from which the translations can be read
off.

2.1 Synchronous LCFRS

An LCFRS1 (Vijay-Shanker et al., 1987; Weir,
1988) is a tuple G = (N,T, V, P, S) where N
is a finite set of non-terminals with a function
dim: N → N determining the fan-out of each
A ∈ N ; T and V are disjoint finite sets of ter-
minals and variables; S ∈ N is the start symbol
with dim(S) = 1; and P is a finite set of rewrit-
ing rules

A(α1, . . . , αdim(A))→ A1(Y (1)1 , . . . , Y (1)dim(A1))
· · ·Am(Y (m)1 , . . . , Y (m)dim(Am))

where A,A1, . . . , Am ∈ N , Y (i)j ∈ V for 1 ≤
i ≤ m, 1 ≤ j ≤ dim(Ai) and αi ∈ (T ∪ V )∗ for
1 ≤ i ≤ dim(A), for a rankm ≥ 0. For all r ∈ P ,
it holds that every variable Y in r occurs exactly
once in the left-hand side (LHS) and exactly once
in the right-hand side (RHS) of r.

A non-terminal is instantiated with respect to
some input string w such that terminals and vari-
ables are consistently mapped to w. A rule r ex-
plains how an instantiated LHS non-terminal can
be rewritten by its instantiated RHS non-terminals.
A derivation starts with the start symbol S instan-
tiated to the input string w. All strings that can

1We use the syntax of simple range concatenation gram-
mars (Boullier, 1998), an equivalent formalism.

229



〈A(a, c) → ε , C(a, c) → ε〉
〈B(b, d) → ε , D(bd) → ε〉
〈A(aX, cZ) → A 1 (X, Z) , C(aX, Zc) → C 1 (X, Z)〉
〈B(bY, dU) → B 1 (Y, U) , D(bY d) → D 1 (Y )〉
〈S(XY ZU) → A 1 (X, Z)B 2 (Y, U) ,

S(XY Z) → C 1 (X, Z)D 2 (Y )〉

Figure 2: Rules of an SLCFRS for L =
{〈anbmcndm, anbmdmcn〉 |n,m > 0}, taken
from Kaeshammer (2013).

be rewritten to ε are in the language of the gram-
mar. For more formal definitions, see for example
Kallmeyer (2010).

The rank of a grammar G is the maximal rank
of any of its rules, and its fan-out is the maximal
fan-out of any of its non-terminals. G is called a
(u, v)-LCFRS if it has rank u and fan-out v. A
CFG is the special case of an LCFRS with fan-out
v = 1. An LCFRS is monotone if, for every rule
and every RHS non-terminal, the order of the vari-
ables in the arguments of this non-terminal is the
same as the order of these variables in the argu-
ments of the LHS non-terminal of this rule. This
means that the order of (instantiated) arguments
of the LHS non-terminal of a rule always corre-
sponds to their order in the input sentence. An
LCFRS is called ε-free if all of its rules in P are
ε-free, which means that none of their LHS argu-
ments is the empty string ε.2

The definition of synchronous LCFRS
(SLCFRS) follows the definition of synchronous
CFG, as for example in Satta and Peserico (2005).
An SLCFRS (Kaeshammer, 2013) is a tuple
G = (Ns, Nt, Ts, Tt, Vs, Vt, P, Ss, St) where Ns,
Ts, Vs, Ss, resp. Nt, Tt, Vt, St are defined as for
LCFRS. They denote the alphabets for the source
and target side respectively. P is a finite set of
synchronous rewriting rules 〈rs, rt,∼〉 where
rs and rt are LCFRS rewriting rules based on
Ns, Ts, Vs and Nt, Tt, Vt respectively, and ∼ is
a bijective mapping of the non-terminals in the
RHS of rs to the non-terminals in the RHS of rt.
This link relation is represented by co-indexation
in the synchronous rules. During a derivation, the
yields of two co-indexed non-terminals have to
be explained from one synchronous rule. 〈Ss, St〉
is the start pair. In such a derivation, we call the
yield of Ss the source side yield and the yield of
St the target side yield. SLCFRS are equivalent to

2An LCFRS is also ε-free if it contains a rule S(ε) → ε,
but S does not appear in any RHS of the rules in P .

〈S 1 (aabccd), S 1 (aabdcc)〉
⇒ 〈A 2 (aa, cc)B 3 (b, d), C 2 (aa, cc)D 3 (bd)〉
⇒ 〈A 2 (aa, cc), C 2 (aa, cc)〉
⇒ 〈A 4 (a, c), C 4 (a, c)〉
⇒ ε

Figure 3: Derivation of 〈aabccd, aabdcc〉 using
the rules in Figure 2.

simple range concatenation transducers (Bertsch
and Nederhof, 2001).

Figure 2 shows an example. The syn-
chronous rules translate cross-serial dependencies
into nested ones. A sample derivation is shown in
Figure 3.

The tuple (Ns, Ts, Vs, Ps, Ss) is called the
source side grammar Gs and (Nt, Tt, Vt, Pt, St)
the target side grammar Gt, where Ps is the set
of all rs in P and Pt is the set of all rt in P . The
rank u of a SLCFRS G is the maximal rank of Gs
and Gt, and the fan-out v of G is the sum of the
fan-outs of Gs and Gt. One may write vvGs |vGt to
make clear how the fan-out ofG is distributed over
the source and the target side. As in the monolin-
gual case, a corresponding grammar G is called
a (u, v)-SLCFRS. The rank of the corresponding
grammar in Figure 2 is 2 and its fan-out 42|2. We
call an SLCFRS monotone if the source side gram-
mar as well as the target side grammar is mono-
tone. We call an SLCFRS ε-free if the source side
grammar as well as the target side grammar is ε-
free.

We further define some terms which will be
used in the following sections. A range in a string
wn1 is a pair 〈l, r〉 with 0 ≤ l ≤ r ≤ n. Its yield
〈l, r〉(w) is the string wrl+1. The yield of a vector
of ranges ρ(w) is the vector of the yields of the
single ranges.

2.2 Definition

Given a source sentence f and an SLCFRS, gen-
erally, many derivations will have f as the source
side yield, leading to many (different) target side
yields, i.e. possible translations e. As it is standard
in statistical machine translation, we use a log-
linear model over derivations D to weight those
translation options. The definition closely follows
the model definition for SCFG, see Chiang (2007)

230



for example.

P (D) ∝
∏
i

φi(D)λi

∝ PLM (e)λLM · w(D)
where φi are features defined on the derivations,
and λi are feature weights to be set during tun-
ing. An n-gram language model provides a fea-
ture PLM (e) for the probability of seeing the tar-
get sentence e as derived by D. The other features
(i 6= LM ) are defined on the rules of a weighted
SLCFRS which are used in the derivation D.

A weighted SLCFRS is an SLCFRS that is addi-
tionally equipped with a weight function w which
assigns a weight to each synchronous rule r ∈ P .
To fit the log-linear model, we define w as

w(r) =
∏
i 6=LM

φi(r)λi

The weight of a derivation D is then

w(D) =
∏
r∈D

w(r)

2.3 Features
We use the following standard features φi(r):

• translation probabilities in both directions
P (rs|rt) and P (rt|rs),
• lexical weights lex(rs|rt) and lex(rt|rs)

(Koehn et al., 2003) that estimate how well
the terminals in the rule translate to each
other,

• a rule penalty exp(1),
• a word penalty exp(−|wt|) where |wt| is the

number of terminals that occur in rt.

In addition, we devise features that character-
ize the amount of expressivity beyond context-
freeness of the applied rules. The source gap de-
gree of r is the fan-out of rs minus 1, and the target
gap degree of r is the fan-out of rt minus 1. See
Maier and Lichte (2011) for more details about
gap degree. These features can be read off the
rules r directly. They allow the model to learn a
preference for or against using the more powerful
rules.

We also use glue rules, as proposed by Chiang
(2005), which allow for a monotone combination
of synchronous constituents as in a phrase-based
model. A glue rule feature of value exp(1) with
its weight λglue controls their usage.

3 Training

The synchronous rules are extracted from a cor-
pus of parallel sentences that have already been
word-aligned. Following Och and Ney (2004) and
Chiang (2005), we extract all rules that are con-
sistent with the word alignment A of a sentence
pair 〈f, e〉 in a two-step procedure. First, initial
phrase pairs are extracted; they correspond to ter-
minal rules. Second, hierarchical rules are created
by replacing phrase pairs that are contained within
other phrase pairs with non-terminals/variables.

The crucial difference to previous work on
translation with SCFG is that initial phrases do not
have to be continuous. Instead, a phrase is a set of
word indices, as in Galley and Manning (2010).
Given 〈f, e〉 and a corresponding word alignment
A, a phrase pair (s̄, t̄) is consistent with A if the
following holds:

∀(i, j) ∈ A : i ∈ s̄↔ j ∈ t̄
∧ ∃i ∈ s̄, j ∈ t̄ : (i, j) ∈ A

For each initial phrase pair (s̄, t̄), a terminal
synchronous rule of the following form is created
and added to P :

〈X(ρs(f))→ ε,X(ρt(e))→ ε〉
ρs and ρt are range vectors, applied to the source
sentence f and target sentence e respectively. ρs
(respectively ρt) is obtained by partitioning s̄ (re-
spectively t̄) such that each subset contains all and
only consecutive indices, designating a continuous
block of the discontinuous phrase. Such a subset
X is turned into a range 〈l, r〉 with l = min(X)
and r = max(X). The ranges obtained from s̄
(respectively t̄), in ascending order, form ρs (re-
spectively ρt).

Furthermore, if P contains a rule 〈X(α) →
Ψ, X(β) → Θ〉 that has been built from a phrase
pair (s̄, t̄) and the set of phrase pairs contains a
pair (s̄′, t̄′) such that s̄′ ⊂ s̄ and t̄′ ⊂ t̄, we add the
following new rule to P :

〈X(α′)→ ΨX k (Y1, . . . , Yhs),
X(β′)→ ΘX k (Z1, . . . , Zht)〉

A new non-terminal X is added to the RHS of
rs and rt. k is an index that is not yet used in
the bijective mapping of non-terminals in Ψ and
Θ. Range vectors ρs′ and ρt′ are deduced from s̄′

and t̄′ as described above. Each range in ρs′ (re-
spectively ρt′) is associated with a variable Yi for

231



je ne veux plus jouer

I do not want to play anymore

Initial phrase pairs:
1. jouer — to play

2. veux — do . . . want

3. ne veux plus — do not want . . . anymore

4. ne veux plus jouer — do not want to play anymore

. . .

Rules:
1. 〈X(jouer) → ε, X(to play) → ε〉
2. 〈X(veux) → ε, X(do, want) → ε〉
3. 〈X(ne veux plus) → ε, X(do not want, anymore) → ε〉
4. 〈X(ne veux plus jouer) → ε,

X(do not want to play anymore) → ε〉
5. 〈X(ne Y1 plus) → X 1 (Y1),

X(Z1 not Z2, anymore) → X 1 (Z1, Z2)〉
6. 〈X(ne veux plus Y1) → X 1 (Y1),

X(do not want Z1 anymore) → X 1 (Z1)〉
7. 〈X(ne Y1 plus Y2) → X 1 (Y1)X 2 (Y2),

X(Z1 not Z2Z3 anymore) → X 1 (Z1, Z2)X 2 (Z3)〉
. . .

Figure 4: Sample rules that are extracted from the
provided aligned sentence pair.

1 ≤ i ≤ hs (respectively Zj for 1 ≤ j ≤ ht),
where hs (respectively ht) is the length of ρs′ (re-
spectively ρs′). They have to be variables that
are not yet in use in α (respectively β). Those
variables constitute the arguments of the new syn-
chronous non-terminal X . Accordingly, hs and ht
are the fan-outs of X on the source and the target
side respectively. α′ (respectively β′) is created
from α (respectively β) by replacing the termi-
nals that correspond to ranges in ρs′ (respectively
ρt′) with the variable Yi (respectively Zj) that as
been associated to the range. Note that this extrac-
tion yields only monotone and ε-free (S)LCFRS,
which simplifies parsing.

The discontinuous rule extraction procedure is
exemplified in Figure 4. Rule #5 for example was
created from rule #3 by substituting phrase pair #2.
Note that phrase pairs #1 and #4 are also extracted
by a phrase-based system, and rules #1, #4 and #6
are also generated by a hierarchical phrase-based,
i.e. SCFG-based, system. Rule #6 would usually
be written down as

X → 〈ne veux plusX 1 , do not want toX 1 anymore〉

However, just as Galley and Manning (2010), we

extract many more rules that also capture discon-
tinuous translation units. In addition, we also ex-
tract rules which are discontinuous and hierarchi-
cal at the same time. They capture relationships
between possibly discontinuous translation units.

Enumerating all discontinuous phrase pairs is
exponential in the maximum phrase length. There-
fore, in addition to the constraints that are gener-
ally set for SCFG extraction (e.g. phrase length,
number of non-terminals, adjacent non-terminals
on the source side, unaligned words at phrase
edges, see Chiang (2007)), we also restrict the
number of words that can be in a gap, we disal-
low unaligned blocks, and we restrict the number
of continuous blocks in a phrase to 2. The latter is
motivated by the results presented in Kaeshammer
(2013) where a fan-out of 42|2 is enough to derive
the alignments in all data sets. We furthermore
analyse the alignments of the training data before
running the extraction and only allow discontinu-
ous phrase pairs in synchronous spans which con-
tain any of the alignment configurations that are
beyond the power of SCFG.

As derivations are not observable in the train-
ing data, we use the method described in Chi-
ang (2007) to hypothesize a distribution based
on the counts of the extracted rules and then use
relative-frequency estimation to obtain P (rs|rt)
and P (rt|rs).

4 Decoder

Our decoder closely follows the methodology of
current SCFG decoders, with the difference that
it is able to handle source and target discontinu-
ities in the form of SLCFRS rules. The goal is
to find the target sequence e of the highest scor-
ing derivationD according to the model defined in
Section 2.2 that yields 〈f, e〉, where f is the given
input sentence.

We parse the input sentence with a bottom-up
CYK parser using the source side of the SLCFRS
translation grammar. This corresponds to mono-
lingual probabilistic LCFRS parsing, which has
been described for example in Kallmeyer and
Maier (2013). Using the rules, parse items are
built. They are of the form [A,ρ], where A is a
non-terminal label and ρ is a range vector indicat-
ing which part of the input is covered by this item.
For the label, we use a combination of the source
side label and the target side label in order to en-
sure valid target side derivations. Smaller items,

232



i.e. items that cover less input words, are created
before larger items. Equal items are combined,
thereby retaining their origin via hyperedges.

When creating a new item using a specific rule,
the variables and arguments in the rule have to be
replaced consistently with ranges 〈l, r〉 of the in-
put sentence. Roughly, this means that terminals
and variables are instantiated with ranges such
that for ranges that are adjacent in an argument
of the LHS non-terminal, the concatenation of the
two ranges has to be defined, i.e. r1 = l2 for
〈l1, r1〉 and 〈l2, r2〉. For example, given the input
0il1ne2mange3plus4, X(〈1, 4〉)→ X(〈2, 3〉) is an
instantiation of the source side of rule #5 from Fig-
ure 4. We can make further assumptions about rule
instantiations, as our rules are all monotone, ε-free
and we do not allow for empty gaps to avoid spu-
rious ambiguity.

In the implementation, we first replace all ter-
minals with all possible ranges with respect to the
input sentence in an initialisation step; for instance
X(〈1, 2〉Y1 〈3, 4〉) → X(Y1) for the previous ex-
ample. During the actual parsing, we are then only
concerned with how variables are instantiated. We
implement different pruning methods, such as lim-
iting the number of target side rules for the same
source side rule, and limiting the number of in-
coming hyperedges for one parse item.

Because of the specific form of the grammar
that we have extracted (rank 2, fan-out 42|2), we
implement a specific parser for (2, 2)-LCFRS. Ac-
cordingly, the range vector ρ of an item has the
form 〈〈i1, j1〉, 〈i2, j2〉〉, where i2 and j2 are un-
defined if the yield of the item is continuous.
Such range vectors can be stored and retrieved
more efficiently than general range vectors, i.e. for
full LCFRS (which are typically implemented as
bit vectors of the size of the input sentence).
Also parsing time complexity is directly depen-
dent on the fan-out vs of the monolingual gram-
mar: O(|Gs| · |f |vs·(u+1)) with rank u = 2 and
fan-out vs = 2 in our case.

Finally, the parse hypergraph that we obtain
from parsing with the source side of the gram-
mar is intersected with an n-gram language model
to also integrate PLM (e). We use cube prun-
ing for this step (Chiang, 2007; Huang and Chi-
ang, 2007). The difference to SCFG-based im-
plementations is that the target string of a hy-
pothesis that is scored by the language model is
not necessarily continuous, but consists of a tu-

ple of continuous blocks of target words, e.g.
〈do not want, anymore〉 if we would like to score
a hypothesis which has been built from rule #3
in Figure 4. Therefore, each continuous block is
scored separately and contributes its score to the
overall score of the hypothesis. Furthermore, we
need to store one language model state (simply put
remembering the first and last n − 1 words of the
block) for each block. This means that a language
model state in our implementation is a vector of
conventional language model states of the length
of the size of the target tuple of the hypothesis.
Note that since our grammar has a target fan-out
of 2, this vector has a maximal length of 2, but this
is not a fixed limit in the implementation.

Since obtaining the k-best translations for a
given input sentence is essential for tuning, we im-
plement k-best extraction on the hypergraph that
we obtain after cube pruning. We adopt the lazy
strategy from Huang and Chiang (2005).

The decoder is implemented in C++, including
code from KenLM3 for language modelling.

5 Experiments

5.1 Setup
We run experiments for German-to-English, based
on data that has been used in the WMT 2014 trans-
lation task4. For training of the translation mod-
els, we use the parallel sentences from Europarl
and the News Commentary Corpus up to a length
of 30 words (1.3M sentence pairs). For language
modeling, we use the KenLM Language Model
Toolkit5. We train a 3-gram language model
on all available monolingual English data (Eu-
roparl, News Commentary, News Crawl, 92.7M
sentences). From the available development data,
we use newstest2013 as the development test
set (max. 25 words). From the rest, we ran-
domly select 3000 sentence pairs of a maximal
length of 25 words as development set. We fur-
ther refine this set to sentences without out-of-
vocabulary source words by decoding the develop-
ment set once and selecting the corresponding sen-
tences. We thus end up with 1694 sentence pairs
for tuning. As our test set, we use the cleaned test
set that has been made available (2280 sentence
pairs with a maximal length of 30 words).

3http://kheafield.com/code/kenlm/
developers/

4http://www.statmt.org/wmt14/
translation-task.html

5http://kheafield.com/code/kenlm/

233



We normalize the punctuation, tokenize and
truecase all our data using the scripts that are avail-
able in Moses6 (Koehn et al., 2007). Furthermore,
we perform compound splitting for German, also
with the script provided in Moses.

The training data is word-aligned by run-
ning multi-threaded GIZA++ in both directions
and then symmetrizing the alignments using the
grow-diag-final-and heuristics as imple-
mented in the Moses training script (step 1–4).
Lexical translation probabilities are also emitted
as part of this pipeline. For grammar extraction,
we limit the length of initial phrases and the num-
ber of words in a gap to 10. We neither allow un-
aligned words at edges of initial phrases nor un-
aligned blocks.

Before decoding a data set with our decoder,
we filter the large translation grammar with re-
spect to the input data by extracting per-sentence-
grammars. These only contain rules whose termi-
nals match the words in the sentence to translate.

For the reported results, we set the buffer size
for cube pruning to 400. We do not limit the num-
ber of words a non-terminal can span. We neither
restrict the number of incoming hyperedges for the
parse items nor the number of target side rules for
the same source side rule.

Tuning the feature weights is done with mini-
mum error rate training (Och, 2003), maximizing
BLEU-4 (Papineni et al., 2002) and using the 200
best translations. For our own decoder, we use
the very flexible implementation Z-MERT v1.50
(Zaidan, 2009). For Moses, we use the provided
tuning script mert-moses.pl.

All reported BLEU scores have been calculated
with the Moses script multi-bleu.perl, us-
ing the lowercase option -lc. Because of the
variance that is introduced by tuning, we repeated
each experiment four times and report the average
of the final BLEU scores as well as the standard
deviation.

5.2 Results

We compare different versions of our system
against each other. The baseline is a system which
uses only SCFG rules, i.e. a hierarchical phrase-
based system. We refer to it as SYS(1,1), as it uses
an SLCFRS of fan-out 21|1. SYS(1,2) is a system
which uses a grammar of fan-out 31|2, i.e. it builds
only continuous constituents on the source side,

6http://www.statmt.org/moses/

devtest test
system feat BLEU std BLEU std

SYS(1,1) - 24.13 0.10 23.23 0.11
SYS(1,2) - 23.39 0.32 23.24 0.09
SYS(2,1) - 24.17 0.09 23.41 0.06
SYS(2,2) - 23.90 0.13 22.90 0.03

SYS(2,2) S 24.06 0.23 23.17 0.19
SYS(2,2) T 24.20 0.15 23.35 0.04
SYS(2,2) S+T 24.18 0.20 23.32 0.13

MOSES 24.33 0.08 23.34 0.20

Table 1: Averaged BLEU scores over four tuning
runs; the feat column indicates whether additional
source/target gap degree features have been used

but allows for discontinuous constituents with two
blocks on the target side. SYS(2,1) is the analo-
gous system which restricts the target side to con-
tinuous constituents. Finally, SYS(2,2) uses an
SLCFRS of fan-out 42|2.

Table 1 displays the main results. Allowing
gaps on the source and the target side (SYS(2,2))
leads to a decline in BLEU score compared to
the baseline. We hypothesize that this is due to
weak probability estimates because of data sparse-
ness and the additional ambiguity that is caused
by the new rules with discontinuities. However,
when adding the features about the gap degree of
the rules used in the derivation, the model has an
additional way of influencing which kind of rules
are used. Especially controlling for the target gap
degree turns out to be important and leads to a
small improvement in BLEU score. Note, how-
ever, that rules with target gaps are not totally dis-
missed when this feature is switched on. Usage
of rules with a target gap goes down from on av-
erage 734.5 rules in SYS(2,2) to on average 76.5
rules in SYS(2,2)-T in the test set. They are used
less often, but, it seems, in a more controlled and
sensible way.

This tendency is further confirmed with the ex-
periments in which the discontinuous rules are
only used on one side. While restricting the source
side derivations to continuous yields does not im-
prove the BLEU score (it rather severely degrades
it in the case of the devtest set), restricting the tar-
get side derivations leads to a small improvement
in BLEU score, and even to the best system for the
test set. This is in particular interesting with re-
spect to translation times since restricting the tar-
get side to continuous yields means removing the
additional complexity that target gaps mean for the

234



SYS(1,1) SYS(2,1) =

e1 43 49 3
e2 46 47 2

Table 2: Result of the manual system comparison

e2
SYS(1,1) SYS(2,1) =

e1
SYS(1,1) 29 13 1
SYS(2,1) 15 33 1

= 2 1 0

Table 3: Confusion matrix of the decisions of the
manual evaluation

language model integration (see Section 4).
We also report results for the hierarchical

phrase-based system in Moses trained on the same
data as our systems. We tried to use the same
settings as for our comparable system SYS(1,1).
However, given the number of parameters dur-
ing training and decoding, the various interpre-
tations thereof and numerous implementation de-
tails to consider, it is not too surprising that the
Moses system actually produces different trans-
lations than ours. The reported numbers merely
serve as a point of reference, indicating that the
translations produced by our system are not totally
far off.

5.3 Manual Evaluation
We furthermore performed a manual evaluation in
form of a system comparison using our own instal-
lation of the Appraise tool (Federmann, 2012). We
compare the baseline SYS(1,1) against SYS(2,1),
the best-performing setup on the test set. For each
of them, we randomly selected one of the four
configurations that lead to the reported averaged
BLEU score. We then selected those translations
of the test set where SYS(2,1) uses at least one
SLCFRS rule with a discontinuity (95 sentences).

We asked two native speakers of English (e1,
e2) with basic knowledge of German to evaluate
our test sentences. They were shown the source
sentence, a reference translation, the SYS(1,1)
translation and the SYS(2,1) translation. The latter
two were presented anonymized and in random or-
der. The options for the evaluators were (a) trans-
lation A is better than B, (b) translation B is better
than A, and (c) translations A and B are of equal
quality. We specifically asked them to use option
(c) as rarely as possible.

Table 2 shows the results. While our human

evaluators do not demonstrate a clear preference
for one of the systems, there is, however, a slight
preference for the system that uses discontinuous
rules (SYS(2,1)). In spite of the inter-annotator
agreement being not very high (Cohen’s κ =
0.338), the tendency for SYS(2,1) is also perceiv-
able for the translations for which the evaluators
agree in their decisions, see Table 3.

5.4 Translation Example
We finish this section with an actual translation ex-
ample. It is picked because it makes crucial use of
the discontinuous SLCFRS rules. It is taken from
the test set.

In Figure 5, the following rule, which has a fan-
out of 2 on the source side, leads to an overall
grammatical sentence structure and a meaningful
translation:
〈X(wäre , Y1 gewesen Y2)→ X 1 (Y1)X 2 (Y2) ,
X(would have been Y1Y2)→ X 1 (Y1)X 2 (Y2) 〉

The rule derives the synchronous constituent la-
belledX 4 in Figure 5. Besides providing a correct
verbal translation in a specific tense, it also estab-
lishes a relationship to the adjective (X 1 ) and the
infinitive subordinate clause (X 2 ), thereby still
leaving room for the adverb in terms of the gap
on the source side. The adverb is then introduced
with the following rule, leading to the constituent
labelled X 5 in Figure 5:

〈X(Y1 damit auch Y2)→ X 1 (Y1, Y2) ,
X(also Y1)→ X 1 (Y1) 〉

This rule can be seen as capturing the different
placement of the adverb auch/also in German and
English.

Note that the alignment that is induced by the
SYS(2,1) derivation is also derivable with a 21|1-
SLCFRS. One general possibility is to allow rules
of rank u > 2. Another possibility is to put the in-
dividual phrases together in a different order and
hierarchy. For example, in an SCFG rule, the dis-
continuous verb phrase could be combined with
the adjective and the adverb first, which leads to
a continuous constituent. Then the subordinate
clause would be added in a later derivation step.
However, in the derivation for the best transla-
tion of SYS(1,1), this does not happen because a
corresponding specific rule has not been learned.
The translation produced by SYS(1,1) is not gram-
matical and misses important concepts, such as
geeignet (suitable).

235



X 6

X 5

X 4

X 2

X 3 X 1

er wäre damit auch geeignet gewesen , um . . . zu fördern

he also would have been appropriate to promote . . .

X 3 X 1

X 2

X 4

X 5

X 6

Source: er wäre damit auch geeignet
gewesen , um die . . . zu fördern

Reference: it would thus be suitable
to assist . . .

SYS(1,1): it would also have to be ,
in order to promote the . . .

SYS(2,1): he also would have been
appropriate to promote the . . .

Figure 5: Test sentence with translations provided by the SCFG and the SLCFRS system, including the
derivation of the SLCFRS system SYS(2,1).

6 Related Work

Several other translation models have been pro-
posed which are expressive enough to generate
the complex alignment configurations in Figure 1.
Most notably, Galley and Manning (2010) propose
a phrase-based translation system which allows for
discontinuous phrase pairs, building upon the idea
of a translation model proposed by Simard et al.
(2005). They evaluate their system on a Chinese-
to-English translation task and achieve some im-
provement in BLEU score over a phrase-based
and a hierarchical phrase-based system. Unfortu-
nately, we could not evaluate directly against their
approach since the current documentation7 of their
system, Phrasal (Green et al., 2014), does not men-
tion the discontinuous phrases anymore. We also
could not obtain the data sets they used for their
experiments.

In some sense, our work is the hierarchical, tree-
based counterpart to the phrase-based approach
of Galley and Manning (2010). This means that
our translation grammar rules unify two types of
“gaps” of previous approaches: (a) gaps in the
sense of non-terminals that are inserted into longer
phrases when hierarchical rules are created, as in
Chiang (2007); their purpose is a better general-
ization of the translation rules, and (b) gaps in the

7http://www-nlp.stanford.edu/wiki/
Software/Phrasal, accessed on June 27, 2015

sense of discontinuities in the yield of a translation
rule, on the source side, on the target side or both,
driven by the idea of allowing for more flexible
phrases such that generated alignment structures
are not restricted.

Besides the suggestion of Kaeshammer (2013)
to use SLCFRS as the translation grammar formal-
ism, which we have detailed and implemented in
this work, Søgaard (2008) proposes to apply range
concatenation grammar, an even more expressive
formalism than LCFRS, and to use its ability to
copy substrings during the derivation. This ap-
proach has downsides, such as no tight probabil-
ities estimators, which are mentioned in Søgaard
and Kuhn (2009).

An early advocate of translation model-
ing beyond context-free grammar formalisms is
Melamed, who proposes to use Generalized Mul-
titext Grammars, which are weakly equivalent to
LCFRS (Melamed, 2004; Melamed et al., 2004).
The incentive for this lies in linguistically moti-
vated translation grammars and the general obser-
vation that discontinuous constituents are neces-
sary for monolingual modelling of syntax.

7 Conclusions and Future Work

With this work, we extend the hierarchical phrase-
based machine translation approach to discontin-
uous phrases, using SLCFRS as the translation
grammar formalism. Since SLCFRS is a direct

236



extension to SCFG, previous work on hierarchical
phrase-based translation, in particular the model
definition, training and decoding, can be extended
to SLCFRS in a more or less direct manner. Eval-
uating our new system on a German-to-English
translation task revealed a modest improvement
in BLEU score over the SCFG baseline. Human
evaluators showed a slight preference for transla-
tions produced by the SLCFRS system.

In the future, we will evaluate our approach on
other language pairs, for example Chinese-English
which has been used in related work. Furthermore,
we would like to make use of recent advances in
monolingual parsing of discontinuous constituents
and use phrase-structure trees supporting discon-
tinuous constituents for tree-based machine trans-
lation.

Acknowledgments

I would like to thank Laura Kallmeyer and Wolf-
gang Maier for discussions and comments and
the reviewers for their suggestions. This research
was funded by the German Research Foundation
as part of the project Grammar Formalisms be-
yond Context-Free Grammars and their use for
Machine Learning Tasks. Computational support
and infrastructure was provided by the Centre for
Information and Media Technology (ZIM) at the
University of Düsseldorf (Germany).

References
Eberhard Bertsch and Mark-Jan Nederhof. 2001. On

the complexity of some extensions of rcg parsing. In
IWPT.

Pierre Boullier. 1998. Proposal for a Natural Language
Processing syntactic backbone. Technical Report
3342, INRIA.

David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263–270.

David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.

Christian Federmann. 2012. Appraise: An open-
source toolkit for manual evaluation of machine
translation output. The Prague Bulletin of Mathe-
matical Linguistics, 98:25–35, September.

Daniel Fernández-González and André Martins.
2015. Parsing as reduction. arXiv preprint
arXiv:1503.00030.

Michel Galley and Christopher D. Manning. 2010.
Accurate non-hierarchical phrase-based translation.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 966–974.

Spence Green, Daniel Cer, and Christopher D. Man-
ning. 2014. Phrasal: A toolkit for new directions
in statistical machine translation. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, pages 114–121. Association for Computational
Linguistics.

Hieu Hoang and Philipp Koehn. 2010. Improved trans-
lation with source syntax labels. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 409–417. As-
sociation for Computational Linguistics.

Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technology, pages 53–64. As-
sociation for Computational Linguistics.

Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Annual Meeting-Association For Computa-
tional Linguistics, volume 45, page 144.

Miriam Kaeshammer and Anika Westburg. 2014. On
complex word alignment configurations. In Pro-
ceedings of the Ninth International Conference on
Language Resources and Evaluation (LREC’14),
pages 1773–1780, Reykjavik, Iceland, May.

Miriam Kaeshammer. 2013. Synchronous linear
context-free rewriting systems for machine transla-
tion. In Proceedings of the Seventh Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation, pages 68–77. Association for Computational
Linguistics.

Laura Kallmeyer and Wolfgang Maier. 2013. Data-
driven parsing using Probabilistic Linear Context-
Free Rewriting Systems. Computational Linguis-
tics, 39(1).

Laura Kallmeyer. 2010. Parsing beyond context-free
grammars. Springer Science & Business Media.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, pages
48–54. Association for Computational Linguistics.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the ACL on
interactive poster and demonstration sessions, pages
177–180. Association for Computational Linguis-
tics.

237



Wolfgang Maier and Timm Lichte. 2011. Character-
izing discontinuity in constituent treebanks. In For-
mal Grammer 2009, Revised Selected Papers, vol-
ume 5591 of LNAI. Springer.

Wolfgang Maier. 2010. Direct parsing of discontin-
uous constituents in German. In Proceedings of
the NAACL HLT 2010 First Workshop on Statistical
Parsing of Morphologically-Rich Languages.

Wolfgang Maier. 2015. Discontinuous incremen-
tal shift-reduce parsing. In Proceedings of ACL-
IJCNLP 2015, Beijing, China.

I. Dan Melamed, Giorgio Satta, and Benjamin Welling-
ton. 2004. Generalized multitext grammars. In Pro-
ceedings of the 42nd Annual Conference of the As-
sociation for Computational Linguistics (ACL).

I. Dan Melamed. 2004. Statistical machine translation
by parsing. In Proceedings of the 42nd Annual Con-
ference of the Association for Computational Lin-
guistics (ACL).

Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational linguistics, 30(4):417–449.

Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160–167. As-
sociation for Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Giorgio Satta and Enoch Peserico. 2005. Some
computational complexity results for synchronous
context-free grammars. In Proceedings of Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Pro-
cessing (HLT/EMNLP), pages 803–810.

Michel Simard, Nicola Cancedda, Bruno Cavestro,
Marc Dymetman, Eric Gaussier, Cyril Goutte, Kenji
Yamada, Philippe Langlais, and Arne Mauser. 2005.
Translating with non-contiguous phrases. In Pro-
ceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Nat-
ural Language Processing (HLT/EMNLP), pages
755–762.

Anders Søgaard and Jonas Kuhn. 2009. Empirical
lower bounds on alignment error rates in syntax-
based machine translation. In Proceedings of the
Third Workshop on Syntax and Structure in Statis-
tical Translation (SSST ’09). Association for Com-
putational Linguistics.

Anders Søgaard and Dekai Wu. 2009. Empirical lower
bounds on translation unit error rate for the full class
of inversion transduction grammars. In Proceed-
ings of the 11th International Conference on Parsing
Technologies, pages 33–36.

Anders Søgaard. 2008. Range concatenation gram-
mars for translation. In Proceedings of Coling 2008:
Companion volume: Posters.

Anders Søgaard. 2010. Can inversion transduction
grammars generate hand alignments? In Procced-
ings of the 14th Annual Conference of the European
Association for Machine Translation (EAMT).

Andreas van Cranenburgh and Rens Bod. 2013. Dis-
continuous parsing with an efficient and accurate
dop model. In Proceedings of the International
Conference on Parsing Technologies (IWPT 2013).

Yannick Versley. 2014. Experiments with easy-first
nonprojective constituent parsing. In Proceedings
of the First Joint Workshop on Statistical Parsing
of Morphologically Rich Languages and Syntactic
Analysis of Non-Canonical Languages, pages 39–
53.

K. Vijay-Shanker, David Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions used by
various formalisms. In Proceedings of the 25th An-
nual Meeting of the Association for Computational
Linguistics.

David Weir. 1988. Characterizing Mildly Context-
Sensitive Grammar Formalisms. Ph.D. thesis, Uni-
versity of Pennsylviania, Philadelphia, PA.

Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the
complexity of translational equivalence. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 977–984.

Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377–403.

Omar Zaidan. 2009. Z-mert: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79–88.

Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the Workshop on Statistical
Machine Translation, HLT/NAACL.

238


