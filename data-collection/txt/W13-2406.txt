










































Parsing Russian: a hybrid approach


Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 34–42,
Sofia, Bulgaria, 8-9 August 2013. c©2010 Association for Computational Linguistics

Parsing Russian: a Hybrid Approach

Dan Skatov, Sergey Liverko,

Vladimir Okatiev, Dmitry Strebkov

Russian Federation, Nizhny Novgorod

Dictum Ltd

{ds,liverko,oka,strebkov}@dictum.ru

Abstract

We present an approach for natural lan-

guage parsing in which dependency and

constituency parses are acquired simul-

taneously. This leads to accurate parses

represented in a specific way, richer than

constituency or dependency tree. It also

allows reducing parsing time complexity.

Within the proposed approach, we show

how to treat some significant phenomena

of the Russian language and also perform

a brief evaluation of the parser implemen-

tation, known as DictaScope Syntax.

1 Introduction

A syntactic parser inputs a sentence and pro-

duces information on syntactic relationships be-

tween parts of the sentence. It is an open ques-

tion which method is the most convenient one to

represent these relationships. In this paper, we are

focusing on two of those methods. The first one,

a constituency tree (CT), is a representation of a

sentence by a set of nested fragments — groups,

each group corresponding to a syntactically co-

herent phrase. The second one, a dependency tree

(DT), expresses relationships by a set of syntactic

links between pairs of tokens.

Figure 1 demonstrates correspondence between

CT and DT: one is clearly derivable from another.

In applications, one usually needs to transform CT

into DT due to the following fact: if a tree is

correct, then subjects, objects and adverbials of

some predicate X are always direct children of

the node X in DT. With a traditional CT frame-

work these children can be obtained in much less

intuitive way by browsing up and down through

constituents, as shown in Figure 1 by dotted lines.

According to this comparison, DT transparently

maps onto the level of semantic representation,

����� ��� �	
�� �� 	��	

Adj N V Pr N

NP

VP

NP

PP

AP

NP

VP

S

Ctrl.

Cont.

Agr.

Agr.

Figure 1: A constituency tree (upper) and a depen-

dency tree (lower) for a sentence “A blue ball lies

on the sand”.

thereby DT-s are considered most appropriate in

applications (Jurafsky and Martin, 2009) like sen-

timent analysis and fact extraction.

Constituency parsing. Despite the usefulness

of DT-s, CT-s have a longer history of application

as a computational model. For now, probabilis-

tic constituency parser by (Charniak, 1997) and

its derivatives are considered the state of the art

for English. Unfortunately, the framework of con-

stituency parsing, taken alone, is not productive

for languages such as Russian. It turns out that the

number of rules in a grammar start to grow fast if

one tries to describe an inflecting language with a

free word order explicitly. As a result, pure con-

stituency parsers are not well known for Russian.

It has recently been confirmed by a Russian syn-

tactic parsers task at the Dialogue conference (see

http://www.dialog-21.ru), at which sev-

eral parsers were presented and all of them used

DT formalism as a basis.

34



Dependency parsing. Modern algorithmic ap-

proaches to dependency parsing are based on

machine learning techniques and are supported

by open-source implementations. Unfortunately,

large DT-s corpora are not widely available for

Russian to train these parsers. The need for the

corpora also brings complications when one wants

to achieve high precision parsing a given subject

domain, and then to switch to parse another do-

main: eventually one will need a separate corpus

for each domain. There is a consent among re-

searches that a “long tail” of special error cases

is definitely hard to fix in pure machine learning

frameworks (Sharov and Nivre, 2011), while it is

necessary for high precision. In contrast to En-

glish, dependency parsing is traditional for Rus-

sian computational linguistics. As a result, mod-

ern Russian parsers produce DT-s. These parsers

are mainly rule-based with an optional statistical

component (Toldova et al., 2012) and standard

expert-verified data sets such as verb subcatego-

rization frames, which are called “control models”

or “set of valences” in Russian. None of the rule-

based parsers that were presented at the Dialogue

task are freely available.

Unfortunately, the practice of using DT-s has

revealed some of their significant deficiencies.

The most frequently discussed one is the repre-

sentation of homogenous parts of the sentence

(Testelets, 2001). Figure 2 shows some known

methods. One can observe that there must be

a syntactic agreement between the group of ho-

mogenous parts 1-3 and their parent 4-61 by

Number, which is Plural, but it is impossible to

capture this relation in a DT where only words can

hold grammar values. No representation among

A-E in Figure 2 keeps this information for the

group. Things get worse if one tries to represent

an agreement for two groups of homogenous parts,

like in 2.F. In addition, it is common to modify the

parsing algorithm, but not the set of decision rules,

directly in order to get nonprojective2 DT-s (Mc-

1In examples, we put indices for words in correspon-
dence with English translation (often with omitted articles
“a”, “the”), refer to any word by its index, and to a phrase
by indices of its starting and finishing word.

2Dependency tree is called projective if each subtree
corresponds to a continuous fragment of the source sen-
tence. There is evidence that more than 80% of the sen-
tences are usually projective in European natural languages.
A famous example of nonprojectivity for Russian is �ß1
ïàìÿòíèê2 ñåáå3 âîçäâèã4 íåðóêîòâîðíûé5� “I1’ve
raised4 a monument2 for myself3 not made by hands5” from
Pushkin, where link 4→1 overlaps 2→5.

�����

�����������

���� �����

�����������

�����

�����

�����������

�����

������������

S

BA

C

�����������

����

�

�����

�

�����������

� !"#$%& '(

�!)�

*$+",

� !"#$%& '(

�!)�

-

*$+", ED

F

����������� �����������

V.Pl.

�����

N.Pl.

���� G

Figure 2: Uncertainty with the representation of

homogenous parts in dependency trees for �äâåðü1
è2 îêíî3 îòêðûâàþòñÿ4 è5 çàêðûâàþòñÿ6�

“door1 and2 window3 open4 and5 close6”

Donald et al., 2005). The list of open problems can

be extended further: paired conjunctions, nested

sentences, introductory phrases etc.

Toward the hybrid approach. It would be pos-

sible to resolve problems with homogenous parts

if additional vertices could be added to the DT, like

in Figure 2.G, representing supplementary con-

stituents with synthesized grammar values. Unfor-

tunately, known approaches for dependency pars-

ing assume that all vertices are predefined before

the algorithm starts, so it is impossible to include

new vertices on the fly without inventing new pars-

ing methods.

The idea of combining dependencies and con-

stituencies directly is not a new one. For Rus-

sian, (Gladkij, 1985) suggested designating a stan-

dard relation between predicate and subject by

a syntactic link, along with adding a separate

constituent for compound predicative groups like

�äîëæåí óñòóïèòü� from �Äîðîãó1 äîëæåí2
óñòóïèòü3 âîäèòåëü4� “The driver4 must2
give3 way1 (to smb.). . . ”, which has a nonpro-

jective DT. This solution immediately reduces the

number of overlapping dependency links for com-

pound predicates, because links that tend to over-

35



lap are packed inside the constituent. In (Kuro-

hashi and Nagao, 1994) constituents for groups

of homogenous parts are prebuilt to be treated

as single units during the next step by a depen-

dency parser for Japanese. In (Okatiev et al., 2010)

preprocessing of certain tokens connected to con-

stituents is performed before the dependency pars-

ing. In (Wang and Zong, 2010) a third-party

black-box dependency parser is used to improve

the results of author’s constituency parser, achiev-

ing 10% boost in F1. Additionally, there is ev-

idence that ABBYY Compreno (Anisimovich et

al., 2012) tracks the order in sequences of consec-

utive tokens so it actually exploits some kind of

constituents throughout the process of dependency

parsing.

In this paper, we propose a method of repre-

senting a parse tree, which is a combination of

CT and DT and eliminates some disadvantages of

both. Then we describe syntactic rules that guide

the process of simultaneous bottom-up acquisition

of multiple syntactically ambiguous DT-s and CT-

s for a given sentence, ranked by syntactic rele-

vance. In addition, we discuss some properties

of the rule base that is built in the framework of

proposed rules description language. After that,

we show that it is possible to extend the clas-

sical Cocke-Younger-Kasami algorithm (Kasami,

1965) to use grammar rules of arbitrary arity with-

out grammar binarization and to exploit interme-

diate DT-s to increase efficiency. Moreover, we

demonstrate how to achieve a reasonable ranking

of solutions without using any additional statisti-

cal information. In conclusion, we discuss possi-

ble extensions of the approach.

2 Representing the Parse Tree

Our goal is to achieve the most complete represen-

tation of syntactic relations between words and/or

chunks of a given sentence. The subject of seman-

tic representation, e.g. semantic labeling of predi-

cate participants or questions on the edges, are not

discussed further. We assume that such informa-

tion can be surfaced either during the process of

analysis or directly afterwards by semantic inter-

pretation of resulting parse trees.

As it was mentioned, it is possible to avoid dis-

advantages of DT-s if one finds a way to bring ad-

ditional vertices to these trees. Though it is not

natural for DT-s to have such vertices, it is com-

mon for constituents in CT-s to acquire derived

grammar values. Thereafter, instead of building

a parse tree of a completely new type, we obtain

the representation that consists of three compo-

nents: 1) constituency tree — CT, 2) dependency

tree — DT, 3) hybrid tree — HT. CT strictly corre-

sponds to a system of syntactic rules given prelim-

inarily, and DT is built by instructions from rules

at each step of the bottom-up analysis driven by

this system. A hybrid representation, HT, that is

based on DT but depicts homogenous parts, de-

pendent clauses and nested sentences in a partic-

ular way. As a next step, we declare a balanced

system of agreements between these different rep-

resentations.

Assumptions for a constituency tree. We only

require that CT correctly corresponds to the given

sentence, and phrases of CT do not need to corre-

spond to the expected expressions of VP, NP, AP

etc. E.g., one can declare a specific rule that gener-

ates a constituent that simultaneously corresponds

to a subject and a verb, e.g. �äîëæåí óñòóïèòü

âîäèòåëü� . Such a phrase corresponds both to

VP and NP and is atypical, but the corresponding

rule can produce a proper fragment of a nonprojec-

tive DT and HT, which is the main goal in this con-

text. To summarize, in the proposed model con-

stituencies play an utilitarian role, they are treated

like carriers for parts of DT and are borrowed at

certain decision points to form a resulting HT.

Assumptions for a dependency tree. During

the parsing, a hypothesis of syntactic locality is

considered, that states: tokens that are close lin-

early in a sentence are more likely to have a syn-

tactic relationship. If there are several methods

to represent a syntactic phenomenon, we choose a

method that fits it best. Recalling Figure 2, one can

observe that in 2.A and 2.B two links correspond

to different linear distances between correspond-

ing vertices, while in C–F each link corresponds

to a unity distance. Let us call the latter kind of

homogeneity representation “a chain scheme” and

require syntactic rules to follow it.

The following assumptions are made: 1)

prepositions become roots of the corresponding

prepositional phrases; 2) subordinate conjunc-

tions become roots of the corresponding depen-

dent clauses; 3) punctuation marks, quotes and co-

ordination conjunctions are removed from DT and

will be properly presented in HT.

The following link types are expected: agree-

ment (Adj+Noun, Noun+Noun, etc), control

36



(Verb+Noun, prepositional phrases etc), conti-

guity (for adverbs, particles etc.), isolation for de-

pendent clauses and coordination for chains of ho-

mogenous parts. For cases when a synthesized

grammar value is needed like in Figure 2.G, we

do not expect a special new vertex to be intro-

duced in DT. Instead, each vertex of DT contains

a reference to the corresponding constituency in

CT, which holds its own grammar value. By de-

fault, this value is inherited from the head element

of the constituent, but can be overridden, which is

the case for homogeneity.

Assumptions for a hybrid tree are revealed by

the example in Figure 3, where an XML-file is

represented for a HT of a sentence with two de-

pendent clauses 8-11 and 12-19, a nested sen-

tence 1-19 and homogenous nouns 17 and 19.

Only significant details are shown in the figure —

shortened grammar values (N for Noun etc) as GV

and wordforms as W. Full representation also in-

cludes normal forms, detailed grammar values, to-

kens’ positions and link types.

Subordinate clauses are presented under

Subord tag as a nested group, while nested

sentences are located under S tag. The group of

two homogenous nouns is placed under Coord,

but, unlike corresponding DT, homogenous

members do not form a chain and are located

on a single level inside a special Group tag.

Such Group can have any number of dependent

elements which are placed between </Group>

and </Coord> (shown in Figure 4 below).

There, the agreement by number between the

group of two nouns 2-4 and the adjective 1 is

taken into account, and the adverb 10 adjoins to

the group of verbs 6 and 9 but not to a single verb.

An intensive research has been performed by

(Okatiev et al., 2010) on the role of punctuation

for Russian. According to it, one has to distin-

guish roles of punctuation marks and conjunctions

(which together are called junctions) to deduce

correct parses. For this reason roles are marked

in a hybrid XML tree. One punctuation mark or

a conjunction can possess several roles: it can be

an isolator (Isol, bounds dependent phrases), a

separator (Sepr, is used to separate homogenous

parts inside coordinated groups) and a connec-

tor (Conn, indicates nested sentences, e.g. quo-

tation marks). Isolators and connectors always

play an opening or a closing role for one or sev-

eral clauses. In the sentence from Figure 3, the

<S T="«. /0 1234 … 2/">
<V W="25678/709" GV="V">
  <S T=". /0 1234 9:052;<9=…">
    <Conn W='«'/>

    <V W="1234" GV="V">
      <V W="." GV="Pron">
      <V W="/0" GV="Part">
      <V W="9:052;<9=" GV="V">
        <V W="50:0>?@;289@" GV="N">
          <V W="29">
            <V W="?AB0C">
              <Subord GrV="V">

                <Isol W=","/>

                <V W=":<529<A9 GV="V">
                  <V W="D292:E0" GV="Pron">
                  <V W="/<" GV="Pr">
                    <V W="10/7" GV="Pron">
                  </V>

                </V>

                <Isol W=","/>

              </Subord>

            </V>

          </V>

        </V>

      </V>

      <Subord GrV="V">

        <Isol W=","/>

        <Isol W="08?@"/>
        <V W="54B4" GV="V">
          <V W="F:2;2B@9=" GV="V">
            <V W=";:017" GV="N">
              <V W=";" GV="Prep">
                <Coord>

                  <Group GrV="N">

                    <V W=":28D2G@" GV="N">
                    <Sepr W="@"/>
                    <V W="D21H2:90" GV="N">
                  </Group>

                </Coord>

              </V>

            </V>

          </V>

        </V>

      </Subord>

    </V>

    <Conn W='»'/>

  </S>

  <Conn W="—"/>

  <V W="2/">
</V>

</S>

Figure 3: A HT for �¾ß1 íå2 ìîãó3 òðåáîâàòü4
áåðåæëèâîñòè5 îò6 ëþäåé7, êîòîðûå8

íà9 ìåíÿ10 ðàáîòàþò11, åñëè12 áóäó13

ïðîâîäèòü14 âðåìÿ15 â16 ðîñêîøè17 è18

êîì�îðòå19¿, � îáúÿñíÿåò20 îí21�

“«I1 can3 not2 require4 thrift5 of6 people7 who8
work11 for9 me10 if12 I spend14 time15 in16
luxury17 and18 comfort19», — he21 explains20”.

comma between 11 and 12 is a closing isolator

for clause 8-11 and also and opening isolator for

12-18. Therefore this comma is mentioned twice

in shown XML file in the beginning and the ending

of the corresponding <Subord>-s. In general,

HT contains at least as many vertices as there are

tokens in a sentence, and possible duplicates cor-

respond to punctuation marks and conjunctions.

Another case of multiple roles for the punc-

tuation is shown by the example �Êðóã1,

çàêðàøåííûé2 ñèíèì3, ðàâíîáåäðåííûé4

37



òðåóãîëüíèê5 è6 æ¼ëòûé7 êâàäðàò8�

“Circle1, shaded2 in2 blue3, isosceles4 triangle5
and6 yellow7 square8” where the comma between

3 and 4 is an ending isolator for 2-3 and also a

separator for parts 1 and 5 of a group of homoge-

nous nouns. In addition, the case of an isolating

group of a comma and a following conjunction,

like �, åñëè� “, if”, is always being discussed: is

it a single token, should one remove a comma or

leave it, etc. In this case, this group is just a pair

of consecutive isolators in HT XML, see Figures

3 and 4.

<S>

<Coord>

   <Group GV="Verb">

      <Sepr W="IJI"/>
      <V W="KLIMNOJPLQR" GV="Verb"/>

<Sepr W=","/>

<Sepr W="LJI S"/>
<V W="TJIMNOJPLQR" GV="Verb"/>

</Group>

<Coord>

<Group GV="Noun">

<V W="UOVMW" GV="Noun"/>
<Sepr W="S"/>
<V W="KIXK" GV="Noun"/>

</Group>

<V W="YKZW[SV" GV="Adj"/>
</Coord>

   <V W="LS\K" GV="Adv"/>
</Coord>

</S>

Figure 4: A HT for �Áîëüøèå1 äâåðü2 è3 îêíî4
êàê5 îòêðûâàþòñÿ6, òàê7 è8 çàêðûâàþòñÿ9

òèõî10� “The large1 door2 and3 window4 both5
open6 and7,8 close9 silently10”.

The way in which paired conjunctions are

treated in HT is synchronized with the represen-

tation of single coordinative conjunctions. E.g.,

�êàê . . . , òàê è . . . � “both . . . and . . . ” is fol-

lowed by a rule �S → êàê S, òàê è S� , where
�S�-s denote clauses, yielding the parse in Figure

4, which is difficult to obtain in a pure DT.

3 The Rule Base

Linguistic data. We use a set of subcategoriza-

tion frames for about 25 000 verbs, deverbatives
and other predicative words, which is collected

at our side through standard techniques of collo-

cations and lexics aquisition from (Manning and

Schütze, 1999). A morphological dictionary con-

sists of about 5 mln wordforms.

Morphological hierarchy. In many cases,

it is useful to unite different parts of speech

into one metapart which possesses some com-

mon properties. E.g., participles share proper-

ties of verbs and adjectives. For verbs, the class

ComVerb is introduced, which includes Verb

in both finite and infinite forms, Participle

and Transgressive, for adjectives — the

class ComAdj with FullAdj, ShortAdj and

Participle. This concept leads to the reduc-

tion in the number of syntactic rules.

The language of syntactic rules is shown by

an example that describes a coordination relation

between an adjective and a noun:

// ]^_`abc _defab “high back”
AgreeNounAdj {

  T: [ComAdj] [ComNoun];

  C: NumberGenderCaseAgree (PH1, PH2);

  Main: 2; L: 2=>Agreement=>1;

}

Section T declares that a template should check

grammar values of consecutive phrases PH1 and

PH2, while section C checks required properties

of them. If phrases fit T and C, then a DT is built

according to L section by a link from the main

word of the second constituent to the main word

of the first, plus trees of PH1 and PH2. The sec-

ond phrase is declared the head one (Main: 2)

for the new phrase built by AgreeNounAdj rule.

Coordination. It is possible to override gram-

mar value of a new phrase PH, which is shown by

an example of homogenous nouns:

// ghijkj, lmnop “apple, pear”
CoordNounComma {

  T: [ComNoun] <,> [ComNoun];

  C: (PH1.Case == PH2.Case) && !PH1.IsCoord

&& PH2.NormalForm != "kjqjmrs";
  Main: 1; L: 1=>Coord=>2; J: 1<=Sepr;

  A: PH.Number = NUMBER_PL;

     PH.IsCoord = true;

}

Number of the entire phrase is set to Plural.

In addition, the role of the comma is set to Sepr.

IsCoord property is introduced to phrases to

prune the propagation of different bracketings.

E.g., for a chain of nouns �A è B, C è D� a

large number of bracketings are possible: �[[A è

B℄, [C è D℄℄� , �[A è [B, [C è D℄℄℄� etc. To

prune this to exactly one correct bracketing, we

deny chaining phrases that both contain chains of

nonunity length and allow only left-to-right chains

by a check !PH1.IsCoord.

Ambiguous compounds. Some compounds in

Russian have their own grammar values in cer-

tain contexts. E.g., �ïî ïðè÷èíå� “by reason

of” is sometimes a preposition equivalent to �èç-

çà� “because of” (as preposition): �[ñóäèëè [ïî

[ïðè÷èíå è ñëåäñòâèþ℄℄℄� “judged on reason

and consequence” vs. �[îïîçäàë [ïî ïðè÷èíå℄

38



ïðîáîê℄� “was late by reason of traffic jams”. In

contrast to context rules for such constructions, we

use pure syntactic rules for testing both versions,

introducing a compound version by the rule:

CompoundPrep {

  T: [Any] [Any];

  C: IsCompoundPrep (PH1, PH2);  

  Main: 1; L: 1=>Compound=>2;

  A: PH.Type = PHRASE_PREP;

}

Nonprojective parses for compound predicates

are processed by the following rule which pro-

duces a nonprojective DT. Despite nonprojectivity,

it brings no problem for CT parsing process:

// tuvuwx yuz{|} x~x… 
ControlNonProjectLeft {

  T: [Any] [Any] [Any];

  C: PredicModel (PH2, PH3) &&

     IsFreeValence (PH2, PH3) &&

     PredicModel (PH3, PH1) &&

     IsFreeValence (PH3, PH1);

  Main: 2;

  L: 2=>Control=>3; 3=>Control=>1;

  A: FillValence (PH, PH3);

}

Punctuation. Roles of junctions guide the pars-

ing process. E.g., we consider that a junction

that has exactly one role, which is ending isola-

tor, is equivalent to a whitespace. Let us see how

the rule AgreeNounAdj will parse the exam-

ple under this assumption: �ñèíèé1, êàê2 ìîðå3,

îòòåíîê4� “shade4, as blue1 as2 a sea3”. One

can verify that, due to consideration, the second

comma no longer prevents this phrase from be-

ing covered by AgreeNounAdj. Another way

to track these roles is to reject false control in

genitive, e.g. �ìíîãî1 [êðóãîâ2, çàïîëíåííûõ3
áåëûì4, êâàäðàòîâ5℄, ýëëèïñîâ6� “lots1 of1
circles2, shaded3 in3 white4, squares5, ellipses6”.

Overall base. For Russian, we have built a rule

base with 90 rules, divided into 7 groups, one for

each type of syntactic relations to be included into

DT. These rules exploit 20 predicates in criterion

sections and 10 additional phrase properties.

4 The Algorithm

We provide a modification of the Cocke-Yanger-

Kasami algorithm (CYK) to find DT-s by corre-

sponding CT-s that can be derived by rules de-

scribed above and that are best in a specified way.

In our interpretation, CYK has the following

structure. It inputs a sentence of n tokens. Empty

n × n matrix M is set up and its main diagonal
is filled with one-token phrases, each phrase at a

cell M [i][i] takes some grammar value from the i-
th token of the sentence, i = 1, . . . , n as its head.
CYK iterates all diagonals from the main diagonal

of M to its upper-right cell M [1][n]. Each cell on
the diagonal of length k ∈ {n−1, . . . , 1}will con-
tain only phrases of exactly k consecutive tokens,

so M [1][n] will contain exactly those phrases that
correspond to consecutive tokens, i.e. the entire

sentence.

For CYK, it is traditionally assumed that each

rule has exactly two nonterminals, and grammars

formed by such rules are called binarized gram-

mars (also known as “in Chomsky normal form”).

Now consider the binarized rule R : Ph1Ph2 →
Ph. If one wants to derive a phrase Ph of con-

secutive tokens by this rule, then one should look

at consecutive phrases Ph1 and Ph2 with prop-

erties defined by R and of j and k − j tokens
correspondently. So, standing at M [i][i + n − k],
i ∈ {1, . . . , k}, CYK searches for Ph1 in j cells in
the current row to the left and then for Ph2 in k−j
lower diagonals in the current column to the bot-

tom, checking them by R if both Ph1 and Ph2 are

found for some j and storing corresponding Ph in

M [i][i+n− k]. Figure 5 shows M at the moment
when CYK has finished for a particular input.

Adj



N1


N3=

Adj+N1

N5=
Adj+N4

N6=

N3+N2

N4=
N1+N2

N2


Figure 5: The CYK matrix after �âûñîêàÿ1
ñïèíêà2 ñòóëà3� “high1 chair3 back2”.

Rules of arbitrary arity. Surprisingly, the ex-

tension of CYK for grammars that are not bina-

rized is not widely discussed in literature. Instead,

issues of binarization and special classes of gram-

mars that do not lead to exponential growth of the

binarized version are proposed (Lange and Leiß,

2009). Indeed, due to the author’s experience, re-

searchers argue to reject using CYK, because the

increase in the size of the grammar through bi-

narization degrades the performance significantly.

Although it is true, we further show that it is not

necessary at all to binarize grammar to use CYK.

39



To use the rule system proposed earlier, we

modify CYK in the following way. Consider the

rule R : Ph1 Ph2 . . . Phr → Ph. One
can treat it as a rule R′ : Ph1 PhL → Ph,
where PhL = Ph2 Ph3 . . . Phr. In this way,
the problem reduces to the former case of bina-

rized grammar. When a reduction is applied to

PhL recursively and finally some Phr is fixed,

a set of Ph1, . . . , Phr can be checked against R.

This check is performed as described in Section

3. One can verify that modification increases the

worst run time of CYK by a polynomial multiplier

O(nr), and it is always an overestimation for nat-
ural languages, for which the case r ≥ 4 is rare.
Moreover, a big room for optimization is left. E.g.,

it is possible to extract checks from R that corre-

spond to Ph1, . . . , Phm, m < r, and apply them

before all r phrases are collected to be checked.

Equivalency of phrases. In Figure 5 two fi-

nal parses are derived by two different ways but

these parses correspond exactly to the same phrase

with no syntactic ambiguity. When n > 5, matrix
cells’ content becomes too large due to this effect,

which leads to a significant decrease in CYK per-

formance. Let us recall that the main goal of the

process is to obtain correct DT-s. Let us notice

then that two parse results in M [1][3] from Fig-
ure 5 carry the same DT. Therefore it is necessary

to merge phrases in a cell if they carry identical

DT-s. Let us assume that CYK had already put S

phrases in a cell, and a new phrase P is pending.

CYK then checks P against all elements of S and

declines to add P in S at the first time when it finds

some p ∈ P that carry the same DT as P .

Notice that it is insufficient to merge two

phrases only by properties of the head. E.g., for

�ß1 íå2 ïîñåùàë3 ïàëàòû4 ìåð5 è6 âåñîâ7�

“I1 did3 not2 attend3 the Chamber4 of Weights5
and6 Measures7” the first possible phrasal cov-

erage is [1 2 3 [4 [5 6 7]]], the second

is [1 2 3 [[4 5] 6 [7]]], and it is not

known which is correct at a syntactic level. For

both coverages, the head of the group is the same

verb with subject and object slots filled, while the

underlying DT-s differ.

Shallow weighting. Due to a high level of am-

biguity in natural languages, a huge amount of

phrases can be obtained in subcells even when the

merging of phrases takes place as described above.

Therefore it is necessary to delete some portion of

irrelevant phrases from subcells.

For every phrase that arises in any step of CYK,

let us add a weight to this phrase by the following

scheme. For each edge e = (i, j) of the corre-
sponding DT that forms a link from i-th to j-th

word of a sentence, we attach a weight |j − i|q.
The weight W of the phrase is a sum of weights of

all of the edges of its DT.

For every cell of M , after the set of phrases S

is complete by CYK, S is sorted by the weights

of phrases. After S is sorted, it turns out to be

separated into layers by the weights. Finally, only

α top layers are left in a cell. Our evaluation has

showed that q = 2 and α = 2 are sufficient.

Processing mistypings as if the correction vari-

ants were additional grammar values of tokens, be-

ing incorporated into the algorithm by a scheme

given by (Erekhinskaya et al., 2011), improves F1
up to 20% on real-world texts from the Internet

without significant loss in the performance.

Partial parses in case there is an error in an in-

put that are good enough and look much like ones

from pure dependency parsers can be obtained by

the proposed algorithm, in contrast to shift-reduce

approaches, in which only some left part of the

sentence with an error is parsed.

5 Evaluation

There is a lack of corpora for Russian to evalu-

ate parsers. In 2012, a task for Russian syntac-

tic parsers was held during the Dialogue confer-

ence. The evaluation was conducted as follows:

every parser processed a set of thousands of sep-

arate sentences from news and fiction, and then a

“golden standard” of 800 sentences was selected

and verified by several assessors. During evalua-

tion, some mistakes, such as prepositional phrase

attachment, were not taken into account as syntac-

tic parsers are originally not intended to deal with

semantics. Ignoring this, the method of evaluation

was exactly UAS (unlabeled attach score, i.e. a

number of nodes in a DT that have correct parents,

see (McDonald et al., 2005)).

Our previous version of DictaScope Syntax

parser, which was based on a modification of

Eisner’s algorithm (Erekhinskaya et al., 2011),

took part in that task in 2012, resulting with 5-

th place out of 7 (systems have been ranged by

F1), with 86,3% precision, 98% recall and 0,917

F1. Our current evaluation of the new version of

DictaScope Syntax parser, based on methods pro-

posed in this paper, follows the technique from the

40



Dialogue-2012 task (Toldova et al., 2012). We

took 800 entries from the same set of sentences

and marked them up in HT XML format. In evalu-

ation we followed the same principles as described

in (Toldova et al., 2012), reaching 93.1% preci-

sion, 97% recall and 95% F1, which correspond to

the 3rd place out of 7, with a lag of half percent

from the second place. We have also marked up a

corpus from Internet-forums and Wikipedia of 300

sentences, reaching 87% F1.

Note on complexity. It is known that for a

sentence of n tokens CYK is O(n3) algorithm by
worst case complexity, and this complexity can be

reduced to O(n2.38) by algebraic tricks (Valiant,
1975). We have performed a time complexity eval-

uation of our parser on a corpus of 1 mln Russian

sentences from Internet-news, averaging the time

for every fixed length of the sentence. We evalu-

ated sentences with lengths from 3 to 40 tokens, 12

tokens average length. Evaluation has showed the

performance of 25 sentences per second average

for one kernel of 3GHz Intel Quad. The evalua-

tion has also led to a plot given in Figure 6.

10 20 30 40 50

n

T

Figure 6: Average complexity of parsing as a func-

tion of the number of tokens

It can be verified that the plot corresponds

only to An2 for some A, but not to An3. We

can explain it in the following way. With our

grammar and Russian language, we have noticed

that for a completed upper-triangular matrix of

CYK, nonempty cells on each row are denser near

the main diagonal. Following this, for the part

of the row that forms m cells to the right of the

main diagonal, the density of nonempty cells in it

is pm ≤
c
m

for some c. Now assume that 1) the

maximum cost of the rule checking operation and

2) the maximum number of phrases’ combinations

that need to be verified against the rule base are

some constants which depend on rules, 3) τ is

the number of rules which are stored in a vocab-

ulary with a key formed by grammar values from

templates. Then, the total number of rule checks is

Ttotal ≤ c ·
1
∑

k=n−1

k
∑

i=1

n−k
∑

j=1

pn−k · log τ ≤

≤ c · log τ ·
n−1
∑

k=1

k
∑

i=1

n−k
∑

j=1

C
n−k

=

= c · C · log τ ·
n−1
∑

k=1

k = O
(

n2 log τ
)

.

6 Discussion

In this paper we proposed a method of parsing

Russian, based on a hybrid representation of the

result, which is derived from a dependency tree

with elements of the corresponding constituency

tree to model phenomena like homogenous mem-

bers, nested sentences and junction roles. This ap-

proach led to the elimination of some disadvan-

tages of both representations. We also presented a

rule system and an algorithm to acquire a ranked

set of syntactically ambiguous representations of

that kind for a given sentence. Properties of the

Cocke–Younger–Kasami algorithm and its modi-

fications, remarkable for natural language parsing,

are particularly discussed. The DictaScope Syntax

parser, based on the proposed results, is embedded

in a commercial NLP system, that is adopted in

Kribrum.ru — a service for Internet-based reputa-

tion management.

The natural question is whether this approach

can be extended to parse other languages. We per-

form the development of rule systems for English

and Arabic, and preliminary evaluation demon-

strates results comparable to those for Russian.

We also intend to propose the described HT

XML format as a standard markup language for

syntax parse trees by building the freely available

corpus for languages that lack such linguistic re-

sources, e.g. for Russian.

References

Konstantin Anisimovich, Konstantin Druzhkin, Fil-
ipp Minlos, M. Petrova, Vladimir Selegey, and
K. Zuev. 2012. Syntactic and Semantic parser
based on ABBYY Compreno linguistic technolo-
gies. In Proceedings of the International Confer-
ence “Dialogue-2012”, 2:91–103.

Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Pro-
ceedings of the Fourteenth National Conference on
Artificial Intelligence.

Tatiana Erekhinskaya, Anna Titova, and Vladimir
Okatiev. 2011. Syntax parsing for texts with mis-
spellings in DictaScope Syntax. In Proceedings

41



of the International Conference “Dialogue-2011”,
pages 186–195.

Alexey Gladkij. 1985. Syntactic structures of natural
language in automated systems for human-machine
interaction. Science, Moscow, USSR.

Daniel Jurafsky and James H. Martin. 2009. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Speech Recognition, and
Computational Linguistics. 2-nd edition. Prentice-
Hall.

Tadao Kasami. 1965. An efficient recognition
and syntax-analysis algorithm for context-free lan-
guages. Scientific report AFCRL-65-758. Air Force
Cambridge Research Lab, Bedford, MA.

Sadao Kurohashi and Makoto Nagao. 1994. Japanese
dependency/case structure analyzer.

Martin Lange and Hans Leiß. 2009. To CNF or not to
CNF? An Efficient Yet Presentable Version of the
CYK Algorithm. Informatica Didactica.

Christopher D. Manning and Hinrich Schütze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. The MIT Press, Cambridge, Mas-
sachusetts.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. of the
Joint Conf. on Human Language Technology and
Empirical Methods in Natural Language Processing
(HLT/EMNLP).

Vladimir Okatiev, Tatiana Erekhinskaya, and Tatiana
Ratanova. 2010. Secret punctuation marks.
In Proceedings of the International Conference
“Dialogue-2010”, pages 356–362.

Sergey Sharov and Joakim Nivre. 2011. The proper
place of men and machines in language technology.
processing Russian without any linguistic knowl-
edge. In Proceedings of the International Confer-
ence “Dialogue-2011”, pages 591–604.

Jacov Testelets. 2001. Introduction to general syntax.
RSUH, Moscow, Russia.

Svetlana Toldova, Elena Sokolova, Irina Astaf’eva,
Anastasia Gareyshina, A. Koroleva, Dmitry
Privoznov, E. Sidorova, L. Tupikina, and Olga
Lyashevskaya. 2012. NLP evaluation 2011–2012:
Russian syntactic parsers. In Proceedings of
the International Conference “Dialogue-2012”,
2:77–90.

Leslie Valiant. 1975. General context-free recognition
in less than cubic time. Journal of Computer and
System Sciences, 2(10):308–314.

Zhiguo Wang and Chengqing Zong. 2010. Phrase
structure parsing with dependency structure. Inter-
national Conference on Computational Linguistics.

42


