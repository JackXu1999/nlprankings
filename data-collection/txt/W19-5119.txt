



















































Cross-lingual Transfer Learning and Multitask Learning for Capturing Multiword Expressions


Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019), pages 155–161
Florence, Italy, August 2, 2019. c©2019 Association for Computational Linguistics

155

Cross-lingual Transfer Learning and Multitask Learning for Capturing
Multiword Expressions

Shiva Taslimipoor, Omid Rohanian, Le An Ha
Research Group in Computational Linguistics

University of Wolverhampton, UK
{shiva.taslimi,omid.rohanian,l.a.ha}@wlv.ac.uk

Abstract

Recent developments in deep learning have
prompted a surge of interest in the applica-
tion of multitask and transfer learning to NLP
problems. In this study, we explore for the
first time, the application of transfer learn-
ing (TRL) and multitask learning (MTL) to
the identification of Multiword Expressions
(MWEs). For MTL, we exploit the shared
syntactic information between MWE and de-
pendency parsing models to jointly train a sin-
gle model on both tasks. We specifically pre-
dict two types of labels: MWE and depen-
dency parse. Our neural MTL architecture
utilises the supervision of dependency pars-
ing in lower layers and predicts MWE tags in
upper layers. In the TRL scenario, we over-
come the scarcity of data by learning a model
on a larger MWE dataset and transferring the
knowledge to a resource-poor setting in an-
other language. In both scenarios, the result-
ing models achieved higher performance com-
pared to standard neural approaches.

1 Introduction

Multiword Expressions (MWEs) are combina-
tions of two or more lexical components that
form non/semi-compositional meaning units. Due
to their idiosyncratic behaviour, MWEs have
been studied using various statistical and machine
learning approaches including supervised classifi-
cation (Diab and Bhutada, 2009), tagging (Schnei-
der et al., 2014), and unsupervised prediction (Fa-
zly et al., 2009). Studies have focused on both
their syntactic (Constant and Nivre, 2016) and se-
mantic (Van de Cruys and Moirón, 2007) features.

Recently, the PARSEME project provided an
extensive multilingual dataset of verbal MWEs
(Ramisch et al., 2018). Datasets of certain lan-
guages in this resource are rich with a huge num-
ber of tagged sequences while others are consider-
ably smaller. Several notable systems have been

proposed to train sequence labelling models on
this dataset including neural (Taslimipoor and Ro-
hanian, 2018) and non-neural systems (Moreau
et al., 2018). MWE prediction for some of these
languages has proved to be more challenging
due to several reasons including scarcity of data,
higher percentage of unseen MWE instances in the
test set, and prevalence of discontinuous or vari-
able MWEs.

In this paper, we focus on one of those lan-
guages for which the results were collectively low
(interestingly it was English) and explore two neu-
ral approaches in order to address the shortcom-
ings of the current neural models and enhance
learning. The two approaches are: multitask learn-
ing and transfer learning, with two different moti-
vations.

Syntactic and semantic idiosyncrasies in MWEs
call for special treatment, with models that take
them into account from different perspectives.
Syntactic and semantic information are commonly
fed to the models as input features. However, we
consider an alternative way to exploit this informa-
tion. Specifically, in a supervised setting, we add
dependency syntax information as auxiliary super-
vision. Therefore we perform multitask learning
between MWE and dependency parse tags.

Syntactic dependency information has been pre-
viously proven to be successful in identifying
MWEs (Constant and Nivre, 2016). However,
neural processing methodologies are yet to be
deeply explored for MWE modelling (Constant
et al., 2017). In multitask learning we have sev-
eral different prediction tasks over the same input.
The idea is that the process of learning features for
one task can be helpful for another.

In order to deal with data scarcity in the English
dataset, in another setting we train our model on a
language with a larger data and transfer the learned
knowledge for predicting MWE tags in English.



156

In this study we build upon recent neural net-
work systems that have proved to be successful
in representing syntactic and semantic features of
text and design novel multitask and transfer learn-
ing architectures for MWE identification. The
contributions of this work are: 1) we propose a
neural model that improves MWE identification
by jointly learning MWE and dependency parse
labels; 2) We show that MWE identification mod-
els, when multitasked with dependency parsing,
outperform the models which naively add depen-
dency parse information as additional features; 3)
we propose, to the best of our knowledge for the
first time, a cross-lingual transfer learning method
for processing MWEs, thus making a contribution
towards the study of low-resource languages.

2 Related Work

Constant and Nivre (2016) proposed joint syntac-
tic and lexical analysis in which the syntactic di-
mension of their structure is represented by a de-
pendency tree, and the lexical dimension is rep-
resented by a forest of trees. The two dimen-
sions share token-level representations. They use
a transition-based system that jointly learns both
lexical and syntactic analysis resulting in an im-
provement for the task of MWE identification.

The idea of multitask learning (MTL) in neu-
ral networks was popularised by the work of Col-
lobert et al. (2011). They improved the perfor-
mance of chunking by jointly learning it with POS
tagging. Søgaard and Goldberg (2016) discuss the
idea further by pinpointing that supervising differ-
ent tasks on different layers is beneficial. Specif-
ically, in their work, for an input sequence, w1:n
they have several RNN layers l for each task,
t, and their task-specific classifier is defined as:
taskt(w1:n, i) = ft(v

l(t)
i ) where 1 ≤ i ≤ n, vi is

the output representation of RNN for word i and
ft is the tagger/classification function. This way,
different tasks might be applied to different RNN
layers (i.e. there are layers shared by several tasks,
and layers that are specific to some tasks). We use
this idea here, by having some specific layers for
final MWE prediction which are not shared with
the auxiliary parsing task.

Using an LSTM-based model, Bingel and
Søgaard (2017) performed a study to find bene-
ficial tasks for the purpose of MTL in a sequence
labelling scenario. In their work, the MWE model
benefited from most auxiliary tasks such as chunk-

ing, CCG parsing, and Super-sense tagging. A
similar finding is reported in Changpinyo et al.
(2018) where performance of an MWE tagger was
consistently improved when jointly trained with
any of the 10 different auxiliary tasks in various
MTL settings.

Transfer learning (TRL) has seen a flurry of
interest with the advent of pre-trained language
models, transformers, and contextualised embed-
dings (Howard and Ruder, 2018; Peters et al.,
2018; Devlin et al., 2018). Transfer learning is par-
ticularly helpful where data scarcity can be an is-
sue, and a related task with more data can be used
to alleviate the issue. Liu et al. (2018) is an ex-
ample of the use of task-aware language models to
enhance sequence labelling using an LSTM-CRF
architecture powered by a language model.

A related scenario in TRL is when tasks re-
main the same but models are designed to trans-
fer knowledge across languages. In NLP, cross-
lingual transfer learning has been extensively ex-
plored in the context of representation learning
where monolingual spaces are mapped into a com-
mon embedding space through methods like retro-
fitting (Faruqui et al., 2015), matrix factorization
(Vyas and Carpuat, 2016) or similar. Outside
representation learning, there have been many at-
tempts to use TRL in NLP tasks. For sequence
labelling, Kim et al. (2017) trained POS tagging
models cross-lingually without access to parallel
resources. The model consisted of two LSTM
components where one is shared between the lan-
guages and the other is private (language-specific).

Yang et al. (2017) is a notable example of
cross-lingual transfer learning under low-resource
settings where sequence labelling models were
trained to transfer knowledge between English,
Spanish, and Dutch for POS tagging, chunking,
and Named Entity Recognition (NER) through the
use of shared and private parameters. In that
work, three different architectures were explored
for cross-domain, cross-application, and cross-
lingual transfer. The core of their proposed mod-
els is similar to Lample et al. (2016), with minor
differences including the incorporation of GRU in-
stead of LSTM and a training objective based on
the max-margin principle.

3 Methodology

The core of our model is a neural architecture that
incorporates CNN and LSTM layers which are



157

commonly employed in sequence tagging mod-
els.1 We adapt the architecture to the two scenar-
ios of multitask and transfer learning. The details
of the layers and input representations for these
models are further explained in Section 4 and de-
picted in Figure 1.

Dense

Dense Dense

Dep Arcs  Dep Tags 

MWE Tags 

W1 W2 W3 W4

CNN CNN

BiLSTM

BiLSTM

auxiliary auxiliary

Figure 1: Overall architecture of the model (consisting
of two auxiliary tasks in case of MTL)

3.1 Multitask Learning

In the multitask learning scenario, the models are
required to simultaneously predict MWE tags, de-
pendency parse arcs and dependency parse labels.
A sample of all three-fold labels that the model
should predict for a sentence is depicted in figure
2. In order to learn the main output, MWE tag,
the model computes loss values for two auxiliary
outputs, Dep arc and Dep tag, and add them to the
main output loss.

Similar to the idea of Søgaard and Goldberg
(2016), we introduce the supervision of depen-
dency parsing in lower layers and aim to boost the
performance of the final MWE tagging layer. To
this end, the parallel CNNs and the first BiLSTM

1Two CNN layers without pooling act like feature extrac-
tors. Their results are then concatenated and given to the next
BiLSTM layer.

INPUT INPUT AUX-OUT AUX-OUT OUTPUT
Word POS Dep arc Dep tag MWE tag
Worse ADJ 10 advmod *
yet ADV 1 advmod *
, PUNCT 1 punct *
what PRON 6 nsubj *
is AUX 6 aux *
going VERB 10 csubj 2:VPC
on ADV 6 compound 2

:prt
will AUX 10 aux *
not PART 10 advmod *
let VERB 0 root 1:VID
us PRON 10 obj *
alone ADJ 10 xcomp 1
. PUNCT 10 punct *

Figure 2: Annotation of one sample sentence containing one
VPC and a verbal idiom in the English data for the Parseme
shared task edition 1.1.

layer is shared between the two tasks. On top of
this, two layers with independent auxiliary losses
are applied to predict dependency tags. Parallel
to this, we add a single BiLSTM before the main
output layer for predicting MWE tags (Figure 1).
In this study, we simply add the main loss to the
two auxiliary losses (which are all computed using
categorical cross-entropy).

3.2 Transfer Learning

In transfer learning, also known as domain adap-
tation, information from a source task is retained
to enhance learning for another related task. In
this study, we use TRL in a multilingual scenario.
Since our target language is low-resource, the aim
is to benefit from richer data of another language.
To this end, a model which is trained on the do-
main of one language is transferred to the domain
of another target language.

The two languages have the same sets of POS
and dependency parse tags. Therefore, one-hot
encoded POS and dependency inputs are shared
between the trained and the transferred models.
When loading pretrained contextualised embed-
dings as inputs, the sentences of individual lan-
guages have their own sets of weights. On the
other hand, we also have a setting in which our
model starts with a trainable embedding layer. In
this case, the vocabularies of both languages are
combined and indexed together. This way, com-
mon vocabularies or proper nouns of the two lan-
guages receive the same indices.

In this study, we first train the model on the Ger-
man data, and then transfer the weights to an iden-
tical model which is re-trained on English for a
fewer number of iterations.



158

4 Experiments

We experiment with the multilingual dataset from
the PARSEME project (Savary et al., 2018) which
was made available for the shared task on identi-
fication of verbal MWEs (Ramisch et al., 2018).
Verbal MWEs in the dataset include idioms, verb
particle constructions, and light verb construc-
tions, among others. MWE tags in the dataset are
similar to IOB labels, since there is a distinction
between the beginning and other components of
an MWE. We target the data for English which is
surprisingly small in this dataset (with 3, 471 train-
ing and 3, 965 test sequences) and try to use MTL
and TRL to improve MWE identification.

The inputs to our system are combinations of
ELMo embeddings which are trained on our data
using the implementation provided by Che et al.
(2018) and one-hot encoded POS tags. In cases
where we add dependency parse information as in-
puts, the representation for dependency arcs and
labels are as follows. In order to represent arcs,
we use adjacency matrix representation for each
sentence. In the adjacency matrix, each token is
assigned a row in which all cells are zero except
for the one corresponding to the head of the token
in dependency tree. Dependency labels, though
are one-hot encoded.

We set hyperparameters based on the ones used
in a similar architecture proposed by Taslimipoor
and Rohanian (2018) which was implemented for
a single task and mono-lingual setting. The CNN
layers have 200 neurons, one with filter size 2 and
the other with size 3, both with relu activation.
BiLSTM layers have both 300 neurons, dropout
0.5, and recurrent dropout of 0.2. We use the
Adam optimizer for all settings. Figure 1 shows
the whole architecture for MTL. The model archi-
tecture for standard setting and TRL is the same
excluding the auxiliary components.

4.1 Evaluation

In the MTL setting, we make comparison between
the case when the model is trained only on MWE
tags (single-task, STL) to when jointly trained to
predict MWE and dependency parsing tags in a
multitask scenario (MTL). We also compare the
results of joint prediction with the case when de-
pendency information is directly fed as additional
input. In the TRL setting, we first train our model
on the German data which has 6, 734 training se-

quences.2 We finally compare the results from
TRL with all other results.

We evaluate the models using F1-score in two
settings: 1) strict matching (MWE-based) in
which all components of an MWE are consid-
ered as a unit that should be correctly classified;
and 2) fuzzy matching (token-based) in which any
correctly predicted token of the data is counted
(Savary et al., 2017).

4.2 Results

The results are reported in Table 1. We report
the average F1-score over five separate runs along
with standard deviation. The first two rows show
the baseline results when we use the neural model
in the standard setting. For the second row, we
use dependency parsing tags as well as ELMo and
POS tags for the input to the system.

In the third and the fourth rows (MTL), we ob-
serve that the results improve when dependency
parse information is predicted as auxiliary out-
put. In particular, we observe these improvements
when adding the dependency loss outputs at one
layer before the outermost BiLSTM. We also see
that the addition of POS to the input is not neces-
sarily effective in the MTL setting (i.e. according
to the third row, the MTL setting without POS re-
sults in a better performance). Our best MTL sys-
tem outperforms the systems that participated in
open track of the Parseme shared task (Ramisch
et al., 2018) for English data. However, it per-
forms slightly worse than the neural system pro-
posed by Rohanian et al. (2019), which deals with
discontinuous MWEs using graph convolutional
network and attention mechanism.

The models are trained on google colab
with GPU: 1xTesla K80, having 2496 CUDA
cores, compute 3.7, and 12GB GDDR5 VRAM.
While the MTL model might seem to be compli-
cated, it does not add much to the time complexity
of the model. Specifically it takes, on average, 45
minutes to train the MTL model compared to 43
minutes to train STL both for 100 epochs.

The performance of TRL is only slightly bet-
ter than STL and lower than MTL. This is not to
our surprise, because ELMo vectors, that are one
of the inputs to all the models, are pre-trained on
huge amount of data and bring enough knowledge
to the low resource.

2The idea is to train on a Germanic language which is a
category that English also belongs to.



159

Token-based MWE-based
setting inputs F1 F1

STL
ELMo 34.86 ± 1.66 32.27 ± 1.36

ELMo+POS+DEP 36.08 ± 2.41 33.68 ± 2.99

MTL
ELMo 40.18 ± 1.52 35.96 ± 1.09

ELMo+POS 38.86 ± 1.63 36.61 ± 1.27

TRL
ELMo+POS 37.55 ± 1.42 35.69 ± 1.99

ELMo+POS+DEP 38.44 ± 1.92 35.84 ± 2.39

Table 1: Comparing the performance of the CNN-biLSTM model (in terms of average F1 over 5 runs with standard
deviation) in single (STL), multitask (MTL) and transfer learning (TRL) scenarios.

setting Token-based F1 MWE-based F1
closed STL 30.34 ± 1.36 28.12 ± 1.37

TRL 33.31 ± 0.75 30.40 ± 0.66

Table 2: Comparing the performance of transfer learn-
ing (TRL) with the standard setting (STL).

Furthermore, in the case of TRL, we hypothe-
size a scenario in which we do not have access to
a huge amount of data and avoid using ELMo as
the input. We perform a preliminary experiment
with a randomly initialized embedding layer as the
first component of the network to be trained with
other layers. We report the results of this experi-
ment in Table 2. This way the model is not using
any extensive external data (hence the name closed
STL). Here we can better see the benefits of trans-
ferring the model cross-lingually. More investiga-
tions need to be done to discover the limits of this
approach (e.g. through the application of different
language models and experimentation with other
architectures of the same kinds).

4.3 The Effect of Learning Rate in TRL

When transferring from the source to the target do-
main, the model is prone to overfitting on the new
data, losing the potentially beneficial information
from the high-resource model. This problem is
sometimes referred to as catastrophic forgetting.
One way to mitigate this issue is to control for
the hyperparameters of the source and target lan-
guage, specially setting the learning rate in a way
that domain adaptation occurs incrementally. On-
going research explore various regularization and
ensemble methods to preserve and transfer knowl-
edge between tasks (Chronopoulou et al., 2019;
Lee et al., 2017; Rusu et al., 2016). These meth-
ods, however, introduce varying degrees of com-
putational complexities.

Even though the sensitivity of TRL to the learn-

ing rate is largely acknowledged in the literature,
previous work is indecisive as to what learning rate
scheduling achieves the best result. Bowman et al.
(2015) lower the starting learning rates after trans-
fer, in order to preserve pre-transfer information in
early training. Kocmi and Bojar (2018) however,
found that, in TRL between language pairs in the
task of neural machine translation, changing hy-
perparameters from the parent to the child model
harmed performance. Mou et al. (2016) set the
best hyperparameters from the source task during
the validation phase and transferred them to the
target domain. They acknowledged that the hy-
perparameters can potentially become biased to-
wards the source domain. The conclusion was that
the best hyperparameters are ready to be trans-
ferred during the epoch range when the perfor-
mance peaks in the source domain.

In this work we refrained from altering the
learning rate, since, consistent with some of the
previous work, we noticed a sharp decline in per-
formance when changing this value.

5 Conclusions and Future Work

In this work we explored two neural architectures
to improve identification of MWEs through learn-
ing of related linguistic tasks. 3 We experimented
with cross-lingual transfer learning between two
Germanic languages, and in a separate scenario,
we designed and tested a multitask learning ap-
proach to tag MWEs while concurrently training
on dependency arcs and labels as auxiliary tasks.
Our results show that the models prove promising
and outperform the standard baseline. In future we
plan to study these techniques in more detail, and
make extensive comparisons between them in or-
der to understand to what extent and under what
circumstances they help MWE identification.

3The code for the experiments is available at https://
github.com/shivaat/VMWE-Identification

https://github.com/shivaat/VMWE-Identification
https://github.com/shivaat/VMWE-Identification


160

References
Joachim Bingel and Anders Søgaard. 2017. Identify-

ing beneficial task relations for multi-task learning
in deep neural networks. In Proceedings of the 15th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics: Volume 2, Short
Papers, pages 164–169.

Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632–642.

Soravit Changpinyo, Hexiang Hu, and Fei Sha. 2018.
Multi-task learning for sequence tagging: An em-
pirical study. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics,
pages 2965–2977.

Wanxiang Che, Yijia Liu, Yuxuan Wang, Bo Zheng,
and Ting Liu. 2018. Towards better UD parsing:
Deep contextualized word embeddings, ensemble,
and treebank concatenation. In Proceedings of the
CoNLL 2018 Shared Task: Multilingual Parsing
from Raw Text to Universal Dependencies, pages
55–64, Brussels, Belgium. Association for Compu-
tational Linguistics.

Alexandra Chronopoulou, Christos Baziotis, and
Alexandros Potamianos. 2019. An embarrass-
ingly simple approach for transfer learning from
pretrained language models. arXiv preprint
arXiv:1902.10547.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

Mathieu Constant, Glen Eryiit, Johanna Monti, Lon-
neke van der Plas, Carlos Ramisch, Michael Rosner,
and Amalia Todirascu. 2017. Multiword expression
processing: A survey. Computational Linguistics,
43(4):837–892.

Matthieu Constant and Joakim Nivre. 2016. A
transition-based system for joint lexical and syn-
tactic analysis. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 161–
171, Berlin, Germany. Association for Computa-
tional Linguistics.

Tim Van de Cruys and Begoña Villada Moirón. 2007.
Semantics-based multiword expression extraction.
In Proceedings of the Workshop on a Broader
Perspective on Multiword Expressions, MWE ’07,
pages 25–32, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep

bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Mona T. Diab and Pravin Bhutada. 2009. Verb noun
construction mwe token supervised classification. In
Proceedings of the Workshop on Multiword Expres-
sions: Identification, Interpretation, Disambigua-
tion and Applications, MWE ’09, pages 17–22,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,
Chris Dyer, Eduard Hovy, and Noah A Smith. 2015.
Retrofitting word vectors to semantic lexicons. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1606–1615.

Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification of
idiomatic expressions. Computational Linguistics,
35(1):61–103.

Jeremy Howard and Sebastian Ruder. 2018. Universal
language model fine-tuning for text classification. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 328–339.

Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya, and
Eric Fosler-Lussier. 2017. Cross-lingual transfer
learning for pos tagging without cross-lingual re-
sources. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2832–2838.

Tom Kocmi and Ondřej Bojar. 2018. Trivial trans-
fer learning for low-resource neural machine trans-
lation. In Proceedings of the Third Conference on
Machine Translation: Research Papers, pages 244–
252.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
arXiv preprint arXiv:1603.01360.

Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-
Woo Ha, and Byoung-Tak Zhang. 2017. Overcom-
ing catastrophic forgetting by incremental moment
matching. In Advances in neural information pro-
cessing systems, pages 4652–4662.

Liyuan Liu, Jingbo Shang, Xiang Ren,
Frank Fangzheng Xu, Huan Gui, Jian Peng,
and Jiawei Han. 2018. Empower sequence la-
beling with task-aware neural language model.
In Thirty-Second AAAI Conference on Artificial
Intelligence.

Erwan Moreau, Ashjan Alsulaimani, Alfredo Maldon-
ado, and Carl Vogel. 2018. Crf-seq and crf-deptree
at parseme shared task 2018: Detecting verbal mwes
using sequential and dependency-based approaches.

http://dl.acm.org/citation.cfm?id=1953048.2078186
http://dl.acm.org/citation.cfm?id=1953048.2078186
https://doi.org/10.1162/COLI_a_00302
https://doi.org/10.1162/COLI_a_00302
https://doi.org/10.18653/v1/P16-1016
https://doi.org/10.18653/v1/P16-1016
https://doi.org/10.18653/v1/P16-1016
http://dl.acm.org/citation.cfm?id=1613704.1613708
http://dl.acm.org/citation.cfm?id=1698239.1698243
http://dl.acm.org/citation.cfm?id=1698239.1698243


161

In Joint Workshop on Linguistic Annotation, Mul-
tiword Expressions and Constructions (LAW-MWE-
CxG-2018) at the 27th International Conference on
Computational Linguistics (COLING 2018), pages
241–247.

Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu,
Lu Zhang, and Zhi Jin. 2016. How transferable are
neural networks in nlp applications? In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 479–489.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Carlos Ramisch, Silvio Cordeiro, Agata Savary,
Veronika Vincze, Verginica Mititelu, Archna Bhatia,
Maja Buljan, Marie Candito, Polona Gantar, Voula
Giouli, et al. 2018. Edition 1.1 of the PARSEME
shared task on automatic identification of verbal
multiword expressions. In the Joint Workshop on
Linguistic Annotation, Multiword Expressions and
Constructions (LAW-MWE-CxG-2018), pages 222–
240.

Omid Rohanian, Shiva Taslimipoor, Samaneh
Kouchaki, Le An Ha, and Ruslan Mitkov. 2019.
Bridging the gap: Attending to discontinuity in
identification of multiword expressions. In Pro-
ceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 2692–
2698, Minneapolis, Minnesota. Association for
Computational Linguistics.

Andrei A Rusu, Neil C Rabinowitz, Guillaume Des-
jardins, Hubert Soyer, James Kirkpatrick, Koray
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
2016. Progressive neural networks. arXiv preprint
arXiv:1606.04671.

Agata Savary, Marie Candito, Verginica Barbu Mi-
titelu, Eduard Bejček, Fabienne Cap, Slavomı́r
Čéplö, Silvio Ricardo Cordeiro, Gülşen Eryigit,
Voula Giouli, Maarten van Gompel, Yaakov
HaCohen-Kerner, Jolanta Kovalevskaite, Simon
Krek, Chaya Liebes kind, Johanna Monti, Carla
Parra Escartı́n, Lonneke van der Plas, Behrang
QasemiZadeh, Carlos Ramisch, Federico Sangati,
Ivelina Stoyanova, and Veronika Vincze. 2018.
PARSEME multilingual corpus of verbal multi-
word expressions. In Stella Markantonatou, Carlos
Ramisch, Agata Savary, and Veronika Vincze, edi-
tors, Multiword expressions at length and in depth.
Extended papers from the MWE 2017 workshop.
Language Science Press, Berlin, Germany.

Agata Savary, Carlos Ramisch, Silvio Cordeiro, Fed-
erico Sangati, Veronika Vincze, Behrang Qasem-
izadeh, Marie Candito, Fabienne Cap, Voula Giouli,
Ivelina Stoyanova, et al. 2017. The PARSEME
shared task on automatic identification of verbal

multiword expressions. In Proceedings of the 13th
Workshop on Multiword Expressions (MWE 2017),
pages 31–47.

Nathan Schneider, Emily Danchik, Chris Dyer, and
Noah A. Smith. 2014. Discriminative lexical se-
mantic segmentation with gaps: Running the MWE
gamut. TACL, 2:193–206.

Anders Søgaard and Yoav Goldberg. 2016. Deep
multi-task learning with low level tasks supervised
at lower layers. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 231–
235, Berlin, Germany. Association for Computa-
tional Linguistics.

Shiva Taslimipoor and Omid Rohanian. 2018. Shoma
at parseme shared task on automatic identifica-
tion of vmwes: Neural multiword expression tag-
ging with high generalisation. arXiv preprint
arXiv:1809.03056.

Yogarshi Vyas and Marine Carpuat. 2016. Sparse
bilingual word representations for cross-lingual lex-
ical entailment. In Proceedings of the 2016 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 1187–1197.

Zhilin Yang, Ruslan Salakhutdinov, and William W
Cohen. 2017. Transfer learning for sequence tag-
ging with hierarchical recurrent networks. arXiv
preprint arXiv:1703.06345.

https://www.aclweb.org/anthology/N19-1275
https://www.aclweb.org/anthology/N19-1275
https://doi.org/10.18653/v1/P16-2038
https://doi.org/10.18653/v1/P16-2038
https://doi.org/10.18653/v1/P16-2038

