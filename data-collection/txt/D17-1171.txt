



















































CRF Autoencoder for Unsupervised Dependency Parsing


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1638–1643
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

CRF Autoencoder for Unsupervised Dependency Parsing∗

Jiong Cai, Yong Jiang and Kewei Tu
{caijiong,jiangyong, tukw}@shanghaitech.edu.cn

School of Information Science and Technology
ShanghaiTech University, Shanghai, China

Abstract

Unsupervised dependency parsing, which
tries to discover linguistic dependency
structures from unannotated data, is a very
challenging task. Almost all previous
work on this task focuses on learning gen-
erative models. In this paper, we de-
velop an unsupervised dependency pars-
ing model based on the CRF autoencoder.
The encoder part of our model is discrim-
inative and globally normalized which al-
lows us to use rich features as well as uni-
versal linguistic priors. We propose an
exact algorithm for parsing as well as a
tractable learning algorithm. We evaluated
the performance of our model on eight
multilingual treebanks and found that our
model achieved comparable performance
with state-of-the-art approaches.

1 Introduction

Unsupervised dependency parsing, which aims to
discover syntactic structures in sentences from un-
labeled data, is a very challenging task in natural
language processing. Most of the previous work
on unsupervised dependency parsing is based on
generative models such as the dependency model
with valence (DMV) introduced by Klein and
Manning (2004). Many approaches have been
proposed to enhance these generative models, for
example, by designing advanced Bayesian priors
(Cohen et al., 2008), representing dependencies
with features (Berg-Kirkpatrick et al., 2010), and
representing discrete tokens with continuous vec-
tors (Jiang et al., 2016).

Besides generative approaches, Grave and El-
hadad (2015) proposed an unsupervised discrim-

∗This work was supported by the National Natural Sci-
ence Foundation of China (61503248).

inative parser. They designed a convex quadratic
objective function under the discriminative clus-
tering framework. By utilizing global features
and linguistic priors, their approach achieves state-
of-the-art performance. However, their approach
uses an approximate parsing algorithm, which has
no theoretical guarantee. In addition, the perfor-
mance of the approach depends on a set of manu-
ally specified linguistic priors.

Conditional random field autoencoder (Ammar
et al., 2014) is a new framework for unsupervised
structured prediction. There are two components
of this model: an encoder and a decoder. The en-
coder is a globally normalized feature-rich CRF
model predicting the conditional distribution of
the latent structure given the observed structured
input. The decoder of the model is a generative
model generating a transformation of the struc-
tured input from the latent structure. Ammar et
al. (2014) applied the model to two sequential
structured prediction tasks, part-of-speech induc-
tion and word alignment and showed that by uti-
lizing context information the model can achieve
better performance than previous generative mod-
els and locally normalized models. However, to
the best of our knowledge, there is no previous
work applying the CRF autoencoder to tasks with
more complicated outputs such as tree structures.

In this paper, we propose an unsupervised dis-
criminative dependency parser based on the CRF
autoencoder framework and provide tractable al-
gorithms for learning and parsing. We performed
experiments in eight languages and show that our
approach achieves comparable results with previ-
ous state-of-the-art models.

1638



These stocks eventually reopenedROOT

x

y1

y2

y3

y4

These

stocks

eventually

reopenedx̂4

x̂3

x̂2

x̂1

Encoder Decoder

Figure 1: The CRF Autoencoder for the input sen-
tence “These stocks eventually reopened” and its
corresponding parse tree (shown at the top). x
and x̂ are the original and reconstructed sentence.
y is the dependency parse tree represented by a
sequence where yi contains the token and index
of the parent of token xi in the parse tree, e.g.,
y1 = 〈stocks, 2〉 and y2 = 〈reopened, 4〉. The
encoder is represented by a factor graph (with a
global factor specifying valid parse trees) and the
decoder is represented by a Bayesian net.

2 Method

2.1 Model

Figure 1 shows our model with an example in-
put sentence. Given an input sentence x =
(x1, x2, . . . , xn), we regard its parse tree as the
latent structure represented by a sequence y =
(y1, y2, . . . , yn) where yi is a pair 〈ti, hi〉, ti is the
head token of the dependency connecting to token
xi in the parse tree, and hi is the index of this head
token in the sentence. The model also contains a
reconstruction output, which is a token sequence
x̂ = (x̂1, x̂2, . . . , x̂n). Throughout this paper, we
set x̂ = x.

The encoder in our model is a log-linear model
represented by a first-order dependency parser.
The score of a dependency tree can be factorized
as the sum of scores of its dependencies. For each
dependency arc (x, i, j), where i and j are the in-
dices of the head and child of the dependency, a
feature vector f(x, i, j) is specified. The score of
a dependency is defined as the inner product of the
feature vector and a weight vector w,

φ(x, i, j) = wT f(x, i, j)

The score of a dependency tree y of sentence x is

φ(x,y) =
n∑
i=1

φ(x, hi, i)

We define the probability of parse tree y given sen-
tence x as

P (y|x) = exp(φ(x,y))
Z(x)

Z(x) is the partition function,

Z(x) =
∑

y′∈Y(x)
exp(φ(x,y′))

where Y(x) is the set of all valid parse trees of x.
The partition function can be efficiently computed
in O(n3) time using a variant of the inside-outside
algorithm (Paskin, 2001) for projective tree struc-
tures, or using the Matrix-Tree Theorem for non-
projective tree structures (Koo et al., 2007).

The decoder of our model consists of a set of
categorical conditional distributions θx|t, which
represents the probability of generating token x
conditioned on token t. So the probability of the
reconstruction output x̂ given the parse tree y is

P (x̂|y) =
n∏
i=1

θx̂i|ti

The conditional distribution of x̂,y given x is

P (y, x̂|x) = P (y|x)P (x̂|y)

2.1.1 Features
Following McDonald et al. (2005) and Grave et
al. (2015), we define the feature vector of a de-
pendency based on the part-of-speech tags (POS)
of the head, child and context words, the direc-
tion, and the distance between the head and child
of the dependency. The feature template used in
our parser is shown in Table 1.

2.1.2 Parsing
Given parameters w and θ, we can parse a sen-
tence x by searching for a dependency tree y
which has the highest probability P (x̂,y|x).

y∗ = arg max
y∈Y(x)

logP (x̂,y|x)

= arg max
y∈Y(x)

n∑
i=1

(
φ(x, hi, i) + log θx̂i|ti

)

1639



POSi × dis× dir
POSj × dis× dir
POSi × POSj × dis× dir
POSi × POSi−1 × POSj × dis× dir
POSi × POSi+1 × POSj × dis× dir
POSi × POSj × POSj−1 × dis× dir
POSi × POSj × POSj+1 × dis× dir

Table 1: Feature template of a dependency, where
i is the index of the head, j is the index of the
child, dis = |i− j|, and dir is the direction of the
dependency.

For projective dependency parsing, we can use
Eisners algorithm (1996) to find the best parse
in O(n3) time. For non-projective dependency
parsing, we can use the Chu-Liu/Edmond algo-
rithm (Chu and Liu, 1965; Edmonds, 1967; Tarjan,
1977) to find the best parse in O(n2) time.

2.2 Parameter Learning

2.2.1 Objective Function
Spitkovsky et al. (2010) shows that Viterbi EM
can improve the performance of unsupervised de-
pendency parsing in comparison with EM. There-
fore, instead of using negative conditional log like-
lihood as our objective function, we choose to use
negative conditional Viterbi log likelihood,

−
N∑
i=1

log
(

max
y∈Y(xi)

P (x̂i,y|xi)
)

+ λΩ(w) (1)

where Ω(w) is a L1 regularization term of the
encoder parameter w and λ is a hyper-parameter
controlling the strength of regularization.

To encourage learning of dependency relations
that satisfy universal linguistic knowledge, we add
a soft constraint on the parse tree based on the
universal syntactic rules following Naseem et al.
(2010) and Grave et al. (2015). Hence our objec-
tive function becomes

−
N∑
i=1

log
(

max
y∈Y(xi)

P (x̂i,y|xi)Qα(xi,y)
)

+λΩ(w)

where Q(x,y) is a soft constraint factor over the
parse tree, and α is a hyper-parameter controlling
the strength of the constraint factor. The factor Q
is also decomposable by edges in the same way
as the encoder and the decoder, and therefore our
parsing algorithm can still be used with this factor

VERB→ VERB NOUN→ NOUN
VERB→ NOUN NOUN→ ADJ
VERB→ PRON NOUN→ DET
VERB→ ADV NOUN→ NUM
VERB→ ADP NOUN→ CONJ
ADJ→ ADV ADP→ NOUN

Table 2: Universal linguistic rules

taken into account.

Q(x,y) = exp

(∑
i

1[(ti → xi) ∈ R]
)

where 1[(ti → xi) ∈ R] is an indicator function
of whether dependency ti → xi satisfies one of
the universal linguistic rules in R. The universal
linguistic rules that we use are shown in Table 2
(Naseem et al., 2010).

2.2.2 Algorithm
We apply coordinate descent to minimize the ob-
jective function, which alternately updates w and
θ. In each optimization step of w, we run two
epochs of stochastic gradient descent, and in each
optimization step of θ, we run two iterations of the
Viterbi EM algorithm.

To update w using stochastic gradient de-
scent, for each sentence x, we first run the pars-
ing algorithm to find the best parse tree y∗ =
arg maxy∈Y(x)(P (x̂,y|x)Qα(x,y)); then we can
calculate the gradient of the objective function
based on the following derivation,

∂logP (x̂,y∗|x)
∂w

=
∂logP (y∗|x)

∂w
+
∂logP (x̂|y∗)

∂w

=
∂logP (y∗|x)

∂w

=
∂
(∑n

i=1 w
T f(x, hi, i)− logZ(x)

)
∂w

=
∑

(i,j)∈D(x)
f(x, i, j)

(
1[y∗j = 〈i, xi〉]− µ(x, i, j)

)

where D(x) is the set of all possible dependency
arcs of sentence x, 1[·] is the indicator function,
and µ(x, i, j) is the expected count defined as fol-
lows,

µ(x, i, j) =
∑

y∈Y(x)

(
1[yj = 〈i, xi〉]P (y|x)

)

1640



Basque Czech Danish Dutch Portuguese Slovene Swedish Avg
Length: ≤ 10

DMV(EM) 41.1 31.3 50.8 47.1 36.7 36.7 43.5 41.0
DMV(Viterbi) 47.1 27.1 39.1 37.1 32.3 23.7 42.6 35.5
Neural DMV (EM) 46.5 33.1 55.6 49.0 30.4 42.2 44.3 43.0
Neural DMV (Viterbi) 48.1 28.6 39.8 37.2 36.5 39.9 47.9 39.7
Convex-MST (No Prior) 29.4 36.5 49.3 31.3 46.4 33.7 35.5 37.4
Convex-MST (With Prior) 30.0 46.1 51.6 35.3 55.4 63.7 50.9 47.5
CRFAE (No Prior) 49.0 33.9 28.8 39.3 47.6 34.7 51.3 40.6
CRFAE (With Prior) 49.9 48.1 53.4 43.9 68.0 52.5 64.7 54.3

Length: All
DMV(EM) 31.2 28.1 40.3 44.2 23.5 25.2 32.0 32.0
DMV(Viterbi) 40.9 20.4 32.6 33.0 26.9 16.5 36.2 29.5
Neural DMV (EM) 38.5 29.3 46.1 46.2 16.2 36.6 32.8 35.1
Neural DMV (Viterbi) 41.8 23.8 34.2 33.6 29.4 30.8 40.2 33.4
Convex-MST (No Prior) 30.5 33.4 44.2 29.3 38.3 32.2 28.3 33.7
Convex-MST (With Prior) 30.6 40.0 45.8 35.6 46.3 51.8 40.5 41.5
CRFAE (No Prior) 39.8 25.4 24.2 35.2 52.2 26.4 40.0 34.7
CRFAE (With Prior) 41.4 36.8 40.5 38.6 58.9 43.3 48.5 44.0

Table 4: Parsing accuracy on seven languages. Our model is compared with DMV (Klein and Manning,
2004), Neural DMV (Jiang et al., 2016), and Convex-MST (Grave and Elhadad, 2015)

Methods WSJ10 WSJ
Basic Setup

Feature DMV (Berg-Kirkpatrick et al., 2010) 63.0 -
UR-A E-DMV (Tu and Honavar, 2012) 71.4 57.0
Neural E-DMV (Jiang et al., 2016) 69.7 52.5
Neural E-DMV (Good Init) (Jiang et al., 2016) 72.5 57.6

Basic Setup + Universal Linguistic Prior
Convex-MST (Grave and Elhadad, 2015) 60.8 48.6
HDP-DEP (Naseem et al., 2010) 71.9 -
CRFAE 71.7 55.7

Systems Using Extra Info
LexTSG-DMV (Blunsom and Cohn, 2010) 67.7 55.7
CS (Spitkovsky et al., 2013) 72.0 64.4
MaxEnc (Le and Zuidema, 2015) 73.2 65.8

Table 3: Comparison of recent unsupervised de-
pendency parsing systems on English. Basic setup
is the same as our setup except that linguistic prior
is not used. Extra info includes lexicalization,
longer training sentences, etc.

The expected count can be efficiently computed
using the Matrix-Tree Theorem (Koo et al., 2007)
for non-projective tree structures or using a variant
of the inside-outside algorithm for projective tree
structures (Paskin, 2001).

To update θ using Viterbi EM, in the E-step we
again use the parsing algorithm to find the best
parse tree y∗ for each sentence x; then in the M-
step we update θ by maximum likelihood estima-
tion.

θc|t =
∑

x

∑
i 1[xi = c, y

∗
i = 〈·, t〉]∑

c′
∑

x

∑
i 1[xi = c′, y

∗
i = 〈·, t〉]

3 Experiments

3.1 Setup

We experimented with projective parsing and used
the informed initialization method proposed by
Klein and Manning (2004) to initialize our model
before learning. We tested our model both with
and without using the universal linguistic rules.
We used AdaGrad to optimize w. We used POS
tags of the input sentence as the tokens in our
model. We learned our model on training sen-
tences of length ≤ 10 and reported the directed
dependency accuracy on test sentences of length
≤ 10 and on all test sentences.

3.2 Results on English

We evaluated our model on the Wall Street Jour-
nal corpus. We trained our model on section 2-
21, tuned the hyperparameters on section 22, and
tested our model on section 23. Table 3 shows the
directed dependency accuracy of our model (CR-
FAE) compared with recently published results. It
can be seen that our method achieves a compara-
ble performance with state-of-the-art systems.

We also compared the performances of CRF au-
toencoder using an objective function with nega-
tive log likelihood vs. using our Viterbi version
of the objective function (Eq.1). We find that the
Viterbi version leads to much better performance
(55.7 vs. 41.8 in parsing accuracy of WSJ), which
echoes Spitkovsky et al. ’s findings on Viterbi EM
(2010).

1641



3.3 Multilingual Results
We evaluated our model on seven languages from
the PASCAL Challenge on Grammar Induction
(Gelling et al., 2012). We did not use the Arabic
corpus because the number of training sentences
with length ≤ 10 is less than 1000. The result is
shown in Table 4. The accuracies of DMV and
Neural DMV are from Jiang et.al (2016). Both
our model (CRFAE) and Convex-MST were tuned
on the validation set of each corpus. It can be seen
that our method achieves the best results on av-
erage. Besides, our method outperforms Convex-
MST both with and without linguistic prior. From
the results we can also see that utilizing universal
linguistic prior greatly improves the performance
of Convex-MST and our model.

4 Conclusion

In this paper, we propose a new discriminative
model for unsupervised dependency parsing based
on CRF autoencoder. Both learning and inference
of our model are tractable. We tested our method
on eight languages and show that our model is
competitive to the state-of-the-art systems.

The code is available at https://github.
com/caijiong/CRFAE-Dep-Parser

References
Waleed Ammar, Chris Dyer, and Noah A Smith. 2014.

Conditional random field autoencoders for unsuper-
vised structured prediction. In Advances in Neural
Information Processing Systems, pages 3311–3319.

Taylor Berg-Kirkpatrick, Alexandre Bouchard-Côté,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 582–590. As-
sociation for Computational Linguistics.

Phil Blunsom and Trevor Cohn. 2010. Unsupervised
induction of tree substitution grammars for depen-
dency parsing. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1204–1213. Association for Com-
putational Linguistics.

Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On shortest
arborescence of a directed graph. Scientia Sinica,
14(10):1396.

Shay B Cohen, Kevin Gimpel, and Noah A Smith.
2008. Logistic normal priors for unsupervised prob-
abilistic grammar induction. In Advances in Neural
Information Processing Systems, pages 321–328.

Jack Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards B,
71(4):233–240.

Jason M Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Pro-
ceedings of the 16th conference on Computational
linguistics-Volume 1, pages 340–345. Association
for Computational Linguistics.

Douwe Gelling, Trevor Cohn, Phil Blunsom, and Joao
Graça. 2012. The pascal challenge on grammar in-
duction. In Proceedings of the NAACL-HLT Work-
shop on the Induction of Linguistic Structure, pages
64–80. Association for Computational Linguistics.

Edouard Grave and Noémie Elhadad. 2015. A convex
and feature-rich discriminative approach to depen-
dency grammar induction. In ACL (1), pages 1375–
1384.

Yong Jiang, Wenjuan Han, and Kewei Tu. 2016. Un-
supervised neural dependency parsing. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 763–771,
Austin, Texas. Association for Computational Lin-
guistics.

Dan Klein and Christopher D Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, page 478. Association for Com-
putational Linguistics.

Terry Koo, Amir Globerson, Xavier Carreras Pérez,
and Michael Collins. 2007. Structured prediction
models via the matrix-tree theorem. In Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 141–150.

Phong Le and Willem Zuidema. 2015. Unsupervised
dependency parsing: Let’s use supervised parsers.
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 651–661, Denver, Colorado. Association for
Computational Linguistics.

Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd an-
nual meeting on association for computational lin-
guistics, pages 91–98. Association for Computa-
tional Linguistics.

Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1234–1244. Asso-
ciation for Computational Linguistics.

Mark A Paskin. 2001. Cubic-time parsing and learn-
ing algorithms for grammatical bigram models.
Citeseer.

1642



Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2013. Breaking out of local optima with
count transforms and model recombination: A study
in grammar induction. In EMNLP, pages 1983–
1995.

Valentin I Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,
and Christopher D Manning. 2010. Viterbi training
improves unsupervised dependency parsing. In Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning, pages 9–17. As-
sociation for Computational Linguistics.

Robert Endre Tarjan. 1977. Finding optimum branch-
ings. Networks, 7(1):25–35.

Kewei Tu and Vasant Honavar. 2012. Unambiguity
regularization for unsupervised learning of proba-
bilistic grammars. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1324–1334. Association for
Computational Linguistics.

1643


