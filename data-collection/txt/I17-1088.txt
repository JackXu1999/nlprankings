



















































Course Concept Extraction in MOOCs via Embedding-Based Graph Propagation


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 875–884,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Course Concept Extraction in MOOCs via Embedding-Based Graph
Propagation

Liangming Pan, Xiaochen Wang, Chengjiang Li, Juanzi Li and Jie Tang
Knowledge Engineering Laboratory

Department of Computer Science and Technology
Tsinghua University, Beijing 100084, China

{panlm14, wangxc15, licj17}@mails.tsinghua.edu.cn
{lijuanzi, tangjie}@tsinghua.edu.cn

Abstract

Massive Open Online Courses (MOOC-
s), offering a new way to study online,
are revolutionizing education. One chal-
lenging issue in MOOCs is how to de-
sign effective and fine-grained course con-
cepts such that students with differen-
t backgrounds can grasp the essence of
the course. In this paper, we conduc-
t a systematic investigation of the problem
of course concept extraction for MOOC-
s. We propose to learn latent repre-
sentations for candidate concepts via an
embedding-based method. Moreover, we
develop a graph-based propagation algo-
rithm to rank the candidate concepts based
on the learned representations. We eval-
uate the proposed method using different
courses from XuetangX and Coursera. Ex-
perimental results show that our method
significantly outperforms all the alterna-
tive methods (+0.013-0.318 in terms of R-
precision; p� 0.01, t-test).

1 Introduction

In contrast with traditional courses that have lim-
ited numbers of students, each online course in
a MOOC platform may draw more than 100,000
registrants (Seaton et al., 2014). The students have
very diverse backgrounds; and knowledge present-
ed in MOOCs is well understood by certain stu-
dents, but might be difficult to others. In MOOC-
s, we use course concepts to refer to the knowl-
edge concepts taught in the course videos, and re-
lated topics that help students better understand
course videos. Identifying course concepts at a
fine level is very important, as students with dif-
ferent backgrounds have totally different require-
ments. Figure 1 shows an example to illustrate the

Figure 1: Example of a video clip and its course concepts

problem addressed in this work. It is a clip (about
ten minutes long) of video captions from the data
structure course in XuetangX 1, one of the largest
MOOCs in China. The related course concepts are
marked in red and blue. We see that there are a
dozen concepts mentioned in this clip. The ma-
jor concept is “quick sort” (marked in red), but
the content is related to many other prerequisite
concepts such as “divide-and-conquer”, “bubble
sort”, and “unstable sorting algorithm” (marked in
blue). For students with a computer science back-
ground, those concepts would be easy; however,
for students from other discipline areas, the con-
cepts may be completely new. With the identified
course concepts, we can build an interactive learn-
ing environment to help individual students better
grasp the knowledge in a course. For example, for
a non-science student, the system may display the
definition of “unstable sorting” and “uniform dis-
tribution”; while for a science student, the system
could recommend advanced concepts or potential
applications of “quick sort”.

Course concepts were previously provided by
the teacher, but only at a coarse level—it is time-
consuming and tedious to annotate all fine-grained
concepts in all videos of a course. Our goal is
to automatically identify all course concepts from
each video clip. Despite quite a few studies on
related research topics, including keyphrase ex-
traction (Salton and Buckley, 1988; Mihalcea and
Tarau, 2004; Liu et al., 2010) and term extrac-

1http://www.xuetangx.com/

875



tion (Hisamitsu et al., 2000; Li et al., 2013), the
problem of course concept extraction in MOOC-
s is far from solved. The most challenging issue
in the MOOC context is the low-frequency prob-
lem. Course video captions often contain many
course concepts with low frequency, primarily for
three reasons: (1) course video captions are rela-
tively short documents, containing small numbers
of words; (2) many infrequent course concepts are
from other prerequisite or related courses (e.g., “u-
niform distribution” is from mathematical courses
and “divide-and-conquer” is from courses about
algorithms); (3) a disambiguated course concept
tends to be expressed in various ways, which pro-
duces many scattered infrequent terms. For exam-
ple, “Q sort” is a colloquial expression referring to
quick sort, and “partition exchange sort” is anoth-
er name for quick sort. They both have infrequent
presence in course videos.

Handling infrequency errors remains an open
challenge for state-of-the-art keyphrase extrac-
tors (Hasan and Ng, 2014). In MOOCs, the low-
frequency problem makes it difficult for video
captions to provide reliable statistical informa-
tion (e.g., tf-idf, c-value, and co-occurrence) for
extracting and ranking terms, and results in the
ignoring of many course-relevant yet infrequent
concepts. For example, in Figure 1, we can cor-
rectly extract the frequent concept “quick sort” us-
ing tf-idf, but fail to extract “partition exchange
sort” due to its infrequent presence, even if these
two concepts have the same meaning. The above
problem can be addressed if we know the seman-
tic relationship between “quick sort” and “parti-
tion exchange sort”. Such an “understanding” can
be facilitated by the incorporation of background
knowledge from external data sources. There ex-
ist quite a few attempts (Vivaldi and Rodr?uez,
2010; Rospocher et al., 2012) that utilize exter-
nal knowledge sources such as Wikipedia cate-
gories and WordNet to incorporate semantic rela-
tions (e.g. synonymy, is-a) into keyphrase extrac-
tion. However, these methods are not suitable in
the MOOC context, because many highly techni-
cal course concepts are not covered by generic se-
mantic resources, including Wikipedia and Word-
Net.

Recently, distributed representations of word-
s, namely word embeddings (Mikolov et al.,
2013b,a), provides us with powerful tools to rep-
resent semantic relations between words. In our

work, to address the low-frequency problem, we
incorporate online encyclopedias to learn the la-
tent representations for candidate course concepts.
Moreover, we present a novel graph-based propa-
gation algorithm to rank the candidates based on
the learned representations. We evaluate the pro-
posed method using real courses from XuetangX
and Coursera 2 with different domains and dif-
ferent languages, and compare our method with
state-of-the-art keyphrase extractors. In summa-
ry, our contributions include: a) the first attemp-
t, to the best of our knowledge, to systematically
investigate the problem of course concept extrac-
tion in MOOCs; b) proposal of an efficient model
for course concept extraction via semantic repre-
sentation learning, and ranking candidates through
graph-based propagation; c) design of four novel
datasets using real courses of different disciplines
from XuetangX and Coursera to evaluate our pro-
posed method.

2 Problem Formulation & Framework

In this section, we first give some necessary defi-
nitions and then formulate the problem of course
concept extraction.

A course corpus is composed of n cours-
es in the same subject area, denoted as D =
{Cj}j=1,...,n, where Cj is one course. We assume
that course Cj = {vij}i=1,··· ,mj consists of mj
course videos, where vij stands for the i-th video.
Each video vij is composed of its video title tij
and its video texts (video subtitles or speech script-
s) dij .

Course concepts are subjects taught in the
course. Formally, a course concept c can be de-
fined as a k-gram in D with the following proper-
ties: a) phraseness: c should be a semantically and
syntactically correct phrase; b) informativeness: c
should represent a scientific or technical concept
related to courses in D.

Course concept extraction is formally defined
as follows. Given the course corpus D, the objec-
tive is to extract candidate course concepts from
D, denoted as T = {c1, . . . , cM}, and output the
confidence score si for each candidate ci ∈ T . si
indicates the likelihood of ci to be a course con-
cept in D.

The fundamental challenge for course concep-
t extraction is how to capture the phraseness and
informativeness of course concepts. The general

2https://www.coursera.org/

876



architecture of our model consists of three parts:
(1) candidate concepts extraction, (2) candidate
concepts ranking, and (3) postprocessing.

Candidate concepts extraction pre-processes
the input corpus and extracts candidate course
concepts. Considering that most concepts are
noun phrases consisting of nouns, adjectives and
some variants of verbs, and end with a noun word
(Liu et al., 2010; Hulth, 2003; Li et al., 2013), we
obtain candidate course concepts by extracting al-
l noun phrases in the course corpus. The input
text is first tokenized and annotated with part-of-
speech (POS) tags. Next, we employ the linguistic
pattern ((A|N)+|(A|N)∗(NP )?(A|N)∗)N , in-
troduced by Justeson and Katz (1995), to extract
all k-gram noun phrases as our candidates, where
A, N , and P denote the adjectives, nouns, and
prepositions, respectively.

Candidate concepts ranking, which is the most
important part of our method, involves ranking the
extracted candidates based on their phraseness and
informativeness. In our model, we utilize local
mutual frequencies to measure the phraseness of
the extracted candidates. The informativeness of
a candidate is largely dependent on its semantic
meaning. Due to the low-frequency problem, lo-
cal statistical features do not provide sufficient in-
formation for capturing the semantic meaning car-
ried by each candidate concept. In our method,
we propose an embedding-based method, which
incorporates external knowledge from online en-
cyclopedias, to provide semantic representation-
s for candidate concepts. Based on the learned
phraseness and informativeness information, we
construct a weighted undirected graph, namely a
course concept graph (CCG), where each vertex
represents a candidate course concept. We then
rank the vertexes in the graph via a novel iter-
ative graph-based propagation algorithm, namely
course concept propagation (CCP).

In the postprocessing step, by choosing an ap-
propriate cut-off value for the ranking list, we can
easily annotate course concepts in video texts via
string matching.

3 Course Concept Graph

We combine the phraseness and informativeness
information of candidate course concepts syner-
gistically to construct the course concept graph,
formally defined as follows.

The course concept graph (CCG) of D is a

weighted undirected fully-connected graph denot-
ed as G = (V,E), where V is the vertex set of G
and E is the edge set of G. Each vertex in V rep-
resents a candidate course concept ci ∈ T and is
associated with a phraseness score ph(ci), indicat-
ing the phraseness of ci. For an edge (ci, cj) ∈ E,
its edge weight e(ci, cj) = SR(ci, cj), where
SR(ci, cj) indicates the semantic relatedness (SR)
between ci and cj , i.e., the likeness of their mean-
ing or semantic content.

A fully-connected CCG stores the SR between
any candidate pair. But in practice, we introduce
a parameter θ for pruning the graph. An edge
(ci, cj) exists in a CCG only if SR(ci, cj) > θ.
The reason for pruning is: (1) the propagation on a
fully connected CCG is computationally challeng-
ing, and (2) low-SR candidate pairs may introduce
noise during propagation. In the following subsec-
tions, we introduce the essential building blocks
of CCG, i.e., the calculation of phraseness and the
representation of semantic relatedness.

3.1 Phraseness Measurement
Phraseness examines the likelihood of a multi-
word term to be a semantically and syntactically
correct phrase. There exists several mechanisms
such as Log-likelihood (LL) (Dunning, 1993) and
Pointwise Mutual Information (PMI) (Church and
Hanks, 1990) to measure the phraseness of a ter-
m. These methods are mostly based on the as-
sumption that if the constituents of a multi-word
candidate term form a collocation, rather than co-
occurring by chance, it is more likely to be con-
sidered as a phrase (Korkontzelos et al., 2008).
In our paper, we propose a simple PMI-based ap-
proach, which utilizes local mutual frequencies in
the course corpus, to calculate the phraseness s-
core for each candidate. Specifically, for a k-
gram candidate c = {w1, · · · , wk} ∈ T , where
k > 1, we split c into fi = {w1, · · · , wi} (pre-
fix) and bi = {wi+1, · · · , wk} (suffix), where
i = 1, · · · , k−1. Then, the phraseness score ph(c)
is defined as follows.

ph(c) = max{pmi(fi, bi) | i = 1, · · · , k − 1} (1)

where pmi(fi, bi) is the PMI of the prefix fi and
the suffix bi, defined as follows.

pmi(fi, bi) =
2× freq(c)

freq(fi) + freq(bi)
(2)

where freq(c), freq(fi), freq(bi) are the occur-
rence frequencies of terms c, fi and bi in the cor-

877



pus, respectively. Due to the low-frequency prob-
lem in the MOOC corpus, the phraseness scores
for infrequent candidates may be statistically un-
reliable. Thus, the final phraseness of a candidate
is estimated on both the MOOC corpus and the on-
line encyclopedia corpus as follows.

ph(c) = α · F [phD(c)] + (1− α) · F [phE(c)] (3)

where phD(c) and phE(c) are the phraseness cal-
culated on the course corpus and the encyclope-
dia corpus, respectively. α ranges between 0 and
1, controlling the phraseness contribution of each
corpus. F [·] is a filter for the value of ph(c).
If the frequency of c is lower than a pre-defined
threshold minc, or c is an unigram, the filter sim-
ply assigns a priori value of 0.5 to the term, i.e.,
F [ph(c)] = 0.5.

3.2 Semantic Relatedness Learning

As mentioned in Section 2, statistical informa-
tion in the course corpus cannot adequately cap-
ture the informativeness of concepts because of
the low-frequency problem. In our method, we
use word embeddings (Mikolov et al., 2013b,a)
based on the online encyclopedia corpus to pro-
vide semantic representations for candidate con-
cepts. Word embeddings represents each word as
a low-dimensional, real-valued vector, and the se-
mantic similarity between two words can be re-
flected by the cosine distance of their vectors. Our
method for calculating semantic relatedness be-
tween candidate concepts consists of three steps:
(1) corpus annotation, (2) representation learning,
and (3) relatedness calculation.

Corpus Annotation. An online encyclopedia
corpus is a set of articles, and can be represented
as a sequence of words W = 〈w1 · · ·wi · · ·wm〉,
where wi denotes a word and m is the length
of the word sequence. In W , we replace any
adjacent words which literally matches a candi-
date in T as an unique token. After this step,
we obtain a concept-annotated corpus W ′ =
〈x1 · · ·xi · · ·xm′〉, where xi corresponds to a
word w ∈ W or a concept c ∈ T . Note that
the length m′ of W ′ is less than the length m of
W because multiple adjacent words are labeled as
one token.

Representation Learning. We then learn word
embeddings on W ′ to obtain vector representa-
tions for all annotated candidates and words inW ′.
For any unannotated candidate concept in T , we

obtain its semantic representations via the vector
addition of its individual word vectors.

Relatedness Calculation. When vector repre-
sentations of candidate concepts are learned, we
define the semantic relatedness between two can-
didates c1 and c2 as the cosine similarity of their
vectors, denoted as SR(c1, c2).

With this method, a candidate course concept
has no corresponding vector only if any of its con-
stituent word is absent in the whole encyclopedia
corpus. This case is unusual because a large online
encyclopedia corpus can easily cover almost all in-
dividual words of the vocabulary. Our experimen-
tal results verify that over 98% of the candidates
have vector representations in this way.

4 Graph Propagation-Based Ranking

After the construction of a course concept graph,
we propose an algorithm to rank the candidate
course concepts, based on the following assump-
tion about CCG. In CCG, a course concept is like-
ly to connect with other course concepts with high
semantic relatedness. We make this assumption
based on two reasons. First, course concepts are
usually scientific concepts. Scientific concepts in
the same domain usually have strong semantic re-
lations. Second, the main ideas of a document can
be captured by a group or groups of words with
strong semantic relationships. Course concepts
are strongly related to the course corpus, which
makes them more likely to connect with each oth-
er with high semantic relatedness.

For a vertex c in CCG, we denote conf(c) as its
confidence score of being a course concept. Fol-
lowing the above assumption, the confidence score
of a candidate course concept is evidenced by its
semantic relations with other course concepts. In
other words, high-confidence course concepts in
CCG could propagate their confidence scores to
their neighbor nodes that have high semantic re-
latedness with them, to discover other potential
course concepts. Therefore, we propose an iter-
ative graph-based algorithm, namely course con-
cept propagation (CCP), which first assigns each
vertex of CCG an initial confidence score, and it-
eratively updates the score for each vertex through
propagation. The initial confidence score of each
candidate course concept is determined by a seed
set S, which contains a list of known course con-
cepts in D. Specifically, let us denote the confi-
dence score of the vertex c in the k-th iteration as

878



confk(c), and the initial confidence score of c as
conf0(c). We set conf0(c) = 1 if c ∈ S and
conf0(c) = 0, otherwise. In the following sub-
sections, we first introduce the construction of a
seed set, and then present the propagation process
in detail.

4.1 Seed Set Construction

In our method, the seed set can be constructed ei-
ther manually or automatically. For some cours-
es, key course concepts may already be included
in course materials; however, this is not the case
for most MOOC courses. For those courses in
which key course concepts are not explicitly pro-
vided, we propose to extract them automatically
from course outlines. A course in a MOOC usu-
ally contains hundreds of videos, each of which
is associated with a video title. These video ti-
tles serve as an outline of a course and become a
potential good resource for providing high-quality
course concepts. Specifically, for each course Ci,
we first extract candidates from video titles of Ci,
following the same pattern-based procedure in the
candidate extraction step. Then, candidates are
ranked based on their tf-idf in Ci. Finally, we se-
lect the top-N ranked candidates to form the seed
set S.

4.2 Propagation Process

To design the propagation process, we need to an-
swer two crucial questions. First, a vertex should
receive confidence scores from which vertexes in
each iteration? Second, how much score should
a vertex receives from another vertex in each it-
eration? Based on the above questions, we de-
fine two general functions as follows: a) voting
score vsk(cj , ci): determines the confidence score
cj propagates to ci in the k-th iteration; b) voter-
s A(ci): it specifies the vertex set from which ci
receives the voting scores in each iteration. There-
fore, a general propagation process can be defined
as follows.

confk+1(ci) =
1

Z

(∑
cj∈A(ci) vs

k(cj , ci)

|A(ci)|

)
(4)

where Z a the normalization factor. The main idea
of Equation 4 can be explained as a voting pro-
cess. During each iteration, each vertex in A(ci)
will vote for ci. The score of ci in the next iter-
ation is dependent on the average voting score of
vertexes in A(ci). We implement the voting score

function as follows.

vsk(cj , ci) = ph(cj) · e(ci, cj) · confk(cj) (5)

where ph(cj) · e(ci, cj) determines the “authorita-
tiveness” of the vote from cj to ci. If cj has a high-
er phraseness (the voter is more credible) and cj is
more relevant to ci (the voter knows more about
the candidate), then cj tends to propagate more of
its score to ci (the vote from cj has a greater im-
pact on determining whether ci is a course concept
than the votes from other voters). As for the im-
plemention of A(ci), a natural way would be to
set A(ci) as the neighbor vertexes of ci in CCG.
Because a course concept is likely to connect with
other course concepts with high semantic related-
ness, it tends to receive high voting scores from
its course concept neighbors, compared with other
vertexes, during propagation.

However, in practice, the propagation process is
usually hampered by the overlapping problem. If
two terms ci and cj contain one or more identical
words, we say that ci and cj are overlapping. For
example, “merge sort” and “bubble sort” are over-
lapping because they both have the word “sort.”
Overlapping frequently occurs among course con-
cepts because scientific terms often have back-
ground words such as “function,” “algorithm,” and
“culture”. We find that vs(cj , ci) is less reliable if
cj and ci overlap. For example, the term “same
algorithm” is not a course concept, but it contain-
s the word “algorithm,” which makes it to have a
high SR with the course concepts including word
“algorithm,” as in “bfs algorithm” and “kmp al-
gorithm”. Therefore, the score of the term “same
algorithm” tends to be blindly increased by votes
from these course concept neighbors. To solve this
problem, we generalize the voting score by intro-
ducing a penalty function opf , which restricts the
propagation of voting scores among overlapping
terms. The generalized voting score (gvs) is de-
fined as follows.

gvsk(cj , ci) = opf(ci, cj) · ph(cj) · e(ci, cj) · confk(cj)
(6)

where opf(ci, cj) = 1 when ci and cj is not over-
lapping, and opf(ci, cj) = λ otherwise, where λ
ranges from 0 to 1.

Termination Condition. To determine when
the iteration process stops, we introduce a termi-
nation set F , including S course concepts. Let us
denote the average ranking of concepts in F af-
ter the k-th iteration as Ark(F). The propagation
process terminates if Ark+1(F) > Ark(F).

879



Dataset Domain Language #courses #videos #tokens #candidates #labeled correlation

CSEN Computer Science English 8 690 1,242,156 59,050 4,096 0.734
EcoEN Economics English 5 381 401,192 27,571 3,652 0.696
CSZH Computer Science Chinese 18 2,849 2,291,258 79,009 5,309 0.721
EcoZH Economics Chinese 8 455 645,016 60,566 3,663 0.646

Table 1: Dataset Statistics

5 Experiments

5.1 Datasets

Because there is no publicly available dataset for
course concept extraction in MOOCs, we con-
struct four course corpuses with different domains
and languages to evaluate our model. Specifical-
ly, we collect Computer Science and Economic-
s courses from two famous MOOC platforms —
Coursera and XuetangX — to construct our evalu-
ation datasets.

Coursera is one of the first MOOC platform-
s in the world. We collect video captions from
8 Computer Science courses to form the CSEN
dataset. Similarly, video captions from 5 Eco-
nomics courses are picked to construct the EcoEN
dataset. All video captions in CSEN and EcoEN
are in English. XuetangX is one of the most popu-
lar MOOC websites in China. Video captions of
18 Computer Science courses and 8 Economics
courses are collected to form the CSZH dataset
and EcoZH dataset, respectively. All video cap-
tions in CSZH and EcoZH are in Chinese.

The statistics of the four datasets are list-
ed in Table 1, where #courses, #videos, and
#tokens are the total number of courses, videos,
and tokens in each dataset. For each dataset, the
video captions are preprocessed following the pro-
cedure of Candidate concepts extraction in Sec-
tion 2. #candidates denotes the number of ex-
tracted candidates for each dataset. To create a
gold standard for evaluation, candidates for each
dataset are sent to two human annotators majoring
in the corresponding domain. For each candidate,
the annotator is asked to make a judgement about
whether it is a course concept based on the course
contents. Thus, each dataset is doubly annotat-
ed, and pearson correlation coefficient is applied
to assess inter-annotator agreement. A candidate
is labeled as a course concept only if the two an-
notators are in agreement. The column #labeled
presents the ground-truth number for each dataset.
In each dataset, we use 10% of ground truth to
form the termination set and others for evaluation.

The datasets will be publicly available later.
As for the online encyclopedia corpus, we em-

ploy the Baidu encyclopedia3, which is the largest
web-based encyclopedia in China, for Chinese
language. Our training corpus includes 6,223,649
web pages crawled from the latest Baidu ency-
clopedia. For the English language, we extract
9,834,664 articles from the latest publicly avail-
able Wikipedia dump.4

5.2 Evaluation Metrics

For our experiments, we select two evaluation
metrics. The first metric is R-precision (Rp)
(Zesch and Gurevych, 2009), which is an IR met-
ric that focuses on ranking. Given a ranking list
with n gold keyphrases, it computes the precision
of a system over its n highest-ranked candidates.
However, R-precision does not take the order of
extracted keyphrases into account. To address the
problem, we select the Mean Average Precision
(MAP), which has been the preferred metric in in-
formation retrieval for evaluating ranked lists.

5.3 Influence of Parameters

We first investigate the parameters that may influ-
ence the performance including: (1) α in Equation
3, (2) the size of seed set N , and (3) the penalty
factor λ.

The phraseness parameter α. The parame-
ter α controls the contribution of a course corpus
when calculating the phraseness score. If α is too
large, the calculated phraseness may suffer from
unreliable estimation; however, the calculated s-
core may fail to reflect the real phraseness distri-
bution in the course course when α is too small.
We investigate the influence of α in Figure 2(a).
This figure shows the R-precision of CCP when
λ = 0.3, N = 100, and α ranges from 0 to 1.
From this figure we find that, when α is set from
0.2 to 0.7, the performance is consistently good,
and remains stable with the variations of α. When
α is larger than 0.85, the R-precision drops in all

3http://baike.baidu.com/
4https://dumps.wikimedia.org/enwiki/20170120/

880



evaluation datasets. The results demonstrate that
the large-scale encyclopedia data is conducive to
making reliable estimations of the phraseness. In
our experiment, α is set as 0.4 for all four datasets.

The size of seed set N . As mentioned in Sec-
tion 4.1, we utilize the video titles to automatically
construct the seed set for each dataset. We set the
size of the seed set N = 10, 50, 100, 200 for each
dataset and explore the influences of N on CCP.
Figure 2(b) shows the R-precision of CCP with d-
ifferent N on CSEH (λ = 0.3, α = 0.4). From
the figures, we observe that the CCP reaches its
best performance through 3 to 5 iterations. When
N becomes larger, the algorithm tends to achieve
the best performance faster, i.e., it terminates with
fewer iterations. However, different settings of N
all lead to a competitive best performance (around
0.43 on CSEN). We obtain similar observation-
s on other datasets. In our experiments, we set
N = 100 for all datasets.

The penalty factor λ. The parameter λ rec-
onciles the influence of voting scores from over-
lapping terms. We demonstrate the influence of
λ in Figure 2(c). This figure shows that the C-
CP achieves its best R-precision when λ ranges
from 0.20 to 0.40 in all datasets. If we do not
severely lower the voting scores from overlap-
ping terms (λ > 0.5), or even have no penalty
(λ = 1.0), the performance is consistently un-
satisfactory. The experimental results verify that
the voting score from a non-overlapping neighbor
(e.g., “data mining” to “machine learning”) is rela-
tively more reliable than an overlapping one (e.g.,
“difficult learning” to “machine learning”) in CCP.
However, voting scores from overlapping terms
still encode some useful semantics (e.g., “merge
sort” to “quick sort”), ignoring them (λ close to
0.0) may cause a decline in performance. We set
λ = 0.3 for all datasets.

5.4 Comparison with Baselines

After we explore the influences of parameters, we
employ four baseline methods to compare with our
proposed method, i.e., course concept propagation
(CCP).

5.4.1 Baseline Approaches
First, we select two statistics-based methods, i.e.,
TF-IDF and PMI, as our baselines. TF-IDF ranks
the candidate course concepts based on their tf-idf
in the course corpus. For a candidate c ∈ T , its
tfidf R(c) = tfc × log(idfc), where tfc is the fre-

(a) α (b) N (CSEN)

(c) λ

Figure 2: The Study of Parameter Influence on CCP

quency of c in D, and idfc is based on the num-
ber of videos in which c appears. In the PMI
method, we directly rank candidates based on their
phraseness, using the method described in Section
3.1. We use the same parameter α with CCP for
a fair comparison. Second, we also employ two
graph-based methods. TextRank (Mihalcea and
Tarau, 2004) and Topical PageRank (TPR) (Liu
et al., 2010) are two state-of-the-art graph-based
keyphrase extraction approaches. They all con-
struct a graph based on word co-occurrences with-
in the corpus, and apply the PageRank algorithm
(Brin and Page, 1998) to score the vertices. The s-
core of a multi-word candidate is the average score
of all words within the candidate. Different from
TextRank, TPR decomposes traditional PageRank
into multiple PageRanks specific to various topics
using LDA (Blei et al., 2003). Topics in TPR are
learned from external data sources, and we use the
same encyclopedia corpus as ours to make a fair
comparison.

5.4.2 Implementation Details

For pruning the CCG, we set θ = 0.3 for al-
l datasets. The minc is heuristically set as 5 in
our experiments. The skip-gram model (Mikolov
et al., 2013b) is applied to train word embeddings
using the Python library gensim 5 with default
parameters. For TextRank and TPR, we use the
THUTag6, a state-of-the-art keyphrase extraction
package, which provides the implementations of
these methods.

5http://radimrehurek.com/gensim/
6https://github.com/YeDeming/THUTag

881



Method CSEN EcoEN CSZH EcoZH

TF-IDF
Rp 0.125 0.303 0.118 0.198

MAP 0.105 0.232 0.109 0.145

PMI
Rp 0.239 0.222 0.246 0.179

MAP 0.141 0.197 0.187 0.121

TextRank
Rp 0.151 0.290 0.142 0.161

MAP 0.137 0.263 0.131 0.115

TPR
Rp 0.284 0.414 0.305 0.303

MAP 0.255 0.387 0.267 0.288

CCP
Rp 0.443 0.427 0.434 0.435

MAP 0.432 0.365 0.416 0.423

Table 2: Performance of Different Methods on Differen-
t Datasets

5.4.3 Performance Comparison
In Table 2 we summarize the results of different
methods across different datasets. From the table,
we find that the proposed methods outperform all
baselines on all datasets, which indicates the ro-
bustness and effectiveness of CCP 7. Specifically,
we have the following observations.

First, TF-IDF performs competitively with Tex-
tRank, but they both perform worse than TPR and
CCP. TF-IDF and TextRank only use statistical
information within the course corpus and have a
strong reliance on term frequency, which hamper-
s their performances. For example, both TF-IDF
and TextRank correctly extract the concept “IP”,
which appears 139 times in the Computer Net-
work course, but failed to extract “Internet Pro-
tocol”, the full name of “IP”, due to its infrequent
presence. Moreover, they also highly ranked many
irrelevant yet frequent terms, such as “English lan-
guage”. These errors are significantly reduced in
CCP with the incorporation of semantic relations.

Second, TPR performs better than TextRank
across all datasets (an average of +0.141 in terms
of R-precision), but is worse than CCP. In MOOC-
s, a course usually contain multiple topics, but
TextRank often falls into a single topic, and fail-
s to cover other substantial topics of a course. For
example, in the Data Structure course, TextRank
highly ranked phrases with “tree”, but lower-
ranked concepts related to “sort”. TPR alleviates
this problem by incorporating topic information
from online encyclopedias. However, TPR also
favors frequent course concepts, because frequen-
t words tend to have high connectivity in the co-
occurrence graph, and thus receive high rankings
using PageRank.

7The improvements are all statistically significant tested
with bootstrap re-sampling with 95% confidence.

6 Related Works

Our work is relevant to automatic keyphrase ex-
traction, which concerns the automatic extrac-
tion of important and topical phrases from the
body of a document” (Turney, 2000). General-
ly, keyphrase extraction techniques can be classi-
fied into two groups: supervised approaches and
unsupervised approaches. In supervised machine-
learning approaches, the training phase usually in-
cludes a classification task: each phrase in the doc-
ument is either a keyphrase or not (You et al.,
2013). Different learning algorithms have been
employed to train the classifier, including naı̈ve
bayes (Frank et al., 1999; Witten et al., 1999), de-
cision trees (Turney, 2000), maximum entropy (Y-
ih et al., 2006; Kim and Kan, 2009) and sup-
port vector machines (Lopez and Romary, 2010;
Kim and Kan, 2009). Unsupervised approach-
es usually involve assigning a saliency score to
each candidate phrase, by considering various fea-
tures (Wan and Xiao, 2008). Generally speak-
ing, the information such as tf-idf, co-occurrence,
or neighbor documents are frequently used in un-
supervised keyphrase extraction. For example,
TextRank (Mihalcea and Tarau, 2004) is a well-
known method that ranks keywords based on the
co-occurrence graph. Huang et al. (2006) utilize
co-occurrence information to construct a semantic
network for each document and derive the impor-
tance of phrases by analyzing the network. The
ExpandRank (Wan and Xiao, 2008) model uses a
set of neighborhood documents to enhance single-
document keyphrase extraction. Recently, Liu et
al., (2015) proposed a new framework that extract-
s quality phrases from text corpora integrated with
phrasal segmentation. This model also rely on lo-
cal statistical information and requires a relatively
large corpus.

However, extracting low-frequency keyphrases
remains an open challenge for all these method-
s. To address this problem, many related works
consider not only the document level information
but also knowledge from external data sources, to
improve the effectiveness of automatic keyphrase
extraction. Representative examples include the
KEA++ system (Medelyan and Witten, 2006) that
obtains candidate phrases from a domain-specific
thesaurus. The work of Gazendam et al. (2010)
also use the thesaurus as a background corpus.
Besides the thesaurus, knowledge bases are wide-
ly used to calculate semantic relations between

882



terms. For example, Rospocher et al. (2012) use
the WordNet to detect synonym terms, and then
rank a synonym term higher even if it is ranked
lower by the statistical method. The work of Vival-
di and Rodrı́guez (2010) utilizes an ontology hier-
archy extracted from Wikipedia categories to pro-
vide background knowledge. Similarly, Berend
and Farkas (2010) invent features concerning the
Wikipedia level to achieve enhancements in per-
formance. All these methods make use of on-
ly the explicit semantic knowledge contained in
the external source. Different from them, we use
word embeddings to learn semantic representa-
tions for candidate concepts and rank them via a
novel graph-based propagation process.

7 Conclusion and Future Work

In this paper, we study the problem of course con-
cept extraction in MOOCs. We precisely define
the problem and propose a graph-based propaga-
tion method to extract course concepts by incorpo-
rating external knowledge from online encyclope-
dias. Experimental results on evaluation datasets
validate the effectiveness of the proposed method.
Incorporating external knowledge of various kind
to help extract course concepts is an intriguing di-
rection for future research. A straightforward task
is to incorporate structured information such as
“is-a” relation into the proposed model.

References
Gábor Berend and Richárd Farkas. 2010. SZTERGAK

: Feature engineering for keyphrase extraction. In
Proceedings of the 5th International Workshop on
Semantic Evaluation, SemEval@ACL 2010, Upp-
sala University, Uppsala, Sweden, July 15-16, 2010,
pages 186–189.

David M. Blei, Andrew Y. Ng, and Michael I. Jor-
dan. 2003. Latent dirichlet allocation. Internation-
al Journal of Machine Learning Research, 3:993–
1022.

Sergey Brin and Lawrence Page. 1998. The anato-
my of a large-scale hypertextual web search engine.
International Journal of Computer Networks, 30(1-
7):107–117.

Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. International Journal of Computational Lin-
guistics, 16(1):22–29.

Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. International Journal
of Computational Linguistics, 19(1):61–74.

Eibe Frank, Gordon W. Paynter, Ian H. Witten, Car-
l Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of IJCAI, pages 668–673.

Luit Gazendam, Christian Wartena, and Rogier
Brussee. 2010. Thesaurus based term ranking for
keyword extraction. In Database and Expert Sys-
tems Applications, Dexa, International Workshops,
Bilbao, Spain, August 30 - September, pages 49–53.

Kazi Saidul Hasan and Vincent Ng. 2014. Automatic
keyphrase extraction: A survey of the state of the art.
In Proceedings of ACL, pages 1262–1273.

Toru Hisamitsu, Yoshiki Niwa, and Jun’ichi Tsujii.
2000. A method of measuring term representative-
ness - baseline method using co-occurrence distribu-
tion. In Proceedings of COLING, pages 320–326.

Chong Huang, YongHong Tian, Zhi Zhou, Charles X.
Ling, and Tiejun Huang. 2006. Keyphrase extrac-
tion using semantic networks structure analysis. In
Proceedings of ICDM, pages 275–284.

Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Pro-
ceedings of EMNLP, pages 216–223.

John S. Justeson and Slava M. Katz. 1995. Technical
terminology: some linguistic properties and an al-
gorithm for identification in text. Natural Language
Engineering, 1(1):9–27.

Su Nam Kim and Min-Yen Kan. 2009. Re-examining
automatic keyphrase extraction approaches in scien-
tific articles. In Proceedings of the ACL-IJCNLP
Workshop on Multiword Expressions, pages 9–16.

Ioannis Korkontzelos, Ioannis P. Klapaftis, and Suresh
Manandhar. 2008. Reviewing and evaluating auto-
matic term recognition techniques. In Proceedings
of GoTAL, pages 248–259.

Sujian Li, Jiwei Li, Tao Song, Wenjie Li, and Baobao
Chang. 2013. A novel topic model for automatic ter-
m extraction. In Proceedings of SIGIR, pages 885–
888.

Jialu Liu, Jingbo Shang, Chi Wang, Xiang Ren, and Ji-
awei Han. 2015. Mining quality phrases from mas-
sive text corpora. In Proceedings of ACM SIGMOD,
pages 1729–1744.

Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2010. Automatic keyphrase extrac-
tion via topic decomposition. In Proceedings of
EMNLP, pages 366–376.

Patrice Lopez and Laurent Romary. 2010. Humb: Au-
tomatic key term extraction from scientific articles
in grobid. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 248–251.
Association for Computational Linguistics.

883



Olena Medelyan and Ian H. Witten. 2006. Thesaurus
based automatic keyphrase indexing. In Proceed-
ings of JCDL, pages 296–297.

Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into text. In Proceedings of EMNLP, pages
404–411.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word repre-
sentations in vector space. International Journal of
CoRR, abs/1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In Proceedings of NIPS, pages 3111–
3119.

Marco Rospocher, Sara Tonelli, Luciano Serafini, and
Emanuele Pianta. 2012. Corpus-based terminolog-
ical evaluation of ontologies. Applied Ontology,
7(4):429–448.

Gerard Salton and Chris Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
ternational Journal of Information Processing and
Management, 24(5):513–523.

Daniel T. Seaton, Yoav Bergner, Isaac L. Chuang, Piotr
Mitros, and David E. Pritchard. 2014. Who does
what in a massive open online course? Commun.
ACM, 57(4):58–65.

Peter D. Turney. 2000. Learning algorithms for
keyphrase extraction. International Journal of In-
formation Retrieval, 2(4):303–336.

Jorge Vivaldi and Horacio Rodr?uez. 2010. Finding
domain terms using wikipedia. In Proceeding of L-
REC 2010.

Xiaojun Wan and Jianguo Xiao. 2008. Single doc-
ument keyphrase extraction using neighborhood
knowledge. In Proceedings of AAAI, pages 855–
860.

Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. Kea:
practical automatic keyphrase extraction. In ACM
Conference on Digital Libraries, pages 254–255.

Wen-tau Yih, Joshua Goodman, and Vitor R. Carvalho.
2006. Finding advertising keywords on web pages.
In Proceedings of WWW, pages 213–222.

Wei You, Dominique Fontaine, and Jean-Paul A.
Barthès. 2013. An automatic keyphrase extraction
system for scientific documents. Knowl. Inf. Syst.,
34(3):691–724.

Torsten Zesch and Iryna Gurevych. 2009. Approxi-
mate matching for evaluating keyphrase extraction.
In Proceedings of RANLP, pages 484–489.

884


