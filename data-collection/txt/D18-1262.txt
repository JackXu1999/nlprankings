



















































A Unified Syntax-aware Framework for Semantic Role Labeling


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2401–2411
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

2401

A Unified Syntax-aware Framework for Semantic Role Labeling
Zuchao Li1,2,∗, Shexia He1,2,∗, Jiaxun Cai1,2, Zhuosheng Zhang1,2, Hai Zhao1,2,†,

Gongshen Liu3, Linlin Li4, Luo Si4
1Department of Computer Science and Engineering, Shanghai Jiao Tong University

2Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China

3School of Cyber Security, Shanghai Jiao Tong University, China
4Alibaba Group, Hangzhou, China

{charlee,heshexia,caijiaxun,zhangzs}@sjtu.edu.cn,
zhaohai@cs.sjtu.edu.cn, lgshen@sjtu.edu.cn,

{linyan.lll,luo.si}@alibaba-inc.com
Abstract

Semantic role labeling (SRL) aims to recog-
nize the predicate-argument structure of a sen-
tence. Syntactic information has been paid
a great attention over the role of enhanc-
ing SRL. However, the latest advance shows
that syntax would not be so important for
SRL with the emerging much smaller gap be-
tween syntax-aware and syntax-agnostic SRL.
To comprehensively explore the role of syn-
tax for SRL task, we extend existing mod-
els and propose a unified framework to inves-
tigate more effective and more diverse ways
of incorporating syntax into sequential neu-
ral networks. Exploring the effect of syn-
tactic input quality on SRL performance, we
confirm that high-quality syntactic parse could
still effectively enhance syntactically-driven
SRL. Using empirically optimized integration
strategy, we even enlarge the gap between
syntax-aware and syntax-agnostic SRL. Our
framework achieves state-of-the-art results on
CoNLL-2009 benchmarks both for English
and Chinese, substantially outperforming all
previous models.

1 Introduction

The purpose of semantic role labeling (SRL) is
to derive the predicate-argument structure of each
predicate in a sentence. A popular formalism to
represent the semantic predicate-argument struc-
ture is based on dependencies, namely depen-
dency SRL, which annotates the heads of argu-
ments rather than phrasal arguments. Given a sen-
tence (in Figure 1), SRL is generally decomposed

∗ These authors made equal contribution.† Correspond-
ing author. This paper was partially supported by National
Key Research and Development Program of China (No.
2017YFB0304100), National Natural Science Foundation of
China (No. 61672343 and No. 61733011), Key Project
of National Society Science Foundation of China (No. 15-
ZDA041), The Art and Science Interdisciplinary Funds of
Shanghai Jiao Tong University (No. 14JCRZ04) and the joint
research project with Youtu Lab of Tencent.

A2
AM-TMP

A0

Someone      always      makes    you   happy
make.02

A1

Figure 1: An example of dependency-based SRL.

into multiple subtasks in pipeline framework, con-
sisting of predicate identification (makes), predi-
cate disambiguation (make.02), argument identifi-
cation (e.g., Someone) and argument classification
(Someone is A0 for the predicate makes). SRL is
beneficial to a wide range of natural language pro-
cessing (NLP) tasks, including machine transla-
tion (Shi et al., 2016) and question answering (Be-
rant et al., 2013; Yih et al., 2016).

Most traditional SRL methods rely heavily on
feature templates that struggle to capture sufficient
discriminative information, while neural models
are capable of extracting features automatically. In
particular, recent works (Zhou and Xu, 2015; He
et al., 2017; Marcheggiani et al., 2017) propose
syntax-agnostic models for SRL and achieve fa-
vorable results, which seems to be in conflict with
the belief that syntactic information is an abso-
lutely necessary prerequisite for high-performance
SRL (Gildea and Palmer, 2002). Despite the suc-
cess of these models, the main reasons for putting
syntax aside are two-fold. First, it is still chal-
lenging to effectively incorporate syntactic infor-
mation into neural SRL models, due to the sophis-
ticated tree structure of syntactic relation. Second,
the syntactic parsers are unreliable on account of
the risk of erroneous syntactic input, which may
lead to error propagation and an unsatisfactory
SRL performance.

However, syntactic information is considered
closely related to semantic relation and plays an
essential role in SRL task (Punyakanok et al.,
2008). Recently, Marcheggiani and Titov (2017)



2402

proposed a syntactic graph convolutional networks
(GCNs) based SRL model and further improved
the SRL performance with relatively better syn-
tactic parser as input. Since syntax can provide
rich structure and information for SRL, we seek to
effectively model complex syntactic tree structure
for incorporating syntax into neural SRL.

In this paper, we present a general framework1

for SRL, which enables us to integrate syntax
into SRL in diverse ways. Following Marcheg-
giani and Titov (2017), we focus on argument
labeling and formulate SRL as sequence label-
ing problem. However, we differ by (1) lever-
aging enhanced word representation, (2) apply-
ing recent advances in recurrent neural networks
(RNNs), such as highway connections (Srivastava
et al., 2015), (3) using deep encoder with resid-
ual connections (He et al., 2016), (4) further ex-
tending Syntax Aware Long Short-Term Memory
(SA-LSTM) (Qian et al., 2017) for SRL, and (5)
introducing the Tree-Structured Long Short-Term
Memory (Tree-LSTM) (Tai et al., 2015) to model
syntactic information for SRL.

In addition, as pointed out by He et al. (2017)
for span SRL, the worse syntactic input will
hurt performance if the syntactically-driven SRL
model trusts syntactic information too much, and
high-quality syntax can still make a large impact
on SRL, which motivates us to investigate the ef-
fect of syntactic quality on dependency SRL. In
summary, our major contributions are as follows:
• We propose a unified neural framework for

dependency SRL to more effectively integrate syn-
tactic information with multiple methods.
• Our SRL framework incorporated with syntax

achieves the new state-of-the-art results on both
English and Chinese CoNLL-2009 benchmarks.
• We explore the impact of different quality of

syntactic input on SRL performance, showing that
high quality syntactic parse may indeed improve
syntax-aware SRL.

2 A Unified SRL Framework

In order to explore the effectiveness of the syntac-
tic feature from various perspectives, we propose
a unified neural framework that is capable of op-
tionally accommodating various types of syntactic
encoders for syntax-based SRL.

Since the CoNLL-2009 shared task (Hajič et al.,

1Our code is available here: https://github.com/
bcmi220/unified_syn_srl.

2009) have beforehand indicated the predicate po-
sitions, we need to identify and label all argu-
ments for each predicate, which is a typical se-
quence tagging problem. In this work, we con-
struct a general SRL framework for argument la-
beling. As shown in Figure 2, our SRL frame-
work includes three main modules, (1) BiLSTM
encoder that directly takes sequential inputs, (2)
MLP with highway connections for softmax out-
put layer, and (3) an optional syntactic encoder
that receives the outputs of the BiLSTM encoder
and then let its own outputs integrate with the BiL-
STM outputs through residual connections.

Note that when the syntactic encoder is com-
pletely removed, MLP only takes inputs directly
from the BiLSTM encoder, which let our frame-
work become a syntax-agnostic labeler.

2.1 Sentence Encoder
Word representation Given a sentence and
known predicate, we consider predicate-specific
word representation, following previous work
(Marcheggiani and Titov, 2017). Specifically,
each word embedding representation ei of input
sentence is the concatenation of several features,
a randomly initialized word embedding eri , a pre-
trained word embedding epi , a randomly initialized
lemma embedding eli, a randomly initialized POS
tag embedding eposi , and a predicate-specific fea-
ture efi , which is a binary flag set 0 or 1 indicating
whether the current word is the given predicate.

To further enhance the word representation,
we leverage an external embedding ELMo (Em-
beddings from Language Models) proposed by
Peters et al. (2018). ELMo is obtained by
deep bidirectional language model that takes char-
acters as input, enriching subword information
and contextual information, which has expres-
sive representation power. Eventually, the result-
ing word representation is concatenated as ei =
[eri , e

p
i , e

l
i, e

pos
i , e

f
i ,ELMoi].

BiLSTM encoder We use bi-directional Long
Short-term Memory neural network (BiLSTM)
(Hochreiter and Schmidhuber, 1997) as the sen-
tence encoder to model sequential inputs. Given
an input sequence (e1, . . . , en), the BiLSTM pro-
cesses these embedding vectors sequentially from
both directions to obtain two separated hidden
states,

−→
h i and

←−
h i respectively. By concatenating

the two states, we get a contextual representation
hi = [

−→
h i,
←−
h i], which will be taken by the next

https://github.com/bcmi220/unified_syn_srl
https://github.com/bcmi220/unified_syn_srl


2403

Word 
representation

cats love hats

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

+ + + +

BiLSTM 
encoder

The

The cats love hats

NMOD SBJ

OBJ

h3

h2

h1

h4

The    cats    love  
  

hats

ReLU(∑∙) ReLU(∑∙) ReLU(∑∙) ReLU(∑∙)

NMOD SBJ OBJ

W
se

lf

W
se

lf

W
se

lf

W
se

lf

The

cats

love

hats

Syntactic Encoder

GCNs

Tree-LSTM 

SA-LSTM
+

Hidden 
Layer

Softmax

Highway

residual connections

Figure 2: The unified syntax-based SRL framework

BiLSTM layer as input. In this work, we stack
four layers of BiLSTM.

2.2 Role Labeler

We adopt a Multi-Layer Perceptron (MLP) with
highway connections (Srivastava et al., 2015) on
the top of our deep encoder, which takesthe con-
catenated representation as input. The MLP con-
sists of 10 layers and we employ ReLU activa-
tions for the hidden layer. To get the final pre-
dicted semantic roles, we use a softmax layer over
the outputs to maximize the likelihood of labels.
The MLP part takes inputs from both the BiLSTM
encoder and syntactic encoder, which are joint
through a residual connection (He et al., 2016) as
shown in Figure 2. It is worth noting that our
deep encoder is different from the one of Marcheg-
giani and Titov (2017), which directly applies a
softmax transformation over the syntactic repre-
sentation and predicts the role label for each word.
That is, their syntactic encoder outputs are directly
taken as the input of hidden layer.

3 Syntactic Encoder

To integrate the syntactic information into sequen-
tial neural networks, we employ a syntactic en-
coder on top of the BiLSTM encoder.

Specifically, given a syntactic dependency tree

T , for each node nk in T , let C(k) denote the syn-
tactic children set of nk,H(k) denote the syntactic
head of nk, and L(k, ·) be the dependency relation
between node nk and those have a direct arc from
or to nk. Then we formulate the syntactic encoder
as a transformation f τ over the node nk, which
may take some of C(k), H(k), or L(k, ·) as input,
and compute a syntactic representation vk for node
nk, namely, vk = f τ (C(k), H(k), L(k, ·), xk).
When not otherwise specified, xk denotes the in-
put feature representation of nk which may be ei-
ther the word representation ek or the output of
BiLSTM hk, σ denotes the logistic sigmoid func-
tion, and � denotes the element-wise multiplica-
tion.

In practice, the transformation f τ can be any
syntax encoding method. In this paper, we will
consider three types of syntactic encoders, syntac-
tic graph convolutional network (Syntactic GCN)
(in Section 3.1), syntax aware LSTM (SA-LSTM)
(in Section 3.2), tree-structured LSTM (Tree-
LSTM) (in Section 3.3). Then, we will provide
a brief introduction in subsequent subsections.

3.1 Syntactic GCN
GCN (Kipf and Welling, 2017) is proposed to in-
duce the representations of nodes in a graph based
on the properties of their neighbors. Given its
effectiveness, Marcheggiani and Titov (2017) in-



2404

troduce a generalized version for the SRL task,
namely syntactic GCN, and shows that syntactic
GCN is effective in incorporating syntactic infor-
mation into neural models.

Syntactic GCN captures syntactic information
flows in two directions, one from heads to depen-
dents (along), the other from dependents to heads
(opposite). Besides, it also models the informa-
tion flows from a node to itself, namely, it as-
sumes that a syntactic graph contains self-loop for
each node. Thus, the syntactic GCN transforma-
tion of a node nk is defined on its neighborhood
N(k) = C(k)∪H(k)∪{nk}. For each edge con-
nects nk and its neighbor nj , we can compute a
vector representation for it,

uk,j = W
dir(k,j)xj + b

L(k,j),

where dir(k, j) denotes the direction type (along,
opposite or self-loop) of the edge from nk to
nj , W dir(k,j) is direction type specific parame-
ter, bL(k,j) is label specific parameter. Consider-
ing that syntactic information from all the neigh-
boring nodes may make different contribution to
semantic role labeling, syntactic GCN introduces
an additional edge-wise gating for each node pair
(nk, nj) as

gk,j = σ(W
dir(k,j)
g xk + b

L(k,j)
g ).

The syntactic representation vk for a node nk can
be then computed as:

vk = ReLU(
∑

j∈N(k)

gk,j � uk,j).

3.2 SA-LSTM

SA-LSTM (Qian et al., 2017) is an extension of
the standard BiLSTM architecture, which aims to
simultaneously encode the syntactic and contex-
tual information for a given word as shown in Fig-
ure 2. On one hand, the SA-LSTM calculates the
hidden state in sequence timestep order like the
standard LSTM,

ig = σ(W
(i)xk + U

(i)hk−1 + b
(i)),

fg = σ(W
(f)xk + U

(f)hk−1 + b
(f)),

og = σ(W
(o)xk + U

(o)hk−1 + b
(o)),

u = f(W (u)xk + U
(u)hk−1 + b

(u)),

ck = ig � u+ fg � ck−1.

On the other hand, it further incorporates the
syntactic information into the representation of
each word by introducing an additional gate,

sg = σ(W
(s)xk + U

(s)hk−1 + b
(s)),

hk = og � f(ck) + sg � h̃k.

where h̃k = f(
∑

tj<tk
αj × hj) is the weighted

sum of all hidden state vectors hj which come
from previous node (word) nj , the weight factor
αj is actually a trainable weight related to the de-
pendency relation L(k, ·) when there exists a di-
rected edge from nj to nk.

Note that h̃k is always the hidden state vector
of the syntactic head of nk according to the defini-
tion of αj . Since a word will be assigned a single
syntactic head, such a strict constraint prevents the
SA-LSTM from incorporating complex syntactic
structures. Inspire by the idea of GCN, we relax
the directed constraint of αj , whenever there is an
edge between nj and nk.

After the SA-LSTM transformation, the outputs
of the SA-LSTM layer from both directions are
concatenated and taken as the syntactic represen-
tation of each word nk, i.e., vk = [

−→
hk,
←−
hk]. Differ-

ent from the syntactic GCN, SA-LSTM encoding
both syntactic and contextual information in a sin-
gle vector vk.

3.3 Tree-LSTM

Tree-LSTM (Tai et al., 2015) can be considered
as an extension of the standard LSTM, which
aims to model the tree-structured topologies. At
each timestep, it composes an input vector and
the hidden states from arbitrarily many child units.
Specifically, the main difference between Tree-
LSTM unit and the standard one is that the mem-
ory cell updating and the calculation of gating vec-
tors are depended on multiple child units. A Tree-
LSTM unit can be connected to arbitrary number
of child units and assigns a single forget gate for
each child unit. This provides Tree-LSTM the
flexibility to incorporate or drop the information
from each child unit.

Given a syntactic tree, the Tree-LSTM trans-
formation is defined on node nk and its children
set C(k), which can be formulated as follows (Tai



2405

et al., 2015):

h̃k =
∑

j∈C(k)

hk, (1)

ig = σ(W
(i)xk + U

(i)h̃k + b
(i)),

fk,jg = σ(W
(f)xk + U

(f)hj + b
(f)), (2)

og = σ(W
(o)xk + U

(o)h̃k + b
(o)),

u = tanh(W (u)xk + U
(u)h̃k + b

(u)),

ck = ig � u+
∑

j∈C(k)

fk,jg � cj ,

hk = og � tanh(ck).

where j ∈ C(k), hj is the hidden state of the j-th
child node, ck is the memory cell of the head node
k, and hk is the hidden state of node k. Note that
in Eq.(2), a single forget gate fk,jg is computed for
each hidden state hj .

However, the primitive form of Tree-LSTM
does not take the dependency relations into con-
sideration. Given the importance of dependency
relations in SRL task, we further extend the Tree-
LSTM by adding an additional gate rg and refor-
mulate the Eq. (1),

rk,jg = σ(W
(r)xk + U

(r)hj + b
L(k,j)),

h̃k =
∑

j∈C(k)

rk,jg � hj .

where bL(k,j) is a relation label specific bias term.
After the Tree-LSTM transformation, the hidden
state of each node in dependency tree is taken as
its syntactic representation, i.e., vk = hk.

4 Experiments

We evaluate our models performance of syntac-
tic GCN (henceforth Syn-GCN), SA-LSTM and
Tree-LSTM on CoNLL-2009 datasets both for En-
glish and Chinese with standard training, devel-
opment and test splits. For predicate disambigua-
tion, we follow previous work (Marcheggiani and
Titov, 2017), using the off-the-shelf disambiguator
from Roth and Lapata (2016). For syntactic de-
pendency tree, we parse the corpus with Biaffine
Parser (Dozat and Manning, 2017).

4.1 Experimental Settings

In our experiments, the pre-trained word embed-
dings for English are 100-dimensional GloVe vec-
tors (Pennington et al., 2014). For Chinese, we

System P R F1
Local model
Lei et al. (2015) − − 86.6
FitzGerald et al. (2015) − − 86.7
Roth and Lapata (2016) 88.1 85.3 86.7
Marcheggiani et al. (2017) 88.7 86.8 87.7
Marcheggiani and Titov (2017) 89.1 86.8 88.0
He et al. (2018) 89.7 89.3 89.5
Cai et al. (2018) 89.9 89.2 89.6
Ours (Syn-GCN) 90.3 89.3 89.8
Ours (SA-LSTM) 90.8 88.6 89.7
Ours (Tree-LSTM) 90.0 88.8 89.4
Global model
Björkelund et al. (2010) 88.6 85.2 86.9
FitzGerald et al. (2015) − − 87.3
Roth and Lapata (2016) 90.0 85.5 87.7
Ensemble model
FitzGerald et al. (2015) − − 87.7
Roth and Lapata (2016) 90.3 85.7 87.9
Marcheggiani and Titov (2017) 90.5 87.7 89.1

Table 1: Results on the English in-domain test set.

exploit Wikipedia documents to train the same di-
mensional Word2Vec embeddings (Mikolov et al.,
2013). All other vectors are randomly initialized,
the dimension of lemma embeddings is 100, and
the dimension of POS tag embedding is 32. In ad-
dition, we use 300-dimensional ELMo embedding
for English2.

During training, we use the categorical cross-
entropy as objective, with Adam optimizer
(Kingma and Ba, 2015) the learning rate 0.001,
and the batch size is set to 64. The BiLSTM
encoder consists of 4-layer BiLSTM with 512-
dimensional hidden units. We apply dropout for
BiLSTM with a 90% keeping probability between
time-steps and layers. We train models for a max-
imum of 20 epochs and obtain the nearly best
model based on English development results.

4.2 Results

We compare our models of Syn-GCN, SA-LSTM
and Tree-LSTM with previous approaches for
dependency SRL on both English and Chinese.
Noteworthily, our model is local (argument identi-
fication and classification decisions are condition-
ally independent) and single without reranking,
which neither includes global inference nor com-

2For Chinese, we do not use pre-trained ELMo whose
weights are only available for English.



2406

System P R F1
Local model
Zhao et al. (2009a) 80.4 75.2 77.7
Marcheggiani et al. (2017) 83.4 79.1 81.2
Marcheggiani and Titov (2017) 84.6 80.4 82.5
He et al. (2018) 84.2 81.5 82.8
Cai et al. (2018) 84.7 84.0 84.3
Ours (Syn-GCN) 84.8 81.2 83.0
Ours (SA-LSTM) 85.2 80.5 82.8
Ours (Tree-LSTM) 84.5 80.7 82.6
Global model
Björkelund et al. (2009) 82.4 75.1 78.6
Roth and Lapata (2016) 83.2 75.9 79.4

Table 2: Results on the Chinese test set.

bines multiple models. The experimental results
on the in-domain English and Chinese test sets are
summarized in Tables 1 and 2, respectively.

For English, our models of Syn-GCN, SA-
LSTM and Tree-LSTM overwhelmingly surpass
most previously published single models, achiev-
ing state-of-the-art results of 89.8%, 89.7% and
89.4% in F1 scores respectively. In comparison
to ensemble models, our Syn-GCN even performs
better than the previous model (Marcheggiani and
Titov, 2017) with a margin of 0.7% F1.

From Table 1, we also see that our Syn-GCN
model provides the best recall and F1 score, while
our SA-LSTM model yields the competitive per-
formance with higher precision at the expense of
recall, which shows that SA-LSTM is better at
classifying arguments. Overall, the Tree-LSTM
gives slightly weaker performance, which may
be attributed to tree-structured network topology.
More specifically, Tree-LSTM only considers in-
formation from arbitrary child units so that each
node lacks of the information from parent. How-
ever, our Syn-GCN and SA-LSTM combine bidi-
rectional information, both head-to-dependent and
dependent-to-head.

For Chinese (Table 2), even though we use the
same parameters as for English, our models are
still comparable with the best reported results.

Table 3 presents the results on English out-of-
domain test set. Our models outperform the high-
est records achieved by He et al. (2018), with
absolute improvements of 0.2-0.5% in F1 scores.
These favorable results on both in-domain and out-
of-domain data demonstrate the effectiveness and
robustness of our proposed unified framework.

System P R F1
Local model
Lei et al. (2015) − − 75.6
FitzGerald et al. (2015) − − 75.2
Roth and Lapata (2016) 76.9 73.8 75.3
Marcheggiani et al. (2017) 79.4 76.2 77.7
Marcheggiani and Titov (2017) 78.5 75.9 77.2
He et al. (2018) 81.9 76.9 79.3
Cai et al. (2018) 79.8 78.3 79.0
Ours (Syn-GCN) 80.6 79.0 79.8
Ours (SA-LSTM) 81.0 78.2 79.6
Ours (Tree-LSTM) 80.4 78.7 79.5
Global model
Björkelund et al. (2010) 77.9 73.6 75.7
FitzGerald et al. (2015) − − 75.2
Roth and Lapata (2016) 78.6 73.8 76.1
Ensemble model
FitzGerald et al. (2015) − − 75.5
Roth and Lapata (2016) 79.7 73.6 76.5
Marcheggiani and Titov (2017) 80.8 77.1 78.9

Table 3: Results on the English out-of-domain test set.

Our system P R F1
Syn-GCN 89.2 87.6 88.4
w/o POS tag 88.5 87.5 88.0
w/o ELMo embedding 87.7 86.7 87.2

Table 4: Ablation on the English development set.

4.3 Ablation and Analysis

To investigate the contributions of word represen-
tation and deep encoder in our method, we conduct
a series of ablation studies on the English develop-
ment set, unless otherwise stated.

Effect of word representation In order to better
understand how the enhanced word representation
influences our model performance, we train our
Syn-GCN model with different settings in input
word embeddings. Table 4 shows results for our
system when we remove POS tag and ELMo em-
bedding respectively. Interestingly, the impact of
POS tag embedding (about 0.4% F1) is less com-
pared to the previous works, which allows us to
build an accuracy model even when the POS tag is
unavailable. We also observe that effect of ELMo
embedding is somewhat surprising (1.2% F1 per-
formance degradation). Experimental results in-
dicate that a combination of these features could
enhance the word representation, leading to SRL
performance improvement.



2407

System syntax-agnostic syntax-aware ∆ F1
M&T (2017) 87.7 88.0 0.3
He et al. (2018) 88.7 89.5 0.8
Cai et al. (2018) 89.6 89.6 ≈0.0
Our model 88.7 89.8 1.1

Table 5: Comparison of our Syn-GCN model with
(Marcheggiani and Titov, 2017), (He et al., 2018) and
(Cai et al., 2018) on the English test set. ∆ F1 shows
the absolute performance gap between syntax-agnostic
and syntax-aware settings.

Effect of deep encoder Table 5 reports F1
scores of our Syn-GCN model, Marcheggiani and
Titov (2017), He et al. (2018) and Cai et al. (2018)
on English test set in both syntax-agnostic and
syntax-aware settings. The comparison shows that
our framework is more effective for incorporat-
ing syntactic information by giving more perfor-
mance improvement through introducing syntax
over syntax-agnostic SRL than previous state-of-
the-art systems did.

To further investigate the impact of deep en-
coder, we perform our Syn-GCN, SA-LSTM and
Tree-LSTM models with another alternative con-
figuration, using the same encoder as (Marcheg-
giani and Titov, 2017) (M&T encoder for short),
which removes the residual connections from our
framework. The corresponding results of our mod-
els are also summarized in Table 6 for compar-
ison. Note that the first row is the results of
our syntax-agnostic model. Surprisingly, we ob-
serve a dramatical performance decline of 1.2%
F1 for our Syn-GCN model with M&T encoder.
A less significant performance loss for our SA-
LSTM (−0.4%) and Tree-LSTM (−0.5%) mod-
els shows that the Syn-GCN is more sensitive to
contextual information. Nevertheless, the overall
results show that applying deep encoder could re-
ceive higher gains.

4.4 Syntactic Role

As mentioned before, syntactic parsers are unreli-
able due to the risk of erroneous syntactic input,
especially on out-of-domain data. This section
thus attempts to explore the impact of different
quality of syntactic input on SRL performance. To
this end, we further carry out experiments on En-
glish test data with different syntactic inputs based
on our Syn-GCN model.

Our system P R F1
Baseline (syntax-agnostic) 89.5 87.9 88.7
Syn-GCN 90.3 89.3 89.8
SA-LSTM 90.8 88.6 89.7
Tree-LSTM 90.0 88.8 89.4
Syn-GCN (M&T encoder) 89.2 88.0 88.6
SA-LSTM (M&T encoder) 89.8 88.8 89.3
Tree-LSTM (M&T encoder) 90.0 87.8 88.9

Table 6: Comparison of models with deep encoder and
M&T encoder (Marcheggiani and Titov, 2017) on the
English test set.

Syntactic Input
Four types of syntactic inputs are used to ex-
plore the role of syntax in our unified frame-
work, (1) the automatically predicted parse pro-
vided by CoNLL-2009 shared task, (2) the parsing
results of the CoNLL-2009 data by state-of-the-
art syntactic parser, the Biaffine Parser (used in
our previous experiments), (3) corresponding re-
sults from another parser, the BIST Parser (Kiper-
wasser and Goldberg, 2016), which is also adopted
by Marcheggiani and Titov (2017), (4) the gold
syntax available from the official data set.

Evaluation Metric
It is worth noting that for SRL task, the standard
evaluation metric is the semantic labeled F1 score
(Sem-F1), and we use the labeled attachment score
(LAS) to quantify the quality of syntactic input. In
addition, the ratio between labeled F1 score for se-
mantic dependencies and the LAS for syntactic de-
pendencies (Sem-F1/LAS) proposed by CoNLL-
2008 shared task3 (Surdeanu et al., 2008), are also
given for reference. To a certain extent, the ratio
Sem-F1/LAS could normalize the semantic score
relative to syntactic parse, impartially estimating
the true performance of SRL, independent of the
performance of the input syntactic parser.

Comparison and Discussion
Table 7 presents the comprehensive results of
our Syn-GCN model on the four syntactic inputs
aforementioned of different quality together with
previous SRL models. A number of observations
can be made from these results. First, our model
gives quite stable SRL performance no matter the
syntactic input quality varies in a broad range, ob-

3CoNLL-2008 is an English-only task, while CoNLL-
2009 extends to a multilingual one. Their main difference
is that predicates have been pre-identified for the latter.



2408

System LAS P R Sem-F1 Sem-F1/LAS
Zhao et al. (2009c) [SRL-only] 86.0 − − 85.4 99.3
Zhao et al. (2009a) [Joint] 89.2 − − 86.2 96.6
Björkelund et al. (2010) 89.8 87.1 84.5 85.8 95.6
Lei et al. (2015) 90.4 − − 86.6 95.8
Roth and Lapata (2016) 89.8 88.1 85.3 86.7 96.5
Marcheggiani and Titov (2017) 90.34∗ 89.1 86.8 88.0 97.41
He et al. (2018) [CoNLL-2009 predicted] 86.0 89.7 89.3 89.5 104.0
He et al. (2018) [Gold syntax] 100 91.0 89.7 90.3 90.3
Our Syn-GCN (CoNLL-2009 predicted) 86.0 90.5 88.5 89.5 104.07
Our Syn-GCN (Biaffine Parser) 90.22 90.3 89.3 89.8 99.53
Our Syn-GCN (BIST Parser) 90.05 90.3 89.1 89.7 99.61
Our Syn-GCN (Gold syntax) 100.0 91.0 90.0 90.5 90.50

Table 7: Results on English test set, in terms of labeled attachment score for syntactic dependencies (LAS), se-
mantic precision (P), semantic recall (R), semantic labeled F1 score (Sem-F1), the ratio Sem-F1/LAS. All numbers
are in percent. A superscript * indicates LAS results from our personal communication with the authors.

taining overall higher scores compared to previ-
ous state-of-the-arts. Second, It is interesting to
note that the Sem-F1/LAS score of our model be-
comes relatively smaller as the syntactic input be-
comes better. Though not so surprised, these re-
sults show that our SRL component is even rela-
tively stronger. Third, when we adopt a syntactic
parser with higher parsing accuracy, our SRL sys-
tem will achieve a better performance. Notably,
our model yields a Sem-F1 of 90.5% taking gold
syntax as input. It suggests that high-quality syn-
tactic parse may indeed enhance SRL, which is
consistent with the conclusion in (He et al., 2017).

5 Related Work

Semantic role labeling was pioneered by Gildea
and Jurafsky (2002), also known as shallow se-
mantic parsing. In early works of SRL, consider-
able attention has been paid to feature engineer-
ing (Pradhan et al., 2005; Zhao and Kit, 2008;
Zhao et al., 2009a,b,c; Li et al., 2009; Björkelund
et al., 2009; Zhao et al., 2013). Along with the
the impressive success of deep neural networks
(Zhang et al., 2016; Cai and Zhao, 2016; Qin et al.,
2016; Wang et al., 2016b,a; Zhang et al., 2018; Li
et al., 2018; Huang et al., 2018), a series of neural
SRL systems have been proposed. For instance,
Foland and Martin (2015) presented a semantic
role labeler using convolutional and time-domain
neural networks. FitzGerald et al. (2015) exploited
neural network to jointly embed arguments and se-
mantic roles, akin to the work (Lei et al., 2015),
which induced a compact feature representation

applying tensor-based approach.

Recently, people have attempted to build end-
to-end systems for span SRL without syntactic in-
put (Zhou and Xu, 2015; He et al., 2017; Tan et al.,
2018). Similarly, Marcheggiani et al. (2017) also
proposed a syntax-agnostic model for dependency
SRL and obtained favorable results. Despite the
success of syntax-agnostic models, there are sev-
eral works focus on leveraging the advantages of
syntax. Roth and Lapata (2016) employed depen-
dency path embedding to model syntactic infor-
mation and exhibited a notable success. Marcheg-
giani and Titov (2017) leveraged the graph convo-
lutional network to incorporate syntax into a neu-
ral SRL model. Qian et al. (2017) proposed SA-
LSTM to model the whole tree structure of depen-
dency relation in an architecture engineering way.

Besides, syntax encoding has also successfully
promoted other NLP tasks. Tree-LSTM (Tai et al.,
2015) is a variant of the standard LSTM that can
encode a dependency tree with arbitrary branch-
ing factors, which has shown effectiveness on se-
mantic relatedness and the sentiment classification
tasks. In this work, we extend the Tree-LSTM
with a relation specific gate and employ it to re-
cursively encode the syntactic dependency tree for
SRL. RCNN (Zhu et al., 2015) is an extension of
the recursive neural network (Socher et al., 2010)
which has been popularly used to encode trees
with fixed branching factors. The RCNN is able
to encode a tree structure with arbitrary number
of factors and is useful in a re-ranking model for
dependency parsing (Zhu et al., 2015).



2409

In our experiments, we simplify and reformu-
late the RCNN model. However, the simplified
model performs poorly on the development and
the test sets. The reason might be that the RCNN
model with a single global composition parameter
is too simple to cover all types of syntactic relation
in a dependency tree. Because of the poor perfor-
mance of the modified RCNN, we do not include
it in this work. Considering there might be other
approach to incorporate the recursive network in
SRL model, we leave it as our future work and
just provide a brief discussion here.

In this work, we extend existing methods and
introduce Tree-LSTM for incorporating syntax
into SRL. Rather than proposing completely new
model, we synthesize these techniques and present
a unified framework to take genuine superiority of
syntactic information.

6 Conclusion

This paper presents a unified neural framework
for dependency-based SRL, effectively incorpo-
rating syntactic information by directly modeling
syntax based on syntactic parse tree. Rather than
proposing completely new model, we extend ex-
isting models and apply tree-structured LSTM for
SRL. Our approach significantly outperforms all
previous models, achieving state-of-the-art results
on the CoNLL-2009 benchmarks for both English
and Chinese.

Our experiments specially show that giving an
enlarged performance gap from syntax-agnostic
to syntax-aware setting, SRL can be further pro-
moted with the help of deep enhanced representa-
tion and effective methods of integrating syntax.
Furthermore, we explore the impact of the qual-
ity of syntactic input. The relevant results indicate
that high-quality syntactic parse is more favorable
to semantic role labeling.

References
Jonathan Berant, Andrew Chou, Roy Frostig, and

Percy Liang. 2013. Semantic parsing on Freebase
from question-answer pairs. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1533–1544,
Seattle, Washington, USA.

Anders Björkelund, Bohnet Bernd, Love Hafdell, and
Pierre Nugues. 2010. A high-performance syntac-
tic and semantic dependency parser. In Proceed-
ings of the 23rd International Conference on Com-

putational Linguistics (COLING 2010), pages 33–
36, Beijing, China.

Anders Björkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning (CoNLL 2009):
Shared Task, pages 43–48, Boulder, Colorado.

Deng Cai and Hai Zhao. 2016. Neural word segmen-
tation learning for Chinese. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 409–420.

Jiaxun Cai, Shexia He, Zuchao Li, and Hai Zhao. 2018.
A full end-to-end semantic role labeler, syntactic-
agnostic over syntactic-aware? In Proceedings of
the 27th International Conference on Computational
Linguistics (COLING 2018), pages 2753–2765.

Timothy Dozat and Christopher D. Manning. 2017.
Deep biaffine attention for neural dependency pars-
ing. In Proceedings of the 5th International Confer-
ence on Learning Representations (ICLR).

Nicholas FitzGerald, Oscar Tckstrm, Kuzman
Ganchev, and Dipanjan Das. 2015. Semantic role
labeling with neural network factors. In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
960–970.

William Foland and James Martin. 2015. Dependency-
based semantic role labeling using convolutional
neural networks. In Joint Conference on Lexical and
Computational Semantics, pages 279–288.

Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245–288.

Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition.
In Proceedings of 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
239–246, Philadelphia, Pennsylvania, USA.

Jan Hajič, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Antònia Martı́, Lluı́s
Màrquez, Adam Meyers, Joakim Nivre, Sebastian
Padó, Jan Štěpánek, Pavel Straňák, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning (CoNLL 2009): Shared Task,
pages 1–18, Boulder, Colorado.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770–
778.



2410

Luheng He, Kenton Lee, Mike Lewis, and Luke Zettle-
moyer. 2017. Deep semantic role labeling: What
works and what’s next. In Proceedings of the 55th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 473–483, Vancou-
ver, Canada.

Shexia He, Zuchao Li, Hai Zhao, and Hongxiao Bai.
2018. Syntax for semantic role labeling, to be, or not
to be. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), volume 1, pages 2061–2071.

Sepp Hochreiter and Jrgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Yafang Huang, Zuchao Li, Zhuosheng Zhang, and
Hai Zhao. 2018. Moon ime: neural-based Chinese
pinyin aided input method with customizable asso-
ciation. Proceedings of ACL 2018, System Demon-
strations, pages 140–145.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceed-
ings of the 3rd International Conference on Learn-
ing Representations (ICLR).

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional LSTM feature representations. Transactions
of the Association for Computational Linguistics,
4:313–327.

Thomas N Kipf and Max Welling. 2017. Semi-
supervised classification with graph convolutional
networks. In Proceedings of the 5th International
Conference on Learning Representations (ICLR).

Tao Lei, Yuan Zhang, Lluı́s Màrquez, Alessandro Mos-
chitti, and Regina Barzilay. 2015. High-order low-
rank tensors for semantic role labeling. In Proceed-
ings of the 2015 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL:
HLT), pages 1150–1160.

Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu,
and Peide Qian. 2009. Improving nominal srl in
Chinese language with verbal srl information and
automatic predicate recognition. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 3-Volume 3,
pages 1280–1288.

Zuchao Li, Jiaxun Cai, Shexia He, and Hai Zhao. 2018.
Seq2seq dependency parsing. In Proceedings of
the 27th International Conference on Computational
Linguistics, pages 3203–3214.

Diego Marcheggiani, Anton Frolov, and Ivan Titov.
2017. A simple and accurate syntax-agnostic neural
model for dependency-based semantic role labeling.
In Proceedings of the 21st Conference on Computa-
tional Natural Language Learning (CoNLL 2017),
pages 411–420, Vancouver, Canada.

Diego Marcheggiani and Ivan Titov. 2017. Encod-
ing sentences with graph convolutional networks
for semantic role labeling. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1506–1515,
Copenhagen, Denmark.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems (NIPS), pages 3111–3119.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543, Doha,
Qatar.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (NAACL: HLT), New Orleans,
Louisiana.

Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Daniel Jurafsky. 2005. Semantic role
labeling using different syntactic views. In Pro-
ceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
581–588, Ann Arbor, Michigan.

Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257–287.

Feng Qian, Lei Sha, Baobao Chang, Lu Chen Liu, and
Ming Zhang. 2017. Syntax aware LSTM model for
semantic role labeling. In The Workshop on Struc-
tured Prediction for Natural Language Processing,
pages 27–32.

Lianhui Qin, Zhisong Zhang, and Hai Zhao. 2016. Im-
plicit discourse relation recognition with context-
aware character-enhanced embeddings. In Proceed-
ings of COLING 2016, the 26th International Con-
ference on Computational Linguistics: Technical
Papers, pages 1914–1924.

Michael Roth and Mirella Lapata. 2016. Neural se-
mantic role labeling with dependency path embed-
dings. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 1192–1202, Berlin, Germany.

Chen Shi, Shujie Liu, Shuo Ren, Shi Feng, Mu Li,
Ming Zhou, Xu Sun, and Houfeng Wang. 2016.
Knowledge-based semantic embedding for machine
translation. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 2245–2254, Berlin, Germany.



2411

Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Advances in Neural Information
Processing Systems (NIPS), pages 1–9.

Rupesh K Srivastava, Klaus Greff, and Jürgen Schmid-
huber. 2015. Training very deep networks. In Ad-
vances in neural information processing systems,
pages 2377–2385.

Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluı́s Màrquez, and Joakim Nivre. 2008. The conll
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the Twelfth
Conference on Computational Natural Language
Learning - Shared Task (CoNLL), pages 159–177,
Manchester, England.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Nat-
ural Language Processing (ACL-IJCNLP), pages
1556–1566, Beijing, China.

Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen,
and Xiaodong Shi. 2018. Deep semantic role label-
ing with self-attention. In Proceedings of the Thirty-
Second AAAI Conference on Artificial Intelligence.

Rui Wang, Hai Zhao, Bao-Liang Lu, Masao Utiyama,
and Eiichiro Sumita. 2016a. Connecting phrase
based statistical machine translation adaptation. In
Proceedings of COLING 2016, the 26th Interna-
tional Conference on Computational Linguistics:
Technical Papers, pages 3135–3145.

Rui Wang, Hai Zhao, Sabine Ploux, Bao-Liang Lu, and
Masao Utiyama. 2016b. A bilingual graph-based se-
mantic model for statistical machine translation. In
IJCAI, pages 2950–2956.

Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-
Wei Chang, and Jina Suh. 2016. The value of se-
mantic parse labeling for knowledge base question
answering. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 201–206, Berlin, Germany.

Zhisong Zhang, Hai Zhao, and Lianhui Qin. 2016.
Probabilistic graph-based dependency parsing with
convolutional neural network. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 1382–1392.

Zhuosheng Zhang, Yafang Huang, and Hai Zhao. 2018.
Subword-augmented embedding for cloze reading
comprehension. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics,
pages 1802–1814.

Hai Zhao, Wenliang Chen, Jun’ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009a. Multi-
lingual dependency learning: Exploiting rich fea-
tures for tagging syntactic and semantic dependen-
cies. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
- Shared Task (CoNLL), pages 61–66, Boulder, Col-
orado.

Hai Zhao, Wenliang Chen, and Chunyu Kit. 2009b. Se-
mantic dependency parsing of nombank and prop-
bank: An efficient integrated approach via a large-
scale feature selection. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1-Volume 1, pages 30–
39.

Hai Zhao, Wenliang Chen, and Guodong Zhou. 2009c.
Multilingual dependency learning: A huge feature
engineering method to semantic dependency pars-
ing. In Proceedings of the Thirteenth Conference
on Computational Natural Language Learning -
Shared Task (CoNLL), pages 55–60, Boulder, Col-
orado.

Hai Zhao and Chunyu Kit. 2008. Parsing syntactic
and semantic dependencies with two single-stage
maximum entropy models. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 203–207.

Hai Zhao, Xiaotian Zhang, and Chunyu Kit. 2013. In-
tegrative semantic dependency parsing via efficient
large-scale feature selection. Journal of Artificial
Intelligence Research, 46:203–233.

Jie Zhou and Wei Xu. 2015. End-to-end learning of
semantic role labeling using recurrent neural net-
works. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 1127–1137, Beijing, China.

Chenxi Zhu, Xipeng Qiu, Xinchi Chen, and Xuanjing
Huang. 2015. A re-ranking model for dependency
parser with recursive convolutional neural network.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), vol-
ume 1, pages 1159–1168.


