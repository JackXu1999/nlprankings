



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 729–740
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1068

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 729–740
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1068

Beyond Binary Labels: Political Ideology Prediction of Twitter Users
Daniel Preoţiuc-Pietro

Positive Psychology Center
University of Pennsylvania

danielpr@sas.upenn.edu

Ye Liu∗
School of Computing

National University of Singapore
liuye@comp.nus.edu.sg

Daniel J. Hopkins
Political Science Department
University of Pennsylvania
danhop@sas.upenn.edu

Lyle Ungar
Computing & Information Science

University of Pennsylvania
ungar@cis.upenn.edu

Abstract

Automatic political preference prediction
from social media posts has to date proven
successful only in distinguishing between
publicly declared liberals and conserva-
tives in the US. This study examines
users’ political ideology using a seven-
point scale which enables us to identify
politically moderate and neutral users –
groups which are of particular interest to
political scientists and pollsters. Using
a novel data set with political ideology
labels self-reported through surveys, our
goal is two-fold: a) to characterize the po-
litical groups of users through language
use on Twitter; b) to build a fine-grained
model that predicts political ideology of
unseen users. Our results identify differ-
ences in both political leaning and engage-
ment and the extent to which each group
tweets using political keywords. Finally,
we demonstrate how to improve ideology
prediction accuracy by exploiting the rela-
tionships between the user groups.

1 Introduction

Social media is used by people to share their opin-
ions and views. Unsurprisingly, an important part
of the population shares opinions and news related
to politics or causes they support, thus offering
strong cues about their political preferences and
ideologies. In addition, political membership is
also predictable purely from one’s interests or de-
mographics — it is much more likely for a reli-
gious person to be conservative or for a younger
person to lean liberal (Ellis and Stimson, 2012).

∗ Work carried out during a research visit at the Univer-
sity of Pennsylvania

User trait prediction from text is based on the as-
sumption that language use reflects a user’s de-
mographics, psychological states or preferences.
Applications include prediction of age (Rao et al.,
2010; Flekova et al., 2016b), gender (Burger et al.,
2011; Sap et al., 2014), personality (Schwartz
et al., 2013; Preoţiuc-Pietro et al., 2016), socio-
economic status (Preoţiuc-Pietro et al., 2015a,b;
Liu et al., 2016c), popularity (Lampos et al., 2014)
or location (Cheng et al., 2010).

Research on predicting political orientation has
focused on methodological improvements (Pen-
nacchiotti and Popescu, 2011) and used data sets
with publicly stated dichotomous political orien-
tation labels due to their easy accessibility (Syl-
wester and Purver, 2015). However, these data
sets are not representative samples of the entire
population (Cohen and Ruths, 2013) and do not
accurately reflect the variety of political attitudes
and engagement (Kam et al., 2007).

For example, we expect users who state their
political affiliation in their profile description,
tweet with partisan hashtags or appear in public
party lists to use social media as a means of popu-
larizing and supporting their political beliefs (Bar-
berASa, 2015). Many users may choose not to
publicly post about their political preference for
various social goals or perhaps this preference
may not be strong or representative enough to be
disclosed online. Dichotomous political prefer-
ence also ignores users who do not have a political
ideology. All of these types of users are very im-
portant for researchers aiming to understand group
preferences, traits or moral values (Lewis and Rei-
ley, 2014; Hersh, 2015).

The most common political ideology spectrum
in the US is the conservative – liberal (Ellis and
Stimson, 2012). We collect a novel data set of
Twitter users mapped to this seven-point spectrum
which allows us to:

729

https://doi.org/10.18653/v1/P17-1068
https://doi.org/10.18653/v1/P17-1068


1. Uncover the differences in language use be-
tween ideological groups;

2. Develop a user-level political ideology predic-
tion algorithm that classifies all levels of en-
gagement and leverages the structure in the po-
litical ideology spectrum.
First, using a broad range of language features

including unigrams, word clusters and emotions,
we study the linguistic differences between the
two ideologically extreme groups, the two ideo-
logically moderate groups and between both ex-
tremes and moderates in order to provide insight
into the content they post on Twitter. In addition,
we examine the extent to which the ideological
groups in our data set post about politics and com-
pare it to a data set obtained similarly to previous
work.

In prediction experiments, we show how accu-
rately we can distinguish between opposing ideo-
logical groups in various scenarios and that previ-
ous binary political orientation prediction has been
oversimplified. Then, we measure the extent to
which we can predict the two dimensions of polit-
ical leaning and engagement. Finally, we build an
ideology classifier in a multi-task learning setup
that leverages the relationships between groups.1

2 Related Work

Automatically inferring user traits from their on-
line footprints is a prolific topic of research, en-
abled by the increasing availability of user gen-
erated data and advances in machine learning.
Beyond its research oriented goals, user profil-
ing has important industry applications in online
marketing, personalization or large-scale audience
profiling. To this end, researchers have used a
wide range of types of online footprints, includ-
ing video (Subramanian et al., 2013), audio (Alam
and Riccardi, 2014), text (Preoţiuc-Pietro et al.,
2015a), profile images (Liu et al., 2016a), social
data (Van Der Heide et al., 2012; Hall et al., 2014),
social networks (Perozzi and Skiena, 2015; Rout
et al., 2013), payment data (Wang et al., 2016) and
endorsements (Kosinski et al., 2013).

Political orientation prediction has been studied
in two related, albeit crucially different scenarios,
as also identified in (Zafar et al., 2016). First, re-
searchers aimed to identify and quantify orienta-
tion of words (Monroe et al., 2008), hashtags (We-
ber et al., 2013) or documents (Iyyer et al., 2014),

1Data is available at http://www.preotiuc.ro

or to detect bias (Yano et al., 2010) or impartial-
ity (Zafar et al., 2016) at a document level.

Our study belongs to the second category, where
political orientation is inferred at a user-level. All
previous studies study labeling US conservatives
vs. liberals using either text (Rao et al., 2010),
social network connections (Zamal et al., 2012),
platform-specific features (Conover et al., 2011)
or a combination of these (Pennacchiotti and
Popescu, 2011; Volkova et al., 2014), with very
high reported accuracies of up to 94.9% (Conover
et al., 2011).

However, all previous work on predicting user-
level political preferences are limited to a binary
prediction between liberal/democrat and conser-
vative/republican, disregarding any nuances in po-
litical ideology. In addition, as the focus of the
studies is more on the methodological or inter-
pretation aspects of the problem, another down-
side is that the user labels were obtained in sim-
ple, albeit biased ways. These include users who
explicitly state their political orientation on user
lists of party supporters (Zamal et al., 2012; Pen-
nacchiotti and Popescu, 2011), supporting par-
tisan causes (Rao et al., 2010), by following
political figures (Volkova et al., 2014) or party
accounts (Sylwester and Purver, 2015) or that
retweet partisan hashtags (Conover et al., 2011).
As also identified in (Cohen and Ruths, 2013) and
further confirmed later in this study, these data sets
are biased: most people do not clearly state their
political preference online – fewer than 5% ac-
cording to Priante et al. (2016) – and those that
state their preference are very likely to be political
activists. Cohen and Ruths (2013) demonstrated
that predictive accuracy of classifiers is signifi-
cantly lower when confronted with users that do
not explicitly mention their political orientation.
Despite this, their study is limited because in their
hardest classification task, they use crowdsourced
political orientation labels, which may not corre-
spond to reality and suffer from biases (Flekova
et al., 2016a; Carpenter et al., 2016). Further, they
still only look at predicting binary political orien-
tation. To date, no other research on this topic has
taken into account these findings.

3 Data Set

The main data set used in this study consists of
3,938 users recruited through the Qualtrics plat-
form (D1). Each participant was compensated

730



1 2 3 4 5 6 7
0

250

500

750

1000 Political Orientation

Figure 1: Distribution of political ideology in our data set,
from 1 – Very Conservative through 7 – Very Liberal.

with 3 USD for 15 minutes of their time. All
participants first answered the same demographic
questions (including political ideology), then were
directed to one of four sets of psychological ques-
tionnaires unrelated to the political ideology ques-
tion. They were asked to self-report their politi-
cal ideology on a seven point scale: Very conser-
vative (1), Conservative (2), Moderately conser-
vative (3), Moderate (4), Moderately liberal (5),
Liberal (6), Very liberal (7). In addition, partic-
ipants had the option of choosing Apathetic and
Other, which have ambiguous fits on the conserva-
tive – liberal spectrum and were removed from our
analysis (399 users). We also asked participants to
self-report their gender (2322 female, 1205 male,
12 other) and age. Participants were all from the
US in order to limit the impact of cultural and po-
litical factors. The political ideology distribution
in our sample is presented in Figure 1.

We asked users their Twitter handle and down-
loaded their most recent 3,200 tweets, leading to
a total of 4,833,133 tweets. Before adding users
to our 3,938 user data set, we performed the fol-
lowing checks to ensure that the Twitter handle
was the user’s own: 1) after compensation, users
were if they were truthful in reporting their handle
and if not, we removed their data from analysis;
2) we manually examined all handles marked as
verified by Twitter or that had over 2000 followers
and eliminated them if they were celebrities or cor-
porate/news accounts, as these were unlikely the
users who participated in the survey. This study
received approval from the Institutional Review
Board (IRB) of the University of Pennsylvania.

In addition, to facilitate comparison to previ-
ous work, we also use a data set of 13,651 users
with overt political orientation (D2). We se-
lected popular political figures unambiguously as-
sociated with US liberal politics (@SenSanders,

@JoeBiden, @CoryBooker, @JohnKerry) or US
conservative politics (@marcorubio, @tedcruz,
@RandPaul, @RealBenCarson). Liberals in our
set (Nl = 7417) had to follow on Twitter all of
the liberal political figures and none of the con-
servative figures. Likewise, conservative users
(Nc = 6234) had to follow all of the conservative
figures and no liberal figures. We downloaded up
to 3,200 of each user’s most recent tweets, leading
to a total of 25,493,407 tweets. All tweets were
downloaded around 10 August 2016.

4 Features

In our analysis, we use a broad range of linguistic
features described below.
Unigrams We use the bag-of-words representa-
tion to reduce each user’s posting history to a nor-
malised frequency distribution over the vocabulary
consisting of all words used by at least 10% of the
users (6,060 words).
LIWC Traditional psychological studies use a
dictionary-based approach to representing text.
The most popular method is based on Linguis-
tic Inquiry and Word Count (LIWC) (Pennebaker
et al., 2001), and automatically counts word fre-
quencies for 64 different categories manually con-
structed based on psychological theory. These in-
clude different parts-of-speech, topical categories
and emotions. Each user is thereby represented as
a frequency distribution over these categories.
Word2Vec Topics An alternative to LIWC is to
use automatically generated word clusters i.e.,
groups of words that are semantically and/or syn-
tactically similar. The clusters help reducing
the feature space and provides additional inter-
pretability.

To create these groups of words, we use an au-
tomatic method that leverages word co-occurrence
patterns in large corpora by making use of the dis-
tributional hypothesis: similar words tend to co-
occur in similar contexts (Harris, 1954). Based
on co-occurrence statistics, each word is repre-
sented as a low dimensional vector of numbers
with words closer in this space being more simi-
lar (Deerwester et al., 1990). We use the method
from (Preoţiuc-Pietro et al., 2015a) to compute
topics using word2vec similarity (Mikolov et al.,
2013a,b) and spectral clustering (Shi and Malik,
2000; von Luxburg, 2007) of different sizes (from
30 to 2000). We have tried other alternatives to
building clusters: using other word similarities to

731



generate clusters – such as NPMI (Lampos et al.,
2014) or GloVe (Pennington et al., 2014) as pro-
posed in (Preoţiuc-Pietro et al., 2015a) – or us-
ing standard topic modelling approached to create
soft clusters of words e.g., Latent Dirichlet Allo-
cation (Blei et al., 2003). For brevity, we present
experiments with the best performing feature set
containing 500 Word2Vec clusters. We aggregate
all the words posted in a users’ tweets and repre-
sent each user as a distribution of the fraction of
words belonging to each cluster.
Sentiment & Emotions We hypothesise that dif-
ferent political ideologies differ in the type and
amount of emotions the users express through
their posts. The most studied model of dis-
crete emotions is the Ekman model (Ekman,
1992; Strapparava and Mihalcea, 2008; Strappa-
rava et al., 2004) which posits the existence of
six basic emotions: anger, disgust, fear, joy, sad-
ness and surprise. We automatically quantify these
emotions from our Twitter data set using a publicly
available crowd-sourcing derived lexicon of words
associated with any of the six emotions, as well as
general positive and negative sentiment (Moham-
mad and Turney, 2010, 2013). Using these lexi-
cons, we assign a predicted emotion to each mes-
sage and then average across all users’ posts to ob-
tain user level emotion expression scores.
Political Terms In order to select unigrams per-
taining to politics, we assigned the most frequent
12,000 unigrams in our data set to three categories:
• Political words: mentions of political terms

(234);
• Political NEs: mentions of politician proper

names out of the political terms (39);
• Media NEs: mentions of political media

sources and pundits out of the political terms
(20).

This coding was initially performed by a re-
search assistant studying political science with
good knowledge of US politics and were further
filtered and checked by one of the authors.

5 Analysis

First, we explore the relationships between lan-
guage use and political ideological groups within
each feature set and pairs of opposing user
groups. To illustrate differences between ideolog-
ical groups we compare the two political extremes
(Very Conservative – Very Liberal) and the politi-
cal moderates (Moderate Conservative – Moderate

Liberal). We further compare outright moderates
with a group combining the two political extremes
to study if we can uncover differences in politi-
cal engagement and extremity, regardless of the
conservative–liberal leaning.

We use univariate partial linear correlations
with age and gender as co-variates to factor out
the influence of basic demographics. For ex-
ample, in D1, users who reported themselves as
very conservative are older and more likely males
(µage = 35.1, pctmale = 44%) than the data av-
erage (µage = 31.2, pctmale = 35%). Addi-
tionally, prior to combining the two ideologically
extreme groups, we sub-sampled the larger class
(Very Liberal) to match the smaller class (Very
Conservative) in age and gender. In the later pre-
diction experiments, we do not perform match-
ing, as this represents useful signal for classifica-
tion (Ellis and Stimson, 2012). Results with uni-
grams are presented in Figure 2 and with the other
features in Table 1. These are selected using stan-
dard statistical significance tests.

5.1 Very Conservatives vs. Very Liberals
The comparison between the extreme categories
reveals the largest number of significant differ-
ences. The unigrams and Word2Vec clusters
specific to conservatives are dominated by re-
ligion specific terms (‘praying’, ‘god’, W2V-
485, W2V-018, W2V-099, L-RELIG), confirming
a well-documented relationship (Gelman, 2009)
and words describing family relationships (‘un-
cle’, ‘son’, L-FAMILY), another conservative
value (Lakoff, 1997). The emphasis on reli-
gious terms among conservatives is consistent
with the claim that many Americans associate
‘conservative’ with ‘religious’ (Ellis and Stim-
son, 2012). Extreme liberals show a tendency to
use more adjectives (W2V-075, W2V-110), ad-
verbs (L-ADVERB), conjunctions (L-CONJ) and
comparisons (L-COMPARE) which indicate more
nuanced and complex posts. Extreme conser-
vatives post tweets higher in all positive emo-
tions than liberals (L-POSEMO, Emot-Joy, Emot-
Positive), confirming a previously hypothesised
relationship (Napier and Jost, 2008). However, ex-
treme liberals are not associated with posting neg-
ative emotions either, only using words that reflect
more anxiety (L-ANX), which is related to neu-
roticism in which the liberals are higher (Gerber
et al., 2010).

Political term analysis reveals the partisan terms

732



(a) V.Con.(1) vs. V.Lib.(7) (c) M.Con.(3) vs. M.Lib.(5) (e) Moderates (4) vs. V.Con.(1) + V.Lib.(7)

(b) V.Con.(1) vs. V.Lib.(7) (d) M.Con.(3) vs. M.Lib.(5) (f) Moderates (4) vs. V.Con.(1) + V.Lib.(7)

Figure 2: Unigrams with the highest 80 Pearson correlations shown as word clouds in three vertical panels with a binary
variable representing the two ideological groups compared. The size of the unigram is scaled by its correlation with the
ideological group in bold. The color indexes relative frequency, from light blue (rarely used) to dark blue (frequently used). All
correlations are significant at p < .05 and controlled for age and gender.

r Category Words r Category Words
V.Con.(1) vs. V.Lib.(7) V.Con.(1) vs. V.Lib.(7)
.249 W2V–485 god, peace, thankful, pray, bless, blessed, prayers, praying .236 W2V–075 bad, kind, weird, kinda, horrible, creepy, strange, extremely
.180 W2V–018 jesus, lord, christ, sin, grace, god’s, praise, gods, glory, thou .195 W2V–238 an, excuse, actual, idiot, asshole, example, absolute
.156 W2V–099 church, bible, serve, worship, preach, christians, pastor .192 W2V–487 into, through, must, myself, decided, completely, upon
.140 W2V–491 soooo, soo, sooooo, soooooo, tooo, sooooooo, toooo .191 W2V–110 quite, awful, exciting, brilliant, perfectly, usual
.119 W2V–027 kno, yu, abt, tht, dnt, wut, tru, somethin, ion, wen .186 W2V–448 off, almost, whole, literally, entire, basically, ridiculous
.204 L–RELIG god, hell, holy, soul, pray, angel, praying, christ, sin, amen .175 L–ANX awkward, worry, scared, fear, afraid, horrible, scary, upset
.145 L–POSEMO love, good, lol, :), great, happy, best, thanks, win, free .164 L–ADVERB just, so, when, about, now, how, too, why, back, really
.127 L–FAMILY baby, family, mom, dad, son, bro, mother, babies, fam, folks .161 L–CONJ and, so, but, if, when, how, as, or, because, then
.118 L–NETSPEAK rt, u, lol, :), twitter, gonna, yo, ur, omg, ya .147 L–COMPARE like, more, as, best, than, better, after, most, before, same
.101 L–YOU you, your, u, you’re, ur, ya, yourself, youre, you’ll, you’ve .138 L–DIFFER not, but, if, or, really, can’t, than, other, didn’t, actually
.152 Emot–Joy love, good, happy, hope, god, birthday, fun, favorite, pretty
.086 Emot–Positive love, good, happy, hope, god, birthday, real, fun, favorite
.107 Emot–Surprise good, hope, birthday, excited, money, finally, chance, guess
.132→

.068
Political Terms #pjnet, #tcot, @foxnews, polls, @realdonaldtrump, @ted-cruz, @yahoonews

.161→

.090
Political Terms

gay, sanders, racism, racist, rape, @barackobama, democ-
racy, feminist, democratic, protesting, protest, bernie, femi-
nism, protesters, transgender

M.Con.(3) vs. M.Lib.(5) M.Con.(3) vs. M.Lib.(5)
.108 W2V–485 god, peace, thankful, pray, bless, blessed, prayers, praying .116 W2V–458 hilarious, celeb, capaldi, corrie, chatty, corden, barrowman
.088 W2V–018 jesus, lord, christ, sin, grace, god’s, praise, gods, glory, thou .106 W2V–373 photo, art, pictures, photos, instagram, photoset, image
.085 W2V–214 frank, savage, brad, ken, kane, pitt, watson, leonardo .106 W2V–316 hot, sex, naked, adult, teen, porn, lesbian, tube, tits
.085 W2V–436 luck, lucky, boss, sir, c’mon, mate, bravo, ace, pal, keeper .087 W2V–024 turn, accidentally, barely, constantly, onto, bug, suddenly

.086 W2V–389 ha, ooo, uh, ohhh, ohhhh, ma’am, gotcha, gee, ohhhhh
.096 L–RELIG god, hell, holy, soul, pray, angel, praying, christ, sin, amen .104 L–SEXUAL fuck, gay, sex, sexy, dick, naked, fucks, cock, aids, cum
.093 L–DRIVES love, good, lol, :), great, happy, best, thanks, win, free .088 L–ANGER hate, fuck, hell, stupid, mad, sucks, suck, war, dumb, ugly
.093 L–WE we, our, us, let’s, we’re, lets, we’ll, we’ve, ourselves, we’d
.087 L–AFFILIATION love, we, our, use, help, twitter, friends, family, join, friend
.086 Emot–Joy love, good, happy, hope, god, birthday, fun, favorite, pretty .097 Emot–Disgust bad, hate, shit, finally, damn, feeling, hell, bitch, boy, sick
.096 Political Terms islamic .136 Political Terms rape

.086 rights
Moderates (4) vs. V.Con.(1)+V.Lib.(7) Moderates (4) vs. V.Con.(1)+V.Lib.(7)
.084 W2V–098 girls, boys, em, ladies, bitches, hoes, grown, dudes, dem .191 W2V–309 obama, president, scott, hillary, romney, clinton, ed, sarah

.188 W2V–237 freedom, violence, revolution, muslim, muslims, terrorists

.184 W2V–269 bill, rights, congress, gop, republicans, republican, passes

.174 W2V–296 justice, rule, crusade, civil, pope, plot, humanity, terror

.160 W2V–288 law, general, legal, safety, officer, emergency, agent

.120 L–POWER up, best, over, win, down, help, god, big, high, top

.103 L–RELIG god, hell, holy, soul, pray, angel, praying, christ, sin, amen

.100 L–ARTICLE the, a, an

.089 L–DEATH dead, die, died, war, alive, dying, wars, dies, buried, bury

.083 L–RISK bad, stop, wrong, worst, lose, trust, safe, worse, losing

.118 Emot–Fear watch, bad, god, hate, change, feeling, hell, crazy, bitch, die

.094 Emot–Disgust bad, hate, shit, finally, damn, feeling, hell, bitch, boy, sick

.086 Emot–Negative wait, bad, hate, shit, black, damn, ass, wrong, vote, feeling

.084 Emot–Sadness bad, hate, music, black, vote, feeling, hell, crazy, lost, bitch

.181→

.103
Political Terms

obama, liberal, president, government, senators, bernie,
law, #demdebate, same-sex, feminist, congress, republi-
cans, clinton, gay, #p2, iran, activists, bush, sanders, oba-
macare, terrorists, justice, debate, republican, #obamacare,
@moveon, @barackobama, #tcot, democrats, politics, ...

Table 1: Pearson correlations between political ideology groups and text features, split into Word2Vec clusters (W2V), LIWC
categories (L), emotions (Emot) and political terms (maximum 5 categories per group). All correlations are significant at
p < .01, two-tailed t-test and are controlled for age and gender. Words in a category are sorted by frequency in our data set.

employed by both sides. For example, conserva-
tives retweet or mention politicians such as Don-
ald Trump or Ted Cruz, while liberals mention

Barack Obama. Extreme conservatives also ref-
erence known partisan conservative media sources
(@foxnews, @yahoonews) and hashtags (#pjnet,

733



#tcot), while extreme liberals focus on issues
(‘gay’, ‘racism’, ‘feminism’, ‘transgender’). This
perhaps reflects the desire for conservatives on
Twitter to identify like-minded individuals, as ex-
treme conservatives are a minority on the plat-
form. Liberals, by contrast, use the platform to
discuss and popularize their causes.

5.2 Moderate Conservatives vs. Moderate
Liberals

Comparing the two sides of moderate users re-
veals a slightly more nuanced view of the two
ideologies. While moderate conservatives still
make heavy use of religious terms and express
positive emotions (Emot-Joy, L-DRIVES), they
also use affiliative language (L-AFFILIATION)
and plural pronouns (L-WE). Moderate liberals
are identified by very different features compared
to their more extreme counterparts. Most striking
is the use of swear and sex words (L-SEXUAL,
L-ANGER, W2V-316), also highlighted by Syl-
wester and Purver (2015). Two word clusters re-
lating to British culture (W2V-458) and art (W2V-
373) reflect that liberals are more inclined towards
arts (Dollinger, 2007). Statistically significant po-
litical terms are very few compared to the previ-
ous comparison, probably due to their lower over-
all usage, which we further investigate later.

5.3 Moderates vs. Extremists
Our final comparison looks at outright moder-
ates compared to the two extreme groups com-
bined, as we hypothesise the existence of a dif-
ference in overall political engagement. Moder-
ates are not characterized by many features be-
sides a topic of casual words (W2V-098), indicat-
ing the heterogeneity of this group of users. How-
ever, regardless of their orientation, the ideologi-
cal extremists stand out from moderates. They use
words and word clusters related to political actors
(W2V-309), issues (W2V-237) and laws (W2V-
296, W2V-288). LIWC analysis uncovers differ-
ences in article use (L-ARTICLE) or power words
(L-POWER) specific of political tweets. The over-
all sentiment of these users is negative (Emot-Fear,
Emot-Disgust, Emot-Sadness, L-DEATH) com-
pared to moderates. This reveals – combined with
the finding from the first comparison – that while
extreme conservatives are overall more positive
than liberals, both groups share negative expres-
sion. Political terms are almost all significantly
correlated with the extreme ideological groups,

2.64

0.76

0.55
0.42

0.36
0.46 0.51

0.76

2.95

0.73

0.24
0.14

0.07 0.07 0.09 0.12
0.19

0.79

0.11 0.03 0.03 0.02 0.02 0.03 0.03 0.04 0.18
0.00

0.50

1.00

1.50

2.00

2.50

3.00

D2: Con. V.Con.(1) Con.(2) M.Con.(3) Mod.(4) M.Lib.(5) Lib.(6) V.Lib.(7) D2: Lib.

Political words

Political NEs

Media NEs

Figure 3: Distribution of political word and entity usage
across political categories in % from the total words used.
Users from data set D2 who are following the accounts of the
four political figures are prefixed with D2. The rest of the
categories are from data set D1.

confirming the existence of a difference in polit-
ical engagement which we study in detail next.

5.4 Political Terms

Figure 3 presents the use of the three types of po-
litical terms across the 7 ideological groups in D1
and the two political groups from D2. We notice
the following:
• D2 has a huge skew towards political words,

with an average of more than three times more
political terms across all three categories than
our extreme classes from D1;
• Within the groups in D1, we observe an almost

perfectly symmetrical U-shape across all three
types of political terms, confirming our hypoth-
esis about political engagement;
• The difference between 1–2/6–7 is larger than

2–3/5–6. The extreme liberals and conserva-
tives are disproportionately political, and have
the potential to give Twitter’s political discus-
sions an unrepresentative, extremist hue (Fio-
rina, 1999). It is also possible, however, that
characterizing one as an extreme liberal or con-
servative indicates as much about her level of
political engagement as it does about her place-
ment on a left-right scale (Converse, 1964;
Broockman, 2016).

6 Prediction

In this section we build predictive models of po-
litical ideology and compare them to data sets ob-
tained using previous work.

734



6.1 Cross-Group Prediction

First, we experiment with classifying between
conservatives and liberals across various levels of
political engagement in D1 and between the two
polarized groups in D2. We use logistic regres-
sion classification to compare three setups in Ta-
ble 2 with results measured with ROC AUC as the
classes are slightly inbalanced:
• 10-fold cross-validation where training is per-

formed on the same task as the testing (principal
diagonal);
• A train–test setup where training is performed

on one task (presented in rows) and testing is
performed on another (presented in columns);
• A domain adaptation setup (results in brack-

ets) where on each of the 10 folds, the 9 train-
ing folds (presented in rows) are supplemented
with all the data from a different task (pre-
sented in columns) using the EasyAdapt algo-
rithm (Daumé III, 2007) as a proof on concept
on the effects of using additional distantly su-
pervised data. Data pooling lead to worse re-
sults than EasyAdapt.

Each of the three tasks from D1 have a similar
number of training samples, hence we do not ex-
pect that data set size has any effects in comparing
the results across tasks.

The results with both sets of features show that:
• Prediction performance is much higher for D2

than for D1, with the more extreme groups in
D1 being easier to predict than the moderate
groups. This confirms that the very high accu-
racies reported by previous research are an arti-
fact of user label collection and that on regular
users, the expected accuracy is much lower (Co-
hen and Ruths, 2013). We further show that, as
the level of political engagement decreases, the
classification problem becomes even harder;
• The model trained on D2 and Word2Vec word

clusters performs significantly worse on D1
tasks even if the training data is over 10 times
larger. When using political words, the D2
trained classifier performs relatively well on all
tasks from D1;
• Overall, using political words as features per-

forms better than Word2Vec clusters in the bi-
nary classification tasks;
• Domain adaptation helps in the majority of

cases, leading to improvements of up to .03 in
AUC (predicting 2v6 supplemented with 3v5
data).

Train Test1v7 2v6 3v5 D2
1v7 .785 .639 (.681) .575 (.598) .705 (.887)
2v6 .729 (.789) .662 .574 (.586) .663 (.889)
3v5 .618 (.778) .617 (.690) .581 .684 (.887)
D2 .708 (.764) .627 (.644) .571 (.574) .891

(a) Word2Vec 500

Train Test1v7 2v6 3v5 D2
1v7 .785 .657 (.679) .589 (.616) .928 (.976)
2v6 .739 (.773) .679 .593 (.612) .920 (.976)
3v5 .727 (.766) .636 (.670) .590 .891 (.976)
D2 .766 (.789) .677 (.683) .625 (.613) .972

(b) Political Terms

Table 2: Prediction results of the logistic regression classi-
fication in ROC AUC when discriminating between two po-
litical groups across different levels of engagement and both
data sets. The binary classifier from data set D2 is repre-
sented by D2, the rest of the categories are from data set
D1. Results on the principal diagonal represent 10-fold cross-
validation results (training in-domain). Results off-diagonal
represent training the classifier from the column and testing
on the problem indicated in the row (training out-of-domain).
Numbers in brackets indicate performance when the training
data was added in the 10-fold cross-validation setup using the
EasyAdapt algorithm (domain adaptation). Best results with-
out domain adaptation are in bold, while the best results with
domain adaptation are in italics.

6.2 Political Leaning and Engagement
Prediction

Political leaning (Conservative – Liberal, exclud-
ing the Moderate group) can be considered an or-
dinal variable and the prediction problem framed
as one of regression. In addition to the political
leaning prediction, based on analysis and previous
prediction results, we hypothesize the existence of
a separate dimension of political engagement re-
gardless of the partisan side. Thus, we merge users
from classes 3–5, 2–6, 1–7 and create a variable
with four values, where the lowest value is repre-
sented by moderate users (4) and the highest value
is represented by either very conservative (1) or
very liberal (7) users.

We use a linear regression algorithm with an
Elastic Net regularizer (Zou and Hastie, 2005)
as implemented in ScikitLearn (Pedregosa et al.,
2011). To evaluate our results, we split our
data into 10 stratified folds and performed cross-
validation on one held-out fold at a time. For all
our methods we tune the parameters of our models
on a separate validation fold. The overall perfor-
mance is assessed using Pearson correlation be-
tween the set of predicted values and the user-
reported score. Results are presented in Table 3.

735



The same patterns hold when evaluating the results
with Root Mean Squared Error (RMSE).

Features #Feat.
Political
Leaning

Political
Engagement

Unigrams 6060 .294 .165
LIWC 73 .286 .149
Word2Vec Clusters 500 .300 .169
Emotions 8 .145 .079
Political Terms 234 .256 .169
All (Ensemble) 5 .369 .196

Table 3: Pearson correlations between the predictions and
self-reported ideologies using linear regression with each fea-
ture category and a linear combination of their predictions in
a 10-fold cross-validation setup. Political leaning is repre-
sented on the 1–7 scale removing the moderates (4). Political
engagement is a scale ranging from 4 through 3–5 and 2–6 to
1–7.

The results show that both dimensions can
be predicted well above chance, with political
leaning being easier to predict than engagement.
Word2Vec clusters obtain the highest predictive
accuracy for political leaning, even though they
did not perform as well in the previous classifi-
cation tasks. For political engagement, political
terms and Word2Vec clusters obtain similar pre-
dictive accuracy. This result is expected based on
the results from Figure 3, which showed how po-
litical term usage varies across groups, and how
it is especially dependent on political engagement.
While political terms are very effective at distin-
guishing between two opposing political groups,
they can not discriminate as well between levels
of engagement within the same ideological orien-
tation. Combining all classifiers’ predictions in
a linear ensemble obtains best results when com-
pared to each individual category.

6.3 Encoding Class Structure
In our previous experiments, we uncovered that
certain relationships exist between the seven
groups. For example, extreme conservatives and
liberals both demonstrate strong political engage-
ment. Therefore, this class structure can be ex-
ploited to improve classification performance. To
this end, we deploy the sparse graph regularized
approach (Argyriou et al., 2007; Zhou et al., 2011)
to encode the structure of the seven classes as a
graph regularizer in a logistic regression frame-
work.

In particular, we employed a multi-task learn-
ing paradigm, where each task is a one-vs-all clas-
sification. Multi-task learning (MTL) is a learn-
ing paradigm that jointly learns multiple related

Method Accuracy
Baseline 19.6%

LR 22.2%
GR–Engagement 24.2%

GR–Leaning 26.2%
GR–Learnt 27.6%

Table 4: Experimental results for seven-way classification
using multi-task learning (GR–Engagement, GR–Leaning,
GR-Learnt) and 500 Word2Vec clusters as features.

tasks and can achieve better generalization per-
formance than learning each task individually, es-
pecially when presented with insufficient training
samples (Liu et al., 2015, 2016b,d). The group
structure is encoded into a matrix R which codes
the groups which are considered similar. The ob-
jective of the sparse graph regularized multi-task
learning problem is:

min
W,c

τ∑

t=1

N∑

i=1

log(1 + exp(−Yt,i(WTi,tXt,i + ct)))

+ γ‖WR‖2F + λ‖W‖1,

where τ is the number of tasks, |N | the number
of samples, X the feature matrix, Y the outcome
matrix, Wi,t and ct is the model for task t and R is
the structure matrix.

We define three R matrices: (1) codes that
groups with similar political engagement are sim-
ilar (i.e. 1–7, 2–6, 3–5); (2) codes that groups
from each ideological side are similar (i.e. 1–2,
1–3, 2–3, 5–6, 5–7, 6–7); (3) learnt from the data.
Results are presented in Table 4. Regular logistic
regression performs slightly better than the major-
ity class baseline, which demonstrates that the 7-
class classification is a very hard problem although
most miss-classifications are within one ideology
point. The graph regularization (GR) improves
the classification performance over logistic regres-
sion (LR) in all cases, with political leaning based
matrix (GR–Leaning) obtaining 2% in accuracy
higher than the political engagement one (GR–
Engagement) and the learnt matrix (GR–Learnt)
obtaining best results.

7 Conclusions

This study analyzed user-level political ideology
through Twitter posts. In contrast to previous
work, we made use of a novel data set where fine-
grained user political ideology labels are obtained
through surveys as opposed to binary self-reports.
We showed that users in our data set are far less

736



likely to post about politics and real-world fine-
grained political ideology prediction is harder and
more nuanced than previously reported. We ana-
lyzed language differences between the ideologi-
cal groups and uncovered a dimension of political
engagement separate from political leaning.

Our work has implications for pollsters or mar-
keters, who are most interested to identify and
persuade moderate users. With respect to polit-
ical conclusions, researchers commonly concep-
tualize ideology as a single, left-right dimension
similar to what we observe in the U.S. Congress
(Ansolabehere et al., 2008; Bafumi and Herron,
2010). Our results suggest a different direction:
self-reported political extremity is more an indi-
cation of political engagement than of ideological
self-placement (Abramowitz, 2010). In fact, only
self-reported extremists appear to devote much of
their Twitter activity to politics at all.

While our study focused solely on text posted
by the user, follow-up work can use other modal-
ities such as images or social network analysis
to improve prediction performance. In addition,
our work on user-level modeling can be integrated
with work on message-level political bias to study
how this is revealed across users with various lev-
els of engagement. Another direction of future
study will look at political ideology prediction in
other countries and cultures, where ideology has
different or multiple dimensions.

Acknowledgments

The authors acknowledge the support of the Tem-
pleton Religion Trust, grant TRT-0048. We wish
to thank Prof. David S. Rosenblum for supporting
the research visit of Ye Liu.

References

Alan I Abramowitz. 2010. The Disappearing Cen-
ter: Engaged Citizens, Polarization, and American
Democracy. Yale University Press.

Firoj Alam and Giuseppe Riccardi. 2014. Predicting
Personality Traits using Multimodal Information. In
Workshop on Computational Personality Recogni-
tion (WCPR). MM, pages 15–18.

Stephen Ansolabehere, Jonathan Rodden, and James M
Snyder. 2008. The strength of issues: Using multi-
ple measures to gauge preference stability, ideologi-
cal constraint, and issue voting. American Political
Science Review 102(02):215–232.

Andreas Argyriou, Theodoros Evgeniou, and Massi-
miliano Pontil. 2007. Multi-task Feature Learning.
In Advances in Neural Information Processing Sys-
tems. NIPS, pages 41–49.

Joseph Bafumi and Michael C Herron. 2010. Leapfrog
Representation and Extremism: A Study of Ameri-
can Voters and their Members in Congress. Ameri-
can Political Science Review 104(03):519–542.

Pablo BarberASa. 2015. Birds of the Same Feather
Tweet Together: Bayesian Ideal Point Estimation us-
ing Twitter Data. Political Analysis 23(1):76–91.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research 3:993–1022.

David E Broockman. 2016. Approaches to Studying
Policy Representation. Legislative Studies Quar-
terly 41(1):181–215.

D. John Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating Gender on
Twitter. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing. EMNLP, pages 1301–1309.

Jordan Carpenter, Daniel Preoţiuc-Pietro, Lucie
Flekova, Salvatore Giorgi, Courtney Hagan, Mar-
garet Kern, Anneke Buffone, Lyle Ungar, and Mar-
tin Seligman. 2016. Real Men don’t say ’Cute’: Us-
ing Automatic Language Analysis to Isolate Inaccu-
rate Aspects of Stereotypes. Social Psychological
and Personality Science .

Zhiyuan Cheng, James Caverlee, and Kyumin Lee.
2010. You are where you Tweet: A Content-Based
Approach to Geo-Locating Twitter Users. In Pro-
ceedings of the 19th ACM Conference on Infor-
mation and Knowledge Management. CIKM, pages
759–768.

Raviv Cohen and Derek Ruths. 2013. Classifying Po-
litical Orientation on Twitter: It’s Not Easy! In Pro-
ceedings of the Seventh International AAAI Confer-
ence on Weblogs and Social Media. ICWSM, pages
91–99.

Michael D Conover, Bruno Gonçalves, Jacob
Ratkiewicz, Alessandro Flammini, and Filippo
Menczer. 2011. Predicting the Political Alignment
of Twitter Users. In IEEE Third International
Conference on Privacy, Security, Risk and Trust
(PASSAT) and the IEEE Third Inernational Con-
ference on Social Computing (SocialCom). pages
192–199.

Philip E Converse. 1964. The Nature of Belief Systems
in Mass Publics. In David Apter, editor, Ideology
and Discontent, Free Press, New York.

Hal Daumé III. 2007. Frustratingly Easy Domain
Adaptation. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics. ACL, pages 256–263.

737



Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society for Information Science
41(6):391–407.

Stephen J Dollinger. 2007. Creativity and Conser-
vatism. Personality and Individual Differences
43(5):1025–1035.

Paul Ekman. 1992. An Argument for Basic Emotions.
Cognition & Emotion 6(3-4):169–200.

Christopher Ellis and James A Stimson. 2012. Ideol-
ogy in America. Cambridge University Press.

Morris P Fiorina. 1999. Extreme Voices: A Dark
Side of Civic Engagement. In Morris P. Fiorina and
Theda Skocpol, editors, Civic engagement in Amer-
ican democracy, Washington, DC: Brookings Insti-
tution Press, pages 405–413.

Lucie Flekova, Jordan Carpenter, Salvatore Giorgi,
Lyle Ungar, and Daniel Preoţiuc-Pietro. 2016a. An-
alyzing Biases in Human Perception of User Age
and Gender from Text. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics. ACL, pages 843–854.

Lucie Flekova, Lyle Ungar, and Daniel Preoctiuc-
Pietro. 2016b. Exploring Stylistic Variation with
Age and Income on Twitter. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics. ACL, pages 313–319.

Andrew Gelman. 2009. Red State, Blue State, Rich
State, Poor State: Why Americans Vote the Way they
Do. Princeton University Press.

Alan S Gerber, Gregory A Huber, David Doherty,
Conor M Dowling, and Shang E Ha. 2010. Person-
ality and Political Attitudes: Relationships across Is-
sue Domains and Political Contexts. American Po-
litical Science Review 104(01):111–133.

Jeffrey A Hall, Natalie Pennington, and Allyn Lueders.
2014. Impression Management and Formation on
Facebook: A Lens Model Approach. New Media &
Society 16(6):958–982.

Z. Harris. 1954. Distributional Structure. Word
10(23):146 – 162.

Eitan D Hersh. 2015. Hacking the Electorate: How
Campaigns Perceive Voters. Cambridge University
Press.

Mohit Iyyer, Peter Enns, Jordan Boyd-Graber, and
Philip Resnik. 2014. Political Ideology Detection
using Recursive Neural Networks. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics. ACL, pages 1113–1122.

Cindy D Kam, Jennifer R Wilking, and Elizabeth J
Zechmeister. 2007. Beyond the Narrow Data base:
Another Convenience Sample for Experimental Re-
search. Political Behavior 29(4):415–440.

Michal Kosinski, David Stillwell, and Thore Graepel.
2013. Private Traits and Attributes are Predictable
from Digital Records of Human Behavior. PNAS
110(15):5802–5805.

George Lakoff. 1997. Moral Politics: What Conser-
vatives Know that Liberals Don’t. University of
Chicago Press.

Vasileios Lampos, Nikolaos Aletras, Daniel Preoţiuc-
Pietro, and Trevor Cohn. 2014. Predicting and Char-
acterising User Impact on Twitter. In Proceedings of
the 14th Conference of the European Chapter of the
Association for Computational Linguistics. EACL,
pages 405–413.

Randall A Lewis and David H Reiley. 2014. On-
line Ads and Offline Sales: Measuring the Effect
of Retail Advertising via a Controlled Experiment
on Yahoo! Quantitative Marketing and Economics
12(3):235–266.

Leqi Liu, Daniel Preoţiuc-Pietro, Zahra Riahi Samani,
Mohsen E. Moghaddam, and Lyle Ungar. 2016a.
Analyzing Personality through Social Media Profile
Picture Choice. In Proceedings of the Tenth Inter-
national AAAI Conference on Weblogs and Social
Media. ICWSM, pages 211–220.

Ye Liu, Liqiang Nie, Lei Han, Luming Zhang, and
David S Rosenblum. 2015. Action2Activity: Rec-
ognizing Complex Activities from Sensor Data. In
Proceedings of the International Joint Conference
on Artificial Intelligence. IJCAI, pages 1617–1623.

Ye Liu, Liqiang Nie, Li Liu, and David S Rosenblum.
2016b. From Action to Activity: Sensor-based Ac-
tivity Recognition. Neurocomputing 181:108–115.

Ye Liu, Luming Zhang, Liqiang Nie, Yan Yan, and
David S Rosenblum. 2016c. Fortune Teller: Predict-
ing your Career Path. In Proceedings of the AAAI
Conference on Artificial Intelligence. AAAI, pages
201–207.

Ye Liu, Yu Zheng, Yuxuan Liang, Shuming Liu, and
David S. Rosenblum. 2016d. Urban Water Quality
Prediction Based on Multi-task Multi-view Learn-
ing. In Proceedings of the International Joint
Conference on Artificial Intelligence. IJCAI, pages
2576–2582.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013a. Distributed Repre-
sentations of Words and Phrases and their Compo-
sitionality. In Advances in Neural Information Pro-
cessing Systems. NIPS, pages 3111–3119.

Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013b. Linguistic Regularities in Continuous Space
Word Representations. In Proceedings of the 2010
annual Conference of the North American Chap-
ter of the Association for Computational Linguistics.
NAACL, pages 746–751.

738



Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions Evoked by Common Words and Phrases: Us-
ing Mechanical Turk to Create an Emotion Lexicon.
In Proceedings of the Workshop on Computational
Approaches to Analysis and Generation of Emotion
in Text. NAACL, pages 26–34.

Saif M. Mohammad and Peter D. Turney. 2013.
Crowdsourcing a Word-Emotion Association Lexi-
con. Computational Intelligence 29(3):436–465.

Burt L Monroe, Michael P Colaresi, and Kevin M
Quinn. 2008. Fightin’ Words: Lexical Feature Se-
lection and Evaluation for Identifying the Content of
Political Conflict. Political Analysis 16(4):372–403.

Jaime L Napier and John T Jost. 2008. Why are Con-
servatives Happier than Liberals? Psychological
Science 19(6):565–572.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al. 2011. Scikit-learn:
Machine Learning in Python. JMLR 12.

Marco Pennacchiotti and Ana-Maria Popescu. 2011. A
Machine Learning Approach to Twitter User Classi-
fication. In Proceedings of the Fifth International
AAAI Conference on Weblogs and Social Media.
ICWSM, pages 281–288.

James W. Pennebaker, Martha E. Francis, and Roger J.
Booth. 2001. Linguistic Inquiry and Word Count.
Mahway: Lawrence Erlbaum Associates.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global Vectors for
Word Representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing. EMNLP, pages 1532–1543.

Bryan Perozzi and Steven Skiena. 2015. Exact Age
Prediction in Social Networks. In Proceedings of
the 24th International Conference on World Wide
Web. WWW, pages 91–92.

Daniel Preoţiuc-Pietro, Jordan Carpenter, Salvatore
Giorgi, and Lyle Ungar. 2016. Studying the Dark
Triad of Personality using Twitter Behavior. In
Proceedings of the 25th ACM Conference on Infor-
mation and Knowledge Management. CIKM, pages
761–770.

Daniel Preoţiuc-Pietro, Vasileios Lampos, and Niko-
laos Aletras. 2015a. An Analysis of the User Oc-
cupational Class through Twitter Content. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing. ACL, pages 1754–1764.

Daniel Preoţiuc-Pietro, Svitlana Volkova, Vasileios
Lampos, Yoram Bachrach, and Nikolaos Aletras.
2015b. Studying User Income through Language,
Behaviour and Affect in Social Media. PLoS ONE .

Anna Priante, Djoerd Hiemstra, Tijs van den Broek,
Aaqib Saeed, Michel Ehrenhard, and Ariana Need.
2016. #WhoAmI in 160 Characters? Classifying
Social Identities Based on Twitter. In Proceedings
of the Workshop on Natural Language Processing
and Computational Social Science. EMNLP, pages
55–65.

Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying Latent User At-
tributes in Twitter. In Proceedings of the 2nd In-
ternational Workshop on Search and Mining User-
generated Contents. SMUC, pages 37–44.

Dominic Rout, Daniel Preoţiuc-Pietro, Bontcheva
Kalina, and Trevor Cohn. 2013. Where’s @wally: A
Classification Approach to Geolocating Users based
on their Social Ties. In Proceedings of the 24th
ACM Conference on Hypertext and Social Media.
HT, pages 11–20.

Maarten Sap, Gregory Park, Johannes C. Eichstaedt,
Margaret L. Kern, David J. Stillwell, Michal Kosin-
ski, Lyle H. Ungar, and Hansen Andrew Schwartz.
2014. Developing Age and Gender Predictive Lex-
ica over Social Media. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing. EMNLP, pages 1146–1151.

H Andrew Schwartz, Johannes C Eichstaedt, Mar-
garet L Kern, Lukasz Dziurzynski, Stephanie M Ra-
mones, Megha Agrawal, Achal Shah, Michal Kosin-
ski, David Stillwell, and Martin EP Seligman. 2013.
Personality, Gender, and Age in the Language of So-
cial Media: The Open-vocabulary Approach. PloS
ONE 8(9).

Jianbo Shi and Jitendra Malik. 2000. Normalized Cuts
and Image Segmentation. Transactions on Pattern
Analysis and Machine Intelligence 22(8):888–905.

Carlo Strapparava and Rada Mihalcea. 2008. Learn-
ing to Identify Emotions in Text. In Proceedings of
the 2008 ACM Symposium on Applied Computing.
pages 1556–1560.

Carlo Strapparava, Alessandro Valitutti, et al. 2004.
WordNet Affect: an Affective Extension of Word-
Net. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation.
volume 4 of LREC, pages 1083–1086.

Ramanathan Subramanian, Yan Yan, Jacopo Staiano,
Oswald Lanz, and Nicu Sebe. 2013. On the Rela-
tionship between Head Pose, Social Attention and
Personality Prediction for Unstructured and Dy-
namic Group Interactions. In Proceedings of the
15th ACM on International Conference on Multi-
modal Interaction. ICMI, pages 3–10.

Karolina Sylwester and Matthew Purver. 2015. Twitter
Language Use Reflects Psychological Differences
between Democrats and Republicans. PLoS ONE
10(9).

739



Brandon Van Der Heide, Jonathan D D’Angelo, and
Erin M Schumaker. 2012. The Effects of Verbal ver-
sus Photographic Self-presentation on Impression
Formation in Facebook. Journal of Communication
62(1):98–116.

Svitlana Volkova, Glen Coppersmith, and Benjamin
Van Durme. 2014. Inferring User Political Pref-
erences from Streaming Communications. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics. ACL, pages
186–196.

Ulrike von Luxburg. 2007. A Tutorial on Spectral
Clustering. Statistics and Computing 17(4):395–
416.

Pengfei Wang, Jiafeng Guo, Yanyan Lan, Jun Xu, and
Xueqi Cheng. 2016. Your Cart tells You: Infer-
ring Demographic Attributes from Purchase Data.
In Proceedings of the Ninth ACM International Con-
ference on Web Search and Data Mining. WSDM,
pages 173–182.

Ingmar Weber, Venkata Rama Kiran Garimella, and
Asmelash Teka. 2013. Political Hashtag Trends.
In European Conference on Information Retrieval.
ECIR, pages 857–860.

Tae Yano, Philip Resnik, and Noah A Smith. 2010.
Shedding (a Thousand Points of) Light on Biased
Language. In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon’s Mechanical Turk. NAACL, pages
152–158.

Muhammad Bilal Zafar, Krishna P Gummadi, and
Cristian Danescu-Niculescu-Mizil. 2016. Message
Impartiality in Social Media Discussions. In Pro-
ceedings of the Tenth International AAAI Confer-
ence on Weblogs and Social Media. ICWSM, pages
466–475.

Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and Latent Attribute Inference: Infer-
ring Latent Attributes of Twitter Users from Neigh-
bors. In Proceedings of the Sixth International AAAI
Conference on Weblogs and Social Media. ICWSM,
pages 387–390.

Jiayu Zhou, Jianhui Chen, and Jieping Ye. 2011. MAL-
SAR: Multi-Task Learning via Structural Regular-
ization. Arizona State University .

Hui Zou and Trevor Hastie. 2005. Regularization and
Variable Selection via the Elastic Net. Journal of the
Royal Statistical Society, Series B .

740


	Beyond Binary Labels: Political Ideology Prediction of Twitter Users

