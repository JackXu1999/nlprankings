










































Distant Supervision for Relation Extraction with an Incomplete Knowledge Base


Proceedings of NAACL-HLT 2013, pages 777–782,
Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics

Distant Supervision for Relation Extraction

with an Incomplete Knowledge Base

Bonan Min, Ralph Grishman, Li Wan

New York University

New York, NY 10003

{min,grishman,wanli}
@cs.nyu.edu

Chang Wang, David Gondek

IBM T. J. Watson Research Center

Yorktown Heights, NY 10598

{wangchan,dgondek}
@us.ibm.com

Abstract

Distant supervision, heuristically labeling a

corpus using a knowledge base, has emerged

as a popular choice for training relation ex-

tractors. In this paper, we show that a sig-

nificant number of “negative“ examples gen-

erated by the labeling process are false neg-

atives because the knowledge base is incom-

plete. Therefore the heuristic for generating

negative examples has a serious flaw. Building

on a state-of-the-art distantly-supervised ex-

traction algorithm, we proposed an algorithm

that learns from only positive and unlabeled

labels at the pair-of-entity level. Experimental

results demonstrate its advantage over existing

algorithms.

1 Introduction

Relation Extraction is a well-studied problem

(Miller et al., 2000; Zhou et al., 2005; Kambhatla,

2004; Min et al., 2012a). Recently, Distant Super-

vision (DS) (Craven and Kumlien, 1999; Mintz et

al., 2009) has emerged to be a popular choice for

training relation extractors without using manually

labeled data. It automatically generates training ex-

amples by labeling relation mentions1 in the source

corpus according to whether the argument pair is

listed in the target relational tables in a knowledge

base (KB). This method significantly reduces human

efforts for relation extraction.

The labeling heuristic has a serious flaw. Knowl-

edge bases are usually highly incomplete. For exam-

1An occurrence of a pair of entities with the source sentence.

ple, 93.8% of persons from Freebase2 have no place

of birth, and 78.5% have no nationality (section 3).

Previous work typically assumes that if the argument

entity pair is not listed in the KB as having a re-

lation, all the corresponding relation mentions are

considered negative examples.3 This crude assump-

tion labeled many entity pairs as negative when in

fact some of their mentions express a relation. The

number of such false negative matches even exceeds

the number of positive pairs, by 3 to 10 times, lead-

ing to a significant problem for training. Previous

approaches (Riedel et al., 2010; Hoffmann et al.,

2011; Surdeanu et al., 2012) bypassed this problem

by heavily under-sampling the “negative“ class.

We instead deal with a learning scenario where we

only have entity-pair level labels that are either posi-

tive or unlabeled. We proposed an extension to Sur-

deanu et al. (2012) that can train on this dataset. Our

contribution also includes an analysis on the incom-

pleteness of Freebase and the false negative match

rate in two datasets of labeled examples generated

by DS. Experimental results on a realistic and chal-

lenging dataset demonstrate the advantage of the al-

gorithm over existing solutions.

2 Related Work

Distant supervision was first proposed by Craven

and Kumlien (1999) in the biomedical domain.

2Freebase is a large collaboratively-edited KB. It is available

at http://www.freebase.com.
3There are variants of labeling heuristics. For example, Sur-

deanu et al. (2011) and Sun et al. (2011) use a pair < e, v >

as a negative example, when it is not listed in Freebase, but e is

listed with a different v′. These assumptions are also problem-

atic in cases where the relation is not functional.

777



Since then, it has gain popularity (Mintz et al., 2009;

Bunescu and Mooney, 2007; Wu and Weld, 2007;

Riedel et al., 2010; Hoffmann et al., 2011; Sur-

deanu et al., 2012; Nguyen and Moschitti, 2011).

To tolerate noisy labels in positive examples, Riedel

et al. (2010) use Multiple Instance Learning (MIL),

which assumes only at-least-one of the relation men-

tions in each “bag“ of mentions sharing a pair of ar-

gument entities which bears a relation, indeed ex-

presses the target relation. MultiR (Hoffmann et

al., 2011) and Multi-Instance Multi-Label (MIML)

learning (Surdeanu et al., 2012) further improve it

to support multiple relations expressed by different

sentences in a bag. Takamatsu et al. (2012) mod-

els the probabilities of a pattern showing relations,

estimated from the heuristically labeled dataset.

Their algorithm removes mentions that match low-

probability patterns. Sun et al. (2011) and Min et

al. (2012b) also estimate the probablities of patterns

showing relations, but instead use them to relabel ex-

amples to their most likely classes. Their approach

can correct highly-confident false negative matches.

3 Problem Definition

Distant Supervision: Given a KB D (a collection
of relational tables r(e1, e2), in which rǫR (R is the
set of relation labels), and < e1, e2 > is a pair of
entities that is known to have relation r) and a cor-
pus C, the key idea of distant supervision is that we
align D to C, label each bag4 of relation mentions
that share argument pair < e1, e2 > with r, other-
wise OTHER. This generates a dataset that has labels

on entity-pair (bag) level. Then a relation extractor

is trained with single-instance learning (by assum-

ing all mentions have the same label as the bag), or

Multiple-Instance Learning (by assuming at-least-

one of the mentions expresses the bag-level label),

or Multi-Instance Multi-Label learning (further as-

suming a bag can have multiple labels) algorithms.

All of these works treat the OTHER class as exam-

ples that are labeled as negative.

The incomplete KB problem: KBs are usually

incomplete because they are manually constructed,

and it is not possible to cover all human knowledge

4A bag is defined as a set of relation mentions sharing the

same entity pair as relation arguments. We will use the terms

bag and entity pair interchangeably in this paper.

nor stay current. We took frequent relations, which

involve an entity of type PERSON, from Freebase

for analysis. We define the incompleteness ∂(r) of a
relation r as follows:

∂(r) = |{e}|−|{e|∃e
′,s.t.r(e,e′)ǫD}|
|{e}|

∂(r) is the percentage of all persons {e} that do
not have an attribute e′ (with which r(e, e′) holds).
Table 1 shows that 93.8% of persons have no place

of birth, and 78.5% of them have no nationality.

These are must-have attributes for a person. This

shows that Freebase is highly incomplete.
Freebase relation types Incompleteness

/people/person/education 0.792

/people/person/employment history 0.923

/people/person/nationality* 0.785

/people/person/parents* 0.988

/people/person/place of birth* 0.938

/people/person/places lived* 0.966

Table 1: The incompleteness of Freebase (* are must-

have attributes for a person).

We further investigate the rate of false negative

matches, as the percentage of entity-pairs that are

not listed in Freebase but one of its mentions gen-

erated by DS does express a relation in the tar-

get set of types. We randomly picked 200 unla-

beled bags5 from each of the two datasets (Riedel

et al., 2010; Surdeanu et al., 2012) generated by DS,

and we manually annotate all relation mentions in

these bags. The result is shown in Table 2, along

with a few examples that indicate a relation holds in

the set of false negative matches (bag-level). Both

datasets have around 10% false negative matches in

the unlabeled set of bags. Taking into considera-

tion that the number of positive bags and unlabeled

bags are highly imbalanced (1:134 and 1:37 in the

Riedel and KBP dataset respectively, before under-

sampling the unlabeled class), the number of false

negative matches are 11 and 4 times the number

of positive bags in Reidel and KBP dataset, respec-

tively. Such a large ratio shows false negatives do

have a significant impact on the learning process.

4 A semi-supervised MIML algorithm

Our goal is to model the bag-level label noise,

caused by the incomplete KB problem, in addition

585% and 95.7% of the bags in the Riedel and KBP datasets

have only one relation mention.

778



Dataset

(train-

ing)

# pos-

itive

bags

# positive :

# unlabeled

% are

false

negatives

# positive

: # false

negative

has human

assessment

Examples of false negative mentions

Riedel 4,700 1:134(BD*) 8.5% 1:11.4 no
(/location/location/contains)... in Brooklyn ’s Williamsburg.

(/people/person/place lived) Cheryl Rogowski , a farmer from

Orange County ...

KBP 183,062 1:37(BD*) 11.5% 1:4 yes
(per:city of birth) Juan Martn Maldacena (born September

10, 1968) is a theoretical physicist born in Buenos Aires

(per:employee of)Dave Matthews, from the ABC News, ...

Table 2: False negative matches on the Riedel (Riedel et al., 2010) and KBP dataset (Surdeanu et al., 2012). All

numbers are on bag (pairs of entities) level. BD* are the numbers before downsampling the negative set to 10% and

5% in Riedel and KBP dataset, respectively.

to modeling the instance-level noise using a 3-layer

MIL or MIML model (e.g., Surdeanu et al. (2012)).

We propose a 4-layer model as shown in Figure 1.

The input to the model is a list of n bags with a
vector of binary labels, either Positive (P), or Un-

labled (U) for each relation r. Our model can be
viewed as a semi-supervised6 framework that ex-

tends a state-of-the-art Multi-Instance Multi-Label

(MIML) model (Surdeanu et al., 2012). Since the

input to previous MIML models are bags with per-

relation binary labels of either Positive (P) or Neg-

ative (N), we add a set of latent variables ℓ which
models the true bag-level labels, to bridge the ob-

served bag labels y and the MIML layers. We con-

sider this as our main contribution to the model. Our

hierarchical model is shown in Figure 1.

Figure 1: Plate diagram of our model.

Let i, j be the index in the bag and mention level,
respectively. Following Surdeanu et al. (2012), we

model mention-level extraction p(zrij |xij ;wz) and
multi-instance multi-label aggregation p(ℓri |zi;w

r
ℓ)

in the bottom 3 layers. We define:

• r is a relation label. rǫR ∪ {OTHER}, in
which OTHER denotes no relation expressed.

• yri ǫ{P, U}: r holds for ith bag or the bag is
unlabeled.

6We use the term semi-supervised because the algorithm

uses unlabeled bags but existing solutions requires bags to be

labeled either positive or negative.

• ℓri ǫ{P, N}: a hidden variable that denotes
whether r holds for the ith bag.
• θ is an observed constant controlling the total

number of bags whose latent label is positive.

We define the following conditional probabilities:

• p(yri |ℓ
r
i ) =















1/2 if yri = P ∧ ℓ
r
i = P ;

1/2 if yri = U ∧ ℓ
r
i = P ;

1 if yri = U ∧ ℓ
r
i = N ;

0 otherwise ;
It encodes the constraints between true bag-

level labels and the entity pair labels in the KB.

• p(θ|ℓ) ∼ N (
∑n

i=1

∑
rǫR δ(ℓ

r
i ,P )

n
, 1

k
) where

δ(x, y) = 1 if x = y, 0 otherwise. k is a large
number. θ is the fraction of the bags that are
positive. It is an observed parameter that de-

pends on both the source corpus and the KB

used.

Similar to Surdeanu et al. (2012), we also define

the following parameters and conditional probabili-

ties (details are in Surdeanu et al. (2012)):

• zijǫR ∪ {OTHER}: a latent variable that de-
notes the relation type of the jth mention in the
ith bag.
• xij is the feature representation of the jth rela-

tion mention in the ith bag. We use the set of
features in Surdeanu et al. (2012).

• wz is the weight vector for the multi-class rela-
tion mention-level classifier.

• wrℓ is the weight vector for the rth binary top-
level aggregation classifier (from mention la-

bels to bag-level prediction). We use wℓ to rep-

resent w1ℓ ,w
2
ℓ , ...w

|R|
ℓ .

• p(ℓri |zi;w
r
ℓ) ∼ Bern(fℓ(w

r
ℓ , zi)) where fℓ is

probability produced by the rth top-level clas-
sifier, from the mention-label level to the bag-

label level.

• p(zrij |xij ;wz) ∼ Multi(fz(wz,xij)) where fz

779



is probability produced by the mention-level

classifier, from the mentions to the mention-

label level.7

4.1 Training

We use hard Expectation-Maximization (EM) algo-

rithm for training the model. Our objective function

is to maximize log-likelihood:

L(wz,wℓ) = logp(y, θ|x;wz,wℓ)

= log
∑

ℓ

p(y, θ, ℓ|x;wz,wℓ)

Since solving it exactly involves exploring an expo-

nential assignment space for ℓ, we approximate and
iteratively set ℓ∗ = argℓ max p(ℓ|y, θ,x;wz,wℓ)

p(ℓ|y, θ,x;wz,wℓ) ∝ p(y, θ, ℓ|x;wz,wℓ)

= p(y, θ|ℓ,x)p(ℓ|x;wz,wℓ)

= p(y|ℓ)p(θ|ℓ)p(ℓ|x;wz,wℓ)
Rewriting in log form:

logp(ℓ|y, θ,x;wz,wℓ)

= logp(y|ℓ) + logp(θ|ℓ) + logp(ℓ|x;wz,wℓ)

=
n

∑

i=1

∑

rǫR

logp(yri |ℓ
r
i )− k(

n
∑

i=1

∑

rǫR

δ(ℓri , P )

n
− θ)2

+
n

∑

i=1

∑

rǫR

logp(ℓri |xi;wz,wℓ) + const

Algorithm 1 Training (E-step:2-11; M-step:12-15)

1: for i = 1, 2 to T do
2: ℓri ← N for all y

r
i = U and rǫR

3: ℓri ← P for all y
r
i = P and rǫR

4: I = {< i, r > |ℓri = N}; I
′ = {< i, r > |ℓri = P}

5: for k = 0, 1 to θn− |I ′| do
6: < i′, r′ >= argmax<i,r>ǫI p(ℓ

r
i |xi;wz,wℓ)

7: ℓr
′

i′ ← P ; I = I\{< i
′, r′ >}

8: end for

9: for i = 1, 2 to n do
10: z∗i = argmaxzi p(zi|ℓi,xi;wz,wℓ)
11: end for

12: w∗z = argmaxwz
∑n

i=1

∑|xi|
j=1 logp(zij |xij ,wz)

13: for all rǫR do
14: w

r(∗)
ℓ = argmaxwrℓ

∑n
i=1 p(ℓ

r
i |zi,w

r
ℓ)

15: end for

16: end for

17: return wz,wℓ

7All classifiers are implemented with L2-regularized logistic

regression with Stanford CoreNLP package.

In the E-step, we do a greedy search (steps 5-8

in algorithm 1) in all p(ℓri |xi;wz,wℓ) and update ℓ
r
i

until the second term is maximized. wz , wℓ are the

model weights learned from the previous iteration.

After fixed ℓ, we seek to maximize:

logp(ℓ|xi;wz,wℓ) =
n

∑

i=1

logp(ℓi|xi;wz,wℓ)

=
n

∑

i=1

log
∑

zi

p(ℓi, zi|xi;wz,wℓ)

which can be solved with an approxi-

mate solution in Surdeanu et al. (2012)

(step 9-11): update zi independently with:

z
∗
i = argmaxzi p(zi|ℓi,xi;wz,wℓ). More details

can be found in Surdeanu et al. (2012).

In the M-step, we retrain both of the mention-

level and the aggregation level classifiers.

The full EM algorithm is shown in algorithm 1.

4.2 Inference

Inference on a bag xi is trivial. For each mention:

z∗ij = argzijǫR∪{OTHER}max p(zij |xij ,wz)
Followed by the aggregation (directly with wℓ):

y
r(∗)
i = argyri ǫ{P,N}max p(y

r
i |zi;w

r
ℓ)

4.3 Implementation details

We implement our model on top of the

MIML(Surdeanu et al., 2012) code base.8 We

use the same mention-level and aggregate-level

feature sets as Surdeanu et al. (2012). We adopt

the same idea of using cross validation for the E

and M steps to avoid overfitting. We initialize our

algorithm by sampling 5% unlabeled examples as

negative, in essence using 1 epoch of MIML to

initialize. Empirically it performs well.

5 Experiments

Data set: We use the KBP (Ji et al., 2011)

dataset9 prepared and publicly released by Surdeanu

et al. (2012) for our experiment since it is 1) large

and realistic, 2) publicly available, 3) most im-

portantly, it is the only dataset that has associated

human-labeled ground truth. Any KB held-out eval-

uation without manual assessment will be signif-

icantly affected by KB incompleteness. In KBP

8Available at http://nlp.stanford.edu/software/mimlre.shtml
9Available from Linguistic Data Consortium (LDC).

http://projects.ldc.upenn.edu/kbp/data/

780



Figure 2: Performance on the KBP dataset. The figures on the left, middle and right show MIML, Hoffmann, and

Mintz++ compared to the same MIML-Semi curve, respectively. MIML-Semi is shown in red curves (lighter curves in

black and white) while other algorithms are shown in black curves (darker curves in black and white).

dataset, the training bags are generated by mapping

Wikipedia (http://en.wikipedia.org) infoboxes (after

merging similar types following the KBP 2011 task

definition) into a large unlabeled corpus (consisting

of 1.5M documents from the KBP source corpus and

a complete snapshot of Wikipedia). The KBP shared

task provided 200 query named entities with their as-

sociated slot values (in total several thousand pairs).

We use 40 queries as development dataset (dev), and

the rest (160 queries) as evaluation dataset. We set

θ = 0.25 by tuning on the dev set and use it in the
experiments. For a fair comparison, we follow Sur-

deanu et al. (2012) and begin by downsampling the

“negative“ class to 5%. We also set T=8 and use

the following noisy-or (for ith bag) of mention-level
probability to rank predicted types (r) of pairs and
plot the precision-recall curves for all experiments.

Probi(r) = 1−
∏

j

(1− p(zij = r|xij ;wz))

Evaluation: We compare our algorithm (MIML-

semi) to three algorithms: 1) MIML (Surdeanu et

al., 2012), the Multiple-Instance Multiple Label al-

gorithm which labels the bags directly with the KB

(y = ℓ). 2) MultiR (denoted as Hoffmann) (Hoff-
mann et al., 2011), a Multiple-Instance algorithm

that supports overlapping relations. It also imposes

y = ℓ. 3) Mintz++ (Surdeanu et al., 2012), a vari-
ant of the single-instance learning algorithm (section

3). The first two are stat-of-the-art Multi-Instance

Multi-Label algorithms. Mintz++ is a strong base-

line (Surdeanu et al., 2012) and an improved ver-

sion of Mintz et al. (2009). Figure 2 shows that

our algorithm consistently outperforms all three al-

gorithms at almost all recall levels (with the excep-

tion of a very small region in the PR-curve). This

demonstrates that by treating unla-beled data set dif-

ferently and leveraging the missing positive bags,

MIML-semi is able to learn a more accurate model

for extraction. Although the proposed solution is a

specific algorithm, we believe the idea of treating

unlabeled data differently can be incorporated into

any of these algorithms that only use unlabeled data

as negative examples.

6 Conclusion

We show that the distant-supervision labeling pro-

cess generates a significant number of false nega-

tives because the knowledge base is incomplete. We

proposed an algorithm that learns from only positive

and unlabeled bags. Experimental results demon-

strate its advantage over existing algorithms.

Acknowledgments

Supported in part by the Intelligence Advanced Re-

search Projects Activity (IARPA) via Department of

Interior National Business Center contract number

D11PC20154. The U.S. Government is authorized

to reproduce and distribute reprints for Governmen-

tal purposes notwithstanding any copyright annota-

tion thereon. The views and conclusions contained

herein are those of the authors and should not be

interpreted as necessarily representing the official

policies or endorsements, either expressed or im-

plied, of IARPA, DoI/NBC, or the U.S. Government.

781



References

Razvan Bunescu and Raymond Mooney. 2007. Learning

to extract relations from the web using minimal super-

vision. In Proceedings of the 45th Annual Meeting of

the Association for Computational Linguistics.

Mark Craven and Johan Kumlien. 1999. Constructing bi-

ological knowledge bases by extracting information

from text sources. In Proceedings of the Seventh Inter-

national Conference on Intelligent Systems for Molec-

ular Biology.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke

Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-

based weak supervision for information extraction of

overlapping relations. In Proceedings of the Annual

Meeting of the Association for Computational Linguis-

tics.

Heng Ji, Ralph Grishman, and Hoa T. Dang. 2011.

Overview of the TAC 2011 knowledge base popula-

tion track. In Proceedings of the Text Analytics Con-

ference.

Jing Jiang and ChengXiang Zhai. 2007. A systematic ex-

ploration of the feature space for relation extraction. In

Proceedings of HLT-NAACL-2007.

Nanda Kambhatla. 2004. Combining lexical, syntactic,

and semantic features with maximum entropy mod-

els for information extraction. In Proceedings of ACL-

2004.

Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph

Weischedel. 2000. A novel use of statistical parsing

to extract information from text. In Proceedings of

NAACL-2000.

Bonan Min, Shuming Shi, Ralph Grishman and Chin-

Yew Lin. 2012a. Ensemble Semantics for Large-scale

Unsupervised Relation Extraction. In Proceedings of

EMNLP-CoNLL 2012.

Bonan Min, Xiang Li, Ralph Grishman and Ang Sun.

2012b. New York University 2012 System for KBP

Slot Filling. In Proceedings of the Text Analysis Con-

ference (TAC) 2012.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-

sky. 2009. Distant supervision for relation extraction

without labeled data. In Proceedings of the 47th An-

nual Meeting of the Association for Computational

Linguistics.

Truc Vien T. Nguyen and Alessandro Moschitti. 2011.

End-to-end relation extraction using distant supervi-

sion from external semantic repositories. In Proceed-

ings of the 49th Annual Meeting of the Association for

Computational Linguistics: Human Language Tech-

nologies.

Sebastian Riedel, Limin Yao, and Andrew McCallum.

2010. Modeling relations and their mentions without

labeled text. In Proceedings of the European Confer-

ence on Machine Learning and Knowledge Discovery

in Databases (ECML PKDD 10).

Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.

2011. New York University 2011 system for KBP slot

filling. In Proceedings of the Text Analytics Confer-

ence.

Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-

Closky, Angel X. Chang, Valentin I. Spitkovsky, and

Christopher D. Manning. 2011. Stanfords distantly-

supervised slot-filling system. In Proceedings of the

Text Analytics Conference.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,

Christopher D. Manning. 2012. Multi-instance Multi-

label Learning for Relation Extraction. In Proceed-

ings of the 2012 Conference on Empirical Methods in

Natural Language Processing and Natural Language

Learning.

TAC KBP 2011 task definition. 2011. http://nlp

.cs.qc.cuny.edu/kbp/2011/KBP2011 TaskDefinition.pdf

Shingo Takamatsu, Issei Sato, Hiroshi Nakagawa. 2012.

Reducing Wrong Labels in Distant Supervision for Re-

lation Extraction. In Proceedings of 50th Annual Meet-

ing of the Association for Computational Linguistics.

Fei Wu and Daniel S. Weld. 2007. Autonomously seman-

tifying wikipedia. In Proceedings of the International

Conference on Information and Knowledge Manage-

ment (CIKM-2007).

Guodong Zhou, Jian Su, Jie Zhang and Min Zhang. 2005.

Exploring various knowledge in relation extraction. In

Proceedings of ACL-2005.

782


