



















































Supervised and Nonlinear Alignment of Two Embedding Spaces for Dictionary Induction in Low Resourced Languages


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 823–832,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

823

Supervised and Nonlinear Alignment of Two Embedding Spaces for
Dictionary Induction in Low Resourced Languages

Masud Moshtaghi
Amazon / Manhattan Beach, CA USA

mmasud@amazon.com

Abstract

Enabling cross-lingual NLP tasks by leverag-
ing multilingual word embedding has recently
attracted much attention. An important moti-
vation is to support lower resourced languages,
however, most efforts focus on demonstrat-
ing the effectiveness of the techniques using
embeddings derived from similar languages to
English with large parallel content. In this
study, we present a noise tolerant piecewise
linear technique to learn a non-linear map-
ping between two monolingual word embed-
ding vector spaces. We evaluate our approach
on inferring bilingual dictionaries. We show
that our technique outperforms the state of the
art in lower resourced settings with an aver-
age improvement of 3.7% for precision @10
across 14 mostly low resourced languages.

1 Introduction

Aligning two embedding spaces is a key compo-
nent in many multilingual tasks in natural lan-
guage processing such as unsupervised word and
sentence translation (Ruder et al., 2017) and large-
scale dictionary creation (Lample et al., 2018).
Large scale dictionaries are used in many NLP
tasks, for example, web data cleaning. The main
feature of the two most successful methods for this
task is based on dictionaries (Xu and Koehn, 2017;
Koehn et al., 2018).

Initial success obtained in aligning two embed-
ding spaces has created a flurry of research ef-
forts into improving the task (Nakashole, 2018;
Doval et al., 2018; Kementchedjhieva et al., 2018;
Chen and Cardie, 2018; Joulin et al., 2018; Hoshen
and Wolf, 2018; Artetxe et al., 2018), to name a
few published in 2018. There are two main ap-
proaches for mapping a monolingual word embed-
ding space into another (Ruder et al., 2017): 1) Su-
pervised methods rely on a few anchor points to
learn the matching of two spaces (Lample et al.,

2018; Mikolov et al., 2013; Xing et al., 2015;
Joulin et al., 2018) and; 2) Unsupervised meth-
ods mostly formulate the problem as an adver-
sarial optimization problem (Lample et al., 2018;
Zhang et al., 2017; Ruder et al., 2017; Chen and
Cardie, 2018; Hoshen and Wolf, 2018). While for
high density languages such as Spanish, German,
French and Italian the accuracy of unsupervised
techniques is comparable, or even higher, than su-
pervised techniques, the accuracy of unsupervised
techniques is significantly lower when lower re-
sourced languages are considered. A number of
the experiments featured in the recent publications
reflect this fact (Søgaard et al., 2018; Irvine and
Callison-Burch, 2017).

To place this work in the context of recent ad-
vances in the field, we divide the aspects targeted
by researchers to improve the precision with some
examples in Fig. 1. While some of the improve-
ments in optimization and space-adjustments can
be applied to almost (any) model, our goal is to
improve the supervised mapping in low resourced
setting. Since the conception of linear mapping
between two spaces, there are only very small
minor improvement in the supervised mapping
function itself and largest improvements are ob-
tained by updating the word translation retrieval in
the shared space and normalization steps (Artetxe
et al., 2018). Therefore, we test our method
against the most widely cited and tested method
by (Lample et al., 2018), which culminates the
previous efforts in the area of the supervised map-
ping (Mikolov et al., 2013; Xing et al., 2015;
Lazaridou et al., 2015; Dinu et al., 2015; Smith
et al., 2017; Artetxe et al., 2017). The main idea
of these methods is to use Procrustes analysis to
determine a linear mapping between two spaces
using a few anchor points, i.e., a small dictionary.
An iterative method is used to expand the dic-
tionary with the most promising candidates and



824

re-learn the mapping (Artetxe et al., 2017; Lam-
ple et al., 2018). However, the main success of
these methods was obtained on linguistically sim-
ilar languages where a large volume of monolin-
gual data from compatible sources is available. A
recent study showed that a simple linear mapping
is not enough for low resourced languages (Nakas-
hole, 2018). However, the proposed method in this
study, NorMA, came short in solving the prob-
lem showing significant gains in mapping pre-
cision for Russian-English and Chinese-English,
compared not to the best model in (Lample et al.,
2018), while underperforming in Spanish-English
French-English and German-English. Due to the
limited set of experiments and inconsistent results
across multiple languages we defer comparison
with this method until a more mature version is
developed. Here, we only acknowledge that our
method achieves 1.5% better precision@1 com-
pared to NorMA’s results for Chinese.

The contributions of this work are three-fold.
First, we introduce a noise tolerant form of Gener-
alized Procrustes (GP) which corrects for rotation,
geometrical translation 1 and dilation. Second,
we adapt an existing technique for piecewise lin-
ear regression to instead perform piecewise linear
mapping. This allows subspaces to have different
mapping functions. Finally, we conduct a large-
scale evaluation and show that our model system-
atically outperforms the state of the art (Lample
et al., 2018) in dictionary inference task with the
average gain of 3.7% in precision @10 across 14
languages.

Multi-Lingual Word Embedding

Mapping

Linear
(Dinu etal, 
Lample et al.)

Non-linear
(Nakashole)

Optimization 
Techniques

Distance
(Dinu etal, 
Conneau et al.)

Regularization
(Joulin etal, 
Lazaridou etal)

Space Adjustment

Unsupervised 
Techniques

Multiple 
Mappings
(Kementchedjhieva
et al., Artetxe etal-
2018)

Re-Alignments
(Doval etal)

Mapping-based
(Alvarez-Melis etal,
Hoshen and Wolf) Direct MLE

(Cao etal)

Supervised

Figure 1: A breakdown of the main area of research
focus for dictionary induction task.

1Note the difference between the translation of a word and
translation as a geometrical transformation of a vector space.

2 Problem Statement

We first provide a formal definition of multilingual
word embedding task using a mapping function,
then we explain the intuition on why this method
can be used to create dictionaries.

2.1 Definition

Given source and target embedding vector spaces

Vs and Vo, we are looking for a mapping Vo
f→ Vs

that minimize ‖f (Vo)− Vs‖F , where ‖.‖F is the
Frobenius norm (a matrix norm calculated by the
square root of the sum of squares of its elements).
Function f performs two tasks, a permutation task
to find the matching of vectors and a mapping
task to map the permuted vectors in Vo to space
spanned by the vectors in Vs. In this general form,
the problem is not convex. The supervised solu-
tions for this problem add two assumptions to the
problem to make it tractable. First, they assume
a set of anchor points are available V̄s ⊆ Vs and
V̄o ⊆ Vo for which we know a one-to-one match-
ing, i.e., from a small dictionary. The second as-
sumption is that the difference between the two
spaces is only in rotation. In this paper, we relax
the assumption about f and propose a non-linear
mapping, which in addition to the rotational dif-
ference, it corrects for dilation and translation be-
tween two embedding vector spaces.

2.2 Intuition

We first give an intuition about why we can map
a word embedding space into another. Word2Vec
(or similar) embedding processes capture the cor-
relation between words (Mikolov et al., 2013) and
embed the words in a vector space so that the
distances between them are representative of this
correlation. If the word embeddings are com-
puted over comparable corpora, a word and its
translation have similar correlated words. Hence,
these words form similar neighborhood structures
in their own monolingual spaces. A reasonable as-
sumption is that the words in this neighborhood
should also be a (dictionary) translation of each
other. These two assumptions of the similarity
of neighborhood structure and the assumption that
the neighborhood consists of words that are trans-
lations of each other are crucial to the success of
mapping methods. The similarity of the corpora
used in the initial embeddings in terms of size
and content significantly contributes to the valid-
ity of the assumptions. This effect is highlighted



825

in multiple studies (Lample et al., 2018; Søgaard
et al., 2018; Grave et al., 2018). That is why
the embeddings calculated on high resourced lan-
guages, with a similar parallel content to English,
are mapped to English embedding space with very
high accuracy as the embedding spaces have sim-
ilar structures. Søgaard et al. highlight the lan-
guage pair, the comparability of the monolingual
corpora, and the parameters of the word embed-
ding algorithms as key aspects affecting the per-
formance of unsupervised methods. The perfor-
mance of supervised methods is also tightly tied
to these factors but they are less affected by the
virtue of having the extra source of supervision.

If the initial two assumptions hold and since
affine mappings, such as orthogonal rotation, do
not change the structure of the space, minimiz-
ing the distance between the vectors should force
the words with the same meaning to end up close
to each other. In low resourced languages with
a significantly lower amount of data compared to
the English corpora structural similarities of the
two spaces are not guaranteed leading to weak ini-
tial assumptions. This is also true in languages
with lower similarity to English such as Slavic
and Asian languages (Nakashole, 2018; Nakas-
hole and Flauger, 2018). Nakashole and Flauger
showed that word neighborhoods require different
linear mappings to correctly capture the word re-
lation. However, the proposed method by (Nakas-
hole, 2018), NorMA, learns a dictionary of neigh-
borhoods and reconstruct the word embedding us-
ing a linear combination of the dictionary items
which does not guarantee locality in the mapping,
, i.e., many dictionary items can contribute to rep-
resentation of a word.

3 Proposed Method

Our proposed method tackles two problems with
the existing mapping techniques. First, instead
of only fixing the rotational differences between
the two embedding vector spaces, the model also
accounts for geometrical translation and dilation.
Second, our model, which we call LLMap, has
different linear mapping functions in subspaces
which allows us to have different levels of correc-
tions in different regions of the embedding space.
We achieve this goal by incorporating a Robust
Generalized Procrustes (RGP) mapping into a Lo-
cally Linear Model (LLNF).

3.1 Robust Generalized Procrustes Analysis

We borrow the concept Generalized Procrustes
and introduce a noise tolerant version, which we
name Robust Generalized Procrustes (RGP), to
solve the first problem with the existing mapping
techniques. RGP adds translation and dilation to
the vanilla Procrustes. Note that Generalized Pro-
crustes (GP) has been used to mean a different
concept in (Kementchedjhieva et al., 2018) refer-
ring to a method that maps multiple spaces to-
gether.

3.1.1 Procrustes Problem

Procrustes analysis has been used in many differ-
ent fields for analyzing the relationship between
two datasets. The Orthogonal Procrustes Prob-
lem (OPP) (Koschat and Swayne, 1991; Crosilla,
2003), aka Procrustes Rotation, used in current su-
pervised multilingual mapping techniques (Lam-
ple et al., 2018; Smith et al., 2017) is defined as,

Definition 1: Given two matrices V̄s and V̄o ∈
<n×p with rank p, the OPP is defined as the
following optimization problem.

min
W

∥∥V̄oW − V̄s∥∥2F , subject to W TW = Ip×p
(1)

The above optimization function has a closed form
solution given by W T = UV ∗ where UDV ∗ =
SV D(V̄ Ts V̄o) . The Generalized Procrustes Prob-
lem (GPP) formulates the problem as a general lin-
ear mapping, correcting for other types of distur-
bances, i.e., translation and dilation.

Definition 2: Given two matrices V̄s and V̄o ∈
<n×p with rank p, the GPP is defined as the
following optimization problem.

min
W

∥∥(V̄oKW + In×1b)− V̄s∥∥2F ,
subject to W TW = Ip×p

where K ∈ <p×p(dilation), b ∈ <1×p(translation),
W ∈ <p×p(rotation)

(2)

The solutions for the three components of this
optimization problem, i.e., K (diagonal (zero off
diagonal) positive matrix), b and W are shown in
Eq. (3). Let ms = mean(V̄s), mo = mean(V̄o),



826

V̄ ∗o = V̄o −mo and V̄ ∗s = V̄s −ms

W T =UV ∗, UDV ∗ = SV D(K−1V̄ ∗Ts V̄
∗
o )

Kjj =
(V̄ ∗s )

T
j (V̄

∗
o W )j

(V̄ ∗o W )
T
j (V̄

∗
o W )j

b =ms −moKW

(3)

where (.)j denote the jth column vector of the ma-
trix.

The translation factor b in Eq. 3 does not appear
in the calculation of rotation and dilation while
the other two have interdependencies. Centering
all vectors prior to mapping makes the translation
vector zero. But it affects the flexibility of sub-
spaces to have different translation factor. Hence,
we operate on the original embedding space with-
out centering the vectors.

For the other two factors, Koschar and
Swayne (Koschat and Swayne, 1991) suggested
using an iterative algorithm starting with K =
Ip×p and computing W, then using W updating
the values of K and iterate between the two until
reaching convergence. We use 3 iterations in all
of our experiments as the values converge rapidly.
We expect to have noisy embedding vectors, so we
use the truncated SVD algorithm to calculate the
rotation matrix, i.e., discarding eigenvectors cor-
responding to small eigenvalues to make the map-
ping robust to outliers.

In the next section, we introduce how this linear
mapping can be incorporated into the LLNF model
to learn a piecewise linear mapping between two
vector spaces.

3.2 Locally Linear Neural Model for
Mapping (LLMap)

The inaccuracies in the embedding process due
to lack of data, and inherent language family dif-
ferences reduce structural similarities in (some)
regions of the embedding space. In this case,
a global minimization of the distance between
the vectors using structure preserving mappings
(Procrustes) could create undesired artifacts. We
devise a piecewise linear mapping between the
two spaces based on a Locally Linear Neural
(LLNF) model with soft switching between the
neurons (Babuška and Verbruggen, 2003) to in-
crease the flexibility of the models when dealing
with different levels of structural similarities in the
embedding space. In this work, we update the neu-

!"

#"

$

!%

#%

$

!&

#&
$

'

'
(

)*

Figure 2: The network structure of an LLNF model.
The green dotted line is used to normalize the output of
neurons. Each neuron contains a membership function
φ that controls the contribution of the neuron function
f to the final output.

ron definition in the standard LLNF model to per-
form localized mapping using RGP, which guar-
antees a reduction in terms of the Sum of Squared
Errors (SSE).

First, we provide a brief introduction to the
LLNF model followed by the description of our
method, LLMap. LLMap replaces the linear re-
gression function in the LLNF neurons with RGP
while exploiting the training algorithm of the
LLNF model for parameter estimation.

3.2.1 Introduction to LLNF Models

The LLNF model is a specific form of Radial Basis
Function Networks (RBF) with an expanded lin-
ear model in each neuron. This model is a general
function approximator (Nelles, 2001). An LLNF
input-output model with input vector x ∈ <p, out-
put y ∈ < and M neurons has the following form:

y =

∑M
i=1 φi(x)fi(x)∑M

i=1 φi(x)
(4)

where f : <p → < is a linear function and
φ : <p → [0 , 1] is called a membership function.
Fig. 2 shows a pictorial form of these networks.
These networks typically contain one hidden layer.
Each neuron has a membership function that con-
trols the output of the neuron generated by a linear
regression function inside the neuron. The final
output is the weighted average of all the neurons.
The two most common membership functions are
the Gaussian and trapezoidal functions. The Gaus-
sian membership function with spherical contours



827

is defined as

φ(x) =

p∏
j=1

e
−

(xj−cj)
2

2σ2
j , (5)

where c ∈ <p and σ ∈ <p are the center and stan-
dard deviation of the neuron. When p is large the
Gaussian function can result in numerical instabil-
ity and usually a trapezoidal function is preferred
in this case. In one dimensional form a trapezoidal
function is defined by a quadruple (a, b, c, d) with
the following function.

φ(xj)j =


0 xj < a or xj ≥ d,
xj−aj
b−aj a ≤ xj < b,

1 b ≤ xj < c,
d−xj
d−c c ≤ xj < d.

(6)

An extension to a multidimensional function
can be achieved by defining φ(x) = min

j
(φ(xj)j).

We use this membership function our model.
The non-linearity of the LLNF comes from φi

functions. If these functions are predefined, the
estimation of fi parameters is simplified to solv-
ing a weighted regression problem. The weight for
input vector x at neuron q is φq(x)/

∑M
i=1 φi(x),

i.e., the normalized membership of the vector to
the neuron. There are a number of strategies for
determining the number of neurons and the φi
functions. A widely used approach is called Lo-
cally Linear Model Tree (LoLiMoT), which fol-
lows a greedy tree construction algorithm.

LoLiMoT- The LoLiMoT algorithm (Nelles,
2001) creates hyper-rectangular partitions over the
input space and centers a neuron in the middle of
each partition. The parameters of the neuron are
set proportional to the size of the rectangle. In Sec-
tion 3.2.2, we describe how the parameters of the
model are set in our experiments. The algorithm
works in a greedy manner. It starts with one hyper-
rectangle covering the whole space. Then, it enters
a loop to optimize the number of partitions where
it finds the rectangle that contributes the most to
the estimation error and split the rectangle in the
dimension that decreases the estimation error the
most as shown in Algorithm 1. The splitting of
each neuron is performed using axis-parallel par-
tition of the space into two equal subspaces with
a neuron at the center of each subspace. Fig. 3
shows a schematic 2-dimensional view of how the
LoLiMoT algorithm partitions the input space.

Figure 3: LoLiMoT structure optimization algorithm in
2 dimensions.

3.2.2 LLNF for Mapping
While LLNF models are used for regression and
classification (Nelles, 2001), we aim to use these
models for mapping by replacing the linear regres-
sion component in each neuron with an RGP. We
leverage the LoLiMoT algorithm to optimize the
network structure. This is achieved by replacing
the function fi in Eq. 4 with a linear mapping func-
tion. This results in having a weighted RGP prob-
lem to estimate the parameters of fi. The weighted
RGP can also be solved in closed-form (Crosilla,
2003; Koschat and Swayne, 1991) by updating
the Eq. 3 to include a diagonal weight matrix Lq
where `xx = φq(xx)/

∑M
i=1 φi(x) is the weight

for each input vector at neuron q. The updated
formula is written as,

W Tq =UV
∗, UDV ∗ = SV D(K−1(LqV̄

∗T
s )(LqV̄

∗
o ))

Kjj =
(V̄ ∗s )

T
j (V̄

∗
o Wq)j

(V̄ ∗o Wq)
T
j (V̄

∗
o Wq)

bq =ms −moKWq
(7)

The LLNF model in Eq. 4 computes a mapping
of each input vector when Eq. 7 is used in place
of fi. With this change, the LoLiMoT algorithm
can still be used to optimize the structure of the
network.

In this paper, we consider a fixed setup in our
experiments for certain hyperparameters of the
LLMap to focus the discussion on the main model.
These parameters can be potentially tuned to in-
crease the performance. The following setup is
used in all of the experiments in this paper.

1. Eq. 7 requires the number of iterations be-
tween calculation of rotation and dilation



828

Input: Matrices V̄s and V̄o, D = # of
dimensions, itr = # of splits

LLMap = Initialize(One neuron
covering the entire space)

while itr ≥ 1 do
// Use Eq.(7) for each

neuron in LLMap
Error = LLMap.Fit(V̄s,V̄o)
foreach N ∈ LLMap do

// Lq is a diag. matrix
of membership values

N.Error = Error*N.Lq
end
N Sel = N in LLMap with Maximum Error
LLMap Sel = LLMap
MaxError = Error.Sum ()
foreach j ∈ D do

LLMap New = LLMap - N Sel
N1, N2 = N Sel.Split(j)
LLMap New.Add(N1, N2)
Error = LLMap New.Fit(V̄s, V̄o)
if Error.Sum () < MaxError then

MaxError = Error.Sum ()
LLMap Sel = LLMap New

end
end
itr −−
LLMap = LLMap Sel

end
Algorithm 1: LLMap Algorithm

matrices. We use 3 iterations in all exper-
iments.

2. To define the quadruple (a, b, c, d) for each
neuron, we use two parameters (α, δ) and
define a = α − 4δ, b = α − δ, c =
α + δ, d = α + 4δ. For the first neuron
α = mo (the mean of input vectors) and
δ = (max{V̄o} − min{V̄o})/4 . This en-
sures that the activation function of the first
neuron is non-zero for all the values in the
space.

3. We set the stopping criterion for the
LoLiMoT algorithm to 4 neurons. We im-
pose this restriction to allow the model and
the data to fit in GPU memory. In addition,
the small number of neurons prevents the
vanilla LoLiMoT from overfitting and re-
moving the need for a validation set.

4. To calculate the rotation parameter W , we
discard eigenvectors in U, V with corre-
sponding eigenvalue smaller than 0.5 up to
a maximum of 50 vectors. This maximum
number is used to ensure that as the num-
ber of neurons increases LLMap does not
remove too many eigenvectors.

4 Experiments

The purpose of the experiments is to compare the
performance of our algorithm in low resourced
languages for dictionary generation in a super-
vised setting with the best existing supervised
technique. To this end, we compare our model
with the supervised MUSE algorithm (Lample
et al., 2018).

4.1 Data

We use the dictionaries made publicly available
through the MUSE library 2. This library contains
110 bilingual dictionaries of varied sized, mostly
with English as the target languages. We also use
the 300-dimensional word embeddings pre-trained
on Wikipedia using the skip-gram method with de-
fault parameters described in (Bojanowski et al.,
2017). 3 This library contains 294 languages.

We select 14 languages from low resourced lan-
guages including a wide range of languages in-
cluding Asian and Slavic and European languages
plus Spanish. Our selection mix is focused on low
resourced and languages with low structural sim-
ilarity to English. We use 200,000 most frequent
tokens in the embedding file.

4.2 Experimental setup

The goal of dictionary inference using multilin-
gual word embedding is to map the monolingual
word embedding spaces onto each other and to
infer the translation of a word by looking at the
neighborhood of that word. We use the Cross-
Domain Similarity Local Scaling (CSLS) distance
introduced in (Lample et al., 2018) to find the top
k. Given a source word, v, and its set of transla-
tions, V ′, any sense retrieval measures the percent-
age of time that at least one v′ ∈ V ′ is in the top
k-neighbors of v. In all senses retrieval the results

2https://github.com/facebookresearch/
MUSE

3https://github.com/facebookresearch/
fastText

https://github.com/facebookresearch/MUSE
https://github.com/facebookresearch/MUSE
https://github.com/facebookresearch/fastText
https://github.com/facebookresearch/fastText


829

Language
P@1 P@5 P@10

LLMap MUSE RGP LLMap MUSE RGP LLMap MUSE RGP
Czech (CS) 28.29 28.37 28.37 56.92 55.65 55.88 65.99 64.72 64.94
Norwegian (NO) 32.9 31.62 31.63 58.74 56.13 56.53 66.23 63.57 64.01
Dutch (NL) 42.3 41.06 41.18 67.13 65.43 65.7 73.73 72.03 72.49
Chinese (ZH) 17.4 14.19 19.51 43.19 35.6 44.05 52.11 44.62 52.68
Korean (KO) 17.12 17.02 16.81 35.8 34.41 34.23 44.05 42.69 42.12
Japanese (JA) 9.46 2.45 9.97 20.3 6.97 20.72 25.83 9.85 26.66
Croatian (HR) 19.29 18.71 18.87 46.4 43.55 44.25 56.36 53.38 54.12
Indonesian (ID) 31.9 30.37 30.45 54.63 51.96 52.4 62.06 59.24 59.67
Farsi (FA) 17.48 16.69 17.12 35.36 33.14 33.7 42.7 40.24 40.75
Bulgarian (BG) 21.64 23.4 23.02 56.19 55.22 55.35 67.12 65.95 66.11
Spanish (ES) 42.36 42.01 42.08 70.75 69.94 70.12 77.32 76.41 76.73
Tamil (TA) 9.79 9.76 10.07 24.3 22.34 23.08 31.38 28.78 29.8
Hindi (HI) 16.83 16.26 16.79 36.51 32.88 34.69 43.91 39.92 41.85
Bengali (BN) 15.51 15.3 15.95 39.67 35.55 37.06 48.43 43.94 45.51
Average
Improvement
(LLMap-MUSE)

1.07 3.36 3.70

Table 1: The comparison between the proposed methods LLMap and RGP, and the MUSE supervised method.
The values are average precision over 10 random 90-10 splits of the dictionaries, statistically significant results
between LLMap and MUSE are shown in bold and between LLMap and RGP are underlined.

are measured by the percentage of time each indi-
vidual v′ ∈ V ′ appearing in the top k-neighbors of
v.

While MUSE supervised was tested for any
sense retrieval using a pre-split train-test, the per-
formance of the dictionary inference task for k >
1 is better to be measured by all senses retrieval.
Any sense retrieval is considered a significantly
easier task, e.g. in Spanish precision@5 of the
MUSE algorithm goes up from 71% to 91% when
any sense is considered. Also, best experimental
design practices suggest using cross validation to
account for variation in the data.

Considering these shortcomings, we use a dif-
ferent experimental setup. We create 10 random
90-10 splits of the big dictionaries for training and
testing of the systems to capture the variations due
to random splits. We also report the precision@5
using the pre-split for compatibility with previ-
ously published results on this dataset for both any
and all senses of a word.

We use double sided paired t-test and p-value
at 0.05 to test for statistical significance in the re-
sults. We used 4 refinement steps in the supervised
MUSE mapping and used the best result on the
test data as the performance of the algorithm. For
our algorithm, we report the final test result after

adding the 4th neuron. We compare the results of
RGP and LLMap to quantify the contribution of
each addition that we made to improve the map-
ping.

4.3 Results

Table 1 shows the average of precision over the 10
random splits @k = 1, 5 and 10. The bold values
are statistically significant results between LLMap
and the MUSE supervised method. The RGP col-
umn refers to our model without the piecewise
mapping, which we discuss later in this section.

In all cases, except Czech (CS) and Bulgarian
(BG) @1 where MUSE has a slight edge over
LLMap, our method achieved higher precision
on average over 10-fold cross-validation than the
MUSE algorithm. In the majority of the cases
the improvements are statistically significant. We
can see the most significant improvements (over
8%) are observed in Japanese (JA) language and
Chinese (ZH). The other languages mostly see be-
tween 1%-3% improvement in the precision. The
average gain in precision @10 sits at 3.7%.

The gains achieved by our method become
larger by increasing the k. It shows that LLMap
can create a better proximity neighborhood around
the words as it is more flexible to handle multiple



830

Lang
Any Sense All Senses

MUSE LLMap MUSE LLMap
CS 76.66 78.86 61.32 62.17
NO 80.00 81.85 65.57 67.35
NL 88.53 89.33 69.28 70.52
ZH 55.00 64.77 41.88 51.50
KO 51.94 54.23 42.81 44.40
JA 3.97 17.62 3.22 14.51
HR 62.46 64.70 52.19 53.78
ID 81.20 85.19 64.87 68.20
FA 53.73 56.03 42.64 44.29
BG 65.07 68.13 59.45 59.48
ES 92.10 92.23 71.46 72.11
TA 28.53 29.00 25.60 26.01
HI 51.20 53.93 38.22 44.53
BN 33.80 36.42 42.42 45.10
AVG 3.45 3.07

Table 2: The comparison between the proposed method
LLMap and MUSE on pre-split train and test dictionar-
ies for any and all senses recovery by the algorithms
for precision@5. Last row shows the average improve-
ment achieved by LLMap over MUSE

senses of a word. In Spanish the gains are not sta-
tistically significant at any k, pointing to the fact
that in high resourced languages with large simi-
larities with English the mapping cannot create a
significant advantage over simpler mappings.

Table 2 shows the precision@5 for the pre-split
dictionaries. In all 14 languages the LLMap out-
performs the MUSE algorithm for recovering both
all senses and any sense of a word with signifi-
cant gains in Chinese and Japanese. These results
are consistent with the more comprehensive cross-
validation settings. Note that the any sense recov-
ery is on average higher than all senses, pointing
to the same fact the model is better at creating a
better neighborhood around words where at least
one sense of a word can be recovered.

The improvements that we observed compared
to the state of the art are the culmination of two
main changes to the baseline OPP in the map-
ping calculation, namely using RGP and the piece-
wise linear mapping. A vanilla RGP method
is an LLMap model with one neuron. Table 1
shows how much increasing the non-linearity in
the mapping by increasing the number of neu-
rons from 1 to 4 contributes to the overall pre-
cision. LLMap has statistically significant (un-
derlined values) higher precision than the vanilla

RGP in most cases. However, in the two lan-
guages with the largest improvements compared
to MUSE, RGP accounts for most of the gains.
LLMap contributes to an overall average of 1% in-
crease in the precision on top of RGP.

4.4 Discussions
The computational complexity of the mapping
increases linearly with the number of neurons.
In terms of training, finding the best dimension
to split is the most time-consuming part, which
increases the complexity of the algorithm from
O(n2p) in OPP to O(n2p2) in LLMap. This step
requires creating p (number of dimensions) mod-
els and estimate the error of each model. In our
implementation, each split took approximately 5
minutes to complete on an Amazon AWS EC2
p3.2xlarge. This task is usually done offline as a
pre-processing step. Therefore, the training time
complexity would not be a barrier to this approach.
While MUSE uses a simpler mapping, the itera-
tions require identifying promising candidates to
add to the dictionary, which results in a similar 5
minutes run-time on a GPU.

We limit the number of neurons in our algorithm
to 4 for experimental purposes. This allows us to
have the models, data and top-k calculation on a
single GPU for quick (10-15 minutes) run-time.

The framework presented in this work, is a
general framework that can accommodate other
mappings and improvements to the distance func-
tions. However, so far to the best of our knowl-
edge, structure preserving mapping such GP out-
performs non-linear mappings that change the
structure of the underlying space. Regularized
and weighted mapping functions that do not have
closed-form solutions can also be incorporated in
this structure with a back-propagation optimiza-
tion technique. In this work, we focused our dis-
cussion on the generalized mapping function as
opposed to other areas of improving this task.

4.5 Comments
We have added this section to explain the The re-
viewers requested comparison of LLMap with a
stronger baseline to compare the supervised map-
ping, suggesting vanilla VecMap (Artetxe et al.,
2018) as a potential baseline. In the introduc-
tion, we argued that in terms of mapping function
MUSE has the strongest and most widely appli-
cable mapping function. Most of improvements
after this work, including the ones in VecMap,



831

are obtained by updating the translation retrieval
and/or normalization around the mapping func-
tion. To ensure the validity of our argument, we
ran VecMap with normalization, whitening and re-
weighting turned off and it produced nearly iden-
tical numbers to MUSE when CSLS is used as the
retrieval method.

5 Conclusion

In this research, we introduced a non-linear map-
ping between two embedding spaces for dictio-
nary induction. The embedding space created
for low resourced languages are noisier than their
English counterpart and introduce challenges for
simple rotational mapping used in the current tech-
niques. We showed that our piecewise linear
model systematically improves the precision of
existing Procrustian mapping for dictionary induc-
tion using in a large-scale cross-validation setting
by over 3% on average. This approach has appli-
cations beyond the scope of this paper and can be
used in any applications requiring solving OPP or
GPP.

References
Mikel Artetxe, Gorka Labaka, , and Eneko Agirre.

2017. Learning bilingual word embeddings with
(almost) no bilingual data. In 55th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 451–462, Vancouver, Canada.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018.
Generalizing and improving bilingual word embed-
ding mappings with a multi-step framework of linear
transformations. In AAAI Conference on Artificial
Intelligence.

Robert Babuška and Henk Verbruggen. 2003. Neuro-
fuzzy methods for nonlinear system identification.
Annual Reviews in Control, 27(1):73 – 85.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Xilun Chen and Claire Cardie. 2018. Unsupervised
multilingual word embeddings. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 261–270, Brussels, Belgium.

Fabio Crosilla. 2003. Procrustes analysis and geode-
tic sciences. In Schwarze V.S. Grafarend E.W.,
Krumm F.W., editor, Geodesy-The Challenge of the
3rd Millennium, pages 287–292. Springer, Berlin,
Heidelberg.

Georgiana Dinu, Angeliki Lazaridou, and Marco Ba-
roni. 2015. Improving zero-shot learning by mitigat-
ing the hubness problem. In Proceedings of ICLR,
San Diego, CA.

Yerai Doval, Jose Camacho-Collados, Luis Es-
pinosa Anke, and Steven Schockaert. 2018. Im-
proving cross-lingual word embeddings by meeting
in the middle. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, pages 294–304, Brussels, Belgium.

Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-
mand Joulin, and Tomas Mikolov. 2018. Learning
word vectors for 157 languages. In Language Re-
sources and Evaluation Conference.

Yedid Hoshen and Lior Wolf. 2018. Non-adversarial
unsupervised word translation. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing, pages 469–478, Brus-
sels, Belgium. Association for Computational Lin-
guistics.

Ann Irvine and Chris Callison-Burch. 2017. A com-
prehensive analysis of bilingual lexicon induction.
Computational Linguistics, 43(2):273–310.

Armand Joulin, Piotr Bojanowski, Tomas Mikolov,
Hervé Jégou, and Edouard Grave. 2018. Loss in
translation: Learning bilingual word mapping with a
retrieval criterion. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2979–2984, Brussels, Belgium.
Association for Computational Linguistics.

Yova Kementchedjhieva, Sebastian Ruder, Ryan Cot-
terell, and Anders Søgaard. 2018. Generalizing Pro-
crustes analysis for better bilingual dictionary induc-
tion. In Proceedings of the 22nd Conference on
Computational Natural Language Learning, pages
211–220, Brussels, Belgium. Association for Com-
putational Linguistics.

Philipp Koehn, Huda Khayrallah, Kenneth Heafield,
and Mikel Forcada. 2018. Findings of the wmt 2018
shared task on parallel corpus filtering. In Proceed-
ings of the Third Conference on Machine Trans-
lation: Shared Task Papers, pages 726–739, Bel-
gium, Brussels. Association for Computational Lin-
guistics.

Martin A. Koschat and Deborah F. Swayne. 1991.
A weighted procrustes criterion. Psychometrika,
56(2):229–239.

Guillaume Lample, Alexis Conneau, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018.
Word translation without parallel data. In Sixth
International Conference on Learning Representa-
tions.

Angeliki Lazaridou, Georgiana Dinu, and Marco Ba-
roni. 2015. Hubness and pollution: Delving into



832

cross-space mapping for zero-shot learning. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural, pages 270–
280.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proceedings of the 26th International Con-
ference on Neural Information Processing Systems -
Volume 2, NIPS’13, pages 3111–3119.

Ndapa Nakashole. 2018. NORMA: Neighborhood sen-
sitive maps for multilingual word embeddings. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
512–522, Brussels, Belgium. Association for Com-
putational Linguistics.

Ndapa Nakashole and Raphael Flauger. 2018. Char-
acterizing departures from linearity in word transla-
tion. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), page 221–227, Melbourne,
Australia. Association for Computational Linguis-
tics.

Oliver Nelles. 2001. Nonlinear System Identification:
From Classical Approaches to Neural Networks and
Fuzzy Models. Springer, Berlin, Heidelberg.

Sebastian Ruder, Ivan Vulić, and Anders Søgaard.
2017. A survey of cross-lingual word embedding
models. arXiv preprint arXiv:1706.04902v2.

Samuel L. Smith, David H. P. Turban, Steven Hamblin,
and Nils Y. Hammerla. 2017. Offline bilingual word
vectors, orthogonal transformations and the inverted
softmax. In Proceedings of International Confer-
ence on Learning Representations (ICLR), Toulon,
France.

Anders Søgaard, Sebastian Ruder, and Ivan Vulić.
2018. On the limitations of unsupervised bilingual
dictionary induction. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 778–
788, Melbourne, Australia. Association for Compu-
tational Linguistics.

Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015.
Normalized word embedding and orthogonal trans-
form for bilingual word translation. In Proceedings
of NAACL, Los Angeles, CA.

Hainan Xu and Philipp Koehn. 2017. Zipporah: a fast
and scalable data cleaning system for noisy web-
crawled parallel corpora. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2945–2950, Copenhagen,
Denmark. Association for Computational Linguis-
tics.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017. Adversarial training for unsupervised
bilingual lexicon induction. In the 55th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1959–1970, Vancouver, Canada.


