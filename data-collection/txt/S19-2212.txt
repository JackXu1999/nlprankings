



















































Lijunyi at SemEval-2019 Task 9: An attention-based LSTM and ensemble of different models for suggestion mining from online reviews and forums


Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 1208–1212
Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics

1208

Lijunyi at SemEval-2019 Task 9: An attention-based LSTM model and
ensemble of different models for suggestion mining from online reviews

and forums

Junyi Li, Haiyan Ding∗
School of Information Science and Engineering

Yunnan University, Yunnan, P.R. China
∗Corresponding author: hyding@ynu.edu.cn

Abstract

In this paper, we describe a suggestion min-
ing system that participated in SemEval 2019
Task 9, SubTask A - Suggestion Mining from
Online Reviews and Forums. Given some
suggestions from online reviews and forums
that can be classified into suggestion and non-
suggestion classes. In this task, we combine
the attention mechanism with the LSTM mod-
el, which is the final system we submitted. The
final submission achieves 14th place in Task 9,
SubTask A with the accuracy of 0.6776. After
the challenge, we train a series of neural net-
work models such as convolutional neural net-
work(CNN), TextCNN, long short-term mem-
ory(LSTM) and C-LSTM. Finally, we make an
ensemble on the predictions of these models
and get a better result.

1 Introduction

Suggestion mining can be defined as the extrac-
tion of suggestions from unstructured text, where
the term “suggestions” refers to the expressions
of tips, advice, recommendations etc(Negi et al.,
2019). These suggestions largely express positive
and negative sentiments towards a given entity, but
also tend to contain suggestions for improving the
entity. Suggestion mining remains a relatively y-
oung area compared to Sentiment Analysis, es-
pecially in the context of recent advancements in
neural network based approaches for learning fea-
ture representations. In this task, suggestion min-
ing that classified sentences into suggestion and
non-suggestion classes was defined by the orga-
nizer.

In this paper, we mainly use an attention-based
LSTM model(Hochreiter and Schmidhuber, 1997)
for this task. The word-embedding used for al-
l models in this task is Word2Vec. Then, the word
vectors are fed into the long short-term memo-
ry (LSTM) layer. Finally, an attention mechanis-

Figure 1: An example from the SemEval 2019 Task 9
dataset

m(Luong et al., 2015) is added into the neural net-
works, and the prediction results are output via the
softmax activation. What’s more, we try a num-
ber of other models (such as the TextCNN(Kim,
2014), the C-LSTM(Zhou et al., 2015) and the
attention-based Bi-LSTM(Lai et al., 2015) ) for
comparative experiments. Furthermore we com-
bine all of the above models to get results by soft
voting.

The rest of our paper is structured as follows.
Section 2 introduces models. Section 3 describes
data preparation. Experiments and evaluation are
described in Section 4. The conclusions are drawn
in Section 5.

2 Model

For this task, we use 6 models for experiments.
Among these models, the attention-based LSTM
models can get the best results. This model com-
bines the attention mechanism with the LSTM.
The attention mechanism is a good solution to the
information vanish problem in long sequence in-
put situations. When dealing with machine com-
prehension problems, the LSTM and the attention
mechanism are more effective than they are used
individually.

For this task, we have 4 chances to submit our
result in the final submission. We use differen-



1209

t methods that are the attention-based LSTM, C-
LSTM and ensemble different models.

In this task, we not only select some single
models but also use the ensemble model architec-
ture(Sarle, 1996). The ensemble model(Kuncoro
et al., 2016) architecture, shown in figure 1, is
an ensemble of many single models(We call them
sub models)(Dietterich, 2000). Because each sub
model is independent of each other, their weights
are not shared and just use the same word embed-
ding when training each sub model. The process
of the whole ensemble model is carried out model
by model. First, each model is run independently,
and then the result file is saved. After running all
the independent models, the result files are taken
out and the final result is determined by the soft
vote(Rokach, 2010).

Figure 2: The architecture of the models ensemble

2.1 CNN and TextCNN

The convolutional neural network was original-
ly used to process image data. In recent years,
the application of convolutional neural networks
has gradually penetrated into many different field-
s, such as speech recognition and natural lan-
guage processing. The convolutional neural net-
work consists of three parts. The first part is the in-
put layer. The second part consists of n cyclic lay-
ers and collection layers. The third part consists of
a fully connected multi-layer perceptual classifi-
er. The difference between a cyclic neural network
and a common neural network is that the convolu-
tional neural network consists of a feature extrac-

tor with a convolutional layer and a sub-sampling
layer. In the convolutional layer, one neuron is on-
ly connected to several adjacent neurons.

TextCNN is a model that uses multiple convolu-
tional neural networks to output in tandem (Kim,
2014). In the model, the convolution window of
each convolutional neural network is different in
size. The convolution results obtained by convolu-
tion windows of different sizes are combined and
output.

In our task, we also use the basic convolutional
neural network and TextCNN to conduct experi-
ments(Zhang and Wallace, 2015). For this task,
we find that TextCNN can get a better result than
a single convolutional neural network. So, we will
be more inclined to choose a TextCNN model in-
stead of a single CNN model for our task.

2.2 LSTM

Traditional recursive neural networks are ineffec-
tive when dealing with very long sentences. The
LSTM (Hochreiter and Schmidhuber, 1997) mod-
el is developed to solve the gradient vanishing or
exploding problems in the RNN. Currently, the L-
STM is mainly used in natural language process-
ing such as speech recognition and machine trans-
lation. Compared with the traditional RNN, an
LSTM unit is added to the traditional model for
judging the usefulness of information. Each unit
mainly contains three gates (the forget gate, the in-
put gate, and the output gate) and a memory cell.
The system will judge the usefulness of the infor-
mation after the input information is fed into an
LSTM(Liu et al., 2016). Only the information that
matches the rules of the algorithm will be saved,
and the other information will be discarded by the
forget gate.

2.3 Bi-LSTM

Single direction LSTM(Lai et al., 2015) suffers a
weakness of not using the contextual information
from the future tokens. Bidirectional LSTM (Bi-
LSTM) exploits both the previous and future con-
text by processing the sequence on two direction-
s and generates two independent sequences of L-
STM(Kim et al., 2016) output vectors(Liu et al.,
2016). One processes the input sequence in the
forward direction, while the other processes the in-
put in the backward direction.

In this task, we also use the Bi-LSTM to get
a better result(Huang et al., 2015). We select the



1210

model that can be compared with other models as
comparative experiments.

2.4 C-LSTM

It has been successfully demonstrated that neu-
ral network models can achieve good results in
tasks such as sentence and document classifica-
tion. Convolutional neural networks (CNN) and
recurrent neural networks (RNN) are two main-
stream methods for this classification task (Zhou
et al., 2015). At the same time, these two methods
can also be used for our tasks, which use a com-
pletely different approach to understanding natural
language. In this model, we combine the advan-
tages of both CNN and RNN models and call it C-
LSTM for sentence representation and text classi-
fication. C-LSTM uses CNN to extract a series of
higher-level phrase representations and feeds them
to the Long-Term Short-Term Memory Recurrent
Neural Network (LSTM) for sentence representa-
tion(Stollenga et al., 2015). C-LSTM captures lo-
cal features of phrases as well as global and tem-
poral sentence semantics. Then, we predict the re-
sults based on the labels of the sentences (Zhou
et al., 2015).

Figure 3: C-LSTM model for our task

In our experiments, the C-LSTM model is com-
pared with a single CNN model, TextCNN, and a
single LSTM, Bi-LSTM model. The results show
that the C-LSTM model can achieve a better result
in this task.

2.5 Attention-based LSTM model

The LSTM model can alleviate the problem of gra-
dient vanishing, but this problem persists in long
range reading comprehension contexts. The at-
tention mechanism(Bahdanau et al., 2014) breaks
the constraint on fix-length vector as the contex-
t vector, and enables the model to focus on those
more helpful to outputs. After LSTM layer, we
use the attention mechanism on the output vectors

produced by previous layer. It is proven effective
to improve the performance of our model.

Figure 4: An attention-based LSTM model for our task

In the attention-based LSTM model, all sen-
tences and labels are converted to word vectors
by the word embedding layer. These word vec-
tors will be fed to the LSTM layer. Subsequently,
the word vector is represented as a hidden vector.
Next, the attention mechanism assigns weights to
each hidden vector, and the mechanism produces
attention weight vectors and weighted hidden rep-
resentations. Note that the weight vector is mainly
obtained by calculating the similarity. An atten-
tion weight vector is generated by calculating a
sentence vector matrix and a label vector matrix.
The attention weight vector is then fed to the soft-
max layer.

The attention mechanism allows the model to
retain some important hidden information when
the sentence is long. In our mission, the informa-
tion of sentences and tags is kept for a relatively
long time. Using the standard LSTM may result in
the loss of hidden information. To solve this pos-
sible problem, we have facilitated the attention-
based LSTM model.

In our task, the attention mechanism(Yang et al.,
2016) can get better results. We think that the at-
tention mechanism(Vaswani et al., 2017) can im-
prove the efficiency of task. So, we combine the
attention mechanism with the LSTM model. This
model can get the best results among the single
models, which is the final system we submitted.



1211

3 Data Preparation

The organizers provided training, trial, and test
sets, containing 8500, 592 and 833 sentences re-
spectively(Negi et al., 2019). Each sentence cor-
responds to one label, 0 or 1. Although official
data is regular, we need to do a further normaliza-
tion. We want to make it possible to read these
sentences easily. First of all, we have completely
restored the abbreviated words. For example “i’m
not asking microsoft to gives permission like an-
droid so any app can take my data” will become “i
am not asking microsoft to gives permission like
android so any app can take my data”. In this sen-
tence “i’m” is an abbreviation. So, we found these
abbreviations and restored it by creating a list.

examples normalization
i’m i am

doesn’t dose not
can’t can not
i’ll i will
i’ve i have
... ...
i’d i would
it’s it is

Table 1: normalization patterns

Then we noticed that it is also very importan-
t to remove some unnecessary characters, such as
“!”,“?” etc. What’s more, we find that the link to
the web-page is useless for this task. So we re-
move all urls.

For data pre-processing, we wrote the code to
realize the functions and we can improve the ef-
ficiency of our final experimental results through
these data pre-processing methods.

4 Experiments and evaluation

After data pre-processing, we start the main part of
the experiment. The preprocessed data is feed in-
to our prepared model for experimentation. At the
same time, we do experiments on different model-
s to compare the test results. In the experiments,
we also find that the same model will get different
results under different parameter adjustments. For
example, we use the C-LSTM model for exper-
iments, and our experimental results range from
0.67 to 0.78 with different parameters in the tri-
al data. Therefore, reasonable adjustment of pa-
rameters during the experiment is also a factor in

obtaining a good experimental result.
We run each individual model 5 times and use

the average as the final result of this model. In all
of models, dropout parameters are changed from
0.2 to 0.6, What’s more, in the LSTM model, we
also select the recurrent dropout (Srivastava et al.,
2014) that are set between 0.2 and 0.45. And we
set epoch = 10 and batch size = 64.

In this task, we mainly select 6 models and en-
semble all of these models. In the table 3 we post
the F1-score and recall of the model.

Model Recall F1-score
CNN 0.78 0.5523

TextCNN 0.80 0.5908
LSTM 0.81 0.6104

C-LSTM 0.83 0.6222
Attention-BiLSTM 0.85 0.6610
Attention-LSTM 0.84 0.6776
ensemble models 0.82 0.6806

Table 2: Recall and F1-score for each models on task
test data

5 Conclusion

In this task, we accomplish this task by integrat-
ing LSTM and attention mechanism. After com-
petition, we try various structurally different mod-
els and an ensemble of all the models. The per-
formance of a single model is slightly worse than
the ensemble model. And there are certain differ-
ences between the different parameter results of
the same model. Our results are still not as satis-
fying as the top teams on the leaderboard.

However, in this task, we have some problem-
s which we can’t solve. For example, we can not
successfully solve the problem of data imbalance.
We can not consider the problem of model opti-
mization too much and we don’t try more ways of
ensemble model.

In the future, we will continue to adjust the
model, improve the hardware configuration of the
computer, collect more external data, and conduct
more experiments to get better results. Further-
more, we will try again to solve the problem of
data imbalance. We will continue to do model op-
timization and we will try more ways of ensemble
model.



1212

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint arX-
iv:1409.0473.

Thomas G Dietterich. 2000. Ensemble methods in ma-
chine learning. In International workshop on multi-
ple classifier systems, pages 1–15. Springer.

Sepp Hochreiter and Jrgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991.

Yoon Kim. 2014. Convolutional neural network-
s for sentence classification. arXiv preprint arX-
iv:1408.5882.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016. Character-aware neural language
models. In Thirtieth AAAI Conference on Artificial
Intelligence.

Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng
Kong, Chris Dyer, and Noah A Smith. 2016. Distill-
ing an ensemble of greedy dependency parsers into
one mst parser. arXiv preprint arXiv:1609.07561.

Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015.
Recurrent convolutional neural networks for tex-
t classification. In Twenty-ninth AAAI conference on
artificial intelligence.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016.
Recurrent neural network for text classification
with multi-task learning. arXiv preprint arX-
iv:1605.05101.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025.

Sapna Negi, Tobias Daudert, and Paul Buitelaar. 2019.
Semeval-2019 task 9: Suggestion mining from on-
line reviews and forums. In Proceedings of the
13th International Workshop on Semantic Evalua-
tion (SemEval-2019).

Lior Rokach. 2010. Ensemble-based classifiers. Artifi-
cial Intelligence Review, 33(1-2):1–39.

Warren S Sarle. 1996. Stopped training and other
remedies for overfitting. Computing science and s-
tatistics, pages 352–360.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Marijn F Stollenga, Wonmin Byeon, Marcus Liwick-
i, and Juergen Schmidhuber. 2015. Parallel multi-
dimensional lstm, with application to fast biomedi-
cal volumetric image segmentation. In Advances in
neural information processing systems, pages 2998–
3006.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1480–1489.

Ye Zhang and Byron Wallace. 2015. A sensitivity anal-
ysis of (and practitioners’ guide to) convolutional
neural networks for sentence classification. arXiv
preprint arXiv:1510.03820.

Chunting Zhou, Chonglin Sun, Zhiyuan Liu, and Fran-
cis Lau. 2015. A c-lstm neural network for text clas-
sification. arXiv preprint arXiv:1511.08630.


