



















































Relation extraction pattern ranking using word similarity


Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 25–32,
Denver, Colorado, June 1, 2015. c©2015 Association for Computational Linguistics

Relation extraction pattern ranking using word similarity

Konstantinos Lambrou-Latreille
(1) École Polytechnique de Montréal

(2) Centre de Recherche Informatique de Montréal
Montréal, Québec, Canada

konstantinos.lambrou-latreille@polymtl.ca

Abstract

Our thesis proposal aims at integrating word
similarity measures in pattern ranking for rela-
tion extraction bootstrapping algorithms. We
note that although many contributions have
been done on pattern ranking schemas, few
explored the use of word-level semantic sim-
ilarity. Our hypothesis is that word similar-
ity would allow better pattern comparison and
better pattern ranking, resulting in less seman-
tic drift commonly problematic in bootstrap-
ping algorithms. In this paper, as a first step
into this research, we explore different pat-
tern representations, various existing pattern
ranking approaches and some word similarity
measures. We also present a methodology and
evaluation approach to test our hypothesis.

1 Introduction

In this thesis, we look at the problem of information
extraction from the web; more precisely at the prob-
lem of extracting structured information, in the form
of triples (predicate, subject, object), e.g. (Object-
MadeFromMaterial, table, wood) from unstructured
text. This topic of Relation Extraction (RE), is a cur-
rent and popular research topic within NLP, given
the large amount of unstructured text on the WWW.

In the literature, machine learning algorithms
have shown to be very useful for RE from tex-
tual resources. Although supervised (Culotta and
Sorensen, 2004; Bunescu and Mooney, 2005)
and unsupervised learning (Hasegawa et al., 2004;
Zhang et al., 2005) have been used for RE, in this
thesis, we will focus on semi-supervised bootstrap-
ping algorithms.

In such algorithms (Brin, 1999; Agichtein and
Gravano, 2000; Alfonseca et al., 2006a), the input
is a set of related pairs called seed instances (e.g.,
(table,wood), (bottle, glass)) for a specific relation
(e.g., ObjectMadeFromMaterial). These seed in-
stances are used to collect a set of candidate pat-
terns representing the relation in a corpus. A sub-
set containing the best candidate patterns is added in
the set of promoted patterns. The promoted patterns
are used to collect candidate instances. A subset
containing the best candidate instances is selected to
form the set of promoted instances. The promoted
instances are either added to the initial seed set or
used to replace it. With the new seed set, the algo-
rithm is repeated until a stopping criterion is met.

The advantage of bootstrapping algorithms is that
they require little human annotation. Unfortunately,
the system may introduce wrongly extracted in-
stances. Due to its iterative approach, errors can
quickly cumulate in the next few iterations; there-
fore, precision will suffer. This problem is called
semantic drift. Different researchers have studied
how to counter semantic drift by using better pattern
representations, by filtering unreliable patterns, and
filtering wrongly extracted instances (Brin, 1999;
Agichtein and Gravano, 2000; Alfonseca et al.,
2006a). Nevertheless, this challenge is far from be-
ing resolved, and we hope to make a contribution in
that direction.

The semantic drift is directly related to which can-
didate patterns become promoted patterns. A cru-
cial decision at that point is how to establish pat-
tern confidence so as to rank the patterns. There are
many ways to estimate the confidence of a pattern.

25



Blohm et al. (2007) identified general types of pat-
tern filtering functions for well-known systems. As
we review pattern ranking approaches, we note that
many include a notion of ”resemblance”, as either
comparing patterns between successive iterations,
or comparing instances generated at an iteration to
instances in the seed set, etc. Although this no-
tion of resemblance seems important to many rank-
ing schemas, we do not find much research which
combines word similarity approaches within pattern
ranking. This is where we hope to make a research
contribution and where our hypothesis lies, that us-
ing word similarity would allow for better pattern
ranking.

In order to suggest better pattern ranking ap-
proaches incorporating word similarity, we need to
look at the different pattern representations sug-
gested in the literature and understand how they
affect pattern similarity measures. This is intro-
duced in Section 2. Then, section 3 provides a non-
exhaustive survey of pattern ranking approaches
with an analysis of commonality and differences;
Section 4 presents a few word similarity approaches;
Section 5 presents the challenges we face, as well
as our methodology toward the validation of our hy-
pothesis; Section 6 briefly explores other anticipated
issues (e.g. seed selection) in relation to our main
contribution and Section 7 presents the conclusion.

2 Pattern representation

In the literature, pattern representations are classi-
fied as lexical or syntactic.

Lexical patterns represent lexical terms around
a relation instance as a pattern. For relation in-
stance (X,Y) where X and Y are valid noun phrases,
Brin (1999), Agichtein and Gravano (2000), Pasca
et al. (2006), Alfonseca et al. (2006a) take N words
before X, N words after Y and all intervening words
between X and Y to form a pattern (e.g., well-known
author X worked on Y daily.). Extremes for the
choice of N exist, as in the CPL subsystem of
NELL (Carlson et al., 2010) setting N = 0 and
the opposite in Espresso (Pantel and Pennacchiotti,
2006) where the whole sentence is used.

Syntactic patterns convert a sentence containing
a relation instance to a structured form such as a
parse tree or a dependency tree. Yangarber (2003)

and Stevenson and Greenwood (2005) use Subject-
Verb-Object (SVO) dependency tree patterns such
as [Company appoint Person] or [Person quit]. Cu-
lotta (2004) uses full dependency trees on which
a tree kernel will be used to measure similarity.
Bunescu and Mooney (2005) and Sun and Grish-
man (2010) use the shortest dependency path (SDP)
between a relation instance in the dependency tree
as a pattern (e.g., ”nsubj←met→ prep in”). Zhang
et al. (2014) add a semantic constraint to the SDP;
they define the semantic shortest dependency path
(SSDP) as a SDP containing at least one trigger
word representing the relation, if any. Trigger words
are defined as words most representative of the tar-
get relation (e.g. home, house, live, for the relation
PersonResidesIn).

We anticipate the use of word similarity to be
possible when comparing either lexical or syntac-
tic patterns, adapting to either words in sequence,
or nodes within parse or dependency trees. In fact,
as researchers have explored pattern generalization,
some have already looked at ways of grouping sim-
ilar words. For example, Alfonseca et al. (2006a)
present a simple algorithm to generalize the set
of lexical patterns using an edit-distance similarity.
Also, Pasca et al. (2006) add term generalization
to a pattern representation similar to Agichtein and
Gravano (2000); terms are replaced with their cor-
responding classes of distributionally similar words,
if any (e.g., let CL3 = {March, October, April,...} in
the pattern CL3 00th : X’s Birthday (Y)).

3 Pattern ranking approaches

We now survey pattern ranking algorithms to bet-
ter understand in which ones similarity measures
would be more likely to have an impact. We
follow a categorization introduced in Blohm et
al. (2007) as they quantified the impact of different
relation pattern/instance filtering functions on their
generic bootstrapping algorithm. The filtering func-
tions proposed by Brin (1999), Agichtein and Gra-
vano (2000), Pantel and Pennacchiotti (2006) and
Etzioni et al. (2004) were described in their work.

Although non-exhaustive, our survey includes
further pattern ranking approaches found in the lit-
erature, in order to best illustrate Blohm’s different
categories. A potential use of those categories would

26



be to define a pattern ranking measure composed of
voting experts representing each category. A com-
bination of these votes might provide a better confi-
dence measure for a pattern.

We define the following notation, as to allow the
description of the different measures in a coherent
way. Let p be a pattern and i be an instance; I is the
set of promoted instances; P is the set of promoted
patterns;H(p) is the set of unique instances matched
by p; K(i) is the set of unique patterns matching
i; count(i, p) is the number of times p matches i;
count(p) is the number of p occurs in a corpus; S is
the set of seed instances.

3.1 Syntactic assessment
This filtering assessment is purely based on the syn-
tactic criteria (e.g., length, structure, etc.) of the pat-
tern. Brin (1999) uses the length of the pattern to
measure its specificity.

3.2 Pattern comparison
Blohm et al. (2007) named this category inter-
pattern comparison. Their intuition was that can-
didate patterns could be rated based on how similar
their generated instances are in comparison to the
instances generated by the promoted patterns. We
generalize this category to also include rating of can-
didate patterns based directly on their semantic sim-
ilarity with promoted pattern.

Stevenson and Greenwood (2005) assign a score
on a candidate pattern based on the similarity with
promoted patterns. The pattern scoring function
uses the Jiang and Conrath (1997) WordNet-based
word similarity for pattern similarity. They rep-
resent the SVO pattern as a vector (e.g., [sub-
ject COMPANY, verb fired, object ceo], or [sub-
ject chairman, verb resign]). The similarity be-
tween two pattern vectors is measured as :

sim(~a,~b) =
~a×W × ~bT
|~a| × |~b|

(1)

where W is a matrix that contains the word sim-
ilarity between every possible element-filler pairs
(e.g., subject COMPANY, verb fired, object ceo)
contained in every SVO pattern extracted from a cor-
pus. The top-N (e.g., 4) patterns with a score larger
than 95% are promoted.

Zhang et al. (2014) defines a bottom-up kernel
(BUK) to filter undesired relation patterns. The

BUK measures the similarity between two depen-
dency tree patterns. The system accepts new pat-
terns that are the most similar to seed patterns. The
BUK defines a matching function t and a similarity
function k on dependency trees. Let dep be the pair
(rel, w) where rel is the dependency relation and w
is the word of the relation (e.g., (nsubj, son)). The
matching function is defined as:

t(dep1, dep2) =

{
1 if dep1.w, dep2.w ∈Wtr
0 otherwise

(2)

where Wtr is the set of trigger words for the target
relation. The similarity function is defined as:

k(dep1, dep2) =
γ1 + γ2 if dep1.rel = dep2.rel && dep1.w = dep2.w
γ1 if dep1.w = dep2.w
0 otherwise

(3)

where γ1 and γ2 are manually defined weights for
attributes dep.w and dep.rel respectively. The word
comparison is string-based.

3.3 Support-based assessment
This ranking assessment estimates the quality of a
pattern based on the set of occurrences/patterns that
generated this pattern. This assessment is usually
used for patterns that were created by a general-
ization procedure. For example, if pattern X BE
mostly/usually made of/from Y was generated by pat-
terns X is usually made of Y and X are mostly made
from Y, then the quality of the generalized pattern
will be based on the last two patterns. Brin (1999)
filters patterns if (specificity(p) × n) > t, where
n is the occurrence count of pattern p applied in a
corpus and t is a manually set threshold.

3.4 Performance-based assessment
The quality of a candidate pattern can be estimated
by the comparing its correctly produced instances
with the set of promoted instances.

Blohm et al. (2007) defines a precision formula
similar to Agichtein and Gravano (2000) to approx-
imate a performance-based precision:

prec(p) =
|H(p) ∩ S|
|H(p)| (4)

Alfonseca et al. (2006b) propose a procedure to
measure the precision of candidate patterns in order

27



to filter overly-general patterns. For every relation,
and every hook X and target Y of the set of pro-
moted instances (X,Y), a hook and target corpus is
extracted from corpus C; C contains only sentences
which contain X or Y. For every pattern p, instances
of H(p) are extracted. Then, a set of heuristics label
every instance as correct/incorrect. The precision of
p is number of correct extracted instances divided by
the total number of extracted instances.

NELL (Carlson et al., 2010) ranks relation pat-
terns by their precision:

prec(p) =
∑
i∈I count(i, p)
count(p)

(5)

Sijia et al. (2013) filters noisy candidate relation
patterns that generate instances which appear in the
seed set of relations other than the target relation.

3.5 Instance-Pattern correlation
Pattern quality can be assessed by measuring its cor-
relation with the set of promoted instances. These
measures estimate the correlation by counting pat-
tern occurrences, promoted instance occurrences,
and pattern occurrences with a specific promoted in-
stance.

Blohm et al. (2007) classified Espresso (Pantel
and Pennacchiotti, 2006) and KnowItAll (Etzioni et
al., 2004) in this category.

Pantel et Pennacchiotti (2006) ranks candidate re-
lation patterns by the following reliability score:

rπ(p) =

∑
i∈I
(
pmi(i,p)
maxpmi

× rl(i)
)

|I| (6)

where maxpmi is the maximum PMI between all
pattern and all instances, and pmi(i, p) can be esti-
mated using the following formula:

pmi(i, p) = log
( |x, p, y|
|x, ∗, y| × |∗, p, ∗|

)
(7)

where i is an instance (x,y), |x, p, y| is the occur-
rence of pattern p with terms x and y and (*) repre-
sents a wildcard. The reliability of an instance rl(i)
is defined as:

rl(i) =

∑
p∈P

(
pmi(i,p)
maxpmi

× rπ(p)
)

|P | (8)

Since rl(i) and rπ(p) are defined recursively,
rl(i) = 1 for any seed instance. The top-N patterns

are promoted where N is the number of patterns of
the previous bootstrapping iteration plus one.

Sun and Grishman (2010) accept the top-N ranked
candidate pattern by the following confidence for-
mula:

Conf(p) =
Sup(p)
|H(p)| × logSup(p) (9)

where Sup(p) =
∑

i∈H(p)Conf(i) is the support
candidate pattern p can get from the set of matched
instances. Every relation instance in Sun and Gr-
ishman (2010) has a cluster membership, where a
cluster contains similar patterns. The confidence of
an newly extracted instance i is defined as:

Conf(i) = 2× Semi Conf(i)×Cluster Conf(i)Semi Conf(i)+Cluster Conf(i) (10)

Semi Conf(i) = 1−∏p∈K(p) (1− Prec(p)) (11)
Cluster Conf(i) = Prob(i ∈ Ct)

=

∑
p∈Ct count(i, p)
|K(i)|

(12)

whereCt is the target cluster where the patterns of
the target relation belong, Semi Conf(i) is defined
as the confidence given by the patterns matching the
candidate relation instance and Cluster Conf(i) is
defined how strongly a candidate instance is associ-
ated with the target cluster.

4 Word similarity

Within the pattern ranking survey, we often saw
the idea of comparing patterns and/or instances, but
only once, was there a direct use of word similarity
measures. Stevenson and Greenwood (2005) assign
a score to a candidate pattern based on its similarity
to promoted patterns using a WordNet-based word
similarity measure (Jiang and Conrath, 1997). This
measure is only one among many WordNet-based
approaches, as can be found in (Lesk, 1986; Wu
and Palmer, 1994; Resnik, 1995; Jiang and Conrath,
1997; Lin, 1998; Leacock and Chodorow, 1998;
Banerjee and Pedersen, 2002).

There are limitations to these approaches, mainly
that WordNet (Miller, 1995), although large, is still
incomplete. Other similarity approaches are corpus-
based (e.g. (Agirre et al., 2009)) where the distribu-
tional similarity between words is measured. Words

28



are no longer primitives, but they are represented by
a feature vector. The feature vector could contain the
co-occurrences, the syntactic dependencies, etc. of
the word with their corresponding frequencies from
a corpus. The cosine similarity (among many pos-
sible measures) between the feature vector of two
words indicates their semantic similarity.

Newer approaches to word similarity are based
on neural network word embeddings. Mikolov et
al. (2013) present algorithms to learn those dis-
tributed word representations which can then be
compared to provide word similarity estimations.

Word similarity could be in itself the topic of a
thesis. Therefore, we will not attempt at develop-
ing new word similarity measures, but rather we will
search for measures which are intrinsically good and
valuable for the pattern ranking task. The few men-
tioned above are a good start toward a more ex-
tensive survey. The methods found can be evalu-
ated on existing datasets such as RG (Rubenstein
and Goodenough, 1965), MC (Miller and Charles,
1991), WordSim353 (Finkelstein et al., 2001; Agirre
et al., 2009), MTurk (Radinsky et al., 2011) and
MEN (Bruni et al., 2013) datasets. However, these
datasets are limited, since they contain only nouns
(except MEN). When using word similarity in pat-
tern ranking schemas, we will likely want to mea-
sure similarity between nouns, verbs, adjectives and
adverbs. Still, these datasets provide a good starting
point for evaluation of word similarity.

5 Word similarity in pattern ranking

The hypothesis of our research is that the use of
word similarity will allow better pattern ranking to
better prevent semantic drift. We face three main
challenges in supporting this hypothesis. First, we
need to understand the interdependence of the three
elements presented in the three previous sections:
pattern representation, pattern confidence estima-
tion, and word similarity. Second, we need to devise
an appropriate set-up to perform our bootstrapping
approach. Third, we need to properly evaluate the
role of the different variations in preventing seman-
tic drift.

An important exploration will be to decide where
the word similarity has the largest potential. For
example, in the work of Stevenson and Green-

wood (2005), similarity is directly applied on parts
of the triples found (Subject, Verb predicate or Ob-
ject), or in the work of Zhang et al. (2014), word
similarity would be integrated in the matching and
similarity functions over dependency trees, instead
of using string equality.

As we see, the integration of word similarity mea-
sures would be different depending on the type of
pattern representation used. Furthermore, in some
representation, there is already a notion of pattern
generalisation, such as in the work of Pasca et
al. (2006), where words are replaced with more gen-
eral classes, if any. In such case, word similarity
measures are used at the core of the pattern repre-
sentation, and will further impact pattern ranking.

As we will eventually be building a complex sys-
tem, we intend to follow a standard methodology of
starting with a baseline system for which we have
an evaluation, and then further evaluate the differ-
ent variations to measure their impact. As the num-
ber of combination of possible variations will be
too large, time will be spent also on partial evalua-
tion, to determine most promising candidates among
word similarity measures, and/or pattern representa-
tion and/or pattern confidence estimation, to under-
stand strength and weaknesses of each aspect inde-
pendently of the others.

Our proposed methodology is to take promising
ranking approaches among the one presented in Sec-
tion 3, and promising pattern representations from
what was presented in Section 2. We can evalu-
ate their combined performance throughN different
iteration intervals and incorporate different similar-
ity measures (some best measures chosen from the
evaluation on known datasets) to measure the per-
formance of the system.

As our baseline system, we are inspired by CPL
subsystem of NELL (Carlson et al., 2010) since it
is one of the largest, currently active, bootstrapping
system in the literature. As in NELL, we will use
ClueWeb1 as our corpus, and for the set of relations,
we will use the same seed instances and relations as
in the evaluation of NELL (Carlson et al., 2010).

As for the bootstrapping RE system, to evaluate
the precision, we will randomly sample knowledge
from the knowledge base and evaluate them by sev-

1http://www.lemurproject.org/clueweb09/

29



eral human judges. The extracted knowledge could
be validated using a crowdsourcing application such
as MTurk. This method is based on NELL (Carlson
et al., 2010). To evaluate its recall, we have to con-
centrate on already annotated relations. For exam-
ple, Pasca et al. (2006) evaluates the relation Person-
BornIN-Year. As a Gold Standard, 6617 instances
were automatically extracted from Wikipedia. In-
stead of measuring recall for specific relation, we
could use relative recall (Pantel et al., 2004; Pan-
tel and Pennacchiotti, 2006). We can evaluate our
contributions by the relative recall of system A (our
system) given system B (baseline).

6 Related issues in pattern ranking

Our main contribution on the impact of word simi-
larity on pattern ranking will necessarily bring for-
ward other interesting questions that we will address
within our thesis.

6.1 Choice of seed
As we saw, pattern ranking is often dependent on
the comparison of instances found from one iteration
to the next. At iteration 0, we start with a seed of
instances. We can imagine that the manual selection
of these seeds will have an impact on the following
decisions. As our similarity measures are used to
compare candidate instances to seed instances, and
as we will start with NELL seed set, we will want to
evaluate its impact on the bootstrapping process.

It was shown that the performance of bootstrap-
ping algorithms highly depend on the seed instance
selection (Kozareva and Hovy, 2010). Ehara et
al. (2013) proposed an iterative approach where un-
labelled instances are chosen to be labelled depend-
ing on their similarity with the seed instances and
are added in the seed set.

6.2 Automatic selection of patterns
Something noticeable among our surveyed pattern
ranking approaches is the inclusion of empirically
set thresholds that will definitely have an impact on
the semantic drift, but which impact is not discussed.
Most authors (e.g (Carlson et al., 2010; Sun and Gr-
ishman, 2010; McIntosh and Yencken, 2011; Zhang
et al., 2014) among recent ones) select the top-N
best ranked patterns to be promoted to next iteration.
Other authors (Pasca et al., 2006; Dang and Aizawa,

2008; Carlson et al., 2010) select the top-M ranked
instances to add in the seed set for the next iteration.
Other authors (Brin, 1999; Agichtein and Gravano,
2000; Sijia et al., 2013) only apply a filtering step
without limiting pattern/instance selection.

In our work, including word similarity within pat-
tern ranking will certainly impact the decision on the
number of patterns to be promoted. We hope to con-
tribute in developing a pattern selection mechanism
that will be based on the pattern confidence them-
selves rather than on an empirically set N or M.

7 Conclusion

In this paper, we have presented our research pro-
posal, aiming at determining the impact of employ-
ing word similarity measures within pattern ranking
approaches in bootstrapping systems for relation ex-
traction. We presented two aspects of pattern rank-
ing on which the integration of word similarity will
be dependent, that of pattern representation and pat-
tern ranking schemas. We showed that there are
minimally lexical and syntactic pattern representa-
tions on which different methods of generalizations
can be applied. We performed a non-exhaustive sur-
vey of pattern ranking measures classified in five dif-
ferent categories. We also briefly looked into differ-
ent word similarity approaches.

This sets the ground for the methodology that we
will pursue, that of implementing a baseline boot-
strapping system (inspired by NELL, and working
with ClueWeb as a corpus), and then measuring the
impact of modifying the pattern representation and
the pattern ranking approaches, with and without the
use of word similarity measures. There is certainly
a complex intricate mutual influence of the preced-
ing aspects which we need to look into. Lastly, we
briefly discussed two related issues: the choice of
seed set and better estimation of number of patterns
to promote.

Acknowledgments

This work has seen the day with the help and ad-
vice of Caroline Barrière, my research supervisor
at CRIM. This research project is partly funded by
an NSERC grant RDCPJ417968-11, titled Toward a
second generation of an automatic product coding
system.

30



References
Eugene Agichtein and Luis Gravano. 2000. Snowball:

Extracting Relations from Large Plain-Text Collec-
tions. In Proceedings of the fifth ACM conference on
Digital libraries - DL ’00, pages 85–94, New York,
New York, USA. ACM Press.

Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Paşca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and WordNet-based approaches. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics on - NAACL
’09, pages 19–27, Morristown, NJ, USA. Association
for Computational Linguistics.

Enrique Alfonseca, Pablo Castells, Manabu Okumara,
and Maria Ruiz-Casado. 2006a. A Rote Ex-
tractor with Edit Distance-based Generalisation and
Multi-corpora Precision Calculation. In COLING-
ACL’06 Proceedings of the COLING/ACL Poster Ses-
sion, pages 9–16, Morristown, NJ, USA. Association
for Computational Linguistics.

Enrique Alfonseca, Maria Ruiz-Casado, Manabu Oku-
mura, and Pablo Castells. 2006b. Towards Large-
scale Non-taxonomic Relation Extraction : Estimating
the Precision of Rote Extractors. In Proceedings of
the second workshop on ontology learning and popu-
lation, Coling-ACL’2006, pages 49–56.

Satanjeev Banerjee and Ted Pedersen. 2002. An Adapted
Lesk Algorithm for Word Sense Disambiguation Us-
ing WordNet. In Alexander Gelbukh, editor, Com-
putational linguistics and intelligent text processing,
volume 2276 of Lecture Notes in Computer Science,
pages 136–145. Springer Berlin Heidelberg, Berlin,
Heidelberg, February.

Sebastian Blohm, Philipp Cimiano, and E Stemle. 2007.
Harvesting Relations from the Web - Quantifiying the
Impact of Filtering Functions. In Proceedings of the
National Conference on Artificial Intelligence, pages
1316–1321.

Sergey Brin. 1999. Extracting Patterns and Relations
from the World Wide Web. In Paolo Atzeni, Al-
berto Mendelzon, and Giansalvatore Mecca, editors,
The World Wide Web and Databases, Lecture Notes
in Computer Science, chapter Extracting, pages 172–
183. Springer Berlin Heidelberg, Berlin, Heidelberg,
March.

Elia Bruni, Nam Khan Tran, and Marco Baroni. 2013.
Multimodal distributional semantics. Journal of Arti-
cifial Intelligence Research, 48:1–47.

Razvan C Bunescu and Raymond J Mooney. 2005. A
Shortest Path Dependency Kernel for Relation Extrac-
tion. In Proceedings of Human Language Technol-

ogy Conference and Conference on Empirical Methods
in Naural Language Processing (HLT/EMNLP), pages
724–731, Vancouver, Canada.

Andrew Carlson, Justin Betteridge, and Bryan Kisiel.
2010. Toward an Architecture for Never-Ending Lan-
guage Learning. In Proceedings of the Conference on
Artificial Intelligence (AAAI), pages 1306–1313.

Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
the 42nd Annual Meeting on Association for Computa-
tional Linguistics - ACL ’04, pages 423–429, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.

VB Dang and Akiko Aizawa. 2008. Multi-class named
entity recognition via bootstrapping with dependency
tree-based patterns. In Proceedings of the 12th Pacific-
Asia conference on Advances in knowledge discovery
and data mining, pages 76–87.

Yo Ehara, Issei Sato, Hidekazu Oiwa, and Hiroshi Nak-
agawa. 2013. Understanding seed selection in boot-
strapping. In Proceedings of the TextGraphs-8 Work-
shop, pages 44–52, Seattle, Washington, USA.

Oren Etzioni, Michael Cafarella, Doug Downey, Stanley
Kok, Ana-maria Popescu, Tal Shaked, Stephen Soder-
land, Daniel S Weld, and Alexander Yates. 2004.
Web-scale information extraction in knowitall: (pre-
liminary results). In Proceedings of the 13th confer-
ence on World Wide Web - WWW ’04, page 100, New
York, New York, USA. ACM Press.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context. In Proceed-
ings of the tenth international conference on World
Wide Web - WWW ’01, pages 406–414, New York,
New York, USA. ACM Press.

Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics - ACL ’04, pages 415–422, Morristown, NJ,
USA. Association for Computational Linguistics.

JJ Jiang and DW Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In In
the Proceedings of ROCLING X, Taiwan, pages 19–33.

Zornitsa Kozareva and Eduard Hovy. 2010. Not All
Seeds Are Equal : Measuring the Quality of Text Min-
ing Seeds. In Proceeding HLT ’10 Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 618–626.

Claudia Leacock and Martin Chodorow. 1998. Combin-
ing Local Context and WordNet Similarity for Word
Sense Identification. In Christiane Fellbaum, editor,

31



WordNet: An electronic lexical database., pages 265–
283. MIT Press.

Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries. In Proceedings
of the 5th annual international conference on Sys-
tems documentation - SIGDOC ’86, pages 24–26, New
York, New York, USA. ACM Press.

Dekang Lin. 1998. An Information-Theoretic Defini-
tion of Similarity. In ICML ’98 Proceedings of the Fif-
teenth International Conference on Machine Learning,
pages 296–304.

T McIntosh and Lars Yencken. 2011. Relation guided
bootstrapping of semantic lexicons. In Proceeding
HLT ’11 Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: short papers - Volume
2, pages 266–270.

Tomas Mikolov, Greg Corrado, Kai Chen, and Jeffrey
Dean. 2013. Efficient Estimation of Word Represen-
tations in Vector Space. In Proceedings of the Interna-
tional Conference on Learning Representations (ICLR
2013), pages 1–12.

George A. Miller and Walter G. Charles. 1991. Contex-
tual correlates of semantic similarity. Language and
Cognitive Processes, 6(1):1–28.

George A. Miller. 1995. WordNet: A Lexical Database
for English. Communications of the ACM, 38(11):39–
41.

Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the ACL - ACL ’06,
pages 113–120, Morristown, NJ, USA. Association for
Computational Linguistics.

Patrick Pantel, Deepak Ravichandran, and Eduard Hovy.
2004. Towards terascale knowledge acquisition. In
Proceedings of the 20th international conference on
Computational Linguistics COLING 04, pages 771–
777.

M Pasca, Dekang Lin, Jeffrey Bigham, Andrei Lifchits,
and A Jain. 2006. Organizing and searching the world
wide web of facts-step one: the one-million fact ex-
traction challenge. AAAI, 6:1400–1405.

Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich,
and Shaul Markovitch. 2011. A word at a time: com-
puting word relatedness using temporal semantic anal-
ysis. In Proceedings of the 20th international confer-
ence on World wide web - WWW ’11, page 337, New
York, New York, USA. ACM Press.

Philip Resnik. 1995. Using IC to Evaluation the Seman-
tic Similarity in a Taxonomy. In Proceeding IJCAI’95
Proceedings of the 14th international joint conference
on Artificial intelligence - Volume 1, pages 448–453.

Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communications
of the ACM, 8(10):627–633, October.

Chen Sijia, Li Yan, and Chen Guang. 2013. Reduc-
ing semantic drift in bootstrapping for entity relation
extraction. In Proceedings 2013 International Con-
ference on Mechatronic Sciences, Electric Engineer-
ing and Computer (MEC), pages 1947–1950. Ieee, De-
cember.

Mark Stevenson and Mark A Greenwood. 2005. A se-
mantic approach to IE pattern induction. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics - ACL ’05, pages 379–386,
Morristown, NJ, USA. Association for Computational
Linguistics.

Ang Sun and Ralph Grishman. 2010. Semi-supervised
Semantic Pattern Discovery with Guidance from Un-
supervised Pattern Clusters. In Proceedings of the
23rd International Conference on Computational Lin-
guistics: Posters, pages 1194–1202, Beijing.

Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational Lin-
guistics -, pages 133–138, Morristown, NJ, USA. As-
sociation for Computational Linguistics.

Roman Yangarber. 2003. Counter-training in discovery
of semantic patterns. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - ACL ’03, volume 1, pages 343–350, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.

Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and
Chew Lim Tan. 2005. Discovering Relations Between
Named Entities form a Large Raw Corpus Using Tree
Similarity-based Clustering. In Robert Dale, Kam-Fai
Wong, Jian Su, and Oi Yee Kwong, editors, Natural
Language Processing – IJCNLP 2005, Lecture Notes
in Computer Science, pages 378–389. Springer Berlin
Heidelberg, Berlin, Heidelberg.

Chunyun Zhang, Weiran Xu, Sheng Gao, and Jun Guo.
2014. A bottom-up kernel of pattern learning for re-
lation extraction. In The 9th International Symposium
on Chinese Spoken Language Processing, pages 609–
613. IEEE, September.

32


