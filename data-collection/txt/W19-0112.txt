








































Jabberwocky Parsing: Dependency Parsing with Lexical Noise

Jungo Kasai ⇤
University of Washington

jkasai@cs.washington.edu

Robert Frank
Yale University

robert.frank@yale.edu

Abstract

Parsing models have long benefited from the
use of lexical information, and indeed current
state-of-the art neural network models for de-
pendency parsing achieve substantial improve-
ments by benefiting from distributed represen-
tations of lexical information. At the same
time, humans can easily parse sentences with
unknown or even novel words, as in Lewis
Carroll’s poem Jabberwocky. In this paper, we
carry out jabberwocky parsing experiments,
exploring how robust a state-of-the-art neu-
ral network parser is to the absence of lex-
ical information. We find that current pars-
ing models, at least under usual training reg-
imens, are in fact overly dependent on lexi-
cal information, and perform badly in the jab-
berwocky context. We also demonstrate that
the technique of word dropout drastically im-
proves parsing robustness in this setting, and
also leads to significant improvements in out-
of-domain parsing.

1 Introduction

Since the earliest days of statistical parsing, lexi-
cal information has played a major role (Collins,
1996, 1999; Charniak, 2000). While some of
the performance gains that had been derived from
lexicalization can be gotten in other ways (Klein
and Manning, 2003), thereby avoiding increases
in model complexity and problems in data sparsity
(Fong and Berwick, 2008), recent neural network
models of parsing across a range of formalisms
continue to use lexical information to guide pars-
ing decisions (constituent parsing Dyer et al.
(2016)); dependency parsing: Chen and Manning
(2014); Kiperwasser and Goldberg (2016); Dozat
and Manning (2017); CCG parsing: Ambati et al.
(2016); TAG parsing: Kasai et al. (2018); Shi and

⇤Work done at Yale University.

Lee (2018)). These models exploit lexical infor-
mation in a way that avoids some of the data spar-
sity issues, by making use of distributed represen-
tations (i.e., word embeddings) that support gen-
eralization across different words.

While humans certainly make use of lexical
information in sentence processing (MacDonald
et al., 1994; Trueswell and Tanenhaus, 1994), it
is also clear that we are able to analyze sentences
in the absence of known words. This can be seen
most readily by our ability to understand Lewis
Carroll’s poem, Jabberwocky (Carroll, 1883), in
which open class items are replaced by non-words.

Twas brillig, and the slithy toves
Did gyre and gimble in the wabe;
All mimsy were the borogoves,
And the mome raths outgrabe

Work in neurolinguistics and psycholinguistics
has demonstrated the human capacity for unlexi-
calized parsing experimentally, showing that hu-
mans can analyze syntactic structure even in pres-
ence of pseudo-words (Stromswold et al., 1996;
Friederici et al., 2000; Kharkwal, 2014).

The word embeddings used by current lexical-
ized parsers are of no help in sentences with nonce
words. Yet, it is at present unknown the degree
to which these parsers are dependent on the in-
formation contained in these embeddings. Pars-
ing evaluation on such nonce sentences is, there-
fore, critical to bridge the gap between cognitive
models and data-driven machine learning models
in sentence processing. Moreover, understanding
the degree to which parsers are dependent upon
lexical information is also of practical importance.
It is advantageous for a syntactic parser to gen-
eralize well across different domains. Yet, heavy
reliance upon lexical information could have detri-
mental effects on out-of-domain parsing because

113
Proceedings of the Society for Computation in Linguistics (SCiL) 2019, pages 113-123.

New York City, New York, January 3-6, 2019



lexical input will carry genre-specific information
(Gildea, 2001).

In this paper, we investigate the contribution of
lexical information (via distributed lexical repre-
sentations) by focusing on a state-of-the-art graph-
based dependency parsing model (Dozat and Man-
ning, 2017) in a series of controlled experiments.
Concretely, we simulate jabberwocky parsing by
adding noise to the representation of words in
the input and observe how parsing performance
varies. We test two types of noise: one in which
words are replaced with an out-of-vocabulary
word without a lexical representation, and a sec-
ond in which words are replaced with others (with
associated lexical representations) that match in
their Penn TreeBank (PTB)-style fine-grained part
of speech. The second approach is similar to the
method that Gulordava et al. (2018) propose to as-
sess syntactic generalization in LSTM language
models.

In both cases, we find that the performance of
the state-of-the-art graph parser dramatically suf-
fers from the noise. In fact, we show that the
performance of a lexicalized graph-based parser
is substantialy worse than an unlexicalized graph-
based parser in the presence of lexical noise, even
when the lexical content of frequent or function
words is preserved. This dependence on lexical
information presents a severe challenge when ap-
plying the parser to a different domain or hetero-
geneous data, and we will demonstrate that indeed
parsers trained on the PTB WSJ corpus achieve
much lower performance on the Brown corpus.

On the positive side, we find that word dropout
(Iyyer et al., 2015), applied more aggressively
than is commonly done (Kiperwasser and Gold-
berg, 2016; de Lhoneux et al., 2017; Nguyen et al.,
2017; Ji et al., 2017; Dozat and Manning, 2017;
Bhat et al., 2017; Peng et al., 2017, 2018), reme-
dies the susceptibility to lexical noise. Further-
more, our results show that models trained on the
PTB WSJ corpus with word dropout significantly
outperform those trained without word dropout in
parsing the out-of-domain Brown corpus, confirm-
ing the practical significance of jabberwocky pars-
ing experiments.

2 Parsing Models

Here we focus ourselves on a graph-based parser
with deep biaffine attention (Dozat and Man-
ning, 2017), a state-of-the-art graph-based parsing

Figure 1: Biaffine parsing architecture. W and p denote
the word and POS embeddings.

model, and assess its ability to generalize over lex-
ical noise.

Input Representations The input for each word
is the concatenation of a 100-dimensional embed-
ding of the word and a 25-dimensional embedding
of the PTB part of speech (POS). We initialize
all word embeddings to be a zero vector and the
out-of-vocabulary word is also mapped to a zero
vector in testing. The POS embeddings are ran-
domly initialized. We do not use any pretrained
word embeddings throughout our experiments in
order to encourage the model to find abstractions
over the POS embeddings. Importantly, PTB POS
categories also encode morphological features that
should be accessible in jabberwocky situations.
We also conducted experiments by taking as input
words, universal POS tags, and character CNNs
(Ma and Hovy, 2016). We observed similar pat-
terns throughout the experiments. While those ap-
proaches can more easily scale to other languages,
one concern is that the character CNNs can en-
code the identity of short words along side their
morphological properties, and therefore would not
achieve a pure jabberwocky situation. For this
reason, we only present results using fine-grained
POS.

Biaffine Parser Figure 1 shows our biaffine
parsing architecture. Following Dozat and
Manning (2017) and Kiperwasser and Goldberg
(2016), we use BiLSTMs to obtain features for
each word in a sentence. We first perform unla-

114



beled arc-factored scoring using the final output
vectors from the BiLSTMs, and then label the re-
sulting arcs. Specifically, suppose that we score
edges coming into the ith word in a sentence i.e.
assigning scores to the potential parents of the ith
word. Denote the final output vector from the BiL-
STM for the kth word by hk and suppose that hk
is d-dimensional. Then, we produce two vectors
from two separate multilayer perceptrons (MLPs)
with the ReLU activation:

h
arc-dep
k = MLP

(arc-dep)(hk)

harc-headk = MLP
(arc-head)(hk)

where harc-depk and h
arc-head
k are darc-dimensional

vectors that represent the kth word as a dependent
and a head respectively. Now, suppose the kth row
of matrix H (arc-head) is harc-headk . Then, the proba-
bility distribution si over the potential heads of the
ith word is computed by

si = softmax(H (arc-head)W (arc)h
arc-dep
i

+H (arc-head)b(arc))
(1)

where W (arc) 2 Rdarc⇥darc and b(arc) 2 Rdarc .
In training, we simply take the greedy maximum
probability to predict the parent of each word. In
the testing phase, we use the heuristics formulated
by Dozat and Manning (2017) to ensure that the
resulting parse is single-rooted and acyclic.

Given the head prediction of each word in the
sentence, we assign labeling scores using vectors
obtained from two additional MLP with ReLU.
For the kth word, we obtain:

h
rel-dep
k = MLP

(rel-dep)(hk)

hrel-headk = MLP
(rel-head)(hk)

where hrel-depk , h
rel-head
k 2 Rdrel . Let pi be the in-

dex of the predicted head of the ith word, and r be
the number of dependency relations in the dataset.
Then, the probability distribution `i over the possi-
ble dependency relations of the arc pointing from
the pith word to the ith word is calculated by:

`i = softmax(hT (rel-head)pi U
(rel)h

(rel-dep)
i

+W (rel)(h(rel-head)i + h
(rel-head)
pi ) + b

(rel))
(2)

where U (rel) 2 Rdrel⇥drel⇥r, W (rel) 2 Rr⇥drel , and
b(rel) 2 Rr.

We generally follow the hyperparameters cho-
sen in Dozat and Manning (2017). Specifically, we

use BiLSTMs layers with 400 units each. Input,
layer-to-layer, and recurrent dropout rates are all
0.33. The depth of all MLPs is 1, and the MLPs for
unlabeled attachment and those for labeling con-
tain 500 (darc) and 100 (drel) units respectively.
We train this model with the Adam algorithm to
minimize the sum of the cross-entropy losses from
head predictions (si from Eq. 1) and label predic-
tions (`i from Eq. 2) with ` = 0.001 and batch
size 100 (Kingma and Ba, 2015). After each train-
ing epoch, we test the parser on the dev set. When
labeled attachment score (LAS) does not improve
on five consecutive epochs, training ends.

3 Dropout as Regularization

Dropout regularizes neural networks by randomly
setting units to zero with some probability dur-
ing training (Srivastava et al., 2014). In addi-
tion to usual dropout, we consider applying word
dropout, a variant of dropout that targets entire
words and therefore entire rows in the lexical em-
bedding matrix (Iyyer et al., 2015). The intuition
we follow here is that a trained network will be
less dependent on lexical information, and more
successful in a jabberwocky context, if lexical in-
formation is less reliably present during training.
We consider a number of ways of word dropout.

Uniform Word Dropout Iyyer et al. (2015)
introduced the regularization technique of word
dropout in which lexical items are replaced by
the “unknown” word with some fixed probability
p and demonstrated that it improves performance
for the task of text classification. Replacing words
with the out-of-vocabulary word exposes the net-
works to out-of-vocabulary words that only occur
in testing. In our experiments, we will use word
dropout rates of 0.2, 0.4, 0.6, and 0.8.

Frequency-based Word Dropout Dropping
words with the same probability across the vo-
cabulary might not behave as an ideal regularizer.
The network’s dependence on frequent words or
function words is less likely to lead to overfitting
on the training data or corpus-specific properties,
as the distribution of such words is less variable
across different corpora. To avoid penalizing the
networks for utilizing lexical information (in the
form of word embeddings) for frequent words,
Kiperwasser and Goldberg (2016) propose that
word dropout should be applied to a word with a
probability inversely proportional to the word’s

115



frequency. Specifically, they drop out each word
w that appears #(w) times in the training data
with probability:

pw =
↵

#(w) + ↵
(3)

Kiperwasser and Goldberg (2016) set ↵ = 0.25,
which leads to relatively little word dropout. In
our WSJ training data, ↵ = 0.25 yields an ex-
pected word dropout rate of 0.009 in training, an
order of magnitude less than commonly used rates
in uniform word dropout. We experimented with
↵ = 0.25, 1, 40, 352, 2536 where the last three
values yield expected word dropout rates of 0.2,
0.4, and 0.6 (the uniform dropout rates we con-
sider). In fact, we will confirm that ↵ needs to be
much larger to significantly improve robustness to
lexical noise.

Open Class Word Dropout The frequency-
based word dropout scheme punishes the model
less for relying upon frequent words in the train-
ing data. However, some words may occur fre-
quently in the training data because of corpus-
specific properties of the data. For instance, in
the PTB WSJ training data, the word “company”
is the 40th most frequent word. If our aim is to
construct a parser that can perform well in dif-
ferent domains or across heterogeneous data, the
networks should not depend upon such corpus-
specific word senses. Hence, we propose to apply
word dropout only on open class (non-function)
words with a certain probability. We experimented
with open class word dropout rates of 0.38 and
0.75 (where open class words are zeroed out 38%
or 75% of the time), corresponding to the expected
overall dropout rates of 0.2 and 0.4 respectively.
To identify open class words in the data we used
the following criteria. We consider a word as an
open class word if and only if: 1) the gold UPOS is
“NOUN”, “PROPN”, “NUM”, “ADJ”, or “ADV”,
or 2) the gold UPOS is “VERB” and the the gold
XPOS (PTB POS) is not “MD” and the lemma is
not “’be”, “have”, or “do”.

4 Experiments

We test trained parsers on input that contains
two types of lexical noise, designed to as-
sess their ability to abstract away from idiosyn-
cratic/collocational properties of lexical items: 1)
colorless green noise and 2) jabberwocky noise.
The former randomly exchanges words with PTB

POS preserved, and the latter zeroes outs the em-
beddings for words (i.e. replacing words with an
out-of-vocabulary word). In either case, we keep
POS input to the parsers intact.

Colorless Green Experiments Gulordava et al.
(2018) propose a framework to evaluate the gen-
eralization ability of LSTM language models that
abstracts away from idiosyncratic properties of
words or collocational information. In particu-
lar, they generate nonce sentences by randomly
replacing words in the original sentences while
preserving part-of-speech and morphological fea-
tures. This can be thought of as a computational
approach to producing sentences that are “gram-
matical” yet meaningless, exemplified by the fa-
mous example “colorless green ideas sleep furi-
ously” (Chomsky, 1957). Concretely, for each
PTB POS category, we pick the 50 most frequent
words of that category in the training set and re-
place each word w in the test set by a word uni-
formly drawn from the 50 most frequent words
for w’s POS category. We consider three situa-
tions: 1) full colorless green experiments where
all words are replaced by random words, 2) top
100 colorless green experiments where all words
but the 100 most frequent words are replaced by
random words, and 3) open class colorless green
experiments where the input word is replaced by
a random word if and only if the word is an open
class word.1

Jabberwocky Experiments One potential
shortcoming with the approach above is that it
produces sentences which might violate con-
straints that are imposed by specific lexical items,
but which are not represented by the POS cate-
gory. For instance, this approach could generate
a sentence like “it stays the shuttle” in which the
intransitive verb “stay” takes an object (Gulordava
et al., 2018).2 Such a violation of argument
structure constraints could mislead parsers (as
well as language models studied in Gulordava
et al. (2018)) and we will show that is indeed the

1We use the same criteria for open class words as in the
open class word dropout.

2This shortcoming might be overcome by using lexical
resources like PropBank (Palmer et al., 2005) or NomBank
(Meyers et al., 2004) to guide word substitutions. In this pa-
per, we do not do this, follow Gulordava et al.’s approach for
the creation of colorless green sentences. We instead use the
jabberwocky manipulation to avoid creating sentences that
violate selectional constraints.

116



case.3 To address this issue, we also experiment
with jabberwocky noise, in which input word
vectors are zeroed out. This noise is equivalent
to replacing words with an out-of-vocabulary
word by construction. Because fine-grained POS
information is retained in the input to the parser,
the parser is still able to benefit from the kind of
morphological information present in Carroll’s
poem. We again consider three situations 1)
full jabberwocky experiments where all word
embeddings are zeroed out, 2) top 100 jabber-
wocky experiments where word embeddings for
all but the most frequent 100 words are zeroed
out, and 3) open class jabberwocky experiments
where the input word vector is zeroed out if and
only if the word is an open class word. Open
class jabberwocky experiments are the closest to
the situation when humans read Lewis Carroll’s
Jabberwocky.4

Out-of-domain experiments We also explore a
practical aspect of our experiments with lexical
noise. We apply our parsers that are trained on
the WSJ corpus to the Brown corpus and observe
how parsers with various configurations perform.5

Prior work showed that parsers trained on WSJ
yield degraded performance on the Brown cor-
pus (Gildea, 2001) despite the fact that the aver-
age sentence length is shorter in the Brown cor-
pus (23.85 tokens for WSJ; 20.56 for Brown). We
show that robustness to lexical noise improves out-
of-domain parsing.

Baseline Parsers Lexical information is clearly
useful for certain parsing decisions, such as PP-
attachment. As a result, a lexicalized parser
clearly should make use of such information when

3Prior work in psycholinguistics argued that verbs can
in fact be used in novel argument structure constructions,
and assigned coherent interpretations on the fly (Johnson and
Goldberg, 2013). Our colorless green parsing experiments
can be interpreted as a simulation for such situations.

4An anonymous reviewer notes that because of its greater
complexity, human performance on a jabberwocky version
of the WSJ corpus may not be at the level we find when
reading the sentences of Lewis Carroll’s poem or in the psy-
cholinguistic work that has explored human ability to process
jabberwocky-like sentences. We leave it for future work to
explore whether human performance in such complex cases
is indeed qualitatively different, and also whether the pattern
of results changes if we restrict our focus to a syntactically
simpler corpus, given a suitable notion of simplicity.

5We initially intended to apply our trained parsers to the
Universal Dependency corpus (Nivre et al., 2015) as well for
out-of-domain experiments, but we found annotation incon-
sistency and the problem of conversion from phrase structures
to universal dependencies. We leave this problem for future.

it is available, and may well perform less well
when it is not. In fact, in jabberwocky and col-
orless green settings, the absence of lexical infor-
mation may lead to an underdetermination of the
parse by the POS or word sequence, so that there
is no non-arbitrary “gold standard” parse. As a
result, simply observing a performance drop of a
parser in the face of lexical noise does not help
to establish an appropriate baseline with respect to
how well a parser can be expected to perform in
a lexically noisy setting. We propose three base-
line parsers: 1) an unlexicalized parser where the
network input is only POS tags, 2) a “top 100”
parser where the network input is only POS tags
and lexical information for the 100 most frequent
words and 3) a “function word” parser where the
network input is only POS tags and lexical infor-
mation for function words. Each baseline parser
can be thought of as specialized to the correspond-
ing colorless green and jabberwocky experiments.
For example, the unlexicalized parser gives us an
upper bound for full colorless green and jabber-
wocky experiments because the parser is ideally
adapted to the unlexicalized situation, as it has no
dependence on lexical information.

Experimental Setup We use Universal Depen-
dency representations obtained from converting
the Penn Treebank (Marcus et al., 1993) using
Stanford CoreNLP (ver. 3.8.0) (Manning et al.,
2014). We follow the standard data split: sections
2-21, 22, and 23 for training, dev, and test sets
respectively. For the out-of-domain experiments,
we converted the Brown corpus in PTB again us-
ing Stanford CoreNLP into Universal Dependency
representations.6

We only use gold POS tags in training for sim-
plicity,7 but we conduct experiments with both
gold and predicted POS tags. Experiments with
gold POS tags allow us to isolate the effect of lex-
ical noise from POS tagging errors, while those
with predicted POS tags simulate more practi-
cal situations where POS input is not fully reli-
able. Somewhat surprisingly, however, we find
that relative performance patterns do not change
even when using predicted POS tags. All pre-

6We exclude the domains of CL and CP in the Brown cor-
pus because the Stanford CoreNLP converter encountered an
error.

7One could achieve better results by training a parser on
predicted POS tags obtained from jackknife training, but im-
proving normal parsing performance is not the focus of our
work.

117



dicted POS tags are obtained from a BiLSTM POS
tagger with character CNNs, trained on the same
training data (sections 2-21) with hyperparameters
from Ma and Hovy (2016) and word embeddings
initialized with GloVe vectors (Pennington et al.,
2014). We train 5 parsing models for each train-
ing configuration with 5 different random initial-
izations and report the mean and standard devia-
tion.8 We use the CoNLL 2017 official script for
evaluation (Zeman et al., 2017).

5 Results and Discussions

Normal Parsing Results Table 1 shows normal
parsing results on the dev set. In both gold and
predicted POS experiments, we see a significant
discrepancy between the performance in rows 2-4
and the rest, suggesting that lexicalization of a de-
pendency parser greatly contributes to parsing per-
formance; having access to the most frequent 100
words (row 3) or the function words (row 4) recov-
ers part of the performance drop from unlexical-
ization (row 2), but the LAS differences from com-
plete lexicalization (row 1, row 5 and below) are
still significant. For each of the three word dropout
schemes in gold POS experiments, we see a com-
mon pattern: performance improves up to a certain
degree of word dropout (Uniform 0.2, Frquency-
based 1-40, Open Class 0.38), and it drops after
as word dropout becomes more aggressive. This
suggests that word dropout also involves the bias-
variance trade-off. Although performance gener-
ally degrades with predicted POS tags, the pat-
terns of relative performance still hold. Again,
for each of the three dropout schemes, there is
a certain point in the spectrum of word dropout
intensity that achieves the best performance, and
such points are almost the same both in the models
trained with gold and predicted POS tags. This is
a little surprising because a higher word dropout
rate encourages the model to rely more on POS
input, and noisy POS information from the POS
tagger can work against the model. Indeed, we ob-
served this parallelism between experiments with
gold and predicted POS tags consistently through-
out the colorless green and jabberwocky experi-
ments, and therefore we only report results with
gold POS tags for the rest of the colorless green
and jabberwocky experiments for simplicity.

8Our code is available at https://github.com/
jungokasai/graph_parser for easy replication.

Gold Predicted
Model UAS LAS UAS LAS
No Dropout 93.60.2 92.30.2 92.70.1 90.60.1
Unlexicalized 88.00.1 85.40.1 87.10.1 83.80.1
Top 100 92.50.1 90.80.1 91.70.1 89.20.1
Function 90.70.4 88.10.6 90.00.3 86.80.5
Uniform 0.2 93.90.1 92.60.1 93.00.1 90.90.2
Uniform 0.4 94.00.1 92.50.1 93.00.1 90.80.1
Uniform 0.6 93.70.1 92.20.1 92.70.1 90.50.1
Uniform 0.8 93.00.1 91.40.1 92.10.1 89.70.2
Freq 0.25 93.70.1 92.40.1 92.90.1 90.80.1
Freq 1 93.90.1 92.60.1 93.00.1 91.00.1
Freq 40 94.00.2 92.60.2 93.00.2 90.90.2
Freq 352 93.60.1 92.20.1 92.70.1 90.50.1
Freq 2536 92.90.1 91.40.1 92.00.1 89.70.1
Open Cl 0.38 93.90.1 92.50.2 93.00.1 90.90.2
Open Cl 0.75 93.50.1 92.10.1 92.70.1 90.50.1

Table 1: Normal Parsing Results on the Dev Set. The
subscripts indicate the standard deviations.

Full Experiments Table 2 shows results for
full colorless green and jabberwocky experiments.
The models without word dropout yield extremely
poor performance both in colorless and jabber-
wocky settings, suggesting that a graph-based
parsing model learns to rely heavily on word in-
formation if word dropout is not performed. Here,
unlike the normal parsing results, we see mono-
tone increasing performance as word dropout is
more aggressively applied, and the performance
rises more dramatically. In particular, with uni-
form word dropout rate 0.2, full jabberwocky per-
formance increases by more than 40 LAS points,
suggesting the importance of the parser’s expo-
sure to unknown words to abstract away from lex-
ical information. Frequency-based word dropout
needs to be performed more aggressively (↵ � 40)
than has previously been done for dependency
parsing (Kiperwasser and Goldberg, 2016; Dozat
and Manning, 2017) in order to achieve robust-
ness to full lexical noise similar to that obtained
with uniform word dropout with p > 0.2. Open
class word dropout does not bring any benefit
to parsers in the full jabberwocky and colorless
green settings. This is probably because parsers
trained with open class word dropout has consis-
tent access to function words, and omitting the lex-
ical representations of the function words is very
harmful to such parsers. Interestingly, in some
of the cases, colorless green outperforms jabber-
wocky performance, perhaps because noisy word
information, even with argument constraint viola-
tions, is better than no word information.

118



Colorless Jabberwocky
Model UAS LAS UAS LAS
No Dropout 62.60.2 56.30.1 51.92.3 39.11.9
Unlexicalized 88.00.1 85.40.1 88.00.1 85.40.2
Top 100 71.70.4 67.10.3 72.70.3 68.20.5
Function 69.20.9 62.10.7 58.83.0 39.83.4
Uniform 0.2 74.00.2 69.10.2 85.70.3 82.70.3
Uniform 0.4 76.90.3 72.30.2 87.10.1 84.30.1
Uniform 0.6 79.20.2 75.00.2 87.70.1 85.00.1
Uniform 0.8 82.00.3 78.50.3 88.00.1 85.40.1
Freq 0.25 62.90.2 56.40.1 55.01.4 43.42.5
Freq 1 63.60.4 57.10.1 60.11.7 48.83.6
Freq 40 67.50.7 61.60.6 76.41.0 72.01.2
Freq 352 74.50.5 69.70.5 82.90.4 79.50.6
Freq 2536 82.60.2 78.80.3 86.50.4 85.40.2
Open Cl 0.38 65.00.5 58.80.4 53.72.6 36.83.6
Open Cl 0.75 66.70.2 60.50.3 53.81.0 34.01.3

Table 2: Full Colorless Green and Jabberwocky Exper-
iments on the Dev Set.

Colorless Jabberwocky
Model UAS LAS UAS LAS
No Dropout 85.50.1 82.70.1 86.40.8 83.40.9
Unlexicalized 88.00.1 85.40.1 88.00.1 85.40.2
Top 100 92.50.1 90.80.1 92.50.1 90.80.1
Function 88.70.4 85.40.7 90.80.3 88.00.6
Uniform 0.2 87.50.2 84.90.2 90.20.6 88.20.6
Uniform 0.4 88.50.2 86.00.2 90.80.4 88.90.4
Uniform 0.6 89.20.2 86.80.1 91.00.3 89.10.2
Uniform 0.8 89.70.1 87.40.2 90.60.2 88.60.2
Freq 0.25 85.80.2 83.00.2 87.80.6 85.00.6
Freq 1 86.10.1 83.30.1 88.90.4 86.30.4
Freq 40 88.10.2 85.50.1 90.90.2 88.70.4
Freq 352 89.70.1 87.40.1 91.90.2 90.00.2
Freq 2536 90.70.2 88.60.3 91.30.2 89.30.3
Open Cl 0.38 88.60.3 86.20.3 90.60.2 88.30.2
Open Cl 0.75 89.60.1 87.40.2 90.80.3 88.00.6

Table 3: Top 100 Colorless Green and Jabberwocky
Experiments on the Dev Set.

Colorless Jabberwocky
Model UAS LAS UAS LAS
No Dropout 84.10.3 81.30.2 84.90.6 82.10.6
Unlexicalized 88.00.1 85.40.1 88.00.1 85.40.2
Top 100 90.50.2 88.40.2 91.80.1 89.90.1
Function 90.70.4 88.10.6 90.70.4 88.10.6
Uniform 0.2 87.40.2 84.80.2 89.70.7 87.80.7
Uniform 0.4 88.30.2 85.90.3 90.60.4 88.70.4
Uniform 0.6 89.20.2 86.80.2 90.90.3 89.00.3
Uniform 0.8 89.90.2 87.70.1 90.50.3 88.60.2
Freq 0.25 84.70.2 82.00.2 86.70.5 84.30.6
Freq 1 85.20.4 82.40.4 88.10.6 85.90.6
Freq 40 87.70.2 85.20.2 91.20.3 89.20.3
Freq 352 89.50.2 87.30.1 92.00.1 90.20.1
Freq 2536 90.70.2 88.70.2 91.70.2 89.90.1
Open Cl 0.38 89.00.2 86.70.3 92.10.2 90.30.2
Open Cl 0.75 90.70.1 88.40.2 92.40.1 90.70.1

Table 4: Open Class Colorless Green and Jabberwocky
Experiments on the Dev Set.

Gold Predicted
Model UAS LAS UAS LAS
No Dropout 89.70.1 87.50.1 88.80.1 85.70.1
Unlexicalized 83.00.1 79.30.2 81.30.1 76.70.1
Top 100 89.40.1 86.80.1 88.50.1 84.90.1
Function 88.30.3 84.80.7 87.40.3 83.10.7
Uniform 0.2 90.00.1 87.80.2 89.10.1 86.00.2
Uniform 0.4 90.10.1 87.90.1 89.30.1 86.10.1
Uniform 0.6 90.10.1 87.70.1 89.10.1 85.80.1
Uniform 0.8 89.40.1 86.80.1 88.50.1 85.00.1
Freq 0.25 89.90.2 87.70.2 89.10.2 86.00.2
Freq 1 90.20.2 88.10.2 89.30.1 86.30.1
Freq 40 90.70.2 88.50.3 89.80.2 86.70.2
Freq 352 90.30.1 88.00.1 89.30.1 86.10.1
Freq 2536 89.30.2 86.80.2 88.40.1 85.00.2
Open Cl 0.38 90.30.2 88.10.2 89.40.1 86.20.2
Open Cl 0.75 90.30.1 88.00.1 87.40.3 86.10.1

Table 5: Brown CF Results.

Top 100 Experiments Table 3 shows the results
of top 100 colorless green and jabberwocky ex-
periments. Performance by parsers trained with-
out word dropout is substantially better than what
is found in the full colorless green or jabber-
wocky settings. However, the performance is still
much lower than the unlexicalized parsers (2.7
LAS points for colorless green and 2.0 LAS points
for jabberwocky), meaning that the parser without
word dropout has significant dependence on less
frequent words. On the other hand, parsers trained
with a high enough word dropout rate outper-
form the unlexicalized parser (e.g., uniform 0.4,
frequency-based ↵ = 40, and open class 0.38).
Frequency-based word dropout is very effective.
Recall that ↵ = 352, 2536 correspond to the ex-
pected word dropout rates of 0.4 and 0.6. The two
configurations yield better results than uniform 0.4
and 0.6.

Open Class Experiments Table 4 gives the re-
sults of open class colorless green and jabber-
wocky experiments. We see similar patterns to the
top 100 jabberwocky experiments except that open
class word dropout performs better and frequency-
based word dropout performs worse, which natu-
rally follows from the way we train the parsers.

Out-of-domain Parsing Table 5 reports pars-
ing performance on the CF domain in the Brown
corpus.9 POS tagging accuracy was 96.4%, rela-
tively low as compared to in-domain performance
in WSJ. Gildea (2001) demonstrated that remov-
ing lexical information from lexicalized PCFG
does not deteriorate out-of-domain parsing perfor-

9We found similar patterns in relative performance when
applied to the other domains of the Brown corpus.

119



Colorless Jabberwocky
relation Pre Rec F1 Prec Rec F1
nsubj 87.8 86.2 87.0 91.0 90.0 90.5
dobj 79.2 81.4 80.3 83.0 90.7 86.7
iobj 29.4 27.8 28.6 71.4 55.6 62.5
csubj 30.4 46.7 36.8 33.3 60.0 42.9
ccomp 75.3 75.3 75.3 77.2 78.2 77.7
xcomp 69.0 69.2 69.1 78.7 64.3 70.8
others 83.6 83.6 83.6 86.7 86.6 86.6

Table 6: Performance Breakdown by Dependency Re-
lation.

mance in the Brown corpus. In contrast, our re-
sults show that lexicalization of the neural network
dependency parsers via distributed representations
facilitates parsing performance. Most of the gains
from lexicalization stem from the 100 most fre-
quent words (row 3) and function words (row 4),
but we get a significant improvement by perform-
ing relatively aggressive word dropout (↵ = 40).
This confirms the practical significance of jabber-
wocky parsing experiments.

6 Analysis of Results

Colorless Green vs. Jabberwocky Earlier we
hypothesized that colorless green lexical noise
could mislead the parser by violating argument
structure constraints. And in fact, we saw in the
previous section that jabberwocky parsing results
are generally better than colorless green parsing
results. In order to further test our hypothesis,
we provide a breakdown of parsing performance
broken down by dependency type (Nivre et al.,
2015). Table 6 provides the open class perfor-
mance breakdown for a parser trained with uni-
form word dropout rate 0.2. We observe that in the
colorless green situation, the parser particularly
suffers in “iobj” and “dobj”, consistent with what
we would expect for parsing sentences with argu-
ment structure violations. For example, the col-
orless green scheme does not prevent us from re-
placing a ditransitive verb with a non-ditransitive
verb. This result validates our hypothesis, and the
colorless green scheme is limited as a framework
to purely assess parser’s generalization ability.

Trained Word Embeddings The logic of our
jabberwocky experiments and the role of word
dropout involves the assumption that the parser
will succeed in the absence of lexical information
if it depends more heavily on the grammatical cat-
egory information present in the POS embeddings.
However, this fails to allow for the possibility that

0.0 0.2 0.4 0.6 0.8
40

45

50

A
cc

ur
ac

y
(%

)

Figure 2: POS Prediction Results from the Word Em-
bedding Induced in Varying Uniform Dropout Rates.

the word embeddings themselves might also rep-
resent POS information. It might instead be the
case that what word dropout does is merely dis-
couraging the model from constructing grammati-
cal abstractions in the word embeddings, depend-
ing instead on the information present in the POS
embeddings.

To test this possibility, we attempt to use a sim-
ple softmax regression on top of the induced word
embeddings to detect the presence of grammati-
cal category information. Concretely, we find the
set of words that occur 100 times or more in the
training data whose unigram probability for the
most frequent POS tag is greater than 0.5. In
this way, we obtain a set of pairs of a word and
its most frequent POS tag, and we then randomly
split the set to run 5-fold cross validation. Figure 2
shows POS prediction accuracy over varying uni-
form word dropout rates. These results do not sup-
port the idea that word dropout reduces the repre-
sentation of grammatical category information in
word embedding. On the contrary, word dropout
rates of 0.2 through 0.6 leads to word embeddings
that better represent POS.

Parser Weight Sizes What then is the effect of
word dropout on the network? Figure 3 shows L2
norms of the trained POS embeddings (46 by 25)
and the input/output gate weights for POS input in
the first BiLSTM layer (400 by 25) across vary-
ing uniform word dropout rates. We can observe
that L2 norm for each becomes larger as the word
dropout rate increases. This suggests that what
word dropout does is to encourage the model to
make greater use of POS information.

120



0.0 0.2 0.4 0.6 0.8

15

20

25

30

L
2

N
or

m

Embeddings
Input Gate

Output Gate

Figure 3: POS Embeddings and Weight Sizes over Uni-
form Word Dropout Rates.

7 Conclusion and Future Work

We conducted extensive analysis on the robust-
ness of a state-of-the-art graph-based dependency
parser against lexical noise. Our experiments
showed that parsers trained under usual regimens
performed poorly in the face of lexical noise.
However, we demonstrated that the technique of
word dropout, when applied aggressively, reme-
dies this problem without sacrificing parsing per-
formance. Word dropout is commonly used in lit-
erature, but our results provide further guidance
about how it should be used in the future. In future
work, we would like to compare our results with
different parsing architectures such as transition-
based parsers.

References
Bharat Ram Ambati, Tejaswini Deoskar, and Mark

Steedman. 2016. Shift-reduce ccg parsing using
neural network models. In NAACL.

Riyaz Ahmad Bhat, Irshad Ahmad Bhat, and
Dipti Misra Sharma. 2017. Leveraging newswire
treebanks for parsing conversational data with argu-
ment scrambling. In IWPT.

Lewis Carroll. 1883. Through the Looking-Glass.
Macmillan and Co., New York.

Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In ANLP.

Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In EMNLP, pages 740–750, Doha, Qatar.
Association for Computational Linguistics.

Noam Chomsky. 1957. Syntactic Structures. Mouton
& Co., Berlin, Germany.

Michael Collins. 1996. A new statistical parser based
on bigram lexical dependencies. In ACL.

Michael Collins. 1999. . Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.

Timothy Dozat and Christopher Manning. 2017. Deep
biaffine attention for neural dependency parsing. In
ICLR.

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A. Smith. 2016. Recurrent neural network
grammars. In NAACL.

Sandiway Fong and Robert C. Berwick. 2008. Tree-
bank parsing and knowledge of language : A cogni-
tive perspective. In CogSci.

Angela D. Friederici, Mia Viktoria Meyer, and D. Yves
von Cramon. 2000. Auditory language comprehen-
sion: an event-related fmri study on the processing
of syntactic and lexical information. Brain and lan-
guage, 75 3:289–300.

Daniel Gildea. 2001. Corpus variation and parser per-
formance. In EMNLP.

Kristina Gulordava, Piotr Bojanowski, Edouard Grave,
Tal Linzen, and Marco Baroni. 2018. Colorless
green recurrent networks dream hierarchically. In
NAACL. Association for Computational Linguistics.

Mohit Iyyer, Varun Manjunatha, Jordan L. Boyd-
Graber, and Hal Daumé. 2015. Deep unordered
composition rivals syntactic methods for text clas-
sification. In ACL.

Tao Ji, Yuanbin Wu, and Man Lan. 2017. A fast
and lightweight system for multilingual dependency
parsing. In CoNLL Shared Task.

Matt A. Johnson and Adele E. Goldberg. 2013. Ev-
idence for automatic accessing of constructional
meaning: Jabberwocky sentences prime associ-
ated verbs. Language and Cognitive Processes,
28(10):14391452.

Jungo Kasai, Robert Frank, Pauli Xu, William Merrill,
and Owen Rambow. 2018. End-to-end graph-based
TAG parsing with neural networks. In NAACL. As-
sociation for Computational Linguistics.

Gaurav Kharkwal. 2014. Taming the Jabberwocky:
Examining Sentence Processing with Novel Words.
Ph.D. thesis, Rutgers University.

Diederik P. Kingma and Jimmy Lei Ba. 2015. ADAM:
A Method for Stochastic Optimization. In ICLR.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional LSTM feature representations. TACL, 4:313–
327.

Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In ACL.

121



Miryam de Lhoneux, Yan Shao, Ali Basirat, Eliyahu
Kiperwasser, Sara Stymne, Yoav Goldberg, and
Joakim Nivre. 2017. From raw text to universal de-
pendencies - look, no tags! In CoNLL Shared Task.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end
sequence labeling via bi-directional LSTM-CNNs-
CRF. In ACL, pages 1064–1074, Berlin, Germany.
Association for Computational Linguistics.

Maryellen C. MacDonald, Neal J. Pearlmutter, and
Mark S. Seidenberg. 1994. Lexical nature of syn-
tactic ambiguity resolution. Psychological Review,
101:676–703.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations.

Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.

Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The NomBank project:
An interim report. In Proceedings of the Workshop
on Frontiers in Corpus Annotation at HLT-NAACL.

Dat Quoc Nguyen, Mark Dras, and Mark Johnson.
2017. A novel neural network model for joint pos
tagging and graph-based dependency parsing. In
CoNLL Shared Task.

Joakim Nivre, Željko Agić, Maria Jesus Aranzabe,
Masayuki Asahara, Aitziber Atutxa, Miguel Balles-
teros, John Bauer, Kepa Bengoetxea, Riyaz Ah-
mad Bhat, Cristina Bosco, Sam Bowman, Giuseppe
G. A. Celano, Miriam Connor, Marie-Catherine
de Marneffe, Arantza Diaz de Ilarraza, Kaja Do-
brovoljc, Timothy Dozat, Tomaž Erjavec, Richárd
Farkas, Jennifer Foster, Daniel Galbraith, Filip
Ginter, Iakes Goenaga, Koldo Gojenola, Yoav
Goldberg, Berta Gonzales, Bruno Guillaume, Jan
Hajič, Dag Haug, Radu Ion, Elena Irimia, An-
ders Johannsen, Hiroshi Kanayama, Jenna Kan-
erva, Simon Krek, Veronika Laippala, Alessan-
dro Lenci, Nikola Ljubešić, Teresa Lynn, Christo-
pher Manning, Cătălina Mărănduc, David Mareček,
Héctor Martı́nez Alonso, Jan Mašek, Yuji Mat-
sumoto, Ryan McDonald, Anna Missilä, Verginica
Mititelu, Yusuke Miyao, Simonetta Montemagni,
Shunsuke Mori, Hanna Nurmi, Petya Osenova, Lilja
Øvrelid, Elena Pascual, Marco Passarotti, Cenel-
Augusto Perez, Slav Petrov, Jussi Piitulainen, Bar-
bara Plank, Martin Popel, Prokopis Prokopidis,
Sampo Pyysalo, Loganathan Ramasamy, Rudolf
Rosa, Shadi Saleh, Sebastian Schuster, Wolfgang
Seeker, Mojgan Seraji, Natalia Silveira, Maria Simi,
Radu Simionescu, Katalin Simkó, Kiril Simov,
Aaron Smith, Jan Štěpánek, Alane Suhr, Zsolt

Szántó, Takaaki Tanaka, Reut Tsarfaty, Sumire Ue-
matsu, Larraitz Uria, Viktor Varga, Veronika Vincze,
Zdeněk Žabokrtský, Daniel Zeman, and Hanzhi
Zhu. 2015. Universal dependencies 1.2. LIN-
DAT/CLARIN digital library at the Institute of For-
mal and Applied Linguistics, Charles University.

Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with se-
mantic roles. Computational Linguistics, 31(1).

Hao Peng, Sam Thomson, and Noah A. Smith. 2017.
Deep multitask learning for semantic dependency
parsing. In ACL.

Hao Peng, Sam Thomson, Swabha Swayamdipta, and
Noah A. Smith. 2018. Learning joint semantic
parsers from disjoint data. In NAACL.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In EMNLP.

Tianze Shi and Lillian Lee. 2018. Valency-augmented
dependency parsing. In EMNLP. Association for
Computational Linguistics.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15:1929–1958.

Karen Stromswold, David O. Caplan, Nathaniel M.
Alpert, and Scott L. Rauch. 1996. Localization of
syntactic comprehension by positron emission to-
mography. Brain and language, 52 3:452–73.

John C. Trueswell and Michael K. Tanenhaus. 1994.
Toward a lexicalist framework for constraint-based
syntactic ambiguity resolution. In Charles Clifton,
Jr., Lyn Frazier, and Keith Rayner, editors, Per-
spectives on Sentence Processing, pages 155–179.
Lawrence Erlbaum Associates, Hillsdale, NJ.

Daniel Zeman, Martin Popel, Milan Straka, Jan Ha-
jic, Joakim Nivre, Filip Ginter, Juhani Luotolahti,
Sampo Pyysalo, Slav Petrov, Martin Potthast, Fran-
cis Tyers, Elena Badmaeva, Memduh Gokirmak,
Anna Nedoluzhko, Silvie Cinkova, Jan Hajic jr.,
Jaroslava Hlavacova, Václava Kettnerová, Zdenka
Uresova, Jenna Kanerva, Stina Ojala, Anna Mis-
silä, Christopher D. Manning, Sebastian Schuster,
Siva Reddy, Dima Taji, Nizar Habash, Herman Le-
ung, Marie-Catherine de Marneffe, Manuela San-
guinetti, Maria Simi, Hiroshi Kanayama, Valeria de-
Paiva, Kira Droganova, Héctor Martı́nez Alonso,
Çağr Çöltekin, Umut Sulubacak, Hans Uszkor-
eit, Vivien Macketanz, Aljoscha Burchardt, Kim
Harris, Katrin Marheinecke, Georg Rehm, Tolga
Kayadelen, Mohammed Attia, Ali Elkahky, Zhuoran
Yu, Emily Pitler, Saran Lertpradit, Michael Mandl,
Jesse Kirchner, Hector Fernandez Alcalde, Jana Str-
nadová, Esha Banerjee, Ruli Manurung, Antonio
Stella, Atsuko Shimada, Sookyoung Kwak, Gustavo

122



Mendonca, Tatiana Lando, Rattima Nitisaroj, and
Josie Li. 2017. CoNLL 2017 shared task: Mul-
tilingual parsing from raw text to universal depen-
dencies. In Proceedings of the CoNLL 2017 Shared
Task: Multilingual Parsing from Raw Text to Univer-
sal Dependencies, pages 1–19, Vancouver, Canada.
Association for Computational Linguistics.

123


