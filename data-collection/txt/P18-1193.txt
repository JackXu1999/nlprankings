



















































Situated Mapping of Sequential Instructions to Actions with Single-step Reward Observation


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2072–2082
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

2072

Situated Mapping of Sequential Instructions to Actions
with Single-step Reward Observation

Alane Suhr and Yoav Artzi
Department of Computer Science and Cornell Tech

Cornell University
New York, NY, 10044

{suhr, yoav}@cs.cornell.edu

Abstract

We propose a learning approach for map-
ping context-dependent sequential instruc-
tions to actions. We address the prob-
lem of discourse and state dependencies
with an attention-based model that consid-
ers both the history of the interaction and
the state of the world. To train from start
and goal states without access to demon-
strations, we propose SESTRA, a learning
algorithm that takes advantage of single-
step reward observations and immediate
expected reward maximization. We eval-
uate on the SCONE domains, and show
absolute accuracy improvements of 9.8%-
25.3% across the domains over approaches
that use high-level logical representations.

1 Introduction

An agent executing a sequence of instruc-
tions must address multiple challenges, includ-
ing grounding the language to its observed en-
vironment, reasoning about discourse dependen-
cies, and generating actions to complete high-level
goals. For example, consider the environment and
instructions in Figure 1, in which a user describes
moving chemicals between beakers and mixing
chemicals together. To execute the second instruc-
tion, the agent needs to resolve sixth beaker and
last one to objects in the environment. The third
instruction requires resolving it to the rightmost
beaker mentioned in the second instruction, and
reasoning about the set of actions required to mix
the colors in the beaker to brown. In this paper,
we describe a model and learning approach to map
sequences of instructions to actions. Our model
considers previous utterances and the world state
to select actions, learns to combine simple actions
to achieve complex goals, and can be trained using

Start

Goal

throw out first beaker
POP 1, STOP

pour sixth beaker into last one
POP 6, POP 6, PUSH 7 O, PUSH 7 O, STOP

it turns brown
POP 7, POP 7, POP 7, PUSH 7 B, PUSH 7 B, PUSH 7 B, STOP

pour purple beaker into yellow one
POP 3, PUSH 5 P, STOP

throw out two units of brown one
POP 7, POP 7, STOP

Start

Goal
Figure 1: Example from the SCONE (Long et al.,
2016) ALCHEMY domain, including a start state (top),
sequence of instructions, and a goal state (bottom).
Each instruction is annotated with a sequence of ac-
tions from the set of actions we define for ALCHEMY.

goal states without access to demonstrations.
The majority of work on executing sequences

of instructions focuses on mapping instructions
to high-level formal representations, which are
then evaluated to generate actions (e.g., Chen and
Mooney, 2011; Long et al., 2016). For example,
the third instruction in Figure 1 will be mapped
to mix(prev_arg1), indicating that the mix action
should be applied to first argument of the previ-
ous action (Long et al., 2016; Guu et al., 2017). In
contrast, we focus on directly generating the se-
quence of actions. This requires resolving refer-
ences without explicitly modeling them, and learn-
ing the sequences of actions required to complete
high-level actions; for example, that mixing re-
quires removing everything in the beaker and re-
placing with the same number of brown items.

A key challenge in executing sequences of in-
structions is considering contextual cues from
both the history of the interaction and the state of
the world. Instructions often refer to previously



2073

mentioned objects (e.g., it in Figure 1) or actions
(e.g., do it again). The world state provides the set
of objects the instruction may refer to, and implic-
itly determines the available actions. For example,
liquid can not be removed from an empty beaker.
Both types of contexts continuously change during
an interaction. As new instructions are given, the
instruction history expands, and as the agent acts
the world state changes. We propose an attention-
based model that takes as input the current instruc-
tion, previous instructions, the initial world state,
and the current state. At each step, the model com-
putes attention encodings of the different inputs,
and predicts the next action to execute.

We train the model given instructions paired
with start and goal states without access to the
correct sequence of actions. During training, the
agent learns from rewards received through ex-
ploring the environment with the learned policy
by mapping instructions to sequences of actions.
In practice, the agent learns to execute instruc-
tions gradually, slowly correctly predicting pre-
fixes of the correct sequences of increasing length
as learning progress. A key challenge is learning
to correctly select actions that are only required
later in execution sequences. Early during learn-
ing, these actions receive negative updates, and the
agent learns to assign them low probabilities. This
results in an exploration problem in later stages,
where actions that are only required later are not
sampled during exploration. For example, in the
ALCHEMY domain shown in Figure 1, the agent
behavior early during execution of instructions can
be accomplished by only using POP actions. As
a result, the agent quickly learns a strong bias
against PUSH actions, which in practice prevents
the policy from exploring them again. We address
this with a learning algorithm that observes the re-
ward for all possible actions for each visited state,
and maximizes the immediate expected reward.

We evaluate our approach on SCONE (Long
et al., 2016), which includes three domains,
and is used to study recovering predicate logic
meaning representations for sequential instruc-
tions. We study the problem of generating a
sequence of low-level actions, and re-define the
set of actions for each domain. For example, we
treat the beakers in the ALCHEMY domain as
stacks and use only POP and PUSH actions. Our
approach robustly learns to execute sequential
instructions with up to 89.1% task-completion

accuracy for single instruction, and 62.7% for
complete sequences. Our code is available at
https://github.com/clic-lab/scone.

2 Technical Overview

Task and Notation Let S be the set of all pos-
sible world states, X be the set of all natural lan-
guage instructions, and A be the set of all actions.
An instruction x̄ ∈ X of length |x̄| is a sequence
of tokens 〈x1, ...x|x̄|〉. Executing an action modi-
fies the world state following a transition function
T : S ×A → S. For example, the ALCHEMY do-
main includes seven beakers that contain colored
liquids. The world state defines the content of each
beaker. We treat each beaker as a stack. The ac-
tions are POP N and PUSH N C, where 1 ≤ N ≤ 7
is the beaker number and C is one of six colors.
There are a total of 50 actions, including the STOP
action. Section 6 describes the domains in detail.

Given a start state s1 and a sequence of in-
structions 〈x̄1, . . . , x̄n〉, our goal is to generate the
sequence of actions specified by the instructions
starting from s1. We treat the execution of a se-
quence of instructions as executing each instruc-
tion in turn. The execution ē of an instruction x̄i
starting at a state s1 and given the history of the
instruction sequence 〈x̄1, . . . , x̄i−1〉 is a sequence
of state-action pairs ē = 〈(s1, a1), ..., (sm, am)〉,
where ak ∈ A, sk+1 = T (sk, ak). The final
action am is the special action STOP, which indi-
cates the execution has terminated. The final state
is then sm, as T (sk, STOP) = sk. Executing a
sequence of instructions in order generates a se-
quence 〈ē1, ..., ēn〉, where ēi is the execution of
instruction x̄i. When referring to states and ac-
tions in an indexed execution ēi, the k-th state and
action are si,k and ai,k. We execute instructions
one after the other: ē1 starts at the interaction ini-
tial state s1 and si+1,1 = si,|ēi|, where si+1,1 is the
start state of ēi+1 and si,|ēi| is the final state of ēi.

Model We model the agent with a neural net-
work policy (Section 4). At step k of execut-
ing the i-th instruction, the model input is the
current instruction x̄i, the previous instructions
〈x̄1, . . . , x̄i−1〉, the world state s1 at the begin-
ning of executing x̄i, and the current state sk.
The model predicts the next action ak to exe-
cute. If ak = STOP, we switch to the next in-
struction, or if at the end of the instruction se-
quence, terminate. Otherwise, we update the state
to sk+1 = T (sk, ak). The model uses attention to

https://github.com/clic-lab/scone


2074

process the different inputs and a recurrent neural
network (RNN) decoder to generate actions (Bah-
danau et al., 2015).

Learning We assume access to a set of N in-
struction sequences, where each instruction in
each sequence is paired with its start and goal
states. During training, we create an exam-
ple for each instruction. Formally, the training
set is {(x̄(j)i , s

(j)
i,1 , 〈x̄

(j)
1 , . . . , x̄

(j)
i−1〉, g

(j)
i )}

N,n(j)

j=1,i=1,

where x̄(j)i is an instruction, s
(j)
i,1 is a start state,

〈x̄(j)1 , . . . , x̄
(j)
i−1〉 is the instruction history, g

(j)
i is

the goal state, and n(j) is the length of the j-th in-
struction sequence. This training data contains no
evidence about the actions and intermediate states
required to execute each instruction.1 We use a
learning method that maximizes the expected im-
mediate reward for a given state (Section 5). The
reward accounts for task-completion and distance
to the goal via potential-based reward shaping.

Evaluation We evaluate exact task comple-
tion for sequences of instructions on a test set
{(s(j)1 , 〈x̄

(j)
1 , . . . , x̄

(j)
nj 〉, g(j))}Nj=1, where g(j) is

the oracle goal state of executing instructions
x̄

(j)
1 , . . . ,x̄

(j)
nj in order starting from s

(j)
1 . We also

evaluate single-instruction task completion using
per-instruction annotated start and goal states.

3 Related Work

Executing instructions has been studied using the
SAIL corpus (MacMahon et al., 2006) with focus
on navigation using high-level logical representa-
tions (Chen and Mooney, 2011; Chen, 2012; Artzi
and Zettlemoyer, 2013; Artzi et al., 2014) and low-
level actions (Mei et al., 2016). While SAIL in-
cludes sequences of instructions, the data demon-
strates limited discourse phenomena, and instruc-
tions are often processed in isolation. Approaches
that consider as input the entire sequence focused
on segmentation (Andreas and Klein, 2015). Re-
cently, other navigation tasks were proposed with
focus on single instructions (Anderson et al., 2018;
Janner et al., 2018). We focus on sequences of
environment manipulation instructions and mod-
eling contextual cues from both the changing en-
vironment and instruction history. Manipulation
using single-sentence instructions has been stud-

1This training set is a subset of the data used in previous
work (Section 6, Guu et al., 2015), in which training uses all
instruction sequences of length 1 and 2.

ied using the Blocks domain (Bisk et al., 2016,
2018; Misra et al., 2017; Tan and Bansal, 2018).
Our work is related to the work of Branavan et al.
(2009) and Vogel and Jurafsky (2010). While both
study executing sequences of instructions, similar
to SAIL, the data includes limited discourse de-
pendencies. In addition, both learn with rewards
computed from surface-form similarity between
text in the environment and the instruction. We
do not rely on such similarities, but instead use a
state distance metric.

Language understanding in interactive scenar-
ios that include multiple turns has been studied
with focus on dialogue for querying database sys-
tems using the ATIS corpus (Hemphill et al., 1990;
Dahl et al., 1994). Tür et al. (2010) surveys work
on ATIS. Miller et al. (1996), Zettlemoyer and
Collins (2009), and Suhr et al. (2018) modeled
context dependence in ATIS for generating formal
representations. In contrast, we focus on environ-
ments that change during execution and directly
generating environment actions, a scenario that is
more related to robotic agents than database query.

The SCONE corpus (Long et al., 2016) was
designed to reflect a broad set of discourse
context-dependence phenomena. It was stud-
ied extensively using logical meaning representa-
tions (Long et al., 2016; Guu et al., 2017; Fried
et al., 2018). In contrast, we are interested in
directly generating actions that modify the envi-
ronment. This requires generating lower-level ac-
tions and learning procedures that are otherwise
hardcoded in the logic (e.g., mixing action in Fig-
ure 1). Except for Fried et al. (2018), previous
work on SCONE assumes access only to the ini-
tial and final states during training. This form of
supervision does not require operating the agent
manually to acquire the correct sequence of ac-
tions, a difficult task in robotic agents with com-
plex control. Goal state supervision has been
studied for instructional language (e.g., Branavan
et al., 2009; Artzi and Zettlemoyer, 2013; Bisk
et al., 2016), and more extensively in question an-
swering when learning with answer annotations
only (e.g., Clarke et al., 2010; Liang et al., 2011;
Kwiatkowski et al., 2013; Berant et al., 2013; Be-
rant and Liang, 2014, 2015; Liang et al., 2017).

4 Model

We map sequences of instructions 〈x̄1, . . . , x̄n〉
to actions by executing the instructions in or-



2075

Utterance

initial state s1
<latexit sha1_base64="pSKeRC6KrabkRj9ZFy6P3Vrmkv4=">AAACUHicbZBNb9QwEIadBUoJX1s4crFYUXFaJRSp5VapF45FIrTSJlpNJrOtVduJ7EnLKkr/B7+mVzhz4qdwAu8HEmwZyfKr9x17NE/ZaOU5SX5Egzt3723d334QP3z0+MnT4c6zT75uHVKGta7daQmetLKUsWJNp40jMKWmk/LiaJGfXJLzqrYfed5QYeDMqplC4GBNh3u78vo6Z/rMXcZMDixSL/Nc7sqVq6xiBVp6BibZSz9Np8NRMk6WJW+LdC1GYl3H051oK69qbA1ZRg3eT9Kk4aIDxwo19XHeemoAL+CMJkFaMOSLbrldL18Fp5Kz2oVjWS7dv190YLyfmzJ0GuBzv5ktzP9mlV98uDGdZwdFWLppmSyuhs9aLbmWC3yyUo6Q9TwIQBfQoMRzcICBnY/j3JGlK6yNAVt1OfaTtOi63Bk5Svs+DuTSTU63RfZm/G6cfHg7OkzWCLfFC/FSvBap2BeH4r04FplA8UXciK/iW/Q9+hn9GkSr1j+3eC7+qUH8G3easyo=</latexit><latexit sha1_base64="pSKeRC6KrabkRj9ZFy6P3Vrmkv4=">AAACUHicbZBNb9QwEIadBUoJX1s4crFYUXFaJRSp5VapF45FIrTSJlpNJrOtVduJ7EnLKkr/B7+mVzhz4qdwAu8HEmwZyfKr9x17NE/ZaOU5SX5Egzt3723d334QP3z0+MnT4c6zT75uHVKGta7daQmetLKUsWJNp40jMKWmk/LiaJGfXJLzqrYfed5QYeDMqplC4GBNh3u78vo6Z/rMXcZMDixSL/Nc7sqVq6xiBVp6BibZSz9Np8NRMk6WJW+LdC1GYl3H051oK69qbA1ZRg3eT9Kk4aIDxwo19XHeemoAL+CMJkFaMOSLbrldL18Fp5Kz2oVjWS7dv190YLyfmzJ0GuBzv5ktzP9mlV98uDGdZwdFWLppmSyuhs9aLbmWC3yyUo6Q9TwIQBfQoMRzcICBnY/j3JGlK6yNAVt1OfaTtOi63Bk5Svs+DuTSTU63RfZm/G6cfHg7OkzWCLfFC/FSvBap2BeH4r04FplA8UXciK/iW/Q9+hn9GkSr1j+3eC7+qUH8G3easyo=</latexit><latexit sha1_base64="pSKeRC6KrabkRj9ZFy6P3Vrmkv4=">AAACUHicbZBNb9QwEIadBUoJX1s4crFYUXFaJRSp5VapF45FIrTSJlpNJrOtVduJ7EnLKkr/B7+mVzhz4qdwAu8HEmwZyfKr9x17NE/ZaOU5SX5Egzt3723d334QP3z0+MnT4c6zT75uHVKGta7daQmetLKUsWJNp40jMKWmk/LiaJGfXJLzqrYfed5QYeDMqplC4GBNh3u78vo6Z/rMXcZMDixSL/Nc7sqVq6xiBVp6BibZSz9Np8NRMk6WJW+LdC1GYl3H051oK69qbA1ZRg3eT9Kk4aIDxwo19XHeemoAL+CMJkFaMOSLbrldL18Fp5Kz2oVjWS7dv190YLyfmzJ0GuBzv5ktzP9mlV98uDGdZwdFWLppmSyuhs9aLbmWC3yyUo6Q9TwIQBfQoMRzcICBnY/j3JGlK6yNAVt1OfaTtOi63Bk5Svs+DuTSTU63RfZm/G6cfHg7OkzWCLfFC/FSvBap2BeH4r04FplA8UXciK/iW/Q9+hn9GkSr1j+3eC7+qUH8G3easyo=</latexit>

Throw out first beaker It turns brownPour sixth beaker into last one
Previous Utterances: x̄1, x̄2

<latexit sha1_base64="7La+CCZxSF6irvZeZZfcBl4yF3Q=">AAACTXicbZBNaxRBEIZ7NprE8WsTj14aF8GDLDMh4Mcp4CXHFRwT2BmWmp7apEl/DN01MUszf8Nf49WcvfpHPInYu1lBNxY0PLxvVVf3W7dKesqy78lg687d7Z3de+n9Bw8fPR7u7X/0tnMCC2GVdac1eFTSYEGSFJ62DkHXCk/qi3dL/+QSnZfWfKBFi5WGMyPnUgBFaTbMSsIrChOHl9J2nhdE6MAI9G95X9bgwlU/y1/yP3gwG46ycbYqfhvyNYzYuiazvWS7bKzoNBoSCryf5llLVQBHUijs07Lz2IK4gDOcRjSg0Vdh9bWeP49Kw+fWxWOIr9S/JwJo7xe6jp0a6Nxvekvxv17jlxdubKf56ypI03aERtwsn3eKk+XL7HgjHQpSiwggnIzv5+IcHIiYmU/T0qHBT8JqDaYJpeineRVC6TQf5X2fxuTyzZxuQ3EwfjPO3h+OjrJ1hLvsKXvGXrCcvWJH7JhNWMEE+8y+sK/sOvmW/Eh+Jr9uWgfJeuYJ+6cGO78Bg3uzvQ==</latexit><latexit sha1_base64="7La+CCZxSF6irvZeZZfcBl4yF3Q=">AAACTXicbZBNaxRBEIZ7NprE8WsTj14aF8GDLDMh4Mcp4CXHFRwT2BmWmp7apEl/DN01MUszf8Nf49WcvfpHPInYu1lBNxY0PLxvVVf3W7dKesqy78lg687d7Z3de+n9Bw8fPR7u7X/0tnMCC2GVdac1eFTSYEGSFJ62DkHXCk/qi3dL/+QSnZfWfKBFi5WGMyPnUgBFaTbMSsIrChOHl9J2nhdE6MAI9G95X9bgwlU/y1/yP3gwG46ycbYqfhvyNYzYuiazvWS7bKzoNBoSCryf5llLVQBHUijs07Lz2IK4gDOcRjSg0Vdh9bWeP49Kw+fWxWOIr9S/JwJo7xe6jp0a6Nxvekvxv17jlxdubKf56ypI03aERtwsn3eKk+XL7HgjHQpSiwggnIzv5+IcHIiYmU/T0qHBT8JqDaYJpeineRVC6TQf5X2fxuTyzZxuQ3EwfjPO3h+OjrJ1hLvsKXvGXrCcvWJH7JhNWMEE+8y+sK/sOvmW/Eh+Jr9uWgfJeuYJ+6cGO78Bg3uzvQ==</latexit><latexit sha1_base64="7La+CCZxSF6irvZeZZfcBl4yF3Q=">AAACTXicbZBNaxRBEIZ7NprE8WsTj14aF8GDLDMh4Mcp4CXHFRwT2BmWmp7apEl/DN01MUszf8Nf49WcvfpHPInYu1lBNxY0PLxvVVf3W7dKesqy78lg687d7Z3de+n9Bw8fPR7u7X/0tnMCC2GVdac1eFTSYEGSFJ62DkHXCk/qi3dL/+QSnZfWfKBFi5WGMyPnUgBFaTbMSsIrChOHl9J2nhdE6MAI9G95X9bgwlU/y1/yP3gwG46ycbYqfhvyNYzYuiazvWS7bKzoNBoSCryf5llLVQBHUijs07Lz2IK4gDOcRjSg0Vdh9bWeP49Kw+fWxWOIr9S/JwJo7xe6jp0a6Nxvekvxv17jlxdubKf56ypI03aERtwsn3eKk+XL7HgjHQpSiwggnIzv5+IcHIiYmU/T0qHBT8JqDaYJpeineRVC6TQf5X2fxuTyzZxuQ3EwfjPO3h+OjrJ1hLvsKXvGXrCcvWJH7JhNWMEE+8y+sK/sOvmW/Eh+Jr9uWgfJeuYJ+6cGO78Bg3uzvQ==</latexit>

Current Utterance x̄3
<latexit sha1_base64="yHXbEQsJZWgzUchi9wCnpE+5bvg=">AAACP3icbVBNT9wwFHSgUJpCu8CxF4tVpZ5WCUVquSFx4QgSKUibaPXivAUL24nsl8LKyr2/hiuc+Rf8A05Vr73Vu+yhXTqSpdHM+/KUjZKOkuQxWlp+tbL6eu1N/HZ949373ubWN1e3VmAmalXb8xIcKmkwI0kKzxuLoEuFZ+XV4dQ/+47Wydqc0qTBQsOFkWMpgII06u3khDfkD1tr0RDPiNCCEci7vATrb7rR51GvnwySGfhLks5Jn81xPNqMVvOqFq0OE4UC54Zp0lDhwZIUCrs4bx02IK7gAoeBGtDoCj/7TMc/BqXi49qGFy6aqX93eNDOTXQZKjXQpVv0puJ/vcpNBy5sp/HXwkvTtIRGPC8ft4pTzadp8UpaFKQmgYCwMtzPxSVYECEmF8d5CA2vRa01mMrnohumhfe51byfdl0ckksXc3pJst3B/iA52esfJPMI19gHtsM+sZR9YQfsiB2zjAn2g92yO3YfPURP0c/o13PpUjTv2Wb/IPr9B/vPry0=</latexit><latexit sha1_base64="yHXbEQsJZWgzUchi9wCnpE+5bvg=">AAACP3icbVBNT9wwFHSgUJpCu8CxF4tVpZ5WCUVquSFx4QgSKUibaPXivAUL24nsl8LKyr2/hiuc+Rf8A05Vr73Vu+yhXTqSpdHM+/KUjZKOkuQxWlp+tbL6eu1N/HZ949373ubWN1e3VmAmalXb8xIcKmkwI0kKzxuLoEuFZ+XV4dQ/+47Wydqc0qTBQsOFkWMpgII06u3khDfkD1tr0RDPiNCCEci7vATrb7rR51GvnwySGfhLks5Jn81xPNqMVvOqFq0OE4UC54Zp0lDhwZIUCrs4bx02IK7gAoeBGtDoCj/7TMc/BqXi49qGFy6aqX93eNDOTXQZKjXQpVv0puJ/vcpNBy5sp/HXwkvTtIRGPC8ft4pTzadp8UpaFKQmgYCwMtzPxSVYECEmF8d5CA2vRa01mMrnohumhfe51byfdl0ckksXc3pJst3B/iA52esfJPMI19gHtsM+sZR9YQfsiB2zjAn2g92yO3YfPURP0c/o13PpUjTv2Wb/IPr9B/vPry0=</latexit><latexit sha1_base64="yHXbEQsJZWgzUchi9wCnpE+5bvg=">AAACP3icbVBNT9wwFHSgUJpCu8CxF4tVpZ5WCUVquSFx4QgSKUibaPXivAUL24nsl8LKyr2/hiuc+Rf8A05Vr73Vu+yhXTqSpdHM+/KUjZKOkuQxWlp+tbL6eu1N/HZ949373ubWN1e3VmAmalXb8xIcKmkwI0kKzxuLoEuFZ+XV4dQ/+47Wydqc0qTBQsOFkWMpgII06u3khDfkD1tr0RDPiNCCEci7vATrb7rR51GvnwySGfhLks5Jn81xPNqMVvOqFq0OE4UC54Zp0lDhwZIUCrs4bx02IK7gAoeBGtDoCj/7TMc/BqXi49qGFy6aqX93eNDOTXQZKjXQpVv0puJ/vcpNBy5sp/HXwkvTtIRGPC8ft4pTzadp8UpaFKQmgYCwMtzPxSVYECEmF8d5CA2vRa01mMrnohumhfe51byfdl0ckksXc3pJst3B/iA52esfJPMI19gHtsM+sZR9YQfsiB2zjAn2g92yO3YfPURP0c/o13PpUjTv2Wb/IPr9B/vPry0=</latexit>

MLP
<latexit sha1_base64="FGxIWEFcvBEhuII9AHi8VO3WDcA=">AAACJHicbZDNSgMxFIUz/tbxr9Wlm2ARXJUZEdRdwY0LhQrWKp2hZDK3NjTJDElGKcM8hVtd+zSuxIUbn8VM24W2Hggczr039/JFKWfaeN6Xs7C4tLyyWllz1zc2t7artZ1bnWSKQpsmPFF3EdHAmYS2YYbDXaqAiIhDJxqel/XOIyjNEnljRimEgjxI1meUGBvd54ES+OqyVfSqda/hjYXnjT81dTRVq1dzVoI4oZkAaSgnWnd9LzVhTpRhlEPhBpmGlNAheYCutZII0GE+vrjABzaJcT9R9kmDx+nviZwIrUcisp2CmIGerZXhv7VYlx/ObDf90zBnMs0MSDpZ3s84NgkukeCYKaCGj6whVDF7P6YDogg1FpzrBgokPNFECCLjPKBF1w/zMbi6XxSuJefPcpo37aPGWcO7Pq43vSnCCtpD++gQ+egENdEFaqE2okigZ/SCXp035935cD4nrQvOdGYX/ZHz/QNn6KPE</latexit><latexit sha1_base64="FGxIWEFcvBEhuII9AHi8VO3WDcA=">AAACJHicbZDNSgMxFIUz/tbxr9Wlm2ARXJUZEdRdwY0LhQrWKp2hZDK3NjTJDElGKcM8hVtd+zSuxIUbn8VM24W2Hggczr039/JFKWfaeN6Xs7C4tLyyWllz1zc2t7artZ1bnWSKQpsmPFF3EdHAmYS2YYbDXaqAiIhDJxqel/XOIyjNEnljRimEgjxI1meUGBvd54ES+OqyVfSqda/hjYXnjT81dTRVq1dzVoI4oZkAaSgnWnd9LzVhTpRhlEPhBpmGlNAheYCutZII0GE+vrjABzaJcT9R9kmDx+nviZwIrUcisp2CmIGerZXhv7VYlx/ObDf90zBnMs0MSDpZ3s84NgkukeCYKaCGj6whVDF7P6YDogg1FpzrBgokPNFECCLjPKBF1w/zMbi6XxSuJefPcpo37aPGWcO7Pq43vSnCCtpD++gQ+egENdEFaqE2okigZ/SCXp035935cD4nrQvOdGYX/ZHz/QNn6KPE</latexit><latexit sha1_base64="FGxIWEFcvBEhuII9AHi8VO3WDcA=">AAACJHicbZDNSgMxFIUz/tbxr9Wlm2ARXJUZEdRdwY0LhQrWKp2hZDK3NjTJDElGKcM8hVtd+zSuxIUbn8VM24W2Hggczr039/JFKWfaeN6Xs7C4tLyyWllz1zc2t7artZ1bnWSKQpsmPFF3EdHAmYS2YYbDXaqAiIhDJxqel/XOIyjNEnljRimEgjxI1meUGBvd54ES+OqyVfSqda/hjYXnjT81dTRVq1dzVoI4oZkAaSgnWnd9LzVhTpRhlEPhBpmGlNAheYCutZII0GE+vrjABzaJcT9R9kmDx+nviZwIrUcisp2CmIGerZXhv7VYlx/ObDf90zBnMs0MSDpZ3s84NgkukeCYKaCGj6whVDF7P6YDogg1FpzrBgokPNFECCLjPKBF1w/zMbi6XxSuJefPcpo37aPGWcO7Pq43vSnCCtpD++gQ+egENdEFaqE2okigZ/SCXp035935cD4nrQvOdGYX/ZHz/QNn6KPE</latexit>

Decoder

snippet
<latexit sha1_base64="ZQxgHwmHHzW9gN4+WfjZUN/xCJ0=">AAACQXicbVDLThtBEJwl4bU8Yocjl1EMiJO1iyIBN0vhwJFIGCx5V9bsbBtGzGM10wtYq/0BviZXcs5P8AucolxzYWyMlNiU1FKpqnt6urJCCodR9BQsfPi4uLS8shqurW9sfmo0P184U1oOXW6ksb2MOZBCQxcFSugVFpjKJFxmN9/G/uUtWCeMPsdRAaliV1oMBWfopUFjZ48mCPdYnQA3OdiaJgl905wWRQFYDxqtqB1NQOdJPCUtMsXZoBksJbnhpQKNXDLn+nFUYFoxi4JLqMOkdFAwfsOuoO+pZgpcWk3OqemuV3I6NNaXRjpR/52omHJupDLfqRheu1lvLL7r5W784Mx2HB6lldBFiaD56/JhKSkaOs6L5sICRznyhHEr/P8pv2aWcfSphmFiQcMdN0oxnVcJr/txWlWJVbQV13Xok4tnc5on3YP2cTv6/rXViaYRrpBt8oXsk5gckg45JWekSzh5ID/II/kZ/Aqeg9/Bn9fWhWA6s0X+Q/D3BTD+rzY=</latexit><latexit sha1_base64="ZQxgHwmHHzW9gN4+WfjZUN/xCJ0=">AAACQXicbVDLThtBEJwl4bU8Yocjl1EMiJO1iyIBN0vhwJFIGCx5V9bsbBtGzGM10wtYq/0BviZXcs5P8AucolxzYWyMlNiU1FKpqnt6urJCCodR9BQsfPi4uLS8shqurW9sfmo0P184U1oOXW6ksb2MOZBCQxcFSugVFpjKJFxmN9/G/uUtWCeMPsdRAaliV1oMBWfopUFjZ48mCPdYnQA3OdiaJgl905wWRQFYDxqtqB1NQOdJPCUtMsXZoBksJbnhpQKNXDLn+nFUYFoxi4JLqMOkdFAwfsOuoO+pZgpcWk3OqemuV3I6NNaXRjpR/52omHJupDLfqRheu1lvLL7r5W784Mx2HB6lldBFiaD56/JhKSkaOs6L5sICRznyhHEr/P8pv2aWcfSphmFiQcMdN0oxnVcJr/txWlWJVbQV13Xok4tnc5on3YP2cTv6/rXViaYRrpBt8oXsk5gckg45JWekSzh5ID/II/kZ/Aqeg9/Bn9fWhWA6s0X+Q/D3BTD+rzY=</latexit><latexit sha1_base64="ZQxgHwmHHzW9gN4+WfjZUN/xCJ0=">AAACQXicbVDLThtBEJwl4bU8Yocjl1EMiJO1iyIBN0vhwJFIGCx5V9bsbBtGzGM10wtYq/0BviZXcs5P8AucolxzYWyMlNiU1FKpqnt6urJCCodR9BQsfPi4uLS8shqurW9sfmo0P184U1oOXW6ksb2MOZBCQxcFSugVFpjKJFxmN9/G/uUtWCeMPsdRAaliV1oMBWfopUFjZ48mCPdYnQA3OdiaJgl905wWRQFYDxqtqB1NQOdJPCUtMsXZoBksJbnhpQKNXDLn+nFUYFoxi4JLqMOkdFAwfsOuoO+pZgpcWk3OqemuV3I6NNaXRjpR/52omHJupDLfqRheu1lvLL7r5W784Mx2HB6lldBFiaD56/JhKSkaOs6L5sICRznyhHEr/P8pv2aWcfSphmFiQcMdN0oxnVcJr/txWlWJVbQV13Xok4tnc5on3YP2cTv6/rXViaYRrpBt8oXsk5gckg45JWekSzh5ID/II/kZ/Aqeg9/Bn9fWhWA6s0X+Q/D3BTD+rzY=</latexit>

Current state s3
<latexit sha1_base64="wP0LmfnIO0hDCuIKBqTz5N56Nd0=">AAACNnicbVDLSsNAFJ34Nr5aXYmbwSK4KokK6q7gxmUFq0ITymRyWwdnkjBzo5YQ/Bq3uvZTXLkSt36C08dCWw8MHM65rzlRJoVBz3t3Zmbn5hcWl5bdldW19Y1KdfPKpLnm0OKpTPVNxAxIkUALBUq4yTQwFUm4ju7OBv71PWgj0uQS+xmEivUS0RWcoZU6le0A4RGLs1xrSJAaZAi0pKZz2KnUvLo3BJ0m/pjUyBjNTtVZCOKU58oO4pIZ0/a9DMOCaRRcQukGuYGM8TvWg7alCVNgwmL4h5LuWSWm3VTbZw8Zqr87CqaM6avIViqGt2bSG4j/erEZDJzYjt2TsBBJliMkfLS8m0uKKR2ERGOhgaPsW8K4FvZ+ym+ZZhxtlK4b2KzggadKsSQuAl62/bAoAq1ozS9L1ybnT+Y0TVoH9dO6d3FUa3jjCJfIDtkl+8Qnx6RBzkmTtAgnT+SZvJBX5835cD6dr1HpjDPu2SJ/4Hz/AG+Oqts=</latexit><latexit sha1_base64="wP0LmfnIO0hDCuIKBqTz5N56Nd0=">AAACNnicbVDLSsNAFJ34Nr5aXYmbwSK4KokK6q7gxmUFq0ITymRyWwdnkjBzo5YQ/Bq3uvZTXLkSt36C08dCWw8MHM65rzlRJoVBz3t3Zmbn5hcWl5bdldW19Y1KdfPKpLnm0OKpTPVNxAxIkUALBUq4yTQwFUm4ju7OBv71PWgj0uQS+xmEivUS0RWcoZU6le0A4RGLs1xrSJAaZAi0pKZz2KnUvLo3BJ0m/pjUyBjNTtVZCOKU58oO4pIZ0/a9DMOCaRRcQukGuYGM8TvWg7alCVNgwmL4h5LuWSWm3VTbZw8Zqr87CqaM6avIViqGt2bSG4j/erEZDJzYjt2TsBBJliMkfLS8m0uKKR2ERGOhgaPsW8K4FvZ+ym+ZZhxtlK4b2KzggadKsSQuAl62/bAoAq1ozS9L1ybnT+Y0TVoH9dO6d3FUa3jjCJfIDtkl+8Qnx6RBzkmTtAgnT+SZvJBX5835cD6dr1HpjDPu2SJ/4Hz/AG+Oqts=</latexit><latexit sha1_base64="wP0LmfnIO0hDCuIKBqTz5N56Nd0=">AAACNnicbVDLSsNAFJ34Nr5aXYmbwSK4KokK6q7gxmUFq0ITymRyWwdnkjBzo5YQ/Bq3uvZTXLkSt36C08dCWw8MHM65rzlRJoVBz3t3Zmbn5hcWl5bdldW19Y1KdfPKpLnm0OKpTPVNxAxIkUALBUq4yTQwFUm4ju7OBv71PWgj0uQS+xmEivUS0RWcoZU6le0A4RGLs1xrSJAaZAi0pKZz2KnUvLo3BJ0m/pjUyBjNTtVZCOKU58oO4pIZ0/a9DMOCaRRcQukGuYGM8TvWg7alCVNgwmL4h5LuWSWm3VTbZw8Zqr87CqaM6avIViqGt2bSG4j/erEZDJzYjt2TsBBJliMkfLS8m0uKKR2ERGOhgaPsW8K4FvZ+ym+ZZhxtlK4b2KzggadKsSQuAl62/bAoAq1ozS9L1ybnT+Y0TVoH9dO6d3FUa3jjCJfIDtkl+8Qnx6RBzkmTtAgnT+SZvJBX5835cD6dr1HpjDPu2SJ/4Hz/AG+Oqts=</latexit>

a2<latexit sha1_base64="SUbkZWmj3PoRhhsHQX1zVFHCx3E=">AAACHnicbVDLSsNAFJ34qDW+Wl26GSyCq5IUQd0V3LisaG2hCWUyuW2HzkzCzEQpIZ/gVtd+jStxq3/j9LHQ1gMXDufcFydKOdPG876dtfWNzdJWedvd2d3bP6hUDx90kikKbZrwRHUjooEzCW3DDIduqoCIiEMnGl9P/c4jKM0SeW8mKYSCDCUbMEqMle5Iv9Gv1Ly6NwNeJf6C1NACrX7VKQVxQjMB0lBOtO75XmrCnCjDKIfCDTINKaFjMoSepZII0GE++7XAp1aJ8SBRtqTBM/X3RE6E1hMR2U5BzEgve1PxXy/W04VL183gMsyZTDMDks6PDzKOTYKnYeCYKaCGTywhVDH7P6Yjogg1NjLXDRRIeKKJEETGeUCLnh/meaAErvlF4drk/OWcVkm7Ub+qe7fntaa3iLCMjtEJOkM+ukBNdINaqI0oGqJn9IJenTfn3flwPueta85i5gj9gfP1A41QoT4=</latexit><latexit sha1_base64="SUbkZWmj3PoRhhsHQX1zVFHCx3E=">AAACHnicbVDLSsNAFJ34qDW+Wl26GSyCq5IUQd0V3LisaG2hCWUyuW2HzkzCzEQpIZ/gVtd+jStxq3/j9LHQ1gMXDufcFydKOdPG876dtfWNzdJWedvd2d3bP6hUDx90kikKbZrwRHUjooEzCW3DDIduqoCIiEMnGl9P/c4jKM0SeW8mKYSCDCUbMEqMle5Iv9Gv1Ly6NwNeJf6C1NACrX7VKQVxQjMB0lBOtO75XmrCnCjDKIfCDTINKaFjMoSepZII0GE++7XAp1aJ8SBRtqTBM/X3RE6E1hMR2U5BzEgve1PxXy/W04VL183gMsyZTDMDks6PDzKOTYKnYeCYKaCGTywhVDH7P6Yjogg1NjLXDRRIeKKJEETGeUCLnh/meaAErvlF4drk/OWcVkm7Ub+qe7fntaa3iLCMjtEJOkM+ukBNdINaqI0oGqJn9IJenTfn3flwPueta85i5gj9gfP1A41QoT4=</latexit><latexit sha1_base64="SUbkZWmj3PoRhhsHQX1zVFHCx3E=">AAACHnicbVDLSsNAFJ34qDW+Wl26GSyCq5IUQd0V3LisaG2hCWUyuW2HzkzCzEQpIZ/gVtd+jStxq3/j9LHQ1gMXDufcFydKOdPG876dtfWNzdJWedvd2d3bP6hUDx90kikKbZrwRHUjooEzCW3DDIduqoCIiEMnGl9P/c4jKM0SeW8mKYSCDCUbMEqMle5Iv9Gv1Ly6NwNeJf6C1NACrX7VKQVxQjMB0lBOtO75XmrCnCjDKIfCDTINKaFjMoSepZII0GE++7XAp1aJ8SBRtqTBM/X3RE6E1hMR2U5BzEgve1PxXy/W04VL183gMsyZTDMDks6PDzKOTYKnYeCYKaCGTywhVDH7P6Yjogg1NjLXDRRIeKKJEETGeUCLnh/meaAErvlF4drk/OWcVkm7Ub+qe7fntaa3iLCMjtEJOkM+ukBNdINaqI0oGqJn9IJenTfn3flwPueta85i5gj9gfP1A41QoT4=</latexit> a3<latexit sha1_base64="GbHL2sz697mZWBzIQupVbiLk95o=">AAACHnicbVDLSsNAFJ3UV42vVpduBovgqiQqqLuCG5cVrS00oUwmt+3QmUmYmSgl5BPc6tqvcSVu9W+cPhbaeuDC4Zz74kQpZ9p43rdTWlldW98ob7pb2zu7e5Xq/oNOMkWhRROeqE5ENHAmoWWY4dBJFRARcWhHo+uJ334EpVki7804hVCQgWR9Romx0h3pnfUqNa/uTYGXiT8nNTRHs1d11oM4oZkAaSgnWnd9LzVhTpRhlEPhBpmGlNARGUDXUkkE6DCf/lrgY6vEuJ8oW9Lgqfp7IidC67GIbKcgZqgXvYn4rxfrycKF66Z/GeZMppkBSWfH+xnHJsGTMHDMFFDDx5YQqpj9H9MhUYQaG5nrBgokPNFECCLjPKBF1w/zPFAC1/yicG1y/mJOy6R1Wr+qe7fntYY3j7CMDtEROkE+ukANdIOaqIUoGqBn9IJenTfn3flwPmetJWc+c4D+wPn6AY8IoT8=</latexit><latexit sha1_base64="GbHL2sz697mZWBzIQupVbiLk95o=">AAACHnicbVDLSsNAFJ3UV42vVpduBovgqiQqqLuCG5cVrS00oUwmt+3QmUmYmSgl5BPc6tqvcSVu9W+cPhbaeuDC4Zz74kQpZ9p43rdTWlldW98ob7pb2zu7e5Xq/oNOMkWhRROeqE5ENHAmoWWY4dBJFRARcWhHo+uJ334EpVki7804hVCQgWR9Romx0h3pnfUqNa/uTYGXiT8nNTRHs1d11oM4oZkAaSgnWnd9LzVhTpRhlEPhBpmGlNARGUDXUkkE6DCf/lrgY6vEuJ8oW9Lgqfp7IidC67GIbKcgZqgXvYn4rxfrycKF66Z/GeZMppkBSWfH+xnHJsGTMHDMFFDDx5YQqpj9H9MhUYQaG5nrBgokPNFECCLjPKBF1w/zPFAC1/yicG1y/mJOy6R1Wr+qe7fntYY3j7CMDtEROkE+ukANdIOaqIUoGqBn9IJenTfn3flwPmetJWc+c4D+wPn6AY8IoT8=</latexit><latexit sha1_base64="GbHL2sz697mZWBzIQupVbiLk95o=">AAACHnicbVDLSsNAFJ3UV42vVpduBovgqiQqqLuCG5cVrS00oUwmt+3QmUmYmSgl5BPc6tqvcSVu9W+cPhbaeuDC4Zz74kQpZ9p43rdTWlldW98ob7pb2zu7e5Xq/oNOMkWhRROeqE5ENHAmoWWY4dBJFRARcWhHo+uJ334EpVki7804hVCQgWR9Romx0h3pnfUqNa/uTYGXiT8nNTRHs1d11oM4oZkAaSgnWnd9LzVhTpRhlEPhBpmGlNARGUDXUkkE6DCf/lrgY6vEuJ8oW9Lgqfp7IidC67GIbKcgZqgXvYn4rxfrycKF66Z/GeZMppkBSWfH+xnHJsGTMHDMFFDDx5YQqpj9H9MhUYQaG5nrBgokPNFECCLjPKBF1w/zPFAC1/yicG1y/mJOy6R1Wr+qe7fntYY3j7CMDtEROkE+ukANdIOaqIUoGqBn9IJenTfn3flwPmetJWc+c4D+wPn6AY8IoT8=</latexit>

zc3
<latexit sha1_base64="EJC7QrFvNSewx3yUndYnljjFodc=">AAACKXicbVA9T8MwFHTKd/gqMLJYVEhMVQJIwFaJhbFIlFZqQuU4L2DVdiLbAZUo/4MVZn4NE7DyR3DaDlA4ydLp7j2/00UZZ9p43odTm5tfWFxaXnFX19Y3Nutb29c6zRWFDk15qnoR0cCZhI5hhkMvU0BExKEbDc8rv3sPSrNUXplRBqEgt5IljBJjpZtAEHMXJcVjeUMHR4N6w2t6Y+C/xJ+SBpqiPdhyFoM4pbkAaSgnWvd9LzNhQZRhlEPpBrmGjNAhuYW+pZII0GExjl3ifavEOEmVfdLgsfpzoyBC65GI7GQVU896lfivF+vqw5nrJjkNCyaz3ICkk+NJzrFJcdULjpkCavjIEkIVs/kxvSOKUGPbc91AgYQHmgpBZFwEtOz7YVEESuCGX5aubc6f7ekv6Rw2z5re5XGj5U0rXEa7aA8dIB+doBa6QG3UQRQp9ISe0Yvz6rw5787nZLTmTHd20C84X98/k6ZN</latexit><latexit sha1_base64="EJC7QrFvNSewx3yUndYnljjFodc=">AAACKXicbVA9T8MwFHTKd/gqMLJYVEhMVQJIwFaJhbFIlFZqQuU4L2DVdiLbAZUo/4MVZn4NE7DyR3DaDlA4ydLp7j2/00UZZ9p43odTm5tfWFxaXnFX19Y3Nutb29c6zRWFDk15qnoR0cCZhI5hhkMvU0BExKEbDc8rv3sPSrNUXplRBqEgt5IljBJjpZtAEHMXJcVjeUMHR4N6w2t6Y+C/xJ+SBpqiPdhyFoM4pbkAaSgnWvd9LzNhQZRhlEPpBrmGjNAhuYW+pZII0GExjl3ifavEOEmVfdLgsfpzoyBC65GI7GQVU896lfivF+vqw5nrJjkNCyaz3ICkk+NJzrFJcdULjpkCavjIEkIVs/kxvSOKUGPbc91AgYQHmgpBZFwEtOz7YVEESuCGX5aubc6f7ekv6Rw2z5re5XGj5U0rXEa7aA8dIB+doBa6QG3UQRQp9ISe0Yvz6rw5787nZLTmTHd20C84X98/k6ZN</latexit><latexit sha1_base64="EJC7QrFvNSewx3yUndYnljjFodc=">AAACKXicbVA9T8MwFHTKd/gqMLJYVEhMVQJIwFaJhbFIlFZqQuU4L2DVdiLbAZUo/4MVZn4NE7DyR3DaDlA4ydLp7j2/00UZZ9p43odTm5tfWFxaXnFX19Y3Nutb29c6zRWFDk15qnoR0cCZhI5hhkMvU0BExKEbDc8rv3sPSrNUXplRBqEgt5IljBJjpZtAEHMXJcVjeUMHR4N6w2t6Y+C/xJ+SBpqiPdhyFoM4pbkAaSgnWvd9LzNhQZRhlEPpBrmGjNAhuYW+pZII0GExjl3ifavEOEmVfdLgsfpzoyBC65GI7GQVU896lfivF+vqw5nrJjkNCyaz3ICkk+NJzrFJcdULjpkCavjIEkIVs/kxvSOKUGPbc91AgYQHmgpBZFwEtOz7YVEESuCGX5aubc6f7ekv6Rw2z5re5XGj5U0rXEa7aA8dIB+doBa6QG3UQRQp9ISe0Yvz6rw5787nZLTmTHd20C84X98/k6ZN</latexit>

zp3
<latexit sha1_base64="8Bany60kVwAIAuetY5K9K0r4ASs=">AAACKXicbVA9T8MwFHT4JnzDyGJRITFVCSABGxILY5EIrdSEynFeWgvbiWwHVKL8D1aY+TVMwMofwWk7QOEkS6e79/xOF+ecaeN5H87M7Nz8wuLSsruyura+sbm1faOzQlEIaMYz1YmJBs4kBIYZDp1cARExh3Z8d1H77XtQmmXy2gxziATpS5YySoyVbkNBzCBOy8fqNu8d9TYbXtMbAf8l/oQ00ASt3pazECYZLQRIQznRuut7uYlKogyjHCo3LDTkhN6RPnQtlUSAjspR7ArvWyXBaabskwaP1J8bJRFaD0VsJ+uYetqrxX+9RNcfTl036WlUMpkXBiQdH08Ljk2G615wwhRQw4eWEKqYzY/pgChCjW3PdUMFEh5oJgSRSRnSqutHZRkqgRt+Vbm2OX+6p78kOGyeNb2r48a5N6lwCe2iPXSAfHSCztElaqEAUaTQE3pGL86r8+a8O5/j0RlnsrODfsH5+gZWBaZa</latexit><latexit sha1_base64="8Bany60kVwAIAuetY5K9K0r4ASs=">AAACKXicbVA9T8MwFHT4JnzDyGJRITFVCSABGxILY5EIrdSEynFeWgvbiWwHVKL8D1aY+TVMwMofwWk7QOEkS6e79/xOF+ecaeN5H87M7Nz8wuLSsruyura+sbm1faOzQlEIaMYz1YmJBs4kBIYZDp1cARExh3Z8d1H77XtQmmXy2gxziATpS5YySoyVbkNBzCBOy8fqNu8d9TYbXtMbAf8l/oQ00ASt3pazECYZLQRIQznRuut7uYlKogyjHCo3LDTkhN6RPnQtlUSAjspR7ArvWyXBaabskwaP1J8bJRFaD0VsJ+uYetqrxX+9RNcfTl036WlUMpkXBiQdH08Ljk2G615wwhRQw4eWEKqYzY/pgChCjW3PdUMFEh5oJgSRSRnSqutHZRkqgRt+Vbm2OX+6p78kOGyeNb2r48a5N6lwCe2iPXSAfHSCztElaqEAUaTQE3pGL86r8+a8O5/j0RlnsrODfsH5+gZWBaZa</latexit><latexit sha1_base64="8Bany60kVwAIAuetY5K9K0r4ASs=">AAACKXicbVA9T8MwFHT4JnzDyGJRITFVCSABGxILY5EIrdSEynFeWgvbiWwHVKL8D1aY+TVMwMofwWk7QOEkS6e79/xOF+ecaeN5H87M7Nz8wuLSsruyura+sbm1faOzQlEIaMYz1YmJBs4kBIYZDp1cARExh3Z8d1H77XtQmmXy2gxziATpS5YySoyVbkNBzCBOy8fqNu8d9TYbXtMbAf8l/oQ00ASt3pazECYZLQRIQznRuut7uYlKogyjHCo3LDTkhN6RPnQtlUSAjspR7ArvWyXBaabskwaP1J8bJRFaD0VsJ+uYetqrxX+9RNcfTl036WlUMpkXBiQdH08Ljk2G615wwhRQw4eWEKqYzY/pgChCjW3PdUMFEh5oJgSRSRnSqutHZRkqgRt+Vbm2OX+6p78kOGyeNb2r48a5N6lwCe2iPXSAfHSCztElaqEAUaTQE3pGL86r8+a8O5/j0RlnsrODfsH5+gZWBaZa</latexit>

zs1,3
<latexit sha1_base64="n3EkG0a5jSiHqVLmztodlUtaw1s=">AAACL3icbVBNS8QwFEz9tn6tevQSXAQPsrQqqLcFLx4VXBW2dUnTVw0maUlSdQ39K1717K/Ri3j1X5iue9DVgcAw817eMEnBmTZB8OaNjU9MTk3PzPpz8wuLS43llTOdl4pCh+Y8VxcJ0cCZhI5hhsNFoYCIhMN5cnNY++e3oDTL5anpFxALciVZxigxTuo1ViJBzHWS2YfqUvdsuLVT9RrNoBUMgP+ScEiaaIjj3rI3FaU5LQVIQznRuhsGhYktUYZRDpUflRoKQm/IFXQdlUSAju0gfIU3nJLiLFfuSYMH6s8NS4TWfZG4yTqqHvVq8V8v1fWHI9dNth9bJovSgKTfx7OSY5Pjuh2cMgXU8L4jhCrm8mN6TRShxnXo+5ECCXc0F4LI1Ea06oaxtZESuBlWle+aC0d7+ks6262DVnCy22wHwwpn0BpaR5soRHuojY7QMeogiu7RI3pCz96L9+q9ex/fo2PecGcV/YL3+QXRdKgL</latexit><latexit sha1_base64="n3EkG0a5jSiHqVLmztodlUtaw1s=">AAACL3icbVBNS8QwFEz9tn6tevQSXAQPsrQqqLcFLx4VXBW2dUnTVw0maUlSdQ39K1717K/Ri3j1X5iue9DVgcAw817eMEnBmTZB8OaNjU9MTk3PzPpz8wuLS43llTOdl4pCh+Y8VxcJ0cCZhI5hhsNFoYCIhMN5cnNY++e3oDTL5anpFxALciVZxigxTuo1ViJBzHWS2YfqUvdsuLVT9RrNoBUMgP+ScEiaaIjj3rI3FaU5LQVIQznRuhsGhYktUYZRDpUflRoKQm/IFXQdlUSAju0gfIU3nJLiLFfuSYMH6s8NS4TWfZG4yTqqHvVq8V8v1fWHI9dNth9bJovSgKTfx7OSY5Pjuh2cMgXU8L4jhCrm8mN6TRShxnXo+5ECCXc0F4LI1Ea06oaxtZESuBlWle+aC0d7+ks6262DVnCy22wHwwpn0BpaR5soRHuojY7QMeogiu7RI3pCz96L9+q9ex/fo2PecGcV/YL3+QXRdKgL</latexit><latexit sha1_base64="n3EkG0a5jSiHqVLmztodlUtaw1s=">AAACL3icbVBNS8QwFEz9tn6tevQSXAQPsrQqqLcFLx4VXBW2dUnTVw0maUlSdQ39K1717K/Ri3j1X5iue9DVgcAw817eMEnBmTZB8OaNjU9MTk3PzPpz8wuLS43llTOdl4pCh+Y8VxcJ0cCZhI5hhsNFoYCIhMN5cnNY++e3oDTL5anpFxALciVZxigxTuo1ViJBzHWS2YfqUvdsuLVT9RrNoBUMgP+ScEiaaIjj3rI3FaU5LQVIQznRuhsGhYktUYZRDpUflRoKQm/IFXQdlUSAju0gfIU3nJLiLFfuSYMH6s8NS4TWfZG4yTqqHvVq8V8v1fWHI9dNth9bJovSgKTfx7OSY5Pjuh2cMgXU8L4jhCrm8mN6TRShxnXo+5ECCXc0F4LI1Ea06oaxtZESuBlWle+aC0d7+ks6262DVnCy22wHwwpn0BpaR5soRHuojY7QMeogiu7RI3pCz96L9+q9ex/fo2PecGcV/YL3+QXRdKgL</latexit>

zs3,3
<latexit sha1_base64="tM1KNEgIwbncVNhrpEpM3h4jr44=">AAACL3icbVBNS8QwFEz9tn6tevQSXAQPsrSuoN4ELx4VXBW2dUnTVw0maUlSdQ39K1717K/Ri3j1X5iue9DVgcAw817eMEnBmTZB8OaNjU9MTk3PzPpz8wuLS43llTOdl4pCh+Y8VxcJ0cCZhI5hhsNFoYCIhMN5cnNY++e3oDTL5anpFxALciVZxigxTuo1ViJBzHWS2YfqUvdse6td9RrNoBUMgP+ScEiaaIjj3rI3FaU5LQVIQznRuhsGhYktUYZRDpUflRoKQm/IFXQdlUSAju0gfIU3nJLiLFfuSYMH6s8NS4TWfZG4yTqqHvVq8V8v1fWHI9dNthdbJovSgKTfx7OSY5Pjuh2cMgXU8L4jhCrm8mN6TRShxnXo+5ECCXc0F4LI1Ea06oaxtZESuBlWle+aC0d7+ks62639VnCy0zwIhhXOoDW0jjZRiHbRATpCx6iDKLpHj+gJPXsv3qv37n18j455w51V9Ave5xfU6qgN</latexit><latexit sha1_base64="tM1KNEgIwbncVNhrpEpM3h4jr44=">AAACL3icbVBNS8QwFEz9tn6tevQSXAQPsrSuoN4ELx4VXBW2dUnTVw0maUlSdQ39K1717K/Ri3j1X5iue9DVgcAw817eMEnBmTZB8OaNjU9MTk3PzPpz8wuLS43llTOdl4pCh+Y8VxcJ0cCZhI5hhsNFoYCIhMN5cnNY++e3oDTL5anpFxALciVZxigxTuo1ViJBzHWS2YfqUvdse6td9RrNoBUMgP+ScEiaaIjj3rI3FaU5LQVIQznRuhsGhYktUYZRDpUflRoKQm/IFXQdlUSAju0gfIU3nJLiLFfuSYMH6s8NS4TWfZG4yTqqHvVq8V8v1fWHI9dNthdbJovSgKTfx7OSY5Pjuh2cMgXU8L4jhCrm8mN6TRShxnXo+5ECCXc0F4LI1Ea06oaxtZESuBlWle+aC0d7+ks62639VnCy0zwIhhXOoDW0jjZRiHbRATpCx6iDKLpHj+gJPXsv3qv37n18j455w51V9Ave5xfU6qgN</latexit><latexit sha1_base64="tM1KNEgIwbncVNhrpEpM3h4jr44=">AAACL3icbVBNS8QwFEz9tn6tevQSXAQPsrSuoN4ELx4VXBW2dUnTVw0maUlSdQ39K1717K/Ri3j1X5iue9DVgcAw817eMEnBmTZB8OaNjU9MTk3PzPpz8wuLS43llTOdl4pCh+Y8VxcJ0cCZhI5hhsNFoYCIhMN5cnNY++e3oDTL5anpFxALciVZxigxTuo1ViJBzHWS2YfqUvdse6td9RrNoBUMgP+ScEiaaIjj3rI3FaU5LQVIQznRuhsGhYktUYZRDpUflRoKQm/IFXQdlUSAju0gfIU3nJLiLFfuSYMH6s8NS4TWfZG4yTqqHvVq8V8v1fWHI9dNthdbJovSgKTfx7OSY5Pjuh2cMgXU8L4jhCrm8mN6TRShxnXo+5ECCXc0F4LI1Ea06oaxtZESuBlWle+aC0d7+ks62639VnCy0zwIhhXOoDW0jjZRiHbRATpCx6iDKLpHj+gJPXsv3qv37n18j455w51V9Ave5xfU6qgN</latexit>

�O(a2)
<latexit sha1_base64="SyYd38ATr7lW/RbaJR76zfrCZnQ=">AAACJnicbVDLSsNAFJ34rPHV6tLNYBHqpiQiqLuCG3dWsCo0sUwmt+3gPMLMRCkhn+FW136NKxF3forT2oVWD1w4nHNfnCTjzNgg+PDm5hcWl5YrK/7q2vrGZrW2dWVUril0qOJK3yTEAGcSOpZZDjeZBiISDtfJ3enYv74HbZiSl3aUQSzIQLI+o8Q6qRtlQ3Z73iC9g/1etR40gwnwXxJOSR1N0e7VvKUoVTQXIC3lxJhuGGQ2Loi2jHIo/Sg3kBF6RwbQdVQSASYuJj+XeM8pKe4r7UpaPFF/ThREGDMSiesUxA7NrDcW//VSM144c932j+OCySy3IOn38X7OsVV4HApOmQZq+cgRQjVz/2M6JJpQ66Lz/UiDhAeqhCAyLSJadsO4KCItcD0sS98lF87m9Jd0DponzeDisN4KphFW0A7aRQ0UoiPUQmeojTqIIoUe0RN69l68V+/Ne/9unfOmM9voF7zPL0QipCk=</latexit><latexit sha1_base64="SyYd38ATr7lW/RbaJR76zfrCZnQ=">AAACJnicbVDLSsNAFJ34rPHV6tLNYBHqpiQiqLuCG3dWsCo0sUwmt+3gPMLMRCkhn+FW136NKxF3forT2oVWD1w4nHNfnCTjzNgg+PDm5hcWl5YrK/7q2vrGZrW2dWVUril0qOJK3yTEAGcSOpZZDjeZBiISDtfJ3enYv74HbZiSl3aUQSzIQLI+o8Q6qRtlQ3Z73iC9g/1etR40gwnwXxJOSR1N0e7VvKUoVTQXIC3lxJhuGGQ2Loi2jHIo/Sg3kBF6RwbQdVQSASYuJj+XeM8pKe4r7UpaPFF/ThREGDMSiesUxA7NrDcW//VSM144c932j+OCySy3IOn38X7OsVV4HApOmQZq+cgRQjVz/2M6JJpQ66Lz/UiDhAeqhCAyLSJadsO4KCItcD0sS98lF87m9Jd0DponzeDisN4KphFW0A7aRQ0UoiPUQmeojTqIIoUe0RN69l68V+/Ne/9unfOmM9voF7zPL0QipCk=</latexit><latexit sha1_base64="SyYd38ATr7lW/RbaJR76zfrCZnQ=">AAACJnicbVDLSsNAFJ34rPHV6tLNYBHqpiQiqLuCG3dWsCo0sUwmt+3gPMLMRCkhn+FW136NKxF3forT2oVWD1w4nHNfnCTjzNgg+PDm5hcWl5YrK/7q2vrGZrW2dWVUril0qOJK3yTEAGcSOpZZDjeZBiISDtfJ3enYv74HbZiSl3aUQSzIQLI+o8Q6qRtlQ3Z73iC9g/1etR40gwnwXxJOSR1N0e7VvKUoVTQXIC3lxJhuGGQ2Loi2jHIo/Sg3kBF6RwbQdVQSASYuJj+XeM8pKe4r7UpaPFF/ThREGDMSiesUxA7NrDcW//VSM144c932j+OCySy3IOn38X7OsVV4HApOmQZq+cgRQjVz/2M6JJpQ66Lz/UiDhAeqhCAyLSJadsO4KCItcD0sS98lF87m9Jd0DponzeDisN4KphFW0A7aRQ0UoiPUQmeojTqIIoUe0RN69l68V+/Ne/9unfOmM9voF7zPL0QipCk=</latexit>

Start state

context zs1,3
<latexit sha1_base64="WWUGQFpqtV8IiAsh8xOrQDgJrZk=">AAACWXicbVBNTxRBEO0dFdZBcIEjl44bjAezmQETIPFA4sUjRFZIdsZNT08NdOiPSXcNuHTmz/hrvOrNxB9jz7IkuviSTr+8V9VV/YpaCodJ8qsXPXn6bGW1/zxee7G+8XKwufXZmcZyGHMjjb0omAMpNIxRoISL2gJThYTz4vpD55/fgHXC6DOc1ZArdqlFJTjDIE0H71/TDOEr+k/ILFKHDKGlWUYfdG50d9MgKoZXReXv2i9u6tO3++10MExGyRz0MUkXZEgWOJlu9lay0vBGgUYumXOTNKkx92G04BLaOGsc1Ixfs0uYBKqZApf7+TdbuhuUklbGhqORztW/OzxTzs1UESq7Vd2y14n/9UrXPbg0HavD3AtdNwia3w+vGknR0C5HWgoLHOUsEMatCPtTfsUs4xjSjuPMgoZbbpRiuvQZbydp7n1mFR2mbRuH5NLlnB6T8d7oaJScvhseJ4sI+2SHvCJvSEoOyDH5SE7ImHDyjXwnP8jP3u8oivpRfF8a9RY92+QfRNt/ACL5tP0=</latexit><latexit sha1_base64="WWUGQFpqtV8IiAsh8xOrQDgJrZk=">AAACWXicbVBNTxRBEO0dFdZBcIEjl44bjAezmQETIPFA4sUjRFZIdsZNT08NdOiPSXcNuHTmz/hrvOrNxB9jz7IkuviSTr+8V9VV/YpaCodJ8qsXPXn6bGW1/zxee7G+8XKwufXZmcZyGHMjjb0omAMpNIxRoISL2gJThYTz4vpD55/fgHXC6DOc1ZArdqlFJTjDIE0H71/TDOEr+k/ILFKHDKGlWUYfdG50d9MgKoZXReXv2i9u6tO3++10MExGyRz0MUkXZEgWOJlu9lay0vBGgUYumXOTNKkx92G04BLaOGsc1Ixfs0uYBKqZApf7+TdbuhuUklbGhqORztW/OzxTzs1UESq7Vd2y14n/9UrXPbg0HavD3AtdNwia3w+vGknR0C5HWgoLHOUsEMatCPtTfsUs4xjSjuPMgoZbbpRiuvQZbydp7n1mFR2mbRuH5NLlnB6T8d7oaJScvhseJ4sI+2SHvCJvSEoOyDH5SE7ImHDyjXwnP8jP3u8oivpRfF8a9RY92+QfRNt/ACL5tP0=</latexit><latexit sha1_base64="WWUGQFpqtV8IiAsh8xOrQDgJrZk=">AAACWXicbVBNTxRBEO0dFdZBcIEjl44bjAezmQETIPFA4sUjRFZIdsZNT08NdOiPSXcNuHTmz/hrvOrNxB9jz7IkuviSTr+8V9VV/YpaCodJ8qsXPXn6bGW1/zxee7G+8XKwufXZmcZyGHMjjb0omAMpNIxRoISL2gJThYTz4vpD55/fgHXC6DOc1ZArdqlFJTjDIE0H71/TDOEr+k/ILFKHDKGlWUYfdG50d9MgKoZXReXv2i9u6tO3++10MExGyRz0MUkXZEgWOJlu9lay0vBGgUYumXOTNKkx92G04BLaOGsc1Ixfs0uYBKqZApf7+TdbuhuUklbGhqORztW/OzxTzs1UESq7Vd2y14n/9UrXPbg0HavD3AtdNwia3w+vGknR0C5HWgoLHOUsEMatCPtTfsUs4xjSjuPMgoZbbpRiuvQZbydp7n1mFR2mbRuH5NLlnB6T8d7oaJScvhseJ4sI+2SHvCJvSEoOyDH5SE7ImHDyjXwnP8jP3u8oivpRfF8a9RY92+QfRNt/ACL5tP0=</latexit>

Current state

context zs3,3
<latexit sha1_base64="IHRIBcXwdmI+lhfupt36Qb2Aieo=">AAACXXicbVBNb9QwEPUGWEooZUsPHLhYrKg4oFXSIlFuK/XSY5HYttImrBxn0lr1R2RPoIsVfg6/ptciceOn4GxzoFtGsvz03ozn+RW1FA6T5PcgevDw0fDxxpP46eazreej7RcnzjSWw4wbaexZwRxIoWGGAiWc1RaYKiScFpeHnX76FawTRn/GZQ25YudaVIIzDNRiNN2lGcIV+sPGWtBIHTKElmYZ3aU/eo0b3d000IrhRVH57+0Xt/D77/bbxWicTJJV0fsg7cGY9HW82B4Ms9LwRoVtXDLn5mlSY+6ZRcEltHHWOKgZv2TnMA9QMwUu96uvtvRNYEpaGRtOcLti/53wTDm3VEXo7Ky6da0j/6uVrntwbTtWB7kXum4QNL9dXjWSoqFdlrQUFjjKZQCMWxH8U37BLOMYEo/jLAQK37hRiunSZ7ydp7n3mVV0nLZtHJJL13O6D2Z7k4+T5NP78TTpI9wgr8hr8pak5AOZkiNyTGaEk5/kmtyQX4M/0TDajLZuW6NBP7ND7lT08i+VNLaa</latexit><latexit sha1_base64="IHRIBcXwdmI+lhfupt36Qb2Aieo=">AAACXXicbVBNb9QwEPUGWEooZUsPHLhYrKg4oFXSIlFuK/XSY5HYttImrBxn0lr1R2RPoIsVfg6/ptciceOn4GxzoFtGsvz03ozn+RW1FA6T5PcgevDw0fDxxpP46eazreej7RcnzjSWw4wbaexZwRxIoWGGAiWc1RaYKiScFpeHnX76FawTRn/GZQ25YudaVIIzDNRiNN2lGcIV+sPGWtBIHTKElmYZ3aU/eo0b3d000IrhRVH57+0Xt/D77/bbxWicTJJV0fsg7cGY9HW82B4Ms9LwRoVtXDLn5mlSY+6ZRcEltHHWOKgZv2TnMA9QMwUu96uvtvRNYEpaGRtOcLti/53wTDm3VEXo7Ky6da0j/6uVrntwbTtWB7kXum4QNL9dXjWSoqFdlrQUFjjKZQCMWxH8U37BLOMYEo/jLAQK37hRiunSZ7ydp7n3mVV0nLZtHJJL13O6D2Z7k4+T5NP78TTpI9wgr8hr8pak5AOZkiNyTGaEk5/kmtyQX4M/0TDajLZuW6NBP7ND7lT08i+VNLaa</latexit><latexit sha1_base64="IHRIBcXwdmI+lhfupt36Qb2Aieo=">AAACXXicbVBNb9QwEPUGWEooZUsPHLhYrKg4oFXSIlFuK/XSY5HYttImrBxn0lr1R2RPoIsVfg6/ptciceOn4GxzoFtGsvz03ozn+RW1FA6T5PcgevDw0fDxxpP46eazreej7RcnzjSWw4wbaexZwRxIoWGGAiWc1RaYKiScFpeHnX76FawTRn/GZQ25YudaVIIzDNRiNN2lGcIV+sPGWtBIHTKElmYZ3aU/eo0b3d000IrhRVH57+0Xt/D77/bbxWicTJJV0fsg7cGY9HW82B4Ms9LwRoVtXDLn5mlSY+6ZRcEltHHWOKgZv2TnMA9QMwUu96uvtvRNYEpaGRtOcLti/53wTDm3VEXo7Ky6da0j/6uVrntwbTtWB7kXum4QNL9dXjWSoqFdlrQUFjjKZQCMWxH8U37BLOMYEo/jLAQK37hRiunSZ7ydp7n3mVV0nLZtHJJL13O6D2Z7k4+T5NP78TTpI9wgr8hr8pak5AOZkiNyTGaEk5/kmtyQX4M/0TDajLZuW6NBP7ND7lT08i+VNLaa</latexit>

Instruction history context zp3
<latexit sha1_base64="1/uQc8uZl9PjheJu1+kCxdeR7gE=">AAACUXicbVBNTxRBEO0ZEHFQWfDIpcNG42kzIyTojcSL3jBhhWRn3PT01LAd+mPSXQOunfkj/hquePbiX/Fkz7IHWKykk5f3qupVv7KRwmGa/onitfUnG083nyVbz1+83B7s7H51prUcxtxIY89L5kAKDWMUKOG8scBUKeGsvPzY62dXYJ0w+hTnDRSKXWhRC84wUNPB4RuaI3xH/1k7tC3vWToLvsbOKTe612hHc8VwVtb+R/etmR5MB8N0lC6KPgbZEgzJsk6mO9FGXhneKtDIJXNukqUNFp5ZFFxCl+Stg4bxS3YBkwA1U+AKv/heR18HpqK1seFppAv2/oRnyrm5KkNnf6Zb1Xryv1rl+oUr7li/L7zQTYug+Z153UqKhvb50UpY4CjnATBuRbif8hmzjGNIOUlyCxquuVGK6crnvJtkhfe5VXSYdV0SkstWc3oMxu9GH0bpl8PhcbqMcJPskX3ylmTkiByTT+SEjAknP8kNuSW/ot/R35jE8V1rHC1nXpEHFW/9AxAhs6I=</latexit><latexit sha1_base64="1/uQc8uZl9PjheJu1+kCxdeR7gE=">AAACUXicbVBNTxRBEO0ZEHFQWfDIpcNG42kzIyTojcSL3jBhhWRn3PT01LAd+mPSXQOunfkj/hquePbiX/Fkz7IHWKykk5f3qupVv7KRwmGa/onitfUnG083nyVbz1+83B7s7H51prUcxtxIY89L5kAKDWMUKOG8scBUKeGsvPzY62dXYJ0w+hTnDRSKXWhRC84wUNPB4RuaI3xH/1k7tC3vWToLvsbOKTe612hHc8VwVtb+R/etmR5MB8N0lC6KPgbZEgzJsk6mO9FGXhneKtDIJXNukqUNFp5ZFFxCl+Stg4bxS3YBkwA1U+AKv/heR18HpqK1seFppAv2/oRnyrm5KkNnf6Zb1Xryv1rl+oUr7li/L7zQTYug+Z153UqKhvb50UpY4CjnATBuRbif8hmzjGNIOUlyCxquuVGK6crnvJtkhfe5VXSYdV0SkstWc3oMxu9GH0bpl8PhcbqMcJPskX3ylmTkiByTT+SEjAknP8kNuSW/ot/R35jE8V1rHC1nXpEHFW/9AxAhs6I=</latexit><latexit sha1_base64="1/uQc8uZl9PjheJu1+kCxdeR7gE=">AAACUXicbVBNTxRBEO0ZEHFQWfDIpcNG42kzIyTojcSL3jBhhWRn3PT01LAd+mPSXQOunfkj/hquePbiX/Fkz7IHWKykk5f3qupVv7KRwmGa/onitfUnG083nyVbz1+83B7s7H51prUcxtxIY89L5kAKDWMUKOG8scBUKeGsvPzY62dXYJ0w+hTnDRSKXWhRC84wUNPB4RuaI3xH/1k7tC3vWToLvsbOKTe612hHc8VwVtb+R/etmR5MB8N0lC6KPgbZEgzJsk6mO9FGXhneKtDIJXNukqUNFp5ZFFxCl+Stg4bxS3YBkwA1U+AKv/heR18HpqK1seFppAv2/oRnyrm5KkNnf6Zb1Xryv1rl+oUr7li/L7zQTYug+Z153UqKhvb50UpY4CjnATBuRbif8hmzjGNIOUlyCxquuVGK6crnvJtkhfe5VXSYdV0SkstWc3oMxu9GH0bpl8PhcbqMcJPskX3ylmTkiByTT+SEjAknP8kNuSW/ot/R35jE8V1rHC1nXpEHFW/9AxAhs6I=</latexit>

Instruction

context zc3
<latexit sha1_base64="HArRW9tQp06NtIVv721UEWaLDn4=">AAACWHicbVBNb9QwEHUC9CN8dFuOXCxWVJxWCSAVxKUSF7gViaWVNmHlTCatVX9E9gRYovBf+DVc6bHiz+Ds7gG2PMny03sznvErGyU9pel1FN+6fWdre2c3uXvv/oO90f7BR29bBzgFq6w7K4VHJQ1OSZLCs8ah0KXC0/LyzeCffkbnpTUfaNFgocW5kbUEQUGaj14f8u854Vfq3hlProVB5j3Pc37I+coBa4Z7ULWgi7LuvvWfYP58Phqnk3QJfpNkazJma5zM96OtvLLQajQESng/y9KGik44kqCwT/LWYyPgUpzjLFAjNPqiW/6y50+CUvHaunAM8aX6d0cntPcLXYbKYU2/6Q3if73KDw9uTKf6ZdFJ07SEBlbD61ZxsnyIkVfSIZBaBCLAybA/hwvhBFAIO0lyhwa/gNVamKrLoZ9lRdflTvNx1vdJSC7bzOkmmT6bvJqk71+Mj9N1hDvsEXvMnrKMHbFj9padsCkD9oP9ZL/YVfQ7juLteHdVGkfrnofsH8QHfwBZUbSf</latexit><latexit sha1_base64="HArRW9tQp06NtIVv721UEWaLDn4=">AAACWHicbVBNb9QwEHUC9CN8dFuOXCxWVJxWCSAVxKUSF7gViaWVNmHlTCatVX9E9gRYovBf+DVc6bHiz+Ds7gG2PMny03sznvErGyU9pel1FN+6fWdre2c3uXvv/oO90f7BR29bBzgFq6w7K4VHJQ1OSZLCs8ah0KXC0/LyzeCffkbnpTUfaNFgocW5kbUEQUGaj14f8u854Vfq3hlProVB5j3Pc37I+coBa4Z7ULWgi7LuvvWfYP58Phqnk3QJfpNkazJma5zM96OtvLLQajQESng/y9KGik44kqCwT/LWYyPgUpzjLFAjNPqiW/6y50+CUvHaunAM8aX6d0cntPcLXYbKYU2/6Q3if73KDw9uTKf6ZdFJ07SEBlbD61ZxsnyIkVfSIZBaBCLAybA/hwvhBFAIO0lyhwa/gNVamKrLoZ9lRdflTvNx1vdJSC7bzOkmmT6bvJqk71+Mj9N1hDvsEXvMnrKMHbFj9padsCkD9oP9ZL/YVfQ7juLteHdVGkfrnofsH8QHfwBZUbSf</latexit><latexit sha1_base64="HArRW9tQp06NtIVv721UEWaLDn4=">AAACWHicbVBNb9QwEHUC9CN8dFuOXCxWVJxWCSAVxKUSF7gViaWVNmHlTCatVX9E9gRYovBf+DVc6bHiz+Ds7gG2PMny03sznvErGyU9pel1FN+6fWdre2c3uXvv/oO90f7BR29bBzgFq6w7K4VHJQ1OSZLCs8ah0KXC0/LyzeCffkbnpTUfaNFgocW5kbUEQUGaj14f8u854Vfq3hlProVB5j3Pc37I+coBa4Z7ULWgi7LuvvWfYP58Phqnk3QJfpNkazJma5zM96OtvLLQajQESng/y9KGik44kqCwT/LWYyPgUpzjLFAjNPqiW/6y50+CUvHaunAM8aX6d0cntPcLXYbKYU2/6Q3if73KDw9uTKf6ZdFJ07SEBlbD61ZxsnyIkVfSIZBaBCLAybA/hwvhBFAIO0lyhwa/gNVamKrLoZ9lRdflTvNx1vdJSC7bzOkmmT6bvJqk71+Mj9N1hDvsEXvMnrKMHbFj9padsCkD9oP9ZL/YVfQ7juLteHdVGkfrnofsH8QHfwBZUbSf</latexit>

Figure 2: Illustration of the model architecture while generating the third action a3 in the third utterance x̄3 from
Figure 1. Context vectors computed using attention are highlighted in blue. The model takes as input vector
encodings from the current and previous instructions x̄1, x̄2, and x̄3, the initial state s1, the current state s3, and
the previous action a2. Instruction encodings are computed with a bidirectional RNN. We attend over the previous
and current instructions and the initial and current states. We use an MLP to select the next action.

der. The model generates an execution ē =
〈(s1, a1), . . . , (smi , ami)〉 for each instruction x̄i.
The agent context, the information available to the
agent at step k, is s̃k = (x̄i, 〈x̄1, . . . , x̄i−1〉, sk, ē[:
k]), where ē[: k] is the execution up until but
not including step k. In contrast to the world
state, the agent context also includes instruc-
tions and the execution so far. The agent policy
πθ(s̃k, a) is modeled as a probabilistic neural net-
work parametrized by θ, where s̃k is the agent con-
text at step k and a is an action. To generate exe-
cutions, we generate one action at a time, execute
the action, and observe the new world state. In
step k of executing the i-th instruction, the net-
work inputs are the current utterance x̄i, the previ-
ous instructions 〈x̄1, . . . , x̄i−1〉, the initial state s1
at beginning of executing x̄i, and the current state
sk. When executing a sequence of instructions,
the initial state s1 is either the state at the begin-
ning of executing the sequence or the final state of
the execution of the previous instruction. Figure 2
illustrates our architecture.

We generate continuous vector representations
for all inputs. Each input is represented as a set
of vectors that are then processed with an atten-
tion function to generate a single vector represen-
tation (Luong et al., 2015). We assume access to
a domain-specific encoding function ENC(s) that,
given a state s, generates a set of vectors S repre-
senting the objects in the state. For example, in the
ALCHEMY domain, a vector is generated for each
beaker using an RNN. Section 6 describes the dif-
ferent domains and their encoding functions.

We use a single bidirectional RNN with a
long short-term memory (LSTM; Hochreiter and
Schmidhuber, 1997) recurrence to encode the in-
structions. All instructions x̄1,. . . ,x̄i are encoded

with a single RNN by concatenating them to x̄′.
We use two delimiter tokens: one separates previ-
ous instructions, and the other separates the previ-
ous instructions from the current one. The forward
LSTM RNN hidden states are computed as:2

−−→
hj+1 =

−−−−−→
LSTME

(
φI(x′j+1);

−→
hj
)

,

where φI is a learned word embedding func-

tion and
−−−−−→
LSTME is the forward LSTM recur-

rence function. We use a similar computation
to compute the backward hidden states

←−
hj . For

each token x′j in x̄
′, a vector representation h′j =[−→

hj ;
←−
hj

]
is computed. We then create two sets of

vectors, one for all the vectors of the current in-
struction and one for the previous instructions:

Xc = {h′j}
J+|x̄i|
j=J

Xp = {h′j}j<Jj=0

where J is the index in x̄′ where the current in-
struction x̄i begins. Separating the vectors to two
sets will allows computing separate attention on
the current instruction and previous ones.

To compute each input representation dur-
ing decoding, we use a bi-linear attention func-
tion (Luong et al., 2015). Given a set of vectors
H , a query vector hq, and a weight matrix W, the
attention function ATTEND(H,hq,W) computes
a context vector z:

αi ∝ exp(hᵀiWh
q) : i = 0, . . . , |H|

z =

|H|∑
i=1

αihi .

2To simplify the notation, we omit the memory cell (often
denoted as cj) from all LSTM descriptions. We use only the
hidden state hj to compute the intended representations (e.g.,
for the input text tokens). All LSTMs in this paper use zero
vectors as initial hidden state h0 and initial cell memory c0.



2076

We use a decoder to generate actions. At each
time step k, we compute an input representation
using the attention function, update the decoder
state, and compute the next action to execute. At-
tention is first computed over the vectors of the
current instruction, which is then used to attend
over the other inputs. We compute the context
vectors zck and z

p
k for the current instruction and

previous instructions:

zck = ATTEND(X
c,hdk−1,W

c)

zpk = ATTEND(X
p, [hdk−1, z

c
k],W

p) ,

where hdk−1 is the decoder hidden state for step
k− 1, and Xc and Xp are the sets of vector repre-
sentations for the current instruction and previous
instructions. Two attention heads are used over
both the initial and current states. This allows the
model to attend to more than one location in a state
at once, for example when transferring items from
one beaker to another in ALCHEMY. The cur-
rent state is computed by the transition function
sk = T (sk−1, ak−1), where sk−1 and ak−1 are the
state and action at step k − 1. The context vectors
for the initial state s1 and the current state sk are:

zs1,k = [ATTEND(ENC(s1), [h
d
k−1, z

c
k],W

sb,1);

ATTEND(ENC(s1), [hdk−1, z
c
k],W

sb,2)]

zsk,k = [ATTEND(ENC(sk), [h
d
k−1, z

c
k],W

sc,1);

ATTEND(ENC(sk), [hdk−1, z
c
k],W

sc,2)] ,

where all W∗,∗ are learned weight matrices.

We concatenate all computed context vectors
with an embedding of the previous action ak−1 to
create the input for the decoder:

hk = tanh([z
c
k; z

p
k; z

s
1,k; z

s
k,k;φ

O(ak−1)]W
d + bd)

hdk = LSTM
D
(
hk;h

d
k−1

)
,

where φO is a learned action embedding function
and LSTMD is the LSTM decoder recurrence.

Given the decoder state hdk, the next action ak
is predicted with a multi-layer perceptron (MLP).
The actions in our domains decompose to an ac-
tion type and at most two arguments.3 For exam-
ple, the action PUSH 1 B in ALCHEMY has the type
PUSH and two arguments: a beaker number and a
color. Section 6 describes the actions of each do-
main. The probability of an action is:

3We use a NULL argument for unused arguments.

hak = tanh(h
d
kW

a)

sk,aT = h
a
kbaT

sk,a1 = h
a
kba1

sk,a2 = h
a
kba2

p(ak = aT (a1, a2) | s̃k; θ) ∝
exp(sk,aT + sk,a1 + sk,a2) ,

where aT , a1, and a2 are an action type, first ar-
gument, and second argument. If the predicted
action is STOP, the execution is complete. Other-
wise, we execute the action ak to generate the next
state sk+1, and update the agent context s̃k to s̃k+1
by appending the pair (sk, ak) to the execution ē
and replacing the current state with sk+1.

The model parameters θ include: the embed-
ding functions φI and φO; the recurrence param-

eters for
−−−−−→
LSTME ,

←−−−−−
LSTME , and LSTMD; WC ,

WP , Wsb,1, Wsb,2, Wsc,1, Wsc,2, Wd, Wa,
and bd; and the domain dependent parameters, in-
cluding the parameters of the encoding function
ENC and the action type, first argument, and sec-
ond argument weights baT , ba1 , and ba2 .

5 Learning

We estimate the policy parameters θ using an
exploration-based learning algorithm that maxi-
mizes the immediate expected reward. Broadly
speaking, during learning, we observe the agent
behavior given the current policy, and for each
visited state compute the expected immediate re-
ward by observing rewards for all actions. We
assume access to a set of training examples
{(x̄(j)i , s

(j)
i,1 , 〈x̄

(j)
1 , . . . , x̄

(j)
i−1〉, g

(j)
i )}

N,n(j)

j=1,i=1, where

each instruction x̄(j)i is paired with a start state
s

(j)
i,1 , the previous instructions in the sequence

〈x̄(j)1 , . . . , x̄
(j)
i−1〉, and a goal state g

(j)
i .

Reward The reward R(j)i : S × S × A → R is
defined for each example j and instruction i:

R
(j)
i (s, a, s

′) = P
(j)
i (s, a, s

′) + φ
(j)
i (s

′)− φ(j)i (s) ,

where s is a source state, a is an action, and s′ is a
target state.4 P (j)i (s, a, s

′) is a problem reward and
φ

(j)
i (s

′)− φ(j)i (s) is a shaping term. The problem
reward P (j)i (s, a, s

′) is positive for stopping at the
goal g(j)i and negative for stopping in an incorrect

4While the reward function is defined for any state-action-
state tuple, in practice, it is used during learning with tuples
that follow the system dynamics, s′ = T (s, a).



2077

Algorithm 1 SESTRA: Single-step Reward Observation.

Input: Training data {(x̄(j)i , s
(j)
i,1 , 〈x̄

(j)
1 , . . . , x̄

(j)
i−1〉,

g
(j)
i )}

N,n(j)

j=1,i=1, learning rate µ, entropy regularization
coefficient λ, episode limit horizon M .

Definitions: πθ is a policy parameterized by θ, BEG is a spe-
cial action to use for the first decoder step, and STOP
indicates end of an execution. T (s, a) is the state transi-
tion function, H is an entropy function, R(j)i (s, a, s

′) is
the reward function for example j and instruction i, and
RMSPROP divides each weight by a running average of
its squared gradient (Tieleman and Hinton, 2012).

Output: Parameters θ defining a learned policy πθ .
1: for t = 1, . . . , T, j = 1, . . . , N do
2: for i = 1, . . . , n(j) do
3: ē← 〈 〉, k ← 0, a0 ← BEG
4: » Rollout up to STOP or episode limit.
5: while ak 6= STOP ∧ k < M do
6: k ← k + 1
7: s̃k ← (x̄i, 〈x̄1, . . . , x̄i−1〉, sk, ē[: k])
8: » Sample an action from policy.
9: ak ∼ πθ(s̃k, ·)

10: sk+1 ← T (sk, ak)
11: ē← [ē; 〈(sk, ak)〉]
12: ∆← 0̄
13: for k′ = 1, . . . , k do
14: » Compute the entropy of πθ(s̃k′ , ·).
15: ∆← ∆ + λ∇θH(πθ(s̃k′ , ·))
16: for a ∈ A do
17: s′ ← T (sk′ , a)
18: » Compute gradient for action a.
19: ∆← ∆ +R(j)i (sk′ , a, s

′)∇θπθ(s̃k′ , a)

20: θ ← θ + µRMSPROP
(

∆

k

)
21: return θ

state or taking an invalid action:

P
(j)
i (s, a, s

′) =


1.0 a = STOP ∧ s′ = g(j)i
−1.0 a = STOP ∧ s′ 6= g(j)i
−1.0− δ s = s′

−δ otherwise

,

where δ is a verbosity penalty. The case s =
s′ indicates that a was invalid in state s, as in
this domain, all valid actions except STOP mod-
ify the state. We use a potential-based shaping
term φ(j)i (s

′)− φ(j)i (s) (Ng et al., 1999), where
φ

(j)
i (s) = −||s− g

(j)
i || computes the edit distance

between the state s and the goal, measured over
the objects in each state. The shaping term densi-
fies the reward, providing a meaningful signal for
learning in nonterminal states.

Objective We maximize the immediate ex-
pected reward over all actions and use entropy reg-
ularization. The gradient is approximated by sam-
pling an execution ē = 〈(s1, a1), . . . , (sk, ak)〉 us-
ing our current policy:

∇θJ =
1

k

k∑
k′=1

(∑
a∈A

R (sk, a, T (sk, a))∇θπ(s̃k, a)

+λ∇θH(π(s̃k, ·))
)
,

where H(π(s̃k, ·) is the entropy term.

Algorithm Algorithm 1 shows the Single-step
Reward Observation (SESTRA) learning algo-
rithm. We iterate over the training data T times
(line 1). For each example j and turn i, we first
perform a rollout by sampling an execution ē from
πθ with at most M actions (lines 5-11). If the roll-
out reaches the horizon without predicting STOP,
we set the problem reward P (j)i to−1.0 for the last
step. Given the sampled states visited, we com-
pute the entropy (line 15) and observe the imme-
diate reward for all actions (line 19) for each step.
Entropy and rewards are used to accumulate the
gradient, which is applied to the parameters using
RMSPROP (Dauphin et al., 2015) (line 20).

Discussion Observing the rewards for all actions
for each visited state addresses an on-policy learn-
ing exploration problem. Actions that consistently
receive negative reward early during learning will
be visited with very low probability later on, and in
practice, often not explored at all. Because the net-
work is randomly initialized, these early negative
rewards are translated into strong general biases
that are not grounded well in the observed con-
text. Our algorithm exposes the agent to such ac-
tions later on when they receive positive rewards
even though the agent does not explore them dur-
ing rollout. For example, in ALCHEMY, POP ac-
tions are sufficient to complete the first steps of
good executions. As a result, early during learn-
ing, the agent learns a strong bias against PUSH
actions. In practice, the agent then will not ex-
plore PUSH actions again. In our algorithm, as the
agent learns to roll out the correct POP prefix, it is
then exposed to the reward for the first PUSH even
though it likely sampled another POP. It then un-
learns its bias towards predicting POP.

Our learning algorithm can be viewed as a cost-
sensitive variant of the oracle in DAGGER (Ross
et al., 2011), where it provides the rewards for all
actions instead of an oracle action. It is also related
to Locally Optimal Learning to Search (LOLS;
Chang et al., 2015) with two key distinctions: (a)
instead of using different roll-in and roll-out poli-
cies, we use the model policy; and (b) we branch
at each step, instead of once, but do not rollout



2078

Rollin
<latexit sha1_base64="DtVmvvygCMSquf1kOAEH6sV0edc=">AAACKHicbVDLSsNAFJ34rPGtSzeDRXBVEhHUneDGpYpVoQkymdzq4DzizI1aQr7Dra79GlfSrV/itHah1QMDh3PumXs5WSGFwyjqBxOTU9Mzs425cH5hcWl5ZXXtwpnScmhzI429ypgDKTS0UaCEq8ICU5mEy+zuaOBfPoB1wuhz7BWQKnajRVdwhl5KE4QnrM6M9Pn6eqUZtaIh6F8Sj0iTjHByvRrMJLnhpQKNXDLnOnFUYFoxi4JLqMOkdFAwfsduoOOpZgpcWg2vrumWV3LaNdY/jXSo/kxUTDnXU5mfVAxv3bg3EP/1cjf4cGw7dvfTSuiiRND8e3m3lBQNHdRCc2GBo+x5wrgV/n7Kb5llHH15YZhY0PDIjVJM51XC606cVlViFW3GdR365uLxnv6S9k7roBWd7jYPo1GFDbJBNsk2ickeOSTH5IS0CSf35Jm8kNfgLXgPPoL+9+hEMMqsk18IPr8ABH2mOQ==</latexit><latexit sha1_base64="DtVmvvygCMSquf1kOAEH6sV0edc=">AAACKHicbVDLSsNAFJ34rPGtSzeDRXBVEhHUneDGpYpVoQkymdzq4DzizI1aQr7Dra79GlfSrV/itHah1QMDh3PumXs5WSGFwyjqBxOTU9Mzs425cH5hcWl5ZXXtwpnScmhzI429ypgDKTS0UaCEq8ICU5mEy+zuaOBfPoB1wuhz7BWQKnajRVdwhl5KE4QnrM6M9Pn6eqUZtaIh6F8Sj0iTjHByvRrMJLnhpQKNXDLnOnFUYFoxi4JLqMOkdFAwfsduoOOpZgpcWg2vrumWV3LaNdY/jXSo/kxUTDnXU5mfVAxv3bg3EP/1cjf4cGw7dvfTSuiiRND8e3m3lBQNHdRCc2GBo+x5wrgV/n7Kb5llHH15YZhY0PDIjVJM51XC606cVlViFW3GdR365uLxnv6S9k7roBWd7jYPo1GFDbJBNsk2ickeOSTH5IS0CSf35Jm8kNfgLXgPPoL+9+hEMMqsk18IPr8ABH2mOQ==</latexit><latexit sha1_base64="DtVmvvygCMSquf1kOAEH6sV0edc=">AAACKHicbVDLSsNAFJ34rPGtSzeDRXBVEhHUneDGpYpVoQkymdzq4DzizI1aQr7Dra79GlfSrV/itHah1QMDh3PumXs5WSGFwyjqBxOTU9Mzs425cH5hcWl5ZXXtwpnScmhzI429ypgDKTS0UaCEq8ICU5mEy+zuaOBfPoB1wuhz7BWQKnajRVdwhl5KE4QnrM6M9Pn6eqUZtaIh6F8Sj0iTjHByvRrMJLnhpQKNXDLnOnFUYFoxi4JLqMOkdFAwfsduoOOpZgpcWg2vrumWV3LaNdY/jXSo/kxUTDnXU5mfVAxv3bg3EP/1cjf4cGw7dvfTSuiiRND8e3m3lBQNHdRCc2GBo+x5wrgV/n7Kb5llHH15YZhY0PDIjVJM51XC606cVlViFW3GdR365uLxnv6S9k7roBWd7jYPo1GFDbJBNsk2ickeOSTH5IS0CSf35Jm8kNfgLXgPPoL+9+hEMMqsk18IPr8ABH2mOQ==</latexit>

Rollout
<latexit sha1_base64="ItenZLBGN1+9hAciWYVGcj3LXKg=">AAACKXicbVDLLgRBFK321l6DpU3FRGI16RYJdhIbS8QgmW5SXX2Hinp0qm5j0un/sGXta6yw9SNqxiwYTnKTk3PuKycrpHAYRe/B2PjE5NT0zGw4N7+wuNRYXjlzprQc2txIYy8y5kAKDW0UKOGisMBUJuE8uz3o++d3YJ0w+hR7BaSKXWvRFZyhly4ThAesToyUpsT6qtGMWtEA9C+Jh6RJhji6Wg6mktzwUoFGLplznTgqMK2YRcEl1GFSOigYv2XX0PFUMwUurQZv13TDKzntGutLIx2oPycqppzrqcx3KoY3btTri/96uesvHLmO3d20ErooETT/Pt4tJUVD+7nQXFjgKHueMG6F/5/yG2YZR59eGCYWNNxzoxTTeZXwuhOnVZVYRZtxXYc+uXg0p7+kvdXaa0XH2839aBjhDFkj62STxGSH7JNDckTahBNLHskTeQ5egtfgLfj4bh0LhjOr5BeCzy8NFKbE</latexit><latexit sha1_base64="ItenZLBGN1+9hAciWYVGcj3LXKg=">AAACKXicbVDLLgRBFK321l6DpU3FRGI16RYJdhIbS8QgmW5SXX2Hinp0qm5j0un/sGXta6yw9SNqxiwYTnKTk3PuKycrpHAYRe/B2PjE5NT0zGw4N7+wuNRYXjlzprQc2txIYy8y5kAKDW0UKOGisMBUJuE8uz3o++d3YJ0w+hR7BaSKXWvRFZyhly4ThAesToyUpsT6qtGMWtEA9C+Jh6RJhji6Wg6mktzwUoFGLplznTgqMK2YRcEl1GFSOigYv2XX0PFUMwUurQZv13TDKzntGutLIx2oPycqppzrqcx3KoY3btTri/96uesvHLmO3d20ErooETT/Pt4tJUVD+7nQXFjgKHueMG6F/5/yG2YZR59eGCYWNNxzoxTTeZXwuhOnVZVYRZtxXYc+uXg0p7+kvdXaa0XH2839aBjhDFkj62STxGSH7JNDckTahBNLHskTeQ5egtfgLfj4bh0LhjOr5BeCzy8NFKbE</latexit><latexit sha1_base64="ItenZLBGN1+9hAciWYVGcj3LXKg=">AAACKXicbVDLLgRBFK321l6DpU3FRGI16RYJdhIbS8QgmW5SXX2Hinp0qm5j0un/sGXta6yw9SNqxiwYTnKTk3PuKycrpHAYRe/B2PjE5NT0zGw4N7+wuNRYXjlzprQc2txIYy8y5kAKDW0UKOGisMBUJuE8uz3o++d3YJ0w+hR7BaSKXWvRFZyhly4ThAesToyUpsT6qtGMWtEA9C+Jh6RJhji6Wg6mktzwUoFGLplznTgqMK2YRcEl1GFSOigYv2XX0PFUMwUurQZv13TDKzntGutLIx2oPycqppzrqcx3KoY3btTri/96uesvHLmO3d20ErooETT/Pt4tJUVD+7nQXFjgKHueMG6F/5/yG2YZR59eGCYWNNxzoxTTeZXwuhOnVZVYRZtxXYc+uXg0p7+kvdXaa0XH2839aBjhDFkj62STxGSH7JNDckTahBNLHskTeQ5egtfgLfj4bh0LhjOr5BeCzy8NFKbE</latexit>

Branch
<latexit sha1_base64="B5fVuaDlea4qVxFuHcErLAiJNxQ=">AAACKHicbVDLSsNAFJ34Nr5aXboZLIKrkoig7opuXCpYLTShTCa3dnBmEmdu1BLyHW517de4Erd+idPahVYPXDicc1+cJJfCYhB8eDOzc/MLi0vL/srq2vpGrb55ZbPCcGjzTGamkzALUmhoo0AJndwAU4mE6+T2dORf34OxItOXOMwhVuxGi77gDJ0URwiPWJ4Ypvmg6tUaQTMYg/4l4YQ0yATnvbq3EKUZLxRo5JJZ2w2DHOOSGRRcQuVHhYWc8Vt2A11HNVNg43L8dUV3nZLSfmZcaaRj9edEyZS1Q5W4TsVwYKe9kfivl9rRwqnr2D+KS6HzAkHz7+P9QlLM6CgWmgoDHOXQEcaNcP9TPmCGcXTh+X5kQMMDz5RiOi0jXnXDuCwjo2gjrCrfJRdO5/SXtPebx83g4qDRCiYRLpFtskP2SEgOSYuckXPSJpzckSfyTF68V+/Ne/c+vltnvMnMFvkF7/MLyXWmFw==</latexit><latexit sha1_base64="B5fVuaDlea4qVxFuHcErLAiJNxQ=">AAACKHicbVDLSsNAFJ34Nr5aXboZLIKrkoig7opuXCpYLTShTCa3dnBmEmdu1BLyHW517de4Erd+idPahVYPXDicc1+cJJfCYhB8eDOzc/MLi0vL/srq2vpGrb55ZbPCcGjzTGamkzALUmhoo0AJndwAU4mE6+T2dORf34OxItOXOMwhVuxGi77gDJ0URwiPWJ4Ypvmg6tUaQTMYg/4l4YQ0yATnvbq3EKUZLxRo5JJZ2w2DHOOSGRRcQuVHhYWc8Vt2A11HNVNg43L8dUV3nZLSfmZcaaRj9edEyZS1Q5W4TsVwYKe9kfivl9rRwqnr2D+KS6HzAkHz7+P9QlLM6CgWmgoDHOXQEcaNcP9TPmCGcXTh+X5kQMMDz5RiOi0jXnXDuCwjo2gjrCrfJRdO5/SXtPebx83g4qDRCiYRLpFtskP2SEgOSYuckXPSJpzckSfyTF68V+/Ne/c+vltnvMnMFvkF7/MLyXWmFw==</latexit><latexit sha1_base64="B5fVuaDlea4qVxFuHcErLAiJNxQ=">AAACKHicbVDLSsNAFJ34Nr5aXboZLIKrkoig7opuXCpYLTShTCa3dnBmEmdu1BLyHW517de4Erd+idPahVYPXDicc1+cJJfCYhB8eDOzc/MLi0vL/srq2vpGrb55ZbPCcGjzTGamkzALUmhoo0AJndwAU4mE6+T2dORf34OxItOXOMwhVuxGi77gDJ0URwiPWJ4Ypvmg6tUaQTMYg/4l4YQ0yATnvbq3EKUZLxRo5JJZ2w2DHOOSGRRcQuVHhYWc8Vt2A11HNVNg43L8dUV3nZLSfmZcaaRj9edEyZS1Q5W4TsVwYKe9kfivl9rRwqnr2D+KS6HzAkHz7+P9QlLM6CgWmgoDHOXQEcaNcP9TPmCGcXTh+X5kQMMDz5RiOi0jXnXDuCwjo2gjrCrfJRdO5/SXtPebx83g4qDRCiYRLpFtskP2SEgOSYuckXPSJpzckSfyTF68V+/Ne/c+vltnvMnMFvkF7/MLyXWmFw==</latexit>

Rollout
<latexit sha1_base64="ItenZLBGN1+9hAciWYVGcj3LXKg=">AAACKXicbVDLLgRBFK321l6DpU3FRGI16RYJdhIbS8QgmW5SXX2Hinp0qm5j0un/sGXta6yw9SNqxiwYTnKTk3PuKycrpHAYRe/B2PjE5NT0zGw4N7+wuNRYXjlzprQc2txIYy8y5kAKDW0UKOGisMBUJuE8uz3o++d3YJ0w+hR7BaSKXWvRFZyhly4ThAesToyUpsT6qtGMWtEA9C+Jh6RJhji6Wg6mktzwUoFGLplznTgqMK2YRcEl1GFSOigYv2XX0PFUMwUurQZv13TDKzntGutLIx2oPycqppzrqcx3KoY3btTri/96uesvHLmO3d20ErooETT/Pt4tJUVD+7nQXFjgKHueMG6F/5/yG2YZR59eGCYWNNxzoxTTeZXwuhOnVZVYRZtxXYc+uXg0p7+kvdXaa0XH2839aBjhDFkj62STxGSH7JNDckTahBNLHskTeQ5egtfgLfj4bh0LhjOr5BeCzy8NFKbE</latexit><latexit sha1_base64="ItenZLBGN1+9hAciWYVGcj3LXKg=">AAACKXicbVDLLgRBFK321l6DpU3FRGI16RYJdhIbS8QgmW5SXX2Hinp0qm5j0un/sGXta6yw9SNqxiwYTnKTk3PuKycrpHAYRe/B2PjE5NT0zGw4N7+wuNRYXjlzprQc2txIYy8y5kAKDW0UKOGisMBUJuE8uz3o++d3YJ0w+hR7BaSKXWvRFZyhly4ThAesToyUpsT6qtGMWtEA9C+Jh6RJhji6Wg6mktzwUoFGLplznTgqMK2YRcEl1GFSOigYv2XX0PFUMwUurQZv13TDKzntGutLIx2oPycqppzrqcx3KoY3btTri/96uesvHLmO3d20ErooETT/Pt4tJUVD+7nQXFjgKHueMG6F/5/yG2YZR59eGCYWNNxzoxTTeZXwuhOnVZVYRZtxXYc+uXg0p7+kvdXaa0XH2839aBjhDFkj62STxGSH7JNDckTahBNLHskTeQ5egtfgLfj4bh0LhjOr5BeCzy8NFKbE</latexit><latexit sha1_base64="ItenZLBGN1+9hAciWYVGcj3LXKg=">AAACKXicbVDLLgRBFK321l6DpU3FRGI16RYJdhIbS8QgmW5SXX2Hinp0qm5j0un/sGXta6yw9SNqxiwYTnKTk3PuKycrpHAYRe/B2PjE5NT0zGw4N7+wuNRYXjlzprQc2txIYy8y5kAKDW0UKOGisMBUJuE8uz3o++d3YJ0w+hR7BaSKXWvRFZyhly4ThAesToyUpsT6qtGMWtEA9C+Jh6RJhji6Wg6mktzwUoFGLplznTgqMK2YRcEl1GFSOigYv2XX0PFUMwUurQZv13TDKzntGutLIx2oPycqppzrqcx3KoY3btTri/96uesvHLmO3d20ErooETT/Pt4tJUVD+7nQXFjgKHueMG6F/5/yG2YZR59eGCYWNNxzoxTTeZXwuhOnVZVYRZtxXYc+uXg0p7+kvdXaa0XH2839aBjhDFkj62STxGSH7JNDckTahBNLHskTeQ5egtfgLfj4bh0LhjOr5BeCzy8NFKbE</latexit>

Single-step

Reward

Observations
<latexit sha1_base64="G/+rqgEQqu+qL8l7reSb6HsUut4=">AAAChXicbVFNbxMxEHUWaMPy0ZQeuViNqLg02q0KhROVeuHWlhJaKV5FXu8ksWp7V/ZsS2TtX+TO/+AKqpNuVUgZydLze29mrOe8UtJhkvzsRI8eP1lb7z6Nnz1/8XKjt/nqmytrK2AoSlXai5w7UNLAECUquKgscJ0rOM8vjxb6+RVYJ0vzFecVZJpPjZxIwTFQ495sh7KZq7gA/67ChjKE7+jPpJkq2HUIVcPYaHevwozSe2ua3nu/wDW3Rbjd+XZoqxznDuzVcpFrxr1+MkiWRR+CtAV90tbJeLOzxopS1BoMCsWdG6VJGO+5RSkUNDGrHYTXXPIpjAI0XIPL/DKShr4JTEEnpQ3HIF2yf3d4rp2b6zw4NceZW9UW5H+1wi0GrmzHyYfMS1PVCEbcLp/UimJJF5nTQloQqOYBcGFleD8VM265wPAzccwsGLgWpdbcFJ6JZpRm3jOraT9tmjgkl67m9BAM9wYfB8npfv8waSPsktdkm7wlKTkgh+QzOSFDIsgP8ov8Jn+ibjSI9qP3t9ao0/ZskX8q+nQDmCjFdQ==</latexit><latexit sha1_base64="G/+rqgEQqu+qL8l7reSb6HsUut4=">AAAChXicbVFNbxMxEHUWaMPy0ZQeuViNqLg02q0KhROVeuHWlhJaKV5FXu8ksWp7V/ZsS2TtX+TO/+AKqpNuVUgZydLze29mrOe8UtJhkvzsRI8eP1lb7z6Nnz1/8XKjt/nqmytrK2AoSlXai5w7UNLAECUquKgscJ0rOM8vjxb6+RVYJ0vzFecVZJpPjZxIwTFQ495sh7KZq7gA/67ChjKE7+jPpJkq2HUIVcPYaHevwozSe2ua3nu/wDW3Rbjd+XZoqxznDuzVcpFrxr1+MkiWRR+CtAV90tbJeLOzxopS1BoMCsWdG6VJGO+5RSkUNDGrHYTXXPIpjAI0XIPL/DKShr4JTEEnpQ3HIF2yf3d4rp2b6zw4NceZW9UW5H+1wi0GrmzHyYfMS1PVCEbcLp/UimJJF5nTQloQqOYBcGFleD8VM265wPAzccwsGLgWpdbcFJ6JZpRm3jOraT9tmjgkl67m9BAM9wYfB8npfv8waSPsktdkm7wlKTkgh+QzOSFDIsgP8ov8Jn+ibjSI9qP3t9ao0/ZskX8q+nQDmCjFdQ==</latexit><latexit sha1_base64="G/+rqgEQqu+qL8l7reSb6HsUut4=">AAAChXicbVFNbxMxEHUWaMPy0ZQeuViNqLg02q0KhROVeuHWlhJaKV5FXu8ksWp7V/ZsS2TtX+TO/+AKqpNuVUgZydLze29mrOe8UtJhkvzsRI8eP1lb7z6Nnz1/8XKjt/nqmytrK2AoSlXai5w7UNLAECUquKgscJ0rOM8vjxb6+RVYJ0vzFecVZJpPjZxIwTFQ495sh7KZq7gA/67ChjKE7+jPpJkq2HUIVcPYaHevwozSe2ua3nu/wDW3Rbjd+XZoqxznDuzVcpFrxr1+MkiWRR+CtAV90tbJeLOzxopS1BoMCsWdG6VJGO+5RSkUNDGrHYTXXPIpjAI0XIPL/DKShr4JTEEnpQ3HIF2yf3d4rp2b6zw4NceZW9UW5H+1wi0GrmzHyYfMS1PVCEbcLp/UimJJF5nTQloQqOYBcGFleD8VM265wPAzccwsGLgWpdbcFJ6JZpRm3jOraT9tmjgkl67m9BAM9wYfB8npfv8waSPsktdkm7wlKTkgh+QzOSFDIsgP8ov8Jn+ibjSI9qP3t9ao0/ZskX8q+nQDmCjFdQ==</latexit>

Figure 3: Illustration of LOLS (left; Chang et al.,
2015) and our learning algorithm (SESTRA, right).
LOLS branches a single time, and samples complete
rollout for each branch to obtain the trajectory loss.
SESTRA uses a complete on-policy rollout and single-
step branching for all actions in each sample state.

ALC SCE TAN
# Sequences (train) 3657 3352 4189
# Sequences (dev) 245 198 199
# Sequences (test) 899 1035 800
Mean instruction

8.0±3.2 10.5±5.5 5.4±2.4length
Vocabulary size 695 816 475

Table 1: Data statistics for ALCHEMY (ALC), SCENE
(SCE), and TANGRAMS (TAN).

Refs/Ex 1 2 3 4

ALCHEMY 1.4 Coref. 28 7 2 0Ellipsis 0 0 3 1

SCENE 2.4 Coref. 49 16 5 3Ellipsis 0 0 0 0

TANGRAMS 1.7 Coref. 25 14 2 1Ellipsis 4 0 0 0

Table 2: Counts of discourse phenomena in SCONE
from 30 randomly selected development interactions
for each domain. We count occurrences of coreference
between instructions (e.g., he leaves in SCENE) and el-
lipsis (e.g., then, drain 2 units in ALCHEMY), when the
last explicit mention of the referent was 1, 2, 3, or 4
turns in the past. We also report the average number of
multi-turn references per interaction (Refs/Ex).

from branched actions since we only optimize the
immediate reward. Figure 3 illustrates the compar-
ison. Our summation over immediate rewards for
all actions is related the summation of estimated
Q-values for all actions in the Mean Actor-Critic
algorithm (Asadi et al., 2017). Finally, our ap-
proach is related to Misra et al. (2017), who also
maximize the immediate reward, but do not ob-
serve rewards for all actions for each state.

6 SCONE Domains and Data

SCONE has three domains: ALCHEMY, SCENE,
and TANGRAMS. Each interaction contains five
instructions. Table 1 shows data statistics. Table 2
shows discourse reference analysis. State encod-
ings are detailed in the Supplementary Material.

ALCHEMY Each environment in ALCHEMY
contains seven numbered beakers, each contain-
ing up to four colored chemicals in order. Figure 1
shows an example. Instructions describe pouring
chemicals between and out of beakers, and mix-
ing beakers. We treat all beakers as stacks. There

are two action types: PUSH and POP. POP takes
a beaker index, and removes the top color. PUSH
takes a beaker index and a color, and adds the color
at the top of the beaker. To encode a state, we en-
code each beaker with an RNN, and concatenate
the last output with the beaker index embedding.
The set of vectors is the state embedding.

SCENE Each environment in SCENE contains
ten positions, each containing at most one person
defined by a shirt color and an optional hat
color. Instructions describe adding or removing
people, moving a person to another position, and
moving a person’s hat to another person. There
are four action types: ADD_PERSON, ADD_HAT,
REMOVE_PERSON, and REMOVE_HAT.
ADD_PERSON and ADD_HAT take a posi-
tion to place the person or hat and the color of
the person’s shirt or hat. REMOVE_PERSON
and REMOVE_HAT take the position to remove
a person or hat from. To encode a state, we use
a bidirectional RNN over the ordered positions.
The input for each position is a concatenation of
the color embeddings for the person and hat. The
set of RNN hidden states is the state embedding.

TANGRAMS Each environment in TANGRAMS
is a list containing at most five unique objects. In-
structions describe removing or inserting an object
into a position in the list, or swapping the positions
of two items. There are two action types: INSERT
and REMOVE. INSERT takes the position to insert
an object, and the object identifier. REMOVE takes
an object position. We embed each object by con-
catenating embeddings for its type and position.
The resulting set is the state embedding.

7 Experimental Setup

Evaluation Following Long et al. (2016), we
evaluate task completion accuracy using exact
match between the final state and the annotated
goal state. We report accuracy for complete in-
teractions (5utts), the first three utterances of each
interaction (3utts), and single instructions (Inst).
For single instructions, execution starts from the
annotated start state of the instruction.

Systems We report performance of ablations
and two baseline systems: POLICYGRADIENT:
policy gradient with cumulative episodic reward
without a baseline, and CONTEXTUALBANDIT:
the contextual bandit approach of Misra et al.
(2017). Both systems use the reward with the



2079

0 25 50 75 100 125 150 175 200
0

0.25

0.5

0.75

1

# Epochs

A
cc

ur
ac

y

Figure 4: Instruction-level training accuracy per epoch
when training five models on SCENE, demonstrating
the effect of randomization in the learning method.
Three of five experiments fail to learn effective models.
The red and blue learning trajectories are overlapping.

shaping term and our model. We also report super-
vised learning results (SUPERVISED) by heuristi-
cally generating correct executions and comput-
ing maximum-likelihood estimate using context-
action demonstration pairs. Only the supervised
approach uses the heuristically generated labels.
Although the results are not comparable, we also
report the performance of previous approaches
to SCONE. All three approaches generate logical
representations based on lambda calculus. In con-
trast to our approach, this requires an ontology of
hand built symbols and rules to evaluate the logical
forms. Fried et al. (2018) uses supervised learning
with annotated logical forms.

Training Details For test results, we run each
experiment five times and report results for the
model with best validation interaction accuracy.
For ablations, we do the same with three experi-
ments. We use a batch size of 20. We stop train-
ing using a validation set sampled from the train-
ing data. We hold the validation set constant for
each domain for all experiments. We use patience
over the average reward, and select the best model
using interaction-level (5utts) validation accuracy.
We tune λ, δ, and M on the development set. The
selected values and other implementation details
are described in the Supplementary Material.

8 Results

Table 3 shows test results. Our approach signifi-
cantly outperforms POLICYGRADIENT and CON-
TEXTUALBANDIT, both of which suffer due to bi-
ases learned early during learning, hindering later
exploration. This problem does not appear in
TANGRAMS, where no action type is dominant at
the beginning of executions, and all methods per-
form well. POLICYGRADIENT completely fails
to learn ALCHEMY and SCENE due to observing
only negative total rewards early during learning.

Using a baseline, for example with an actor-critic
method, will potentially close the gap to CONTEX-
TUALBANDIT. However, it is unlikely to address
the on-policy exploration problem.

Table 4 shows development results, including
model ablation studies. Removing previous in-
structions (– previous instructions) or both states
(– current and initial state) reduces performance
across all domains. Removing only the initial state
(– initial state) or the current state (– current state)
shows mixed results across the domains. Provid-
ing access to both initial and current states in-
creases performance for ALCHEMY, but reduces
performance on the other domains. We hypoth-
esize that this is due to the increase in the num-
ber of parameters outweighing what is relatively
marginal information for these domains. In our
development and test results we use a single ar-
chitecture across the three domains, the full ap-
proach, which has the highest interactive-level ac-
curacy when averaged across the three domains
(62.7 5utts). We also report mean and standard
deviation for our approach over five trials. We ob-
serve exceptionally high variance in performance
on SCENE, where some experiments fail to learn
and training performance remains exceptionally
low (Figure 4). This highlights the sensitivity of
the model to the random effects of initialization,
dropout, and ordering of training examples.

We analyze the instruction-level errors made by
our best models when the agent is provided the
correct initial state for the instruction. We study
fifty examples in each domain to identify the type
of failures. Table 5 shows the counts of major error
categories. We consider multiple reference resolu-
tion errors. State reference errors indicate a failure
to resolve a reference to the world state. For exam-
ple, in ALCHEMY, the phrase leftmost red beaker
specifies a beaker in the environment. If the model
picked the correct action, but the wrong beaker,
we count it as a state reference. We distinguish
between multi-turn reference errors that should be
feasible, and these that that are impossible to solve
without access to states before executing previous
utterances, which are not provided to our model.
For example, in TANGRAMS, the instruction put
it back in the same place refers to a previously-
removed item. Because the agent only has access
to the world state after following this instruction, it
does not observe what kind of item was previously
removed, and cannot identify the item to add. We



2080

ALCHEMY SCENE TANGRAMS
System Inst 3utts 5utts Inst 3utts 5utts Inst 3utts 5utts
Long et al. (2016) – 56.8 52.3 – 23.2 14.7 – 64.9 27.6
Guu et al. (2017) – 66.9 52.9 – 64.8 46.2 – 65.8 37.1
Fried et al. (2018) – – 72.0 – – 72.7 – – 69.6
SUPERVISED 89.4 73.3 62.3 88.8 78.9 66.4 86.6 81.4 60.1
POLICYGRADIENT 0.0 0.0 0.0 0.0 1.3 0.2 84.1 77.4 54.9
CONTEXTUALBANDIT 73.8 36.0 25.7 15.1 2.9 4.4 84.8 76.9 57.9
Our approach 89.1 74.2 62.7 87.1 73.9 62.0 86.6 80.8 62.4

Table 3: Test accuracies for single instructions (Inst), first-three instructions (3utts), and full interactions (5utts).

ALCHEMY SCENE TANGRAMS
System Inst 3utts 5utts Inst 3utts 5utts Inst 3utts 5utts
SUPERVISED 92.0 83.3 71.4 85.3 72.7 60.6 86.1 81.9 58.3
POLICYGRADIENT 0.0 0.0 0.0 0.9 1.0 0.5 85.2 74.9 52.3
CONTEXTUALBANDIT 58.8 6.9 5.7 12.0 0.5 1.5 85.6 78.4 52.6
Our approach 92.1 82.9 71.8 83.9 68.7 56.1 88.5 82.4 60.3
– previous instructions 90.1 77.1 66.1 79.3 60.6 45.5 76.4 55.8 27.6
– current and initial state 25.7 4.5 3.3 17.5 0.0 0.0 45.4 15.1 3.5
– current state 89.8 78.0 62.9 83.0 68.7 54.0 87.6 78.4 60.8
– initial state 81.1 68.6 42.9 82.7 67.7 57.1 88.6 82.9 63.3
Our approach (µ± σ) 91.5

±1.4
80.4
±2.6

69.5
±5.0

62.9
±17.7

37.8
±23.5

29.0
±21.1

88.2
±0.6

80.8
±2.8

59.2
±2.3

Table 4: Development results, including model ablations. We also report mean µ and standard deviation σ for all
metrics for our approach across five experiments. We bold the best performing variations of our model.

Class ALC SCE TAN
State reference 23 13 7
Multi-turn reference 12 5 13
Impossible multi-turn reference 2 5 13
Ambiguous or incorrect label 2 19 12

Table 5: Common error counts in the three domains.

also find a significant number of errors due to am-
biguous or incorrect instructions. For example, the
SCENE instruction person in green appears on the
right end is ambiguous. In the annotated goal, it
is interpreted as referring to a person already in
the environment, who moves to the 10th position.
However, it can also be interpreted as a new person
in green appearing in the 10th position.

We also study performance with respect to
multi-turn coreference by observing whether the
model was able to identify the correct referent
for each occurrence included in the analysis in
Table 2. The models were able to correctly re-
solve 92.3%, 88.7%, and 76.0% of references in
ALCHEMY, SCENE, and TANGRAMS respectively.

Finally, we include attention visualization for
examples from the three domains in the Supple-
mentary Material.

9 Discussion

We propose a model to reason about context-
dependent instructional language that display
strong dependencies both on the history of the

interaction and the state of the world. Future
modeling work may include using intermediate
world states from previous turns in the interaction,
which is required for some of the most complex
references in the data. We propose to train our
model using SESTRA, a learning algorithm that
takes advantage of single-step reward observations
to overcome learned biases in on-policy learning.
Our learning approach requires additional reward
observations in comparison to conventional rein-
forcement learning. However, it is particularly
suitable to recovering from biases acquired early
during learning, for example due to biased action
spaces, which is likely to lead to incorrect blame
assignment in neural network policies. When the
domain and model are less susceptible to such bi-
ases, the benefit of the additional reward observa-
tions is less pronounced. One possible direction
for future work is to use an estimator to predict re-
wards for all actions, rather than observing them.

Acknowledgements

This research was supported by the NSF (CRII-
1656998), Schmidt Sciences, and cloud comput-
ing credits from Amazon. We thank John Lang-
ford and Dipendra Misra for helpful and insightful
discussions with regards to our learning algorithm.
We also thank the anonymous reviewers for their
helpful comments.



2081

References
Peter Anderson, Qi Wu, Damien Teney, Jake Bruce,

Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen
Gould, and Anton van den Hengel. 2018. Vision-
and-Language Navigation: Interpreting visually-
grounded navigation instructions in real environ-
ments. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition.

Jacob Andreas and Dan Klein. 2015. Alignment-
based compositional semantics for instruction fol-
lowing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.

Yoav Artzi, Dipanjan Das, and Slav Petrov. 2014.
Learning compact lexicons for CCG semantic pars-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.

Yoav Artzi and Luke S. Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion of Computational Linguistics, 1:49–62.

Kavosh Asadi, Cameron Allen, Melrose Roderick,
Abdel-rahman Mohamed, George Konidaris, and
Michael L. Littman. 2017. Mean actor critic. CoRR,
abs/1709.00503.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the International Conference on Learning Represen-
tations.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.

Jonathan Berant and Percy Liang. 2014. Semantic
parsing via paraphrasing. In Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics.

Jonathan Berant and Percy Liang. 2015. Imitation
learning of agenda-based semantic parsers. Trans-
actions of the Association for Computational Lin-
guistics, 3:545–558.

Yonatan Bisk, Kevin Shih, Yejin Choi, and Daniel
Marcu. 2018. Learning interpretable spatial oper-
ations in a rich 3D blocks world. In Proceedings
of the Thirty-Second Conference on Artificial Intel-
ligence.

Yonatan Bisk, Deniz Yuret, and Daniel Marcu. 2016.
Natural language communication with robots. In
Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies.

S.R.K. Branavan, Harr Chen, Luke S. Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for

mapping instructions to actions. In Proceedings
of the Joint Conference of the Annual Meeting of
the Association for Computational Linguistics and
the International Joint Conference on Natural Lan-
guage Processing of the AFNLP.

Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agar-
wal, Hal Daumé, and John Langford. 2015. Learn-
ing to search better than your teacher. In Proceed-
ings of the International Conference on Machine
Learning.

David Chen. 2012. Fast online lexicon learning for
grounded language acquisition. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.

David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the Na-
tional Conference on Artificial Intelligence.

James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world’s response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning.

Deborah A. Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the ATIS
task: The ATIS-3 corpus. In Proceedings of the
Workshop on Human Language Technology.

Yann Dauphin, Harm de Vries, , and Yoshua Bengio.
2015. Equilibrated adaptive learning rates for non-
convex optimization. CoRR, abs/1502.04390.

Daniel Fried, Jacob Andreas, and Dan Klein. 2018.
Unified pragmatic models for generating and follow-
ing instructions. Proceedings of the Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.

Kelvin Guu, John Miller, and Percy Liang. 2015.
Traversing knowledge graphs in vector space. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.

Kelvin Guu, Panupong Pasupat, Evan Liu, and Percy
Liang. 2017. From language to programs: Bridging
reinforcement learning and maximum marginal like-
lihood. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.

Charles T. Hemphill, John J. Godfrey, and George R.
Doddington. 1990. The ATIS spoken language sys-
tems pilot corpus. In Proceedings of the DARPA
speech and natural language workshop.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9.

https://doi.org/10.18653/v1/D15-1138
https://doi.org/10.18653/v1/D15-1138
https://doi.org/10.18653/v1/D15-1138
https://doi.org/10.3115/v1/D14-1134
https://doi.org/10.3115/v1/D14-1134
http://aclweb.org/anthology/Q13-1005
http://aclweb.org/anthology/Q13-1005
http://aclweb.org/anthology/Q13-1005
http://www.aclweb.org/anthology/D13-1160
http://www.aclweb.org/anthology/D13-1160
https://doi.org/10.3115/v1/P14-1133
https://doi.org/10.3115/v1/P14-1133
http://aclweb.org/anthology/Q15-1039
http://aclweb.org/anthology/Q15-1039
https://doi.org/10.18653/v1/N16-1089
http://aclweb.org/anthology/P09-1010
http://aclweb.org/anthology/P09-1010
http://www.aclweb.org/anthology/P12-1045
http://www.aclweb.org/anthology/P12-1045
http://www.aclweb.org/anthology/W10-2903
http://www.aclweb.org/anthology/W10-2903
http://www.aclweb.org/anthology/H94-1010
http://www.aclweb.org/anthology/H94-1010
https://doi.org/10.18653/v1/D15-1038
https://doi.org/10.18653/v1/P17-1097
https://doi.org/10.18653/v1/P17-1097
https://doi.org/10.18653/v1/P17-1097


2082

Michael Janner, Karthik Narasimhan, and Regina
Barzilay. 2018. Representation learning for
grounded spatial reasoning. Transactions of the As-
sociation for Computational Linguistics, 6:49–61.

Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.

Chen Liang, Jonathan Berant, Quoc Le, Kenneth D.
Forbus, and Ni Lao. 2017. Neural symbolic ma-
chines: Learning semantic parsers on freebase with
weak supervision. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.

Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies.

Reginald Long, Panupong Pasupat, and Percy Liang.
2016. Simpler context-dependent logical forms via
model projections. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.

Matthew MacMahon, Brian Stankiewics, and Ben-
jamin Kuipers. 2006. Walk the talk: Connecting
language, knowledge, action in route instructions.
In Proceedings of the National Conference on Ar-
tificial Intelligence.

Hongyuan Mei, Mohit Bansal, and Matthew R. Wal-
ter. 2016. Listen, attend, and walk: Neural mapping
of navigational instructions to action sequences. In
Association for the Advancement of Artificial Intel-
ligence.

Scott Miller, David Stallard, Robert Bobrow, and
Richard Schwartz. 1996. A fully statistical approach
to natural language interfaces. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics.

Dipendra Misra, John Langford, and Yoav Artzi. 2017.
Mapping instructions and visual observations to ac-
tions with reinforcement learning. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.

Andrew Y. Ng, Daishi Harada, and Stuart J. Russell.
1999. Policy invariance under reward transforma-
tions: Theory and application to reward shaping. In
Proceedings of the International Conference on Ma-
chine Learning.

Stéphane Ross, Geoffrey Gordon, and Drew Bagnell.
2011. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. In Pro-
ceedings of the International Conference on Artifi-
cial Intelligence and Statistics.

Alane Suhr, Srinivasan Iyer, and Yoav Artzi. 2018.
Learning to map context-dependent sentences to ex-
ecutable formal queries. In Proceedings of the An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies.

Hao Tan and Mohit Bansal. 2018. Source-target in-
ference models for spatial instruction understanding.
In AAAI Conference on Artificial Intelligence.

Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture
6.5-RMSProp: Divide the gradient by a running av-
erage of its recent magnitude. COURSERA: Neural
networks for machine learning, 4(2):26–31.

Gökhan Tür, Dilek Hakkani-Tür, and Larry Heck.
2010. What is left to be understood in ATIS? In Pro-
ceedings of the Spoken Language Technology Work-
shop.

Adam Vogel and Daniel Jurafsky. 2010. Learning to
follow navigational directions. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics.

Luke S. Zettlemoyer and Michael Collins. 2009.
Learning context-dependent mappings from sen-
tences to logical form. In Proceedings of the Joint
Conference of the Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP.

http://aclweb.org/anthology/Q18-1004
http://aclweb.org/anthology/Q18-1004
http://www.aclweb.org/anthology/D13-1161
http://www.aclweb.org/anthology/D13-1161
https://doi.org/10.18653/v1/P17-1003
https://doi.org/10.18653/v1/P17-1003
https://doi.org/10.18653/v1/P17-1003
http://www.aclweb.org/anthology/P11-1060
http://www.aclweb.org/anthology/P11-1060
https://doi.org/10.18653/v1/P16-1138
https://doi.org/10.18653/v1/P16-1138
https://doi.org/10.18653/v1/D15-1166
https://doi.org/10.18653/v1/D15-1166
http://www.aclweb.org/anthology/P96-1008
http://www.aclweb.org/anthology/P96-1008
http://aclweb.org/anthology/D17-1106
http://aclweb.org/anthology/D17-1106
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17257
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17257
http://aclweb.org/anthology/P10-1083
http://aclweb.org/anthology/P10-1083
http://www.aclweb.org/anthology/P09-1110
http://www.aclweb.org/anthology/P09-1110

