



















































Improvement of a Naive Bayes Sentiment Classifier Using MRS-Based Features


Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 22–29,
Dublin, Ireland, August 23-24 2014.

Improvement of a Naive Bayes Sentiment Classifier Using MRS-Based
Features

Jared Kramer
University of Washington

Seattle, WA
jaredkk@uw.edu

Clara Gordon
University of Washington

Seattle, WA
cgordon1@uw.edu

Abstract

This study explores the potential of us-
ing deep semantic features to improve bi-
nary sentiment classification of paragraph-
length movie reviews from the IMBD
website. Using a Naive Bayes classifier as
a baseline, we show that features extracted
from Minimal Recursion Semantics repre-
sentations in conjunction with back-off re-
placement of sentiment terms is effective
in obtaining moderate increases in accu-
racy over the baseline’s n-gram features.
Although our results are mixed, our most
successful feature combination achieves
an accuracy of 89.09%, which represents
an increase of 0.76% over the baseline per-
formance and a 6.48% reduction in error.

1 Introduction

Text-based sentiment analysis offers valuable in-
sight into the opinions of large communities of re-
viewers, commenters and customers. In their sur-
vey of the field, Pang and Lee (2008) highlight the
importance of sentiment analysis across a range
of industries, including review aggregation web-
sites, business intelligence, and reputation man-
agement. Detection and classification of sentiment
can improve downstream performance in applica-
tions sensitive to user opinions, such as question-
answering, automatic product recommendations,
and social network analysis (ibid., p. 12).

While previous research in sentiment analysis
has investigated the extraction of features from
syntactic dependency trees, semantic representa-
tions appear to be underused as a resource for
modeling opinion in text. Indeed, to our knowl-
edge, there has been no research using seman-
tic dependencies created by a precision grammar
for sentiment analysis. The goal of the present
research is to address this gap by augmenting a

baseline classifier with features based on Min-
imal Resursion Semantics (MRS; Copestake et
al., 2005), a formal semantic representation pro-
vided by the English Resource Grammar (ERG;
Flickinger, 2000). An MRS is a connected graph
in which semantic entities may be linked directly
through shared arguments or indirectly through
handle or qeq constraints, which denote equal-
ity modulo quantifier insertion (Copestake et al.,
2005). This schema allows for underspecification
of quantifier scope.

Using Narayanan et al.’s (2013) Naive Bayes
sentiment classifier as a baseline, we test the effec-
tiveness of eight feature types derived from MRS.
Our feature pipeline crawls various links in the
MRS representations of sentences in our corpus
of paragraph-length movie reviews and outputs
simple, human-readable features based on various
types of semantic relationships. This improved
system achieves modest increases in binary senti-
ment classification accuracy for several of the fea-
ture combinations tested.1

In the following sections, we summarize previ-
ous research in MRS feature extraction and senti-
ment classification, describe the baseline system
and our modifications to it, and outline our ap-
proach to parsing our data, constructing features,
and integrating them into the existing system. Fi-
nally, we report our findings, examine in more
detail where our improved system succeeded and
failed in relation to the baseline, and suggest av-
enues for further research in sentiment analysis
with MRS-based features.

2 Context and Related Work

Current approaches to sentiment analysis tasks
typically use supervised machine learning meth-

1Because this task consists of binary classification on an
evenly split dataset and every test document is assigned a
class, simple accuracy is the most appropriate measure of per-
formance.

22



ods with bag-of-words features as a baseline, and
for classification of longer documents like the ones
in our dataset, such features remain a powerful
tool of analysis. Wang and Manning (2012) com-
pare the performance of several machine learning
algorithms using uni- and bigram features from a
variety of common sentiment datasets, including
the IMDB set used in this project. They report that
that SVM classifiers generally perform better sen-
timent classification on paragraph-length reviews,
while Native Bayes classifiers produce better re-
sults for “snippets,” or short phrases (ibid., p. 91).
For our dataset, they obtain the highest accuracies
using a hybrid approach, SVM with Naive Bayes
features, which results in 91.22% accuracy (ibid.,
p. 93). This appears to be the best test result to
date on this dataset. Although we use a Naive
Bayes classifier in our project, alternative machine
learning algorithms are a promising topic of fur-
ther future investigation (see §6).

Two existing areas of research have direct rele-
vance to this project: MRS feature extraction, and
sentiment analysis using features based on deep
linguistic representations of data. In their work on
machine translation, Oepen et al. (2007) define a
type of MRS triple based on elementary dependen-
cies, a simplified “variable-free” representation of
predicate-argument relations in MRS (p. 5). Fu-
jita et al. (2007) and Pozen (2013) develop simi-
lar features for HPSG parse selection, and Pozen
experiments with replacing segments of predicate
values in triple features with WordNet sense, POS,
and lemma information (2013, p. 32).

While there has not yet been any research on us-
ing MRS features in sentiment analysis, there has
been work on extracting features from deep repre-
sentations of data for sentiment analysis. In work-
ing with deep representations such as MRSes or
dependency parses, there are myriad sub-graphs
that can be used as features. However these fea-
tures are often quite sparse and do not general-
ize well. Joshi & Rose (2009) improve perfor-
mance of a sentiment classifier by incorporating
triples consisting of words and grammatical rela-
tions extracted from dependency parses. To in-
crease the generalizability of these triples, they
perform back-off by replacing words with part-of-
speech tags. Similarly, Arora et al. (2010) extract
features from dependency parses by using senti-
ment back-off to identify potentially meaningful
portions of the dependency graph. Given this suc-

cess combining back-off with sub-graph features,
we design several feature types following a similar
methodology.

2.1 The IMBD Dataset
We use a dataset of 50,000 movie reviews crawled
from the IMDB website, originally developed by
Maas et al. (2011). The dataset is split equally
between training and test sets. Both training and
test sets contain equal numbers of positive and
negative reviews, which are defined according to
the number of stars assigned by the author on
the IMBD website: one to four stars for nega-
tive reviews, and seven to ten stars for positive
reivews. The reviews vary in length but gener-
ally contain between five and fifteen sentences.
The Natural Language ToolKit’s (NLTK; Loper
and Bird, 2002) sentence tokenizer distinguishes
616,995 sentences in the dataset.

Unlike previous research over this dataset, we
divide the 25,000 reviews of the test set into two
development sets and a final test set. As such,
our results are not directly comparable to those of
Wang & Manning (2012).

2.2 The Baseline System
The system we use as a baseline, created by
Narayanan et al. (2013), implements several small
but innovative improvements to a simple Naive
Bayes classifier. In the training phase, the base-
line performs simple scope of negation annota-
tion on the surface string tokens. Any word con-
taining the characters not, n’t or no triggers a
“negated” state, in which all following n-grams are
prepended with not . This continues until either
a punctuation delimiter (?.,!:;) or another negation
trigger is encountered.

During training, when an n-gram feature is read
into the classifier, it is counted toward P (f |c), and
the same feature with not prepended is counted
toward P (f |ĉ), where c is the document class
and ĉ is the opposite class. Singleton features
are then pruned away. Finally, the system runs a
set of feature-filtering trials, in which the pruned
features are ranked by mutual information score.
These trials start at a base threshold of 100,000
features, and the number of features is increased
stepwise in increments of 50,000. The feature set
that produces the highest accuracy in trials over a
development data set is then retained and used to
classify the test data. Table 1 shows the ten most
informative features, ranked by mutual informa-

23



Top N-Grams

1. worst 6. awful

2. bad 7. great

3. not the worst 8. waste

4. the worst 9. excellent

5. not worst 10. not not even

Table 1: Top MI-ranked baseline n-gram Features.

tion score, out of the 12.1 million n-gram features
generated by our baseline.

Before modifying the baseline system’s code,
we reproduced their reported accuracy figure of
88.80% over the entire 25,000 review test set.
However, it appears the baseline system used the
test data as development data. In order to address
this, we split the data as into development sets as
described above. When we ran the baseline sys-
tem over our final test set, we obtained accuracies
of 88.34% pre-feature filtering and 88.29% post-
feature filtering; our division of the original test
set into development and test sets accounts for this
discrepancy.

3 Methodology

Our approach to this task consisted of three gen-
eral stages: obtaining MRSes for the dataset,
implementing a feature pipeline to process the
MRSes, and integrating the new features into the
classifier. In this section we will describe each of
these processes in turn.

3.1 Parsing with the ERG

Because most of the reviews in our data set ap-
pear to be written in Standard English, we per-
form minimal pre-processing before parsing the
dataset with the ERG. We use NLTK’s sentence
tokenization function in our pipeline, along with
their HTML-cleaning function to remove some
stray HTML-style tags we encountered in the data.

To obtain MRS parses of the data, we use
ACE version 0.9.17, an “efficient processor
for DELPH-IN HPSG grammars.”2 ACE’s sim-
ple command line interface allows the parsing
pipeline to output MRS data in a single line to
a separate directory of MRS data files. We used
the 1212 ERG grammar image3 and specified root

2Available at http://sweaglesw.org/linguistics/ace/. Ac-
cessed January 15, 2014.

3Available at http://www.delph-in.net/erg/. Accessed Jan-

conditions that would allow for parses of the infor-
mal and fragmented sentences sometimes found
in our dataset: namely, the root informal,
root frag and root inffrag ERG root
nodes.

Parsing with these conditions resulted in
81.11% coverage over the entire dataset. After
manual inspection of sentences that failed to parse,
we found that irregularities in spelling and punc-
tuation accounted for the majority of these failures
and further cleaning of the data would yield higher
coverage.

3.2 Feature Design
Our main focus in feature design is capturing rel-
evant semantic relationships between sentiment
terms that extend beyond the trigram boundary.
Our entry point into the MRS is the elementary
predication (EP), and our pipeline algorithm ex-
plores the three main EP components: arguments
and associated variables, label, and predicate sym-
bol. We also use the set of handle constraints in
crawling the links between EPs.

We use two main categories of crawled MRS
features: Predicate-Relation-Predicate (PRP)
triples, a term borrowed from (Pozen, 2013), and
Shared-Label (SL) features. Our feature template
consists of eight feature subtypes, including plain
EP symbols (type 1), five PRP features (types 2
through 6) and two SL features (types 7 and 8).
Table 2 gives examples of each type, along with
the unpruned counts of distinct features gathered
from our training data. The examples for types
1 through 6 are taken from the abridged MRS
example in Figure 1. Note that an & character
separates predicate and argument components in
the feature strings. The type 7 and 8 examples
are taken from MRS of sentences featuring the
phrases successfully explores and didn’t flow well,
respectively.

In our feature extraction pipeline, we use Good-
man’s pyDelphin4 tool, a Python module that al-
lows for easy manipulation and querying of MRS
constituents. This tool allows our pipeline to
quickly process the ERG output files, obtain argu-
ment and handle constraint information, and out-
put the features for each MRS into a feature file to
be read by our classifier. If the grammar has not
returned an analysis for a particular sentence, the

uary 15, 2014.
4Available at https://github.com/goodmami/pydelphin.

Accessed January 20, 2014.

24



There is nothing redeeming about this trash.
[LTOP: h0

INDEX:e2 [e SF:prop TENSE:pres MOOD:indicative PROG:- PERF:-]

<[ be v there rel<6:8> LBL:h1 ARG0:e2 ARG1:x4] [thing rel<9:16> LBL:h5 ARG0:x4] [ no q rel<9:16>

LBL:h6 ARG0:x4 RSTR:h7 BODY:h8] [" redeem v for rel"<17:26> LBL:h5 ARG0:e9 ARG1:x4 ARG2:x10]

[" about x deg rel"<27:32> LBL:h11 ARG0:e12 ARG1:u13] [ this q dem rel<33:37> LBL:h11 ARG0:x10 RSTR:h14

BODY:h15] [" trash n 1 rel"<38:44> LBL:h16 ARG0:x10]>

HCONS: <h0 qeq h1 h7 qeq h5 h14 qeq h16>]

Figure 1: Sample abridged MRS, with mood, tense, and other morphosemantic features removed. Each
EP is enclosed in square brackets, bold type denotes predicate values.

Type Description Example Count

1 Pred value no q rel 4,505,389
2 PRP: all no q rel&RSTR&" redeem v for rel" 10,255,021
3 PRP: string preds only " redeem v for rel"&ARG2&" trash n 1 rel" 941,831
4 PRP: first pred back-off " POS v rel"&ARG2&" trash n 1 rel" 635,047
5 PRP: seond pred back-off " redeem v for rel"&ARG2&" NEG n rel" 621,929
6 PRP: double back-off " POS v rel"&ARG2&" NEG n rel" 20,962
7 SL: handle not a neg rel arg " successful a 1 rel"&" explore v 1 rel" 589,887
8 SL: handle a neg rel arg neg rel&" flow v 1 rel"&" well a 1 rel" 43,427

Table 2: Sample features (Note: Types 1 - 6 are taken from the MRS in Figure 1)

pipeline simply does not output any features for
that sentence.

3.2.1 MRS Crawling

In their revisiting of the 2012 SEM scope of nega-
tion shared task, Packard et al. (2014) improve on
the previous best performance using a relatively
simple set of MRS crawling techniques. We make
use of two of these techniques, “argument crawl-
ing” and “label crawling” in extracting our PRP
and SL features (ibid., p. 3). Both include select-
ing an “active EP” and adding to its scope all EPs
that conform to certain specifications. Argument
crawling selects all EPs whose distinguished vari-
able or label is an argument of the active EP, while
label crawling adds EPs that share a label with the
active EP (ibid., p. 3).

Our features are constructed in a similar fash-
ion; for every EP in an MRS, the pipeline se-
lects all EPs linked to the current EP and con-
structs features from this group of “in-scope”
EPs. PRP and SL features are obtained through
one “layer” of argument and label crawling, re-
spectively. After observing a number of noisy
and uninformative features in our preliminary
feature vectors, we excluded a small number

of EPs from being considered as the “active
EP” in our pipeline algorithm: udef q rel,
proper q rel, named rel, pron rel, and
pronoun q rel. More information about what
exactly these EPs represent can be found in Copes-
take et al. (2005).

3.2.2 PRP Features

These feature types are a version of the depen-
dency triple features used in Oepen et al. (2007)
and Fujita et al. (2007). We define the linking re-
lation as one in which the value of any argument of
the first EP matches the distinguished variable or
label of the second EP. For handle variables, we
count any targets of a qeq constraint headed by
that variable as equivalent. We use the same set of
EP arguments as Pozen (2013) to link predicates
in our PRP features: ARG, ARG1-N, L-INDEX,
R-INDEX, L-HANDL, R-HANDL, and RESTR (p.
31).

We also use a set of negative and positive word
lists from the social media domain, developed by
Hu and Liu (2004), for back-off replacement in
PRP features. Our pipeline algorithm attempts
back-off replacement for all EPs in all PRP triples.
If the surface string portion of the predicate value

25



Pre-Feature Post-Feature
Feature Types Filtering Filtering

baseline (n-grams only) 88.337 88.289
1 88.289 88.517
2 87.857 87.809
3 88.589 88.757
4 88.673 88.757
5 88.709 88.817
6 88.337 88.301
7 88.193 88.205
8 88.361 88.265

Table 3: Individual MRS feature trial results

matches any of the entries in the lexicon, the
pipeline produces a back-off predicate value by re-
placing that portion with NEG or POS and strip-
ping the sense category marker. These replace-
ments appear in various positions in feature types
4, 5, and 6 (see Table 2).

3.2.3 SL Features
To further explore the relationships in the MRS,
we include this second feature category in our fea-
ture template, which links together EPs that share
a handle variable. We limit SL features to groups
of EPs linked by a handle variable that is also an
argument of another EP, or the target of a qeq con-
straint of such a variable. Our pipeline is therefore
able to extract both PRP and SL features in a sin-
gle pass through the arguments of each EP. Feature
type 7 consists of shared-label groupings of two
or more EPs, where the handle is not the ARG1
of a neg rel EP. Type 8 includes groups of one
or more EPs where the handle is a neg rel ar-
gument, with neg rel prepended to the feature
string.

Features of type 7 tend to capture relationships
between modifiers, such as adverbs and adjectives,
and modified entities. Features of type 8 were
intended to provide some negation information,
though our goals of more fully analyzing scope
of negation in our dataset remain unrealized at
this point. We reasoned that the lemmatization of
string predicate values might provide some useful
back-off for the semantic entities involved in nega-
tion and modification.

4 Evaluation

To test our MRS features, we adapted our base-
line to treat them much like the n-gram features.

Pre-Feature Post-Feature
Feature Types Filtering Filtering

baseline (n-grams only) 88.337 88.289
n-grams with back-off 87.293 87.503
MRS only (all types) 88.253 87.977
n-grams, 4, 5 88.709 88.781
n-grams, 3, 4, 5, 7, 8 88.961 88.853
n-grams, 1, 4, 5 88.637 88.865
n-grams, 3, 4, 5 8 88.853 88.961
n-grams, 3, 4, 5, 7 88.889 88.973
n-grams, 1, 3, 4, 5 88.793 89.021
n-grams, 3, 4, 5 88.865 89.093

Table 4: Combination feature results

As with n-grams, each MRS feature is counted to-
ward the probability of the class of its source doc-
ument, and a negated version of that feature, with
not prepended, is counted toward the opposite
class. We ran our feature filtering trials using the
first development set, then obtained preliminary
accuracy figures from our second development set.
We began with each feature type in isolation and
used these results to inform later experiments us-
ing combinations of feature types. The numbers
reported here are the results over the final, held-
out test set.

Our final test accuracies indicate that three fea-
ture types produce the best gains in accuracy:
back-off PRPs with first- and second-predicate re-
placement (types 4 and 5), and PRPs with string
predicates only (type 3). Table 3 displays isolated
feature test results, while Table 4 ranks the top
seven feature combinations in ascending order by
post-feature filtering accuracies. The bolded fea-
ture types show that all of the best combination
runs include one or more of the top three features
mentioned above. Notable also are the accura-
cies for MRS-based features alone, which fall very
close to the baseline. The best accuracies for pre-
and post-feature filtering tests appear in bold.

The highest accuracy, achieved by running a
feature-filtered combination of the baseline’s n-
gram features and feature types 3, 4, and 5, re-
sulted in a 0.80% increase over the baseline per-
formance with feature filtering, and a 0.76% in-
crease in the best baseline accuracy overall (ob-
tained without feature filtering). The experimental
best run successfully categorizes 63 more of the
8333 test documents than the baseline best run.
Although these gains are small, they account for

26



a 6.48% reduction in error.

Most Informative MRS Features

not " NEG a rel"&ARG1& the q rel

" NEG a rel"&ARG1& the q rel

" POS a rel"&ARG1& the q rel

not " POS a rel"&ARG1& the q rel

" POS a rel"&ARG1& a q rel

not " POS a rel"&ARG1& a q rel

not " NEG a rel"&ARG1&" movie n of rel"

" NEG a rel"&ARG1&" movie n of rel"

a q rel&RSTR&" POS a rel"

not a q rel&RSTR&" POS a rel"

not " NEG a rel"&ARG1&udef q rel

" NEG a rel"&ARG1&udef q rel

superl rel&ARG1&" POS a rel"

not superl rel&ARG1&" POS a rel"

and c rel&LHNDL&" POS a rel"

Table 5: Most informative MRS features

5 Discussion

5.1 The Most Successful Experiments

The test accuracies indicate that our back-off re-
placement method, in combination with the simple
predicate-argument relationships captured in PRP
triples, is the most successful aspect of feature de-
sign in this project. However, as our error analysis
indicates, back-off is the likely source of many of
our system’s errors (see §5.2). Table 5 lists the 15
most informative MRS features from our best run
based on mutual information score, all of which
are of feature type 4 or 5. Note that the not
prepended to some features is a function of way
our classifier reads in binary features (as described
in §2.2), not an indication of grammatical nega-
tion. The success of these partial back-off fea-
tures confirms our intuition that the semantic rela-
tionships between sentiment-laden terms and other
entities in the sentence offer a reliable indicator
of author sentiment. When we performed back-
off replacement directly on the surface strings
and ran our classifier with n-grams only, we ob-
tained accuracies of 87.29% pre-feature filtering
and 87.50% post-feature filtering, a small decrease
from the baseline performance (see Table 4). This
lends additional support to the idea that the com-
bination of sentiment back-off and semantic de-
pendencies is significant. These results also fit
with the findings of of Joshi and Rose (2009), who

determined that back-off triple features provide
“more generalizable and useful patterns” in sen-
timent data than lexical dependency features alone
(p. 316).

Despite these promising results, we found that
the separate EP values (type 1), PRP triples with-
out replacement (type 2), PRPs with double re-
placement (type 6) and SL features (types 7 and
8) have very little effect on accuracy by them-
selves. For type 1, we suspect that EP values alone
don’t contribute enough information beyond ba-
sic n-gram features. We had hypothesized that the
lemmatization in these values might provide some
helpful back-off. However, this effect is likely
drowned out by the lack of any scope of negation
handling in the MRS features.

We attribute the failure of the SL features to the
fact that they often capture EPs originating in ad-
jacent tokens in the surface string, which does not
improve on the n-gram features. Lastly, we be-
lieve the relative sparsity of double back-off fea-
tures was the primary reason they did not produce
meaningful results.

These results also call into question the use-
fulness of the feature filtering trials in our base-
line. By design, these trials produce performance
increases on the dataset on which they are run.
However, filtering produces small and inconsistent
gains for the final held-out test set.

Error Types

Misleading back-off 31
Plot summary / Noise 20
Obscure Words / Data Sparsity 7
Data Error 3
Nonsensical Review 3
Reason Unsure 40

Table 6: Error types from top MRS experiment

5.2 Error Analysis

We manually inspected the 104 reviews from the
final test set that were correctly classified by the
best run of the baseline system but incorrectly
classified by the best run of our improved sys-
tem. This set contains 50 false negatives, and 54
false positives. We classified them according to
five subjective categories: misleading back-off, in
which many of the sentiment terms have a polarity
opposite to the overall review; excess plot sum-

27



Incorrectly classified Correctly classified
N

eg
at

iv
e

do
cs "_POS_a__rel"&ARG1&_the_q_rel "_NEG_n__rel"&ARG0&udef_q_rel

"_POS_a__rel"&ARG1&_a_q_rel "_NEG_a__rel"&ARG1&_the_q_rel

"_POS_a__rel"&ARG1&udef_q_rel "_NEG_a__rel"&ARG1&udef_q_rel

"_NEG_n__rel"&ARG0&udef_q_rel "_POS_a__rel"&ARG1&_a_q_rel

_a_q_rel&RSTR&"_POS_a__rel" _the_q_rel&RSTR&"_NEG_n__rel"

Po
si

tiv
e

do
cs "_NEG_n__rel"&ARG0&udef_q_rel "_POS_a__rel"&ARG1&_a_q_rel

"_NEG_v__rel"&ARG1&pronoun_q_rel "_POS_a__rel"&ARG1&_the_q_rel

"_NEG_v__rel"&ARG1&pron_rel _a_q_rel&RSTR&"_POS_a__rel"

"_NEG_a__rel"&ARG1&udef_q_rel "_NEG_n__rel"&ARG0&udef_q_rel

"_NEG_a__rel"&ARG1&_the_q_rel "_POS_a__rel"&ARG1&udef_q_rel

Table 7: Most frequent features in test data by polarity and classification result

mary or off-topic language; use of obscure words
not likely to occur frequently in the data; miscat-
egorization in the dataset; and confusing or non-
sensical language. The counts for these categories
appear in Table 6.

The prevalence of errors in the first category
is revealing, and relates to certain subcategories
of review that confound our sentiment back-off
features. For horror films in particular, words
that would generally convey negative sentiment
(creepy, horrible, gruesome) are instead used pos-
itively. This presents an obvious problem for senti-
ment back-off, which relies on the assumption that
words are generally used with the same intent.

To explore this further, we collected counts of
the most frequent features in these 104 reviews,
and compared them to feature counts for correctly
classified documents of the same class. The stark
contrast between the back-off polarities of the fea-
tures extracted and the polarity of the documents
suggests that these feature types are overgener-
alizing and misleading the classifier (see Table
7). While the course-grained polarity of sentiment
terms is often a good indicator of overall review
polarity, our system has difficulty with cases in
which many sentiment terms do not align with the
review sentiment. Our back-off PRP features do
not include scope of negation handling, so even if
these terms are negated, our classifier in its current
form is unable to take advantage of that informa-
tion.

Further manual observation of the feature vec-
tors from these documents suggests that the senti-
ment lexicon contains elements that are not suited
to the movie review domain; plot, for example is

classified as a negative term. These results point
to the need for a more domain-specific sentiment
lexicon, and perhaps additional features that look
at the combination of sentiment terms present in a
review. LDA models could provide some guidance
in capturing and analyzing co-occurring groups of
sentiment terms.

6 Conclusions and Future Work

Our attempt to improve binary sentiment classifi-
cation with MRS-based features is motivated by a
desire to move beyond shallow approaches and ex-
plore the potential for features based on semantic
dependencies. Our preliminary results are promis-
ing, if modest, and point to back-off replacement
as a useful tool in combination with the relation-
ships captured by predicate triples.

There are a number of potential areas for im-
provement and further development of our ap-
proach. In light of Wang and Manning’s (2012) re-
sults using an SVM classifier on the same dataset,
one obvious direction would be to experiment with
this and other machine learning algorithms. Addi-
tionally, the ability to account for negation in the
MRS features types as in Packard et al. (2014)
would likely mitigate some of the errors caused
by the back-off PRP features

Another possibility for expansion would be the
development of features using larger feature sub-
graphs. Because of concerns about runtime and
data sparsity, we crawl only one level of the MRS
and examine a limited set of relationships. The
success of Socher et al.’s (2013) Recursive Neural
Tensor Network suggest that with enough data, it
is possible to capture the complex compositional

28



effects of various sub-components. Given their
success with syntactic dependencies, and the re-
search presented here, we believe semantic de-
pendencies will be a fruitful avenue for future re-
search in sentiment analysis. This project has been
an exciting first step into uncharted territory, and
suggests the potential to further exploit the MRS
in sentiment analysis applications. Nonetheless,
the performance gains we were able to observe
demonstrate the power of using semantic repre-
sentations produced by a linguistically motivated,
broad-coverage parser as an information source
in a semantically sensitive task such as sentiment
analysis.

Acknowledgments

Thanks to our professor, Emily Bender, for pro-
viding her expertise and guidance at all stages of
this project.

We’re grateful to Michael Goodman for making
his pyDelphin module freely available, guiding
us in using it, and providing timely troubleshoot-
ing and support via email for the duration of this
project. Thanks to Woodley Packard, who pro-
vided helpful advice on getting the best use out
of ACE.

References
S. Arora, E. Mayfield, C Penstein-Rosé, and E Nyberg.

2010. Sentiment Classification using Automatically
Extracted Subgraph Features. In Proceedings of the
NAACL HLT 2010 Workshop on Computational Ap-
proaches to Analysis and Generation of Emotion in
Text, pp. 131 - 139. Los Angeles, CA.

A. Copestake, D. Flickinger, C. Pollard, and I. A. Sag.
2005. Minimal Recursion Semantics: An Introduc-
tion. Research on Natural Language and Computa-
tion, 3(4), pp. 281 - 332.

D. Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering,, 6(1), pp. 15 - 28.

S. Fujita, F. Bond, S. Oepen, T. Tanaka. 2010. Exploit-
ing semantic information for HPSG parse selection.
Research on Language and Computation. 8(1): 1-22

M. Hu and B. Liu. 2004. Mining and Summariz-
ing Customer Reviews. In Proceedings of the ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD-2004). Seattle,
WA.

M. Joshi and C Penstein Rosé. 2009. Generalizing
Dependency Features for Opinion Mining. In Pro-

ceedings of the ACL-IJCNLP 2009 Conference Short
Papers, pp. 313 - 316. Suntec, Singapore.

E. Loper, and S., Bird. 2002. NLTK: The Natu-
ral Language Toolkit. In Proceedings of the ACL
Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing and Compu-
tational Linguistics. Philadelphia: Association for
Computational Linguistics

L. Jia, C. Yu, and W. Meng. 2009. The effect of nega-
tion on sentiment analysis and retrieval effective-
ness. In Proceedings of the 18th ACM conference
on Information and knowledge management (CIKM
’09), pp. 1827 - 1830. Hong Kong, China.

A. L. Maas, R. E. Daly, Peter T. Pham, Dan Huang,
Andrew Y. Ng, and C. Potts. 2011. Learning word
vectors for sentiment analysis. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pp. 142 - 150. Portland, Oregon.

V. Narayanan, I. Arora, and A. Bhatia. 2013. Fast and
accurate sentiment classification using an enhanced
Naive Bayes’model. Intelligent Data Engineering
and Automated Learning IDEAL function Lecture
Notes in Computer Science, 8206:194 - 201.

S. Oepen, E. Velldal, J. Lonning, P. Meurer, V. Rosn,
and D. Flickinger. 2007. Towards Hybrid Qual-
ity Oriented Machine Translation. In Proceedings
of the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation.

W. Packard, E. M. Bender, J. Read, S. Oepen and R.
Dridan. 2014. Simple Negation Scope Resolution
Through Deep Parsing: A Semantic Solution to a
Semantic Problem. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics. Baltimore, MD.

B. Pang and L. Lee. 2008. Opinion Mining and Senti-
ment Analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1 - 135.

Z. Pozen. 2013. Using Lexical and Compositional Se-
mantics to Improve HPSG Parse Selection. Master’s
Thesis, University of Washington.

R. Socher, A. Perelygin, J. Wu, J. Chuang. C. Man-
ning, A. Ng, and C. Potts 2013. Recursive Deep
Models for Semantic Compositionality Over a Sen-
timent Treebank In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pp. 1631-1642. Seattle, WA.

S. Wang and C. D. Manning. 2012. Baselines and Bi-
grams: Simple, Good Sentiment and Topic Classica-
tion. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics, pp.
90 94. Jeju, Republic of Korea.

A Yeh. 2000. More accurate tests for the statistical
significance of result differences. In Proceedings of
the 18th Conference on Computational Linguistics
(COLING), pp. 147 - 153.

29


