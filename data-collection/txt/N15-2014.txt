



















































Detecting Translation Direction: A Cross-Domain Study


Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 103–109,
Denver, Colorado, June 1, 2015. c©2015 Association for Computational Linguistics

Detecting Translation Direction: A Cross-Domain Study

Sauleh Eetemadi
Michigan State University, East Lansing, MI

Microsoft Research, Redmond, WA
saulehe@microsoft.com

Kristina Toutanova
Microsoft Research

Redmond, WA
kristout@microsoft.com

Abstract

Parallel corpora are constructed by taking a
document authored in one language and trans-
lating it into another language. However,
the information about the authored and trans-
lated sides of the corpus is usually not pre-
served. When available, this information can
be used to improve statistical machine trans-
lation. Existing statistical methods for trans-
lation direction detection have low accuracy
when applied to the realistic out-of-domain
setting, especially when the input texts are
short. Our contributions in this work are three-
fold: 1) We develop a multi-corpus paral-
lel dataset with translation direction labels at
the sentence level, 2) we perform a compara-
tive evaluation of previously introduced fea-
tures for translation direction detection in a
cross-domain setting and 3) we generalize a
previously introduced type of features to out-
perform the best previously proposed features
in detecting translation direction and achieve
0.80 precision with 0.85 recall.

1 Introduction
Translated text differs from authored text (Baker,

1993). The main differences are simplification, ex-
plicitation, normalization and interference (Volan-
sky et al., 2013). Statistical classifiers have been
trained to detect Translationese1. Volansky et al.
(2013) state two motivations for automatic detection
of Translationese: empirical validation of Trans-
lationese linguistic theories and improving statis-
tical machine translation (Kurokawa et al., 2009).

1Translated text is often referred to as “Translationese”
(Volansky et al., 2013).

Most of the prior work focus on in-domain Trans-
lationese detection (Baroni and Bernardini, 2006;
Kurokawa et al., 2009). That is, the training and
test set come from the same, usually narrow, do-
main. Cross-domain Translationese detection serves
the two stated motivations better than in-domain de-
tection. First, automatic classification validates lin-
guistic theories only if it works independent of the
domain. Otherwise, the classifier could perform
well by memorizing lexical terms unique to a spe-
cific domain without using any linguistically mean-
ingful generalizations. Second, a Translationese
classifier can improve statistical machine translation
in two ways: 1) By labeling the parallel training
data with translation direction2; 2) By labeling in-
put sentences to a decoder at translation time and
use matching models. The accuracy of the classifier
is the main factor determining its impact on statis-
tical machine translation. Most parallel or mono-
lingual training data sources do not contain transla-
tion direction meta-data. Also, the input sentences
at translation time can be from any domain. There-
fore, a cross-domain setting for translation direction
detection is more appropriate for improving statisti-
cal machine translation as well. We develop a cross-
domain training and test data set and compare some
of the linguistically motivated features from prior
work (Kurokawa et al., 2009; Volansky et al., 2013)
in this setting. In addition, we introduce a new bilin-
gual feature that outperforms all prior work in both

2Detection of translation direction refers to classifying a
text block pair (A and B) as A was translated to B or vice versa.
In contrast, Translationese detection usually refers to classify-
ing a single block of text as “Translationese” versus “Original”.

103



in-domain and cross-domain settings.
Our work also differs from many prior works by

focusing on sentence level, rather than block level
classification. Although Kurokawa et al. (2009)
compare sentence level versus block level detection
accuracy, most other research focuses on block level
detection (Baroni and Bernardini, 2006; Volansky
et al., 2013). Sentence level classification serves
the stated motivations above better than block level
classification. For empirical validation of linguis-
tic theories, features that are detectable at the sen-
tence level are more linguistically meaningful than
block level statistics. Sentence level detection is also
more appropriate for labeling decoder input as well
as some statistical machine translation training data.

In the rest of the paper, we first review prior work
on sentence level and cross-domain translation di-
rection detection. In Section 3 we motivate the se-
lection of features used in this study. Next, we de-
scribe our cross-domain data set and the classifica-
tion algorithm we use to build and evaluate models
given a set of features. Experimental results are pre-
sented in Section 5.2.

2 Related Work
Volansky et al. (2013) provide a comprehensive

list of monolingual features used for Translationese
detection. These features include POS n-grams,
character n-grams, function word frequency, punc-
tuation frequency, mean word length, mean sentence
length, word n-grams and type/token ratio. We are
aware of only one prior work that presented a cross-
domain evaluation. Koppel and Ordan (2011) use a
logistic regression classifier with function word un-
igram frequencies to achieve 92.7% accuracy with
ten fold cross validation on the EuroParl (Koehn,
2005) corpus and 86.3% on the IHT corpus. How-
ever testing the EuroParl trained classifier on the
IHT corpus yields an accuracy of 64.8% (and the
accuracy is 58.8% when the classifier is trained on
IHT and tested on EuroParl). The classifiers in this
study are trained and tested on text blocks of approx-
imately 1500 tokens, and there is no comparative
evaluation of models using different feature sets.

We are also aware of two prior works that in-
vestigate Translationese detection accuracy at the
sentence level. First Kurokawa et al (2009) use
the Hansard English-French corpus for their ex-

Label Description
ENG.LEX English word n-grams
FRA.LEX French word n-grams
ENG.POS English POS Tag n-grams
FRA.POS French POS Tag n-grmas
ENG.BC English Brown Cluster n-grams
FRA.BC French Brown Cluster n-grams
POS.MTU POS MTU n-grams
BC.MTU Brown Cluster MTU n-grams

Table 1: Classification features and their labels.

periments. For sentence level translation direc-
tion detection they reach F-score of 77% using
word n-grams and stay slightly below 70% F-score
with POS n-grams using an SVM classifier. Sec-
ond, Eetemadi and Toutanova (2014) leverage word
alignment information by extracting POS tag mini-
mal translation units (MTUs) (Quirk and Menezes,
2006) along with an online linear classifier trained
on the Hansard English-French corpus to achieve
70.95% detection accuracy at the sentence level.

3 Feature Sets
The goal of our study is to compare novel and pre-

viously introduced features in a cross-domain set-
ting. Due to the volume of experiments required
for comparison, for an initial study, we select a lim-
ited number of feature sets for comparison. Prior
works claim POS n-gram features capture linguis-
tic phenomena of translation and should generalize
across domains (Kurokawa et al., 2009; Eetemadi
and Toutanova, 2014). We chose source and tar-
get POS n-gram features for n = 1 . . . 5 to test this
claim. Another feature we have chosen is from the
work of Eetemadi and Toutanova (2014) where they
achieve higher accuracy by introducing POS MTU3

n-gram features.
POS MTUs incorporate source and target side in-

formation in addition to word alignment. Prior work
has also claimed lexical features such as word n-
grams do not generalize across domains due to cor-
pus specific vocabulary (Volansky et al., 2013). We
test this hypothesis using source and target word n-
gram features. Using n-grams of length 1 through 5
we run 45 (nine data matrix entries times n-gram
lengths of five) experiments for each feature set
mentioned above.

In addition to the features mentioned above, we
3Minimal Translation Units (Quirk and Menezes, 2006)

104



Corpus Authored Language Translation Language Training Sentences Test Sentences
EuroParl English French 62k 6k
EuroParl French English 43k 4k
Hansard English French 1,697k 169k
Hansard French English 567k 56k
Hansard-Committees English French 2,930k 292k
Hansard-Committees French English 636k 63k

Table 2: Cross-Domain Data Sets

make a small modification to the feature used to ob-
tain the best previously reported sentence level per-
formance (Eetemadi and Toutanova, 2014) to derive
a new type of features. POS MTU n-gram fea-
tures are the most linguistically informed features
amongst prior work. We introduce Brown cluster
(Brown et al., 1992) MTUs instead. Our use of
Brown clusters is inspired by recent success on their
use in statistical machine translation systems (Bhatia
et al., 2014; Durrani et al., 2014). Finally, we also
include source and target Brown cluster n-grams as
a comparison point to better understand their effec-
tiveness compared to POS n-grams and their contri-
bution to the effectiveness of Brown cluster MTUs.

Given these 8 feature types summarized in Table
1, n-gram lengths of up to 5 and the 3 × 3 data ma-
trix explained in the next section, we run 360 exper-
iments for this cross-domain study.

4 Data, Preprocessing and Feature
Extraction

We chose the English-French language pair for
our cross-domain experiments based on prior work
and availability of labeled data. Existing sentence-
parallel datasets used for training machine trans-
lation systems, do not normally contain gold-
standard translation direction information, and addi-
tional processing is necessary to compile a dataset
with such information (labels). Kurokawa et al
(2009) extract translation direction information from
the English-French Hansard parallel dataset using
speaker language tags. We use this dataset, and treat
the two sections “main parliamentary proceedings”
and “committee hearings” as two different corpora.
These two corpora have slightly different domains,
although they share many common topics as well.
We additionally choose a third corpus, whose do-
main is more distinct from these two, from the Eu-
roParl English-French corpus. Islam and Mehler
(2012) provided a customized version of Europarl

with translation direction labels, but this dataset only
contains sentences that were authored in English
and translated to French, and does not contain ex-
amples for which the original language of author-
ing was French. We thus prepare a new dataset
from EuroParl and will make it publicly available
for use. The original unprocessed version of Eu-
roParl (Koehn, 2005) contains speaker language tags
(original language of authoring) for the French and
English sides of the parallel corpus. We filter out in-
consistencies in the corpus. First, we filter out sec-
tions where the language tag is missing from one or
both sides. We also filter out sections with conflict-
ing language tags. Parallel sections with different
number of sentences are also discarded to maintain
sentence alignment. This leaves us with three data
sets (two Hansard and one EuroParl) with transla-
tion direction information available, and which con-
tain sentences authored in both languages. We hold
out 10% of each data set for testing and use the rest
for training. Our 3×3 corpus data matrix consists of
all nine combinations of training on one corpus and
testing on another (Table 2).

4.1 Preprocessing

First, we clean all data sets using the following sim-
ple techniques.
• Sentences with low alphanumeric density are

discarded.
• A character n-gram based language detection

tool is used to identify the language of each
sentence. We discard sentences with a detected
language other than their label.
• We discard sentences with invalid unicode

characters or control characters.
• Sentences longer than 2000 characters are ex-

cluded.
Next, an HMM word alignment model (Vogel et

al., 1996) trained on the WMT English-French cor-
pus (Bojar et al., 2013) word-aligns sentence pairs.

105



Figure 1: POS Tagged and Brown Cluster Aligned Sentence Pairs

We discard sentence pairs where the word alignment
fails. We use the Stanford POS tagger (Toutanova
and Manning, 2000) for English and French to tag
all sentence pairs. A copy of the alignment file with
words replaced with their POS tags is also gener-
ated. French and English Brown clusters are trained
separately on the French and English sides of the
WMT English-French corpus (Bojar et al., 2013).
The produced models assign cluster IDs to words in
each sentence pair. We create a copy of the align-
ment file with cluster IDs instead of words as well.

4.2 Feature Extraction

The classifier of our choice (Section 5) extracts n-
gram features with n specified as an option. In
preparation for classifier training and testing, feature
extraction only needs to produce the unigram fea-
tures while preserving the order (n-grams of higher
length are automatically extracted by the classifier).
POS, word, and Brown cluster n-gram features are
generated by using the respective representation for
sequences of tokens in the sentences. For POS and
Brown cluster MTU features, the sequence of MTUs
is defined as the left-to-right in source order se-
quence (due to reordering, the exact enumeration or-
der of MTUs matters). For example, for the sen-
tence pair in Figure 1, the sequence of Brown clus-
ter MTUs is: 73⇒(390,68), 208⇒24, 7689⇒3111,
7321⇒1890, 2⇒16.
5 Experiments

We chose the Vowpal Wabbit (Langford et al.,
2007) (VW) online linear classifier since it is fast,
scalable and it has special (bag of words and n-gram
generation) options for text classification. We found
that VW was comparable in accuracy to a batch lo-
gistic regression classifier. For training and test-
ing the classifier, we created balanced datasets with
the same number of training examples in both di-

rections. This was achieved by randomly removing
sentence pairs from the English to French direction
until it matches the French to English direction. For
example, 636k sentence pairs are randomly chosen
from the 2,930k sentence pairs in English to French
Hansard-Committees corpus to match the number of
examples in the French to English direction.

5.1 Evaluation Method
We are interested in comparing the performance of
various feature sets in translation direction detec-
tion. Performance evaluation of different classifi-
cation features objectively is challenging in the ab-
sence of a downstream task. Specifically, depending
on the preferred balance between precision and re-
call, different features can be superior. Ideally an
ROC graph (Fawcett, 2006) visualizes the tradeoff
between precision and recall and can serve as an ob-
jective comparison between different classification
feature sets. However, it is not practical to present
ROC graphs for 360 experiments. Hence, we resort
to the Area Under the ROC graph (AUC) measure as
a good measure to provide an objective comparison.
Theoretically, the area under the curve can be inter-
preted as the probability that the classifier scores a
random negative example higher than a random pos-
itive example (Fawcett, 2006). As a point of refer-
ence, we also provide F-scores for experimental set-
tings that are comparable to the prior work reviewed
in Section 2.

5.2 Results
Figure 2 presents AUC points for all experiments.
Rows and columns are labeled with corpus names
for training and test data sets respectively. For ex-
ample, the graph on the third row and first column
corresponds to training on the Hansard-Committees
corpus and testing on EuroParl. Within each graph
we compare the AUC performance of different fea-

106



50

55

60

65

70

75

80

85

90

1 2 3 4 5

HANSARD-COMMITTES

50

55

60

65

70

75

80

85

90

1 2 3 4 5

1 2 3 4 5

N-GRAM COUNTBC.MTU ENU.BC ENU.LEX ENU.POS FRA.BC FRA.LEX FRA.POS POS.MTU

50

55

60

65

70

75

80

85

90

1 2 3 4 5

HANSARD

50

55

60

65

70

75

80

85

90

1 2 3 4 5

50

55

60

65

70

75

80

85

90

1 2 3 4 5

50

55

60

65

70

75

80

85

90

1 2 3 4 5

EU
R

O
P

A
R

L

EUROPARL

50

55

60

65

70

75

80

85

90

1 2 3 4 5

H
A

N
SA

R
D

50

55

60

65

70

75

80

85

90

1 2 3 4 5

H
A

N
SA

R
D

-C
O

M
M

IT
TE

ES

AUC
TEST CORPUS

TR
A

IN
IN

G
C

O
R

P
U

S

Figure 2: Comparing area under the ROC curve for the translation direction detection task when training and testing
on different corpora using each of the eight feature sets. See Table 1 for experiment label description.

tures with n-gram lengths of 1 through 5.
Graphs on the diagonal correspond to in-domain

detection and demonstrate higher performance com-
pared to off diagonal graphs. This confirms the ba-
sic assumption that cross-domain translation direc-
tion detection is a more difficult task. The over-
all performance is also higher when trained on the
Hansard corpus and tested on Hansard-Commitee
and vice versa. This is because the Hansard cor-
pus is more similar to the Hansard-Committees cor-
pus compared to the EuroParl corpus. It is also ob-
servable that the variation in performance of dif-
ferent features diminishes as the training and test
corpora become more dissimilar. For instance, this
phenomenon can be observed on the second row of
graphs where the features are most spread out when
tested on the Hansard corpus. They are less spread
out when tested on the Hansard-Committees corpus,
and compressed together when tested on the Eu-
roParl corpus. The same phenomenon can be ob-
served for classifiers trained on other corpora.

For different feature types, different n-gram or-
der of the features is best, depending on the feature
granularity. To make it easier to observe patterns
in the performance of different feature types, Figure
3 shows the performance for each feature type and

each train-test corpus combination as a single point,
by using the best n-gram order for that feature/data
combination. Each of the 9 train/test data combina-
tions is shown as a curve over feature types.

We can see that MTU features (which look at both
languages at the same time) outperform individual
source or target features (POS or Brown cluster) for
all datasets. Brown clusters are unsupervised and
can provide different levels of granularity. On the
other hand, POS tags usually provide a fixed gran-
ularity and require lexicons or labeled data to train.
We see that Brown clusters outperform correspond-
ing POS tags across data settings. As an example,
when training and testing on the Hansard corpus
FRA.BC outperforms FRA.POS by close to 20 AUC
points.

Lexical features outperform monolingual POS
and Brown cluster features in most settings although
their advantages diminish as the training and test
corpus become more dissimilar. This is somewhat
contrary to prior claims that lexical features will not
generalize well across domains – we see that lexical
features do capture important generalizations across
domains and models that use only POS tag features
have lower performance, both in and out-of-domain.

Figure 4 shows the rank of each feature amongst

107



50

55

60

65

70

75

80

85

90

95

BC.MTU FRA.LEX ENU.LEX ENU.BC FRA.BC POS.MTU ENU.POS FRA.POS

M
A

X
 A

U
C

FEATURES

EU-EU EU-H EU-HC HC-EU HC-H HC-HC H-EU H-H H-HC

C
ro

ss
-D

o
m

ai
n

Figure 3: Translation detection performance matrix for
training and testing on three different corpora - We ran
experiments for n-grams of up to length five for each
feature (See Table 1 for feature label descriptions). Un-
like Figure 2 where we report AUC values for all n-gram
lengths, in this graph we only present the highest AUC
number for each feature. Each marker type indicates a
training and test set combination. The format of experi-
ment labels in the legend is [TrainingSet]-[TestSet] and
EU: EuroParl, H: Hansard, HC: Hansard Committees.
For example, EU-HC means training on EuroParl corpus
and testing on Hansard Committees corpus.

all 8 different features for each entry in the cross-
corpus data matrix (Similar to Figure 3 the highest
performing n-gram length has been chosen for each
feature). Brown cluster MTUs outperform all other
features with rank one in all dataset combinations.
Source and target POS tag features are the lowest
performing features in 8 out of 9 data set combina-
tions. The POS.MTU has its lowest ranks (7 and
8) when it is trained on the EuroParl corpus and its
highest ranks (2 and 3) when trained on the Hansard-
Committees corpus. High number of features in
POS.MTU requires a large data set for training. The
variation in performance for POS.MTU can be ex-
plained by the significant difference in training data
size between EuroParl and Hansard-Committees.
Finally, while FRA.LEX and ENG.LEX are mostly
in rank 2 and 3 (after BC.MTU) they have their low-
est ranks (6 and 4) in cross-corpus settings (HC-EU
and HC-H).

Finally, we report precision and recall numbers
to enable comparison between our experiments and
previous work reported in Section 2. When train-

0

1

2

3

4

5

6

7

8

H-H EU-EU HC-HC H-HC HC-H H-EU EU-HC EU-H HC-EU

M
A

X
 A

U
C

 R
A

N
K

TRAINING AND TEST CORPUS COMBINATIONS

BC.MTU FRA.LEX ENU.LEX ENU.BC FRA.BC POS.MTU ENU.POS FRA.POS

Cross-Domain

Figure 4: Translation direction detection AUC perfor-
mance rank for each training and test set combination.
For corpus combination abbreviations see description of
Figure 3. For feature label descriptions see Table 1.

ing and testing on the Hansard corpus, BC.MTU
achieves 0.80 precision with 0.85 recall. In compari-
son, ENG.POS achieves 0.65 precision with 0.64 re-
call and POS.MTU achieves 0.73 precision and 0.74
recall. These are the highest performance of each
feature with n-grams of up to length 5.

6 Conclusion and Future Work

From among eight studied sets of features, Brown
cluster MTUs were the most effective at identify-
ing translation direction at the sentence level. They
were superior in both in-domain and cross-domain
settings. Although English-Lexical features did not
perform as well as Brown cluster MTUs, they per-
formed better than most other methods. In future
work, we plan to investigate lexical MTUs and to
consider feature sets containing any subset of the
eight or more basic feature types we have consid-
ered here. With these experiments we hope to gain
further insight into the performance of feature sets
in in out out-of-domain settings and to improve the
state-of-the-art in realistic translation direction de-
tection tasks. Additionally, we plan to use this clas-
sifier to extend the work of Twitto-Shmuel (2013) by
building a more accurate and larger parallel corpus
labeled for translation direction to further improve
SMT quality.

108



References
Mona Baker. 1993. Corpus linguistics and translation

studies: Implications and applications. Text and tech-
nology: in honour of John Sinclair, 233:250.

Marco Baroni and Silvia Bernardini. 2006. A new
approach to the study of translationese: Machine-
learning the difference between original and trans-
lated text. Literary and Linguistic Computing,
21(3):259–274.

Austin Matthews Waleed Ammar Archna Bhatia, We-
ston Feely, Greg Hanneman Eva Schlinger Swabha
Swayamdipta, Yulia Tsvetkov, and Alon Lavie Chris
Dyer. 2014. The cmu machine translation systems at
wmt 2014. ACL 2014, page 142.

Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp Koehn,
Christof Monz, Matt Post, Radu Soricut, and Lucia
Specia. 2013. Findings of the 2013 workshop on
statistical machine translation. In Proceedings of the
Eighth Workshop on Statistical Machine Translation,
pages 1–44.

Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-
cent J Della Pietra, and Jenifer C Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional linguistics, 18(4):467–479.

Nadir Durrani, Philipp Koehn, Helmut Schmid, and
Alexander Fraser. 2014. Investigating the usefulness
of generalized word representations in smt. In Pro-
ceedings of the 25th Annual Conference on Computa-
tional Linguistics (COLING), Dublin, Ireland, pages
421–432.

Sauleh Eetemadi and Kristina Toutanova. 2014. Asym-
metric features of human generated translation. In
Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 159–164. Association for Computational Lin-
guistics.

Tom Fawcett. 2006. An introduction to roc analysis.
Pattern recognition letters, 27(8):861–874.

Zahurul Islam and Alexander Mehler. 2012. Customiza-
tion of the europarl corpus for translation studies. In
LREC, page 2505–2510.

Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Summit,
pages 79–86, Phuket, Thailand. AAMT, AAMT.

Moshe Koppel and Noam Ordan. 2011. Translationese
and its dialects. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies-Volume 1, page
1318–1326. Association for Computational Linguis-
tics.

David Kurokawa, Cyril Goutte, and Pierre Isabelle.
2009. Automatic detection of translated text and
its impact on machine translation. Proceedings. MT
Summit XII, The twelfth Machine Translation Sum-
mit International Association for Machine Translation
hosted by the Association for Machine Translation in
the Americas.

J Langford, L Li, and A Strehl, 2007. Vowpal wabbit
online learning project.

Chris Quirk and Arul Menezes. 2006. Do we need
phrases?: Challenging the conventional wisdom in sta-
tistical machine translation. In Proceedings of the
Main Conference on Human Language Technology
Conference of the North American Chapter of the As-
sociation of Computational Linguistics, HLT-NAACL
’06, pages 9–16, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proceedings of the
2000 Joint SIGDAT Conference on Empirical Methods
in Natural Language Processing and Very Large Cor-
pora: Held in Conjunction with the 38th Annual Meet-
ing of the Association for Computational Linguistics
- Volume 13, EMNLP ’00, pages 63–70, Stroudsburg,
PA, USA. Association for Computational Linguistics.

Naama Twitto-Shmuel. 2013. Improving Statistical Ma-
chine Translation by Automatic Identification of Trans-
lationese. Ph.D. thesis, University of Haifa.

Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics-Volume 2, pages 836–841. Asso-
ciation for Computational Linguistics.

Vered Volansky, Noam Ordan, and Shuly Wintner. 2013.
On the features of translationese. Literary and Lin-
guistic Computing, page 31.

109


