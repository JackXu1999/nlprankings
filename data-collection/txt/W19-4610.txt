



















































Constrained Sequence-to-sequence Semitic Root Extraction for Enriching Word Embeddings


Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 88–96
Florence, Italy, August 1, 2019. c©2019 Association for Computational Linguistics

88

Constrained Sequence-to-sequence Semitic Root Extraction for Enriching
Word Embeddings

Ahmed El-Kishky†∗, Xingyu Fu†∗, Aseel Addawood†,
Nahil Sobh†, Clare Voss‡, and Jiawei Han†

†Department of Computer Science, The University of Illinois at Urbana Champaign
‡Computational & Information Sciences Directorate, Army Research Laboratory

†Urbana, IL, USA, ‡Adelphi, MD, USA
{elkishk2,xingyuf2,aaddaw2,sobh,hanj}@illinois.edu

clare.r.voss.civ@mail.mil

Abstract

In this paper, we tackle the problem of “root
extraction” from words in the Semitic lan-
guage family. A challenge in applying natural
language processing techniques to these lan-
guages is the data sparsity problem that arises
from their rich internal morphology, where the
substructure is inherently non-concatenative
and morphemes are interdigitated in word for-
mation. While previous automated methods
have relied on human-curated rules or multi-
class classification, they have not fully lever-
aged the various combinations of regular, se-
quential concatenative morphology within the
words and the internal interleaving within tem-
platic stems of roots and patterns. To address
this, we propose a constrained sequence-to-
sequence root extraction method. Experimen-
tal results show our constrained model outper-
forms a variety of methods at root extraction.
Furthermore, by enriching word embeddings
with resulting decompositions, we show im-
proved results on word analogy, word similar-
ity, and language modeling tasks.

1 Introduction

The Semitic languages are a language family com-
monly spoken throughout North Africa, the Horn
of Africa, the Arabian peninsula, and the regions
between. With approximately 500 million speak-
ers, the proliferation of large online text collec-
tions of such news articles, social media, digitized
literature, and web blogs has created a wealth of
data offering challenges and opportunities for se-
mantic understanding of Semitic texts. In these
languages, a majority of words are derived from
a small number of mostly triliteral consonantal
roots, with some quadriliteral roots and a trace
number of biliteral and quintliteral roots. It is es-
timated that two of the most prominent Semitic

∗*Equal contribution

languages, Arabic and Hebrew, possess approxi-
mately 10,000 and 3,000 roots, respectively (Dar-
wish, 2002; Daya et al., 2008). As such, root
identification of a given Semitic word is often an
important task in morphological analysis and the
first step to morphological decomposition. Mor-
phological analysis of Semitic languages poses a
unique challenge to traditional NLP techniques
due to the non-contiguous morphology inherent
in these languages. This morphology is best de-
scribed as the application of a pattern resulting
in the interdigitation of morphemes within a sin-
gle root to form derivative words (Habash, 2010).
This fusional morphology allows for many sur-
face form words derived from the same single
root, but with different, yet abstractly-related se-
mantic meanings depending on constituent mor-
phemes. Because many surface words can be
formed through this root and pattern word forma-
tion process, and the root’s characters may not
necessarily be contiguously situated within each
resultant surface word, morpheme boundaries are
often difficult to identify.

Unlike other fusional languages, the Semitic
languages are unique in that the word forma-
tion process follows a highly-structured process
of adding vowels and consonants to roots. This
word formation process consists of a fixed num-
ber of slots for different morphemes, which are
fixed in their position and order relative to each
other. As such, these languages contain significant
sequential (albeit not necessarily contiguous) sub-
structure. In this work, we propose to leverage this
sequential substructure to improve the root extrac-
tion process and morphological decomposition.

Morphological analysis is essential in working
with Semitic languages as well as other highly-
inflectional languages due to data sparsity. For
instance, previous research has shown that many
text corpora demonstrate long-tail distributions in



89

1 2 3 4 >4
Words by Frequency

0%

10%

20%

30%

40%

50%

60%

Pe
rc

en
ta

ge
 o

f W
or

ds

Figure 1: Word distribution in Arabic Wikipedia corpus.

relation to word frequency. This long-tail often re-
sults in corpora with many infrequent words, with
40% − 60% of words appearing just once (Kor-
nai, 2007). We can verify this for Arabic in Fig-
ure 1, where, on a Wikipedia monolingual Arabic
corpus (described in Section 5.1), approximately
80% of words occur fewer than five times and 60%
occur once. To process such long-tailed corpora,
it is necessary to exploit finer-granularity, highly-
shared substructures between words that can be
used to infer semantic meaning. In Table 1, we
look at a selection of Arabic words sharing the
common root –H.

�
H ¼ – (transliteration K-T-B),

which means to “write”. These words are formed
by appending different prefixes, suffixes, and other
templatic interleavings of morphemes within the
root. Despite the many surface words, the deriva-
tions share a semantic relationship based on the
root, as well as other concatenative and interdigi-
tated templatic morphemes. Additionally, as seen
in the example, the root word’s characters are
not necessarily contiguous within the word; this
is due to the non-concatenative templatic process
whereby morphemes are inserted between char-
acters of the root as part of the word formation
process. Finally, not all characters in the root
are necessarily found in the final surface-form of
the word as some root characters can be dropped.
Traditional concatenative morphological analyz-
ers struggle to identify and extract roots precisely
because root word characters are not necessarily
contiguous or even present in the surface word.

To address these challenges, we present a su-
pervised root extraction algorithm that, given a
word, directly extracts the root with high accu-
racy. Given this root and the original word, we
demonstrate how the templatic pattern-based word
formation process that transforms the root to the
original word can be used for further morpholog-
ical decomposition. Our root extraction method
differentiates itself from other methods in three

Word Translit. Meaning Pref. Suff. R-1 R-2
�

I�.
�
J» KTBT she wrote N/A T N/A N/A

I.
�
KA¿ KĀTB writer N/A N/A Ā N/A

H. A
�
J» KTĀB book N/A N/A N/A Ā

H. A
�
JºË@ ALKTĀB the book AL N/A N/A Ā

I.
�
JºÓ MKTB desk M N/A N/A N/A

�
éJ.

�
JºÓ MKTBA library M A N/A N/A

Table 1: Common Roots

ways: (1) It is fully data-driven, without any re-
liance on human-curated patterns; (2) it directly
extracts word roots without stripping dictionary
affixes, which can lead to incorrect roots when
false affixes are stripped; and (3) by applying a
novel sequence-to-sequence (seq2seq) model with
a constrained decoding mechanism that leverages
shared sequential semantics in the label (root) and
input (word) space, it outperforms standard multi-
class classification algorithms and achieves better
generalization performance.

We demonstrate that our method outperforms
unsupervised rule-based root extraction meth-
ods (Taghva et al., 2005; Khoja and Garside, 1999;
Zerrouki, 2010) and our seq2seq classifier outper-
forms general multiclass classifiers (Kim, 2014;
Chung et al., 2014). As a testament to the utility
of root extraction, we demonstrate how one can
leverage the root information alongside a simple
slot-based morphological decomposition to im-
prove upon word embedding representations as
evaluated through word similarity, word analogy,
and language modeling tasks.

2 Related Work
With the growth of the internet and the digitiza-
tion of Arabic and other Semitic corpora, prior
work has extensively studied root extractors with
the goal of improving document retrieval (Larkey
et al., 2002; Aljlayl and Frieder, 2002).

Early approaches to the problem of Arabic
root extraction were predominantly unsupervised
methods. Some researchers developed stemmers
that remove some prefixes and suffixes while
ignoring the templatic, interleaved morphemes
within stems. A few of these methods relied on
pattern matching and prefix/suffix pruning in or-
der to extract roots (Taghva et al., 2005; Khoja
and Garside, 1999). These methods may fail to
identify the roots in many nouns and, like all pre-
fix and suffix stripping algorithms, fail to cor-
rectly extract non-contiguous roots. Similar meth-
ods operate by removing not only prefixes and
suffixes, but also “extra letters” until the tricon-
sonantal roots remain (Momani and Faraj, 2007).



90

This method, however, may incorrectly remove
many letters that are part of the root. Another
of these models achieves high accuracy by in-
corporating sentence-level context and inferred
syntactic categories into a parametric Bayesian
model (Lee et al., 2011). Our model forgoes
these context features as it attempts to identify
the root solely on the word itself. Additionally,
this method cannot model non-contiguous roots,
of which Semitic languages have many. Other un-
supervised methods utilize dictionaries to select
the characters from within words (Darwish, 2002;
Boudlal et al., 2011; Alhanini and Ab Aziz, 2011).
Another line of research leverages the templatic
nature for human-constructed rule-based con-
straints (Elghamry, 2005; Rodrigues and Cavar,
2007; Choueka, 1990). Finally, methods have
been proposed that utilize both a root dictionary
and rule-based templatic constraints (Yaseen and
Hmeidi, 2014).

Supervised methods have been developed for
identifying Hebrew roots by combining various
multiclass classification models with Hebrew-
specific linguistic constraints (Daya et al., 2004).
This same technique was extended to extract both
Arabic and Hebrew roots (Daya et al., 2008).
While these supervised methods effectively ad-
dress the non-contiguous nature of Semitic roots,
they fail to leverage the sequential structure of the
root label space. We show that such methods that
forgo the sequential structure in the label space un-
derperform on words with rare roots. Additionally,
these methods are only applied to triconsonantal
leaving out many biconsonantal and quadriliteral
roots.

Sequence-to-sequence models have been uti-
lized for learning to map sequences to other se-
quences and predominantly applied to machine
translation (Sutskever et al., 2014), with later
variations of these models enhanced with atten-
tion mechanisms (Luong et al., 2015). While
LSTM variants have been dominant, previous
work has shown that GRU-based models perform
comparably to LSTM-based models with supe-
rior train time (Chung et al., 2014). More re-
cent work has investigated character-level lan-
guage models in order to handle the many out-
of-vocabulary (OOV) words in morphologically
rich languages (Gerz et al., 2018). Such meth-
ods have shown large improvements in language
modeling across many morphologically rich lan-
guages. While such methods share the same

character-level input space as does our own
method, they ignore the sequential nature in the
target class. Closely related to our model, con-
strained sequence-to-sequence models have been
used for sentence simplification forcing the model
to select simple words (Zhang et al., 2017). Simi-
lar approaches have been used for constrained im-
age captioning (Anderson et al., 2017). Our model
differs in that it constrains not only on specific vo-
cabulary, but on specific sequences.

3 Root Extraction Framework

We introduce a framework for extracting the root
from templatic words within the Semitic family.
The proposed framework leverages the shared se-
quential semantics in both the word and root space
to more accurately extract root morphemes.

3.1 Preliminaries
The input is a set of word-root pairs W , R, consist-
ing of |W | words and |R| roots where |W | = |R|
and W = w1, . . . , w|W | and R = r1, . . . , r|R|.
In addition, the jth word wj is a sequence of
|wj | characters: cwj ,i, i = 1, . . . , |wj |. For con-
venience we index all the unique characters that
compose the input vocabulary with C characters
and cw,i = x, where x ∈ {1, . . . , C} means that
the ith character in wth word is the xth character
in the character vocabulary. Similarly the kth root,
rk corresponding to the jth word wj is a sequence
of |rk| characters: crk,i, i = 1, . . . , |rk|. Given the
input, the goal is to learn a function, F : W → R
that maps an input word onto its correct Semitic
root.

3.2 Constrained Seq2Seq Root Extraction
Our main innovation and contribution is a unique
way of extracting roots by utilizing seq2seq mod-
els for multiclass classification. While many
methods traditionally approach root extraction
through unsupervised application of templates or
traditional supervised multiclass classification al-
gorithms, we posit that the shared semantics be-
tween words and roots merits a different approach.
As such, we apply a hybrid approach between
multiclass classification and seq2seq models for
root extraction. By constraining the outputs of the
seq2seq models to the dictionary table of roots, the
algorithm becomes a sequential multiclass classi-
fication model that implicitly leverages shared se-
quential substructure in both the input space and
in the label space.



91

encoder	representation	 (e)

GRU GRU GRU GRU

GRU GRU GRU GRU

concat concat concat concat

K T A B

average

(a) Encoder Network

GRU

en
co
de
r	r
ep
re
se
nt
at
io
n	
(e
)

<sow>

GRU

K

GRU

T

softmax softmax softmax

K T B

GRU

B

softmax

<eor>

(b) Decoder Network
Figure 2: Sequence-to-sequence root extraction.

3.2.1 Encoder Network
As seen in Figure 2a, we begin with an encoder
network that takes a word as input. Each of the
input word’s characters (from a total of C possi-
ble characters) is associated with a vector c ∈ Rd.
Using word, KTĀB from Table 1, the input be-
comes vector [c0, c1, c2, c3] ∈ Rd×4. We then run
this sequence of embedding vectors through both
directions of a bi-directional GRU (BiGRU) and
concatenate the resulting hidden vectors from each
pass. Finally, we average the concatenated hidden
vectors of the BiGRU across all time-steps. This
serves as the encoder representation of the input
word, which we denote as e. The encoding is then
fed into a decoder network that attempts to gener-
ate the most likely root for the word.

3.2.2 Decoder Network
In Figure 2b, the decoder takes the encoder repre-
sentation e that captures the input word and pre-
dicts a root word. This is done by feeding e and
a special “start-of-word” character 〈sow〉 as the
input. A GRU computes the next hidden state
h0 ∈ Rh. A scoring function is then applied, re-
sulting in an output the size of the character vo-
cabulary, C. This function: g : Rh → RC , is
then softmaxed to obtain a valid probability distri-
bution over characters for each hidden state. The
decoding stops when the predicted root is termi-
nated with a special “end-of-root” token 〈eor〉.

3.2.3 Constrained Beam Search
Traditional decoders select the best character at
each step to feed into the next time step of the
RNN. However, this decoding maps the input se-
quence into an infinite space of possible output
sequences and, as such, may result in an invalid
root that is not part of the dictionary set of roots.
As such, we propose an alternative output that re-
stricts the decoder, forcing the decoded sequence
to map onto a root within the valid roots set.

We realize this constraint by modifying the de-
coding scheme itself. During decoding, a greedy
approach is often used where the single best char-
acter output is selected and propagated to later
time steps. This greedy approach may not only
lead to suboptimal output sequences, but also re-
sult in invalid sequences (not corresponding to any
class). This can be circumvented using a beam
search decoding scheme. When decoding to ob-
tain the predicted roots, instead of utilizing the
character with the highest probability at each step,
the top k characters are considered at each step.
As such, at each new time-step, for each of the
k hypotheses, there are C possible choices. The
top k are then once again selected and this pro-
cess is applied to each time step. Once all candi-
date roots reach their special 〈eor〉 token, the most
probable root is selected. To tailor beam search to

B BB R

T DB

K

R

(a) Constraint Trie

Candidate Roots
K - T - A
K - T - B

K - T - A - B
K  - T  - R

K - T - B - B

(b) Candidate root pool.
Figure 3: Constrained beam search.

root extraction from a dictionary of roots, we seek
to modify beam search by enforcing the linguistic
sequential constraints present in the label root set.
This leverages our classification tasks’s relatively
small and enumerable root label set, contrasted
with an unbounded sequence as found in machine
translation models. Simultaneously, by using a
decoder, the model exploits the task’s sequential
structure by generating the target label character-
by-character. We utilize the target roots as guid-
ance for the decoding process in order to imple-
ment this sequential prediction. We demonstrate
on a toy example in Figure 3a, where by storing
all the possible target roots in a trie data structure
(a.k.a a prefix tree), invalid roots can be pruned



92

during the decoding process. For example, as seen
in Figure 3b, during a typical beam-search pro-
cess, the top k candidate characters are selected.
By cross-referencing the current prefix of the root
with the trie storing all valid roots, many invalid
roots can be pruned. As such, we can enforce that
the top-k selections all correspond to valid prefixes
present in the target roots. This strictly improves
overall extraction accuracy over traditional beam
search.

4 Templatic Word Embeddings
As the Semitic languages are templatic, there ex-
ist fixed slots that can contain morphemes. Given
the correct root for a word identified as described
in Section 3, we introduce a simple slot-based
template. We indicate how to identify these slots
within a word utilizing the Semitic root. Finally,
we demonstrate how the morphemes within these
slots, along with the root, can be utilized to enrich
distributed word representations.

4.1 Morphological Decomposition
We posit that each word possesses a fixed number
of slots allocated to certain morphemes, whereby
the slots are fixed in their position and order rel-
ative to each other. As demonstrated in Table 1,
in addition to the root word, we propose a sim-
plified template that consists of four slots – two
concatenative (prefixes and suffixes) and two non-
concatenative (morphemes interdigitated within
the stem). While we demonstrate the simplic-
ity of identifying these within Arabic, this same
template-based structure can, without loss of gen-
erality, be trivially created for other members of
the Semitic family.

Example 1 (Stem, Prefix, and Suffix Identification) For
the root K-T-B, we can identify the consecutive characters
that encompass the full root.

AL + [KTĀB] + EEN
	áK
 + [H. A

�
J»] + È@

The characters grouped together by [] form the stem, the
smallest consecutive set of characters containing the full root.
Any characters not falling within the stem are, respectively,
the prefixes and suffixes.

As seen in Example 1, given the root, the stem
can be identified as the shortest contiguous sub-
string containing the root in correct order. Once
the stem is identified, the two concatenative slots
containing prefix and suffix are trivially identi-
fied by selecting the remaining affixes after remov-
ing the stem. The non-concatenative slots can be
found interdigitated within the word stem whose
boundary is demarcated by the root. Given the

stem (as shown in square brackets in Example 1)
and the root, these interdigitated slots can be iden-
tified as follows:

Example 2 (Interdigitated Slots) Given a stem containing
the core root K-T-B, the candidate slots are as follows.

In stem, KĀTB, Ā occurs in the first slot.
In stem, KTĀB, Ā occurs in the second slot.

If a contiguous morpheme occurs after the first character
in the root by before middle characters, it is a slot-1 addition.
If after the middle character(s) of the root, it is slot-2.

Example 2 shows the identification of interdig-
itated slots within the stem. Once again, it is evi-
dent that correct extraction of the root is essential
to correct identification of the slot positions within
the word. In the next subsection we demonstrate
how these extractions can be systematically lever-
aged to enrich distributed word representations in
these templatic languages.

4.2 Morpheme-Enriched Embeddings
To demonstrate the utility of templatic subword
extractions, we demonstrate how enriching word
embeddings with these morphemes can improve
word representations by providing parameter-
sharing between words sharing common substruc-
ture. With this motivation, we propose Tem-
platicVec, an intuitive extension to FastText (Pi-
otr Bojanowski and Mikolov, 2017), that uti-
lizes the templatic decomposition of semantically-
meaningful roots, affixes, and interdigitated mor-
phemes for representation enrichment. By using
these structures as embedding base units by and
combining them to construct a word’s distributed
vector representation, the resultant word embed-
dings are robust to infrequent word-induced data-
sparsity and can be constructed on many out-of-
vocabulary (OOV) words. We begin with a brief
review of FastText, and then demonstrate how one
can naturally integrate roots as well as concate-
native and templatic morphemes in place of Fast-
Text’s standard naive subwords. FastText utilizes
the skip-gram objective with negative sampling
yielding the following objective (for simplicity,
`(x) = log(1 + exp(−x))):
|W |∑
x=1

[ ∑
c∈Cx

`(s(wx, wc)) +
∑

t∈Nx,c

`(−s(wx, t))
]

In the above equation, wx is the xth word in the
corpus, Cx denotes the set of context words within
a predefined window of word wx, and Nx,c de-
notes the set of negative examples sampled from
outside the context window.



93

The scoring function is then adapted to incorpo-
rate subword information as follows:

s(wx, wc) =
∑

m∈wx
zᵀmvc

In the above equation, each zm denotes a sub-
word embedding vector, so that the scoring func-
tion equates to the inner product of the summa-
tion each over subword embedding vector with
the context word vector. While FastText incorpo-
rates all contiguous substrings of lengths three to
seven as morphemes in the scoring function, be-
cause Semitic roots are not necessarily contigu-
ous, two words sharing the same root may not
share the same subwords using FastText. Because
this important semantic morpheme is not shared
among words, we posit that FastText’s indiscrimi-
nate enumeration of contiguous subwords does not
capture the essential semantic substructure. We
claim that directly incorporating the root embed-
ding and each slot’s morpheme embeddings that
have been extracted for each word and summing
over these embeddings results in higher quality
distributed representations. As such, similar to the
approach in (El-Kishky et al., 2018), we modify
the scoring function to incorporate the extracted
root and slot-based templatic information:

s(wx, wc) = (zr + zp + zs + zr1 + zr2)
ᵀvc

This modification yields a scoring function that is
the inner product of the summation over the root
word embedding (zr), prefix embedding (zp), suf-
fix embedding (zs), as well as the two possible in-
root interdigitated morphemes (zr1 and zr2).

5 Experiments
We introduce the datasets and methods for com-
parison used. We then describe evaluations for
root extraction and embedding quality.

5.1 Datasets and comparison methods
We use the following datasets and ground-truth la-
bels for evaluation purposes:
• Arabic Word & Root Pairs: 140K words

along associated with 11K roots from dictio-
nary (al Zabidi and Murthada, 1886).

• Hebrew Word & Root Pairs. 11.5K
words associated with approximately 500
roots from Wiktionary1 and human curation.

• Arabic Wikipedia Corpora. Wikipedia cor-
pus with 274K articles and 62.5M tokens and
1.26M unique words.

1wiktionary.org

For baseline methods to compare against our
proposed constrained seq2seq (Constrain-S2S),
we evaluate against three standard multiclass clas-
sification models: (1) a standard convolutional
neural network, CNN-Class, (Kim, 2014), a GRU
model, GRU-Class, and a bi-directional GRU
model, BiGRU-Class. In addition, we com-
pare against two unconstrained seq2seq models,
encoder-decoder models using GRUs, GRU-S2S
and bi-directional GRUs, BiGRU-S2S. Finally, for
Arabic, we evaluate against three unsupervised
Arabic root-extraction algorithms from the litera-
ture: Tashaphyne, ISRI, and Khoja. To evaluate on
the quality of the resultant morphological decom-
position, we compare against three variants of em-
beddings: (1) SkipGram (2) FastText (3) RootVec
(Embedding enriched with solely the root) .

5.2 Root Extraction Accuracy
To evaluate the effectiveness of our proposed
seq2seq extraction of roots, we perform five-
fold cross-validation evaluation of our method
compared to a variety of supervised and rule-
based root-extraction methods. During each cross-
validation, each supervised method is trained on
four-fifth of the dictionary mappings of word to
root pairs, and evaluated on a held-out 20%.

5.2.1 General Root Extraction
We first compare the performance of each super-
vised extraction method on extracting roots irre-
spective of root frequency. In Table 2, we re-

Method Arabic HebrewACC. SE ACC. SE
CNN-Class .6753 ±.0009 .9622 ±.0019
GRU-Class .7539 ±.0023 .9591 ±.0033

BiGRU-Class .7548 ±.0015 .9629 ±.0009
GRU-S2S .7596 ±.0017 .9692 ±.0013

BiGRU-S2S .7854 ±.0010 .9788 ±.0016
Constrain-S2S .8324 ±.0011 .9879 ±.0008

Tashaphyne .2778 0 - -
ISRI .4508 0 - -

Khoja .4434 0 - -
Table 2: Root Extraction Accuracy.

port the performance of each extractor at suc-
cessfully identifying the ground-truth root in each
held-out word in a five-fold cross-validation eval-
uation. It is apparent that the unsupervised meth-
ods under-perform at extracting the ground-truth
root as compared to the supervised methods. This
is likely due to errors from human-curated pat-
terns which possess many exceptions as well as
many Semitic roots being non-contiguously sit-
uated with the word due to interdigitated mor-



94

phemes. Additionally, both the CNN-based and
four RNN-based multiclass classification methods
severely under-perform compared to our proposed
constrained seq2seq model. This verifies our in-
tuition that leveraging the shared semantic space
between the words and the target roots is essential
in extraction.

5.2.2 Rare Root Extraction
We claimed earlier that by decomposing root clas-
sification into seq2seq classification, sequential
patterns within the roots can be leveraged for root
extraction. This can be useful for identifying the
correct root, even when the root is infrequent or
even absent from the training data. To support
this claim, we report the performance of each su-
pervised extractor at successfully identifying the
ground-truth of infrequent roots (appear three or
fewer times in training) and a zero-shot case where
the root is not present in the training data. As our
Hebrew dataset consists of frequent roots, and per-
formance is near perfect, we report results for the
Arabic dataset.

Method Infreq. Zero-ShotACC. SE ACC. SE
CNN-Class .4823 ±.0096 - -
GRU-Class .5697 ±.0103 - -

BiGRU-Class .5706 ±.0091 - -
GRU-S2S .6074 ±.0166 .5389 ±.0188

BiGRU-S2S .6231 ±.0191 .5532 ±.0141
Constrain-S2S .6929 ±.0164 .6292 ±.0160
Table 3: Arabic Rare Root Extraction Accuracy

As seen in Table 3, the seq2seq methods greatly
outperform all multiclass methods with Constrain-
S2S outperforming all methods on the infrequent
roots. This effect is amplified in the zero-shot
case, with only the seq2seq models handling un-
seen roots. This demonstrates the utility in jointly
learning the sequential structure in semantically-
shared label (root) and word space.

5.3 Word Analogy Evaluation
Given our comprehensive dataset of Arabic roots
and human-curated evaluation set of Arabic word
embeddings, we show the effectiveness of enrich-
ing Arabic word embeddings with their morpho-
logical decompositions via a word analogy task.
The goal of said task is to identify the best value
for D in analogies of the form “A is to B as C is
to D”. After training each embedding model on
the Arabic Wikipedia dataset, we use an analogy
dataset (Elrazzaz et al., 2017) curated for method-
ological evaluation of Arabic word embeddings.

We further differentiate the analogies into two cat-
egories: (1) morphemic analogies (e.g. plurals,
tense or gender) where a derivational or inflec-
tional morpheme is inserted, removed, or replaced
while the root remains unchanged, and (2) seman-
tic analogies where the root itself changes between
the analogous pairs (e.g. bird is to fly as fish is to
swim).

Embedding Model Semantic Morphemic
SkipGram 19.1 11.4
FastText 13.8 16.8

ISRI-RootVec 15.4 11.2
BiGRU-RootVec 14.2 11.9

S2S-RootVec 18.0 11.9
CS2S-RootVec 18.9 12.2

ISRI-TemplaticVec 15.3 14.5
Class-TemplaticVec 16.3 16.9
S2S-TemplaticVec 17.6 20.2

CS2S-TemplaticVec 18.8 22.9
Table 4: Word Analogies

As seen in Table 4, embeddings that utilize
morphemes or subword-level features perform sig-
nificantly better at morphemic analogies than do
SkipGram word embeddings. This does not ex-
tend to semantic analogies where all methods ap-
pear to degrade with the use of morpheme and
subword-level enrichment. This is not surprising
since, under the vector algebra that is used to com-
pute the word analogies, the summation of the
morphemes used to enrich the embeddings cap-
tures morphemic relationships but not necessarily
semantic ones. This can be seen in the perfor-
mance gap between the morpheme-enriched em-
beddings and SkipGram. Unlike the other meth-
ods, Templatic embeddings based on constrained
roots maintains comparable performance to Skip-
Gram on the semantic analogies while demon-
strating superior performance on the morphemic
analogies.

5.4 Word Similarity
The next embedding evaluation we consider is a
word similarity task. The ground truth data con-
sists of pairs of words and a human-annotated sim-
ilarity score averaged across all human evaluations
from a translation of the WS-353 dataset (Freitas
et al., 2016). The scores are computed via the co-
sine similarity between the vector representation
of each word in a pair. Their results are quanti-
fied through Spearman and Pearson rank correla-
tion coefficients.

As seen in Table 5, enriching the embed-
ding vectors with the template-based extracted



95

Embedding Model Pearson Spearman
SkipGram 0.496 0.520
FastText 0.459 0.468

ISRI-RootVec 0.491 0.518
BiGRU-RootVec 0.492 0.510

S2S-RootVec 0.508 0.516
CS2S-RootVec 0.507 0.514

ISRI-TemplaticVec 0.482 0.501
Class-TemplaticVec 0.474 0.491
S2S-TemplaticVec 0.514 0.529

CS2S-TemplaticVec 0.512 0.533
Table 5: Word Similarity

morphemes substantially improves embeddings in
capturing word similarity. This is in contrast with
lower correlation coefficients from FastText em-
bedding vectors, likely due to the indiscriminate
generation of subwords that may degrade the over-
all embedding. On this task, template-based de-
composition using unconstrained and constrained
root extraction appears to perform similarly, yet
both greatly outperform the other baselines.

5.5 Language Modeling Perplexity

Finally, we evaluate the effect of utilizing the
extracted root and templatic decomposition on a
downstream language modeling task. On each lan-
guage model, the model quality is evaluated by
computing the perplexity on a held-out portion of
the corpus. The model used for language mod-
eling is an LSTM with three hidden layers, 600
hidden units per layer, regularized with 0.2 proba-
bility drop-out, unrolled for 35 steps with a batch
of 20. Parameters are learned using Adagrad with
a gradient clipping of 1. We evaluate on two sub-
sets of the Wikipedia dataset: (1) LM-1, a small
subset (2) LM-2, a larger subset. LM-1 consists
of 3.3M tokens and a vocabulary of 260K words
while LM-2 consists of 7.6M tokens and a vo-
cabulary of 400K unique words. Each language
model instance is trained for 5 epochs on the train-
ing data. Evaluation of perplexity was computed
for each model on the independent test set con-
sisting of 900K tokens where 62K tokens were
OOV in LM-1 and 27K in LM-2. Evaluation is
performed after selecting the best performing iter-
ation of the model on a validation set. While the
morpheme-enriched method can generate embed-
ding vectors for many OOV tokens, for SkipGram
and instances when they cannot, an unknown to-
ken with fixed embedding is used.

The results are summarized in Table 6. Al-

Embedding Model Perplexity
LM-One LM-Two

SkipGram 1757 1075
FastText 1720 1069

ISRI-RootVec 1729 1072
BiGRU-RootVec 1731 1071

S2S-RootVec 1728 1071
CS2S-RootVec 1726 1071

ISRI-TemplaticVec 1728 1071
Class-TemplaticVec 1724 1070
S2S-TemplaticVec 1718 1065

CS2S-TemplaticVec 1716 1065
Table 6: Language Modeling

though perplexity is high, this is common for
morphologically-rich languages such as Arabic as
shown in (Gerz et al., 2018). It appears our con-
strained model’s extracted roots yield a benefit
over other baseline roots, yet utilizing the full de-
composition outperforms all other methods, yield-
ing lower held-out perplexity. The results also
verify the intuition that morphemic decomposi-
tion is necessary to handle data-sparsity and OOV
words when little training data is present, whereby
perplexity is greatly reduced through the use of
morpheme-based embeddings.

6 Conclusions

We proposed leveraging the shared semantic space
between Semitic words and their roots for more
effective root extraction. This was accomplished
through a novel constrained sequence-to-sequence
classifier. Experiments show a performance boost
over unsupervised and supervised extraction mod-
els. We introduce a simple template-based mor-
phological decomposition, and by enriching word
embeddings with this decomposition, we show im-
proved results on word analogy, word similarity,
and language modeling tasks.

References
Yasir Alhanini and Mohd Juzaiddin Ab Aziz. 2011.

The enhancement of arabic stemming by using light
stemming and dictionary-based stemming. JSEA.

Mohammed Aljlayl and Ophir Frieder. 2002. On arabic
search: improving the retrieval effectiveness via a
light stemming approach. In CIKM.

Peter Anderson, Basura Fernando, Mark Johnson, and
Stephen Gould. 2017. Guided open vocabulary im-
age captioning with constrained beam search. In
EMNLP.



96

Abderrahim Boudlal, Mohamed Ould Abdallahi Ould
Bebah, Abdelhak Lakhouaja, Azzeddine Mazroui,
and Abdelouafi Meziane. 2011. A markovian ap-
proach for arabic root extraction. Int. Arab J. Inf.
Technol.

Yaacov Choueka. 1990. Mlim-a system for full, exact,
on-line grammatical analysis of modern hebrew. In
ICCE.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555.

Kareem Darwish. 2002. Building a shallow arabic
morphological analyzer in one day. In ACL-02
workshop on Computational approaches to semitic
languages.

Ezra Daya, Dan Roth, and Shuly Wintner. 2004. Learn-
ing hebrew roots: Machine learning with linguistic
constraints. In EMNLP.

Ezra Daya, Dan Roth, and Shuly Wintner. 2008. Iden-
tifying semitic roots: Machine learning with linguis-
tic constraints. Computational Linguistics.

Ahmed El-Kishky, Frank Xu, Aston Zhang, Stephen
Macke, and Jiawei Han. 2018. Entropy-based sub-
word mining with an application to word embed-
dings. In SCLeM 2018.

Khaled Elghamry. 2005. A constraint-based algorithm
for the identification of arabic roots. In Proceed-
ings of the Midwest Computational Linguistics Col-
loquium. Indiana University. Bloomington, IN.

Mohammed Elrazzaz, Shady Elbassuoni, Khaled Sha-
ban, and Chadi Helwe. 2017. Methodical evaluation
of arabic word embeddings. In ACL.

André Freitas, Siamak Barzegar, Juliano Efson Sales,
Siegfried Handschuh, and Brian Davis. 2016. Se-
mantic Relatedness for All (Languages): A Compar-
ative Analysis of Multilingual Semantic Relatedness
Using Machine Translation.

Daniela Gerz, Ivan Vulić, Edoardo Ponti, Jason Narad-
owsky, Roi Reichart, and Anna Korhonen. 2018.
Language modeling for morphologically rich lan-
guages: Character-aware modeling for word-level
prediction. TACL.

Nizar Y Habash. 2010. Introduction to arabic natural
language processing. Synthesis Lectures on Human
Language Technologies.

Shereen Khoja and Roger Garside. 1999. Stemming
arabic text. Lancaster, UK, Computing Department,
Lancaster University.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882.

Andras Kornai. 2007. Mathematical linguistics.
Springer Science & Business Media.

Leah S Larkey, Lisa Ballesteros, and Margaret E Con-
nell. 2002. Improving stemming for arabic infor-
mation retrieval: light stemming and co-occurrence
analysis. In ACM SIGIR.

Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2011. Modeling syntactic context improves mor-
phological segmentation. In CoNLL.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025.

Mohanned Momani and Jamil Faraj. 2007. A novel
algorithm to extract tri-literal arabic roots. In Com-
puter Systems and Applications, 2007. AICCSA’07.

Armand Joulin Piotr Bojanowski, Edouard Grave and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. TACL.

Paul Rodrigues and Damir Cavar. 2007. Learn-
ing arabic morphology using statistical constraint-
satisfaction models. Amsterdam studies in the the-
ory and history of linguistic science. Series 4.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In NIPS.

Kazem Taghva, Rania Elkhoury, and Jeffrey Coombs.
2005. Arabic stemming without a root dictionary.
In Information Technology: Coding and Computing.

Qussai Yaseen and Ismail Hmeidi. 2014. Extracting
the roots of arabic words without removing affixes.
Journal of Information Science.

Muhammad Murtada al Zabidi and Sayyid Muhammad
Murthada. 1886. Taj al-‘arus min jawahir al-qamus.
Dar al-Hidayah.

Taha Zerrouki. 2010. Tashaphyne, arabic light stem-
mer/segment.

Yaoyuan Zhang, Zhenxu Ye, Yansong Feng, Dongyan
Zhao, and Rui Yan. 2017. A constrained sequence-
to-sequence neural model for sentence simplifica-
tion. arXiv preprint arXiv:1704.02312.


