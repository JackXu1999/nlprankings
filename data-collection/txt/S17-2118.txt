



















































SSNMLRG1 at SemEval-2017 Task 4: Sentiment Analysis in Twitter Using Multi-Kernel Gaussian Process Classifier


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 709–712,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

SSN MLRG1 at SemEval-2017 Task 4: Sentiment Analysis in Twitter
Using Multi-Kernel Gaussian Process Classifier

Angel Deborah S, S Milton Rajendram, T T Mirnalinee
SSN College of Engineering
Kalavakkam 603 110, India

angeldeboarahs@ssn.edu.in

Abstract

The SSN MLRG1 team for Semeval-2017
task 4 has applied Gaussian Process, with
bag of words feature vectors and fixed rule
multi-kernel learning, for sentiment anal-
ysis of tweets. Since tweets on the same
topic, made at different times, may exhibit
different emotions, their properties such as
smoothness and periodicity also vary with
time. Our experiments show that, com-
pared to single kernel, multiple kernels
are effective in learning the simultaneous
presence of multiple properties.

1 Introduction

Twitter is a huge microblogging service with more
than 500 million tweets per day from different lo-
cations of the world and in different languages
(Nabil et al., 2016). The sentiment analysis in
Twitter has been applied in various domains such
as commerce (Jansen et al., 2009), disaster man-
agement (Verma et al., 2011) and health (Chew
and Eysenbach, 2010). The task is challenging
because of the informal writing style, the seman-
tic diversity of content as well as the “unconven-
tional” grammar. These challenges in building
a classification model can be handled by using
proper approaches to feature generation and ma-
chine learning.

The heart of every Gaussian process model
is a covariance kernel. Multi Kernel Learning
(MKL)—using multiple kernels instead of a sin-
gle one—can be useful in two ways:
• Different kernels correspond to different no-

tions of similarity, and instead of trying to
find which works best, a learning method
does the picking for us, or may use a combi-
nation of them. Using a specific kernel may
be a source of bias which is avoided by allow-

ing the learner to choose from among a set of
kernels.
• Different kernels may use inputs coming

from different representations, possibly from
different sources or modalities.

(Gonen and Alpaydn, 2011) and (Wilson and
Adams, 2013) explain how multiple kernels defi-
nitely give a powerful performance. (Gonen and
Alpaydn, 2011) also describe in detail various
methodologies to combine kernels. (Wilson and
Adams, 2013) introduces simple closed form ker-
nels that can be used with Gaussian Processes to
discover patterns and enable extrapolation. The
kernels support a broad class of stationary co-
variances, but Gaussian Process inference remains
simple and analytic.

We studied the possibility of using multiple ker-
nels to explain the relation between the input data
and the labels. While there is a body of work on
using Multi Kernel Learning (MKL) on numerical
data and images, yet applying MKL on text is still
an exploration.

2 Gaussian Process

Gaussian Process is a non-parametric Bayesian
modelling in supervised setting. Gaussian pro-
cess is a collection of random variables, any fi-
nite number of which have a joint Gaussian dis-
tribution (Rasmussen and Williams, 2006). Using
a Gaussian process, we can define a distribution
over functions f(x),

f(x) ∼ GP (m(x), k(x,x′)) (1)
where m(x) is the mean function, usually defined
to be zero, and k(x,x′) is the covariance function
(or kernel function) that defines the prior prop-
erties of the functions considered for inference.
Gaussian Process has the following main advan-
tages (Cohn and Specia, 2013; Cohn et al., 2014).

709



• The kernel hyper-parameters can be learned
via evidence maximization.
• GP provides full probabilistic prediction, and

an estimate of uncertainty in the prediction.
• Unlike SVMs which need unbiased version

of dataset for probabilistic prediction, yet
does not take into account the uncertainty of
f(x), GP does not suffer from this problem.
• GP can be easily extended and incorporated

into a hierarchical Bayesian model.
• GP works really well when combined with

kernel models.
• GP works well for small datasets too.

2.1 Gaussian Process Classification

In Gaussian Process Classification (GPC), we
place a GP prior over a latent function f(x) and
then “squash” this prior through the logistic func-
tion to obtain a prior on π(x) =∆ p(y = +1|x) =
σ(f(x)). Note that π is a deterministic function of
f , and since f is stochastic, so is π.

Inference is divided into two steps: first, com-
puting the distribution of the latent variable corre-
sponding to a test case

p(f∗|X,y,x∗) =
∫
p(f∗|X,x∗, f)p(f |X,y)df

(2)
where p(f |X,y) = p(y|f)p(f |X)/p(y|X) is the
posterior over the latent variables, and subse-
quently using this distribution over the latent to
produce a probabilistic prediction

π∗(x) =∆ (y∗ = +1|X,y,x∗) (3)
=
∫
σ(f∗)p(f∗|X,y,x∗)df∗ (4)

In classification, the non-Gaussian likelihood in
Equation 2 makes the integral analytically in-
tractable. Similarly, Equation 4 can also be ana-
lytically intractable for certain sigmoid functions.
Therefore, we need an analytical approximation of
integrals. We can approximate the non-Gaussian
joint posterior with a Gaussian one, using Expec-
tation Propagation (EP) method (Minka, 2001).
EP, however, uses the probit likelihood

p(yi|fi) = Φ(fiyi), (5)

which makes the posterior analytically intractable.
To overcome this hurdle in the EP framework, the
likelihood is approximated by a local likelihood
approximation in the form of an un-normalized

Gaussian function in the latent variable fi which
defines the site parameters Z̃i, µ̃i and σ̃2i .

p(yi|fi) ' ti(fi|Z̃i, µ̃i, σ̃2i ) =∆ Z̃iN (fi|µ̃i, σ̃2i )
(6)

The posterior p(f |X,y) is approximated by
q(f |X,y) = N (µ,Σ), where µ = ΣΣ̃−1µ̃, Σ̃ is
diagonal with Σ̃ii = σ̃2i , Σ = (K

−1 + Σ̃−1)−1,
and K is the covariance matrix.

A practical implementation of Gaussian Process
Classification (GPC) for binary class (Rasmussen
and Williams, 2006) is outlined in the following
algorithm:
Algorithm: Predictions for Expectation Propaga-
tion GPC.
Input:ν̃, τ̃ (Natural site param), X (Training in-
puts), y (Training targets), k (Covariance func-
tion), x∗ (Test input).
Output: Predictive class probability.

1. L := cholesky(In+S̃1/2KS̃1/2)
2. z :=S̃1/2LT \(L\S̃1/2Kν̃)
3. f∗ := k(x∗)

T (ν̃ − z)
4. v := L\(S̃1/2k(x∗))
5. V [f∗] := k(x∗,x∗)− vTv
6. π∗ := Φ(f∗/

√
1 + V [f∗])

7. return: π∗ (predictive class probability)
The natural site parameters ν̃ and τ̃ for Expecta-
tion Propagation GPC are found using EP approx-
imation algorithm. Multi-class classification can
be performed using either one-versus-rest or one-
versus-one for training and prediction. For Gaus-
sian Process classification, “one-vs-one” might be
computationally cheaper, so we have used it to for
subtasks A and C.

2.2 Multiple Kernel Gaussian Process

The covariance kernel k of Gaussian Process di-
rectly specifies the covariance between every pair
of input points in the dataset. The particular choice
of covariance function determines the properties
such as smoothness, length scales, and amplitude,
drawn from the GP prior.

We have used Exponential kernel and Multi-
Layer Perceptron kernel combined with Squared
Exponential kernel, and found the combinations
to give better results. The text data used in sen-
timent analysis is collected over a period of time.
Comments on the same topic may exhibit differ-
ent emotions, depending on the time it was made,
and hence their properties, such as smoothness and
periodicity, also vary with time. Since any one

710



kernel learns only certain properties well, multiple
kernels are effective in detecting the simultaneous
presence of different emotions in the data.

The MKL algorithms use different learning
methods for determining the kernel combination
function. It is divided into five major categories:
Fixed rules, Heuristic approaches, Optimization
approaches, Bayesian approaches and Boosting
approaches. The combination of kernels in differ-
ent learning methods can be performed in one of
the two basic ways, either using linear combina-
tion or using non-linear combination. Linear com-
bination seems more promising (Gonen and Al-
paydn, 2011), and have two basic categories: un-
weighted sum (i.e., using sum or mean of the ker-
nels as the combined kernel) and weighted sum.
Non-linear combination uses non-linear functions
of kernels, namely multiplication, power, and ex-
ponentiation. We have studied the fixed rule linear
combination in this work which can be represented
as

k(x, x′) = k1(x, x′)+k2(x, x′)+. . .+kn(x, x′).
(7)

For training, we have used one-step method to-
gether with the simultaneous approach. One-step
methods, in a single pass, calculate both the pa-
rameters of the combination function, and those
of the combined base learner; and the simultane-
ous approach ensures that both sets of parameters
are learned together.

3 System Overview

The system comprises of the following modules:
data extraction, preprocessing, feature vector gen-
eration, and multi-kernel Gaussian Process model
building. The data is preprocessed with lemmati-
zation and tokenization, using NLTK toolkit. Then
train variable is assigned an integer value. A data
dictionary is built using training sentences, and
feature vectors for train sets are generated by en-
coding BoW representation. These feature vectors
are given as input to build the MKGPC model.

The Multi-Kernel Gaussian Process Classifica-
tion (MKGPC) model building is outlined in the
following algorithm.
Algorithm: Build a Multi-Kernel Gaussian
Process model.
Input: Input dataset with BoW feature represen-
tation.
Output: Learned model.

begin
1. Split the training dataset into XTrain which

contains the features and YTrain that contains
the emotion scores.

2. Build the initial classification model using
appropriate kernel function.

3. Optimize the classification model with the
hyper-parameters (length scale, variance,
noise).

4. Return the learned model.
end

There are different kernels that can be used to
build a GPC model. The Squared Exponential
(SE) kernel, sometimes called the Gaussian or Ra-
dial Basis Function (RBF), has become the default
kernel in GPs. To model the long-term smooth-
rising trend, we use a Squared Exponential covari-
ance term.

k(x, x′) = σ2 exp
(
−(x− x

′)2

2l2

)
. (8)

where σ2 is the variance and l is the length-scale.
The usage of Exponential kernel is particularly

common in machine learning and hence is also
used in GPs. They perform tasks such as statis-
tical classification, regression analysis, and cluster
analysis on data in an implicit space.

k(x, x′) = σ2 exp
(
−(x− x

′)
2l2

)
(9)

The Multi-Layer Perceptron kernel has also
found use in GP as it can learn the periodicity
property present in the dataset; its k(x, x′) is given
by

2σ2

π
sin−1

(σ2wx
Tx′ + σ2b )√

σ2wx
Tx+ σ2b + 1

√
σ2wx

′Tx′σ2b + 1
(10)

where σ2 is the variance, σ2w is the vector of the
variances of the prior over input weights and σ2b
is the variance of the prior over bias parameters.
The kernel can learn more effectively because of
the additional parameters σ2w and σ

2
b .

4 Results and Discussion

The output submitted for the task was obtained
using MKGPC with Radial Basis Function ker-
nel and Exponential Kernel. We also used Multi-
Layer Perceptron Kernel. The results of the SGPC
using SE kernel for subtask B and MKGPC for

711



subtask B are shown in Table 1. The evaluation
was done on SemEval-2017 labeled test dataset.
Only 1000 tweets were used to train the model due
to the time-complexity of GP and hardware limita-
tions, and from among the remaining 9551 tweets
test set was taken.

Table 1: A Performance Evaluation based on
Recall, F-measure and Precision (all macro-
averaged) for subtask B

Model Recall F-measure Precision

SGPC 0.57 0.58 0.64
MKGPC(R+E) 0.56 0.56 0.63
MKGPC(R+M) 0.61 0.62 0.64
MKGPC(R+E+M) 0.62 0.63 0.64

The kernel combinations used in Table 1 are
SGPC: Single Kernel Gaussian Process Classifier

with Radial Basis Function (RBF) kernel,
MKGPC(R+E): Multi Kernel Gaussian Process

with sum of RBF and Exponential kernels,
MKGPC(R+E+M): Multi Kernel Gaussian Pro-

cess Classifier with sum of RBF, Exponential,
and Multi-Layer Perceptron kernels,

MKGPC(R+M): Multi Kernel Gaussian Process
Classifier with sum of RBF and Multi-Layer
Perceptron kernels.

We observe from Table 1 that though the macro-
averaged precision of the MKGPC models is
the same as SGPC, their macro-averaged recall
and F-measure are better than SGPC (except for
MKGPC(R+E)), because the Multi-Layer Percep-
tron kernel learns the periodicity better than RBF
and Exponential kernels do. These different mod-
els, when evaluated on dataset for subtask A and
subtask C, exhibited similar performance as in
subtask B. The system underperform compared to
the baseline system in task C, and to logistic re-
gression on 1-gram in tasks A and B since only a
small fraction of the dataset was used for training.

5 Official Evaluation

Our system scored a macro-averaged recall of
0.431 and was ranked 35 for subtask A, macro-
averaged recall of 0.586 and was ranked 20 for
subtask B, and macro-averaged mean absolute er-
ror of 1.325 and was ranked 15 for subtask C.

6 Conclusion

In this paper, we have presented a Gaussian Pro-
cess classification model for sentiment analysis in

Twitter. We used Bag of Words feature vectors
and fixed rule multi kernel learning to build the GP
model. We observed that combining Multi-Layer
Perceptron kernel improves the performance of the
system, perhaps due to its more effective learning
of the periodicity property in the dataset. There
is scope for enhancing the results by using differ-
ent feature generation algorithms, different multi-
kernel learning approaches, and increasing the
data size.

References
C. Chew and G. Eysenbach. 2010. Pandemics in the

age of twitter: content analysis of tweets during the
2009 h1n1 outbreak. PloS ONE 5(11):1–13.

Trevor Cohn, Daniel Beck, and Lucia Specia. 2014.
Joint emotion analysis via multi-task gaussian pro-
cesses. In Proceedings of EMNLP 2014, the
International Conference onEmpirical Methods in
Natural Language Processing. Journal of Machine
Learning Research, pages 1798 – 1803.

Trevor Cohn and Lucia Specia. 2013. Modelling anno-
tator bias with multi-task gaussian processes: An ap-
plication to machine translation quality estimation.
In Proceedings of the 51st Annual Meeting of the
ACL-2013. ACL, pages 32–42.

Mehmet Gonen and Ethem Alpaydn. 2011. Multi-
ple kernel learning algorithms. Journal of Machine
Learning Research 24(11):2211 – 2268.

B. J. Jansen, M. Zhang, K. Sobel, and A. Chowdury.
2009. Twitter power: Tweets as electronic word of
mouth. Journal of the American Society for Infor-
mation Science and Technology 60(11):21692188.

T P Minka. 2001. Family of Algorithms for Approxi-
mate Bayesian Inference. PhD thesis, MIT.

Mahmoud Nabil, Mohamed Aly, and Amir F. Atiya.
2016. Cufe at semeval-2016 task 4: A gated recur-
rent model for sentiment classification. In Proceed-
ings of SemEval-2016. ACL, pages 52 –57.

Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian processes for machine
learning, volume 1. MIT Press Cambridge.

S. Verma, S. Vieweg, W. J. Corvey, L. Palen, J. H. Mar-
tin, M. Palmer, A. Schram, and K. M. Anderson.
2011. Natural language processing to the rescue?
extracting situational awareness tweets during mass
emergency. In Proceedings of 5th International
Conference on Web and Social Media (ICWSM).

A. G. Wilson and R. P. Adams. 2013. Gaussian process
kernels for pattern discovery and extrapolation. In
Proceedings of ICML 2013, the International Con-
ference onMachine Learning. Journal of Machine
Learning Research, pages 1067 – 1075.

712


