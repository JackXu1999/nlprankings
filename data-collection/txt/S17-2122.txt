



















































XJSA at SemEval-2017 Task 4: A Deep System for Sentiment Classification in Twitter


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 728–731,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

ACL 2017 Submission ***. Confidential review Copy. DO NOT DISTRIBUTE.

1

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

XJSA at SemEval-2017 Task 4：A Deep System for Sentiment
Classification in Twitter

Yazhou Hao Yangyang Lan
University of Xi’an Jiaotong University of Xi’an Jiaotong
yazhouhao@gmail.com 1564018606@qq.com

Yufei Li Chen Li
University of Xi’an Jiaotong University of Xi’an Jiaotong
18391819285@163.com cli@xjtu.edu.cn

Abstract

This paper describes the XJSA System
submission from XJTU. Our system was
created for SemEval2017 Task 4 – subtask
A which is very popular and fundamental.
The system is based on convolutional
neural network and word embedding. We
used two pre-trained word vectors and
adopt a dynamic strategy for k-max
pooling.

1 Introduction

Several years ago, the typical approaches to
sentiment analysis of tweets were based on
classifiers trained using several hand-crafted
features, in particular lexicons of words with an
assigned polarity value. About since 2014 the
deep neural network methods have got state-of-
the-art results in many NLP tasks, especially in
sentiment classification. The work of Harvard
NLP group in 2014 and Kalchbrenner’s work in
2014 have suggested that convolutional neural
network and word embedding play important
roles in this field. General word embedding has
got excellent results. If we can embed sentiment
information in vectors, we will get better results.
There are some open word vectors on the web
already such as Word2Vec (Mikolov et al., 2013),
Glove (Pennington et al., 2014), SSWE (Tang et
al., 2014). In our system we use Word2Vec and
SSWE at the same time.
Deep learning models have achieved excellent
results in computer vision and speech recognition
in recent years. In the field of natural language
processing, much work with deep learning
methods has involved learning word vectors

representations for their own task or problem
(Bengio et al., 2003; Mikolov et al., 2013,
Collobert C&W et al., 2011).The others exploit
the open word vectors which was mentioned
above.Word vectors is a transformation of the
feature of letter,word,sentence and paragraph or
even text. It’s a lower dimensional, dense and
continuous vectors. In this vector, the words have
similar syntactic are close – in Euclidean or cosine
distance in the vector space. So one can study and
compare the syntactic functionality between
different words via word vectors.
Convolutional neural network (CNN) utilize

layers with convolutional filters that are applied to
local features (LeCun et al., 1998). CNN
originally invented for computer vision, recently
CNN models have achieved remarkably results in
many natural language processing problem, such
as sentence modeling (Kalchbrenner et al., 2014),
semantic parsing (Yih et al., 2014), sentiment
classification (kim et al., 2014) and other
traditional natural language processing
tasks(Collobert C&W et al., 2011).
Our system was inspired by the work (kim et

al., 2014) and another work (Tang et al., 2014). In
the aspect of CNN, we use a simple 3 layers CNN
to automatic extract features. In the aspect of pre-
trained vectors, we use the Word2Vec and SSWE
to filter our training set to get a proper input for
CNN. The reason that we use the vectors trained
by Mikolov et al. (2013) is the 100 billion words
of Google News and the vectors are publicly for
free. We use the SSWE vectors because the
vectors was especially trained for sentiment
classification by Tang et al (2014). SSWE
contains sentiment information which is not in
word vectors trained by Mikolov.

728



ACL 2017 Submission ***. Confidential review Copy. DO NOT DISTRIBUTE.

2

100

101

102

103

104

105

106

107

108

109

110

111

112

113

114

115

116

117

118

119

120

121

122

123

124

125

126

127

128

129

130

131

132

133

134

135

136

137

138

139

140

141

142

143

144

145

146

147

148

149

150

151

152

153

154

155

156

157

158

159

160

161

162

163

164

165

166

167

168

169

170

171

172

173

174

175

176

177

178

179

180

181

182

183

184

185

186

187

188

189

190

191

192

193

194

195

196

197

198

199

2 Background

As is shown above,traditional methods typically
model the syntactic context of words but ignore
the sentiment information of text. As a result,
words with opposite polarity are mapped into
close vectors, such as good and bad, just as
Word2Vec.So in our system,we use SSWE and
Word2Vec at the same time for word embedding,
SSWE first.
Tang et al.(2014) introduce SSWE model to

learn word embedding for Twitter sentiment
classification. In our task,We use the word vector
trained by uSSWE ,which captures the sentiment
information of sentences as well as the syntactic
contexts of words. uSSWE is illustrated in Figure1.
Given an original(or corrupted)n-gram and the

sentiment polarity of a sentence as the input,
SSWEu predicts a two-dimensional vector for
each input n-gram. t is the original n-gram, rt is the
corrupted n-gram.The two scalars ),( 10

uu ff stand
for language model score and sentiment score of
the input n-gram,where uf0 stands the
positive, uf1 the negative.
The training goal of SSWE are that (1) the

original n-gram should obtain a higher language
model score )(0 tf

u than the corrupted n-gram

)(0
ru tf ,and (2) the sentiment score of original n-

gram )(1 tf
u should be more consistent with the

gold polarity annotation of sentence than
corrupted n-gram )(1

ru tf .The loss function of
SSWEu is shown behind,

 ),(),( rcw
r

u ttlossttloss 

),()1( rus ttloss (1)
where ),( rcw ttloss is the syntactic loss as given in
Equation 1, ),( rus ttloss is the sentiment loss as
shown in Equation 2.The hyper-parameter ɑ
weights the two parts. )(ts is an indicator
function reflecting the sentiment polarity of a
sentence.

)()(1,0max(),( 1 tftttloss
u

s
r

us 

))()( 1
ru

s tft (2)

3 Model

The architecture of our system shown in figure 2
is a simple 3 layers CNN just like the architecture
of Kim et al (2013). kix  is the k-
dimensional word vector corresponding to the i-th
word in the sentence. A sentence of length n is
described as

,...21:1 nn xxxx  (3)

Where⊕ is the concatenation operator. Then we
let jiix : stand for the concatenation of words

jiii xxx  ,...,, 1 . A convolutional filter
hkw 

is applied to a window of h words to produce a
new feature. For example, a feature ic is
generated from a window of words 1: hiix by

)( 1: bxwfc hiii   (4)

where b is a bias term and f is a non-linear
function. This filter is applied to each possible
window of words in the sentence

},...,,{ :11:2:1 nhnhh xxx  to produce a feature map

],,...,,[ 121  hncccc (5)

Here 1 hnc . Then a max-over-time
pooling operation is applied just like Collobert
C&W et al. (2011).over the feature map and take
the maximum value }max{

^
cc 

4 Experimental Setup

We test our system on the following settings:

4.1 Hyper-parameters and Training

There are four models in Kim et al (2013):CNN-
rand,CNN-static,CNN-non-static,CNN-
multichannel.

Figure 1:The SSWEu model

729



ACL 2017 Submission ***. Confidential review Copy. DO NOT DISTRIBUTE.

3

200

201

202

203

204

205

206

207

208

209

210

211

212

213

214

215

216

217

218

219

220

221

222

223

224

225

226

227

228

229

230

231

232

233

234

235

236

237

238

239

240

241

242

243

244

245

246

247

248

249

250

251

252

253

254

255

256

257

258

259

260

261

262

263

264

265

266

267

268

269

270

271

272

273

274

275

276

277

278

279

280

281

282

283

284

285

286

287

288

289

290

291

292

293

294

295

296

297

298

299

For all our experiments we use CNN-non-static:
A model with pre-trained vectors from SSWE and
wrd2vec.The pre-trained vectors are fine-tuned for
each task.

We use rectified linear units, filter windows (h)
of 3, 4, 5 with 100 feature maps each, dropout rate
(p) of 0.5, 2l constraint (s) of 3, and mini-batch
size of 50. These values were chosen via a grid
search on the dev sets.

4.2 Pre-trained Word Vectors

Initializing word vectors with those obtained from
an unsupervised neural language model is a
popular method to improve performance in the
absence of a large supervised training set
(Collobert C&W et al., 2011; Socher et al., 2011;
Iyyer et al., 2014). First we use SSWE (Tang et al.
2014) which is a word vector contains sentiment
information.We also use the publicly available
word2vec vectors which were trained on 100
billion words from Google News. The vectors
have dimensionality of 300 and were trained using
the CBOW architecture.Because the
dimensionality of vectors in SWEE is 50, so we
extended it to 300 dimension by padding the 250
dimension randomly.

4.3 Environment of experiment

The experiments were run on a linux server with
an nVIDIA GTX 1080 accelerated GPU.

5 Results

In order to compare the results of our system with
other better system’s results, here we show

enough results generated by our system and the
top one. The official submission achieved results
presented in Table 1, compared to the top scoring
system. We also list our detailed scores in Table 2

6 Conclusion

Our work based on the method with deep learning
neural network built on the top of word2vec and
SSWE. We can find if we exploit the sentiment
information in the pre-trained word vector we
would get better result. Our work and some
previous work mentioned in this paper show that
unsupervised pre-training of word vectors plays
an important role in deep learning for sentiment
analysis.

P R F1
Positive 0.5791 0.6748 0.5423
Negative 0.5655 0.5592 0.5065
Neutral 0.5264 0.4340 0.4962
AvgP = 0.557, AvgR = 0.556, AvgF1= 0.519
Overall score: 0.556

system AvegF1 AvegR Acc
BB_twtr 0.685 0.681 0.658
XJSA 0.519 0.556 0.575

Figure 2: Architecture of XJSA
system

Table 1: Official results of our submission
compared to the top one.

Table 2: Detailed scores of XJSA official
submission.

730



ACL 2017 Submission ***. Confidential review Copy. DO NOT DISTRIBUTE.

4

300

301

302

303

304

305

306

307

308

309

310

211

312

313

314

315

316

317

318

319

320

321

322

323

324

325

326

327

328

329

330

331

332

333

334

335

336

337

338

339

340

341

342

343

344

345

346

347

348

349

350

351

352

353

354

355

356

357

358

359

360

361

362

363

364

365

366

367

368

369

370

371

372

373

374

375

376

377

378

379

380

381

382

383

384

385

386

387

388

389

390

391

392

393

394

395

396

397

398

399

References
Alfred. V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation and Compiling,
volume 1. Prentice-Hall, Englewood Cliffs, NJ.

American Psychological Association. 1983.
Publications Manual. American Psychological
Association, Washington, DC

Collobert, Ronan, and Jason Weston. “A Unified
Architecture for Natural Language Processing:
Deep Neural Networks with Multitask Learning.”
In Proceedings of the 25th International
Conference on Machine Learning, 160–167. ACM,
2008. http://dl.acm.org/citation.cfm?id=1390177.

Collobert, J Weston, L.Bottou, M.Karlen,
K.Kavukcugla, P.Kuksa. 2011“Natural Language
Processing(Almost)from Scratch.” Journal of
Machine Learning Research 12:2493-2537

Kalchbrenner, Nal, Edward Grefenstette, and Phil
Blunsom. “A Convolutional Neural Network for
Modelling Sentences.” arXiv Preprint
arXiv:1404.2188, 2014.
http://arxiv.org/abs/1404.2188.

Kim, Yoon. “Convolutional Neural Networks for
Sentence Classification.” arXiv Preprint
arXiv:1408.5882, 2014.
http://arxiv.org/abs/1408.5882.

Tang, Duyu, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. “Learning Sentiment-Specific
Word Embedding for Twitter Sentiment
Classification.” In ACL (1), 1555–1565, 2014.
http://anthology.aclweb.org/P/P14/P14-1146.pdf.

Gao J, He X, Yih W, et al. Learning continuous phrase
representations for translation modeling//In ACL.
2014.

Pennington J, Socher R, Manning C D. Glove: Global
Vectors for Word Representation//EMNLP. 2014,
14: 1532-1543.

Socher R, Perelygin A, Wu J Y, et al. Recursive deep
models for semantic compositionality over a
sentiment treebank//Proceedings of the conference
on empirical methods in natural language
processing (EMNLP). 2013, 1631: 1642.

Iyyer M, Boyd-Graber J L, Claudino L M B, et al. A
Neural Network for Factoid Question Answering
over Paragraphs//EMNLP. 2014: 633-644.

731


