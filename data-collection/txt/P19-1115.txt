



















































Self-Attentional Models for Lattice Inputs


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1185–1197
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

1185

Self-Attentional Models for Lattice Inputs

Matthias Sperber1, Graham Neubig2, Ngoc-Quan Pham1, Alex Waibel1,2
1Karlsruhe Institute of Technology, Germany

2Carnegie Mellon University, USA
{first}.{last}@kit.edu, gneubig@cs.cmu.edu

Abstract
Lattices are an efficient and effective method
to encode ambiguity of upstream systems in
natural language processing tasks, for example
to compactly capture multiple speech recogni-
tion hypotheses, or to represent multiple lin-
guistic analyses. Previous work has extended
recurrent neural networks to model lattice in-
puts and achieved improvements in various
tasks, but these models suffer from very slow
computation speeds. This paper extends the
recently proposed paradigm of self-attention
to handle lattice inputs. Self-attention is a se-
quence modeling technique that relates inputs
to one another by computing pairwise simi-
larities and has gained popularity for both its
strong results and its computational efficiency.
To extend such models to handle lattices, we
introduce probabilistic reachability masks that
incorporate lattice structure into the model and
support lattice scores if available. We also pro-
pose a method for adapting positional embed-
dings to lattice structures. We apply the pro-
posed model to a speech translation task and
find that it outperforms all examined baselines
while being much faster to compute than pre-
vious neural lattice models during both train-
ing and inference.

1 Introduction

In many natural language processing tasks, graph-
based representations have proven useful tools
to enable models to deal with highly structured
knowledge. Lattices are a common instance of
graph-based representations that allows capturing
a large number of alternative sequences in a com-
pact form (Figure 1). Example applications in-
clude speech recognition lattices that represent al-
ternative decoding choices (Saleem et al., 2004;
Zhang et al., 2005; Matusov et al., 2008), word
segmentation lattices that capture ambiguous de-
cisions on word boundaries or morphological al-
ternatives (Dyer et al., 2008), word class lattices

fa
S

c
E

d
b

e

g1

0.4 1 1 1

0.6

0.2

0.8 1

f
E

fa
S

c

d
b

ea
S E

c
b d

1
0.4 1

1

0.6 0.2
0.8

ea
S E

c
b d

1
0.45 0.88

1

1
0.55

0.12

1

Figure 1: Example of a node-labeled lattice. Nodes are
labeled with word tokens and posterior scores.

(Navigli and Velardi, 2010), and lattices for alter-
native video descriptions (Senina et al., 2014).

Prior work has made it possible to handle these
through the use of recurrent neural network (RNN)
lattice representations (Ladhak et al., 2016; Su
et al., 2017; Sperber et al., 2017), inspired by
earlier works that extended RNNs to tree struc-
tures (Socher et al., 2013; Tai et al., 2015; Zhu
et al., 2015). Unfortunately, these models are
computationally expensive, because the extension
of the already slow RNNs to tree-structured in-
puts prevents convenient use of batched compu-
tation. An alternative model, graph convolutional
networks (GCN) (Duvenaud et al., 2015; Deffer-
rard et al., 2016; Kearnes et al., 2016; Kipf and
Welling, 2017), is much faster but considers only
local context and therefore requires combination
with slower RNN layers for typical natural lan-
guage processing tasks (Bastings et al., 2017; Ce-
toli et al., 2017; Vashishth et al., 2018).

For linear sequence modeling, self-attention
(Cheng et al., 2016; Parikh et al., 2016; Lin et al.,
2017; Vaswani et al., 2017) now provides an alter-
native to RNNs. Self-attention encodes sequences
by relating sequence items to one another through
computation of pairwise similarity, with addition
of positional encoding to model positions of words
in a linear sequence. Self-attention has gained
popularity thanks to strong empirical results and
computational efficiency afforded by paralleliz-



1186

able computations across sequence positions.
In this paper, we extend the previously purely

sequential self-attentional models to lattice inputs.
Our primary goal is to obtain additional modeling
flexibility while avoiding the increased cost of pre-
vious lattice-RNN-based methods. Our technical
contributions are two-fold: First, we incorporate
the global lattice structure into the model through
reachability masks that mimic the pairwise condi-
tioning structure of previous recurrent approaches.
These masks can account for lattice scores if avail-
able. Second, we propose the use of lattice posi-
tional embeddings to model positioning and order-
ing of lattice nodes.

We evaluate our method on two standard speech
translation benchmarks, replacing the encoder
component of an attentional encoder-decoder
model with our proposed lattice self-attentional
encoder. Results show that the proposed model
outperforms all tested baselines, including LSTM-
based and self-attentional sequential encoders, a
LatticeLSTM encoder, and a recently proposed
self-attentional model that is able to handle graphs
but only considers local context, similar to GCNs.
The proposed model performs well without sup-
port from RNNs and offers computational advan-
tages in both training and inference settings.

2 Background

2.1 Masked Self-Attention

We start by introducing self-attentional models for
sequential inputs, which we will extend to lattice-
structured inputs in § 4.

Attentional models in general can be described
using the terminology of queries, keys, and val-
ues. The input is a sequence of l values, along
with a key corresponding to each value. For some
given query, the model computes how closely each
key matches the query. Here, we assume values,
keys, and queries vk,kk,q∈Rd, for some dimen-
sionality d and sequence indices k∈{1 . . . l}. Us-
ing the computed similarity scores f(q,kk), at-
tention computes a weighted average of the values
to obtain a fixed-size representation of the whole
sequence conditioned on this query. In the self-
attentional case, the sequence items themselves
are used as queries, yielding a new sequence of
same length as output in which each of the orig-
inal input elements has been enriched by the re-
spectively relevant global context.

The following equations formalize this idea. We

are given a sequence of input vectors xk ∈ Rd. For
every query index i, we compute an output vector
yi as:

eij = f (q (xi) , k (xj))+mij (∀1≤j≤l) (1)
αi = softmax (ei) (2)

yi =
l∑

j=1

αijv (xj) . (3)

Here, unnormalized pairwise similarities eij
are computed through the similarity function f ,
and then normalized as αij for computation of
a weighted sum of value vectors. q, k, v denote
parametrized transformations (e.g. affine) of the
inputs into queries, keys, and values.

Equation 1 also adds an attention masking term
mij ∈ R that allows adjusting or disabling the in-
fluence of context at key position j on the output
representation at query position i. Masks have, for
example, been used to restrict self-attention to ig-
nore future decoder context (Vaswani et al., 2017)
by settingmij = −∞ for all j>i. We will use this
concept in § 4.1 to model reachability structure.

2.2 Lattices

We aim to design models for lattice inputs that
store a large number of sequences in a compact
data structure, as illustrated in Figure 1. We define
lattices as directed acyclic graphs (DAGs) with the
additional property that there is exactly one start
node (S) and one end node (E). We call the se-
quences contained in the lattice complete paths,
running from the start node to the end node. Each
node is labeled with a word token.1

To make matters precise, let G=(V,E) be a
DAG with nodes V and edges E. For k∈V , let
R+G(k) denote all successors (reachable nodes) of
node k, and let N+G(k) denote the neighborhood,
defined as the set of all adjacent successor nodes.
R–G(k),N

–
G(k) are defined analogously for prede-

cessors. j � i indicates that node j is a successor
of node i.

For arbitrary nodes i, j, let pG (j � i | i) be
the probability that a complete path in G con-
tains j as a successor of i, given that i is con-
tained in the path. Note that j /∈ R+G(i) im-
plies pG (j � i | i)=0. The probability structure

1Edge-labeled lattices can be easily converted to node-
labeled lattices using the line-graph algorithm (Hemminger
and Beineke, 1978).



1187

of the whole lattice can be represented through
transition probabilities ptransk,j :=pG (k � j | j) for
j ∈ N+G(k). We drop the subscript G when clear
from context.

3 Baseline Model

Our proposed model builds on established archi-
tectures from prior work, described in this section.

3.1 Lattice-Biased Attentional Decoder

The common attentional encoder-decoder model
(Bahdanau et al., 2015) serves as our starting
point. The encoder will be described in § 4.
As cross-attention mechanism, we use the lattice-
biased variant (Sperber et al., 2017), which adjusts
the attention scores αcrossij between encoder posi-
tion j and decoder position i according to marginal
lattice scores p (j � S | S) (§ 4.1.2 describes how
to compute these) as follows:2

αcrossij ∝ exp (score(•) + log p (j � S | S)) . (4)

Here, score(•) is the unnormalized attention score.
In the decoder, we use long short-term mem-

ory (LSTM) networks, although it is straightfor-
ward to use alternative decoders in future work,
such as the self-attentional decoder proposed by
Vaswani et al. (2017). We further use input feed-
ing (Luong et al., 2015), variational dropout in the
decoder LSTM (Gal and Ghahramani, 2016), and
label smoothing (Szegedy et al., 2016).

3.2 Multi-Head Transformer Layers

To design our self-attentional encoder, we use
Vaswani et al. (2017)’s Transformer layers that
combine self-attention with position-wise feed-
forward connections, layer norm (Ba et al., 2016),
and residual connections (He et al., 2016) to
form deeper models. Self-attention is modeled
with multiple heads, computing independent self-
attentional representations for several separately
parametrized attention heads, before concatenat-
ing the results to a single representation. This
increases model expressiveness and allows using
different masks (Equation 1) between different at-
tention heads, a feature that we will exploit in
§ 4.1. Transformer layers are computed as follows:

2We have removed the trainable peakiness coefficient
from the original formulation for simplicity and because
gains of this additional parameter were unclear according to
Sperber et al. (2017).

Qk = XW
(q)
k ,Kk=XW

(k)
k ,Vk=XW

(v)
k (5)

Hk = softmax

(
dropout

(
QiK

>
k +M

)
√
d

)
Vk

(6)

H = concat(H1,H2, . . . ,Hn) (7)

L = LN [dropout (H+X)] (8)

Y = LN [dropout (FF (L) + L)] (9)

Here, X∈Rl×d,Qk,Kk,Vk∈Rl×d/n denote in-
puts and their query-, key-, and value transforma-
tions for attention heads with index k∈{1, . . . , n},
sequence length l, and hidden dimension d.
M∈Rl×l is an attention mask to be defined in
§ 4.1. Similarity between keys and queries is
measured via the dot product. The inputs are
word embeddings in the first layer, or the out-
put of the previous layer in the case of stacked
layers. Y∈Rl×d denotes the final output of
the Transformer layer. W(q)k ,W

(k)
k ,W

(v)
k ∈

Rd×d/n are parameter matrices. FF is a position-
wise feed-forward network intended to introduce
additional depth and nonlinearities, defined as
FF(x)=max (0,xW1 + b1)W2 + b2. LN de-
notes layer norm. Note that dropout regularization
(Srivastava et al., 2014) is added in three places.

Up to now, the model is completely agnostic of
sequence positions. However, position informa-
tion is crucial in natural language, so a mecha-
nism to represent such information in the model is
needed. A common approach is to add positional
encodings to the word embeddings used as inputs
to the first layer. We opt to use learned positional
embeddings (Gehring et al., 2017), and obtain the
following after applying dropout:

x′i = dropout (xi + embed [i]) . (10)

Here, a position embedding embed [i] of equal di-
mension with sequence item xi at position i is
added to the input.

4 Self-Attentional Lattice Encoders

A simple way to realize self-attentional modeling
for lattice inputs would be to linearize the lattice in
topological order and then apply the above model.
However, such a strategy would ignore the lat-
tice structure and relate queries to keys that cannot
possibly appear together according to the lattice.



1188

fa
S

c
E

d
b

e

g

1
0.4 1 1

1

0.6

0.2

0.8
1

fb
a

d
E

e
c

f

h

fa
S

c
i

d
b

f

h

fa
S

c
E

d
b

e

g

1
0.4 1 1

1

0.6

0.2

0.8
1

fa
S

c
E

d
b

e

g

1
1 0.45 0.88

1

1

1

0.55
0.12

Figure 2: Example for binary masks in forward- and
backward directions. The currently selected query is
node f, and the mask prevents all solid black nodes
from being attended to.

We find empirically that this naive approach per-
forms poorly (§ 5.4). As a remedy, we introduce
a masking scheme to incorporate lattice structure
into the model (§ 4.1), before addressing positional
encoding for lattices (§ 4.2).

4.1 Lattice Reachability Masks
We draw inspiration from prior works such as the
TreeLSTM (Tai et al., 2015) and related works.
Consider how the recurrent conditioning of hidden
representations in these models is informed by the
graph structure of the inputs: Each node is condi-
tioned on its direct predecessors in the graph, and
via recurrent modeling on all its predecessor nodes
up to the root or leaf nodes.

4.1.1 Binary Masks
We propose a masking strategy that results in
the same conditioning among tokens based on
the lattice structure, preventing the self-attentional
model from attending to lattice nodes that are not
reachable from some given query node i. Figure 2
illustrates the concept of such reachability masks.
Formally, we obtain masks in forward and back-
ward direction as follows:

−→mbinij =
{

0 if i∈R– (j) ∨ i=j
−∞ else

←−mbinij =
{

0 if i∈R+ (j) ∨ i=j
−∞ else

The resulting conditioning structure is analo-
gous to the conditioning in lattice RNNs (Ladhak
et al., 2016) in the backward and forward direc-
tions, respectively. These masks can be obtained
using standard graph traversal algorithms.

4.1.2 Probabilistic Masks
Binary masks capture the graph structure of the in-
puts, but do not account for potentially available
lattice scores that associate lattice nodes with a
probability of being correct. Prior work has found

fa
S

c
E

d
b

e

g1

0.4 1 1 1

0.6

0.2

0.8 1

f
E

fa
S

c

d
b

ea
S E

c
b d

10.4 1

10.6

0.2

0.8

ea
S E

c
b d

1 0.45 0.88

1
1
0.55

0.12

1

1
1

→ S a b c d e E

S 1 0.4 0.6 0.48 0.12 0.88 1
a 0 1 0 1 0 1 1
b 0 0 1 0.8 0.2 0.8 1
c 0 0 0 1 0 1 1
d 0 0 0 0 1 0 1
e 0 0 0 0 0 1 1
E 0 0 0 0 0 0 1
← S a b c d e E

S 1 0 0 0 0 0 0
a 1 1 0 0 0 0 0
b 1 0 1 0 0 0 0
c 1 0 0 1 0 0 0
d 1 0 1 0 1 0 0
e 1 0.45 0.55 0.55 0 1 0
E 1 0.4 0.6 0.48 0.12 0.88 1

Figure 3: Example for pairwise conditional reaching
probabilities for a given lattice, which we logarith-
mize to obtain self-attention masks. Rows are queries,
columns are keys.

it critical to exploit lattice scores, especially for
noisy inputs such as speech recognition lattices
(Sperber et al., 2017). In fact, the previous bi-
nary masks place equal weight on all nodes, which
will cause the influence of low-confidence regions
(i.e., dense regions with many alternative nodes)
on computed representations to be greater than the
influence of high-confidence regions (sparse re-
gions with few alternative nodes).

It is therefore desirable to make the self-
attentional lattice model aware of these scores, so
that it can place higher emphasis on confident con-
text and lower emphasis on context with low con-
fidence. The probabilistic masks below generalize
binary masks according to this intuition:

−→mprobij =
{

log pG (j � i | i) if i6=j
0 if i=j

←−mprobij =
{

log pG> (j � i | i) if i6=j
0 if i=j

Here, we set log(0):=−∞. Figure 3 illus-
trates the resulting pairwise probability matrix for
a given lattice and its reverse, prior to applying the
logarithm. Note that the first row in the forward
matrix and the last row in the backward matrix are
the globally normalized scores of Equation 4.

Per our convention regarding log(0), the −∞
entries in the mask will occur at exactly the same



1189

Algorithm 1 Computation of logarithmized prob-
abilistic masks via dynamic programming.
– given: DAG G = (V,E); transition probs ptransk,j

1: ∀i, j ∈ V : qi,j ← 0
2: for i ∈ V do . loop over queries
3: qi,i ← 1
4: for k ∈ topologic-order (V ) do
5: for next ∈ N+ (k) do
6: qi,next ← qi,next + ptransk,next · qi,k
7: end for
8: end for
9: end for

10: ∀i, j ∈ V : mprobij ← log qi,j

places as with the binary reachability mask, be-
cause the traversal probability is 0 for unreachable
nodes. For reachable nodes, the probabilistic mask
causes the computed similarity for low-confident
nodes (keys) to be decreased, thus increasing the
impact of confident nodes on the computed hidden
representations. The proposed probabilistic masks
are further justified by observing that the result-
ing model is invariant to path duplication (see Ap-
pendix A), unlike the model with binary masks.

The introduced probabilistic masks can be com-
puted in O

(
|V |3

)
from the given transition prob-

abilities by using the dynamic programming ap-
proach described in Algorithm 1. The backward-
directed probabilistic mask can be obtained by ap-
plying the same algorithm on the reversed graph.

4.1.3 Directional and Non-Directional Masks

The above masks are designed to be plugged into
each Transformer layer via the masking term M
in Equation 6. However, note that we have de-
fined two different masks, −→mij and ←−mij . To em-
ploy both we can follow two strategies: (1) Merge
both into a single, non-directional mask by using
←→m ij = max {−→mij ,←−mij}. (2) Use half of the
attention heads in each multi-head Transformer
layer (§ 3.2) with forward masks, the other half
with backward masks, for a directional strategy.

Note that when the input is a sequence (i.e.,
a lattice with only one complete path), the non-
directional strategy reduces to unmasked sequen-
tial self-attention. The second strategy, in contrast,
reduces to the directional masks proposed by Shen
et al. (2018) for sequence modeling.

S a

d

e

f

g

c

E
0 1

2 3

2

2 4 5
b
1

Figure 4: Lattice positions, computed as longest-path
distance from the start node S.

4.2 Lattice Positional Encoding
Encoding positional information in the inputs is a
crucial component in self-attentional architectures
as explained in § 3.2. To devise a strategy to en-
code positions of lattice nodes in a suitable fash-
ion, we state a number of desiderata: (1) Positions
should be integers, so that positional embeddings
(§ 3.2) can be used. (2) Every possible lattice path
should be assigned strictly monotonically increas-
ing positions, so that relative ordering can be in-
ferred from positions. (3) For a compact represen-
tation, unnecessary jumps should be avoided. In
particular, for at least one complete path the posi-
tions should increase by exactly 1 across all adja-
cent succeeding lattice nodes.

A naive strategy would be to use a topologi-
cal order of the nodes to encode positions, but
this clearly violates the compactness desideratum.
Dyer et al. (2008) used shortest-path distances be-
tween lattice nodes to account for distortion, but
this violates monotonicity. Instead, we propose
using the longest-path distance (ldist) from the
start node, replacing Equation 10 with:

x′i = dropout (xi + embed [ldist (S→ i)]) .

This strategy fulfills all three desiderata, as illus-
trated in Figure 4. Longest-path distances from
the start node to all other nodes can be computed
in O

(
|V |2

)
using e.g. Dijkstra’s shortest-path al-

gorithm with edge weights set to −1.

4.3 Computational Complexity
The computational complexity in the self-
attentional encoder is dominated by generating
the masks (O

(
|V |3

)
), or by the computation of

pairwise similarities (O
(
|V |2

)
) if we assume that

masks are precomputed prior to training. Our
main baseline model, the LatticeLSTM, can be
computed in O (|E|), where |E| ≤ |V |2. Never-
theless, constant factors and the effect of batched
operations lead to considerably faster computa-
tions for the self-attentional approach in practice
(§ 5.3).



1190

5 Experiments

We examine the effectiveness of our method on a
speech translation task, in which we directly trans-
late decoding lattices from a speech recognizer
into a foreign language.

5.1 Settings

We conduct experiments on the Fisher–Callhome
Spanish–English Speech Translation corpus (Post
et al., 2013). This corpus contains translated tele-
phone conversations, along with speech recogni-
tion transcripts and lattices. The Fisher portion
(138k training sentences) contains conversations
between strangers, and the smaller Callhome por-
tion (15k sentences) contains conversations be-
tween family members. Both and especially the
latter are acoustically challenging, indicated by
speech recognition word error rates of 36.4% and
65.3% on respective test sets for the transcripts
contained in the corpus. The included lattices have
oracle word error rates of 16.1% and 37.9%.

We use XNMT (Neubig et al., 2018) which
is based on DyNet (Neubig et al., 2017a), with
the provided self-attention example as a starting
point.3 Hidden dimensions are set to 512 unless
otherwise noted. We use a single-layer LSTM-
based decoder with dropout rate 0.5. All self-
attentional encoders use three layers with hidden
dimension of the FF operation set to 2048, and
dropout rate set to 0.1. LSTM-based encoders use
2 layers. We follow Sperber et al. (2017) to tok-
enize and lowercase data, remove punctuation, and
replace singletons with a special unk token. Beam
size is set to 8.

For training, we find it important to pretrain on
sequential data and finetune on lattice data (§ 5.6).
This is in line with prior work (Sperber et al.,
2017) and likely owed to the fact that the lattices
in this dataset are rather noisy, hampering train-
ing especially during the early stages. We use
Adam for training (Kingma and Ba, 2014). For se-
quential pretraining, we follow the learning sched-
ule with warm-up and decay of Vaswani et al.
(2017). Finetuning was sometimes unstable, so
we finetune both using the warm-up/decay strat-
egy and using a fixed learning rate of 0.0001 and
report the better result. We use large-batch train-
ing with minibatch size of 1024 sentences, accu-
mulated over 16 batched computations of 64 sen-

3Our code is available: http://msperber.com/
research/acl-lattice-selfatt/

Encoder model Inputs Fisher Callh.

LSTM4 1-best 35.9 11.8
Seq. SA 1-best 35.71 12.36
Seq. SA (directional) 1-best 37.42 13.00

Graph attention lattice 35.71 11.87
LatticeLSTM4 lattice 38.0 14.1

Lattice SA (proposed) lattice 38.73 14.74

Table 1: BLEU scores on Fisher (4 references) and
Callhome (1 reference), for proposed method and sev-
eral baselines.

tences each, due to memory constraints. Early
stopping is applied when the BLEU score on a
held-out validation set does not improve over 15
epochs, and the model with the highest validation
BLEU score is kept.

5.2 Main Results

Table 1 compares our model against several base-
lines. Lattice models tested on Callhome are
pretrained on Fisher and finetuned on Callhome
lattices (Fisher+Callhome setting), while lattice
models tested on Fisher use a Fisher+Fisher train-
ing setting. All sequential baselines are trained on
the reference transcripts of Fisher. The first set
of baselines operates on 1-best (sequential) inputs
and includes a bidirectional LSTM, an unmasked
self-attentional encoder (SA) of otherwise identi-
cal architecture with our proposed model, and a
variant with directional masks (Shen et al., 2018).
Next, we include a graph-attentional model that
masks all but adjacent lattice nodes (Veličković
et al., 2018) but is otherwise identical to the pro-
posed model, and a LatticeLSTM. Note that these
lattice models both use the cross-attention lattice-
score bias (§ 3.1).

Results show that our proposed model outper-
forms all examined baselines. Compared to the
sequential self-attentional model, our models im-
proves by 1.31–1.74 BLEU points. Compared
to the LatticeLSTM, our model improves results
by 0.64–0.73 BLEU points, while at the same
time being more computationally efficient (§ 5.3).
Graph attention is not able to improve over the se-
quential baselines on our task due to its restriction
to local context.

http://msperber.com/research/acl-lattice-selfatt/
http://msperber.com/research/acl-lattice-selfatt/


1191

Training Inference
Encoder Batching Speed Batching Speed

Sequential encoder models
LSTM M 4629 – 715
SA M 5021 – 796

LatticeLSTM and lattice SA encoders
LSTM – 178 – 391
LSTM A 710 A 538
SA M 2963 – 687
SA A 748 A 718

Table 2: Computation speed (words/sec), averaged
over 3 runs. Batching is conducted manually (M),
through autobatching (A), or disabled (–). The self-
attentional lattice model displays superior speed de-
spite using 3 encoder layers, compared to 2 layers for
the LSTM-based models.

5.3 Computation Speed

The self-attentional lattice model was motivated
not only by promising model accuracy (as con-
firmed above), but also by potential speed gains.
We therefore test computation speed for train-
ing and inference, comparing against LSTM- and
LatticeLSTM-based models. For fair compari-
son, we use a reimplementation of the Lattice-
LSTM so that all models are run with the exact
same toolkits and identical decoder architectures.
Again, LSTM-based models have two encoder
layers, while self-attentional models have three
layers. LatticeLSTMs are difficult to speed up
through manually implemented batched compu-
tations, but similar models have been reported to
strongly benefit from autobatching (Neubig et al.,
2017b) which automatically finds operations that
can be grouped after the computation graph has
been defined. Autobatching is implemented in
DyNet but not available in many other deep learn-
ing toolkits, so we test both with and without au-
tobatching. Training computations are manually
or automatically batched across 64 parallel sen-
tences, while inference speed is tested for sin-
gle sentences with forced decoding of gold trans-
lations and without beam search. We test with
DyNet commit 8260090 on an Nvidia Titan Xp
GPU and average results over three runs.

Table 2 shows the results. For sequential inputs,
the self-attentional model is slightly faster than the
LSTM-based model. The difference is perhaps

4BLEU scores taken from Sperber et al. (2017).

reachability
mask

dir. prob.
latt.
pos.

Fisher Callh.

38.73 14.74

38.25 12.45
37.52 14.37
35.49 12.83

30.58 9.41

Table 3: Ablation over proposed features, including
reachability masks, directional (vs. non-directional)
masking, probabilistic (vs. binary) masking, and lattice
positions (vs. topological positions).

smaller than expected, which can be explained by
the larger number of layers in the self-attentional
model, and the relatively short sentences of the
Fisher corpus that reduce the positive effect of par-
allel computation across sequence positions. For
lattice-based inputs, we can see a large speed-up of
the self-attentional approach when no autobatch-
ing is used. Replacing manual batching with au-
tobatching during training for the self-attentional
model yields no benefits. Enabling autobatching
at inference time provides some speed-up for both
models. Overall, the speed advantage of the self-
attentional approach is still very visible even with
autobatching available.

5.4 Feature Ablation

We next conduct a feature ablation to examine the
individual effect of the improvements introduced
in § 4. Table 3 shows that longest-path position
encoding outperforms topological positions, the
probabilistic approach outperforms binary reach-
ability masks, and modeling forward and reversed
lattices with separate attention heads outperforms
the non-directional approach. Consistently with
the findings by Sperber et al. (2017), lattice scores
are more effectively exploited on Fisher than on
Callhome as a result of the poor lattice quality for
the latter. The experiment in the last row demon-
strates the effect of keeping the lattice contents but
removing all structural information, by rearrang-
ing nodes in linear, arbitrary topological order, and
applying the best sequential model. Results are
poor and structural information clearly beneficial.

5.5 Behavior At Test Time

To obtain a better understanding of the proposed
model, we compare accuracies to the sequential



1192

Lattice oracle 1-best Lattice

Fisher
Sequential SA 47.84 37.42 –
Lattice SA 47.69 37.56 38.73

Callhome
Sequential SA 17.94 13.00 –
Lattice SA 18.54 13.90 14.74

Table 4: Fisher and Callhome models, tested by in-
putting lattice oracle paths, 1-best paths, and full lat-
tices.

self-attentional model when translating either lat-
tice oracle paths, 1-best transcripts, or lattices.
The lattice model translates sequences by treat-
ing them as lattices with only a single complete
path and all transition probabilities set to 1. Ta-
ble 4 shows the results for the Fisher+Fisher
model evaluated on Fisher test data, and for the
Fisher+Callhome model evaluated on Callhome
test data. We can see that the lattice model out-
performs the sequential model even when translat-
ing sequential 1-best transcripts, indicating bene-
fits perhaps due to more robustness or increased
training data size for the lattice model. However,
the largest gains stem from using lattices at test
time, indicating that our model is able to exploit
the actual test-time lattices. Note that there is still
a considerable gap to the translation of lattice ora-
cles which form a top-line to our experiments.

5.6 Effect of Pretraining and Finetuning

Finally, we analyze the importance of our strategy
of pretraining on clean sequential data before fine-
tuning on lattice data. Table 5 shows the results for
several combinations of pretraining and finetuning
data. The first thing to notice is that pretraining
is critical for good results. Skipping pretraining
performs extremely poorly, while pretraining on
the much smaller Callhome data yields results no
better than the sequential baselines (§ 5.2). We
conjecture that pretraining is beneficial mainly due
to the rather noisy lattice training data, while for
tasks with cleaner training lattices pretraining may
play a less critical role.

The second observation is that for the finetuning
stage, domain appears more important than data
size: Finetuning on Fisher works best when test-
ing on Fisher, while finetuning on Callhome works
best when testing on Callhome, despite the Call-

Sequential data Lattice data Fisher Callh.

– Fisher 1.45 1.78
Callhome Fisher 34.52 13.04

Fisher Callhome 35.47 14.74
Fisher Fisher 38.73 14.59

Table 5: BLEU scores for several combinations of
Fisher (138k sentences) and Callhome (15k sentences)
training data.

home finetuning data being an order of magnitude
smaller. This is encouraging, because the collec-
tion of large amounts of training lattices can be
difficult in practice.

6 Related Work

The translation of lattices rather than sequences
has been investigated with traditional machine
translation models (Ney, 1999; Casacuberta et al.,
2004; Saleem et al., 2004; Zhang et al., 2005;
Matusov et al., 2008; Dyer et al., 2008), but
these approaches rely on independence assump-
tions in the decoding process that no longer hold
for neural encoder-decoder models. Neural lattice-
to-sequence models were proposed by Su et al.
(2017); Sperber et al. (2017), with promising re-
sults but slow computation speeds. Other related
work includes gated graph neural networks (Li
et al., 2016; Beck et al., 2018). As an alter-
native to these RNN-based models, GCNs have
been investigated (Duvenaud et al., 2015; Def-
ferrard et al., 2016; Kearnes et al., 2016; Kipf
and Welling, 2017), and used for devising tree-to-
sequence models (Bastings et al., 2017; Marcheg-
giani et al., 2018). We are not aware of any ap-
plication of GCNs to lattice modeling. Unlike our
approach, GCNs consider only local context, must
be combined with slower LSTM layers for good
performance, and lack support for lattice scores.

Our model builds on previous works on self-
attentional models (Cheng et al., 2016; Parikh
et al., 2016; Lin et al., 2017; Vaswani et al., 2017).
The idea of masking has been used for various
purposes, including occlusion of future informa-
tion during training (Vaswani et al., 2017), in-
troducing directionality (Shen et al., 2018) with
good results for machine translation confirmed by
Song et al. (2018), and soft masking (Im and Cho,
2017; Sperber et al., 2018). The only extension
of self-attention beyond sequence modeling we



1193

are aware of is graph attention (Veličković et al.,
2018) which uses only local context and is outper-
formed by our model.

7 Conclusion

This work extended existing sequential self-
attentional models to lattice inputs, which have
been useful for various purposes in the past. We
achieve this by introducing probabilistic reacha-
bility masks and lattice positional encodings. Ex-
periments in a speech translation task show that
our method outperforms previous approaches and
is much faster than RNN-based alternatives in both
training and inference settings. Promising future
work includes extension to tree-structured inputs
and application to other tasks.

Acknowledgments

The work leading to these results has received
funding from the European Union under grant
agreement no 825460.

References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffry E. Hin-

ton. 2016. Layer Normalization. arXiv:1607.06450.

Dzmitry Bahdanau, KyungHyun Cho, and Yoshua
Bengio. 2015. Neural Machine Translation by
Jointly Learning to Align and Translate. In In-
ternational Conference on Representation Learning
(ICLR), San Diego, USA.

Joost Bastings, Ivan Titov, Wilker Aziz, Diego
Marcheggiani, and Khalil Sima’an. 2017. Graph
Convolutional Encoders for Syntax-aware Neural
Machine Translation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), Copenhagen,
Denmark.

Daniel Beck, Gholamreza Haffari, and Trevor Cohn.
2018. Graph-to-Sequence Learning using Gated
Graph Neural Networks. In Association for Com-
putational Linguistic (ACL), pages 273–283, Mel-
bourne, Australia.

Francisco Casacuberta, Hermann Ney, Franz Josef
Och, Enrique Vidal, J. M. Vilar, S. Barrachina,
I. Garcı́a-Varea, D. Llorens, C. Martı́nez, S. Mo-
lau, F. Nevado, M. Pastor, D. Picó, A. Sanchis, and
C. Tillmann. 2004. Some approaches to statistical
and finite-state speech-to-speech translation. Com-
puter Speech and Language, 18(1):25–47.

Alberto Cetoli, Stefano Bragaglia, Andrew D.
O’Harney, and Marc Sloan. 2017. Graph Convolu-
tional Networks for Named Entity Recognition. In

International Workshop on Treebanks and Linguis-
tic Theories (TLT16), pages 37–45, Prague, Czech
Republic.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
reading. In Empirical Methods in Natural Language
Processing (EMNLP), Austin, Texas, USA.

Michaël Defferrard, Xavier Bresson, and Pierre Van-
dergheynst. 2016. Convolutional Neural Networks
on Graphs with Fast Localized Spectral Filtering. In
Advances in Neural Information Processing Systems
(NIPS), pages 3844–3852, Barcelona, Spain.

David Duvenaud, Dougal Maclaurin, Jorge Aguilera-
Iparraguirre, Rafael Gómez-Bombarelli, Timothy
Hirzel, Alán Aspuru-Guzik, and Ryan P. Adams.
2015. Convolutional Networks on Graphs for
Learning Molecular Fingerprints. In Advances
in Neural Information Processing Systems (NIPS),
pages 2224–2232, Montréal, Canada.

Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing Word Lattice Trans-
lation. Technical Report LAMP-TR-149, Univer-
sity of Maryland, Institute For Advanced Computer
Studies.

Yarin Gal and Zoubin Ghahramani. 2016. A Theoreti-
cally Grounded Application of Dropout in Recurrent
Neural Networks. In Neural Information Process-
ing Systems Conference (NIPS), pages 1019–1027,
Barcelona, Spain.

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N. Dauphin. 2017. Convolutional
Sequence to Sequence Learning. In International
Conference on Machine Learning (ICML), Sydney,
Australia.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 770—-778, Las Ve-
gas, USA.

Robert L. Hemminger and Lowell W. Beineke. 1978.
Line graphs and line digraphs. In Selected Topics in
Graph Theory, pages 271–305. Academic Press Inc.

Jinbae Im and Sungzoon Cho. 2017. Distance-based
Self-Attention Network for Natural Language Infer-
ence. arXiv:1712.02047.

Steven Kearnes, Kevin McCloskey, Marc Berndl, Vi-
jay Pande, and Patrick Riley. 2016. Molecu-
lar Graph Convolutions: Moving Beyond Finger-
prints. Journal of Computer-Aided Molecular De-
sign, 30(8):595–608.

Diederik P. Kingma and Jimmy L. Ba. 2014. Adam:
A Method for Stochastic Optimization. In Inter-
national Conference on Learning Representations
(ICLR), Banff, Canada.

https://arxiv.org/pdf/1607.06450.pdf
http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1704.04675
http://arxiv.org/abs/1704.04675
http://arxiv.org/abs/1704.04675
http://aclweb.org/anthology/P18-1026
http://aclweb.org/anthology/P18-1026
https://www-i6.informatik.rwth-aachen.de/publications/download/174/CasacubertaFranciscoNeyHermannOchFranzJosefVidalEnriqueVilarJuanMiguelBarrachinaSergioGarc{%}257Bi{%}257Da-VareaIsmaelLlorensDavidMart{%}257Bi{%}257DnezC{%}257Be{%}257DsarMolauSirkoNevadoFranciscoPastorMois{%}257Be{%}257
https://www-i6.informatik.rwth-aachen.de/publications/download/174/CasacubertaFranciscoNeyHermannOchFranzJosefVidalEnriqueVilarJuanMiguelBarrachinaSergioGarc{%}257Bi{%}257Da-VareaIsmaelLlorensDavidMart{%}257Bi{%}257DnezC{%}257Be{%}257DsarMolauSirkoNevadoFranciscoPastorMois{%}257Be{%}257
http://arxiv.org/abs/1709.10053
http://arxiv.org/abs/1709.10053
https://aclweb.org/anthology/D16-1053
https://aclweb.org/anthology/D16-1053
https://papers.nips.cc/paper/6081-convolutional-neural-networks-on-graphs-with-fast-localized-spectral-filtering.pdf
https://papers.nips.cc/paper/6081-convolutional-neural-networks-on-graphs-with-fast-localized-spectral-filtering.pdf
https://papers.nips.cc/paper/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints.pdf
https://papers.nips.cc/paper/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints.pdf
https://apps.dtic.mil/dtic/tr/fulltext/u2/a481994.pdf
https://apps.dtic.mil/dtic/tr/fulltext/u2/a481994.pdf
http://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.pdf
http://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.pdf
http://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.pdf
http://arxiv.org/abs/1705.03122
http://arxiv.org/abs/1705.03122
https://doi.org/10.3389/fpsyg.2013.00124
https://doi.org/10.3389/fpsyg.2013.00124
http://arxiv.org/abs/1712.02047
http://arxiv.org/abs/1712.02047
http://arxiv.org/abs/1712.02047
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5028207/pdf/nihms-812806.pdf
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5028207/pdf/nihms-812806.pdf
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5028207/pdf/nihms-812806.pdf
https://arxiv.org/pdf/1412.6980.pdf
https://arxiv.org/pdf/1412.6980.pdf


1194

Thomas N. Kipf and Max Welling. 2017. Semi-
Supervised Classification with Graph Convolutional
Networks. International Conference on Learning
Representations (ICLR).

Faisal Ladhak, Ankur Gandhe, Markus Dreyer, Lam-
bert Mathias, Ariya Rastrow, and Björn Hoffmeis-
ter. 2016. LatticeRnn: Recurrent Neural Networks
over Lattices. In Annual Conference of the Inter-
national Speech Communication Association (Inter-
Speech), pages 695–699, San Francisco, USA.

Yujia Li, Richard Zemel, Mark Brockschmeidt, and
Daniel Tarlow. 2016. Gated Graph Sequence Neural
Networks. In International Conference on Learning
Representations (ICLR).

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A Structured Self-attentive Sentence
Embedding. In International Conference on Repre-
sentation Learning (ICLR), Toulon, France.

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective Approaches to Attention-
based Neural Machine Translation. In Empir-
ical Methods in Natural Language Processing
(EMNLP), pages 1412–1421, Lisbon, Portugal.

Diego Marcheggiani, Joost Bastings, and Ivan Titov.
2018. Exploiting Semantics in Neural Machine
Translation with Graph Convolutional Networks.
In North American Chapter of the Association for
Computational Linguistics (NAACL), pages 486–
492, New Orleans, USA.

Evgeny Matusov, Björn Hoffmeister, and Hermann
Ney. 2008. ASR word lattice translation with ex-
haustive reordering is possible. In Annual Con-
ference of the International Speech Communication
Association (InterSpeech), pages 2342–2345, Bris-
bane, Australia.

Roberto Navigli and Paola Velardi. 2010. Learning
Word-Class Lattices for Definition and Hypernym
Extraction. In Association for Computational Lin-
guistic (ACL), pages 1318–1327, Uppsala, Sweden.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji,
Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku-
mar, Chaitanya Malaviya, Paul Michel, Yusuke
Oda, Matthew Richardson, Naomi Saphra, Swabha
Swayamdipta, and Pengcheng Yin. 2017a. DyNet:
The Dynamic Neural Network Toolkit. arXiv
preprint arXiv:1701.03980.

Graham Neubig, Yoav Goldberg, and Chris Dyer.
2017b. On-the-fly Operation Batching in Dynamic
Computation Graphs. In Neural Information Pro-
cessing Systems Conference (NIPS), Long Beach,
USA.

Graham Neubig, Matthias Sperber, Xinyi Wang,
Matthieu Felix, Austin Matthews, Sarguna Pad-
manabhan, Ye Qi, Devendra Singh Sachan, Philip
Arthur, Pierre Godard, John Hewitt, Rachid Riad,
and Liming Wang. 2018. XNMT: The eXtensible
Neural Machine Translation Toolkit. In Conference
of the Association for Machine Translation in the
Americas (AMTA) Open Source Software Showcase,
Boston, USA.

Hermann Ney. 1999. Speech Translation: Coupling of
Recognition and Translation. In International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP), pages 517–520, Phoenix, USA.

Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A Decomposable Atten-
tion Model for Natural Language Inference. In Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 2249–2255, Austin, USA.

Matt Post, Gaurav Kumar, Adam Lopez, Damianos
Karakos, Chris Callison-Burch, and Sanjeev Khu-
danpur. 2013. Improved Speech-to-Text Transla-
tion with the Fisher and Callhome Spanish–English
Speech Translation Corpus. In International Work-
shop on Spoken Language Translation (IWSLT),
Heidelberg, Germany.

Shirin Saleem, Szu-Chen Jou, Stephan Vogel, and
Tanja Schultz. 2004. Using Word Lattice Informa-
tion for a Tighter Coupling in Speech Translation
Systems. In International Conference on Spoken
Language Processing (ICSLP), pages 41–44, Jeju Is-
land, Korea.

Anna Senina, Marcus Rohrbach, Wei Qiu, Annemarie
Friedrich, Sikandar Amin, Mykhaylo Andriluka,
Manfred Pinkal, and Bernt Schiele. 2014. Coherent
multi-sentence video description with variable level
of detail. In German Conference on Pattern Recog-
nition (GCPR), pages 184–195, Münster, Germany.
Springer.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2018. DiSAN:
Directional Self-Attention Network for RNN/CNN-
free Language Understanding. In Conference on Ar-
tificial Intelligence (AAAI), New Orleans, USA.

Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive Deep Mod-
els for Semantic Compositionality Over a Senti-
ment Treebank. In Empirical Methods in Natural
Language Processing (EMNLP), pages 1631–1642,
Seattle, USA.

Kaitao Song, Xu Tan, Furong Peng, and Jianfeng Lu.
2018. Hybrid Self-Attention Network for Machine
Translation. arXiv:1811.00253v2.

Matthias Sperber, Graham Neubig, Jan Niehues, and
Alex Waibel. 2017. Neural Lattice-to-Sequence
Models for Uncertain Inputs. In Conference on

https://arxiv.org/pdf/1609.02907.pdf
https://arxiv.org/pdf/1609.02907.pdf
https://arxiv.org/pdf/1609.02907.pdf
http://amazon.jobs-public-documents.s3.amazonaws.com/Lattice{_}Interspeech{_}Final.pdf
http://amazon.jobs-public-documents.s3.amazonaws.com/Lattice{_}Interspeech{_}Final.pdf
http://arxiv.org/abs/arXiv:1511.05493v4
http://arxiv.org/abs/arXiv:1511.05493v4
http://arxiv.org/abs/1703.03130
http://arxiv.org/abs/1703.03130
http://aclweb.org/anthology/D15-1166
http://aclweb.org/anthology/D15-1166
http://arxiv.org/abs/1804.08313
http://arxiv.org/abs/1804.08313
https://pdfs.semanticscholar.org/18b2/aa78d8e9e5672e7f9b41edf6b110ea429818.pdf
https://pdfs.semanticscholar.org/18b2/aa78d8e9e5672e7f9b41edf6b110ea429818.pdf
http://www.anthology.aclweb.org/P/P10/P10-1134.pdf
http://www.anthology.aclweb.org/P/P10/P10-1134.pdf
http://www.anthology.aclweb.org/P/P10/P10-1134.pdf
https://arxiv.org/pdf/1701.03980.pdf
https://arxiv.org/pdf/1701.03980.pdf
http://arxiv.org/abs/1705.07860
http://arxiv.org/abs/1705.07860
https://arxiv.org/pdf/1803.00188.pdf
https://arxiv.org/pdf/1803.00188.pdf
http://www.mirlab.org/conference{_}papers/International{_}Conference/ICASSP 1999/PDF/AUTHOR/IC991675.PDF
http://www.mirlab.org/conference{_}papers/International{_}Conference/ICASSP 1999/PDF/AUTHOR/IC991675.PDF
https://aclweb.org/anthology/D16-1244
https://aclweb.org/anthology/D16-1244
http://cs.jhu.edu/{~}gkumar/papers/post2013improved.pdf
http://cs.jhu.edu/{~}gkumar/papers/post2013improved.pdf
http://cs.jhu.edu/{~}gkumar/papers/post2013improved.pdf
https://pdfs.semanticscholar.org/f205/07214ccb0512400fd68be78acb47106fb3b2.pdf
https://pdfs.semanticscholar.org/f205/07214ccb0512400fd68be78acb47106fb3b2.pdf
https://pdfs.semanticscholar.org/f205/07214ccb0512400fd68be78acb47106fb3b2.pdf
https://doi.org/10.1007/978-3-319-11752-2_15
https://doi.org/10.1007/978-3-319-11752-2_15
https://doi.org/10.1007/978-3-319-11752-2_15
http://arxiv.org/abs/1709.04696
http://arxiv.org/abs/1709.04696
http://arxiv.org/abs/1709.04696
https://doi.org/10.1371/journal.pone.0073791
https://doi.org/10.1371/journal.pone.0073791
https://doi.org/10.1371/journal.pone.0073791
http://arxiv.org/abs/arXiv:1811.00253v2
http://arxiv.org/abs/arXiv:1811.00253v2
http://aclweb.org/anthology/D17-1145
http://aclweb.org/anthology/D17-1145


1195

Empirical Methods in Natural Language Process-
ing (EMNLP), pages 1380–1389, Copenhagen, Den-
mark.

Matthias Sperber, Jan Niehues, Graham Neubig, Se-
bastian Stüker, and Alex Waibel. 2018. Self-
Attentional Acoustic Models. In Annual Conference
of the International Speech Communication Associ-
ation (InterSpeech), Hyderabad, India.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A Simple Way to Prevent Neural Networks
from Overfitting. Journal of Machine Learning Re-
search, 15(1):1929–1958.

Jinsong Su, Zhixing Tan, Deyi Xiong, Rongrong Ji,
Xiaodong Shi, and Yang Liu. 2017. Lattice-Based
Recurrent Neural Network Encoders for Neural Ma-
chine Translation. In Conference on Artificial In-
telligence (AAAI), pages 3302–3308, San Francisco,
USA.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jon Shlens, and Zbigniew Wojna. 2016. Rethinking
the Inception Architecture for Computer Vision. In
Computer Vision and Pattern Recognition (CVPR),
pages 2818–2826, Las Vegas, USA.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved Semantic Representa-
tions From Tree-Structured Long Short-Term Mem-
ory Networks. In Association for Computational
Linguistic (ACL), pages 1556–1566, Beijing, China.

Shikhar Vashishth, Shib Sankar Dasgupta,
Swayambhu Nath Ray, and Partha Talukdar.
2018. Dating Documents using Graph Convolution
Networks. In Association for Computational
Linguistic (ACL), pages 1605–1615, Melbourne,
Australia.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention Is All
You Need. In Neural Information Processing Sys-
tems Conference (NIPS), pages 5998–6008, Long
Beach, USA.

Petar Veličković, Guillem Cucurull, Arantxa Casanova,
Adriana Romero, Pietro Liò, and Yoshua Bengio.
2018. Graph Attention Networks. In International
Conference on Learning Representations (ICLR),
Vancouver, Canada.

Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a Foreign Language. In Neural Information
Processing Systems Conference (NIPS), Montréal,
Canada.

Ruiqiang Zhang, Genichiro Kikui, Hirofumi Ya-
mamoto, and Wai-Kit Lo. 2005. A Decoding Al-
gorithm for Word Lattice Translation in Speech
Translation. In International Workshop on Spoken
Language Translation (IWSLT), pages 23–29, Pitts-
burgh, USA.

Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015. Long Short-Term Memory Over Recursive
Structures. In International Conference on Machine
Learning (ICML), pages 1604–1612, Lille, France.

http://arxiv.org/abs/1803.09519
http://arxiv.org/abs/1803.09519
http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm{_}content=buffer79b43{&}utm{_}medium=social{&}utm{_}source=twitter.com{&}utm{_}campaign=buffer
http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm{_}content=buffer79b43{&}utm{_}medium=social{&}utm{_}source=twitter.com{&}utm{_}campaign=buffer
http://arxiv.org/abs/1609.07730
http://arxiv.org/abs/1609.07730
http://arxiv.org/abs/1609.07730
https://www.cv-foundation.org/openaccess/content{_}cvpr{_}2016/papers/Szegedy{_}Rethinking{_}the{_}Inception{_}CVPR{_}2016{_}paper.pdf
https://www.cv-foundation.org/openaccess/content{_}cvpr{_}2016/papers/Szegedy{_}Rethinking{_}the{_}Inception{_}CVPR{_}2016{_}paper.pdf
https://www.aclweb.org/anthology/P15-1150
https://www.aclweb.org/anthology/P15-1150
https://www.aclweb.org/anthology/P15-1150
http://aclweb.org/anthology/P18-1149
http://aclweb.org/anthology/P18-1149
http://arxiv.org/abs/1706.03762
http://arxiv.org/abs/1706.03762
http://arxiv.org/abs/arXiv:1710.10903v3
https://doi.org/10.1146/annurev.neuro.26.041002.131047
https://doi.org/10.1146/annurev.neuro.26.041002.131047
https://pdfs.semanticscholar.org/d68f/095d977841508df5e375a8a183a5e48de8b6.pdf
https://pdfs.semanticscholar.org/d68f/095d977841508df5e375a8a183a5e48de8b6.pdf
https://pdfs.semanticscholar.org/d68f/095d977841508df5e375a8a183a5e48de8b6.pdf
http://proceedings.mlr.press/v37/zhub15.pdf
http://proceedings.mlr.press/v37/zhub15.pdf


1196

A Path Duplication Invariance

Figure 5 shows a sequential lattice, and a lattice
derived from it but with a duplicated path. Seman-
tically, both are equivalent, and should therefore
result in identical neural representations. Note that
while in practice duplicated paths should not oc-
cur, paths with partial overlap are quite frequent.
It is therefore instructive to consider this hypo-
thetical situation. Below, we demonstrate that
the binary masking approach (§ 4.1.1) is biased
such that computed representations are impacted
by path duplication. In contrast, the probabilistic
approach (§ 4.1.2) is invariant to path duplication.

We consider the example of Figure 5, discussing
only the forward direction, because the lattice is
symmetric and computations for the backward di-
rection are identical. We follow notation of Equa-
tions 1 through 3, using 〈a,b〉 as abbrevation for
f (q (xa) , k (xb)) and va to abbreviate v(xa). Let
us consider the computed representation for the
node S as query. For the sequential lattice with
binary mask, it is:

yS =
1

C

(
e〈S,S〉vS + e

〈S,a〉va + e
〈S,b〉vb

)
(11)

Here, C is the softmax normalization term that en-
sures that exponentiated similarities sum up to 1.

In contrast, the lattice with duplication results
in a doubled influence of va:

yS =
1

C

(
e〈S,S〉vS + e

〈S,a〉va

+ e〈S,a’〉va’ + e
〈S,E〉vE

)
=

1

C

(
e〈S,S〉vS + 2e

〈S,a〉va + e
〈S,E〉vE

)
.

The probabilistic approach yields the same re-
sult as the binary approach for the sequential lat-
tice (Equation 11). For the lattice with path dupli-
cation, the representation for the node S is com-

a

S E

a
1

p 1

1-p
‘

aS E
1 1

sequential S a E

S 1 1 1
a 0 1 1
E 0 0 1

duplicated S a a’ E

S 1 p (1− p) 1
a 0 1 0 1
a’ 0 0 1 1
E 0 0 0 1

Figure 5: A sequential lattice, and a variant with a du-
plicated path, where nodes a and a’ are labeled with
the same word token. The matrices contain pairwise
reaching probabilities in forward direction, where rows
are queries, columns are keys.

puted as follows:

yS =
1

C

(
e〈S,S〉vS + e

〈S,a〉+log pva

+ e〈S,a’〉+log(1−p)va’ + e
〈S,E〉vE

)
=

1

C

(
e〈S,S〉vS + e

〈S,a〉elog pva

+ e〈S,a’〉elog(1−p)va’ + e
〈S,E〉vE

)
=

1

C

(
e〈S,S〉vS + pe

〈S,a〉va

+ (1− p)e〈S,a’〉va’ + e〈S,E〉vE
)

=
1

C

(
e〈S,S〉vS + e

〈S,a〉va + e
〈S,E〉vE

)
.

The result is the same as in the semantically
equivalent sequential case (Equation 11), the com-
putation is therefore invariant to path duplica-
tion. The same argument can be extended to other
queries, to other lattices with duplicated paths, as
well as to the lattice-biased encoder-decoder atten-
tion.

B Qualitative Analysis

We conduct a manual inspection and showcase
several common patterns in which the lattice in-
put helps improve translation quality, as well as
one counter example. In particular, we compare
the outputs of the sequential and lattice models ac-
cording to the 3rd and the last row in Table 1, on
Fisher.



1197

B.1 Example 1

In this example, the ASR 1-best contains a bad
word choice (quedar instead of qué tal). The
correct word is in the lattice, and can be disam-
biguated by exploiting long-range self-attentional
encoder context.

gold transcript: Qué tal, eh, yo soy Guillermo,
¿Cómo estás?

ASR 1-best: quedar eh yo soy guillermo cómo
estás

seq2seq output: stay eh i ’ m guillermo how are
you

ASR lattice:

quedar

S

.7

.2
que dar

qué tal

…
…

….1
1

1

lat2seq output: how are you eh i ’ m guillermo
how are you

B.2 Example 2

Here, the correct word graduar does not appear
in the lattice, instead the lattice offers many in-
correct alternatives of high uncertainty. The trans-
lation model evidently goes with a linguistically
plausible guess, ignoring the source side.

gold transcript: Claro Es, eh, eh, o sea, yo me,
me voy a graduar con un tı́tulo de esta uni-
versidad.

ASR 1-best: claro existe eh o sea yo me me puedo
habar con un tı́tulo esta universidad

seq2seq output: sure it exists i mean i can talk
with a title

ASR lattice:

quedar

S

.7

.2
que dar

qué tal

…
…

….1
1

1

puedo

voy

habar

ahora

a hablar

grabar

lavar

…

…

…
…

…1
.1

.9
.1
.7

.2

lat2seq output: sure i mean i ’ m going to take a
university title

B.3 Example 3
In this example, o sea (I mean) appears with
slightly lower confidence than saben (they know),
but is chosen for a more natural sounding target
sentence

gold transcript: No, o sea, eso es eh, cları́simo
para mi

ASR 1-best: no saben eso es eh cları́simo para
mi

seq2seq output: they don ’ t know that ’ s eh sure
for me

ASR lattice:

quedar

S

.7

.2
que dar

qué tal

…
…

….1
1

1

puedo

voy

habar

ahora

a hablar

grabar

lavar

…

…

…
…

…1
.1

.9
.1
.7

.2

o

S

.34
.37 sabenno

…

sea …
…

.29

lat2seq output: no i mean that ’ s very clear for
me

B.4 Counter Example
In this counter example, the translation model gets
confused from the additional and wrong lattice
context and no longer produces the correct output.

gold transcript: sı́

ASR 1-best: sı́

seq2seq output: yes

ASR lattice:

quedar

S

.7

.2
que dar

qué tal

…
…

….1
1

1

puedo

voy

habar

ahora

a hablar

grabar

lavar

…

…

…
…

…1
.1

.9
.1
.7

.2

o

S

.34
.37 sabenno

…

sea …
…

.29

S

.107

mhm

mm

sí

.392 E

.502

lat2seq output: mm


