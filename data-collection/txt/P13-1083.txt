Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 841–851,

Soﬁa, Bulgaria, August 4-9 2013. c(cid:13)2013 Association for Computational Linguistics

841

Part-of-Speech Induction in Dependency Trees for Statistical Machine

Translation

Akihiro Tamura†,‡, Taro Watanabe†, Eiichiro Sumita†,

Hiroya Takamura‡, Manabu Okumura‡

{akihiro.tamura, taro.watanabe, eiichiro.sumita}@nict.go.jp

† National Institute of Information and Communications Technology
† Precision and Intelligence Laboratory, Tokyo Institute of Technology

{takamura, oku}@pi.titech.ac.jp

Abstract

This paper proposes a nonparametric
Bayesian method for inducing Part-of-
Speech (POS) tags in dependency trees
to improve the performance of statistical
machine translation (SMT). In particular,
we extend the monolingual inﬁnite tree
model (Finkel et al., 2007) to a bilin-
gual scenario: each hidden state (POS tag)
of a source-side dependency tree emits a
source word together with its aligned tar-
get word, either jointly (joint model), or
independently (independent model). Eval-
uations of Japanese-to-English translation
on the NTCIR-9 data show that our in-
duced Japanese POS tags for dependency
trees improve the performance of a forest-
to-string SMT system. Our independent
model gains over 1 point in BLEU by re-
solving the sparseness problem introduced
in the joint model.

1 Introduction

In recent years, syntax-based SMT has made
promising progress by employing either depen-
dency parsing (Lin, 2004; Ding and Palmer, 2005;
Quirk et al., 2005; Shen et al., 2008; Mi and Liu,
2010) or constituency parsing (Huang et al., 2006;
Liu et al., 2006; Galley et al., 2006; Mi and Huang,
2008; Zhang et al., 2008; Cohn and Blunsom,
2009; Liu et al., 2009; Mi and Liu, 2010; Zhang
et al., 2011) on the source side, the target side,
or both. However, dependency parsing, which
is a popular choice for Japanese, can incorporate
only shallow syntactic information, i.e., POS tags,
compared with the richer syntactic phrasal cate-
gories in constituency parsing. Moreover, exist-
ing POS tagsets might not be optimal for SMT
because they are constructed without considering
the language in the other side. Consider the ex-
amples in Figure 1. The Japanese noun “利用” in

Figure 1: Examples of Existing Japanese POS
Tags and Dependency Structures

Example 1 corresponds to the English verb “use”,
while that in Example 2 corresponds to the English
noun “usage”. Thus, Japanese nouns act like verbs
in English in one situation, and nouns in English
in another. If we could discriminate POS tags for
two cases, we might improve the performance of a
Japanese-to-English SMT system.

In the face of the above situations,

this pa-
per proposes an unsupervised method for inducing
POS tags for SMT, and aims to improve the perfor-
mance of syntax-based SMT by utilizing the in-
duced POS tagset. The proposed method is based
on the inﬁnite tree model proposed by Finkel et
al. (2007), which is a nonparametric Bayesian
method for inducing POS tags from syntactic de-
pendency structures. In this model, hidden states
represent POS tags, the observations they generate
represent the words themselves, and tree structures
represent syntactic dependencies between pairs of
POS tags.

The proposed method builds on this model by
incorporating the aligned words in the other lan-
guage into the observations. We investigate two
types of models: (i) a joint model and (ii) an in-
dependent model.
In the joint model, each hid-
den state jointly emits both a source word and its
aligned target word as an observation. The in-
dependent model separately emits words in two
languages from hidden states. By inferring POS

[Example 1]

Japanese POS:

[Example 2]

Japanese POS:

あなた は インターネット が 利用利用利用利用 でき ない
noun particle

particle

noun verb auxiliary verb

noun

You can not use the Internet  .

私 が 利用利用利用利用 料金 を 払う
verb

noun particle noun noun

particle

I  pay  usage fees  .

842

tags based on bilingual observations, both mod-
els can induce POS tags by incorporating infor-
mation from the other language. Consider, for ex-
ample, inducing a POS tag for the Japanese word “
利用” in Figure 1. Under a monolingual induction
method (e.g., the inﬁnite tree model), the “利用”
in Example 1 and 2 would both be assigned the
same POS tag since they share the same observa-
tion. However, our models would assign separate
tags for the two different instances since the “利
用” in Example 1 and Example 2 could be disam-
biguated by encoding the target-side information,
either “use” or “usage”, in the observations.

Inference is efﬁciently carried out by beam sam-
pling (Gael et al., 2008), which combines slice
sampling and dynamic programming. Experi-
ments are carried out on the NTCIR-9 Japanese-
to-English task using a binarized forest-to-string
SMT system with dependency trees as its source
side.
Our bilingually-induced tagset signiﬁ-
cantly outperforms the original
tagset and the
monolingually-induced tagset. Further, our inde-
pendent model achieves a more than 1 point gain
in BLEU, which resolves the sparseness problem
introduced by the bi-word observations.

2 Related Work

A number of unsupervised methods have been
proposed for inducing POS tags. Early methods
have the problem that the number of possible POS
tags must be provided preliminarily. This limita-
tion has been overcome by automatically adjust-
ing the number of possible POS tags using non-
parametric Bayesian methods (Finkel et al., 2007;
Gael et al., 2009; Blunsom and Cohn, 2011; Sirts
and Alum¨ae, 2012). Gael et al. (2009) applied
inﬁnite HMM (iHMM) (Beal et al., 2001; Teh
et al., 2006), a nonparametric version of HMM,
to POS induction. Blunsom and Cohn (2011)
used a hierarchical Pitman-Yor process prior to the
transition and emission distribution for sophisti-
cated smoothing. Sirts and Alum¨ae (2012) built a
model that combines POS induction and morpho-
logical segmentation into a single learning prob-
lem. Finkel et al. (2007) proposed the inﬁnite
tree model, which represents recursive branching
structures over inﬁnite hidden states and induces
POS tags from syntactic dependency structures. In
the following, we overview the inﬁnite tree model,
which is the basis of our proposed model. In par-
ticular, we will describe the independent children

Figure 2: A Graphical Representation of the Finite
Tree Model

model (Finkel et al., 2007), where children are
dependent only on their parents, used in our pro-
posed model1.

2.1 Finite Tree Model
We ﬁrst review the ﬁnite tree model, which can
be graphically represented in Figure 2.
Let
Tt denote the tree whose root node is t. A
node t has a hidden state zt
(the POS tag)
and an observation xt (the word). The prob-
is recursively de-
ability of a tree Tt, pT (Tt),
ﬁned:
p(zt′|zt)pT (Tt′),

pT (Tt) = p(xt|zt) ∏t′∈c(t)

where c(t) is the set of the children of t.

Let each hidden state variable have C possible
values indexed by k. For each state k, there is
a parameter ϕk which parameterizes the observa-
tion distribution for that state: xt|zt ∼ F (ϕzt). ϕk
is distributed according to a prior distribution H:
ϕk ∼ H.
Transitions between states are governed by
Markov dynamics parameterized by π, where
πij = p(zc(t) = j|zt = i) and πk are the transition
probabilities from the parent’s state k. πk is dis-
tributed according to a Dirichlet distribution with
parameter ρ: πk|ρ ∼ Dirichlet(ρ, . . . , ρ). The
hidden state of each child zt′ is distributed accord-
ing to a multinomial distribution πzt speciﬁc to the
parent’s state zt: zt′|zt ∼ Multinomial(πzt).
2.2 Inﬁnite Tree Model
In the inﬁnite tree model, the number of possible
hidden states is potentially inﬁnite. The inﬁnite
model is formed by extending the ﬁnite tree model
using a hierarchical Dirichlet process (HDP) (Teh
et al., 2006). The reason for using an HDP rather
1Finkel et al. (2007) originally proposed three types of
models: besides the independent children model, the simul-
taneous children model and the markov children model. Al-
though we could apply the other two models, we leave this
for future work.

ρ

ππππk

z1

H φk

k=1,…,C

x1

Dirichlet

(

r

r

)

,...,

p
f

r
~|

k

Hk

~

z2

x2

z3

x3

843

Figure 3: A Graphical Representation of the Inﬁ-
nite Tree Model

than a simple Dirichlet process (DP)2 (Ferguson,
1973) is that we have to introduce coupling across
transitions from different parent’s states. A similar
measure was adopted in iHMM (Beal et al., 2001).
HDP is a set of DPs coupled through a shared
random base measure which is itself drawn from
a DP: each Gk ∼ DP(α0, G0) with a shared base
measure G0, and G0 ∼ DP(γ, H) with a global
base measure H. From the viewpoint of the stick-
breaking construction3
(Sethuraman, 1994), the

HDP is interpreted as follows: G0 =

∞∑k′=1

βk′δϕk′

∞∑k′=1

πkk′δϕk′

, where β ∼ GEM(γ),

and Gk =
πk ∼ DP(α0, β), and ϕk′ ∼ H.
We regard each Gk as two coindexed distribu-
tions: πk, a distribution over the transition prob-
abilities from the parent’s state k, and ϕk′, an ob-
servation distribution for the state k′. Then, the
inﬁnite tree model is formally deﬁned as follows:

β|γ ∼ GEM(γ),
πk|α0, β ∼ DP(α0, β),
ϕk ∼ H,
zt′|zt ∼ Multinomial(πzt),
xt|zt ∼ F (ϕzt).

Figure 3 shows the graphical representation of the
inﬁnite tree model. The primary difference be-

2DP is a measure on measures. It has two parameters, a

scaling parameter α and a base measure H: DP (α, H).

3Sethuraman (1994) showed a deﬁnition of a measure
G ∼ DP(α0, G0). First, inﬁnite sequences of i.i.d variables
(π′k)∞k=1 and (ϕk)∞k=1 are generated: π′k|α0 ∼ Beta(1, α0),
ϕk ∼ G0. Then, G is deﬁned as: πk = π′k∑k−1
l=1 (1 − π′l),
G =∑∞k=1 πkδϕk . If π is deﬁned by this process, then we

write π ∼ GEM(α0).

Figure 4: An Example of the Joint Model

tween Figure 2 and Figure 3 is whether the number
of copies of the state is ﬁnite or not.

3 Bilingual Inﬁnite Tree Model

We propose a bilingual variant of the inﬁnite tree
model, the bilingual inﬁnite tree model, which uti-
lizes information from the other language. Speciﬁ-
cally, the proposed model introduces bilingual ob-
servations by embedding the aligned target words
in the source-side dependency trees. This paper
proposes two types of models that differ in their
processes for generating observations:
the joint
model and the independent model.

3.1 Joint Model

The joint model is a simple application of the in-
ﬁnite tree model under a bilingual scenario. The
model is formally deﬁned in the same way as in
Section 2.2 and is graphically represented simi-
larly to Figure 3. The only difference from the
inﬁnite tree model is the instances of observations
(xt). Observations in the joint model are the com-
bination of source words and their aligned target
words4, while observations in the monolingual in-
ﬁnite tree model represent only source words. For
each source word, all the aligned target words are
copied and sorted in alphabetical order, and then
concatenated into a single observation. Therefore,
a single target word may be emitted multiple times
if the target word is aligned with multiple source
words. Likewise, there may be target words which
may not be emitted by our model, if the target
words are not aligned.

Figure 4 shows the process of generating Exam-
ple 2 in Figure 1 through the joint model, where
aligned words are jointly emitted as observations.
In Figure 4, the POS tag of “利用” (z5) generates
4When no target words are aligned, we simply add a

NULL target word.

γ ββββ

g
)(
a
b
(DP~,

gb
GEM~|
a
p
f

H

|

0

k

~

k

b

)

,

0

α0

ππππk

z1

z2

H φk

∞

x1

x2

z3

x3

γ ββββ z1

z2

z3

z4

z5

z6

“払う
+pay”

“私+I”

“が”

“料金
+fees”

“利用
+usage”

“を”

ππππkα0

H φk

∞

844

3.3 Introduction of Other Factors

We assumed the surface form of aligned target
words as additional observations in previous sec-
tions. Here, we introduce additional factors, i.e.,
the POS of aligned target words, in the observa-
tions. Note that POSs of target words are assigned
by a POS tagger in the target language and are not
inferred in the proposed model.

First, we can simply replace surface forms of
target words with their POSs to overcome the
sparseness problem. Second, we can incorporate
both information from the target language as ob-
servations. In the joint model, two pieces of in-
formation are concatenated into a single observa-
tion. In the independent model, we introduce ob-
servation variables (e.g., x′t and x′′t ) and parame-
ters (e.g., ϕ′k and ϕ′′k) for each information. Specif-
ically, x′t and ϕ′k are introduced for the surface
form of aligned words, and x′′t and ϕ′′k for the POS
of aligned words. Consider, for example, Example
1 in Figure 1. The POS tag of “利用” generates the
string “利用+use+verb” as the observation in the
joint model, while it generates “利用”, “use”, and
“verb” independently in the independent model.

3.4 POS Reﬁnement

We have assumed a completely unsupervised way
of inducing POS tags in dependency trees. An-
other realistic scenario is to reﬁne the existing POS
tags (Finkel et al., 2007; Liang et al., 2007) so
that each reﬁned sub-POS tag may reﬂect the in-
formation from the aligned words while preserv-
ing the handcrafted distinction from original POS
tagset. Major difference is that we introduce sep-
arate transition probabilities πs
k and observation
distributions (ϕs
k ) for each existing POS tag s.
Then, each node t is constrained to follow the dis-
tributions indicated by the initially assigned POS
tag st, and we use the pair (st, zt) as a state repre-
sentation.

k, ϕ′s

3.5 Inference

In inference, we ﬁnd the state set that maximizes
the posterior probability of state transitions given
observations (i.e., P (z1:n|x1:n)). However, we
cannot evaluate the probability for all possible
states because the number of states is inﬁnite.
Finkel et al. (2007) presented a sampling algo-
rithm for the inﬁnite tree model, which is based on
the Gibbs sampling in the direct assignment rep-
resentation for iHMM (Teh et al., 2006). In the

Figure 5: A Graphical Representation of the Inde-
pendent Model

the string “利用+usage” as the observation (x5).
Similarly, the POS tag of “利用” in Example 1
would generate the string “利用+use”. Hence, this
model can assign different POS tags to the two dif-
ferent instances of the word “利用”, based on the
different observation distributions in inference.

3.2 Independent Model
The joint model is prone to a data sparseness prob-
lem, since each observation is a combination of a
source word and its aligned target word. Thus, we
propose an independent model, where each hidden
state generates a source word and its aligned target
word separately. For the aligned target side, we in-
troduce an observation variable x′t for each zt and
a parameter ϕ′k for each state k, which parame-
terizes a distinct distribution over the observations
x′t for that state. ϕ′k is distributed according to a
prior distribution H′. Speciﬁcally, the indepen-
dent model is formally deﬁned as follows:

β|γ ∼ GEM(γ),
πk|α0, β ∼ DP(α0, β),
ϕk ∼ H, ϕ′k ∼ H′,
zt′|zt ∼ Multinomial(πzt),
xt|zt ∼ F (ϕzt), x′t|zt ∼ F ′(ϕ′zt).

When multiple target words are aligned to a single
source word, each aligned word is generated sepa-
rately from observation distribution parameterized
by ϕ′k.

Figure 5 graphs the process of generating Ex-
ample 2 in Figure 1 using the independent model.
x′t and ϕ′k are introduced for aligned target words.
The state of “利用” (z5) generates the Japanese
word “利用” as x5 and the English word “usage”
as x′5. Due to this factorization, the independent
model is less subject to the sparseness problem.

γ ββββ z1

ππππkα0

払う

pay

z2

z3

g
)(
a
b
(DP~,

gb
GEM~|
a
p
f

f

H

|

0

k

~

k

~'

k

H

'

,

b

)

,

0

z4

料金

fees

z5

z6

が NONE

利用

usage

を

NONE

H φk

私

I

H’ φ'k

∞

845

Gibbs sampling, individual hidden state variables
are resampled conditioned on all other variables.
Unfortunately, its convergence is slow in HMM
settings because sequential data is likely to have
a strong correlation between hidden states (Gael
et al., 2008).

We present an inference procedure based on
beam sampling (Gael et al., 2008) for the joint
model and the independent model. Beam sam-
pling limits the number of possible state transi-
tions for each node to a ﬁnite number using slice
sampling (Neal, 2003), and then efﬁciently sam-
ples whole hidden state transitions using dynamic
programming. Beam sampling does not suffer
from slow convergence as in Gibbs sampling by
sampling the whole state variables at once. In ad-
dition, Gael et al. (2008) showed that beam sam-
pling is more robust to initialization and hyperpa-
rameter choice than Gibbs sampling.

Speciﬁcally, we introduce an auxiliary variable
ut for each node in a dependency tree to limit
the number of possible transitions. Our procedure
alternates between sampling each of the follow-
ing variables: the auxiliary variables u, the state
assignments z, the transition probabilities π, the
shared DP parameters β, and the hyperparameters
α0 and γ. We can parallelize procedures in sam-
pling u and z because the slice sampling for u and
the dynamic programing for z are independent for
each sentence. See Gael el al. (2009) for details.

The only difference between inferences in the
joint model and the independent model is in com-
puting the posterior probability of state transi-
tions given observations (e.g., p(z1:n|x1:n) and
p(z1:n|x1:n, x′1:n)) in sampling z. In the follow-
ing, we describe each sampling stage. See Teh et
al., (2006) for details of sampling π, β, α0 and γ.

Sampling u:
Each ut is sampled from the uniform distribu-
tion on [0, πzd(t)zt], where d(t) is the parent of
t: ut ∼ Uniform(0, πzd(t)zt). Note that ut is a
positive number, since each transition probability
πzd(t)zt is larger than zero.
Sampling z:
Possible values k of zt are divided into the two
sets using ut: a ﬁnite set with πzd(t)k > ut and
an inﬁnite set with πzd(t)k ≤ ut. The beam
sampling considers only the former set. Owing
to the truncation of the latter set, we can compute
the posterior probability of a state zt given ob-

in

servations for all t (t = 1, . . . , T ) using dynamic
programming as follows:
In the joint model, p(zt|xσ(t), uσ(t)) ∝

p(zd(t)|xσ(d(t)), uσ(d(t))),
model,
independent

p(xt|zt) · ∑zd(t):πzd(t)zt >ut
and
p(zt|xσ(t), x′σ(t), uσ(t)) ∝ p(xt|zt) · p(x′t|zt)
· ∑zd(t):πzd(t)zt >ut

p(zd(t)|xσ(d(t)), x′σ(d(t)), uσ(d(t))),
where xσ(t) (or uσ(t)) denotes the set of xt (or ut)
on the path from the root node to the node t in a
tree.

the

In our experiments, we assume that F (ϕk)
is Multinomial(ϕk) and H is Dirichlet(ρ, . . . , ρ),
which is the same in Finkel et al. (2007). Un-
der this assumption, the posterior probability of an
˙nxtk + ρ
observation is as follows: p(xt|zt) =
,
˙n·k + N ρ
where ˙nxk is the number of observations x with
state k, ˙n·k is the number of hidden states whose
values are k, and N is the total number of observa-
˙nx′tk + ρ′
tions x. Similarly, p(x′t|zt) =
˙n·k + N′ρ′
N′ is the total number of observations x′.

, where

When the posterior probability of a state zt
given observations for all t can be computed,
we ﬁrst sample the state of each leaf node and
then perform backtrack sampling for every other
zt where the zt
is sampled given the sample
for zc(t) as follows: p(zt|zc(t), x1:T , u1:T ) ∝

p(zt|xσ(t), uσ(t))∏t′∈c(t) p(zt′|zt, ut′).

Sampling π:
We introduce a count variable nij ∈ n,
which is
the number of observations with
state j whose parent’s
Then,
sample π using the Dirichlet distri-
we
bution:
∼
Dirichlet(nk1
+
the

(πk1, . . . , πkK,∑∞k′=K+1 πkk′)
α0βK, α0∑∞k′=K+1 βk′), where K is

number of distinct states in z.

α0β1, . . . , nkK

state is

+

i.

Sampling β:
We introduce a set of auxiliary variables m, where
mij ∈ m is the number of elements of πj
corresponding to βi. The conditional distribu-
tion of each variable is p(mij = m|z, β, α0) ∝
S(nij, m)(α0βj)m, where S(n, m) are unsigned
Stirling numbers of the ﬁrst kind5.

5S(0, 0) = S(1, 1) = 1, S(n, 0) = 0 for n > 0,
S(n, m) = 0 for m > n, and S(n + 1, m) = S(n, m −
1) + nS(n, m) for others.

846

The parameters β are sampled using the Dirich-

let distribution:
Dirichlet(m·1, . . . , m·K, γ), where m·k

(β1, . . . , βK,∑∞k′=K+1 βk′) ∼

=

k′=1 mk′k.

∑K

)

Sampling α0:
α0 is parameterized by a gamma hyperprior
with hyperparameters αa and αb. We introduce
two types of auxiliary variables for each state
(k = 1, . . . , K), wk ∈ [0, 1] and vk ∈ {0, 1}.
is
The conditional distribution of each wk
p(wk|α0) ∝ wα0
k (1−wk)n·k−1 and that of each vk
n·k
k′=1 nk′k.
is p(vk|α0) ∝ (
α0
The conditional distribution of α0 given wk
and vk (k = 1, . . . , K)
is p(α0|w, v) ∝
k=1 logwk), where
α

vk, where n·k =∑K
e−α0(αb−∑K
k′′=1 mk′k′′.

αa−1+m..−∑K
m·· =∑K
k′=1∑K

Sampling γ:
γ is parameterized by a gamma hyperprior with
hyperparameters γa and γb. We introduce an
auxiliary variable η, whose conditional distribu-
tion is p(η|γ) ∝ ηγ(1 − η)m··−1. The con-
ditional distribution of γ given η is p(γ|η) ∝
γγa−1+Ke−γ(γb−logη).

k=1 vk

0

4 Experiment

We tested our proposed models under
the
NTCIR-9 Japanese-to-English patent translation
task (Goto et al., 2011), consisting of approxi-
mately 3.2 million bilingual sentences. Both the
development data and the test data consist of 2,000
sentences. We also used the NTCIR-7 develop-
ment data consisting of 2,741 sentences for devel-
opment testing purposes.

4.1 Experimental Setup

We evaluated our bilingual inﬁnite tree model
for POS induction using an in-house developed
syntax-based forest-to-string SMT system.
In
the training process, the following steps are per-
formed sequentially: preprocessing, inducing a
POS tagset for a source language, training a POS
tagger and a dependency parser, and training a
forest-to-string MT model.

Step 1. Preprocessing

We used the ﬁrst 10,000 Japanese-English sen-
tence pairs in the NTCIR-9 training data for in-

ducing a POS tagset for Japanese6. The Japanese
sentences were segmented using MeCab7, and the
English sentences were tokenized and POS tagged
using TreeTagger (Schmid, 1994), where 43 and
58 types of POS tags are included in the Japanese
sentences and the English sentences, respectively.
The Japanese POS tags come from the second-
level POS tags in the IPA POS tagset (Asahara and
Matsumoto, 2003) and the English POS tags are
derived from the Penn Treebank. Note that the
Japanese POS tags are used for initialization of
hidden states and the English POS tags are used
as observations emitted by hidden states.

Word-by-word alignments for

the sentence
pairs are produced by ﬁrst running GIZA++ (Och
and Ney, 2003) in both directions and then com-
bining the alignments using the “grow-diag-ﬁnal-
and” heuristic (Koehn et al., 2003). Note that we
ran GIZA++ on all of the NTCIR-9 training data
in order to obtain better alignements.

The Japanese sentences are parsed using
CaboCha (Kudo and Matsumoto, 2002), which
generates dependency structures using a phrasal
unit called a bunsetsu8, rather than a word unit as
in English or Chinese dependency parsing. Since
we focus on the word-level POS induction, each
bunsetsu-based dependency tree is converted into
its corresponding word-based dependency tree us-
ing the following heuristic9: ﬁrst, the last func-
tion word inside each bunsetsu is identiﬁed as
the head word10; then, the remaining words are
treated as dependents of the head word in the same
bunsetsu; ﬁnally, a bunsetsu-based dependency
structure is transformed to a word-based depen-
dency structure by preserving the head/modiﬁer
relationships of the determined head words.

Step 2. POS Induction
A POS tag for each word in the Japanese sentences
is inferred by our bilingual inﬁnite tree model, ei-

6Due to the high computational cost, we did not use all
the NTCIR-9 training data. We leave scaling up to a larger
dataset for future work.

7http://mecab.googlecode.com/svn/

trunk/mecab/doc/index.html

8A bunsetsu is the smallest meaningful sequence con-
sisting of a content word and accompanying function words
(e.g., a noun and a particle).

9We could use other word-based dependency trees such
as trees by the inﬁnite PCFG model (Liang et al., 2007)
and syntactic-head or semantic-head dependency trees in
Nakazawa and Kurohashi (2012), although it is not our major
focus. We leave this for future work.

10If no function words exist in a bunsetsu, the last content

word is treated as the head word.

847

ther jointly (Joint) or independently (Ind). We
also performed monolingual induction of Finkel et
al. (2007) for comparison (M ono). In each model,
a sequence of sampling u, z, π, β, α0, and γ is
repeated 10,000 times. In sampling α0 and γ, hy-
perparameters αa, αb, γa, and γb are set to 2, 1,
1, and 1, respectively, which is the same setting in
Gael et al. (2008). In sampling z, parameters ρ, ρ′,
. . ., are set to 0.01. In the experiments, three types
of factors for the aligned English words are com-
pared: surface forms (‘s’), POS tags (‘P’), and the
combination of both (‘s+P’). Further, two types of
inference frameworks are compared: induction
(IN D) and ref inement (REF ). In both frame-
works, each hidden state zt is ﬁrst initialized to
the POS tags assigned by MeCab (the IPA POS
tagset), and then each state is updated through
the inference procedure described in Section 3.5.
Note that in REF , the sampling distribution over
zt is constrained to include only states that are a
reﬁnement of the initially assigned POS tag.

Step 3. Training a POS Tagger and a
Dependency Parser
In this step, we train a Japanese dependency parser
from the 10,000 Japanese dependency trees with
the induced POS tags which are derived from Step
2. We employed a transition-based dependency
parser which can jointly learn POS tagging and
dependency parsing (Hatori et al., 2011) under an
incremental framework11. Note that the learned
parser can identify dependencies between words
and attach an induced POS tag for each word.

Step 4. Training a Forest-to-String MT
In this step, we train a forest-to-string MT model
based on the learned dependency parser in Step 3.
We use an in-house developed hypergraph-based
toolkit, cicada, for training and decoding with a
tree-to-string model, which has been successfully
employed in our previous work for system com-
bination (Watanabe and Sumita, 2011) and online
learning (Watanabe, 2012). All the Japanese and
English sentences in the NTCIR-9 training data
are segmented in the same way as in Step 1, and
then each Japanese sentence is parsed by the de-
pendency parser learned in Step 3, which simul-
taneously assigns induced POS tags and word de-
pendencies. Finally, a forest-to-string MT model
is learned with Zhang et al., (2011), which ex-
tracts translation rules by a forest-based variant of

11http://triplet.cc/software/corbit/

BS

M ono
Joint[s]
Joint[P]
Joint[s+P]

Ind[s]
Ind[P]
Ind[s+P]

IN D REF

27.54

27.66
28.00
26.36
27.99
28.00
28.11
28.13

26.83
28.00
26.72
27.82
27.93
28.63
28.62

Table 1: Performance on Japanese-to-English
Translation Measured by BLEU (%)

the GHKM algorithm (Mi and Huang, 2008) af-
ter each parse tree is restructured into a binarized
packed forest. Parameters are tuned on the devel-
opment data using xBLEU (Rosti et al., 2011) as
an objective and L-BFGS (Liu and Nocedal, 1989)
as an optimization toolkit, since it is stable and less
prone to randomness, unlike MERT (Och, 2003)
or PRO (Hopkins and May, 2011). The develop-
ment test data is used to set up hyperparameters,
i.e., to terminate tuning iterations.

When translating Japanese sentences, a parse
tree for each sentence is constructed in the same
way as described earlier in this step, and then the
parse trees are translated into English sentences
using the learned forest-to-string MT model.

4.2 Experimental Results
Table 1 shows the performance for the test data
measured by case sensitive BLEU (Papineni et
al., 2002). We also present the performance of
our baseline forest-to-string MT system (BS) us-
ing the original IPA POS tags. In Table 1, num-
bers in bold indicate that the systems outperform
the baselines, BS and M ono. Under the Moses
phrase-based SMT system (Koehn et al., 2007)
with the default settings, we achieved a 26.80%
BLEU score.

Table 1 shows that the proposed systems outper-
form the baseline M ono. The differences between
the performance of Ind[s+P] and M ono are statis-
tically signiﬁcant in the bootstrap method (Koehn,
2004), with a 1% signiﬁcance level both in IN D
and REF . The results indicate that integrating the
aligned target-side information in POS induction
makes inferred tagsets more suitable for SMT.

Table 1 also shows that the independent model
is more effective for SMT than the joint model.
This means that sparseness is a severe problem in

848

Model

Joint[s+P]
Ind[s+P]

IPA POS tags

IN D REF
164
620
517
102

42

Table 2: The Number of POS Tags

POS induction when jointly encoding bilingual in-
formation into observations. Additionally, all the
systems using the independent model outperform
BS. The improvements are statistically signiﬁcant
in the bootstrap method (Koehn, 2004), with a 1%
signiﬁcance level. The results show that the pro-
posed models can generate more favorable POS
tagsets for SMT than an existing POS tagset.

In Table 1, REF s are at least comparable to, or
better than, IN Ds except for M ono. This shows
that REF achieves better performance by preserv-
ing the clues from the original POS tagset. How-
ever, REF may suffer sever overﬁtting problem
for M ono since no bilingual information was in-
corporated. Further, when the full-level IPA POS
tags12 were used in BS, the system achieved a
27.49% BLEU score, which is worse than the re-
sult using the second-level IPA POS tags. This
means that manual reﬁnement without bilingual
information may also cause an overﬁtting problem
in MT.

5 Discussion

5.1 Comparison to the IPA POS Tagset
Table 2 shows the number of the IPA POS tags
used in the experiments and the POS tags induced
by the proposed models. This table shows that
each induced tagset contains more POS tags than
the IPA POS tagset.
In the experimental data,
some of Japanese verbs correspond to genuine En-
glish verbs, some are nominalized, and others cor-
respond to English past participle verbs or present
participle verbs which modify other words. Re-
spective examples are “I use a card.”, “U sing the
index is faster.”, and “I explain using an exam-
ple.”, where all the underlined words correspond
to the same Japanese word, “用い”, whose IPA
POS tag is a verb. Ind[s+P] in REF generated
the POS tagset where the three types are assigned
to separate POS groups.

The Japanese particle “に” is sometimes at-
tached to nouns to give them adverb roles. For
12377 types of full-level IPA POS tags were included in our

experimental data.

Tagging

Dependency
IN D REF IN D REF

90.37

93.62

90.75
89.08
80.54
87.56
87.62
90.21
89.57

88.04
86.73
79.98
84.92
84.33
88.50
86.12

91.77
91.55
91.06
91.31
92.06
92.85
92.96

91.51
91.14
91.29
91.10
92.58
93.03
92.78

Original

M ono
Joint[s]
Joint[P]
Joint[s+P]

Ind[s]
Ind[P]
Ind[s+P]

Table 3: Tagging and Dependency Accuracy (%)

example, “相互 (mutual) 　に” is translated as
the adverb “mutually” in English. Other times,
it is attached to words to make them the objects
of verbs. For example, “彼 (he) 　に　与える
(give)” is translated as “give him”. The POS tags
by Ind[s+P] in REF discriminated the two types.
These examples show that the proposed mod-
els can disambiguate POS tags that have different
functions in English, whereas the IPA POS tagset
treats them jointly. Thus, such discrimination im-
proves the performance of a forest-to-string SMT.

5.2 Impact of Tagging and Dependency

Accuracy

The performance of our methods depends not only
on the quality of the induced tag sets but also on
the performance of the dependency parser learned
in Step 3 of Section 4.1. We cannot directly eval-
uate the tagging accuracy of the parser trained
through Step 3 because we do not have any data
with induced POS tags other than the 10,000-
sentence data gained through Step 2. Thus we split
the 10,000 data into the ﬁrst 9,000 data for train-
ing and the remaining 1,000 for testing, and then
a dependency parser was learned in the same way
as in Step 3.

Table 3 shows the results. Original is the per-
formance of the parser learned from the training
data with the original POS tagset. Note that the de-
pendency accuracies are measured on the automat-
ically parsed dependency trees, not on the syntac-
tically correct gold standard trees. Thus Original
achieved the best dependency accuracy.

In Table 3, the performance for our bilingually-
induced POSs, Joint and Ind, are lower than
Original and M ono. It seems performing pars-
ing and tagging with the bilingually-induced POS
tagset is too difﬁcult when only monolingual in-

849

formation is available to the parser. However, our
bilingually-induced POSs, except for Joint[P ],
with the lower accuracies are more effective for
SMT than the monolingually-induced POSs and
the original POSs, as indicated in Table 1. The
tagging accuracies for Joint[P ] both in IN D and
REF are signiﬁcantly lower than the others, while
the dependency accuracies do not differ signiﬁ-
cantly. The lower tagging accuracies may directly
reﬂect the lower translation qualities for Joint[P ]
in Table 1.

6 Conclusion

We proposed a novel method for inducing POS
tags for SMT. The proposed method is a non-
parametric Bayesian method, which infers hidden
states (i.e., POS tags) based on observations repre-
senting not only source words themselves but also
aligned target words. Our experiments showed
that a more favorable POS tagset can be induced
by integrating aligned information, and further-
more, the POS tagset generated by the proposed
method is more effective for SMT than an existing
POS tagset (the IPA POS tagset).

Even though we employed word alignment
from GIZA++ with potential errors, large gains
were achieved using our proposed method. We
would like to investigate the inﬂuence of align-
ment errors in the future. In addition, we are plan-
ning to prove the effectiveness of our proposed
method for language pairs other than Japanese-to-
English. We are also planning to introduce our
proposed method to other syntax-based SMT, such
as a string-to-tree SMT and a tree-to-tree SMT.

Acknowledgments

We thank Isao Goto for helpful discussions and
anonymous reviewers for valuable comments. We
also thank Jun Hatori for helping us to apply his
software, Corbit, to our induced POS tagsets.

References
Masayuki Asahara and Yuji Matsumoto.

IPADIC User Manual. Technical report, Japan.

2003.

Matthew J. Beal, Zoubin Ghahramani, and Carl E. Ras-
mussen. 2001. The Inﬁnite Hidden Markov Model.
In Advances in Neural Information Processing Sys-
tems, pages 577–584.

Phil Blunsom and Trevor Cohn. 2011. A Hierarchical
Pitman-Yor Process HMM for Unsupervised Part of

Speech Induction.
In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, pages 865–874.

Trevor Cohn and Phil Blunsom. 2009. A Bayesian
Model of Syntax-Directed Tree to String Grammar
Induction. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 352–361.

Yuan Ding and Martha Palmer. 2005. Machine Trans-
lation Using Probabilistic Synchronous Dependency
Insertion Grammars. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics, pages 541–548.

Thomas S. Ferguson. 1973. A Bayesian Analysis
of Some Nonparametric Problems. The Annals of
Statistics, 1(2):209–230.

Jenny Rose Finkel, Trond Grenager, and Christo-
pher D. Manning. 2007. The Inﬁnite Tree. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 272–279.

Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam Sampling for
the Inﬁnite Hidden Markov Model. In Proceedings
of the 25th International Conference on Machine
Learning, pages 1088–1095.

Jurgen Van Gael, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The inﬁnite HMM for unsuper-
vised PoS tagging. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2 - Volume 2, pages 678–687.

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer.
2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models.
In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 961–968.

Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the Patent
Machine Translation Task at the NTCIR-9 Work-
shop. In Proceedings of the 9th NTCIR Workshop,
pages 559–578.

Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun’ichi Tsujii. 2011. Incremental Joint POS Tag-
ging and Dependency Parsing in Chinese.
In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 1216–1224.

Mark Hopkins and Jonathan May. 2011. Tuning as
Ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352–1362.

Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A Syntax-Directed Translator with Extended Do-
main of Locality. In Proceedings of the Workshop on

850

Computationally Hard Problemsand Joint Inference
in Speech and Language Processing, pages 1–8.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference: North American Chapter of the Associ-
ation for Computational Linguistics, pages 48–54.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constrantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics on In-
teractive Poster and Demonstration Sessions, pages
177–180.

Philipp Koehn. 2004. Statistical Signiﬁcance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 388–395.

Taku Kudo and Yuji Matsumoto. 2002. Japanese De-
pendency Analysis using Cascaded Chunking.
In
Proceedings of the 6th Conference on Natural Lan-
guage Learning, pages 63–69.

Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The Inﬁnite PCFG using Hierarchi-
cal Dirichlet Processes. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 688–697.

Dekang Lin. 2004. A Path-based Transfer Model for
Machine Translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguis-
tics, pages 625–630.

Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming B, 45(3):503–528.

Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine
Translation.
In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 609–616.

Yang Liu, Yajuan L¨u, and Qun Liu. 2009.

Improv-
ing Tree-to-Tree Translation with Packed Forests.
In Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics and the
4th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing, pages 558–566.

Haitao Mi and Liang Huang.

2008. Forest-based
Translation Rule Extraction. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 206–214.

Haitao Mi and Qun Liu. 2010. Constituency to De-
pendency Translation with Forests. In Proceedings
of the 48th Annual Conference of the Association for
Computational Linguistics, pages 1433–1442.

Toshiaki Nakazawa and Sadao Kurohashi.

2012.
Alignment by Bilingual Generation and Monolin-
gual Derivation.
In Proceedings of the 24th Inter-
national Conference on Computational Linguistics,
pages 1963–1978.

Radford M. Neal. 2003. Slice Sampling. Annals of

Statistics, 31:705–767.

Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29:19–51.

Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 160–167.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318.

Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency Treelet Translation: Syntactically In-
formed Phrasal SMT.
In Proceedings of the 43rd
Annual Conference of the Association for Computa-
tional Linguistics, pages 271–279.

Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz.
2011. Expected BLEU
Training for Graphs: BBN System Description for
WMT11 System Combination Task. In Proceedings
of the Sixth Workshop on Statistical Machine Trans-
lation, pages 159–165.

Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees.
In Proceedings of
the International Conference on New Methods in
Language Processing, pages 44–49.

Jayaram Sethuraman. 1994. A Constructive Deﬁnition
of Dirichlet Priors. Statistica Sinica, 4(2):639–650.

Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A New String-to-Dependency Machine Translation
Algorithm with a Target Dependency Language
Model. In Proceedings of the 46th Annual Confer-
ence of the Association for Computational Linguis-
tics: Human Language Technologies, pages 577–
585.

Kairit Sirts and Tanel Alum¨ae. 2012. A Hierarchi-
cal Dirichlet Process Model for Joint Part-of-Speech
and Morphology Induction.
In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 407–416.

851

Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2006. Hierarchical Dirichlet
Processes. Journal of the American Statistical Asso-
ciation, 101(476):1566–1581.

Taro Watanabe and Eiichiro Sumita. 2011. Machine
Translation System Combination by Confusion For-
est. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1249–1257.

Taro Watanabe. 2012. Optimized Online Rank Learn-
ing for Machine Translation. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 253–262.

Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A Tree Se-
quence Alignment-based Tree-to-Tree Translation
Model. In Proceedings of the 46th Annual Confer-
ence of the Association for Computational Linguis-
tics: Human Language Technologies, pages 559–
567.

Hao Zhang, Licheng Fang, Peng Xu, and Xiaoyun Wu.
2011. Binarized Forest to String Translation.
In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 19–24.

