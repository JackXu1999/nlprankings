











































Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4083–4093,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

4083

Achieving Verified Robustness to Symbol Substitutions
via Interval Bound Propagation

Po-Sen Huang† Robert Stanforth†§ Johannes Welbl‡§ Chris Dyer†
Dani Yogatama† Sven Gowal† Krishnamurthy Dvijotham† Pushmeet Kohli†

†DeepMind ‡University College London
{posenhuang, stanforth, cdyer, dyogatama, sgowal, dvij, pushmeet}@google.com

{j.welbl}@cs.ucl.ac.uk

Abstract

Neural networks are part of many contempo-
rary NLP systems, yet their empirical suc-
cesses come at the price of vulnerability to
adversarial attacks. Previous work has used
adversarial training and data augmentation to
partially mitigate such brittleness, but these
are unlikely to find worst-case adversaries due
to the complexity of the search space arising
from discrete text perturbations. In this work,
we approach the problem from the opposite di-
rection: to formally verify a system’s robust-
ness against a predefined class of adversarial
attacks. We study text classification under syn-
onym replacements or character flip perturba-
tions. We propose modeling these input per-
turbations as a simplex and then using Interval
Bound Propagation – a formal model verifica-
tion method. We modify the conventional log-
likelihood training objective to train models
that can be efficiently verified, which would
otherwise come with exponential search com-
plexity. The resulting models show only lit-
tle difference in terms of nominal accuracy,
but have much improved verified accuracy un-
der perturbations and come with an efficiently
computable formal guarantee on worst case ad-
versaries.

1 Introduction

Deep models have been shown to be vulnerable
against adversarial input perturbations (Szegedy
et al., 2013; Kurakin et al., 2016). Small, seman-
tically invariant input alterations can lead to dras-
tic changes in predictions, leading to poor perfor-
mance on adversarially chosen samples. Recent
work (Jia and Liang, 2017; Belinkov and Bisk,
2018; Ettinger et al., 2017) also exposed the vul-
nerabilities of neural NLP models, e.g. with small

‡Work done during an internship at DeepMind.
§Equal contribution.

fantastic

Co
nv

Re
LU FC FC

Input perturbations
Interval bounds

Propagated 
regions Decision boundary

logits

conference

occasion

good

    great event

great

event

Upper bounds

Figure 1: Illustration of verification with the input
simplex and Interval Bound Propagation. From the
left, input perturbations define the extreme points of
a simplex (in red, projected to 2D here) around the
statement “great event” that is propagated through a
model. At each layer, this shape deforms itself, but can
be bounded by axis-parallel bounding boxes, which are
propagated similarly. Finally, in logit space, we can
compute an upper bound on the worst-case specifica-
tion violation (e.g., prediction changes).

character perturbations (Ebrahimi et al., 2018) or
paraphrases (Ribeiro et al., 2018; Iyyer et al., 2018).
These adversarial attacks highlight often unintu-
itive model failure modes and present a challenge
to deploying NLP models.

Common attempts to mitigate the issue are ad-
versarial training (Ebrahimi et al., 2018) and data
augmentation (Belinkov and Bisk, 2018; Li et al.,
2017), which lead to improved accuracy on adver-
sarial examples. However, this might cause a false
sense of security, as there is generally no guarantee
that stronger adversaries could not circumvent de-
fenses to find other successful attacks (Carlini and
Wagner, 2017; Athalye et al., 2018; Uesato et al.,
2018). Rather than continuing the race with adver-
saries, formal verification (Baier and Katoen, 2008;
Barrett and Tinelli, 2018; Katz et al., 2017) offers
a different approach: it aims at providing provable
guarantees to a given model specification. In the
case of adversarial robustness, such a specification
can be formulated as prediction consistency under



4084

any altered – but semantically invariant – input
change.

In this paper, we study verifiable robustness, i.e.,
providing a certificate that for a given network
and test input, no attack or perturbation under the
specification can change predictions, using the ex-
ample of text classification tasks, Stanford Sen-
timent Treebank (SST) (Socher et al., 2013) and
AG News (Zhang et al., 2015). The specification
against which we verify is that a text classification
model should preserve its prediction under char-
acter (or synonym) substitutions in a character (or
word) based model. We propose modeling these
input perturbations as a simplex and then using
Interval Bound Propagation (IBP) (Gowal et al.,
2018; Mirman et al., 2018; Dvijotham et al., 2018)
to compute worst case bounds on specification sat-
isfaction, as illustrated in Figure 1. Since these
bounds can be computed efficiently, we can fur-
thermore derive an auxiliary objective for models
to become verifiable. The resulting classifiers are
efficiently verifiable and improve robustness on ad-
versarial examples, while maintaining comparable
performance in terms of nominal test accuracy.

The contributions of this paper are twofold:

• To the best of our knowledge, this paper is
the first to introduce verification and verifi-
able training for neural networks in natural
language processing (§3).

• Through a series of experiments (§4), we
demonstrate (a) the effectiveness of model-
ing input perturbations as a simplex and using
simplex bounds with IBP for training and test-
ing, (b) the weakness of adversarial training
under exhaustive verification, (c) the effects
of perturbation space on the performance of
different methods, and (d) the impact of using
GloVe and counter-fitted embeddings on the
IBP verification bounds.

2 Related Work

Adversarial Examples in NLP. Creating adver-
sarial examples for NLP systems requires identi-
fying semantically invariant text transformations
to define an input perturbation space. In this pa-
per, given our specification, we study word- and
character-level HotFlip attacks (Ebrahimi et al.,
2018) – which consist of character and synonym
replacements – on text classification tasks. We
compare our verifiable approach to other defenses

including adversarial training (Goodfellow et al.,
2014) and data augmentation (Li et al., 2017; Be-
linkov and Bisk, 2018). Note that some existing
adversarial perturbations such as syntactically con-
trolled paraphrasing (Iyyer et al., 2018), exploiting
backtranslation systems (Ribeiro et al., 2018), or
using targeted keyword attack (Cheng et al., 2018)
are beyond the specification in this paper.

Formal Verification of Neural Networks. For-
mal verification provides a provable guarantee that
models are consistent with a specification for all
possible model inputs. Previous work can be cat-
egorised into complete methods that use Mixed-
Integer Programming (MIP) (Bunel et al., 2017;
Cheng et al., 2017) or Satisfiability Modulo The-
ory (SMT) (Katz et al., 2017; Carlini et al., 2017),
and incomplete methods that solve a convex relax-
ation of the verification problem (Weng et al., 2018;
Wong and Kolter, 2018; Wang et al., 2018). Com-
plete methods perform exhaustive enumeration to
find the worst case. Hence, complete methods are
expensive and difficult to scale, though they pro-
vide exact robustness bounds. Incomplete meth-
ods provide loose robustness bounds, but can be
more scalable and used inside the training loop for
training models to be robust and verifiable (Raghu-
nathan et al., 2018; Wong and Kolter, 2018; Dvi-
jotham et al., 2018; Gowal et al., 2018). Our work
is the first to extend incomplete verification to text
classification, considering input perturbations on a
simplex and minimising worst case bounds to ad-
versarial attacks in text classification. We highlight
that the verification of neural networks is an ex-
tremely challenging task, and that scaling complete
and incomplete methods to large models remains
an open challenge.

Representations of Combinatorial Spaces.
Word lattices and hypergraphs are data structures
that have often been used to efficiently represent
and process exponentially large numbers of
sentences without exhaustively enumerating them.
Applications include automatic speech recognition
(ASR) output rescoring (Liu et al., 2016), machine
translation of ASR outputs (Bertoldi et al., 2007),
paraphrase variants (Onishi et al., 2010), and
word segmentation alternatives (Dyer et al., 2008).
The specifications used to characterise the space
of adversarial attacks are likewise a compact
representation, and the algorithms discussed below
operate on them without exhaustive enumeration.



4085

3 Methodology
We assume a fixed initial vector representation z0
of a given input sentence z1 (e.g. the concatenation
of pretrained word embeddings) and use a neural
network model, i.e. a series of differentiable trans-
formations hk:

zk = hk(zk�1) k = 1, . . . ,K (1)

where zk is the vector of activations in the k-th
layer and the final output zK consists of the logits
for each class. Typically each hk will be an affine
transformation followed by an activation function
(e.g. ReLU or sigmoid). The affine transformation
can be a convolution (with the inputs and outputs
having an implied 2D structure) of a vector of acti-
vations at each point in a sequence; in what follows
these activations will be concatenated along the
sequence to form a vector zk.

3.1 Verification
Verification is the process of examining whether
the output of a model satisfies a given specification.
Formally, this means establishing whether the fol-
lowing holds true for a given normal model input
x0: 8z0 2 Xin(x0) : zK 2 Xout, where Xout char-
acterizes a constraint on the outputs, and Xin(x0)
defines a neighbourhood of x0 throughout which
the constraint should be satisfied.

In our concrete use case, we consider a speci-
fication of robustness against adversarial attacks
which are defined by bounded input perturbations
(synonym flips up to � words, or character flips up
to � characters) of the original sentence x. The
attack space Xin(x0) is the set of vector repre-
sentations (embeddings) of all such perturbed sen-
tences. Denoting by zK,y the logit of label y, we
formulate the output constraint that for all classes
y : zK,ytrue � zK,y. This specification estab-
lishes that the prediction of all perturbed sentences
z0 2 Xin(x0) should correspond to the correct la-
bel ytrue. This specification may equivalently be
formulated as a set of half-space constraints on the
logits: for each class y

(ey � eytrue)>zK  0 8z0 2 Xin(x0) (2)

where ei is a one-hot vector with 1 in the i-th posi-
tion. In other words, the true class logit should be
greater or equal than those for all other classes y,
which means the prediction remains constant.

1For brevity, we will refer both to the original symbol
sequence and its corresponding vector representation with the
same variable name, distinguishing them by styling.

3.2 Verification as Optimisation
Verifying the specification in Eq. (2) can be done
by solving the following constrained optimisation
problem to find the input that would most strongly
violate it:

maximize
z02Xin(x0)

c>zK

subject to zk = hk(zk�1) k = 1, . . . ,K
(3)

where c is a vector with entries cy = 1, cytrue = �1
and 0 everywhere else. If the optimal value of the
above optimisation problem is smaller than 0, then
the specification in Eq. (2) is satisfied, otherwise a
counter-example has been found. In our case, this
corresponds to a successful adversarial attack.

3.3 Modeling Input Perturbations using
Simplices

In the interests of computational feasibility, we
will actually attempt to verify the specification on a
larger, but more tractable input perturbation space
X̄in ◆ Xin. Any data point that is verifiable on
this larger input perturbation space is necessarily
verifiable with respect to the original specification.

In the domain of image classification, Xin is of-
ten modeled as an L1-ball, corresponding to input
perturbations in which each pixel may be indepen-
dently varied within a small interval. However,
using such interval bounds is unsuitable for our sit-
uation of perturbations consisting of a small num-
ber � of symbol substitutions. Although we could
construct an axis-aligned bounding box X̄in in em-
bedding space that encompasses all of Xin, it would
over-approximate the perturbation space to such an
extent that it would contain perturbations where
all symbols in the sentence have been substituted
simultaneously.

To remedy this, we propose a tighter over-
approximation in the form of a ‘simplex’ in embed-
ding space. We first define this for the special case
� = 1, in which Xin = {x0} [ {p(m)0 : 1  m 
M} consists of the representations of all M sen-
tences p(m) derived from x by performing a single
synonym (or character) substitution, together with
the unperturbed sentence x itself. In this case we
define X̄in to be the convex hull S1 of Xin. Note we
are not considering contextual embeddings (Peters
et al., 2018) here. Each ‘vertex’ p(m)0 is a sequence
of embedding vectors that differs from x0 at only
one word (or character) position.

For a larger perturbation radius � > 1, the cardi-
nality of Xin grows exponentially, so manipulating



4086

its convex hull becomes infeasible. However, di-
lating S1 centered at x0, scaling it up by a factor
of �, yields a simplex S� with M + 1 vertices that
contains Xin.

More formally, we define a region in the in-
put embedding space based on the M ‘elemen-
tary’ perturbations {p(m)0 : m = 1 . . .M} of x0
defined earlier for the � = 1 case. For perturba-
tions of up to � substitutions, we define X̄in(x0)
as the convex hull of {z(m)0 : m = 0 . . .M},
where z(0)0 = x0 denotes the original (unper-
turbed) sentence representation and, for m � 1,
z(m)0 = x0 + � · (p

(m)
0 � x0). The convex hull is

an over-approximation of Xin(x0): it contains the
representations of all sentences derived from x by
performing up to � substitutions at distinct word
(or character) positions.

3.4 Interval Bound Propagation
To estimate the optimal value of the problem
(3), given an input z0, we can propagate the up-
per/lower bounds on the activations zk of each layer
using interval arithmetic (Gowal et al., 2018).

We begin by computing interval bounds on the
first layer’s activations. Recall that any input
z0 2 Xin will lie within the convex hull of certain
vertices {z(m)0 : m = 0 . . .M}. Then, assuming
that the first layer h1 is an affine transformation (e.g.
linear or convolutional) followed by a monotonic
activation function, the lower and upper bounds on
the components z1,i of the first layer’s activations
z1 are as follows:

z1,i(�) = min
m=0,...,M

e>i h1(z
(m)
0 )

z1,i(�) = max
m=0,...,M

e>i h1(z
(m)
0 )

(4)

Note that these bounds are efficient to compute
(by passing each perturbation z(m)0 through the first
layer); in particular there is no need to compute the
convex hull polytope.

For subsequent layers k > 1, the bounds on the
components zk,i of zk are:

zk,i(�) = min
zk�1(�)zk�1zk�1(�)

e>i hk(zk�1)

zk,i(�) = max
zk�1(�)zk�1zk�1(�)

e>i hk(zk�1)
(5)

The above optimisation problems can be solved
in closed form quickly for affine layers and mono-
tonic activation functions, as illustrated in Gowal

et al. (2018). Finally, the lower and upper bounds
of the output logits zK can be used to construct an
upper bound on the solution of (3):

maximize
zK(�)zKzK(�)

c>zK (6)

Verifiable Training. The upper bound in (6) is
fast to compute (only requires two forward passes
for upper and lower bounds through the network).
Hence, we can define a loss to optimise models
such that the models are trained to be verifiable.
Solving (6) is equivalent to finding the worst-case
logit difference, and this is achieved when the logit
of the true class is equal to its lower bound, and all
other logits equal to their upper bounds. Concretely,
for each class y 6= ytrue: ẑK,y(�) = zK,y(�), and
ẑK,ytrue(�) = zK,ytrue(�). The training loss can then
be formulated as

L =  `(zK , ytrue)| {z }
Lnormal

+(1� ) `(ẑK(�), ytrue)| {z }
Lspec

(7)

where ` is the cross-entropy loss,  a hyperparam-
eter that controls the relative weights between the
classification loss Lnormal and specification loss
Lspec. If � = 0 then zK = ẑK(�), and thus L
reduces to a standard classification loss. Empiri-
cally, we found that a curriculum-based training,
starting with =1 and linearly decreasing to 0.25,
is effective for verifiable training.

4 Experiments

We conduct verification experiments on two text
classification datasets, Stanford Sentiment Tree-
bank (SST) (Socher et al., 2013) and AG News cor-
pus, processed in (Zhang et al., 2015). We focus
on word-level and character-level experiments on
SST and character-level experiments on AG News.
Our specification is that models should preserve
their prediction against up to � synonym substitu-
tions or character typos, respectively.

4.1 A Motivating Example
We provide an example from Table 2 to highlight
different evaluation metrics and training methods.
Given a sentence, “you ’ ve seen them a million
times .”, that is predicted correctly (called Nomi-
nal Accuracy2) by a classification model, we want
to further examine whether the model is robust
against character typos (e.g., up to � = 3 typos) to

2We use the term “nominal accuracy” to indicate the accu-
racy under various adversarial perturbations is much lower.



4087

this example. One way is to use some heuristic to
search for a valid example with up to 3 typos that
can change the prediction the most (called adver-
sarial example). We evaluate the model using this
adversarial example and report the performance
(called Adversarial Accuracy). However, even if
the adversarial example is predicted correctly, one
can still ask: is the model truly robust against any
typos (up to 3) to this example? In order to have a
certificate that the prediction will not change under
any � = 3 character typos (called verifiably ro-
bust), we could in theory exhaustively search over
all possible cases and check whether any of the
predictions is changed (called Oracle Accuracy). If
we only allow a character to be replaced by another
character nearby on the keyboard, already for this
short sentence we need to exhaustively search over
2,951 possible perturbations. To avoid this combi-
natorial growth, we can instead model all possible
perturbations using the proposed simplex bounds
and propagate the bounds through IBP at the cost
of two forward passes. Following Eq. (3), we can
check whether this example can be verified to be
robust against all perturbations (called IBP-Verified
Accuracy).

There are also a number of ways in which the
training procedure can be enhanced to improve the
verifiable robustness of a model against typos to
the sentence. The baseline is to train the model
with the original/normal sentence directly (called
Normal Training). Another way is to randomly
sample typo sentences among the 2,951 possible
perturbations and add these sentences to the train-
ing data (called Data Augmentation Training). Yet
another way is to find, at each training iteration, the
adversarial example among the (subset of) 2,951
possible perturbations that can change the predic-
tion the most; we then use the adversarial example
alongside the training example (called Adversarial
Training). Finally, as simplex bounds with IBP is
efficient to run, we can train a model to be verifiable
by minimising Eq. (7) (called Verifiable Training).

4.2 Baselines
In this section we detail our baseline models.

Adversarial Training. In adversarial training
(Madry et al., 2018; Goodfellow et al., 2014), the
goal is to optimise the following saddle point prob-
lem:

min
✓

E
(x0,y)


max

z02Xin(x0)
`✓(z0, y)

�
(8)

where the inner maximisation problem is to find
an adversarial perturbation z0 2 Xin(x0) that can
maximise the loss. In the inner maximisation prob-
lem, we use HotFlip (Ebrahimi et al., 2018) with
perturbation budget � to find the adversarial ex-
ample. The outer minimisation problem aims to
update model parameters such that the adversar-
ial risk of (8) is minimised. To balance between
the adversarial robustness and nominal accuracy,
we use an interpolation weight of 0.5 between the
original cross-entropy loss and the adversarial risk.

Data Augmentation Training. In the data aug-
mentation setup, we randomly sample a valid per-
turbation z with perturbation budget � from a nor-
mal input x, and minimise the cross-entropy loss
given the perturbed sample z (denoted as data
augmentation loss). We also set the interpolation
weight between the data augmentation loss and the
original normal cross-entropy loss to 0.5.

Normal Training. In normal training, we use the
likelihood-based training using the normal training
input x.

4.3 Setup

We use a shallow convolutional network with a
small number of fully-connected layers for SST
and AG News experiments. The detailed model
architectures and hyperparameter details are intro-
duced in the supplementary material. Although
we use shallow models for ease of verifiable train-
ing, our nominal accuracy is on par with previous
work such as Socher et al. (2013) (85.4%) and
Shen et al. (2018) (84.3%) in SST and Zhang et al.
(2015) (87.18%) in AG News. During training,
we set the maximum number of perturbations to
� = 3, and evaluate performance with the maxi-
mum number of perturbations from � = 1 to 6 at
test time.

For word-level experiments, we construct the
synonym pairs using the PPDB database (Ganitke-
vitch et al., 2013) and filter the synonyms with
fine-grained part-of-speech tags using Spacy (Hon-
nibal and Montani, 2017). For character-level ex-
periments, we use synthetic keyboard typos from
Belinkov and Bisk (2018), and allow one possible
alteration per character that is adjacent to it on an
American keyboard. The allowable input pertur-
bation space is much larger than for word-level
synonym substitutions, as shown in Table 3.



4088

SST-Char-Level SST-Word-Level AG-Char-Level
Training Acc. Adv. Acc. Oracle Acc. Adv. Acc. Oracle Acc. Adv. Acc. Oracle
Normal 79.8 36.5 10.3 84.8 71.3 69.8 89.5 75.4 65.1
Adversarial 79.0 74.9 25.8 85.0 76.8 74.6 90.5 85.5 81.6
Data aug. 79.8 37.8 13.7 85.4 72.7 71.6 88.4 77.5 72.0
Verifiable (IBP) 74.2 73.1 73.1 81.7 77.2 76.5 87.6 87.1 87.1

Table 1: Experimental results for changes up to �=3 and �=2 symbols on SST and AG dataset, respectively. We
compare normal training, adversarial training, data augmentation and IBP-verifiable training, using three metrics
on the test set: the nominal accuracy, adversarial accuracy, and exhaustively verified accuracy (Oracle) (%).

Prediction SST word-level examples (by exhaustive verification, not by adversarial attack)
+ it ’ s the kind of pigeonhole-resisting romp that hollywood too rarely provides .
- it ’ s the kind of pigeonhole-resisting romp that hollywood too rarely gives .
- sets up a nice concept for its fiftysomething leading ladies , but fails loudly in execution .
+ sets up a nice concept for its fiftysomething leading ladies , but fails aloud in execution .

Prediction SST character level examples (by exhaustive verification, not by adversarial attack)
- you ’ ve seen them a million times .
+ you ’ ve sern them a million times .
+ choose your reaction : a. ) that sure is funny !
- choose tour reaction : a. ) that sure is funny !

Table 2: Pairs of original inputs and adversarial examples for SST sentiment classification found via an exhaus-
tive verification oracle, but not found by the HotFlip attack (i.e., the HotFlip attack does not change the model
prediction). The bold words/characters represent the flips found by the adversary that change the predictions.

4.4 Evaluation Metrics

We use the following four metrics to evaluate our
models: i) test set accuracy (called Acc.), ii) ad-
versarial test accuracy (called Adv. Acc.), which
uses samples generated by HotFlip attacks on the
original test examples, iii) verifiable accuracy un-
der IBP verification (called IBP-verified), that is,
the ratio of test samples for which IBP can verify
that the specification is not violated, and iv) exhaus-
tively verified accuracy (called Oracle), computed
by enumerating all possible perturbations given the
perturbation budget �, where a sample is verifiably
robust if the prediction is unchanged under all valid
perturbations.

4.5 Results

Table 1 shows the results of IBP training and base-
line models under � = 3 and � = 23 perturbations
on SST and AG News, respectively. Figures 2 and
3 show the character- and word-level results with �
between 1 and 6 under four metrics on the SST test
set; similar figures for SST word-level (adversarial
training, data augmentation) models and AG News
dataset can be found in the supplementary material.

Oracle Accuracy and Adversarial Accuracy.
In Table 1, comparing adversarial accuracy with

3Note that the exhaustive oracle is not computationally
feasible beyond � = 2 on AG News.

exhaustive verification accuracy (oracle), we ob-
serve that although adversarial training is effective
at defending against HotFlip attacks (74.9 / 76.8 /
85.5%), the oracle adversarial accuracy under ex-
haustive testing (25.8 / 74.6 / 81.6%) is much lower
in SST-character / SST-word / AG-character level,
respectively. For illustration, we show some con-
crete adversarial examples from the HotFlip attack
in Table 2. For some samples, even though the
model is robust with respect to HotFlip attacks, its
predictions are incorrect for stronger adversarial
examples obtained using the exhaustive verification
oracle. This underscores the need for verification,
as robustness with respect to suboptimal adversarial
attacks alone might give a false sense of security.

Effectiveness of Simplex Bounds with IBP.
Rather than sampling individual points from the
perturbation space, IBP training covers the full
space at once. The resulting models achieve the
highest exhaustively verified accuracy at the cost
of only moderate deterioration in nominal accuracy
(Table 1). At test time, IBP allows for constant-time
verification with arbitrary �, whereas exhaustive
verification requires evaluation over an exponen-
tially growing search space.

Perturbation Space Size. In Table 1, when the
perturbation space is larger (SST character-level vs.
SST word-level), (a) across models, there is a larger
gap in adversarial accuracy and true robustness



4089

(a) Normal Training (b) Adversarial Training

(c) Data Augmentation Training (d) Verifiable Training (IBP)

Figure 2: SST character-level models with different training objectives (trained at �=3) against different perturba-
tion budgets in nominal accuracy, adversarial accuracy, exhaustively verified accuracy (Oracle), and IBP verified
accuracy. Note that exhaustive verification is not scalable to perturbation budget 4 and beyond.

(a) Normal Training (GloVe) (b) Verifiable Training (IBP) (GloVe)

(c) Normal Training (CF) (d) Verifiable Training (IBP) (CF)

Figure 3: SST word-level models with normal and verifiable training objectives (trained at �=3) using GloVe and
counter-fitted (CF) embeddings against different perturbation budgets in nominal accuracy, adversarial accuracy,
exhaustively verified accuracy (Oracle), and IBP verified accuracy. Note that exhaustive verification is not scalable
to perturbation budget 6 and beyond.

(oracle); (b) the difference in oracle robustness
between IBP and adversarial training is even larger
(73.1% vs. 25.8% and 76.5% vs. 74.6%).

Perturbation Budget. In Figures 2 and 3, we
compare normal training, adversarial training, data
augmentation, and verifiable training models with
four metrics under various perturbation budgets on
the SST dataset. Overall, as the perturbation bud-
get increases, the adversarial accuracy, oracle accu-
racy, and IBP-verified accuracy decrease. We can
observe that even for large perturbation budgets,
verifiably trained models are still able to verify a

sizable number of samples. Again, although ad-
versarial accuracy flattens for larger perturbation
budgets in the word level experiments, oracle veri-
fication can further find counterexamples to change
the prediction. Note that exhaustive verification
becomes intractable with large perturbation sizes.

Computational Cost of Exhaustive Verification.
The perturbation space in NLP problems is discrete
and finite, and a valid option to verify the specifica-
tion is to exhaustively generate predictions for all
z0 2 Xin(x0), and then check if at least one does
not match the correct label. Conversely, such an



4090

Perturbation radius � =1 � =2 � =3

SST-word 49 674 5,136
SST-character 206 21,116 1,436,026
AG-character 722 260,282 -

Table 3: Maximum perturbation space size in the SST
and AG News test set using word / character substitu-
tions, which is the maximum number of forward passes
per sentence to evaluate in the exhaustive verification.

Figure 4: Verified accuracy vs. computation budget
for exhaustive verification oracles on the SST character-
level test set, for an IBP-trained model trained with �=3.
Solid lines represent the number of forward passes re-
quired to verify a given proportion of the test set using
exhaustive search. Dashed lines indicate verification
levels achievable using IBP verification, which comes
at small and constant cost, and is thus orders of magni-
tude more efficient.

exhaustive (oracle) approach can also identify the
strongest possible attack. But the size of Xin grows
exponentially with �, and exhaustive verification
quickly becomes prohibitively expensive.

In Table 3, we show the maximum perturbation
space size in the SST and AG News test set for
different perturbation radii �. This number grows
exponentially as � increases. To further illustrate
this, Figure 4 shows the number of forward passes
required to verify a given proportion of the SST test
set for an IBP-trained model using exhaustive veri-
fication and IBP verification. IBP reaches verifica-
tion levels comparable to an exhaustive verification
oracle, but requires only two forward passes to ver-
ify any sample – one pass for computing the upper,
and one for the lower bounds. Exhaustive verifica-
tion, on the other hand, requires several orders of
magnitude more forward passes, and there is a tail
of samples with extremely large attack spaces.

4.6 Counter-Fitted Embeddings
As shown in Figures 2 and 3a, although IBP
can verify arbitrary networks in theory, the ver-

ification bound is very loose except for models
trained to be IBP-verifiable. One possible rea-
son is the potentially large volume of the perturba-
tion simplex. Since representations of substitution
words/characters are not necessarily close to those
of synonyms/typos in embedding space, the ver-
tices of the simplex could be far apart, and thus
cover a large area in representation space. There-
fore, when propagating the interval bounds through
the network, the interval bounds become too loose
and fail to verify most of the examples if the mod-
els are not specifically trained. To test this hy-
pothesis, we follow Mrkšić et al. (2016) and use
fine-tuned GloVe embeddings trained to respect
linguistic constraints; these representations (called
counter-fitted embeddings) force synonyms to be
closer and antonyms to be farther apart using word
pairs from the PPDB database (Ganitkevitch et al.,
2013) and WordNet (Miller, 1995). We repeat the
word level experiments with these counter-fitted
embeddings, Figures 3c and 3d show the experi-
mental results. We observe that IBP verified ac-
curacy is now substantially higher across models,
especially for � = 1, 2, 3. The examples which
IBP can verify increase by up to 33.2% when us-
ing the counter-fitted embeddings (normal training,
� = 1). Moreover, adversarial and exhaustively
verified accuracy are also improved, at the cost of
a mild deterioration in nominal test accuracy. The
IBP-trained model also further improves both its
oracle accuracy and IBP verified accuracy. These
results validate our hypothesis that reducing the
simplex volume via soft linguistic constraints can
provide even tighter bounds for IBP, resulting in
larger proportions of verifiable samples.

5 Discussion

Our experiments indicate that adversarial attacks
are not always the worst adversarial inputs, which
can only be revealed via verification. On the other
hand, exhaustive verification is computationally
very expensive. Our results show that using the
proposed simplex bounds with IBP can verify a siz-
able amount of test samples, and can be considered
a potent verification method in an NLP context. We
note however two limitations within the scope of
this work: i) limited model depth: we only inves-
tigated models with few layers. IBP bounds are
likely to become looser as the number of layers
increases. ii) limited model types: we only studied
models with CNN and fully connected layers.



4091

We focused on the HotFlip attack to showcase
specification verification in the NLP context, with
the goal of understanding factors that impact its
effectiveness (e.g. the perturbation space volume,
see Section 4.6). It is worth noting that symbol
substitution is general enough to encompass other
threat models such as lexical entailment perturba-
tions (Glockner et al., 2018), and could potentially
be extended to the addition of pre/postfixes (Jia and
Liang, 2017; Wallace et al., 2019).

Interesting directions of future work include:
tightening IBP bounds to allow applicability to
deeper models, investigating bound propagation
in other types of neural architectures (e.g. those
based on recurrent networks or self-attention), and
exploring other forms of specifications in NLP.

6 Conclusion

We introduced formal verification of text classifi-
cation models against synonym and character flip
perturbations. Through experiments, we demon-
strated the effectiveness of the proposed simplex
bounds with IBP both during training and test-
ing, and found weaknesses of adversarial training
compared with exhaustive verification. Verifiably
trained models achieve the highest exhaustive veri-
fication accuracy on SST and AG News. IBP veri-
fies models in constant time, which is exponentially
more efficient than naive verification via exhaustive
search.

References
Anish Athalye, Nicholas Carlini, and David A. Wag-

ner. 2018. Obfuscated gradients give a false sense
of security: Circumventing defenses to adversarial
examples. In ICML, pages 274–283.

Christel Baier and Joost-Pieter Katoen. 2008. Princi-
ples of Model Checking. MIT press.

Clark Barrett and Cesare Tinelli. 2018. Satisfiability
modulo theories. In Handbook of Model Checking,
pages 305–343. Springer.

Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic
and natural noise both break neural machine transla-
tion. In International Conference on Learning Rep-
resentations.

Nicola Bertoldi, Richard Zens, and Marcello Federico.
2007. Speech translation by confusion network de-
coding. In Proc. ICASSP.

Rudy Bunel, Ilker Turkaslan, Philip HS Torr, Pushmeet
Kohli, and M Pawan Kumar. 2017. Piecewise lin-

ear neural network verification: a comparative study.
arXiv preprint arXiv:1711.00455.

Nicholas Carlini, Guy Katz, Clark Barrett, and David L
Dill. 2017. Ground-truth adversarial examples.
arXiv preprint arXiv:1709.10207.

Nicholas Carlini and David Wagner. 2017. Adver-
sarial examples are not easily detected: Bypassing
ten detection methods. In Proceedings of the 10th
ACM Workshop on Artificial Intelligence and Secu-
rity, pages 3–14. ACM.

Chih-Hong Cheng, Georg Nührenberg, and Harald
Ruess. 2017. Maximum resilience of artificial neu-
ral networks. In International Symposium on Au-
tomated Technology for Verification and Analysis,
pages 251–268. Springer.

Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen,
and Cho-Jui Hsieh. 2018. Seq2Sick: Evaluating the
robustness of sequence-to-sequence models with ad-
versarial examples. CoRR, abs/1803.01128.

Krishnamurthy Dvijotham, Sven Gowal, Robert Stan-
forth, Relja Arandjelovic, Brendan O’Donoghue,
Jonathan Uesato, and Pushmeet Kohli. 2018. Train-
ing verified learners with learned verifiers. arXiv
preprint arXiv:1805.10265.

Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012–
1020, Columbus, Ohio. Association for Computa-
tional Linguistics.

Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing
Dou. 2018. HotFlip: White-box adversarial exam-
ples for text classification. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
31–36. Association for Computational Linguistics.

Allyson Ettinger, Sudha Rao, Hal Daumé III, and
Emily M. Bender. 2017. Towards linguistically gen-
eralizable NLP systems: A workshop and shared
task. In Proceedings of the First Workshop on Build-
ing Linguistically Generalizable NLP Systems.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 758–764, Atlanta, Georgia. Associa-
tion for Computational Linguistics.

Max Glockner, Vered Shwartz, and Yoav Goldberg.
2018. Breaking NLI systems with sentences that re-
quire simple lexical inferences. In Proceedings of
the 56th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
pages 650–655, Melbourne, Australia. Association
for Computational Linguistics.

http://proceedings.mlr.press/v80/athalye18a.html
http://proceedings.mlr.press/v80/athalye18a.html
http://proceedings.mlr.press/v80/athalye18a.html
https://openreview.net/forum?id=BJ8vJebC-
https://openreview.net/forum?id=BJ8vJebC-
https://openreview.net/forum?id=BJ8vJebC-
https://www.aclweb.org/anthology/P08-1115
https://www.aclweb.org/anthology/P08-1115
http://aclweb.org/anthology/P18-2006
http://aclweb.org/anthology/P18-2006
https://www.aclweb.org/anthology/N13-1092
https://www.aclweb.org/anthology/N13-1092
https://doi.org/10.18653/v1/P18-2103
https://doi.org/10.18653/v1/P18-2103


4092

Ian J Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2014. Explaining and harnessing adversar-
ial examples. arXiv preprint arXiv:1412.6572.

Sven Gowal, Krishnamurthy Dvijotham, Robert Stan-
forth, Rudy Bunel, Chongli Qin, Jonathan Uesato,
Relja Arandjelovic, Timothy A. Mann, and Push-
meet Kohli. 2018. On the effectiveness of inter-
val bound propagation for training verifiably robust
models. CoRR, abs/1810.12715.

Matthew Honnibal and Ines Montani. 2017. spaCy 2:
Natural language understanding with Bloom embed-
dings, convolutional neural networks and incremen-
tal parsing. To appear.

Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke
Zettlemoyer. 2018. Adversarial example generation
with syntactically controlled paraphrase networks.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 1875–1885, New
Orleans, Louisiana. Association for Computational
Linguistics.

Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.
In Empirical Methods in Natural Language Process-
ing (EMNLP).

Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and
Mykel J Kochenderfer. 2017. Reluplex: An efficient
SMT solver for verifying deep neural networks. In
International Conference on Computer Aided Verifi-
cation, pages 97–117. Springer.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. International
Conference on Learning Representations.

Alexey Kurakin, Ian Goodfellow, and Samy Bengio.
2016. Adversarial examples in the physical world.
arXiv preprint arXiv:1607.02533.

Yitong Li, Trevor Cohn, and Timothy Baldwin. 2017.
Robust training under linguistic adversity. In Pro-
ceedings of the 15th Conference of the European
Chapter of the Association for Computational Lin-
guistics: Volume 2, Short Papers, pages 21–27.

Xunying Liu, Xie Chen, Yongqiang Wang, Mark J. F.
Gales, and Philip C. Woodland. 2016. Two efficient
lattice rescoring methods using recurrent neural net-
work language models. IEEE/ACM Trans. Audio,
Speech & Language Processing, 24(8):1438–1449.

Aleksander Madry, Aleksandar Makelov, Ludwig
Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018.
Towards deep learning models resistant to adversar-
ial attacks. In International Conference on Learning
Representations.

George A. Miller. 1995. WordNet: A lexical database
for English. Commun. ACM, 38(11):39–41.

Matthew Mirman, Timon Gehr, and Martin Vechev.
2018. Differentiable abstract interpretation for prov-
ably robust neural networks. In Proceedings of the
35th International Conference on Machine Learning,
volume 80, pages 3578–3586.

Nikola Mrkšić, Diarmuid Ó Séaghdha, Blaise Thom-
son, Milica Gašić, Lina M. Rojas-Barahona, Pei-
Hao Su, David Vandyke, Tsung-Hsien Wen, and
Steve Young. 2016. Counter-fitting word vectors to
linguistic constraints. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 142–148, San Diego,
California. Association for Computational Linguis-
tics.

Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.
2010. Paraphrase lattice for statistical machine
translation. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 1–5, Uppsala, Sweden.
Association for Computational Linguistics.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word rep-
resentation. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 1532–1543. Association for
Computational Linguistics.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
2227–2237, New Orleans, Louisiana. Association
for Computational Linguistics.

Aditi Raghunathan, Jacob Steinhardt, and Percy Liang.
2018. Certified defenses against adversarial exam-
ples. In International Conference on Learning Rep-
resentations.

Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2018. Semantically equivalent adversar-
ial rules for debugging NLP models. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 856–865. Association for Computational Lin-
guistics.

Dinghan Shen, Guoyin Wang, Wenlin Wang, Mar-
tin Renqiang Min, Qinliang Su, Yizhe Zhang, Chun-
yuan Li, Ricardo Henao, and Lawrence Carin.
2018. Baseline needs more love: On simple
word-embedding-based models and associated pool-
ing mechanisms. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 440–
450, Melbourne, Australia. Association for Compu-
tational Linguistics.

http://arxiv.org/abs/1810.12715
http://arxiv.org/abs/1810.12715
http://arxiv.org/abs/1810.12715
https://doi.org/10.18653/v1/N18-1170
https://doi.org/10.18653/v1/N18-1170
https://openreview.net/forum?id=rJzIBfZAb
https://openreview.net/forum?id=rJzIBfZAb
https://doi.org/10.18653/v1/N16-1018
https://doi.org/10.18653/v1/N16-1018
https://www.aclweb.org/anthology/P10-2001
https://www.aclweb.org/anthology/P10-2001
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.18653/v1/N18-1202
https://doi.org/10.18653/v1/N18-1202
http://aclweb.org/anthology/P18-1079
http://aclweb.org/anthology/P18-1079
https://doi.org/10.18653/v1/P18-1041
https://doi.org/10.18653/v1/P18-1041
https://doi.org/10.18653/v1/P18-1041


4093

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 1631–1642. Association for Computational
Linguistics.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and
Rob Fergus. 2013. Intriguing properties of neural
networks. arXiv preprint arXiv:1312.6199.

Dimitris Tsipras, Shibani Santurkar, Logan Engstrom,
Alexander Turner, and Aleksander Madry. 2019. Ro-
bustness may be at odds with accuracy. In Interna-
tional Conference on Learning Representations.

Jonathan Uesato, Brendan O’Donoghue, Pushmeet
Kohli, and Aron van den Oord. 2018. Adversarial
risk and the dangers of evaluating against weak at-
tacks. In ICML, pages 5032–5041.

Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-
ner, and Sameer Singh. 2019. Universal trigger se-
quences for attacking and analyzing NLP. In Em-
pirical Methods in Natural Language Processing
(EMNLP).

Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng
Yang, and Suman Jana. 2018. Formal security
analysis of neural networks using symbolic inter-
vals. In 27th USENIX Security Symposium (USENIX
Security 18), pages 1599–1614, Baltimore, MD.
USENIX Association.

Lily Weng, Huan Zhang, Hongge Chen, Zhao Song,
Cho-Jui Hsieh, Luca Daniel, Duane Boning, and In-
derjit Dhillon. 2018. Towards fast computation of
certified robustness for ReLU networks. In Proceed-
ings of the 35th International Conference on Ma-
chine Learning, volume 80 of Proceedings of Ma-
chine Learning Research, pages 5276–5285, Stock-
holmsmssan, Stockholm Sweden. PMLR.

Eric Wong and Zico Kolter. 2018. Provable defenses
against adversarial examples via the convex outer ad-
versarial polytope. In International Conference on
Machine Learning, pages 5283–5292.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Advances in Neural Information Pro-
cessing Systems, pages 649–657.

http://aclweb.org/anthology/D13-1170
http://aclweb.org/anthology/D13-1170
http://aclweb.org/anthology/D13-1170
http://proceedings.mlr.press/v80/uesato18a.html
http://proceedings.mlr.press/v80/uesato18a.html
http://proceedings.mlr.press/v80/uesato18a.html
https://www.usenix.org/conference/usenixsecurity18/presentation/wang-shiqi
https://www.usenix.org/conference/usenixsecurity18/presentation/wang-shiqi
https://www.usenix.org/conference/usenixsecurity18/presentation/wang-shiqi
http://proceedings.mlr.press/v80/weng18a.html
http://proceedings.mlr.press/v80/weng18a.html

