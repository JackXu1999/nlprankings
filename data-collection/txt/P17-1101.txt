



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1095–1104
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1101

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1095–1104
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1101

Selective Encoding for Abstractive Sentence Summarization

Qingyu Zhou†∗ Nan Yang‡ Furu Wei‡ Ming Zhou‡
†Harbin Institute of Technology, Harbin, China

‡Microsoft Research, Beijing, China
qyzhou@hit.edu.cn {nanya,fuwei,mingzhou}@microsoft.com

Abstract

We propose a selective encoding model
to extend the sequence-to-sequence frame-
work for abstractive sentence summariza-
tion. It consists of a sentence encoder,
a selective gate network, and an atten-
tion equipped decoder. The sentence en-
coder and decoder are built with recur-
rent neural networks. The selective gate
network constructs a second level sen-
tence representation by controlling the in-
formation flow from encoder to decoder.
The second level representation is tailored
for sentence summarization task, which
leads to better performance. We evalu-
ate our model on the English Gigaword,
DUC 2004 and MSR abstractive sentence
summarization datasets. The experimen-
tal results show that the proposed selective
encoding model outperforms the state-of-
the-art baseline models.

1 Introduction

Sentence summarization aims to shorten a given
sentence and produce a brief summary of it. This
is different from document level summarization
task since it is hard to apply existing techniques
in extractive methods, such as extracting sentence
level features and ranking sentences. Early works
propose using rule-based methods (Zajic et al.,
2007), syntactic tree pruning methods (Knight and
Marcu, 2002), statistical machine translation tech-
niques (Banko et al., 2000) and so on for this task.
We focus on abstractive sentence summarization
task in this paper.

Recently, neural network models have been ap-
plied in this task. Rush et al. (2015) use auto-
constructed sentence-headline pairs to train a neu-

∗Contribution during internship at Microsoft Research.

ral network summarization model. They use a
Convolutional Neural Network (CNN) encoder
and feed-forward neural network language model
decoder for this task. Chopra et al. (2016) ex-
tend their work by replacing the decoder with Re-
current Neural Network (RNN). Nallapati et al.
(2016) follow this line and change the encoder to
RNN to make it a full RNN based sequence-to-
sequence model (Sutskever et al., 2014).

the sri lankan government on wednesday announced 
the closure of government schools with immediate 
effect as a military campaign against tamil separatists 
escalated in the north of the country .

sri lanka closes schools as war escalates

Figure 1: An abstractive sentence summarization
system may produce the output summary by dis-
tilling the salient information from the highlight to
generate a fluent sentence. We model the distilling
process with selective encoding.

All the above works fall into the encoding-
decoding paradigm, which first encodes the in-
put sentence to an abstract representation and
then decodes the intended output sentence based
on the encoded information. As an extension
of the encoding-decoding framework, attention-
based approach (Bahdanau et al., 2015) has been
broadly used: the encoder produces a list of vec-
tors for all tokens in the input, and the decoder
uses an attention mechanism to dynamically ex-
tract encoded information and align with the out-
put tokens. This approach achieves huge success
in tasks like machine translation, where alignment
between all parts of the input and output are re-
quired. However, in abstractive sentence summa-
rization, there is no explicit alignment relationship
between the input sentence and the summary ex-

1095

https://doi.org/10.18653/v1/P17-1101
https://doi.org/10.18653/v1/P17-1101


cept for the extracted common words. The chal-
lenge here is not to infer the alignment, but to se-
lect the highlights while filtering out secondary
information in the input. A desired work-flow
for abstractive sentence summarization is encod-
ing, selection, and decoding. After selecting the
important information from an encoded sentence,
the decoder produces the output summary using
the selected information. For example, in Fig-
ure 1, given the input sentence, the summariza-
tion system first selects the important information,
and then rephrases or paraphrases to produce a
well-organized summary. Although this is implic-
itly modeled in the encoding-decoding framework,
we argue that abstractive sentence summarization
shall benefit from explicitly modeling this selec-
tion process.

In this paper we propose Selective Encoding for
Abstractive Sentence Summarization (SEASS).
We treat the sentence summarization as a three-
phase task: encoding, selection, and decoding. It
consists of a sentence encoder, a selective gate net-
work, and a summary decoder. First, the sentence
encoder reads the input words through an RNN
unit to construct the first level sentence represen-
tation. Then the selective gate network selects the
encoded information to construct the second level
sentence representation. The selective mechanism
controls the information flow from encoder to de-
coder by applying a gate network according to
the sentence information, which helps improve en-
coding effectiveness and release the burden of the
decoder. Finally, the attention-equipped decoder
generates the summary using the second level sen-
tence representation. We conduct experiments
on English Gigaword, DUC 2004 and Microsoft
Research Abstractive Text Compression test sets.
Our SEASS model achieves 17.54 ROUGE-2 F1,
9.56 ROUGE-2 recall and 10.63 ROUGE-2 F1 on
these test sets respectively, which improves perfor-
mance compared to the state-of-the-art methods.

2 Related Work

Abstractive sentence summarization, also known
as sentence compression and similar to headline
generation, is used to help compress or fuse the
selected sentences in extractive document sum-
marization systems since they may inadvertently
include unnecessary information. The sentence
summarization task has been long connected to the
headline generation task. There are some previous

methods to solve this task, such as the linguistic
rule-based method (Dorr et al., 2003). As for the
statistical machine learning based methods, Banko
et al. (2000) apply statistical machine translation
techniques by modeling headline generation as a
translation task and use 8000 article-headline pairs
to train the system.

Rush et al. (2015) propose leveraging news data
in Annotated English Gigaword (Napoles et al.,
2012) corpus to construct large scale parallel data
for sentence summarization task. They propose an
ABS model, which consists of an attentive Con-
volutional Neural Network encoder and an neural
network language model (Bengio et al., 2003) de-
coder. On this Gigaword test set and DUC 2004
test set, the ABS model produces the state-of-the-
art results. Chopra et al. (2016) extend this work,
which keeps the CNN encoder but replaces the de-
coder with recurrent neural networks. Their exper-
iments showes that the CNN encoder with RNN
decoder model performs better than Rush et al.
(2015). Nallapati et al. (2016) further change the
encoder to an RNN encoder, which leads to a full
RNN sequence-to-sequence model. Besides, they
enrich the encoder with lexical and statistic fea-
tures which play important roles in traditional fea-
ture based summarization systems, such as NER
and POS tags, to improve performance. Experi-
ments on the Gigaword and DUC 2004 test sets
show that the above models achieve state-of-the-
art results.

Gu et al. (2016) and Gulcehre et al. (2016)
come up similar ideas that summarization task can
benefit from copying words from input sentences.
Gu et al. (2016) propose CopyNet to model the
copying action in response generation, which also
applies for summarization task. Gulcehre et al.
(2016) propose a switch gate to control whether
to copy from source or generate from decoder vo-
cabulary. Zeng et al. (2016) also propose using
copy mechanism and add a scalar weight on the
gate of GRU/LSTM for this task. Cheng and Lap-
ata (2016) use an RNN based encoder-decoder for
extractive summarization of documents.

Yu et al. (2016) propose a segment to seg-
ment neural transduction model for sequence-to-
sequence framework. The model introduces a
latent segmentation which determines correspon-
dences between tokens of the input sequence and
the output sequence. Experiments on this task
show that the proposed transduction model per-

1096



forms comparable to the ABS model. Shen et al.
(2016) propose to apply Minimum Risk Train-
ing (MRT) in neural machine translation to di-
rectly optimize the evaluation metrics. Ayana et al.
(2016) apply MRT on abstractive sentence sum-
marization task and the results show that optimiz-
ing for ROUGE improves the test performance.

3 Problem Formulation

For sentence summarization, given an input sen-
tence x = (x1, x2, . . . , xn), where n is the sen-
tence length, xi ∈ Vs and Vs is the source vo-
cabulary, the system summarizes x by producing
y = (y1, y2, . . . , yl), where l ≤ n is the summary
length , yi ∈ Vt and Vt is the target vocabulary.

If |y| ⊆ |x|, which means all words in sum-
mary y must appear in given input, we denote this
as extractive sentence summarization. If |y| * |x|,
which means not all words in summary come from
input sentence, we denote this as abstractive sen-
tence summarization. Table 1 provides an exam-
ple. We focus on abstracive sentence summariza-
tion task in this paper.

Input: South Korean President Kim Young-Sam
left here Wednesday on a week - long state
visit to Russia and Uzbekistan for talks on
North Korea ’s nuclear confrontation and
ways to strengthen bilateral ties .

Output: Kim leaves for Russia for talks on NKorea
nuclear standoff

Table 1: An abstractive sentence summarization
example.

4 Model

As shown in Figure 2, our model consists of a
sentence encoder using the Gated Recurrent Unit
(GRU) (Cho et al., 2014), a selective gate network
and an attention-equipped GRU decoder. First, the
bidirectional GRU encoder reads the input words
x = (x1, x2, . . . , xn) and builds its representation
(h1, h2, . . . , hn). Then the selective gate selects
and filters the word representations according to
the sentence meaning representation to produce a
tailored sentence word representation for abstrac-
tive sentence summarization task. Lastly, the GRU
decoder produces the output summary with atten-
tion to the tailored representation. In the following
sections, we introduce the sentence encoder, the
selective mechanism, and the summary decoder
respectively.

4.1 Sentence Encoder
The role of the sentence encoder is to read the in-
put sentence and construct the basic sentence rep-
resentation. Here we employ a bidirectional GRU
(BiGRU) as the recurrent unit, where GRU is de-
fined as:

zi = σ(Wz[xi, hi−1])
ri = σ(Wr[xi, hi−1])

h̃i = tanh(Wh[xi, ri � hi−1])
hi = (1− zi)� hi−1 + zi � h̃i

(1)

(2)

(3)

(4)

where Wz , Wr and Wh are weight matrices.
The BiGRU consists of a forward GRU and a

backward GRU. The forward GRU reads the input
sentence word embeddings from left to right and
gets a sequence of hidden states, (~h1,~h2, . . . ,~hn).
The backward GRU reads the input sentence
embeddings reversely, from right to left, and
results in another sequence of hidden states,
( ~h1, ~h2, . . . , ~hn):

~hi = GRU(xi,~hi−1)
~hi = GRU(xi, ~hi+1)

(5)

(6)

The initial states of the BiGRU are set to zero
vectors, i.e., ~h1 = 0 and ~hn = 0. After reading the
sentence, the forward and backward hidden states
are concatenated, i.e., hi = [~hi; ~hi], to get the ba-
sic sentence representation.

4.2 Selective Mechanism
In the sequence-to-sequence machine translation
(MT) model, the encoder and decoder are respon-
sible for mapping input sentence information to
a list of vectors and decoding the sentence rep-
resentation vectors to generate an output sentence
(Bahdanau et al., 2015). Some previous works ap-
ply this framework to summarization generation
tasks (Nallapati et al., 2016; Gu et al., 2016; Gul-
cehre et al., 2016). However, abstractive sentence
summarization is different from MT in two ways.
First, there is no explicit alignment relationship
between the input sentence and the output sum-
mary except for the common words. Second, sum-
marization task needs to keep the highlights and
remove the unnecessary information, while MT
needs to keep all information literally.

Herein, we propose a selective mechanism to
model the selection process for abstractive sen-
tence summarization. The selective mechanism

1097



𝑥1

ℎ1

𝑥2

ℎ2

𝑥3

ℎ3

𝑥4

ℎ4

𝑥5

ℎ5

𝑥6

ℎ6

ℎ1′ ℎ2′ ℎ3′ ℎ4′ ℎ5′ ℎ6′

ℎ𝑖 𝑠

MLP
GRU

Attention

𝑐𝑡−1 𝑠𝑡−1 𝑦𝑡−1

𝑠𝑡

𝑐𝑡

maxout

𝑦𝑡

Selective Gate 
Network

Encoder

Decoder

softmax

Figure 2: Overview of the Selective Encoding for Abstractive Sentence Summarization (SEASS).

extends the sequence-to-sequence model by con-
structing a tailored representation for abstractive
sentence summarization task. Concretely, the se-
lective gate network in our model takes two vector
inputs, the sentence word vector hi and the sen-
tence representation vector s. The sentence word
vector hi is the output of the BiGRU encoder and
represents the meaning and context information of
word xi. The sentence vector s is used to represent
the meaning of the sentence. For each word xi,
the selective gate network generates a gate vector
sGatei using hi and s, then the tailored represen-
tation is constructed, i.e., h′i.

In detail, we concatenate the last forward hid-
den state ~hn and backward hidden state ~h1 as the
sentence representation s:

s =

[
~h1

~hn

]
(7)

For each time step i, the selective gate takes the
sentence representation s and BiGRU hidden hi as
inputs to compute the gate vector sGatei:

sGatei = σ(Wshi +Uss+ b)

h′i = hi � sGatei
(8)

(9)

where Ws and Us are weight matrices, b is the
bias vector, σ denotes sigmoid activation func-
tion, and � is element-wise multiplication. After
the selective gate network, we obtain another se-
quence of vectors (h′1, h

′
2, . . . , h

′
n). This new se-

quence is then used as the input sentence represen-
tation for the decoder to generate the summary.

4.3 Summary Decoder
On top of the sentence encoder and the selective
gate network, we use GRU with attention as the
decoder to produce the output summary.

At each decoding time step t, the GRU reads the
previous word embedding wt−1 and previous con-
text vector ct−1 as inputs to compute the new hid-
den state st. To initialize the GRU hidden state, we
use a linear layer with the last backward encoder
hidden state ~h1 as input:

st = GRU(wt−1, ct−1, st−1)

s0 = tanh(Wd ~h1 + b)

(10)

(11)

where Wd is the weight matrix and b is the bias
vector.

The context vector ct for current time step t is
computed through the concatenate attention mech-
anism (Luong et al., 2015), which matches the
current decoder state st with each encoder hidden
state h′i to get an importance score. The impor-
tance scores are then normalized to get the current
context vector by weighted sum:

et,i = v
>
a tanh(Wast−1 +Uah

′
i)

αt,i =
exp(et,i)∑n
i=1 exp(et,i)

ct =
n∑

i=1

αt,ih
′
i

(12)

(13)

(14)

We then combine the previous word embedding
wt−1, the current context vector ct, and the de-
coder state st to construct the readout state rt. The
readout state is then passed through a maxout hid-
den layer (Goodfellow et al., 2013) to predict the

1098



next word with a softmax layer over the decoder
vocabulary.

rt = Wrwt−1 +Urct +Vrst

mt = [max{rt,2j−1, rt,2j}]>j=1,...,d
p(yt|y1, . . . , yt−1) = softmax(Womt)

(15)

(16)

(17)

where Wa, Ua, Wr, Ur, Vr and Wo are weight
matrices. Readout state rt is a 2d-dimensional
vector, and the maxout layer (Equation 16) picks
the max value for every two numbers in rt and pro-
duces a d-dimensional vector mt.

4.4 Objective Function
Our goal is to maximize the output summary prob-
ability given the input sentence. Therefore, we op-
timize the negative log-likelihood loss function:

J(θ) = − 1|D|
∑

(x,y)∈D
log p(y|x) (18)

where D denotes a set of parallel sentence-
summary pairs and θ is the model parameter. We
use Stochastic Gradient Descent (SGD) with mini-
batch to learn the model parameter θ.

5 Experiments

In this section we introduce the dataset we use, the
evaluation metric, the implementation details, the
baselines we compare to, and the performance of
our system.

5.1 Dataset
Training Set For our training set, we use a par-
allel corpus which is constructed from the Anno-
tated English Gigaword dataset (Napoles et al.,
2012) as mentioned in Rush et al. (2015). The
parallel corpus is produced by pairing the first sen-
tence and the headline in the news article with
some heuristic rules. We use the script1 released
by Rush et al. (2015) to pre-process and extract the
training and development datasets. The script per-
forms various basic text normalization, including
PTB tokenization, lower-casing, replacing all digit
characters with #, and replacing word types seen
less than 5 times with 〈unk〉. The extracted corpus
contains about 3.8M sentence-summary pairs for
the training set and 189K examples for the devel-
opment set.

For our test set, we use the English Gigaword,
DUC 2004, and Microsoft Research Abstractive
Text Compression test sets.

1https://github.com/facebook/NAMAS

English Gigaword Test Set We randomly sam-
ple 8000 pairs from the extracted development set
as our development set since it is relatively large.
For the test set, we use the same randomly held-
out test set of 2000 sentence-summary pairs as
Rush et al. (2015).2

We also find that except for the empty titles, this
test set has some invalid lines like the input sen-
tence containing only one word. Therefore, we
further sample 2000 pairs as our internal test set
and release it for future works3.

DUC 2004 Test Set We employ DUC 2004 data
for tasks 1 & 2 (Over et al., 2007) in our experi-
ments as one of the test sets since it is too small to
train a neural network model on. The dataset pairs
each document with 4 different human-written ref-
erence summaries which are capped at 75 bytes. It
has 500 input sentences with each sentence paired
with 4 summaries.

MSR-ATC Test Set Toutanova et al. (2016) re-
lease a new dataset for sentence summarization
task by crowdsourcing. This dataset contains ap-
proximately 6,000 source text sentences with mul-
tiple manually-created summaries (about 26,000
sentence-summary pairs in total). Toutanova et al.
(2016) provide a standard split of the data into
training, development, and test sets, with 4,936,
448 and 785 input sentences respectively. Since
the training set is too small, we only use the test set
as one of our test sets. We denote this dataset as
MSR-ATC (Microsoft Research Abstractive Text
Compression) test set in the following.

Table 2 summarizes the statistic information of
the three datasets we used.

5.2 Evaluation Metric

We employ ROUGE (Lin, 2004) as our evaluation
metric. ROUGE measures the quality of summary
by computing overlapping lexical units, such as
unigram, bigram, trigram, and longest common
subsequence (LCS). It becomes the standard eval-
uation metric for DUC shared tasks and popular
for summarization evaluation. Following previous
work, we use ROUGE-1 (unigram), ROUGE-2 (bi-

2Thanks to Rush et al. (2015), we acquired the test set they
used. Following Chopra et al. (2016), we remove pairs with
empty titles resulting in slightly different accuracy compared
to Rush et al. (2015) for their systems. The cleaned test set
contains 1951 sentence-summary pairs.

3Our development and test sets can be found at https:
//res.qyzhou.me

1099



Data Set Giga DUC† MSR†

#(sent) 3.99M 500 785
#(sentWord) 125M 17.8K 29K
#(summWord) 33M 20.9K 85.9K
#(ref) 1 4 3-5
AvgInputLen 31.35 35.56 36.97
AvgSummLen 8.23 10.43 25.5

Table 2: Data statistics for the English Giga-
word, DUC 2004 and MSR-ATC datasets. #(x)
denotes the number of x, e.g., #(ref) is the num-
ber of reference summaries of an input sentence.
AvgInputLen is the average input sentence length
and AvgSummLen is the average summary length.
†DUC 2004 and MSR-ATC datasets are for test
purpose only.

gram) and ROUGE-L (LCS) as the evaluation met-
rics in the reported experimental results.

5.3 Implementation Details

Model Parameters The input and output vocab-
ularies are collected from the training data, which
have 119,504 and 68,883 word types respectively.
We set the word embedding size to 300 and all
GRU hidden state sizes to 512. We use dropout
(Srivastava et al., 2014) with probability p = 0.5.

Model Training We initialize model parame-
ters randomly using a Gaussian distribution with
Xavier scheme (Glorot and Bengio, 2010). We use
Adam (Kingma and Ba, 2015) as our optimizing
algorithm. For the hyperparameters of Adam op-
timizer, we set the learning rate α = 0.001, two
momentum parameters β1 = 0.9 and β2 = 0.999
respectively, and � = 10−8. During training, we
test the model performance (ROUGE-2 F1) on de-
velopment set for every 2,000 batches. We halve
the Adam learning rate α if the ROUGE-2 F1 score
drops for twelve consecutive tests on development
set. We also apply gradient clipping (Pascanu
et al., 2013) with range [−5, 5] during training. To
both speed up the training and converge quickly,
we use mini-batch size 64 by grid search.

Beam Search We use beam search to generate
multiple summary candidates to get better results.
To avoid favoring shorter outputs, we average the
ranking score along the beam path by dividing it
by the number of generated words. To both decode
fast and get better results, we set the beam size to

12 in our experiments.

5.4 Baseline
We compare SEASS model with the following
state-of-the-art baselines:

ABS Rush et al. (2015) use an attentive CNN en-
coder and NNLM decoder to do the sentence
summarization task. We trained this baseline
model with the released code1 and evaluate
it with our internal English Gigaword test set
and MSR-ATC test set.

ABS+ Based on ABS model, Rush et al. (2015)
further tune their model using DUC 2003
dataset, which leads to improvements on
DUC 2004 test set.

CAs2s As an extension of the ABS model,
Chopra et al. (2016) use a convolutional
attention-based encoder and RNN decoder,
which outperforms the ABS model.

Feats2s Nallapati et al. (2016) use a full
RNN sequence-to-sequence encoder-decoder
model and add some features to enhance the
encoder, such as POS tag, NER, and so on.

Luong-NMT Neural machine translation model
of Luong et al. (2015) with two-layer LSTMs
for the encoder-decoder with 500 hidden
units in each layer implemented in (Chopra
et al., 2016).

s2s+att We also implement a sequence-to-
sequence model with attention as our
baseline and denote it as “s2s+att”.

5.5 Results
We report ROUGE F1, ROUGE recall and ROUGE
F1 for English Gigaword, DUC 2004 and MSR-
ATC test sets respectively. We use the official
ROUGE script (version 1.5.5) 4 to evaluate the
summarization quality in our experiments. For
English Gigaword5 and MSR-ATC6 test sets, the
outputs have different lengths so we evaluate the
system with F1 metric. As for the DUC 2004 test
set7, the task requires the system to produce a fixed
length summary (75 bytes), therefore we employ
ROUGE recall as the evaluation metric. To satisfy
the length requirement, we decode the output sum-
mary to a roughly expected length following Rush
et al. (2015).

4http://www.berouge.com/
5The ROUGE evaluation option is the same as Rush et al.

(2015), -m -n 2 -w 1.2
6The ROUGE evaluation option is, -m -n 2 -w 1.2
7The ROUGE evaluation option is, -m -b 75 -n 2 -w 1.2

1100



English Gigaword We acquire the test set from
Rush et al. (2015) so we can make fair compar-
isons to the baselines.

Models RG-1 RG-2 RG-L

ABS (beam)‡ 29.55- 11.32- 26.42-

ABS+ (beam)‡ 29.76- 11.88- 26.96-

Feats2s (beam)‡ 32.67- 15.59- 30.64-

CAs2s (greedy)‡ 33.10- 14.45- 30.25-

CAs2s (beam)‡ 33.78- 15.97- 31.15-

Luong-NMT (beam)‡ 33.10- 14.45- 30.71-

s2s+att (greedy) 33.18- 14.79- 30.80-

s2s+att (beam) 34.04- 15.95- 31.68-

SEASS (greedy) 35.48 16.50 32.93
SEASS (beam) 36.15 17.54 33.63

Table 3: Full length ROUGE F1 evaluation results
on the English Gigaword test set used by Rush
et al. (2015). RG in the Table denotes ROUGE.
Results with ‡ mark are taken from the correspond-
ing papers. The superscript - indicates that our
SEASS model with beam search performs signif-
icantly better than it as given by the 95% confi-
dence interval in the official ROUGE script.

Models RG-1 RG-2 RG-L

ABS (beam) 37.41- 15.87- 34.70-

s2s+att (greedy) 42.41- 20.76- 39.84-

s2s+att (beam) 43.76- 22.28- 41.14-

SEASS (greedy) 45.27 22.88 42.20
SEASS (beam) 46.86 24.58 43.53

Table 4: Full length ROUGE F1 evaluation on our
internal English Gigaword test data. The super-
script - indicates that our SEASS model performs
significantly better than it as given by the 95%
confidence interval in the official ROUGE script.

In Table 3, we report the ROUGE F1 score of
our model and the baseline methods. Our SEASS
model with beam search outperforms all baseline
models by a large margin. Even for greedy search,
our model still performs better than other methods
which used beam search. For the popular ROUGE-
2 metric, our SEASS model achieves 17.54 F1
score and performs better than the previous works.
Compared to the ABS model, our model has a
6.22 ROUGE-2 F1 relative gain. Compared to the
highest CAs2s baseline, our model achieves 1.57

ROUGE-2 F1 improvement and passes the signifi-
cant test according to the official ROUGE script.

Table 4 summarizes our results on our internal
test set using ROUGE F1 evaluation metrics. The
performance on our internal test set is compara-
ble to our development set, which achieves 24.58
ROUGE-2 F1 and outperforms the baselines.

DUC 2004 We evaluate our model using the
ROUGE recall score since the reference summaries
of the DUC 2004 test set are capped at 75 bytes.
Therefore, we decode the summary to a fixed
length 18 to ensure that the generated summary
satisfies the minimum length requirement. As
summarized in Table 5, our SEASS outperforms
all the baseline methods and achieves 29.21, 9.56
and 25.51 for ROUGE 1, 2 and L recall. Compared
to the ABS+ model which is tuned using DUC
2003 data, our model performs significantly bet-
ter by 1.07 ROUGE-2 recall score and is trained
only with English Gigaword sentence-summary
data without being tuned using DUC data.

Models RG-1 RG-2 RG-L

ABS (beam)‡ 26.55- 7.06- 22.05-

ABS+ (beam)‡ 28.18- 8.49- 23.81-

Feats2s (beam)‡ 28.35- 9.46 24.59-

CAs2s (greedy)‡ 29.13 7.62- 23.92-

CAs2s (beam)‡ 28.97 8.26- 24.06-

Luong-NMT (beam)‡ 28.55 8.79- 24.43-

s2s+att (greedy) 27.03- 7.89- 23.80-

s2s+att (beam) 28.13 9.25 24.76
SEASS (greedy) 28.68 8.55 25.04
SEASS (beam) 29.21 9.56 25.51

Table 5: ROUGE recall evaluation results on DUC
2004 test set. All these models are tested using
beam search. Results with ‡ mark are taken from
the corresponding papers. The superscript - in-
dicates that our SEASS model performs signifi-
cantly better than it as given by the 95% confi-
dence interval in the official ROUGE script.

MSR-ATC We report the full length ROUGE F1
score on the MSR-ATC test set in Table 6. To the
best of our knowledge, this is the first work that
reports ROUGE metric scores on the MSR-ATC
dataset. Note that we only compare our model
with ABS since the others are not publicly avail-
able. Our SEASS achieves 10.63 ROUGE-2 F1 and
outperforms the s2s+att baseline by 1.02 points.

1101



th
e

co
u
n
ci

l

o
f

e
u
ro

p
e

's h
u
m

a
n

ri
g
h
ts

co
m

m
is

si
o
n
e
r

sl
a
m

m
e
d

th
u
rs

d
a
y

a
s

`` u
n
a
cc

e
p
ta

b
le

'' co
n
d
it

io
n
s

in fr
a
n
ce

's o
v
e
rc

ro
w

d
e
d

a
n
d

d
ila

p
id

a
te

d

ja
ils

, w
h
e
re

so
m

e

#
#

in
m

a
te

s

h
a
v
e

co
m

m
it

te
d

su
ic

id
e

th
is

y
e
a
r

.

Figure 3: First derivative heat map of the output with respect to the selective gate. The important
words are selected in the input sentence, such as “europe”, “slammed” and “unacceptable”. The output
summary of our system is “council of europe slams french prison conditions” and the true summary is
“council of europe again slams french prison conditions”.

Models RG-1 RG-2 RG-L

ABS (beam) 20.27- 5.26- 17.10-

s2s+att (greedy) 15.15- 4.48- 13.62-

s2s+att (beam) 22.65- 9.61- 21.39-

SEASS (greedy) 19.77 6.44 17.36
SEASS (beam) 25.75 10.63 22.90

Table 6: Full length ROUGE F1 evaluation on
MSR-ATC test set. Beam search are used in both
the baselines and our method. The superscript -

indicates that our SEASS model performs signif-
icantly better than it as given by the 95% confi-
dence interval in the official ROUGE script.

6 Discussion

In this section, we first compare the performance
of SEASS with the s2s+att baseline model to illus-
trate that the proposed method succeeds in select-
ing information and building tailored representa-
tion for abstractive sentence summarization. We
then analyze selective encoding by visualizing the
heat map.

Effectiveness of Selective Encoding We fur-
ther test the SEASS model with different sentence
lengths on English Gigaword test sets, which are
merged from the Rush et al. (2015) test set and our
internal test set. The length of sentences in the test
sets ranges from 10 to 80. We group the sentences
with an interval of 4 and get 18 different groups
and we draw the first 14 groups. We find that the
performance curve of our SEASS model always
appears to be on the top of that of s2s+att with a
certain margin. For the groups of 16, 20, 24, 32,
56 and 60, the SEASS model obtains big improve-
ments compared to the s2s+att model. Overall,
these improvements on all groups indicate that the
selective encoding method benefits the abstractive
sentence summarization task.

10 20 30 40 50 60

Input Sentence Length

0

5

10

15

20

25

30

R
O

U
G

E
-2

 F
1
 S

co
re

s

SEASS

s2s+att

Figure 4: ROUGE-2 F1 score on different groups
of input sentences in terms of their length for
s2s+att baseline and our SEASS model on English
Gigaword test sets.

Saliency Heat Map of Selective Gate Since the
output of the selective gate network is a high di-
mensional vector, it is hard to visualize all the gate
values. We use the method in Li et al. (2016) to
visualize the contribution of the selective gate to
the final output, which can be approximated by the
first derivative. Given sentence words x with asso-
ciated output summary y, the trained model asso-
ciates the pair (x, y) with a score Sy(x). The goal
is to decide which gate g associated with a spe-
cific word makes the most significant contribution
to Sy(x). We approximate the Sy(g) by comput-
ing the first-order Taylor expansion since the score
Sy(x) is a highly non-linear function in the deep
neural network models:

Sy(g) ≈ w(g)T g + b (19)

where w(g) is first the derivative of Sy with re-
spect to the gate g:

w(g) =
∂(Sy)

∂g
|g (20)

1102



We then draw the Euclidean norm of the first
derivative of the output y with respect to the se-
lective gate g associated with each input words.

Figure 3 shows an example of the first derivative
heat map, in which most of the important words
are selected by the selective gate such as “eu-
rope”, “slammed”, “unacceptable”, “conditions”,
and “france”. We can observe that the selective
gate determines the importance of each word be-
fore decoder, which releases the burden of it by
providing tailored sentence encoding.

7 Conclusion

This paper proposes a selective encoding model
which extends the sequence-to-sequence model
for abstractive sentence summarization task. The
selective mechanism mimics one of the human
summarizers’ behaviors, selecting important in-
formation before writing down the summary. With
the proposed selective mechanism, we build an
end-to-end neural network summarization model
which consists of three phases: encoding, selec-
tion, and decoding. Experimental results show
that the selective encoding model greatly improves
the performance with respect to the state-of-the-
art methods on English Gigaword, DUC 2004 and
MSR-ATC test sets.

Acknowledgments

We thank Chuanqi Tan, Junwei Bao, Shuangzhi
Wu and the anonymous reviewers for their helpful
comments. We also thank Alexander M. Rush for
providing the dataset for comparison and helpful
discussions.

References
Ayana, Shiqi Shen, Zhiyuan Liu, and Maosong Sun.

2016. Neural headline generation with minimum
risk training. CoRR abs/1604.01904.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of 3rd
International Conference for Learning Representa-
tions. San Diego.

Michele Banko, Vibhu O Mittal, and Michael J Wit-
brock. 2000. Headline generation based on statis-
tical translation. In Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics. Association for Computational Linguistics,
pages 318–325.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. journal of machine learning research
3(Feb):1137–1155.

Jianpeng Cheng and Mirella Lapata. 2016. Neural
summarization by extracting sentences and words.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers). Association for Computational Lin-
guistics, Berlin, Germany, pages 484–494.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP). Association
for Computational Linguistics, Doha, Qatar, pages
1724–1734.

Sumit Chopra, Michael Auli, and Alexander M. Rush.
2016. Abstractive sentence summarization with at-
tentive recurrent neural networks. In Proceedings of
the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies. Association
for Computational Linguistics, San Diego, Califor-
nia, pages 93–98.

Bonnie Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: A parse-and-trim approach to head-
line generation. In Proceedings of the HLT-NAACL
03 on Text summarization workshop-Volume 5. As-
sociation for Computational Linguistics, pages 1–8.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Aistats. volume 9, pages 249–256.

Ian J Goodfellow, David Warde-Farley, Mehdi Mirza,
Aaron C Courville, and Yoshua Bengio. 2013. Max-
out networks. ICML (3) 28:1319–1327.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers).
Association for Computational Linguistics, Berlin,
Germany, pages 1631–1640.

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati,
Bowen Zhou, and Yoshua Bengio. 2016. Pointing
the unknown words. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers). Asso-
ciation for Computational Linguistics, Berlin, Ger-
many, pages 140–149.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of 3rd International Conference for Learning Repre-
sentations. San Diego.

1103



Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence 139(1):91–107.

Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Juraf-
sky. 2016. Visualizing and understanding neural
models in nlp. In Proceedings of the 2016 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, San Diego, California, pages 681–691.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Text summariza-
tion branches out: Proceedings of the ACL-04 work-
shop. Barcelona, Spain, volume 8.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Compu-
tational Linguistics, Lisbon, Portugal, pages 1412–
1421.

Ramesh Nallapati, Bowen Zhou, Ça glar Gulçehre,
and Bing Xiang. 2016. Abstractive text summariza-
tion using sequence-to-sequence rnns and beyond.
In Proceedings of The 20th SIGNLL Conference on
Computational Natural Language Learning.

Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction. Association for Computational Linguis-
tics, Stroudsburg, PA, USA, AKBC-WEKEX ’12,
pages 95–100.

Paul Over, Hoa Dang, and Donna Harman. 2007. Duc
in context. Information Processing & Management
43(6):1506–1520.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neural
networks. ICML (3) 28:1310–1318.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Lisbon, Portugal, pages 379–389.

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Minimum
risk training for neural machine translation. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers). Association for Computational Linguistics,
Berlin, Germany, pages 1683–1692.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks

from overfitting. Journal of Machine Learning Re-
search 15(1):1929–1958.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.

Kristina Toutanova, Chris Brockett, Ke M. Tran, and
Saleema Amershi. 2016. A dataset and evaluation
metrics for abstractive compression of sentences and
short paragraphs. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, Austin, Texas, pages 340–350.

Lei Yu, Jan Buys, and Phil Blunsom. 2016. Online
segment to segment neural transduction. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, Austin, Texas, pages
1307–1316.

David Zajic, Bonnie J Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sen-
tence compression as a tool for document summa-
rization tasks. Information Processing & Manage-
ment 43(6):1549–1570.

Wenyuan Zeng, Wenjie Luo, Sanja Fidler, and Raquel
Urtasun. 2016. Efficient summarization with
read-again and copy mechanism. arXiv preprint
arXiv:1611.03382 .

1104


	Selective Encoding for Abstractive Sentence Summarization

