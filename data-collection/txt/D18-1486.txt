




































MCapsNet: Capsule Network for Text with Multi-Task Learning


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4565–4574
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

4565

MCapsNet: Capsule Network for Text with Multi-Task Learning

Liqiang Xiao1,2, Honglun Zhang1,2, Wenqing Chen1,2, Yongkun Wang3, Yaohui Jin1,2
1 State Key Lab of Advanced Optical Communication System and Network,

Shanghai Jiao Tong University
2 MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University

3 Network and Information Center, Shanghai Jiao Tong University
{jinyh}@sjtu.edu.cn

Abstract

Multi-task learning has an ability to share the
knowledge among related tasks and implic-
itly increase the training data. However, it
has long been frustrated by the interference
among tasks. This paper investigates the per-
formance of capsule network for text, and pro-
poses a capsule-based multi-task learning ar-
chitecture, which is unified, simple and ef-
fective. With the advantages of capsules for
feature clustering, proposed task routing algo-
rithm can cluster the features for each task in
the network, which helps reduce the interfer-
ence among tasks. Experiments on six text
classification datasets demonstrate the effec-
tiveness of our models and their characteristics
for feature clustering.

1 Introduction

Multi-task learning (MTL) has achieved a great
success in the field of natural language processing,
which can share the knowledge among multiple
tasks, implicitly increasing the volume of training
data. The combination of multi-task learning and
deep neural networks (DNNs) generates a further
synergy via the regularization effect on the DNNs
(Collobert and Weston, 2008), which helps allevi-
ate the overfitting and learn a more universal pre-
sentation.

Inspired by this, more DNN-based multi-task
learning models are proposed to improve the per-
formance. As depicted in Figure 1, they can be
categorized into three groups by structure: tree
scheme (Collobert and Weston, 2008; Liu et al.,
2015), parallel scheme (Liu et al., 2016) and me-
diate scheme (Zhang et al., 2017). Tree scheme
reuses some shallow layers of network and sepa-
rates the higher layers for different tasks, which is
the most common architecture for MTL but can
only share the low-level knowledge. To share
deeper level knowledge among the tasks, more
layers are linked in parallel and mediate schemes.

Tree Scheme Parallel Scheme Mediate Scheme

y n ym

X

y n ym

XX X X

y n ym

layers layers

layers

layers layers layers layers

layers layers layers layers

layers

Figure 1: Three schemes of multi-task learning

But this would severely suffer from the interfer-
ence among tasks. Useless features following the
helpful ones are fully shared among tasks, which
may contaminate the feature spaces of tasks by
useless ones. Besides, models under these two
schemes usually employ multiple subnets in the
structures, which would contain more parameters
and are hard to train.

Apparently, there is a contradiction between
knowledge sharing and interference. Sharing too
much between tasks would inevitably bring about
the interference that feature space for each task
may be contaminated by others. Shared useless
features may mislead the prediction of network.
This dilemma is caused by the lack of manage-
ment for sharing process, in which the network
can not discriminate the features and collect the
appropriate features for each task.

Capsule network (Hinton et al., 2011; Mousa
et al., 2017; Hinton et al., 2018) embeds the fea-
tures into capsules and connects the neighbor lay-
ers via “routing-by-agreement”. The dynamic
routing algorithm has an ability to decide the
routes of capsules, namely, to cluster the features
for each category. So, intuitively this property of
capsule network can be employed in MTL to dis-
criminate the features for tasks.

In this paper, we explore the performance of
capsule network for text (CapsNet-1, CapsNet-
1) and show the benefits and potential of cap-



4566

sule network for NLP. Then we mainly propose
a capsule-based architecture for multi-task learn-
ing (McapsNet), which is unified, simple, effec-
tive and can cluster the feature for tasks. We
designed a Task Routing algorithm to route the
feature flows to tasks and vote for the classes,
which can reduce the interference. In extensive ex-
periences, our approach achieves competitive re-
sults in single-task scenario and shows obvious
improvement in multi-task scenario, which proves
our approach effective and its ability to reduce the
interference among multiple tasks. Also, our visu-
alization experiments intuitively show the feature
clustering mechanism and how it helps make right
predictions.

The contribution of this paper are three-folds:

• This paper investigates the performance of
capsule network on text and designs two ef-
fective capsule-based models for text classifi-
cation, which give clear improvement to sev-
eral benchmarks.

• We novelly combine the capsule and multi-
task learning, which can help reduce the in-
terference among tasks.

• Proposed task routing algorithm can route the
capsules to multiple tasks, by which the fea-
tures is clustered into groups for tasks.

2 Convolutional Neural Network and
Multi-Task Learning

Capsule network is based on the convolutional
neural network (CNN) and uses a lot of convo-
lution operations. The main differences between
them are that capsule network uses vectors to rep-
resent the features and discards the pooling oper-
ation. CNN is good at feature extraction and can
capture short and long range relations through the
feature graph over text sequence (Kalchbrenner
et al., 2014; Kim, 2014). In this section, we pro-
vide some formulations for CNN and some back-
ground knowledge for multi-task learning.

2.1 Single-Task CNN for Text Classification

Given a text sequence x1:l = x1x2 · · ·xl of length
l, the target of CNN is to predict the category ŷ of
x1:l from a set {y1, y2, · · · , yC}, or a one-hot form
of ŷ, where C is the class number. Using f(·)
denote the network, the prediction process can be
formalized as f(x1, x2, · · · , xl) = ŷ.

For details, convolutional neural network f(·)
first uses a lookup table to embed the word se-
quence x1:l into vectors x. Then CNN produces
the representation of the input sequence by stack-
ing the layers of convolution, pooling and fully-
connected in order.

F = K ∗ x (1)
F̂ = p(F) (2)

ŷ = wF̂+ b, (3)

where K is the kernel of convolution operation ∗;
p(·) denotes the pooling operation; F and F̂ repre-
sent the feature maps; w and b denote the weight
and bias respectively in fully connected layer.

The parameters of the network are optimized
via all kinds of SGD (stochastic gradient decent)
algorithms to minimize the loss between predic-
tion ŷ and ground truth label ỹ

l(ŷ, ỹ) = −
N∑
i=1

C∑
j=1

ỹij log(ŷ
i
j), (4)

where i, j enumerate the training samples and
classes respectively.

2.2 Multi-Task Learning
Multi-task learning model is usually the vari-
ant or combination of single-task ones (CNNs,
RNNs or DNNs) like the architectures illustrated
in Figure 1. Given K text classification tasks
{T1, T2, · · · , TK}, a multi-task learning model
f(·) shall have the ability to make prediction for
samples x(k)i from each task Tk.

ŷ
(k)
i = f(x

(k)
i ) (5)

And the overall loss for all the tasks is usually a
linear combination of the costs for each.

L = −
K∑
k=1

λk

Nk∑
i=1

Ck∑
j=1

ỹ
(k)
i,j log ŷ

(k)
i,j (6)

where λk,Nk and Ck denote the weight, number
of training samples and class number of task Tk.

3 Capsule Networks for Text

Capsule network (CapsNet) is first proposed by
Sabour et al. (2017) for image classification,
which is position sensitive and shows strong per-
formance on some classification tasks. As de-
picted in Figure 2, we propose several capsule net-
works for text, which are suitable for text rep-
resentation and multi-task learning. They are



4567

comprised of convolutional layer, primary capsule
layer and class capsule layer. In the rest of this sec-
tion, we will first give the formulation of single-
task capsule networks (CapsNet-1 and CapsNet-
2) for text classification, and then transfer it into a
multi-task version (McapsNet).

3.1 Primary Capsule Layer
Given an embedded sample of x ∈ Rl×d with
length of l and word vectors of d-dimension, cap-
sule network first employs a plain convolution
layer to extract the local features from N-grams.
Each kernel Ki with a bias b emits a feature maps
Fi by convolution.

Fi = x ∗Ki + b (7)

By assembling I feature maps together, we have a
I-channel layer

F = [F1,F2, · · · ,FI ] (8)

The generated feature maps are then fed into
the primary capsule layer, piecing the instantiated
parts together via another convolution. Primary
capsules use vectors instead of scales to preserve
the instantiated parameters for each feature, which
can not only represent the intensity of activation
but also record some details of the instantiated
parts in input. In this way, capsule can be regarded
as a short representation of instantiated parts that
are detected by kernel.

Sliding over the feature map F, each kernel Kj
would output a series of capsules pj ∈ Rd of d-
dimension. These capsules comprise a channel Pj
of primary capsule layer.

Pj = g(Kj ∗ F+ b) (9)

where g is the nonlinear squash function and b is
the capsule bias term. All the J channels can be
arranged as

P = [P1,P2, · · · ,PJ ] (10)

3.2 Connection Between Capsule Layers
Capsule network generates the capsules in next
layer using “routing-by-agreement”. This pro-
cess takes the place of pooling operation that usu-
ally discards the location information, which helps
augment the robust of the network and also helps
cluster features for prediction.

Between two neighbor layers l and l + 1, a
“prediction vectors” ûj|i is first computed from

the capsule ui in lower layer l, by multiplying a
weight matrix Wij

ûj|i = Wijui (11)

Then, in the higher layer l+1 a capsule sj is gener-
ated by the linear combination of all the prediction
vectors with weights cij

sj =
∑
i

cijuj|i (12)

where cij are coupling coefficients decided by the
iterative dynamic routing process.

Coupling coefficients are calculated by a “rout-
ing softmax” function on original logits bij , which
are the log prior probability that capsule i should
be coupled to capsule j.

cij =
exp(bij)∑
j exp(bij)

(13)

This process of “routing softmax” guarantee the
sum of all the coefficients for capsule j is 1.

The length of capsule represents the probability
that the input sample has the object capsule de-
scribes, that is the activation of capsule. So the
length of capsule is limited in range [0, 1] with a
non-linear squashing function.

vj =
∥sj∥2

1 + ∥sj∥
sj

∥sj∥2
(14)

By that, the short vectors are pushed to shrink to
zero length, and long ones are pushed to one.

3.3 Dynamic Routing
Suppose capsule layer l has been generated. We
have to decide the intensity of the connections be-
tween capsule i and j from l-th layer to (l + 1)-th
layer, that is the coupling coefficient cij . The ini-
tial digit of coupling coefficient bij is updated with
routing by agreement aij , which is calculated by a
scale product between capsules in two layers.

aij = ûj|i · vj (15)

Value of agreement aij is added to the digit to
calculated the capsules in the next layer.

bij ← bij + aij (16)

And the whole process for update (Eq.(13)→(12)
→(14)→(15)→(16)) is conducted iteratively to
optimize the coupling coefficients and the capsules
in the next layer.



4568

……

Conv1 PrimaryCaps
ClassCaps

Length

Dynamic

Routing

Word Vector

256 8×32 16

||L2||

Concat3

4

5

100{

{300
Conv2

…
…

Task 1

Task 2

Task K

…
…

ClassCaps

Task

Routing

16
Task Routing

{
{
{

…
…

Capsule

Scale

……

Length

||L2||

kernel 

size

CapsNet-1

CapsNet-2 MCapsNet

Figure 2: Architectures of capsule networks for text

3.4 Class Capsule Layer and Loss
Class capsule layer, as the top-level layer, is com-
prised of C class capsules, in which each one cor-
responds to a category. The length of instantiated
parameters in each capsule represents the proba-
bility that the input sample belongs to this cate-
gory, and the direction of each set of instantiated
parameters preserves the characteristics of the fea-
tures, which could be regarded as an encoded vec-
tor for the input sample.

Margin Loss To increase the difference between
the lengths of classes, CapsNet utilizes a separated
margin loss:

Lj =Gj max(0,m
+ − ∥vj∥)2+

λ(1−Gj)max(0,m− − ∥vj∥)2
(17)

where vj is the capsule for class j; m+ and m−

is the top and bottom margins respectively, which
help push the length to shrink beyond two mar-
gins; Gj = 1 if and only if class j is the ground
truth:

Gj =

{
1 ỹj = 1
0 ỹj = 0

(18)

λ is the weight for the absent classes, which
reduces the weight of absent classes, avoiding

shrinking the lengths of all the capsules too much
at prophase training. In this paper, λ is set to 0.5.

Orphan Category A drawback of CapsNet is
that it tends to account for everything in the input
sampling, including some “background” informa-
tion such as stop word and punctuations that would
interfere the prediction. So an orphan category is
added in class capsules in the output layer, which
belongs to none of the categories of the task. The
orphan category would help collect the less con-
tributive capsules that contain too much “back-
ground” information, which reduces the interfer-
ence for normal categories.

3.5 Substitutional Modules for Multi-Task
Task Routing
Dynamic routing algorithm is first proposed by
Sabour et al. (2017), which displaces the pooling
operation used in conventional convolution neural
network. It maintains the position information for
features, which is beneficial to both image and text
representation. More importantly, this routing-by-
agreement method has an ability to cluster the fea-
tures into each class.

Inspirited by this, we employ this thought to
cluster the features for different tasks and propose
the Task Routing algorithm, which gives a simple
and efficient solution to the problem that existing



4569

Algorithm 1 Task Routing Algorithm

1: function ROUTING(û(k)j|i , a
(k)
ij , r, l )

2: for i = 0→ r do
3: for all capsule i in layer l and capsule
4: j in task k:
5: c

(k)
i = softmax(b

(k)
i )

6: for all capsule j in layer l + 1:
7: v

(k)
j = g(

∑
i c

(k)
j|i û

(k)
j|i )

8: for all capsule i in layer l and capsule
9: j in task k:

10: b
(k)
ij = b

(k)
ij +a

(k)
ij , a

(k)
ij = û

(k)
j|i ·v

(k)
j

11: end for
12: return v(k)j
13: end function

MTL models (Liu et al., 2017; Ruder et al., 2017;
Fang et al., 2017) want to address: “What fea-
ture should be shared and what should not among
tasks?” By that, network can decide the contri-
bution of the features for each tasks and set the
appropriate coupling coefficients between features
and tasks.

More concretely, we introduce a coupling coef-
ficient c(k)ij between capsule i in l-th layer and cap-
sule j in class capsule (l + 1)-th layer for task k,
which is the result of a softmax function on b(k)ij .

c
(k)
ij = softmax(b

(k)
ij ) (19)

Then, instantiated parameter v(k)j of capsule j in
task k is calculated by

v
(k)
j =

∑
i

c
(k)
ij · û

(k)
j|i (20)

where û(k)j|i = Wk,j,iui

Coupling coefficient c(k)ij is restricted in range
[0, 1], which represents the probability that cap-
sule i belongs to class capsule j in task k. And
it is update by the algorithm is described in Algo-
rithm 3.5.

Multi-Task Loss
The loss for each task is the sum of margin losses
for all the classes

∑C
j=1 L

(k)
j . By linearly combin-

ing the loss for every task, we get multi-task loss

L =
K∑
k=1

β(k)
C∑

j=1

L
(k)
j . (21)

where β(k) is the weight for each loss and∑K
k=1 β

(k) = 1. In this paper, all the β(k) are set
to be 1/K to make a balance among K tasks.

Multi-Task Training
In order to juggle several tasks in a unified net-

work, following (Collobert and Weston, 2008),
each task is trained alternatively in a stochastic
manner. The steps can be described as follows:
1. Pick up a task k randomly; 2. Select an arbi-
trary sample s from the task k; 3. Feed the sample
s into the McapsNet and update the parameters; 4.
Go back to 1.

3.6 Architectures of CapsNets for Text

As illustrated in Figure 2, we propose a capsule-
based multi-task learning architecture Mcap-
sNet, which is base on the single-task structures
CapsNet-1 and CapsNet-2. Architectures for them
are detailed as following.

CapsNet-1 As depicted in Figure 2, CapsNet-1
is a fundamental framework with three layers. The
first layer is a plain convolution operation with 256
kernels with window size of 3 and stride of 1. For
activation function, we use ReLU to augment non-
linearity. This layer helps extract local features
from the input sequences, which is the base to con-
struct primary capsules.

Primary capsule layer employs 32 kernels with
window size of 3 and stride of 1. The emitted pri-
mary capsules are 8-dimensional, which have big-
ger respective field, helping reassemble the piece
features into wholes.

Last one is the class capsule layer, which is
comprised of 16-dimensional capsules for the
classes. They are connected to PrimaryCaps
with routing-by-agreement and the coupling coef-
ficients are updated by dynamic routing algorithm.

CapsNet-2 On this basis of CapsNet-1,
CapsNet-2 upgrades the convolutional layer and
uses multiple kernel sizes, which enriches the
features. And concatenating1 them up allows
primary capsule see the features with different
kernel sizes in the same time.

MCapsNet McapsNet is a unified multi-task
structure based on CapsNet-2. It replaces the dy-
namic routing with task routing (Algorithm 3.5),
which enables the network to route the features to

1We use padding to ensure the sizes of feature maps are
equal.



4570

Dataset Train Dev Test Classes Type
MR 9500 - 1100 2 review
SST-1 8544 1101 2210 5 sentiment
SST-2 6920 872 1821 2 sentiment
Subj 9000 - 1000 2 subjectivity
TREC 5900 - 500 6 question
AG’s 120k - 7600 4 news

Table 1: Statistics for six datasets

each tasks. And the whole network is optimized in
a stochastic way with multi-task training (Section
3.5).

Implement Details For word embedding, we
use the word vectors in Word2Vec (Mikolov et al.,
2013), which is 300-dimensional and has 3M vo-
cabularies. And all the routing logits b(k)ij is ini-
tialized to zero, so that all the capsules in adjacent
layers (ûj|i,vj) are connected with equal possi-
bility cij . The coupling coefficients are updated
by routing with 3 iterations, which performs best
for our approach. For training, we use Adam opti-
mizer (Kingma and Ba, 2014) with exponentially
decaying learning rate. Moreover, we use mini-
batch with size of 8 for all the datasets.

4 Experiment

We test our capsule-based models on six datasets
in both single-task and multi-task scenarios to
demonstrate the effectiveness of our approaches.
We also in this section conduct some investiga-
tions like ablation study and visualization to give
a comprehensive understanding to the characteris-
tics of our models.

4.1 Datasets
For both single-task and multi-task scenarios, we
conduct extensive experiments on six benchmarks:
movie reviews (MR) (Bo and Lee, 2005), Stanford
Sentiment Treebank (SST-1 and SST-2) (Socher
et al., 2013), subjectivity classification (Subj)
(Pang et al., 2004), question dataset (TREC) (Li
and Roth, 2002), AG’s news corpus (Mousa et al.,
2017). These datasets cover a wide range of text
classification tasks, which can fully test a model
and the details are listed in Table 1.

4.2 Competitors
To demonstrate the effectiveness of our capsule
network, we compare the single-task architec-
tures with several state-of-the-art models, involv-
ing LSTM/BiLSTM (Cho et al., 2014), LSTM

Dataset MR SST-1 SST-2 Subj TREC AG’s
LSTM 75.9 45.9 80.6 89.3 86.8 86.1

BiLSTM 79.3 46.2 83.2 90.5 89.6 88.2
LR-LSTM 81.5 48.2 87.5 89.9 - -
VD-CNN - - - - - 91.3

DCNN - 48.5 86.8 - 93.0 -
CNN-MC 81.1 47.4 88.1 93.2 92.2 -
CapsNet-1 81.5 48.1 86.4 93.3 91.8 91.1
CapsNet-2 82.4 48.7 87.8 93.6 92.9 92.3
- Orphan 81.9 48.3 87.2 93.4 92.6 91.7

Table 2: Single-task results. Row “- Orphan category”
denotes a variant of CapsNet-2 without orphan cate-
gory

regularized by linguistic knowledge (LR-LSTM)
(Qian et al., 2016), very deep network (VD-CNN)
(Conneau et al., 2016), dynamic CNN (DCNN)
(Kalchbrenner et al., 2014), CNN with multiple
channels (CNN-MC) (Kim, 2014). Also, we com-
pare the multi-task architecture (Figure 2) with
several strong baselines of multi-task learning, in-
cluding a general architecture for multi-task learn-
ing (MT-GRNN) (Zhang et al., 2017), recurrent
neural network based multi-task learning (MT-
RNN) (Liu et al., 2016), convolutional neural net-
work with multi-task learning (MT-DNN) (Col-
lobert and Weston, 2008), deep neural network
with multi-task learning (MT-CNN) (Liu et al.,
2015).

4.3 Single-Task Learning Results

We first test our approach on six datasets for text
classification under the scheme of single-task. As
Table 2 shows, our single-task network enhanced
by capsules is already a strong model. CapsNet-
1 that has one kernel size obtains the best accu-
racy on 2 out of 6 datasets, and gets competitive
results on the others. And CapsNet-2 with multi-
ple kernel sizes further improves the performance
and get best accuracy on 4 datasets. This proves
our capsule networks are effective for text. Partic-
ularly, our capsule network outperforms conven-
tional CNNs like DCNN, CNN-MC and VD-CNN
with a large margin (by average 1.1%, 0.7% and
1.0% respectively), which shows the advantages
of capsule network over conventional CNNs for
clustering features and leveraging the position in-
formation.

Routing Iteration The coupling coefficients cij
are updated by dynamic routing algorithm, which
determines the connections between the capsules.
To find the best updating iteration for coupling co-



4571

mr_iter3_loss

step 1 iteration 2 iteration 3 iteration 4 iteration 5 iteration 7 iteration

0 0.31990772 0.32777038 0.35708323 0.30876166 0.33376288 0.43252164

100 0.2165044 0.22952877 0.19316821 0.22000548 0.21656269 0.22370228

200 0.21138513 0.22009197 0.23139681 0.2095856 0.22682545 0.21267971

300 0.22478297 0.2110215 0.24012111 0.21540533 0.21765772 0.21499902

400 0.21555959 0.22160663 0.21874493 0.2191698 0.23053917 0.21699865

500 0.20600384 0.21363162 0.18633538 0.21734108 0.21815333 0.21746612

600 0.2130103 0.20203112 0.17911553 0.21643545 0.21735527 0.21233499

700 0.19938451 0.23581365 0.17411417 0.21574022 0.21573226 0.21654496

800 0.19907556 0.227563 0.14065415 0.21677263 0.21779424 0.21902196

900 0.19709052 0.1909296 0.22373466 0.21057303 0.21403673 0.22480017

1000 0.18997657 0.19407482 0.24778445 0.21447049 0.2167942 0.21594681

1100 0.20069836 0.22341812 0.17452972 0.21565828 0.21165203 0.21713184

1200 0.1358376 0.19161437 0.12416928 0.22827208 0.21767393 0.1865164

1300 0.24192365 0.2502587 0.14073405 0.21395075 0.1956843 0.2076964

1400 0.24594069 0.17846859 0.15433379 0.21534327 0.19485447 0.19316548

1500 0.14343393 0.23199289 0.058949206 0.21565448 0.20615155 0.2027618

1600 0.13204841 0.07546147 0.13606499 0.21872896 0.1940549 0.18980762

1700 0.078342795 0.1006265 0.1095981 0.21600798 0.24048766 0.21178418

1800 0.19616298 0.084692195 0.13713281 0.21660198 0.20626166 0.2086841

1900 0.18376194 0.1457739 0.108214475 0.21599032 0.17014022 0.18203387

2000 0.2505575 0.15862198 0.13291651 0.22306885 0.20859854 0.16577458

2100 0.20313756 0.150289 0.16300716 0.21577688 0.21566515 0.21336424

2200 0.113498576 0.17007406 0.12576085 0.21669921 0.19889027 0.21370491

2300 0.08823628 0.18244454 0.09739958 0.21900685 0.19862902 0.13970172

2400 0.18449302 0.13019164 0.059194855 0.2158218 0.21441466 0.24924363

2500 0.14137962 0.027922168 0.06349837 0.21553266 0.18297584 0.15660347

2600 0.06318494 0.0083872555 0.13506271 0.22020619 0.049219918 0.06674059

2700 0.12604533 0.057891004 0.0863096 0.21330038 0.09685729 0.075393796

2800 0.06678611 0.056647964 0.11831023 0.211519 0.034514483 0.11877781

2900 0.013979337 0.055679005 0.043369725 0.21656843 0.11880093 0.139684

3000 0.09075422 0.01834401 0.069736615 0.20968968 0.06694293 0.11415998

3100 0.071527496 0.071867496 0.04040062 0.21676293 0.062198877 0.13664688

3200 0.053069513 0.011909003 0.074893095 0.21546358 0.2097177 0.09671463

3300 0.13160191 0.05359852 0.105486326 0.21507561 0.0346353 0.103552915

3400 0.05588995 0.041564412 0.066810705 0.21444169 0.11614575 0.13282628

3500 0.049774844 0.023115948 0.07867427 0.21574582 0.24141549 0.015245671

3600 0.00533909 0.07540489 0.070457004 0.21627496 0.111639 0.09389045

3700 0.06328302 0.0769447 0.03755875 0.21654026 0.13476305 0.08049823

3800 0.037597798 0.015297024 0.024833435 0.21532282 0.019858109 0.07756405

3900 0.017376851 0.03153728 0.028519789 0.21998401 0.19239725 0.0706502

4000 0.015832186 0.06606675 0.019566512 0.22106846 0.081912905 0.07202703

4100 0.024164455 0.015849056 0.025040613 0.21752763 0.10874183 0.021766221

4200 0.05139856 0.013023706 0.011992501 0.20341182 0.04201607 0.1068824

4300 0.03430857 0.059574388 0.019949485 0.21218589 0.10918676 0.047409087

4400 0.06850299 0.05789706 0.03192132 0.1934472 0.060641877 0.017321965

4500 0.028439771 0.04577714 0.06652445 0.18472269 0.031587753 0.02464495

4600 0.00997417 0.005764379 0.01647066 0.17637597 0.028014591 0.01935349

4700 0.04940702 0.032073725 0.07881074 0.1750345 0.13018186 0.023507616

4800 0.020413583 0.07540906 0.029650116 0.2195204 0.038481377 0.045384612

4900 0.019826425 0.031753153 0.021980677 0.1424555 0.03871046 0.019794926

5000 0.028047139 0.009816681 0.01173399 0.14830378 0.034620147 0.060023524

5100 0.03570168 0.022174701 0.013476413 0.18717153 0.010447311 0.014840336

5200 0.050749723 0.005398215 0.033909425 0.2046503 0.011018614 0.024433482

5300 0.099622525 0.004649097 0.047538925 0.2092239 0.03152306 0.0054741725

5400 0.0053426665 0.008371854 0.006597732 0.15079452 0.006303721 0.0047035264

5500 0.05191873 0.016815454 0.016768238 0.07482985 0.0057420097 0.040190328

5600 0.049730867 0.02349401 0.008496291 0.13487497 0.033842973 0.006400237

5700 0.031844895 0.03890676 0.042231962 0.16211028 0.035531938 0.017351853

5800 0.011545923 0.012152319 0.007615051 0.09252592 0.09976231 0.014460339

5900 0.018437417 0.021393463 0.03422167 0.1987526 0.013888625 0.011644581

6000 0.018853988 0.005956734 0.07615088 0.091175675 0.005306372 0.0042689797

6100 0.018992342 0.010288426 0.053335432 0.16724806 0.0107863145 0.010407194

6200 0.0056506954 0.00776174 0.020618532 0.08833132 0.034110814 0.012261426

6300 0.011831854 0.009550323 0.009947613 0.09596658 0.01110738 0.020155292

6400 0.006566655 0.013650014 0.012848858 0.11858772 0.021222282 0.014613601

6500 0.024205782 0.012546196 0.0048239767 0.16028889 0.00926006 0.0149123315

6600 0.014806292 0.0058124983 0.041874792 0.039060276 0.021106286 0.011592761

6700 0.012256767 0.00361195 0.008751678 0.2021615 0.026892414 0.008640145

6800 0.004717383 0.017772006 0.007069681 0.09619468 0.0032965664 0.004729362

6900 0.014133487 0.031820357 0.01449428 0.037848447 0.020252377 0.01289978

7000 0.019541442 0.06382499 0.023019891 0.050981153 0.017323725 0.0105407955

7100 0.021704191 0.01138805 0.023877008 0.051143013 0.03438755 0.054347344

7200 0.012751048 0.012124202 0.0038398597 0.043297127 0.018402876 0.029276593

7300 0.006295314 0.021420598 0.029609926 0.041524984 0.02546952 0.0075945454

7400 0.0075894385 0.006048003 0.0075462335 0.067313015 0.0053267237 0.0069935857

7500 0.006741333 0.0040838285 0.0066743465 0.059338994 0.03350287 0.006115186

7600 0.014603565 0.029511223 0.015551431 0.0480805 0.0073549934 0.0062353685

7700 0.015592159 0.016174346 0.014908929 0.03194722 0.006968726 0.009626625

7800 0.013729516 0.01248635 0.011632278 0.045669746 0.019316316 0.004811878

7900 0.017690767 0.006325948 0.020566542 0.06595372 0.01010148 0.007357938

8000 0.011015004 0.010788693 0.00907688 0.009306814 0.01526524 0.008390946

8100 0.03067701 0.006466888 0.036440548 0.091463834 0.0040217703 0.0048725065

8200 0.0068208063 0.021921193 0.030572822 0.031231066 0.013773766 0.025552973

8300 0.07634035 0.013739873 0.008865972 0.043190647 0.033992216 0.005076555

8400 0.02858127 0.004716243 0.02021956 0.02758816 0.013960054 0.0045451336

8500 0.008293323 0.004557795 0.007140737 0.0143441325 0.019154403 0.018727811

8600 0.06278364 0.016065609 0.005512653 0.010879653 0.006276448 0.008771067

8700 0.0069642887 0.0069935825 0.018281303 0.0067787454 0.008038342 0.0049465247

8800 0.015416967 0.025033943 0.011917496 0.038743842 0.0135429185 0.0123538375

8900 0.011273504 0.011572633 0.014581191 0.015278485 0.0037106564 0.0046575256

9000 0.036496855 0.0129312975 0.017274378 0.01666871 0.0052732183 0.0149699785

9100 0.0076952637 0.0111958925 0.012719973 0.0069943983 0.011427318 0.004719188

9200 0.011958325 0.0049098213 0.008090422 0.0062036156 0.030276377 0.053471938

9300 0.007843321 0.026734378 0.012808654 0.02021877 0.0037612547 0.009854263

9400 0.014780243 0.007878659 0.0062521882 0.014980534 0.0062496727 0.017763212

9500 0.021542635 0.0539474 0.0089662215 0.019601598 0.00997231 0.011595588

9600 0.020767815 0.0036838378 0.025650078 0.03254368 0.040382072 0.0051811123

Tr
ai

ni
ng

 L
os

s

0

0.1

0.2

0.3

0.4

Training Step
0 1200 2400 3600 4800 6000 7200 8400 9600

3 iteration 5 iteration 1 iteration

Figure 3: Influence of routing iteration

efficients, we test the CapsNet-2 with a series of
iterations (1, 3 and 5) on MR dataset. As shown in
Figure 3, network with 3 iterations convergences
fast and performs best, which stays in line with the
conclusion in (Sabour et al., 2017). So we utilize
3 iterations in all our experiments.

Ablation Study on Orphan Category Orphan
category in class capsule layer helps collect the
noise capsules that contain the ‘background’ infor-
mation like stop words, punctuations or any unre-
lated words. We conduct the ablation experiment
on orphan category, and result (Table 2) shows that
network with orphan category perform better than
the without one by 0.4%. This demonstrates the
effectiveness of orphan category.

4.4 Multi-Task Learning Results

Up to now, we have obtained an optimized single-
task architecture. In this section, we equip
CapsNet-2 with the task routing and multi-task
training procedure, namely the model MCapsNet,
so that this capsule based architecture can learn
several datasets in a unified network. Exten-
sive experiments are conducted in this section to
demonstrate the effectiveness of our multi-task
learning architecture, as well as its ability for fea-
ture clustering.

Multi-Task Performance
We simultaneously train our model McapsNet on
six tasks in Table 1 and compare it with single-
task scenario (Table 3). We can see that our multi-
task architecture clearly improves the performance
over the single task models, which demonstrates
the benefits of our multi-task architecture.

Dataset MR SST-1 SST-2 Subj TREC AG’s Avg.△
BiLSTM 79.3 46.2 83.2 90.5 89.6 88.2 +0
MT-GRNN - 49.2 87.7 89.3 93.8 - +2.6
MT-RNN - 49.6 87.9 94.1 91.8 - +3.5
MT-DNN 82.1 48.1 87.3 93.9 92.2 91.8 +2.9
MT-CNN 81.6 49.0 86.9 93.6 91.8 91.9 +3.0
CapsNet-1 81.5 48.1 86.4 93.3 91.8 91.1 +2.5
CapsNet-2 82.4 48.7 87.8 93.6 92.9 92.3 +3.3
MCapsNet 83.5 49.7 88.6 94.5 94.2 93.8 +4.6

Table 3: Multi-task results of MCapsNet. In column
Avg.△, we use BiLSTM as baseline and calculate the
average improvements over it.

As Table 3 shows, MCapsNet also outperforms
the state-of-the-art multi-task learning models by
at least 1.1%. This shows the advantages of our
task routing algorithm, which can cluster the fea-
tures for each task, instead of freely sharing the
features among tasks.

4.5 Routing Visualization

To show the mechanism how capsule benefits the
multi-task learning, we visualize the coupling co-
efficient c(k)ij ∈ [0, 1] between primary and class
capsules. We use kernel with size 1 for primary
capsule layer so that every capsule represents only
one 3-gram phrase. The strength of these connec-
tions indicates the importance of these 3-grams to
their corresponding task and class.

We feed a random sample from the dataset MR
into MCapsNet. In the first row of Table 4, we
show the most important 3-gram phrases for two
tasks MR and Subj (two classes for each) with
word cloud. The sizes of the grams represent the
weights of coupling coefficients. We can see that
task routing algorithm helps lead the grams into
the most related tasks, which allows each task only
consider the helpful features for them. In another
word, task routing builds a feature space for each
task and avoids they contaminate each other. This
demonstrates that MCapsNet has the ability of fea-
ture clustering, which can benefit MTL by reduc-
ing the interference.

We also illustrate the coupling coefficients se-
quentially for each task. The height of the blue
and gray lines represents the polarity of positiv-
ity and subjectivity respectively. It is clear that
MCapsNet can focus on the appropriate positions
for each task, which helps make the final correct
predication for every task.



4572

MR Subj
positive negative subjective objective

0.49

0.495

0.5

0.505

0.51

it 


's
 

no
t 

so
 

m
uc

h 


en
jo

ya
bl

e 


to
 

w
at

ch
 

as
  it 


is
 

en
lig

ht
en

in
g 


to
 

lis
te

n 


to
 

ne
w

 

si
de

s 


of
  a 


pr
ev

io
us

 

re
al

ity
  ,

an
d 


to
 

vi
si

t 

w
ith

 

so
m

e 


of
 

th
e 


pe
op

le
 

w
ho

 

w
er

e 


ab
le

 

to
 

m
ak

e 


an
 

im
pa

ct
 

in
 

th
e 


th
ea

te
r 

w
or

ld
 

MR
Subj

pos/sub

Table 4: Visualization of the task routing for a positive sample from MR, “it 's not so much enjoyable to watch as
it is enlightening to listen to new sides of a previous reality , and to visit with some of the people who were able to
make an impact in the theater world”

5 Related Work

Related work can be divided into two threads. The
first thread is capsule network, which has been
proven effective on many classification tasks.

Concept of capsule is first proposed by Hin-
ton et al. (2011), which first use vector to de-
scribe the pose of object. This work improves
the representation ability of the neural networks
against the vanilla CNNs and also enhances the
robust of network for transformation. Then dy-
namic routing algorithm is proposed in (Sabour
et al., 2017), which is aimed to displace the pool-
ing operation, building a part-whole relationship
for object recognition. Dynamic routing can main-
tain the position information of features for objects
that pooling operations generally discard. And the
result shows the proposed method improves the
state-of-the-art performance for MNIST dataset.
Next, Hinton et al. (2018) employs the matrix to
depict the pose and, based on EM algorithm de-
signs a new routing procedure between capsule
layers. This work shows strong ability for address-
ing transformation problem and gains significant
improvement on smallNORB dataset.

All these methods are proposed for computer vi-
sion, while in this paper we investigate the benefits
of capsules for text.

The other thread is about multi-task learning.
The earliest idea can be traced back to (Caruana,
1997) and there have been some work completed

in this field to augment the performance. Collobert
and Weston (2008) develop a multi-task learning
model based on CNN. It shares only one lookup
table to train a better word embedding. And Liu
et al. (2015) propose a DNN-based model for
multi-task learning, which shares some low lay-
ers but separate the high-level layers to complete
several different tasks.

Some models are proposed to share deeper lay-
ers of networks, which can exchange high-level
knowledge among tasks and gain better perfor-
mance. (Zhang et al., 2017) and (Liu et al., 2016)
introduce some RNN architectures and design dif-
ferent schemes for knowledge sharing. These tri-
als promote the performance of models, but they
give no consideration to the interference in multi-
task learning. Liu et al. (2017) add the adversar-
ial losses in multi-task RNNs, which can allevi-
ate the interference among tasks by finding a com-
mon feature space for tasks. However, the model
has multiple subnets and various losses, which re-
quires more computation and training skills.

Different from these methods, we use the
thought of capsule in natural language processing
(NLP) field. And proposed a capsule based multi-
task learning architecture with task routing algo-
rithm. This approach can cluster the features for
each task, reducing the interference among them.



4573

6 Conclusion and Future Work

This paper investigates the performance of capsule
network for text representation, and proposes sev-
eral effective architectures. By means of the char-
acteristics of capsule network, we design a unified,
sample yet effective architecture with task routing
for multi-task learning, which has the ability to
clustering the features, building a private feature
space for every task.

In future work, we would like to investigate the
relations of various tasks in multi-task learning by
exploiting the potential of capsule network.

7 Acknowledge

We appreciate the valuable comments from anony-
mous reviewers. We also thank Xuan Luo for
building and maintaining the GPU platforms. And
this research was funded by Major State Re-
search Development Program under Grant No.
2018YFC0830400.

References
Pang Bo and Lillian Lee. 2005. Seeing stars: exploit-

ing class relationships for sentiment categorization
with respect to rating scales. pages 115–124.

Rich Caruana. 1997. Multitask learning. Machine
Learning, 28(1):41–75.

Kyunghyun Cho, Bart Van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. Computer Sci-
ence.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In Ma-
chine Learning, Proceedings of the Twenty-Fifth In-
ternational Conference (ICML 2008), Helsinki, Fin-
land, June 5-9, 2008, pages 160–167.

Alexis Conneau, Holger Schwenk, Loc Barrault, and
Yann Lecun. 2016. Very deep convolutional net-
works for text classification. pages 1107–1116.

Yuchun Fang, Zhengyan Ma, Zhaoxiang Zhang, Xu-
Yao Zhang, and Xiang Bai. 2017. Dynamic multi-
task learning with convolutional neural network. In
Proceedings of the Twenty-Sixth International Joint
Conference on Artificial Intelligence, IJCAI 2017,
Melbourne, Australia, August 19-25, 2017, pages
1668–1674.

Geoffrey Hinton, Nicholas Frosst, and Sara Sabour.
2018. Matrix capsules with em routing.

Geoffrey E. Hinton, Alex Krizhevsky, and Sida D.
Wang. 2011. Transforming auto-encoders. In Inter-
national Conference on Artificial Neural Networks,
pages 44–51.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, ACL 2014, June 22-27, 2014,
Baltimore, MD, USA, Volume 1: Long Papers, pages
655–665.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1746–1751.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. Computer Sci-
ence.

Xin Li and Dan Roth. 2002. Learning question classi-
fiers. Coling, 12(24):556–562.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016.
Recurrent neural network for text classification with
multi-task learning.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.
Adversarial multi-task learning for text classifica-
tion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics, ACL
2017, Vancouver, Canada, July 30 - August 4, Vol-
ume 1: Long Papers, pages 1–10.

Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng,
Kevin Duh, and Ye Yi Wang. 2015. Representation
learning using multi-task deep neural networks for
semantic classification and information retrieval. In
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 912–921.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. Advances in Neural Information Processing
Systems, 26:3111–3119.

Amr Mousa, Bjrn Schuller, Amr Mousa, Bjrn Schuller,
Amr Mousa, and Bjrn Schuller. 2017. Contex-
tual bidirectional long short-term memory recurrent
neural network language models: A generative ap-
proach to sentiment analysis. In Conference of the
European Chapter of the Association for Computa-
tional Linguistics: Volume 1, Long Papers, pages
1023–1032.

Pang, Bo, Lee, and Lillian. 2004. A sentimental ed-
ucation: sentiment analysis using subjectivity sum-
marization based on minimum cuts. Proceedings of
Acl, pages 271–278.



4574

Qiao Qian, Minlie Huang, and Xiaoyan Zhu. 2016.
Linguistically regularized lstms for sentiment clas-
sification. CoRR, abs/1611.03949.

Sebastian Ruder, Joachim Bingel, Isabelle Augenstein,
and Anders Sgaard. 2017. Sluice networks: Learn-
ing what to share between loosely related tasks.

Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton.
2017. Dynamic routing between capsules.

R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D.
Manning, A. Y. Ng, and C. Potts. 2013. Recursive
deep models for semantic compositionality over a
sentiment treebank.

Honglun Zhang, Liqiang Xiao, Yongkun Wang, and
Yaohui Jin. 2017. A generalized recurrent neural
architecture for text classification with multi-task
learning. In Proceedings of the Twenty-Sixth Inter-
national Joint Conference on Artificial Intelligence,
IJCAI 2017, Melbourne, Australia, August 19-25,
2017, pages 3385–3391.


