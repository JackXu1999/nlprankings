



















































Learning Prototypical Event Structure from Photo Albums


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1769–1779,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Learning Prototypical Event Structure from Photo Albums

Antoine Bosselut†, Jianfu Chen‡, David Warren‡, Hannaneh Hajishirzi† and Yejin Choi†
†Computer Science & Engineering, University of Washington, Seattle, WA
{antoineb, hannaneh, yejin}@cs.washington.edu

‡Department of Computer Science, Stony Brook University, Stony Brook, NY
{jianchen, warren}@cs.stonybrook.edu

Abstract

Activities and events in our lives are struc-
tural, be it a vacation, a camping trip, or
a wedding. While individual details vary,
there are characteristic patterns that are
specific to each of these scenarios. For ex-
ample, a wedding typically consists of a
sequence of events such as walking down
the aisle, exchanging vows, and dancing.
In this paper, we present a data-driven ap-
proach to learning event knowledge from a
large collection of photo albums. We for-
mulate the task as constrained optimiza-
tion to induce the prototypical temporal
structure of an event, integrating both vi-
sual and textual cues. Comprehensive
evaluation demonstrates that it is possible
to learn multimodal knowledge of event
structure from noisy web content.

1 Introduction

Many common scenarios in our lives, such as a
wedding or a camping trip, show characteristic
structural patterns. As illustrated in Figure 1, these
patterns can be sequential, such as in a wedding,
where exchanging vows generally happens before
cutting the cake. In other scenarios, there may be a
set of composing events, but no prominent tempo-
ral relations. A camping trip, for example, might
include events such as hiking, which can happen
either before or after setting up a tent.

This observation on the prototypical patterns in
everyday scenarios goes back to early artificial in-
telligence research. Scripts (Schank and Abelson,
1975), an early formalism, were developed to en-
code the necessary background knowledge to sup-
port an inference engine for common sense rea-
soning in limited domains. However, early ap-

-Ring	'me.	
-Exchanging	our	rings.	
-Rings	and	promises.	
	

Kiss	

-Our	first	ever	kiss.	
-You	may	kiss	the	bride.	
-Sealed	with	a	kiss.	
	

Cut	the	cake	

-Cake	cuBng.	
-The	cake	was	so	solid.	
	

-Dancing	excitement.	
-First	dance.	
-Ballroom	dancing.	

reading	vows	 presen'ng	rings	 best	cake	ever	

Photo	albums	

down	the	aisle	

Prototypical	Cap2ons:		

Exchange	rings	Dance	

Learned	Events:	

-Reading	our	vows.	
-Our	vows.	
	

Vows	

Temporal	Knowledge:	

Dance	

…	 …	

Kiss	 Cake	 Vows	 Rings	

Figure 1: We collect photo albums of common scenar-
ios (e.g., weddings) and cluster their images and captions to
learn the hierarchical events that make up these scenarios. We
use constrained optimization to decode the temporal order of
these events, and we extract the prototypical descriptions that
define them.

proaches based on hand-coded symbolic represen-
tations proved to be brittle and difficult to scale.

An alternative direction in recent years has
been statistical knowledge induction, i.e., learn-
ing script or common sense knowledge bottom-up
from large-scale data. While most prior work is
based on text (Pichotta and Mooney, 2014; Jans
et al., 2012; Chambers and Jurafsky, 2008; Cham-
bers, 2013), recent work begins exploring the use
of images as well (Bagherinezhad et al., 2016;
Vedantam et al., 2015).

In this paper, we present the first study for learn-
ing knowledge about common life scenarios (e.g.,
weddings, camping trips) from a large collection
of online photo albums with time-stamped images
and their captions. The resulting dataset includes
34,818 time-stamped photo albums corresponding
to 12 distinct event scenarios with 1.5 million im-
ages and captions (see Table 1 for more details).

We cast unsupervised learning of event struc-
ture as a sequential multimodal clustering prob-

1769



lem, which requires solving two subproblems con-
currently: identifying the boundaries of events and
assigning identities to each of these events. We
formulate this process as constrained optimiza-
tion, where constraints encode the temporal event
patterns that are induced directly from the data.
The outcome is a statistically induced prototypi-
cal structure of events characterized by their visual
and textual representations.

We evaluate the quality and utility of the learned
knowledge in three tasks: temporal event ordering,
segmentation prediction, and multimodal summa-
rization. Our experimental results show the per-
formance of our model in predicting the order of
photos in albums, partitioning photo albums into
event sequences, and summarizing albums.

2 Overview

The high-level goal of this work is unsupervised
induction of the prototypical event structure of
common scenarios from multimodal data. We
assume a two-level structure: high-level events,
which we refer to as scenarios (e.g., wedding, fu-
neral), are given, and low-level events (e.g., dance,
kiss, vows), which we refer to as events, are to be
automatically induced. In this section, we provide
the overview of the paper (Section 2.1), and intro-
duce our new dataset (Section 2.2).

2.1 Approach

Given a large collection of photo albums corre-
sponding to a scenario, we want to learn three as-
pects of event knowledge by (1) identifying events
common to the given scenario (Section 4.1), (2)
learning temporal relations across events (Sec-
tion 4.2), and (3) extracting prototypical captions
for each event (Section 4.3).

To induce the prototypical event structure, an
important subproblem we consider is individual
photo album analysis, where the task is (1) par-
titioning each photo album into a sequence of seg-
ments, and (2) assigning the event identity to each
segment. We present an inference model based
on Integer Linear Programming (ILP) in Section 3
to perform both segmentation and event identi-
fication simultaneously, in consideration of the
learned knowledge that we describe in Section 4.

Finally, we evaluate the utility of the automat-
ically induced knowledge in the context of three
concrete tasks: temporal ordering of photos (Sec-
tion 6.1), album segmentation (Section 6.2), and

scenario # of albums # of images
WEDDING 4689 192K
MARATHON 3961 158K
COOKING 1168 36K
FUNERAL 781 28K
BARBECUE 735 22K
BABY BIRTH 688 21K
PARIS TRIP 4603 306K
NEW YORK TRIP 4205 267K
CAMPING 4063 159K
THANKSGIVING 5928 153K
CHRISTMAS 3449 98K
INDEPENDENCE DAY 548 22K
TOTAL 34,818 1.5M

Table 1: Dataset Statistics: the number of albums and im-
ages compiled for each scenario. The middle horizontal line
separates the scenarios we predict have a well-defined tem-
poral structure (top) from those we predict do not (bottom).

photo album summarization (Section 6.3).

2.2 Dataset
For this study, we have compiled a new corpus of
multimodal photo albums across 12 distinct sce-
narios. It comprises of 34,818 albums containing
1.5 million pairs of online photographs and their
textual descriptions. Table 1 shows the list of sce-
narios and the corresponding data statistics. We
choose six scenarios (the top half of Table 1) that
we expect have an inherent temporal event struc-
ture that can be learned and six that we expect do
not (the bottom half of Table 1).

The dataset is collected using the Flickr API1,2.
We use the scenario names and variations of them
(e.g., Paris Trip, Paris Vacation) as queries for
images. We then form albums from these im-
ages by grouping images by user, sorting them
by timestamp, and extracting groups that are
within a contained time frame (e.g., 24 hours
for a wedding, 5 days for a trip). For all im-
ages, we extract the first sentences of the cor-
responding textual descriptions as captions and
also store their timestamps. This data is publicly
available at https://www.cs.washington.edu/
projects/nlp/protoevents.

3 Inference Model for Multimodal Event
Segmentation and Identification

Given a photo album, the goal of the inference is to
assign events to photos and to segment albums by
event. More formally, given a sequence ofM pho-
tos P = {p1, . . . , pM}, andN learned eventsE =
{e1, . . . , eN}, the task is to assign each photo to a

1https://www.flickr.com/services/api/
2https://pypi.python.org/pypi/flickrapi/1.4.5

1770



PL(kiss	  	  	  	  	  dance)	  =	  .04	  →

vows	  kiss	   toasts	  cut	  cake	   dancing	   aisle	  ready	  

dancing	  ready	   toasts	  aisle	   vows	   kiss	  

Event	  affinity	  scores	  Ac	  Av	  

bc	  :	  	  	  	  	  .31	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  .09	  	  	  	  	  	  	  	  	  	  	  	  	  	  .82	   PL(dance	  	  	  	  	  toast)	  =	  .17	  

PG(vows	  	  	  	  	  toast)	  =	  .84	  

Learned	  
	  Events:	  

Photo	  Album:	  

Event	  Assignments	  and	  Segments:	  
bv	  :	  	  	  	  	  .69	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  .32	  	  	  	  	  	  	  	  	  	  	  	  	  	  .91	  	  	  	  	  

…	  	  
⇒

PG(vows	  	  	  	  	  	  dance)	  =	  .79	  ⇒
→

φEvent

φSeg
φtemporal

Figure 2: The events learned in Section 4 are assigned to photos based on textual (Ac) and visual (Av) affinities, which encode
how well a photo represents an event (φevent). Segmentation scores (φseg) between adjacent photos encourage similar photos
to be assigned the same event. Local transition, PL, and global pairwise ordering, PG, probabilities encode the learned temporal
knowledge between events. φtemporal encourages event assignments toward a learned temporal structure of the scenario.

single event. The event assignment can be viewed
as a latent variable for each photo. We formulate
a constrained optimization (depicted in Figure 2)
that maximizes the objective function, F , which
consists of three scoring components: (a) event as-
signment scores φevent (Section 3.1), (b) segmen-
tation scores φseg (Section 3.2), and (c) temporal
knowledge scores φtemporal (Section 3.3):

F = φevent + φseg + φtemporal (1)

Decision Variables. The binary decision variable
Xi,k indicates that photo pi is assigned to event ek.
The binary decision variable Zi,j,k,l indicates that
photos pi and pj are assigned to events ek and el,
respectively:

Zi,j,k,l := Xi,k ∧Xj,l (2)
3.1 Event Assignment Scores
Event assignment scores quantify the textual and
visual affinity between a photo pi and an event
ek. Affinities are measures of representation sim-
ilarity between photos and events. These scores
push photos displaying a certain event to be as-
signed to that event. For now we assume the tex-
tual affinity matrix Ac ∈ [0, 1]M×N and the vi-
sual affinity matrix Av ∈ [0, 1]M×N are given.
We describe how we obtain these affinity matri-
ces in Section 4.1. Event assignment scores are
defined as the weighted sum of both textual and
visual affinity:

φevent =
M∑
i=1

N∑
k=1

(
γceAci,k + γveA

v
i,k

)
Xi,k (3)

where Xi,k is a photo-event assignment decision
variable, and γce and γve are hyperparameters that
balance the contribution of both affinities.

3.2 Segmentation Scores
Segmentation scores quantify textual and visual
similarities between adjacent photos. These scores
encourage similar adjacent photos to be assigned
to the same event. We define a similarity score be-
tween adjacent photos equal to the weighted sum
of their textual (bc) and visual (bv) similarities:

φseg =
M−1∑
i=1

N∑
k=1

(
γcsbci + γvsb

v
i

)
Zi,i+1,k,k (4)

where bc, bv ∈ [0, 1](M−1)×1 are vectors of tex-
tual and visual similarity scores between adjacent
photos whose ith element corresponds to the sim-
ilarity score between photos pi and pi+1, Z is a
decision variable defined by Equation 2, and γcs
and γvs are hyperparameters balancing the contri-
bution of both types of similarity. The similarity
scores in the b vectors are computed using cosine
similarity of the feature representations of adja-
cent images in both the textual and visual modes.

3.3 Temporal Knowledge Scores
Temporal knowledge scores quantify the compati-
bilities across different event assignments in terms
of their relative ordering. For now, we assume two
types of temporal knowledge matrices are given:
L ∈ [0, 1]N×N which stores local transition prob-
abilities for every pair of events, ek and el, and
G ∈ [0, 1]N×N which stores global pairwise or-
dering probabilities for every pair of events, ek

1771



and el. We describe how we obtain these temporal
knowledge matrices in Section 4.2. The temporal
knowledge score, defined below, encourages the
inference model to assign events that are compati-
ble with the learned temporal knowledge:

φtemporal = γlp
M∑
i=0

N∑
k,l=1

Lk,lZi,i+1,k,l (5)

+ γgp
M∑
i=1

M∑
j=i

N∑
k,l=1

Gk,lZi,j,k,l

where Z is a decision variable defined by Equa-
tion 2, and γlp and γgp are hyperparameters that
balance the contribution of local and global tem-
poral knowledge in the objective.

3.4 Constraints
We include hard constraints that force each photo
to be assigned to exactly one event:

N∑
k=1

Xi,k = 1 (6)

The number of these constraints is linear in the
number of photos in an album. We also include
hard constraints to ensure consistencies among bi-
nary decision variables X and Z:

1
2
(Xi,k + Xj,l)− Zi,j,k,l ≥ 0 (7)

which states that Zi,j,k,l can be 1 only if both Xi,k
and Xj,l are 1. The number of constraints for seg-
mentation scores and local transition probabilities
is O(MN2) because they model interactions be-
tween adjacent photos for all event pairs. The
number of these constraints for global pairwise
ordering probabilities is O(M2N2) because they
model interactions between all pairs of photos in
an album for all event pairs.

4 Learned Event Knowledge

We learn base events for each scenario by cluster-
ing photos from training albums related to that sce-
nario (Figure 3). As described in Section 3, these
base events and their temporal knowledge are in-
corporated in a joint model for event induction in
unseen albums.

4.1 Learned Event Representation
We perform k-means clustering over captions to
create a base event model. We perform text-only
clustering at first since visual cues are significantly

-Arc	de	Triomphe	
-At	the	Arc.	
	

Louvre	Arc	de	Triomphe	

-Louvre	Square.	
-Pyramid	of	the	
Louvre.	
	

Invalides	

-The	Hotel	des		
Invalides.	
-Cannons	at	les	
Invalides.	
	

Notre	Dame	

-Notre	Dame	
Cathedral.	
-Front	of	Notre	
Dame	.	
	

Eiffel	

-I	spy	the	Eiffel	
Tower.	
-Under	the	Eiffel.	

Local transition probabilities Global pairwise ordering probabilities 
PG(e5					e1)	=	.47			eiffel					arc	de	triomphe	
PG(e2					e4)	=	.48			louvre						notre	dame	
PG(e3					e2)	=	.68			invalides					louvre	
	

PL(e1					e5)	=	.24			arc	de	triomphe					eiffel	
PL(e4					e3)	=	.01			notre	dame					invalides	
PL(e2					e5)	=	.12			louvre					eiffel	
	

⇒

⇒

⇒

⇒

⇒

⇒

→

→

→

→

→

→

Event	Visual	Center,	eV	

Event	Textual	Center,	eC	

Figure 3: Photos are clustered by their captions. We can
compute the visual, evk, and caption, e

c
k, centers for all the

clusters, as well as the local transition, PL, and global pair-
wise ordering, PG, probabilities between these events based
on the sequential patterns they exhibit in the training set.

noisier. Because not all photos have informative
captions, it is expected that this base clustering
will form meaningful clusters only over a subset
of the data. For each scenario, the largest clus-
ter corresponds to the “miscellaneous” cluster as
the captions in it tend to be relatively uninforma-
tive about specific events. This cluster is excluded
when computing temporal knowledge probabili-
ties (Section 4.2).

The visual and textual representations of an
event are computed using the average of the vi-
sual and textual features, respectively, of photos
assigned to that event. We compute each textual
affinity Aci,k in the event assignment scores (Equa-
tion 3) as the cosine similarity between the textual
features of the caption for photo pi and the textual
representation of event ek. For textual features,
we extract noun and verb unigrams using Turbo-
Tagger (Martins et al., 2013) and weigh them by
their discriminativeness relative to their scenario,
P (S|w). Given scenario S and word w, P (S|w)
is defined as the number of albums for the scenario
the word occurs in divided by the total number of
albums in that scenario. The visual affinity Avi,k is
the similarity between the visual features of photo
pi and the visual representation of event ek. For
visual features, we use the convolutional features
from the final layer activations of the 16-layer VG-
GNet model (Simonyan and Zisserman, 2015).

4.2 Temporal Knowledge

Local transition probabilities. These probabili-
ties, denoted as PL, encode an expected sequence
of events using temporal patterns among adjacent

1772



Wedding Camping Funeral

aisle Walking down the aisle tent Inside our tent service Graveside serviceBride walking down the aisle Setting up the tent The service

vow
Exchanging vows

fire
Building the Fire

pay respect
Paying Respects

Reading the vows Getting the Fire going Respect
Reciting vows to each other Around the Fire

dance
First Dance

sunset
Watching the Sunset

goodbye
Saying Goodbye

Everybody Dancing Sunset from camp
Dancing the Night Away Sunset on the first night

Table 2: Sample learned events and prototypical captions

photos. We model PL for each pair of events as a
multinomial distribution,

PL(ek → el) = C(ek → el)∑N
m=1C(ek → em)

(8)

where C is the observed counts of that specific
event transition. This is the likelihood that an
event ek is immediately followed by event el.
Global pairwise ordering probabilities. These
probabilities, denoted as PG, encode global struc-
tural patterns about events. We model PG for each
pair of events as a binomial distribution by com-
puting the likelihood that an event occurs before
another at any point in an album,

PG(ek ⇒ el) = C(ek ⇒ el)
C(ek ⇒ el) + C(el ⇒ ek) (9)

where C(ek ⇒ el) is the observed counts of ek
occurring anytime before el in all photo albums.
These global probabilities model relations among
events assigned to all photos in the album, not
just events assigned to photos that are adjacent to
one another. This distinction is important because
these probabilities can encode global patterns be-
tween events and are not limited to modeling a se-
quential event chain.

We use these learned temporal probabilities, PL
and PG, in matrices L and G from φtemporal
(Equation 5). These matrices are used to index lo-
cal transition probabilities and global pairwise or-
dering probabilities for pairs of events when com-
puting temporal knowledge scores in the inference
model (Section 3.3).

4.3 Prototypical Captions
After clustering the photos, the representative lan-
guage of the captions in each cluster begins to
tell a story about each scenario. The event names
are automatically extracted using the most com-
mon content words among captions in the clus-
ter. For each cluster, we also compile prototypical
captions by extracting captions whose lemmatized

forms are frequently observed throughout multiple
albums in the scenario. Sample events and their
prototypical captions from three scenarios are dis-
played in Table 2.

5 Experimental Setup

Data split. For scenarios with more than 1000 al-
bums, we use 100 albums for each of the develop-
ment and test sets and use the rest for training. For
scenarios with less than 1000 albums, we use 50
albums for each of the development and test sets,
and the rest for training.
Implementation details. We optimize our ob-
jective function using integer linear programming
(Roth and Yih, 2004) with the Gurobi solver (Inc.,
2015). For computational efficiency, temporally
close sets of consecutive photos are treated as one
unit during the optimization. We use these units to
reduce the number of variables and constraints in
the model from a function of the number of photos
to a function of the number of units. We form these
units heuristically by merging images agglomera-
tively when their timestamps are within a certain
range of the closest image in a unit. When merg-
ing photos, the textual affinity of each unit for a
particular event is the maximum affinity for that
event among photos in that unit. The visual affin-
ity of each unit is the average of all affinities for
that event among photos in that unit. The textual
and visual similarities of consecutive units are de-
fined in terms of the similarities between the two
photos at the units’ boundary. Temporal informa-
tion for events not aligned well with a particu-
lar unit should not influence the objective, so we
include temporal scores only for unit-event pairs
which have both textual and visual event assign-
ment scores greater than 0.05.
Hyperparameters. We tune the hyperparameters
using grid search on the development set. In mod-
els where the corresponding objective components
are included, we set γce = 1, γve = 1, γcs = .5,
γvs = .15, γlp = 1, and γgp = 4Q (where Q is the

1773



Model Wedding Baby Birth Marathon Cooking Funeral Barbecue Indep. Day Camping Thanksgiving Paris Trip NY Trip Christmas
k-MEANS 52.7 52.5 53.8 53.0 50.5 50.2 53.2 52.3 51.4 53.1 50.3 51.4
NO TEMPORAL 58.6 66.3 62.6 56.5 50.8 51.7 58.0 52.6 54.3 51.7 49.1 50.9
FULL MODEL 60.0 66.5 64.5 63.2 53.1 58.6 56.0 55.5 56.1 52.3 48.5 52.4

Table 3: Temporal ordering pairwise photo results. The metric reported is accuracy, the percentage of time the correct photo
is picked as coming first based on the event assigned to it. Scenarios with an expected temporal structure are in the left half of
the table.

number of event units). For k-means clustering,
we use 10 random restarts and 40 cluster centers
for the WEDDING, CAMPING, PARIS TRIP, and NY TRIP
scenarios. For all other scenarios, we use 30 clus-
ter centers.

6 Experimental Results

We evaluate the performance of our model on
three tasks. The first task evaluates the effect
of learned temporal knowledge in predicting the
correct order of photos in an unseen album (Sec-
tion 6.1). The second task evaluates the model’s
ability to segment albums into logical groupings
(Section 6.2). The third task evaluates the qual-
ity of prototypical captions and their use in photo
album summarization (Section 6.3).

6.1 Temporal Ordering of Photos
We evaluate the model’s ability to capture the tem-
poral relationships between events in the scenario.
Given two randomly selected photos pi and pj
from an album, the task is to predict which of
the photos appears earlier in the album using their
event assignments. We compare the full model
that assigns events to photos using ILP (Section 3)
with two baselines: k-MEANS , which assigns events
to photos using k-means clustering over captions
(Section 4), and NO TEMPORAL: a variant of the
full model that does not use temporal knowledge
scores (φtemporal in Equation 1) for optimization.

We run each method over a test photo album,
in which the events ek and el are assigned to
the photos pi and pj , respectively. We then use
the learned global pairwise ordering probabilities
(Section 4.2) to predict which photo appears ear-
lier in the album. We report the accuracy of each
method in predicting the order of photos compared
to the actual order of photos in the albums. We
perform this experiment 50 times for each album
and average the number of correct choices across
every album and every trial.
Results. Table 3 reports the results of the full
model compared to the baselines. The results
show that temporal knowledge generally helps
in predicting photo ordering. We observe that

the full model achieves higher scores for scenar-
ios for which we expect would have a sequential
structure (e.g., WEDDING, BABY BIRTH, MARATHON).
Conversely, the full model achieves lower over-
all scores in non-sequential scenarios (e.g., PARIS
TRIP, NEW YORK TRIP). Qualitatively, we notice in-
teresting temporal patterns such as the fact that
during a marathon, the starting line occurs before
the medal awards with 92.3% probability, or that
Parisian tourists have a 24% chance (∼10× higher
than random chance) of visiting the Eiffel Tower
immediately after the Arc de Triomphe (a high
local transition probability that correctly implies
their real world proximity).

6.2 Album Segmentation
Our model partitions photos in albums into coher-
ent events. The album segmentation evaluation
tests if the model recovers the same sequences of
photos that a human would identify in a photo al-
bum as events.
Evaluation. We had an impartial annotator label
where they thought events began and ended in 10
candidate albums of greater than 100 photos for
three scenarios: WEDDING, FUNERAL, CAMPING. We
evaluate how well our model can replicate these
boundaries with two metrics. The first metric is
the F1 score of recovering the same boundaries
annotated by humans. The second metric is d,
the difference between the number of events seg-
mented by the model compared to the annotated
albums. We report results for exact event bound-
aries as well as relaxed boundaries where the start
of an event can be r photos away from the start
of an annotated event, where r is the relaxation
coefficient. For reference, we note that albums in
the wedding scenario were dual annotated and the
agreement between annotators is 56.9% for r = 0
and 77.5% for r = 2.
Results. Table 4 shows comparison of the the full
ILP model with same baselines we described be-
fore, k-MEANS and NO TEMPORAL. The table shows
that the full model generally outperforms the k-
MEANS baseline for all three scenarios.

In the WEDDING scenario, the F1 score for the full

1774



Model r WEDDING FUNERAL CAMPING
F1 d F1 d F1 d

k-MEANS
0

27.1 32.9 27.9 29.6 31.2 46.0
NO TEMPORAL 32.0 -5.6 35.9 .9 22.0 -15.6
FULL MODEL 37.8 1.3 32.2 4.2 27.5 -10.1
k-MEANS

2
40.8 32.9 38.3 29.6 46.2 46.0

NO TEMPORAL 49.6 -5.6 57.6 .9 35.4 -15.8
FULL MODEL 57.5 1.3 51.6 5.0 51.4 -10.1

Table 4: Segmentation results for our full model. F1 scores
how often our model recovers the same boundaries annotated
by humans. d is the average difference between the number
of events identified by the model in an album and marked by
annotators. r is the relaxation coefficient.

Feature Group Excluded P R F1 d
FULL MODEL 36.7 42.8 37.8 1.3
- Visual Event Affinity 37.7 37.0 35.3 -1.7
- Textual Segmentation 37.1 41.5 37.4 .8
- Visual Segmentation 35.1 42.1 36.5 1.7
- Local Ordering Probs. 36.9 40.3 36.9 .2
- Global Ordering Probs. 40.5 25.0 29.5 -5.8

Table 5: Ablation study of objective function components
for the wedding scenario. P, R, and F1 are the precision,
recall and F-measure of recovering the same boundaries an-
notated by humans. d is the average difference between the
number of events identified by our models and the annotators.

model is consistently higher. The k-MEANS base-
line oversamples the number of events in albums,
which is indicated by an average d significantly
greater than 0. For the FUNERAL scenario, the
NO TEMPORAL baseline outperforms the full model.
We attribute this difference to the smaller data sub-
set (see Table 1) making it harder to learn the
temporal relations in the scenario, which makes
the contributions of the local and global temporal
probabilities unexpected. In the CAMPING scenario,
the F1 score for the k-MEANS baseline is higher
than that of the full model when r = 0. At a high-
level, CAMPING is a scenario we expect has less of a
known structure compared to other scenarios and
may be harder to segment into its events.

Ablation Study. Table 5 depicts the performance
of ablations of the full model for the wedding sce-
nario. Results show that removing any component
of the objective functions yields lower recall and
F1 scores than the full model for r = 0. The ex-
ception is removing local ordering probabilities,
which yields a higher d. These observations sup-
port the hypothesis that all of the components of
the objective function contribute to segmenting the
album into subsequences of photos depicting the
same event. Particularly, we note the degradation
when removing the global ordering probabilities,
indicating that approaches which model only local
event transitions such as hidden Markov models
would not be suitable for this task.

6.3 Photo Album Summarization

The final experiment evaluates how our learned
prototypical captions can improve downstream
tasks such as summarization and captioning.

6.3.1 Summaries
The goal of a good summary is to select the most
salient pictures of an album. In our setting, a
good summary should have a high coverage of the
events in an album and choose the photos that most
appropriately depict these events. Given a photo
budget b, we choose a subset of photos that aims
for these goals. To summarize a test album, we
run our model over the entire album. This will
yield h unique events assigned to the photos in the
album. For each of these h events, we choose the
photo with the highest event assignment score for
that event (Equation 3) to be in the summary. If
h > b, we count the number of photos in the train-
ing set assigned to each of the h events and choose
the photos corresponding to the b events with the
largest membership of photos in the training set. If
h < b, we complete the summary with b− h pho-
tos from the “miscellaneous” event that are spaced
evenly throughout the album. Finally, we replace
the caption of each selected photo with a prototyp-
ical caption (Section 4.3) for the assigned event.

Baseline. We evaluate against two baselines. The
first baseline, KTH, involves including a photo in
the summary every k = M/b photos. The sec-
ond baseline, k-MEANS, uses the events assigned to
photos from k-means clustering and then picks b
photos in the same manner as our main model.

Evaluation. We evaluate the summaries pro-
duced by each method with a human evaluation
using Amazon Mechanical Turk (AMT). We use
albums from the test set that contain more than 40
photos for the wedding scenario. For each album,
at random, we present two summaries generated
by two algorithms. AMT workers are instructed
to choose the better summary considering both the
images and the captions. For each comparison of
two summaries for an album, we aggregate an-
swers from three workers by majority voting. We
set b = 7. The number of assigned events in an
album, h, varies by album.

Results. As seen in Table 6, the summary from
the full model is preferred 57.7% of the time com-
pared to the KTH baseline. The summaries gener-
ated using the full model perform slightly better
than the summaries from k-MEANS. We attribute

1775



Figure 4: Example summaries from the wedding, Paris trip, and baby birth scenarios. In cases where the album had less events
than b, the additionally chosen photos are outlined in red. These photos do not have their caption replaced by a prototypical
captions and merely fill out the summary.

Method Selection Rates
FULL MODEL vs. KTH 57.7 42.3
FULL MODEL vs. k-MEANS 53.8 47.2
k-MEANS vs. KTH 53.8 47.2

Table 6: Summarization results. The selection rates indi-
cate the percentage of time the corresponding method in the
left-most column was picked.

the superior performance of the full model to the
fact that it redistributes photos with noisy captions
throughout the events, allowing for a larger sample
to estimate visual representations of events, yield-
ing more accurate visual affinity measurements to
choose the summarization photos. As can be seen
from qualitative examples in Figure 4, the photos
chosen and the captions assigned cover key events
that would occur during the scenario and describe
them in a coherent way. Additional examples are
available at https://www.cs.washington.edu/
projects/nlp/protoevents.

6.3.2 Prototypical Captions
We also evaluate the quality of the prototypical
captions assigned to every photo in the summaries.
For each album, we use the same sets of b pho-
tos from the full model in the summarization task
and evaluate the quality of the prototypical cap-
tions paired with that group of photos.

Evaluation. We evaluate the quality of captions
assigned to every photo by asking AMT work-
ers to rate the captions on three different metrics:
grammaticality, relevance to the scenario to which
the image belongs, and relevance to its paired im-

Method Scenario Image GrammarRelevance Relevance
LSTM 4.90 2.85 3.74
FULL MODEL 4.55 3.66 4.08
RAW CAPTIONS 4.10 4.36 4.28

Table 7: Captioning results. We evaluate the caption qual-
ity of the prototypical captions of the full model, those gen-
erated by an LSTM trained on the raw captions, and original
captions. Captions were evaluated on 3 metrics: grammatical
correctness, how relevant they were to the scenario, and how
relevant they were to their assigned image.

age. Five AMT workers rate each group of b pho-
tos on a five point Likert scale for each metric.
We compare the prototypical captions for every
photo in the summary with captions generated by
an LSTM model3 trained on every photo-caption
pair in the training set for a scenario. We also com-
pare with the original raw captions for each image
in the summary. Because we chose photos with
the highest event assignment scores (Equation 3)
to be in the summary, the raw captions for this
evaluation are cleaner and more descriptive than
most captions in the dataset.
Results. Our model outperforms the LSTM-
generated captions in the image relevance and
grammaticality scores, but did worse in scenario

3We use a single-layer encoder-decoder LSTM. The cell
state and the input embedding dimensions are 256. Visual in-
puts are the final layer convolutional features of the VGG-16
model and are fine-tuned during training. We use RMSprop
to train the network with a base learning rate of .0001 and
30% dropout. We train the model for 45 epochs on a single
NVIDIA Titan X GPU with mini batch size 100. To decode,
we use beam search with beam size 5.

1776



relevance. We attribute this result to LSTM
captions having little caption variation because
the model learns frequency statistics without any
knowledge of latent events. Almost all LSTM cap-
tions mention the words bride, wedding, or groom,
yielding a very high scenario score for the caption,
even if that caption is grammatically incorrect or
irrelevant to the image. As expected the raw cap-
tions have high relevance to the original image,
and they are grammatical, but can be less relevant
to the corresponding scenario.

7 Related Work

Previous studies have explored unsupervised in-
duction of salient content structure in newswire
texts (Barzilay and Lee, 2004), temporal graph
representations (Bramsen et al., 2006), and story-
line extraction and event summarization (Xu et al.,
2013). Another line of research finds the common
event structure from children’s stories (McIntyre
and Lapata, 2009), where the learned plot struc-
ture is used to stochastically generate new stories
(Goyal et al., 2010; Goyal et al., 2013). Our work
similarly aims to learn the typical temporal pat-
terns and compositional elements that define com-
mon scenarios, but with multimodal integration.

Compared to studies that learn narrative
schemas from natural language (Pichotta and
Mooney, 2014; Jans et al., 2012; Chambers and
Jurafsky, 2009; Chambers, 2013; Cassidy et al.,
2014), or compile script knowledge from crowd-
sourcing (Regneri et al., 2010), our work explores
a new source of knowledge that allows grounded
event learning with temporal dimensions, result-
ing in a new dataset of scenario types that are not
naturally accessible from newswire or literature.

While recent studies have explored videos and
photo streams as a source of discovering com-
plex events and learning their sequential patterns
(Kim and Xing, 2014; Kim and Xing, 2013; Tang
et al., 2012; Tschiatschek et al., 2014), their fo-
cus was mostly on the visual modality. Zhang
et al. (2015) explored multimodal information ex-
traction focusing specifically on identifying video
clips that referred to the same event in television
news. This contrasts to the goal of our study that
aims to learn the temporal structure by which com-
mon scenarios unfold.

Integrating language and vision has attracted in-
creasing attention in recent years across diverse
tasks such as image captioning (Karpathy and Fei-

Fei, 2015; Vinyals et al., 2015; Fang et al., 2015;
Xu et al., 2015; Chen et al., 2015), cross modal
semantic modeling (Lazaridou et al., 2015), infor-
mation extraction (Morency et al., 2011; Rosas
et al., 2013; Zhang et al., 2015; Izadinia et al.,
2015), common-sense knowledge (Vedantam et
al., 2015; Bagherinezhad et al., 2016), and visual
storytelling (Huang et al., 2016). Our work is sim-
ilar to both common sense knowledge learning and
visual story completion. Our model learns com-
monsense knowledge on the hierarchical and tem-
poral event structure from scenario-specific multi-
modal photo albums, which can be viewed as vi-
sual stories about common life events.

Recent work focused on photo album summa-
rization using visual (Sadeghi et al., 2015) and
multimodal representations (Sinha et al., 2011).
Our work identifies the nature of common events
in scenarios and learns their timelines and charac-
teristic forms.

8 Conclusion

We introduce a novel exploration to learn script-
like knowledge from photo albums. We model
stochastic event structure to learn both the event
representations (textual and visual) and the tempo-
ral relations among those events. Our event induc-
tion method incorporates learned knowledge about
events, partitions photo albums into segments, and
assigns events to those segments. We show the
significance of our model in learning and using
learned knowledge for photo ordering, album seg-
mentation, and summarization. Finally, we pro-
vide a dataset depicting 12 scenarios with ∼1.5 M
images for future research. Future directions could
include exploring nuances in the type of tempo-
ral knowledge that can be learned across different
scenarios.

Acknowledgements

We thank the anonymous reviewers for many in-
sightful comments and members of UW NLP for
feedback and support. The work is in part sup-
ported by NSF grants IIS-1408287, IIS-1524371,
IIS-1616112, DARPA under the CwC program
through the ARO (W911NF-15-1-0543), the Allen
Institute for AI (66-9175), the Allen Distinguished
Investigator Award, and gifts by Google and Face-
book.

1777



References
Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin

Choi, and Ali Farhadi. 2016. Are elephants bigger
than butterflies? reasoning about sizes of objects. In
Proceedings of the Conference in Artificial Intelli-
gence (AAAI).

Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In NAACL-HLT.

Philip Bramsen, Pawan Deshpande, Yoong Keok Lee,
and Regina Barzilay. 2006. Inducing temporal
graphs. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 189–198. Association for Computational
Linguistics.

Taylor Cassidy, Bill McDowell, Nathanael Chambers,
and Steven Bethard. 2014. An annotation frame-
work for dense event ordering. In ACL.

Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In ACL.

Nathanael Chambers and Dan Jurafsky. 2009. Un-
supervised learning of narrative schemas and their
participants. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2, pages 602–610. Association for Computational
Linguistics.

Nathanael Chambers. 2013. Event schema induction
with a probabilistic entity-driven model. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing.

Jianfu Chen, Polina Kuznetsova, David S Warren, and
Yejin Choi. 2015. Déja image-captions: A corpus
of expressive descriptions in repetition. In NAACL-
HLT.

Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K.
Srivastava, Li Deng, Piotr Dollar, Jianfeng Gao,
Xiaodong He, Margaret Mitchell, John C. Platt,
C. Lawrence Zitnick, and Geoffrey Zweig. 2015.
From captions to visual concepts and back. In The
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June.

Amit Goyal, Ellen Riloff, and Hal Daumé III. 2010.
Automatically producing plot unit representations
for narrative text. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 77–86. Association for Computa-
tional Linguistics.

Amit Goyal, Ellen Riloff, et al. 2013. A computational
model for plot units. Computational Intelligence,
29(3):466–488.

Ting-Hao Huang, Francis Ferraro, Nasrin
Mostafazadeh, Ishan Misra, Aishwarya Agrawal,

Jacob Devlin, Ross Girshick, Xiaodong He, Push-
meet Kohli, Dhruv Batra, C. Lawrence Zitnick,
Devi Parikh, Lucy Vanderwende, Michel Galley,
and Margaret Mitchell. 2016. Visual storytelling.
In NAACL.

Gurobi Optimization Inc. 2015. Gurobi optimizer ref-
erence manual.

Hamid Izadinia, Fereshteh Sadeghi, Santosh K Div-
vala, Hannaneh Hajishirzi, Yejin Choi, and Ali
Farhadi. 2015. Segment-phrase table for seman-
tic segmentation, visual entailment and paraphras-
ing. In Proceedings of the IEEE International Con-
ference on Computer Vision, pages 10–18.

Bram Jans, Steven Bethard, Ivan Vuli, and Marie-
Francine Moens. 2012. Skip n-grams and ranking
functions for predicting script events. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 336–344.

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June.

Gunhee Kim and Eric P Xing. 2013. Jointly align-
ing and segmenting multiple web photo streams for
the inference of collective photo storylines. In Com-
puter Vision and Pattern Recognition (CVPR), 2013
IEEE Conference on, pages 620–627. IEEE.

Gunhee Kim and Eric Xing. 2014. Reconstructing sto-
ryline graphs for image recommendation from web
community photos. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 3882–3889.

Angeliki Lazaridou, Nghia The Pham, and Marco Ba-
roni. 2015. Combining language and vision with a
multimodal skip-gram model. In Proceedings of the
2015 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 153–163, Den-
ver, Colorado, May–June. Association for Compu-
tational Linguistics.

André FT Martins, Miguel B Almeida, and Noah A
Smith. 2013. Turning on the turbo: Fast third-order
non-projective turbo parsers. In ACL.

Neil McIntyre and Mirella Lapata. 2009. Learning to
tell tales: A data-driven approach to story genera-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 1-Volume 1, pages
217–225. Association for Computational Linguis-
tics.

Louis-Philippe Morency, Rada Mihalcea, and Payal
Doshi. 2011. Towards multimodal sentiment analy-
sis: Harvesting opinions from the web. In Proceed-
ings of the 13th international conference on multi-
modal interfaces, pages 169–176. ACM.

1778



Karl Pichotta and Raymond J Mooney. 2014. Statis-
tical script learning with multi-argument events. In
EACL, volume 14, pages 220–229.

Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with web
experiments. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 979–988. Association for Computa-
tional Linguistics.

Verónica Pérez Rosas, Rada Mihalcea, and Louis-
Philippe Morency. 2013. Multimodal sentiment
analysis of spanish online videos. IEEE Intelligent
Systems, (3):38–45.

Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. Technical report, DTIC Document.

Fereshteh Sadeghi, J Rafael Tena, Ali Farhadi, and
Leonid Sigal. 2015. Learning to select and order
vacation photographs. In Applications of Computer
Vision (WACV), 2015 IEEE Winter Conference on,
pages 510–517. IEEE.

Roger C Schank and Robert P Abelson. 1975. Scripts,
plans, and knowledge. Yale University.

K. Simonyan and A. Zisserman. 2015. Very deep con-
volutional networks for large-scale image recogni-
tion. In Proceedings of the International Conference
on Learning Representations (ICLR).

Pinaki Sinha, Sharad Mehrotra, and Ramesh Jain.
2011. Summarization of personal photologs using
multidimensional content and context. In Proceed-
ings of the 1st ACM International Conference on
Multimedia Retrieval, page 4. ACM.

Kevin Tang, Li Fei-Fei, and Daphne Koller. 2012.
Learning latent temporal structure for complex event
detection. In Computer Vision and Pattern Recog-
nition (CVPR), 2012 IEEE Conference on, pages
1250–1257. IEEE.

Sebastian Tschiatschek, Rishabh K Iyer, Haochen Wei,
and Jeff A Bilmes. 2014. Learning mixtures of sub-
modular functions for image collection summariza-
tion. In Advances in Neural Information Processing
Systems, pages 1413–1421.

R. Vedantam, X. Lin, T. Batra, C. L. Zitnick, and
D. Parikh. 2015. Learning common sense through
visual abstraction. In Proceedings of the Interna-
tional Conference in Computer Vision (ICCV).

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR),
June.

Shize Xu, Shanshan Wang, and Yan Zhang. 2013.
Summarizing complex events: a cross-modal so-
lution of storylines extraction and reconstruction.

In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1281–1291.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun
Cho, Aaron C. Courville, Ruslan Salakhutdinov,
Richard S. Zemel, and Yoshua Bengio. 2015.
Show, attend and tell: Neural image caption gen-
eration with visual attention. In Proceedings of The
32nd International Conference on Machine Learn-
ing, pages 2048–2057.

Tongtao Zhang, Hongzhi Li, Heng Ji, and Shih-Fu
Chang. 2015. Cross-document event coreference
resolution based on cross-media features. In Proc.
Conference on Empirical Methods in Natural Lan-
guage Processing.

1779


