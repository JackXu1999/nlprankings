



















































The AMU-UEdin Submission to the WMT 2017 Shared Task on Automatic Post-Editing


Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 639–646
Copenhagen, Denmark, September 711, 2017. c©2017 Association for Computational Linguistics

The AMU-UEdin Submission to the WMT 2017 Shared Task
on Automatic Post-Editing

Marcin Junczys-Dowmunt
Information Systems Laboratory

Adam Mickiewicz University in Poznań
junczys@amu.edu.pl

Roman Grundkiewicz
School of Informatics

University of Edinburgh
rgrundki@exseed.ed.ac.uk

Abstract

This work describes the AMU-UEdin sub-
mission to the WMT 2017 shared task on
Automatic Post-Editing. We explore mul-
tiple neural architectures adapted for the
task of automatic post-editing of machine
translation output. We focus on neural
end-to-end models that combine both in-
putsmt and src in a single neural architec-
ture, modeling {mt, src} → pe directly.
Apart from that, we investigate the influ-
ence of hard-attention models which seem
to be well-suited for monolingual tasks, as
well as combinations of both ideas.

1 Introduction

During the WMT 2016 APE two systems relied
on neural models, the CUNI system (Libovický
et al., 2016) and the shared task winner, the sys-
tem submitted by the Adam Mickiewicz Univer-
sity (AMU) team (Junczys-Dowmunt and Grund-
kiewicz, 2016). This submission explored the ap-
plication of neural translation models to the APE
problem and achieved good results by treating
different models as components in a log-linear
model, allowing for multiple inputs (the source
src and the translated sentence mt) that were de-
coded to the same target language (post-edited
translation pe). Two systems were considered, one
using src as the input (src→ pe) and another us-
ing mt as the input (mt → pe). A simple string-
matching penalty integrated within the log-linear
model was used to control for higher faithfulness
with regard to the raw MT output. The penalty
fires if the APE system proposes a word in its out-
put that has not been seen in mt. The influence of
the components on the final result was tuned with
Minimum Error Rate Training (Och, 2003) with
regard to the task metric TER.

With neural encoder-decoder models, and
multi-source models in particular, the combination
of mt and src can be now achieved in more natu-
ral ways than for previously popular phrase-based
statistical machine translation (PB-SMT) systems.
Despite this, results for multi-source or double-
source models in APE scenarios are incomplete or
unsatisfying in terms of performance.

In this work, we explore a number of single-
source and double-source neural architectures
which we believe to be better fits to the APE task
than vanilla encoder-decoder models with soft at-
tention. We focus on neural end-to-end models
that combine both inputs mt and src in a single
neural architecture, modeling {mt, src} → pe di-
rectly. Apart from that, we investigate the influ-
ence of hard-attention models which seem to be
well-suited for monolingual tasks. Finally, we cre-
ate combinations of both architectures.

Following (Junczys-Dowmunt and Grund-
kiewicz, 2016), we also attempt to generate more
artificial data for the task. Instead of relying on
filtering towards specific error rates, we generate
text with fitting error rates from the start which
allows us to retain more data.

2 Encoder-Decoder Models with
APE-specific Attention Models

2.1 Standard Attentional Encoder-Decoder
The attentional encoder-decoder model in Marian1

is a re-implementation of the NMT model in Ne-
matus (Sennrich et al., 2017). The model differs
from the standard model introduced by Bahdanau
et al. (2014) by several aspects, the most important
being the conditional GRU with attention. The
summary provided in this section is based on the
description in Sennrich et al. (2017). More de-
tails on the specific architectures in this shared

1https://github.com/marian-nmt/marian

639



task submission are given in Junczys-Dowmunt
and Grundkiewicz (2017).

Given the raw MT output sequence
(x1, . . . , xTx) of length Tx and its manually
post-edited equivalent (y1, . . . , yTy) of length Ty,
we construct the encoder-decoder model using the
following formulations.

Encoder context A single forward encoder state−→
h i is calculated as:

−→
h i = GRU(

−→
h i−1,F[xi])

where F is the encoder embeddings matrix. The
GRU RNN cell (Cho et al., 2014) is defined as:

GRU (s,x) =(1− z)� s+ z� s, (1)
s = tanh (Wx+ r�Us) ,
r = σ (Wrx+Urs) ,

z = σ (Wzx+Uzs) ,

where x is the cell input, s is the previous recurrent
state, W, U, Wr, Ur, Wz , Uz are trained model
parameters2; σ is the logistic sigmoid activation
function. The backward encoder state is calculated
analogously over a reversed input sequence with
its own set of trained parameters.

Let hi be the annotation of the source symbol
at position i, obtained by concatenating the for-
ward and backward encoder RNN hidden states,
hi = [

−→
h i;
←−
h i], the set of encoder states C =

{h1, . . . ,hTx} then forms the encoder context.
Decoder initialization The decoder is initial-
ized with start state s0, computed as the average
over all encoder states:

s0 = tanh

(
Winit

∑Tx
i=1 hi
Tx

)

Conditional GRU with attention We follow
the Nematus implementation of the conditional
GRU with attention, cGRUatt:

sj = cGRUatt (sj−1,E[yj−1],C) (2)

where sj is the newly computed hidden state, sj−1
is the previous hidden state, C the source context
and E[yj−1] is the embedding of the previously
decoded symbol yi−1.

The conditional GRU cell with attention,
cGRUatt, has a complex internal structure, consist-
ing of three parts: two GRU layers and an inter-
mediate attention mechanism ATT.

2Biases have been omitted.

Layer GRU1 generates an intermediate repre-
sentation s′j from the previous hidden state sj−1
and the embedding of the previous decoded sym-
bol E[yj−1]:

s′j = GRU1 (sj−1,E[yj−1]) .

The attention mechanism, ATT, inputs the en-
tire context set C along with intermediate hidden
state s′j in order to compute the context vector cj
as follows:

cj =ATT
(
C, s′j

)
=

Tx∑

i

αijhi,

αij =
exp(eij)∑Tx

k=1 exp(ekj)
,

eij =v
ᵀ
a tanh

(
Uas

′
j +Wahi

)
,

where αij is the normalized alignment weight
between source symbol at position i and target
symbol at position j and va,Ua,Wa are trained
model parameters.

Layer GRU2 generates sj , the hidden state of
the cGRUatt, from the intermediate representation
s′j and context vector cj :

sj = GRU2
(
s′j , cj

)
.

Deep output Finally, given sj , yj−1, and cj , the
output probability p(yj |sj , yj−1, cj) is computed
by a softmax activation as follows:

p(yj |sj ,yj−1, cj) = softmax (tjWo)
tj = tanh (sjWt1 +E[yj−1]Wt2 + cjWt3)

Wt1 ,Wt2 ,Wt3 ,Wo are the trained model pa-
rameters.

This rather standard encoder-decoder model
with attention is our baseline and denoted as
ENCDEC-ATT.

The following models reuse most parts of the
architecture described above wherever possible,
most differences occur in the decoder RNN cell
and the attention mechanism. The encoders are
identical, so are the deep output layers.

2.2 Hard Monotonic Attention
Aharoni and Goldberg (2016) introduce a sim-
ple model for monolingual morphological re-
inflection with hard monotonic attention. This
model looks at one encoder state at a time, start-
ing with the left-most encoder state and progress-
ing to the right until all encoder states have been
processed.

640



The target word vocabulary Vy is extended with
a special step symbol (V ′y = Vy ∪ {〈STEP〉}) and
whenever 〈STEP〉 is predicted as the output sym-
bol, the hard attention is moved to the next encoder
state. Formally, the hard attention mechanism
is represented as a precomputed monotonic se-
quence (a1, . . . , aTy) which can be inferred from
the target sequence (y1, . . . , yTy) (containing orig-
inal target symbols and Tx step symbols) as fol-
lows:

a1 = 1

aj =

{
aj−1 + 1 if yj−1 = 〈STEP〉
aj−1 otherwise.

For a given context C = {h1, . . . ,hTx}, the at-
tended context vector at time step j is simply haj .

Following the description by Aharoni and Gold-
berg (2016) for their LSTM-based model, we now
adapt the previously described encoder-decoder
model to incorporate hard attention. The encoder
as well as the output layer of the previous model
remain unchanged. Given the sequence of atten-
tion indices (a1, . . . , aTy), the conditional GRU
cell (Eq. 2) used for hidden state updates of the de-
coder is replaced with a simple GRU cell (Eq. 1)
(thus removing the soft-attention mechanism):

sj = GRU
(
sj−1,

[
E[yj−1];haj

])
(3)

where the cell input is now a concatenation of the
embedding of the previous target symbol E[yj−1]
and the currently attended encoder state haj . This
model is labeled ENCDEC-HARD.

We find this architecture compelling for mono-
lingual tasks that might require higher faithfulness
with regard to the input. With hard monotonic at-
tention, the translation algorithm can enforce cer-
tain constraints:

1. The end-of-sentence symbol can only be gen-
erated if the hard attention mechanism has
reached the end of the input sequence, en-
forcing full coverage;

2. The 〈STEP〉 symbol cannot be generated once
the end-of-sentence position in the source has
been reached. It is however still possible to
generate content tokens.

Obviously, this model requires a target se-
quence with correctly inserted 〈STEP〉 symbols.
For the described APE task, using the Longest

Common Subsequence algorithm (Hirschberg,
1977), we first generate a sequence of match,
delete and insert operations which transform the
raw MT output (x1, · · ·xTx) into the corrected
post-edited sequence (y1, · · · yTy)3. Next, we map
these operations to the final sequence of steps and
target tokens according to the following rules:

• For each matched pair of tokens x, y we pro-
duce symbols: 〈STEP〉 y;

• For each inserted target token y, we produce
the same token y;

• For each deleted source token x we produce
〈STEP〉;

• Since at initialization of the model a1 = 1,
i.e. the first encoder state is already attended
to, we discard the first symbol in the new se-
quence if it is a 〈STEP〉 symbol.

2.3 Hard and Soft Attention
While the hard attention model can be used to en-
force faithfulness to the original input, we would
also like the model to be able to look at informa-
tion anywhere in the source sequence which is a
property of the soft attention model.

By re-introducing the conditional GRU cell
with soft attention into the ENCDEC-HARD model
while also inputting the hard-attended encoder
state haj , we can try to take advantage of both at-
tention mechanisms. Combining Eq. 2 and Eq. 3,
we get:

sj = cGRUatt
(
sj−1,

[
E[yj−1];haj

]
,C
)
. (4)

The rest of the model is unchanged; the transla-
tion process is the same as before and we use the
same target step/token sequence for training. This
model is called ENCDEC-HARD-ATT.

2.4 Soft Double-Attention
Neural multi-source models (Zoph and Knight,
2016) seem to be natural fit for the APE task, as
raw MT output and original source language in-
put are available. Although application to the APE
problem have been reported (Libovický and Helcl,
2017), state-of-the-art results seem to be missing.

In this section we give details about our double-
source model implementation. We rename the ex-
isting encoder C to Cmt to signal that the first en-
coder consumes the raw MT output and introduce

3Similar to GNU wdiff.

641



a structurally identical second encoder Csrc =
{hsrc1 , . . . ,hsrcTsrc} over the source language. To
compute the decoder start state s0 for the multi-
encoder model we concatenate the averaged en-
coder contexts before mapping them into the de-
coder state space:

s0 = tanh

(
Winit

[∑Tmt
i=1 h

mt
i

Tmt
;

∑Tsrc
i=1 h

src
i

Tsrc

])
.

In the decoder, we replace the conditional GRU
with attention, with a doubly-attentive cGRU cell
(Calixto et al., 2017) over contexts Cmt and Csrc:

sj = cGRU2-att
(
sj−1,E[yj−1],Cmt,Csrc

)
(5)

The procedure is similar to the original cGRU,
differing only in that in order to compute the con-
text vector cj , we first calculate contexts vectors
cmtj and c

src
j for each context and then concate-

nate the results:

s′j =GRU1 (sj−1,E[yj−1]) ,

cmtj =ATT
(
Cmt, s′j

)
=

Tmt∑

i

αijh
mt
i ,

csrcj =ATT
(
Csrc, s′j

)
=

Tsrc∑

i

αijh
src
i ,

cj =
[
cmtj ; c

src
j

]
,

sj =GRU2
(
s′j , cj

)
.

This could be easily extended to an arbitrary
number of encoders with different architectures.
During training this model is fed with a tri-parallel
corpus, during translation both input sequences
are processed simultaneously to produce the cor-
rected output. This model is denoted as ENCDEC-
DOUBLE-ATT.

2.5 Hard Attention with Soft
Double-Attention

Analogously to the procedure described in sec-
tion 2.3, we can extend the doubly-attentive cGRU
to take the hard-attended encoder context as addi-
tional input:

sj = cGRU2-att
(
sj−1,

[
E[yj−1];hmtaj

]
,Cmt,Csrc

)

In this formulation, only the first encoder con-
text Cmt is attended to by the hard monotonic at-
tention mechanism. The target training data con-
sists of the step/token sequences used for all pre-
vious hard-attention models. We call this model
ENCDEC-HARD+DOUBLE-ATT.

Data set Sentences TER

training set 2016 12,000 26.22
training set 2017 11,000 –
development set 2016 1,000 24.81
test set 2016 2,000 –

artificial-large 2016 4,335,715 36.63
artificial-small 2016 531,839 25.28

artificial 2017 15,158,354 27.45

Table 1: Statistics for artificial data sets in
comparison to official training and development
data, adapted from Junczys-Dowmunt and Grund-
kiewicz (2016).

3 Artifical Data

We also attempt to generate more artificial data for
the task. Instead of relying on filtering towards
specific error rates, we generate text with fitting
error rates from the start which allows us to re-
tain more data. To obtain the monolingual source
data we follow the steps described by (Junczys-
Dowmunt and Grundkiewicz, 2016). Next we
train a English-to-German MT system using data
from the WMT2016 shared task on IT translation.
This system is used to translate it’s own training
data into German. Although input sentence have
been seen, the translations are far from perfect.
Next we create an MT system to translate from
correct German to imperfect German MT output.
This system can now be applied to create raw Ger-
man MT output from correct German text.

In order to achieve matching TER statistics we
use a simple implementation of the Nelder-Mead
algorithm for parameter tuning. For unknown rea-
sons, MERT or kb-Mira would not create output
with the desired error-rates.

Using this system we create a new large set
of pseudo-PE data, translating domain-selected
monolingual data from German into German
pseudo-MT output. The English input is created
with an German-to-English phrase-based MT sys-
tem. We translate about 15 million sentences in
this manner, creating new artificial APE triplets.

4 Experiments and Results

4.1 Training, Development, and Test Data

We perform all our experiments with the offi-
cial WMT16 (Bojar et al., 2016) automatic post-

642



dev 2016 test 2016 test 2017
Model TER↓ BLEU↑ TER↓ BLEU↑ TER↓ BLEU↑
WMT17-baseline 1 – – – – 24.48 62.49
WMT17-baseline 2 – – – – 24.69 62.97

CONTRASTIVE 19.74 70.61 19.30 70.34 19.83 69.38
PRIMARY – – 19.21 70.51 19.77 69.50

Table 2: Submitted system results

editing data and the respective development and
test sets. The training data consists of a small
set of 23,000 post-editing triplets (src,mt, pe),
where src is the original English text, mt is
the raw MT output generated by an English-to-
German system, and pe is the human post-edited
MT output. The MT system used to produce the
raw MT output is unknown, so is the original train-
ing data. The task consist of automatically correct-
ing the MT output so that it resembles human post-
edited data. The main task metric is TER (Snover
et al., 2006) – the lower the better – with BLEU
(Papineni et al., 2002) as a secondary metric.

Table 1 summarizes the data sets used in this
work. To produce our final training data set we
oversample the original training data 20 times and
add all three artificial data sets (they may overlap).
This results in a total of slightly more than 21M
training triplets. We keep the development set as
a validation set for early stopping and report re-
sults on the WMT16 test set. The data is already
tokenized, additionally we truecase all files and
apply segmentation into BPE subword units. We
reuse the subword units distributed with the arti-
ficial data set. For the hard-attention models, we
create new target training and development files
following the procedure from section 2.2.

4.2 Training parameters

All models are trained on the same training data.
Models with single input encoders take only the
raw MT output (mt) as input, double-encoder
models use raw MT output (mt) and the original
source (pe). The training procedures and model
settings are the same whenever possible:

• All embedding vectors consist of 512 units,
the RNN states use 1024 units. We choose
a vocabulary size of 40,000 for all inputs
and outputs. When hard attention models are
trained the maximum sentence length is 100

to accommodate the additional step symbols,
otherwise 50.

• To avoid overfitting, we use pervasive
dropout (Gal, 2015) over GRU steps and in-
put embeddings, with dropout probabilities
0.2, and over source and target words with
probabilities 0.2.

• We use Adam (Kingma and Ba, 2014) as our
optimizer, with a mini-batch size of 64. All
models are trained with Asynchronous SGD
(Adam) on three to four GPUs.

• We train all models until convergence (early-
stopping with a patience of 10 based on dev-
set cross-entropy cost), saving model check-
points every 10,000 mini-batches.

• The best eight model checkpoints w.r.t. dev-
set cross-entropy of each training run are
averaged element-wise (Junczys-Dowmunt
et al., 2016) resulting in new single models
with generally improved performance.

• For the multi-source models we repeat the
mentioned procedure four times with differ-
ent randomly initialized weights and random
seeds to later form model ensembles.

Training time for one model on four NVIDIA
GTX 1080 GPUs or NVIDIA TITAN X (Pascal)
GPUs is between two and three days, depending
on model complexity.

4.3 Submitted System

We chose an ensemble of four ENCDEC-
HARD+DOUBLE-ATT systems (four distinct train-
ing runs with different random weights initializa-
tions) as our final system. In Table 2, this system
is marked as CONSTRASTIVE. We also noticed
that providing the system output once more as sys-
tem input to the same system results in a small im-

643



dev 2016 test 2016
Model TER↓ BLEU↑ TER↓ BLEU↑
WMT16-baseline 1 (Bojar et al., 2016) 25.14 62.92 24.76 62.11
WMT16-baseline 2 (Bojar et al., 2016) – – 24.64 63.47
Junczys-Dowmunt and Grundkiewicz (2016) 21.46 68.94 21.52 67.65

Pal et al. (2017) SYMMETRIC – – 21.07 67.87
Pal et al. (2017) RERANKING – – 20.70 69.90

Table 3: Results from the literature for the WMT 2016 APE development and test set

dev 2016 test 2016
Model TER↓ BLEU↑ TER↓ BLEU↑
ENCDEC-ATT 22.01 68.11 22.27 66.90

ENCDEC-HARD 22.72 66.82 22.72 65.86
ENCDEC-HARD+ATT 22.11 67.82 22.10 67.15

ENCDEC-DOUBLE-ATT 20.79 69.28 20.69 68.56
ENCDEC-DOUBLE-ATT × 4 20.10 70.24 19.92 69.40
ENCDEC-HARD+DOUBLE-ATT 20.83 69.02 20.87 68.14
ENCDEC-HARD+DOUBLE-ATT × 4 20.08 70.05 20.34 68.96

Table 4: Post-submission results, the main task metric is TER (the lower the better)

provement. This one-time looped system is our
primary submission PRIMARY.

5 Post-submission analysis

This section is based on the work in Junczys-
Dowmunt and Grundkiewicz (2017). After the
submission we performed a number of in-depth
experiments to verify our intuitions about the se-
lected models for a better controlled data setting.
We restricted all training, development data to data
available during the WMT 2016 shared task on
APE and test on test set 2016. We also only
used artificial data made available by Junczys-
Dowmunt and Grundkiewicz (2016), dicarding the
newly created data in this work. To produce our
final training data set we oversample the original
training data 20 times and add the artificial data
sets. This results in a total of slightly more than
5M training triplets. For the hard-attention mod-
els, we create new target training and development
files following the LCS-based procedure outlined
in section 2.2.

Table 3 contains a selection of most relevant re-
sults for the WMT16 APE shared task – during the
task and afterwards. WMT 2016-baseline 1 is the
raw uncorrected mt output, baseline 2 is the results

of a vanilla phrase-based Moses system (Koehn
et al., 2007) trained only on the official 12,000
sentences. Junczys-Dowmunt and Grundkiewicz
(2016) is the best system at the shared task. Pal
et al. (2017) SYMMETRIC is the currently best re-
ported result on the WMT16 APE test set for a sin-
gle neural model (single source), whereas Pal et al.
(2017) RERANKING – the overall best reported re-
sult on the test set – is a system combination of Pal
et al. (2017) SYMMETRIC with phrase-based mod-
els via n-best list re-ranking.

In Table 4 we present the results for the mod-
els discussed in this work. The double-attention
models outperform the best WMT16 system and
the currently reported best single-model Pal et al.
(2017) SYMMETRIC. The ensembles also beat the
system combination Pal et al. (2017) RERANK-
ING in terms of TER (not in terms of BLEU
though). The simpler double-attention model with
no hard-attention ENCDEC-DOUBLE-ATT reaches
slightly better results on the test set than its
counterpart with added hard attention ENCDEC-
HARD+DOUBLE-ATT, but the situation would
have been less clear if only the dev set were used
to choose the best model.

644



Acknowledgments

This research was funded by the Amazon Aca-
demic Research Awards program.

References
Roee Aharoni and Yoav Goldberg. 2016. Sequence to

sequence transduction with hard monotonic atten-
tion. arXiv preprint arXiv:1611.01487 .

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR
abs/1409.0473. http://arxiv.org/abs/1409.0473.

Ondřej Bojar, Rajen Chatterjee, Christian Feder-
mann, Yvette Graham, Barry Haddow, Matthias
Huck, Antonio Jimeno Yepes, Philipp Koehn,
Varvara Logacheva, Christof Monz, Matteo Negri,
Aurelie Neveol, Mariana Neves, Martin Popel,
Matt Post, Raphael Rubino, Carolina Scarton,
Lucia Specia, Marco Turchi, Karin Verspoor,
and Marcos Zampieri. 2016. Findings of the
2016 conference on machine translation. In
Proceedings of the First Conference on Ma-
chine Translation. Association for Computational
Linguistics, Berlin, Germany, pages 131–198.
http://www.aclweb.org/anthology/W/W16/W16-
2301.

Iacer Calixto, Qun Liu, and Nick Campbell. 2017.
Doubly-attentive decoder for multi-modal neu-
ral machine translation. CoRR abs/1702.01287.
http://arxiv.org/abs/1702.01287.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learn-
ing Phrase Representations Using RNN Encoder-
Decoder for Statistical Machine Translation. In
Proc. of Empirical Methods in Natural Language
Processing.

Yarin Gal. 2015. A Theoretically Grounded Appli-
cation of Dropout in Recurrent Neural Networks.
ArXiv e-prints .

Daniel S. Hirschberg. 1977. Algorithms for the longest
common subsequence problem. J. ACM 24(4):664–
675. https://doi.org/10.1145/322033.322044.

Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu
Hoang. 2016. Is neural machine translation ready
for deployment? A case study on 30 transla-
tion directions. arXiv preprint arXiv:1610.01108
http://arxiv.org/abs/1610.01108.

Marcin Junczys-Dowmunt and Roman Grundkiewicz.
2016. Log-linear combinations of monolingual and
bilingual neural machine translation models for au-
tomatic post-editing. In Proceedings of the First
Conference on Machine Translation. pages 751–
758.

Marcin Junczys-Dowmunt and Roman Grund-
kiewicz. 2017. An exploration of neural
sequence-to-sequence architectures for auto-
matic post-editing. CoRR abs/1706.04138.
https://arxiv.org/abs/1706.04138.

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 http://arxiv.org/abs/1412.6980.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics. Association for
Computational Linguistics, pages 177–180.

Jindrich Libovický and Jindrich Helcl. 2017. At-
tention strategies for multi-source sequence-
to-sequence learning. CoRR abs/1704.06567.
http://arxiv.org/abs/1704.06567.

Jindřich Libovický, Jindřich Helcl, Marek Tlustý,
Ondřej Bojar, and Pavel Pecina. 2016. CUNI
system for WMT16 automatic post-editing
and multimodal translation tasks. In Pro-
ceedings of the First Conference on Machine
Translation. Association for Computational
Linguistics, Berlin, Germany, pages 646–654.
http://www.aclweb.org/anthology/W/W16/W16-
2361.

Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of the An-
nual Meeting on Association for Computational Lin-
guistics. pages 160–167.

Santanu Pal, Sudip Kumar Naskar, Mihaela Vela, Qun
Liu, and Josef van Genabith. 2017. Neural auto-
matic post-editing using prior alignment and rerank-
ing. In Proceedings of the European Chapter of the
Association for Computational Linguistics. pages
349–355.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: A method for au-
tomatic evaluation of machine translation. In
Proceedings of the 40th Annual Meeting on As-
sociation for Computational Linguistics. Asso-
ciation for Computational Linguistics, Strouds-
burg, PA, USA, ACL ’02, pages 311–318.
https://doi.org/10.3115/1073083.1073135.

Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexan-
dra Birch, Barry Haddow, Julian Hitschler, Marcin
Junczys-Dowmunt, Samuel Läubli, Antonio Vale-
rio Miceli Barone, Jozef Mokry, and Maria Nade-
jde. 2017. Nematus: a toolkit for neural ma-
chine translation. In Proceedings of the Soft-
ware Demonstrations of the 15th Conference of
the European Chapter of the Association for Com-
putational Linguistics. Association for Computa-
tional Linguistics, Valencia, Spain, pages 65–68.
http://aclweb.org/anthology/E17-3017.

645



Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of Association for Machine
Translation in the Americas,.

Barret Zoph and Kevin Knight. 2016. Multi-
source neural translation. CoRR abs/1601.00710.
http://arxiv.org/abs/1601.00710.

646


