



















































Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2979–2984
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

2979

Loss in Translation:
Learning Bilingual Word Mapping with a Retrieval Criterion

Armand Joulin, Piotr Bojanowski, Tomas Mikolov, Hervé Jégou and Edouard Grave
Facebook AI Research

{ajoulin,bojanowski,tmikolov,rvj,egrave}@fb.com

Abstract

Continuous word representations learned sepa-
rately on distinct languages can be aligned so
that their words become comparable in a com-
mon space. Existing works typically solve a
quadratic problem to learn a orthogonal matrix
aligning a bilingual lexicon, and use a retrieval
criterion for inference. In this paper, we pro-
pose an unified formulation that directly op-
timizes a retrieval criterion in an end-to-end
fashion. Our experiments on standard bench-
marks show that our approach outperforms the
state of the art on word translation, with the
biggest improvements observed for distant lan-
guage pairs such as English-Chinese.

1 Introduction

Previous work has proposed to learn a linear map-
ping between continuous representations of words
by employing a small bilingual lexicon as supervi-
sion. The transformation generalizes well to words
that are not observed during training, making it
possible to extend the lexicon. Another applica-
tion is to transfer predictive models between lan-
guages (Klementiev et al., 2012).

The first simple method proposed by Mikolov
et al. (2013b) has been subsequently improved
by changing the problem parametrization. One
successful suggestion is to `2–normalize the word
vectors and to constrain the linear mapping to
be orthogonal (Xing et al., 2015). An alignment
is then efficiently found using orthogonal Pro-
crustes (Artetxe et al., 2016; Smith et al., 2017),
improving the accuracy on standard benchmarks.

Yet, the resulting models suffer from the so-
called “hubness problem”: some word vectors tend
to be the nearest neighbors of an abnormally high
number of other words. This limitation is now ad-
dressed by applying a corrective metric at inference
time, such as the inverted softmax (ISF) (Smith
et al., 2017) or the cross-domain similarity local

scaling (CSLS) (Conneau et al., 2017). This is
not fully satisfactory because the loss used for in-
ference is not consistent with that employed for
training. This observation suggests that the square
loss is suboptimal and could advantageously be
replaced by a loss adapted to retrieval.

In this paper, we propose a training objective
inspired by the CSLS retrieval criterion. We in-
troduce convex relaxations of the corresponding
objective function, which are efficiently optimized
with projected subgradient descent. This loss can
advantageously include unsupervised information
and therefore leverage the representations of words
not occurring in the training lexicon.

Our contributions are as follows. First we in-
troduce our approach and empirically evaluate it
on standard benchmarks for word translation. We
obtain state-of-the-art bilingual mappings for more
than 25 language pairs. Second, we specifically
show the benefit of our alternative loss function
and of leveraging unsupervised information. Fi-
nally, we show that with our end-to-end formu-
lation, a non-orthogonal mapping achieves better
results. The code for our approach is a part of
the fastText library1 and the aligned vectors are
available on https://fasttext.cc/.

2 Preliminaries on bilingual mappings

This section introduces pre-requisites and prior
works to learn a mapping between two languages,
using a small bilingual lexicon as supervision.

We start from two sets of continuous representa-
tions in two languages, each learned on monolin-
gual data. Let us introduce some notation. Each
word i ∈ {1, . . . , N} in the source language (re-
spectively target language) is associated with a vec-
tor xi ∈ Rd (respectively yi ∈ Rd). For simplicity,

1https://github.com/facebookresearch/
fastText/tree/master/alignment/

https://fasttext.cc/
https://github.com/facebookresearch/fastText/tree/master/alignment/
https://github.com/facebookresearch/fastText/tree/master/alignment/


2980

we assume that our initial lexicon, or seeds, cor-
responds to the first n pairs (xi,yi)i∈{1,...,n}. The
goal is to extend the lexicon to all source words i ∈
{n + 1, . . . , N} that are not seeds. Mikolov et
al. (2013b) learn a linear mapping W ∈ Rd×d be-
tween the word vectors of the seed lexicon that min-
imizes a measure of discrepancy between mapped
word vectors of the source language and word vec-
tors of the target language:

min
W∈Rd×d

1

n

n∑
i=1

`(Wxi,yi), (1)

where ` is a loss function, typically the square loss
`2(x,y) = ‖x− y‖22. This leads to a least squares
problem, which is solved in closed form.

Orthogonality. The linear mapping W is con-
strained to be orthogonal, i.e. such that W>W =
Id, where Id is the d-dimensional identity ma-
trix. This choice preserves distances between word
vectors, and likewise word similarities. Previous
works (Xing et al., 2015; Artetxe et al., 2016; Smith
et al., 2017) experimentally observed that constrain-
ing the mapping in such a way improves the quality
of the inferred lexicon. With the square loss and
by enforcing an orthogonal mapping W, Eq. (1)
admits a closed form solution (Gower and Dijkster-
huis, 2004): W∗ = UV>, where UDV> is the
singular value decomposition of the matrix Y>X.

Inference. Once a mapping W is learned, one
can infer word correspondences for words that are
not in the initial lexicon. The translation t(i) of a
source word i is obtained as

t(i) ∈ argmin
j∈{1,...,N}

`(Wxi,yj). (2)

When the squared loss is used, this amounts to com-
puting Wxi and to performing a nearest neighbor
search with respect to the Euclidean distance:

t(i) ∈ argmin
j∈{1,...,N}

‖Wxi − yj‖22. (3)

Hubness. A common observation is that near-
est neighbor search for bilingual lexicon inference
suffers from the “hubness problem” (Doddington
et al., 1998; Dinu et al., 2014). Hubs are words that
appear too frequently in the neighborhoods of other
words. To mitigate this effect, a simple solution is
to replace, at inference time, the square `2-norm
in Eq. (3) by another criterion, such as ISF (Smith
et al., 2017) or CSLS (Conneau et al., 2017).

This solution, both with ISF and CSLS criteria,
is applied with a transformation W learned using
the square loss. However, replacing the loss in
Eq. (3) creates a discrepancy between the learning
of the translation model and the inference.

3 Word translation as a retrieval task

In this section, we propose to directly include the
CSLS criterion in the model in order to make learn-
ing and inference consistent. We also show how to
incorporate unsupervised information..

The CSLS criterion is a similarity measure be-
tween the vectors x and y defined as:

CSLS(x,y) = −2 cos(x,y)

+
1

k

∑
y′∈NY (x)

cos(x,y′)+
1

k

∑
x′∈NX(y)

cos(x′,y),

where NY (x) is the set of k nearest neighbors
of the point x in the set of target word vec-
tors Y = {y1, . . . ,yN}, and cos is the cosine sim-
ilarity. Note, the second term in the expression of
the CSLS loss does not change the neighbors of x.
However, it gives a loss function that is symmet-
rical with respect to its two arguments, which is a
desirable property for training.

Objective function. Let us now write the opti-
mization problem for learning the bilingual map-
ping with CSLS. At this stage, we follow previ-
ous work and constrain the linear mapping W
to belong to the set of orthogonal matrices Od.
Here, we also assume that word vectors are `2-
normalized. Under these assumptions, we have
cos(Wxi,yi) = x

>
i W

>yi. Similarly, we have
‖yj −Wxi‖22 = 2− 2x>i W>yj . Therefore, find-
ing the k nearest neighbors of Wxi among the ele-
ments of Y is equivalent to finding the k elements
of Y which have the largest dot product with Wxi.
We adopt this equivalent formulation because it
leads to a convex formulation when relaxing the
orthogonality constraint on W. In summary, our
optimization problem with the Relaxed CSLS loss
(RCSLS) is written as:

min
W∈Od

1

n

n∑
i=1

−2x>i W>yi

+
1

k

∑
yj∈NY (Wxi)

x>i W
>yj

+
1

k

∑
Wxj∈NX(yi)

x>j W
>yi. (4)



2981

Convex relaxation. Eq. (4) involves the min-
imization of a non-smooth cost function over
the manifold of orthogonal matrices Od. As
such, it can be solved using manifold optimiza-
tion tools (Boumal et al., 2014). In this work, we
consider as an alternative to the set Od, its convex
hull Cd, i.e., the unit ball of the spectral norm. We
refer to this projection as the “Spectral” model. We
also consider the case where these constraints on
the alignment matrix are simply removed.

Having a convex domain allows us to reason
about the convexity of the cost function. We ob-
serve that the second and third terms in the CSLS
loss can be rewritten as follows:∑
yj∈Nk(Wxi)

x>i W
>yj = max

S∈Sk(n)

∑
j∈S

x>i W
>yj ,

where Sk(n) denotes the set of all subsets of
{1, . . . , n} of size k. This term, seen as a func-
tion of W, is a maximum of linear functions of W,
which is convex (Boyd and Vandenberghe, 2004).
This shows that our objective function is convex
with respect to the mapping W and piecewise lin-
ear (hence non-smooth). Note, our approach could
be generalized to other loss functions by replac-
ing the term x>i W

>yj by any function convex in
W. We minimize this objective function over the
convex set Cd by using the projected subgradient
descent algorithm.

The projection onto the set Cd is solved by tak-
ing the singular value decomposition (SVD) of the
matrix, and thresholding the singular values to one.

Extended Normalization. Usually, the number
of word pairs in the seed lexicon n is small with
respect to the size of the dictionaries N . To benefit
from unlabeled data, it is common to add an itera-
tive “refinement procedure” (Artetxe et al., 2017)
when learning the translation model W. Given a
model Wt, this procedure iterates over two steps.
First it augments the training lexicon by keeping
the best-inferred translation in Eq. (3). Second it
learns a new mapping Wt+1 by solving the prob-
lem in Eq. (1). This strategy is similar to standard
semi-supervised approaches where the training set
is augmented over time. In this work, we propose
to use the unpaired words in the dictionaries as
“negatives” in the RCSLS loss: instead of comput-
ing the k-nearest neighbors NY (Wxi) amongst
the annotated words {y1, . . . ,yn}, we do it over
the whole dictionary {y1, . . . ,yN}.

4 Experiments

This section reports the main results obtained with
our method. We provide complementary results
and an ablation study in the appendix. We refer
to our method without constraints as RCSLS and
as RCSLS+spectral if the spectral constraints are
used.

4.1 Implementation details
We choose a learning rate in {1, 10, 25, 50} and
a number of epochs in {10, 20} on the validation
set. For the unconstrained RCSLS, a small `2 regu-
larization can be added to prevent the norm of W
to diverge. In practice, we do not use any regular-
ization. For the English-Chinese pairs (en-zh), we
center the word vectors. The number of nearest
neighbors in the CSLS loss is 10. We use the `2-
normalized fastText word vectors by Bojanowski
et al. (2017) trained on Wikipedia.

4.2 The MUSE benchmark
Table 1 reports the comparison of RCSLS with stan-
dard supervised and unsupervised approaches on
5 language pairs (in both directions) of the MUSE
benchmark (Conneau et al., 2017). Every approach
uses the Wikipedia fastText vectors and supervi-
sion comes in the form of a lexicon composed of
5k words and their translations. Regardless of the
relaxation, RCSLS outperforms the state of the art
by, on average, 3 to 4% in accuracy. This shows
the importance of using the same criterion during
training and inference. Note that the refinement
step (“refine”) also uses CSLS to finetune the align-
ments but leads to a marginal gain for supervised
methods.

Interestingly, RCSLS achieves a better perfor-
mance without constraints (+0.8%) for all pairs.
Contrary to observations made in previous works,
this result suggests that preserving the distance be-
tween word vectors is not essential for word trans-
lation. Indeed, previous works used a `2 loss where,
indeed, orthogonal constraints lead to an improve-
ment of +5.3% (Procrustes versus Least Square
Error). This suggests that a linear mapping W
with no constraints works well only if it is learned
with a proper criterion.

Impact of extended normalization. Table 2 re-
ports the gain brought by including words not in
the lexicon (unannotated words) to the performance
of RCSLS. Extending the dictionary significantly
improves the performance on all language pairs.



2982

Method en-es es-en en-fr fr-en en-de de-en en-ru ru-en en-zh zh-en avg.

Adversarial + refine 81.7 83.3 82.3 82.1 74.0 72.2 44.0 59.1 32.5 31.4 64.3
ICP + refine 82.2 83.8 82.5 82.5 74.8 73.1 46.3 61.6 - - -
Wass. Proc. + refine 82.8 84.1 82.6 82.9 75.4 73.3 43.7 59.1 - - -

Least Square Error 78.9 80.7 79.3 80.7 71.5 70.1 47.2 60.2 42.3 4.0 61.5
Procrustes 81.4 82.9 81.1 82.4 73.5 72.4 51.7 63.7 42.7 36.7 66.8
Procrustes + refine 82.4 83.9 82.3 83.2 75.3 73.2 50.1 63.5 40.3 35.5 66.9

RCSLS + spectral 83.5 85.7 82.3 84.1 78.2 75.8 56.1 66.5 44.9 45.7 70.2
RCSLS 84.1 86.3 83.3 84.1 79.1 76.3 57.9 67.2 45.9 46.4 71.0

Table 1: Comparison between RCSLS, Least Square Error, Procrustes and unsupervised approaches in the setting
of Conneau et al. (2017). All the methods use the CSLS criterion for retrieval. “Refine” is the refinement step
of Conneau et al. (2017). Adversarial, ICP and Wassertsein Proc. are unsupervised (Conneau et al., 2017; Hoshen
and Wolf, 2018; Grave et al., 2018).

en-es en-fr en-de en-ru avg.

Train 80.7 82.3 74.8 51.9 72.4
Ext. 84.1 83.3 79.1 57.9 76.1

Table 2: Accuracy with and without an extended nor-
malization for RCSLS. “Ext.” uses the full 200k vocab-
ulary and “Train” only uses the pairs from the training
lexicon.

en-it it-en

Adversarial + refine + CSLS 45.1 38.3

Mikolov et al. (2013b) 33.8 24.9
Dinu et al. (2014) 38.5 24.6
Artetxe et al. (2016) 39.7 33.8
Smith et al. (2017) 43.1 38.0
Procrustes + CSLS 44.9 38.5

RCSLS 45.5 38.0

Table 3: Accuracy on English and Italian with the set-
ting of Dinu et al. (2014). “Adversarial” is an unsuper-
vised technique. The adversarial and Procrustes results
are from Conneau et al. (2017). We use a CSLS crite-
rion for retrieval.

4.3 The WaCky dataset

Dinu et al. (2014) introduce a setting where word
vectors are learned on the WaCky datasets (Baroni
et al., 2009) and aligned with a noisy bilingual
lexicon. We select the number of epochs within
{1, 2, 5, 10} on a validation set. Table 3 shows that
RCSLS is on par with the state of the art. RCSLS
is thus robust to relatively poor word vectors and
noisy lexicons.

Original Aligned

Sem. Synt. Tot. Sem. Synt. Tot.

CS 26.4 76.7 63.7 27.3 77.7 64.6
DE 62.2 56.9 59.5 61.4 57.1 59.3
ES 54.5 59.4 56.8 55.1 61.1 57.9
FR 76.0 54.7 68.5 75.2 55.1 68.1
IT 51.8 62.0 56.9 52.7 63.8 58.2
PL 49.7 62.4 53.4 50.9 63.2 54.5
ZH 42.6 - 42.6 47.2 - 47.2

Avg. 51.9 62.0 57.3 52.8 58.5 58.5

Table 4: Performance on word analogies for differ-
ent languages. We compare the original embeddings to
their mapping to English. The mappings are learned
using the full MUSE bilingual lexicons. We use the
fastText vectors of Bojanowski et al. (2017).

4.4 Comparison with existing aligned vectors

Recently, word vectors based on fastText have been
aligned and released by Smith et al. (2017, Baby-
lonPartners, BP) and Conneau et al. (2017, MUSE).
Both use a variation of Procrustes to align word
vectors in the same space.

We compare these methods to RCSLS and re-
port results in Table 5. RCSLS improves the perfor-
mance by +3.5% over MUSE vectors when trained
with the same lexicon (Original). Training RSCSL
on the full training lexicon (Full) brings an addi-
tional improvement of +2.9% on average with a
CSLS criterion, and +6.1% with a NN criterion.
For reference, the performance of Procrustes only
improves by +1.4% with CSLS and even degrades
with a NN criterion. RCSLS benefits more from
additional supervision than Procrustes. Finally,



2983

BP∗ MUSE Proc. RCSLS

Orig. Full Orig. Full

BG 55.7 57.5 58.1 63.9 65.2
CA 66.5 70.9 70.5 73.8 75.0
CS 63.9 64.5 66.3 68.2 71.1
DA 66.8 67.4 68.3 71.1 72.9
DE 68.9 72.7 73.5 76.9 77.6
EL 54.9 58.5 60.1 62.7 64.5
ES 82.1 83.5 84.5 86.4 87.1
ET 41.5 45.7 47.3 49.5 53.7
FI 56.7 59.5 61.9 65.8 69.9
FR 81.7 82.4 82.5 84.7 84.7
HE 51.5 54.1 55.4 57.8 60.0
HR 48.9 52.2 53.4 55.6 60.2
HU 61.9 64.9 66.1 69.3 73.1
ID 62.8 67.9 67.9 69.7 72.9
IT 75.3 77.9 78.5 81.5 82.8
MK 53.9 54.6 55.4 59.9 60.4
NL 72.0 75.3 76.1 79.7 80.5
NO 65.3 67.4 68.3 71.2 73.3
PL 63.3 66.9 68.1 70.5 73.5
PT 77.7 80.3 80.4 82.9 84.6
RO 66.3 68.1 67.6 74.0 73.9
RU 61.3 63.7 64.3 67.1 70.3
SK 55.1 55.3 57.9 59.0 61.7
SL 51.1 50.4 52.5 54.2 58.2
SV 55.9 60.0 64.0 63.7 69.5
TR 57.4 59.2 61.4 61.9 65.8
UK 48.7 49.3 51.3 51.5 55.5
VI 35.0 55.8 63.0 55.8 66.9

CSLS 60.8 63.8 65.2 67.4 70.2

NN 54.6 57.4 57.5 62.4 68.5

Table 5: Comparison with publicly available aligned
vectors over 28 languages. All use supervision. Aligne-
ments are learned either on the “Original” or “Full”
MUSE training. We report the detailed performance
with a CSLS criterion and the average for both NN and
CSLS criteria.
∗BP uses a different training set of comparable size.

the gap between RCSLS and the other methods is
higher with a NN criterion, suggesting that RCSLS
imports some of the properties of CSLS to the dot
product between aligned vectors.

4.5 Impact on word vectors

Non-orthogonal mapping of word vectors changes
their dot products. We evaluate the impact of this
mapping on word analogy tasks (Mikolov et al.,

Original Aligned

DE
GUR350 72 74
WS350 68 70
ZG222 46 44

ES WS353 59 61

IT WS350 64 64

PT WS353 60 58

RO WS353 58 55

HJ 67 65
WS350 59 58

ZH SIM 35 44

Avg. 58.8 59.2

Table 6: Performance on word similarities for differ-
ent languages. We compare the original embeddings
to their mapping to English. The mappings are learned
with the full MUSE bilingual lexicons over the fastText
vectors of Bojanowski et al. (2017).

2013a). In Table 4, we report the accuracy on analo-
gies for raw word vectors and our vectors mapped
to English with an alignement trained on the full
MUSE training set. Regardless of the source lan-
guage, the mapping does not negatively impact the
word vectors. Similarly, our alignement has also lit-
tle impact on word similarity, as shown in Table 6.

We confirm this observation by running the re-
verse mapping, i.e., by mapping the English word
vectors of Mikolov et al. (2018) to Spanish. It leads
to an improvement of 1% both for vectors trained
on Common Crawl (85% to 86%) and Wikipedia +
News (87% to 88%).

5 Conclusion

This paper shows that minimizing a convex relax-
ation of the CSLS loss significantly improves the
quality of bilingual word vector alignment. We use
a reformulation of CSLS that generalizes to convex
functions beyond dot products and provides a sin-
gle end-to-end training that is consistent with the
inference stage. Finally, we show that removing
the orthogonality constraint does not degrade the
quality of the aligned vectors.



2984

References
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.

Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2289–2294.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.
Learning bilingual word embeddings with (almost)
no bilingual data. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
451–462.

Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language resources and evalua-
tion, 43(3):209–226.

Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2017. Enriching word vec-
tors with subword information. Transactions
of the Association for Computational Linguistics,
5:135–146. https://fasttext.cc/docs/
en/pretrained-vectors.html.

Nicolas Boumal, Bamdev Mishra, P.-A. Absil, and
Rodolphe Sepulchre. 2014. Manopt, a Matlab tool-
box for optimization on manifolds. Journal of Ma-
chine Learning Research, 15:1455–1459.

Stephen Boyd and Lieven Vandenberghe. 2004. Con-
vex optimization. Cambridge university press.

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2017.
Word translation without parallel data. arXiv
preprint arXiv:1710.04087. http://github.
com/facebookresearch/MUSE.

Georgiana Dinu, Angeliki Lazaridou, and Marco Ba-
roni. 2014. Improving zero-shot learning by mit-
igating the hubness problem. arXiv preprint
arXiv:1412.6568.

George Doddington, Walter Liggett, Alvin Martin,
Mark Przybocki, and Douglas Reynolds. 1998.
Sheep, goats, lambs and wolves: A statistical analy-
sis of speaker performance in the NIST 1998 speaker
recognition evaluation. Technical report.

John C Gower and Garmt B Dijksterhuis. 2004. Pro-
crustes problems, volume 30. Oxford University
Press on Demand.

Edouard Grave, Armand Joulin, and Quentin Berthet.
2018. Unsupervised alignment of embeddings
with wasserstein procrustes. arXiv preprint
arXiv:1805.11222.

Yedid Hoshen and Lior Wolf. 2018. An iterative clos-
est point method for unsupervised word translation.
arXiv preprint arXiv:1801.06126.

Alexandre Klementiev, Ivan Titov, and Binod Bhattarai.
2012. Inducing crosslingual distributed represen-
tations of words. Proceedings of COLING 2012,
pages 1459–1474.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Edouard Grave, Piotr Bojanowski,
Christian Puhrsch, and Armand Joulin. 2018. Ad-
vances in pre-training distributed word representa-
tions. In Proceedings of the Eleventh International
Conference on Language Resources and Evaluation
(LREC 2018), Paris, France. European Language Re-
sources Association (ELRA).

Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013b.
Exploiting similarities among languages for ma-
chine translation. arXiv preprint arXiv:1309.4168.

Samuel L Smith, David HP Turban, Steven
Hamblin, and Nils Y Hammerla. 2017. Of-
fline bilingual word vectors, orthogonal
transformations and the inverted softmax.
arXiv preprint arXiv:1702.03859. https:
//github.com/Babylonpartners/
fastText_multilingual.

Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015.
Normalized word embedding and orthogonal trans-
form for bilingual word translation. In Proceedings
of the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 1006–1011.

https://fasttext.cc/docs/en/pretrained-vectors.html
https://fasttext.cc/docs/en/pretrained-vectors.html
http://www.manopt.org
http://www.manopt.org
http://github.com/facebookresearch/MUSE
http://github.com/facebookresearch/MUSE
https://github.com/Babylonpartners/fastText_multilingual
https://github.com/Babylonpartners/fastText_multilingual
https://github.com/Babylonpartners/fastText_multilingual

