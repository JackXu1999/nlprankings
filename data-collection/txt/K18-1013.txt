



















































Learning to Embed Semantic Correspondence for Natural Language Understanding


Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 131–140
Brussels, Belgium, October 31 - November 1, 2018. c©2018 Association for Computational Linguistics

131

Learning to Embed Semantic Correspondence
for Natural Language Understanding

Sangkeun Jung1, Jinsik Lee2, and Jiwon Kim3

1,2,3T-Brain, AI Research Center, SK telecom
1Dept. of Computer Science and Engineering, Chungnam National University

1hugmanskj@gmail.com
2,3 {jinsik16.lee, jk}@sktbrain.com

Abstract

While learning embedding models has yielded
fruitful results in several NLP subfields, most
notably Word2Vec, embedding correspon-
dence has relatively not been well explored es-
pecially in the context of natural language un-
derstanding (NLU), a task that typically ex-
tracts structured semantic knowledge from a
text. A NLU embedding model can facilitate
analyzing and understanding relationships be-
tween unstructured texts and their correspond-
ing structured semantic knowledge, essential
for both researchers and practitioners of NLU.
Toward this end, we propose a framework that
learns to embed semantic correspondence be-
tween text and its extracted semantic knowl-
edge, called semantic frame. One key con-
tributed technique is semantic frame recon-
struction used to derive a one-to-one mapping
between embedded vectors and their corre-
sponding semantic frames. Embedding into
semantically meaningful vectors and comput-
ing their distances in vector space provide a
simple, but effective way to measure seman-
tic similarities. With the proposed framework,
we demonstrate three key areas where the em-
bedding model can be effective: visualization,
semantic search and re-ranking.

1 Introduction

The goal of NLU is to extract meaning from a nat-
ural language and infer the user intention. NLU
typically involves two tasks: identifying user in-
tent and extracting domain-specific entities, the
second of which is often referred to as slot-filling
(Mesnil et al., 2013; Jeong and Lee, 2006; Kim
et al., 2016). Typically, the NLU task can be
viewed as an extraction of structured text from a
raw text. In NLU literature, the structured form of
intent and filled slots is called a semantic frame.

�

��: text �� : semantic 

frame

ℛ� ℛ��

“Please list all 

flights from Dallas 

to Philadelphia on 

Monday.”

���

	

intent : atis_flight

fromloc.

city_name
Dallas

toloc.

city_name
Philadelphia

depart_date.

day_name
Monday


 ∶ shared vector space

�

(a) Framework

distance

�

distance

�

distance
comparison

��
��

��

visualization

distance

��

�

�

�

(b) Applications

Figure 1: Semantic vector learning framework and
applications. We assume a pair of corresponding
text and semantic frame (t, s), which has semanti-
cally the same meaning in a raw text domain (χT ),
and a semantic frame domain (χS) can be encoded
to a vector v in a shared embedding vector space
Z . RT and RS are two reader functions that en-
code raw and structured text to a semantic vector.
W is a writing function that decodes a semantic
vector to a symbolic semantic frame.

In this study, we aim to learn the meaningful
distributed semantic representation, rather than fo-
cusing on building the NLU system itself. Once
we obtained a reliable and reasonable semantic
representation in a vector form, we can devise
many useful and new applications around the NLU
(Figure 1). Because all the instances of text and se-
mantic frame are placed on a single vector space,
we can obtain the natural and direct distance mea-
sure between them. Using the distance measure,
the similar text or semantic frame instances can



132

be searched directly and interchangeably by the
distance comparison. Moreover, re-ranking of
multiple NLU results can be applied without fur-
ther learning by comparing the distances between
the text and the corresponding predicted seman-
tic frame. Converting symbols to vectors makes it
possible to do visualization naturally as well.

In this study, we assumed that the reasonable
semantic vector representation satisfies the follow-
ing properties.

• Property - embedding correspondence:
Distributed representation of text should be
the same as the distributed representation of
the corresponding semantic frame.

• Property - reconstruction: Symbolic se-
mantic frame should be recovered from the
learned semantic vector.

We herein introduce a novel semantic vector learn-
ing framework called ESC (Embedding Semantic
Correspondence learning), which satisfies the as-
sumed properties.

The remainder of the paper is structured as fol-
lows: Section 2 describes the detailed structure of
the framework. Section 3 introduces semantic vec-
tor applications in NLU. Section 4 describes the
experimental settings and results. Section 5 dis-
cusses the related work. Finally, section 6 presents
the conclusion.

2 ESC Framework

Our framework consists of text reader, semantic
frame reader, and semantic frame writer. The
text reader embeds a sequence of tokens to a dis-
tributed vector representation. The semantic frame
reader reads the structured texts and encodes each
to a vector. vt represents a vector semantic frame
derived from the text reader, and vs represents a
vector semantic frame derived from the semantic
frame reader. Finally, the semantic frame writer
generates a symbolic semantic frame from a vec-
tor representation.

2.1 Text Reader
A text reader (Figure 2), implementing a neural
sentence encoder, reads a sequence of input tokens
and encodes each to a vector. In this study, we
used long short-term memory (LSTM) (Hochre-
iter and Schmidhuber, 1997) for encoding input
sequences. The encoding process can be defined
as

−→
h s = Rtext(EX(xs),

−→
h s−1)

vt = sigmoid(
−→
h S)

where s = {1, 2, ..., S} and
−→
h s is the forward

hidden states over the input sequence at time s;
Rtext is an RNN cell; and EX(xs) is the token em-
bedding function, which returns a distributed vec-
tor representation of token x at time s. The final
RNN output

−→
h S is taken as vt, which is a seman-

tic vector derived from the text.

2.2 Semantic Frame Reader
A semantic frame consists of structured tags such
as the intent and slot-tag and slot-values. In this
study, the intent tag is handled as a symbol, and the
slot-tags and slot-values are handled as a sequence
of symbols. For example, the sentence, “Please list
all flights from Dallas to Philadelphia on Monday.”
is handled as

• intent tag : atis flight
• slot-tag sequence :

[ fromloc.city name, toloc.city name, de-
part date.day name ]

• slot-value sequence :
[Dallas, Philadelphia, Monday].

The intent reader is a simple embedding func-
tion vintent = EI(i), which returns a distributed
vector representation of the intent tag i for a sen-
tence.

Stacked LSTM layer is used to read the se-
quences of slot-tags and slot-values. ES(o) is a
slot-tag embedding function with o as a token.
EV (a) is an embedding function with a as a to-
ken. The embedding result ES(om) and EV (am)
are concatenated at time-step m, and the merged
vectors are fed to the stacked layer for each time-
step (Figure 2). vtag,value - the reading result of
sequence of slot-tags and values - is taken from
the final output of RNN at time M . Finally, intent,
slot-tag and value encoded vectors are merged to
construct a distributed semantic frame representa-
tion as

vs = sigmoid(Wsf ([vintent; vtag,value]) + bsf )

where [; ] denotes the vector concatenation opera-
tor. The dimension of vs is same as vt. All embed-
ding weights are randomly initialized and learned
through the training process.



133

Semantic Frame ReaderText Reader

������ ������

��

⊚
��	
���, ���

�′� �′� �′�

⨀ ⨀ ⨀ ⨁

�������

�����
�

���
����

�����

⨀ �	
��
�

�	
��
� ��	
��
�

�����	

�
����
slot-tag 

writer

intent writer

Semantic Frame Writer

��

[ Please, list, all, 
flights, from, Dallas, 
to, Philadelphia, on,
Monday]

��, ��, … , ��
� [ fromloc.city_name, 

toloc.city_name, 
depart_date.day_name]

�� , ��, … , �� �

[ Dallas, Philadelphia ]�� , �� , … , �� �

atis_flight� �

��
�

�
⊛

������

������

⊛

������

������

� ⊛
��
�����

Figure 2: Text reader, semantic reader and semantic frame writer neural architecture. EX is an embedding
function for the input text token x. EI , ES , and EV are the embedding functions for the intent tag, slot-
tag and slot-value, respectively. � is a vector concatenation operation; � is a cross-entropy; ⊕ is an
average calculation; � represents the distance calculation. ŷintent is a reference intent tag vector and
ŷmslot is a reference slot tag vector at time m. M is the number of slots in a sentence (in the above
example, M = 3).

2.3 Semantic Frame Writer and Loss
Functions

One of the objectives of this study is to learn se-
mantically the reasonable vector representations
of text and a related semantic frame. Hence, we
set the properties of the desirable semantic vec-
tor, and the loss functions are defined to satisfy
the properties.

Loss for Property “embedding correspon-
dence” Distance loss measures the dissimilarity
between the encoded semantic vectors from the
text reader and those from the semantic frame
reader in the vector space. The loss is defined as

Ldist = dist(vt, vs)

where the dist function can be any vector distance
measure; however, in this study, we employed a
Euclidean and a cosine distance (=1.0 - cosine
similarity).

Loss for Property “reconstruction” Content
loss provides a measure of how much semantic
information the semantic frame vector contains.
Without the content loss, vt and vs tend to quickly

converge to zero vectors, implying the failure to
learn the semantic representation. To measure the
content keeping, symbolic semantic frame gen-
erations from semantic vector is performed, and
the difference between the original semantic frame
and the generated semantic frame is calculated.

Because the semantic frame’s slot-value has a
large vocabulary size to generate the slot values, a
reduced semantic frame is devised to ease the gen-
eration problem. A reduced semantic frame is cre-
ated by simply dropping the slot values from the
corresponding semantic frame. For example, in
Figure 2, slot values [Dallas, Philadelphia, Mon-
day] are removed to create a reduced semantic
frame. Content loss calculation is performed on
this reduced semantic frame. Another advantage
of employing reduced semantic frame is that the
learned distributed semantic vectors have more ab-
stract power because the learned semantic vectors
are less sensitive to the lexical vocabulary.

For content loss, the intent and slot-tags’ gen-
eration qualities are measured. The intent gener-
ation network can be simply defined using linear



134

Notation Dim. Description
EX 50 Token embedding
ES 50 Slot-tag embedding
EV 50 Slot-value embedding
vintent 50 Intent reader output
vtag,value 200 Slot-tag and value reader output
v 200 Semantic vector

Table 1: Hyperparameters of the model.

projection as

yintent = W
′
Iv + bI

where v is the semantic vector, and yintent is the
output vector.

The slot-tag generation networks are defined as
−→q m = RG(v,−→q m−1)
ymslot = W

′
S
−→q m + bS

where RG is an RNN cell. The semantic vector v
is copied and repeatedly fed into each RNN input.
The outputs from the RNN are projected onto the
slot tag space with W

′
S .

Figure 2 shows the intent and slot tag generation
networks and the corresponding loss calculation
methods. The generational losses can be defined
with the cross entropy between the generated tag
vector and the reference tag vector as

Lintent = CrossEntropy(ŷintent, yintent)

Lslot =
1

M

M∑
m=1

CrossEntropy(ŷmslot, y
m
slot)

where M is the number of slots in a sentence.
With the combination of intent and slot losses,

the content loss(Lcontent) to reconstruct a seman-
tic frame from a semantic vector v can be defined
as follows:

Lcontent = Lintent + Lslot

Finally, the total loss value (L) for learning the
semantic frame representation is defined with the
distance loss and content loss as

L = Ldist + Lcontent

The hyperparameters of the proposed model are
summarized in Table 1.

3 Applications

3.1 Multi-form Distance Measurement

Using the learned text- and semantic-frame reader,
we can measure not only the instances from the
same form (text or semantic frame form) but also

from different forms. Let’s denote a text as t and
a semantic frame as s, and the text and semantic
frame reader as RT and RS , respectively. The
distance measurements between them can be per-
formed as follows:

• dist(vit, v
j
t ) :

ti → RT (ti) = vit
tj → RT (tj) = vjt

• dist(vit, v
j
s):

ti → RT (ti) = vit
sj → RS(tj) = vjs

• dist(vis, v
j
s):

si → RS(si) = vis
sj → RS(sj) = vjs

3.2 Visualization
With vector semantic representation, we can vi-
sualize the instances (sentences) in an easier and
more natural way. Once the symbolic text or se-
mantic frame are converted to vector, vector visu-
alization methods such as t-sne (Maaten and Hin-
ton, 2008) can be used directly to check the rela-
tionship between instances or the distribution of
the entire corpus.

3.3 Re-ranking Without Further Learning
Re-ranking the NLU results from multiple NLU
modules is difficult but important if a robust NLU
system is to be built. Typically, a choice is made
by comparing the scores produced by each system.
However, this technique is not always feasible be-
cause the scores are often in different scales, or are
occasionally not provided at all (e.g., in the purely
rule-based NLU systems). The vector form of the
semantic frame provides a very clear and natural
solution for the re-ranking problem.

Figure 3 shows the flow of the re-ranking algo-
rithm with the proposed vector semantic represen-
tation. In this study, we reordered the NLU results
from multiple NLU systems according to their cor-
responding distances of vt to vs. It is noteworthy
that the proposed re-ranking algorithm does not re-
quire further learning for ranking such as ensem-
ble learning or learning-to-rank techniques. Fur-
ther, the proposed methods are applicable to any
type of NLU system. Even purely rule-based sys-
tems can be satisfactorily compared to purely sta-
tistical systems.



135

���

��

text

��������

	�
����������

��

���������

������	�����

����

���

��

��������

	�
����������

��

���������

������	�����

����

���

��

��������

	�
����������

��

���������

������	�����

����

�����

	�����

��

������� , ��
��

�

������� , ��
��

�

������� , ��
��

�

��

Figure 3: Re-ranking multiple NLU results using
the semantic vector. The semantic vector from the
text (vt) functions as a pivot. We show three dif-
ferent NLU systems in this illustration.

4 Experiments

For training and testing purposes, we used the
ATIS2 dataset (Price, 1990). The ATIS2 dataset
consists of an annotated intent and slot corpus for
an air travel information search task. ATIS2 data
set comes with a commonly used training and test
split. For tuning parameters, we further split the
training set into 90% training and 10% develop-
ment set.

4.1 Validity of Learned Semantic Vector with
Visualization

The intuition behind the proposed method is that
semantically similar instances will be grouped to-
gether if the semantic vector learning is performed
successfully. Figure 4 supports that the intuition
is correct. In the early stages of training, the in-
stances are scattered randomly; however, as the
training progresses, semantically similar instances
gather closer to each other. We observed that the
proposed framework groups and depicts the sen-
tences based on the intent tag remarkably well.

4.2 Multi-form Distance Measurement

In our framework, the instances having differ-
ent forms (text or semantic frame) can be com-
pared directly on a semantic vector space. To
demonstrate that multi-form distance measure-
ment works well, the sentence and semantic frame
search results with a sentence and a semantic

60 40 20 0 20 40 60

60

40

20

0

20

40

60

80

(a) Initial (Before Training)
60 40 20 0 20 40 60

80

60

40

20

0

20

40

60

(b) After 3 Epochs

80 60 40 20 0 20 40 60 80

60

40

20

0

20

40

60

(c) After 30 Epochs
60 40 20 0 20 40 60

60

40

20

0

20

40

60

80

(d) After 300 Epochs

60 40 20 0 20 40 60

60

40

20

0

20

40

60

ATIS_FLIGHT
ATIS_AIRFARE
ATIS_FLIGHT#ATIS_AIRFARE
ATIS_GROUND_SERVICE
ATIS_MEAL
ATIS_AIRPORT
ATIS_AIRLINE
ATIS_FLIGHT_TIME
ATIS_CITY
ATIS_GROUND_FARE
ATIS_QUANTITY
ATIS_ABBREVIATION
ATIS_DISTANCE
ATIS_AIRCRAFT
ATIS_CAPACITY
ATIS_FLIGHT_NO

(e) After Full Training

Figure 4: Visualization of semantic vectors through
training process. The plotted points are vt from
the text reader by t-sne processing in the testing
sentences. The different colors and shape combi-
nations represent different intent tags.

frame query are shown in Table 2.

Table 2 shows that text to text search is very well
done with the learned vector. The retrieved sen-
tence patterns are similar to the given text, and the
vocabulary presented is also similar. On the other
hand, in the case of the text to semantic frame
search, The sentence patterns are similar, but the
content words such as city name are not similar.
In fact, this is what we predicted, because the con-
tent loss for reconstruction property is measured
on reduced semantic frame which does not include
slot-values. In semantic frame to text search, we
can find similar behaviors. Retrieved results have
almost same intent tag and slot-tags, but have dif-



136

No. Text Semantic Frame

“Show Delta Airlines from Boston to Salt Lake”

1 Show Delta Airlines flights from Boston to
Salt Lake

1

ATIS FLIGHT
airline name Delta Airlines

fromloc.city name Boston
toloc.city name Salt Lake

2 Show Delta Airlines flights from Boston to
Salt Lake City

2

ATIS FLIGHT
airline name American Airlines

fromloc.city name Phonenix
toloc.city name Milwaukee

3 List Delta flights from Seattle to Salt Lake
City

3

ATIS FLIGHT
airline name Delta Airlines

fromloc.city name Montreal
toloc.city name Orlando

(a) Text as Query

No. Text Semantic Frame

[ATIS FLIGHT] flight mod(last), depart date.day name(Wednesday),
fromloc.city name(Oakland), toloc.city name(Salt Lake City)

1 Get last flight from Oakland to Salt Lake
City on Wednesday

1

ATIS FLIGHT
flight mod last

depart date.day name Wednesday
fromloc.city name Oakland

toloc.city name Salt Lake City

2 Get last flight from Oakland to Salt Lake
City on Wednesday or first flight from Oak-
land to Salt Lake City on Thursday

2

ATIS FLIGHT
flight mod first

depart date.day name Thursday
fromloc.city name Oakland

toloc.city name Salt Lake City

3 Get first flight from Oakland to Salt Lake
City on Thursday

3

ATIS FLIGHT
flight mod last

depart date.day name Wednesday
fromloc.city name Oakland

toloc.city name Salt Lake City
or or

flight mod first
depart date.day name Thursday

fromloc.city name Oakland
toloc.city name Salt Lake City

(b) Semantic Frame as Query

Table 2: Example of most similar instance search results in the test data according to the proposed frame-
work. Top-3 text and semantic frame retrieved instances given a single query are shown in left and right
side respectively.

ferent city or airport names which are correspond-
ing to slot-values. If we could include the slot-
value generation in the reconstruction loss with
large data, a better multi-form semantic search re-
sult might be expected.

To measure the quantitative search perfor-
mance, precision at K are reported in Table 3. Pre-
cision at K corresponds to the number of same sen-
tence pattern instances in the top K results. From
the search result, we can conclude that the learned
semantic vectors keep sentence pattern (intent tag
and slot-tags) information very well.

4.3 Re-ranking

We prepared 11 NLU systems for re-ranking.
Nine intent-/slot-combined classifiers and two in-

K Text Query SF Query
I S J I S J

1 98.30 63.15 62.93 99.43 70.75 70.52
3 99.09 72.45 72.00 99.55 77.78 77.44
5 99.09 75.17 74.72 99.77 78.80 78.68

10 99.09 76.98 76.42 99.77 80.39 80.27

Table 3: Same sentence pattern (intent and slot-
tags should be matched) search performance (Pre-
cision @K). SF, I, S and J stand for semantic
frame, intent, slot-tag and joint of intent and slot-
tag.



137

Identifier Combinations
C1 CRF
C2 CNN RNN
C3 RNN+CRF
C4 CRF
C5 MaxEnt RNN
C6 RNN+CRF
C7 CRF
C8 SVM RNN
C9 RNN+CRF
C10 Joint Liu (Liu and Lane, 2016)
C11 Joint Tur (Hakkani-Tür et al., 2016)

Table 4: Multiple NLU systems for re-ranking.

tent/slot joint classifiers were implemented. For
the combined classifiers, three intent classifiers
and three slot sequential classifiers were prepared
and combined. For the joint classifiers, those of
Liu and Lane (2016) and Hakkani-Tür et al. (2016)
were each implemented. Here, we did not signif-
icantly tune the NLU systems, as the purpose of
this paper is to learn the semantic vector, not to
build the state-of-the-art NLU systems.

A maximum-entropy (MaxEnt)- and a sup-
port vector machine (SVM)-based intent classi-
fier were implemented as a traditional sentence
classification method. Both classifiers share the
same feature set (1-gram, 2-gram, 3-gram, and 4-
gram around each word). Also, a convolutional-
neural network-based (CNN-based) (Kim, 2014)
sentence classification method was implemented.

A conditional random field (CRF)-based se-
quential classifier was implemented as a tradi-
tional slot classifier. Also, an RNN- and an
RNN+CRF-based sequential classifier were im-
plemented as a deep learning method. Bidirec-
tional LSTMs were used to build the simple RNN-
based classifier. By placing a CRF layer on top of
the bidirectional LSTM network (Lee, 2017), an
RNN+CRF-based network was implemented. In
addition, two joint NLU systems (Liu and Lane,
2016; Hakkani-Tür et al., 2016) are prepared by
reusing their codes, which are publicly accessi-
ble12 3.

Table 4 shows the summary of the NLU sys-
tems that we prepared and used for the re-ranking
experiments.

Table 5 shows the performance of all the NLU

1https://github.com/yvchen/JointSLU.git
2https://github.com/DSKSD/RNN-for-Joint-NLU
3The reported performance of C10 and C11 in their paper

were not reproduced with the open code.

NLU
systems

Intent Slot
acc. prec. rec. f m

C1 90.70 94.36 89.61 91.92
C2 90.70 92.33 92.46 92.40
C3 90.70 93.53 92.39 92.96
C4 94.10 94.36 89.61 91.92
C5 94.10 92.33 92.46 92.40
C6 94.10 93.53 92.39 92.96
C7 91.84 94.36 89.61 91.92
C8 91.84 92.33 92.46 92.40
C9 91.84 93.53 92.39 92.96
C10 96.03 93.68 92.64 93.16
C11 93.54 94.74 94.00 94.37
random 92.86 93.96 92.29 93.12
majority vote 94.10 95.63 93.68 94.64(baseline)
NLU score 95.58 94.81 93.89 94.35(baseline)
re-ranked 97.05 93.74 91.96 92.84(Euclidean)
re-ranked 97.05 95.40 94.11 94.75(cosine)
oracle 97.85 96.77 95.29 96.02

Table 5: NLU performance of multiple NLU sys-
tems and re-ranked results. Acc., prec., rec., and
f m stand for accuracy, precision, recall, and f-
measure, respectively.

systems, the proposed re-ranking algorithm’s per-
formance, and the oracle performance. Typical
choices in re-ranking NLU results are majority
voting and score-based ranking. In the majority
voting method, the semantic frame most predicted
by the NLU systems is selected. The score of the
NLU scoring method in Table 5 is the prediction
probability. In the case of joint NLU classifiers
(C10 and C11), the joint prediction probabilities
are used for the score. In the case of combina-
tion NLU systems (C1 to C9), the product of the
intent and slot prediction probabilities is used for
the score.

The proposed distance-based re-ranking
method using semantic vector shows superior se-
lection performance at both intent and slot-filling
tasks. It is noteworthy that the re-ranked intent
prediction performance (acc. 97.05) is relatively
close to the oracle intent performance (acc.
97.85), which is the upper bound. Compared to
the baseline re-ranker (NLU score), the proposed
re-ranker (cosine) achieves 33.25% and 7.07%
relative error reduction for intent prediction and
slot-filling task, respectively.



138

5 Related Work

The task of spoken NLU consists of intent clas-
sification and domain entity slot filling. Tradi-
tionally, both tasks are approached using statis-
tical machine-learning methods (Schwartz et al.,
1997; He and Young, 2005; Dietterich, 2002). Re-
cently, with the advances in deep learning, RNN-
based sequence encoding techniques have been
used to detect the intent or utterance type (Ravuri
and Stolcke, 2015), and RNN-based neural archi-
tectures have been employed for slot-filling tasks
(Mesnil et al., 2013, 2015). The combinations of
CRF and neural networks have also been explored
by Xu and Sarikaya (2013).

Recent works have focused on enriching the
representations for neural architectures to imple-
ment NLU. For example, Chen et al. focused on
leveraging substructure embeddings for joint se-
mantic frame parsing (Chen et al., 2016). Kim
et al. utilized several semantic lexicons, such as
WordNet, PPDB, and the Macmillan dictionary, to
enrich the word embeddings, and later used them
in the initial representation of words for intent de-
tection (Kim et al., 2016).

Previous NLU works have used statistical mod-
eling for the intent and slot-filling tasks, and input
representation. None of the work performed has
represented the text and semantic frame as a vec-
tor form simultaneously. To our best knowledge,
this is the first presentation of a method for learn-
ing the distributed semantic vector for both text
and semantic frame and it’s applications in NLU
research.

In general natural language processing litera-
ture, many raw text to vector studies to learn
the vector representations of text have been per-
formed. Mikolov et al. (2013); Pennington et al.
(2014); Collobert et al. (2011) proposed word
to vector techniques. Mueller and Thyagarajan
(2016); Le and Mikolov (2014) introduced embed-
ding methods at the sentence and document level.
Some attempts have shown that in this embed-
ding process, certain semantic information such as
analogy, antonym, and gender can be obtained in
the vector space.

Further, many structured text to vector tech-
niques have been introduced recently. Preller
(2014) introduced a logic formula embedding
method while Bordes et al. (2013); Do et al. (2018)
proposed translating symbolic structured knowl-
edge such as Wordnet and freebase.

We herein introduce a novel semantic frame em-
bedding method by simultaneously executing the
raw text to vector and structured text to vector
method in a single framework to learn semantic
representations more directly. In this framework,
the text and semantic frame are each projected
onto a vector space, and the distance loss between
the vectors is minimized to satisfy embedding cor-
respondence. Our research goes a step further to
guarantee that the learned vector indeed keep the
semantic information by checking the reconstruc-
tion the symbolic semantic frame from the vector.

In learning the parameters by minimizing the
vector distances, this work is similar to a Siamese
constitutional neural network (Chopra et al., 2005;
Mueller and Thyagarajan, 2016) or an autoencoder
(Hinton and Salakhutdinov, 2006); however, the
weights are not shared or transposed in this work.

6 Conclusion

In this study, we have proposed a new method
to learn a correspondence embedding model for
NLU. To learn a valid and meaningful distributed
semantic representation, two properties - embed-
ding correspondence and reconstruction - are con-
sidered. By minimizing the distance between the
semantic vectors which are the outputs of text
and semantic frame reader, the semantically equiv-
alent vectors are placed very close in the vec-
tor space. In addition, reconstruction consistency
from a semantic vector to symbol semantic frame
was jointly enforced to prevent the method from
learning trivial degenerate mappings (e.g. map-
ping all to zeros).

Through various experiments with ATIS2
dataset, we confirmed that the learned semantic
vectors indeed contain semantic information. Se-
mantic vector visualization and the results of sim-
ilar text and semantic frame search showed that
semantically similar instances are actually located
near on the vector space. Also, using the learned
semantic vector, re-ranking multiple NLU systems
can be implemented without further learning by
comparing semantic vector values of text and se-
mantic frame.

Based on the results of the proposed research,
various research directions can be considered in
the future. A semantic operation or algebra on
a vector space will be a very promising research
topic. Furthermore, with enough training data and
appropriate modification to our method, adding



139

text reconstruction constraint can be pursed and
generating text directly from a semantic vector
would be possible, somewhat resembling problem
settings of neural machine translation tasks.

References

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in neural information
processing systems, pages 2787–2795.

Yun-Nung Chen, Dilek Hakanni-Tür, Gokhan Tur, Asli
Celikyilmaz, Jianfeng Guo, and Li Deng. 2016.
Syntax or semantics? knowledge-guided joint se-
mantic frame parsing. In Spoken Language Tech-
nology Workshop (SLT), 2016 IEEE, pages 348–355.
IEEE.

Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005.
Learning a similarity metric discriminatively, with
application to face verification. In Computer Vision
and Pattern Recognition, 2005. CVPR 2005. IEEE
Computer Society Conference on, volume 1, pages
539–546. IEEE.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12(Aug):2493–2537.

Thomas Dietterich. 2002. Machine learning for se-
quential data: A review. Structural, syntactic, and
statistical pattern recognition, pages 227–246.

Kien Do, Truyen Tran, and Svetha Venkatesh. 2018.
Knowledge graph embedding with multiple relation
projections. arXiv preprint arXiv:1801.08641.

Dilek Hakkani-Tür, Gökhan Tür, Asli Celikyilmaz,
Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye-
Yi Wang. 2016. Multi-domain joint semantic frame
parsing using bi-directional rnn-lstm. In Inter-
speech, pages 715–719.

Yulan He and Steve Young. 2005. Semantic process-
ing using the hidden vector state model. Computer
speech & language, 19(1):85–106.

Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006.
Reducing the dimensionality of data with neural net-
works. science, 313(5786):504–507.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Minwoo Jeong and Gary Geunbae Lee. 2006. Jointly
predicting dialog act and named entity for spoken
language understanding. In Spoken Language Tech-
nology Workshop, 2006. IEEE, pages 66–69. IEEE.

Joo-Kyung Kim, Gokhan Tur, Asli Celikyilmaz, Bin
Cao, and Ye-Yi Wang. 2016. Intent detection using
semantically enriched word embeddings. In Spoken
Language Technology Workshop (SLT), 2016 IEEE,
pages 414–419. IEEE.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882.

Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Inter-
national Conference on Machine Learning, pages
1188–1196.

Changki Lee. 2017. Lstm-crf models for named en-
tity recognition. IEICE Transactions on Information
and Systems, 100(4):882–887.

Bing Liu and Ian Lane. 2016. Attention-based recur-
rent neural network models for joint intent detection
and slot filling. arXiv preprint arXiv:1609.01454.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research, 9(Nov):2579–2605.

Grégoire Mesnil, Yann Dauphin, Kaisheng Yao,
Yoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xi-
aodong He, Larry Heck, Gokhan Tur, Dong Yu, et al.
2015. Using recurrent neural networks for slot fill-
ing in spoken language understanding. IEEE/ACM
Transactions on Audio, Speech and Language Pro-
cessing (TASLP), 23(3):530–539.

Grégoire Mesnil, Xiaodong He, Li Deng, and Yoshua
Bengio. 2013. Investigation of recurrent-neural-
network architectures and learning methods for spo-
ken language understanding. In Interspeech, pages
3771–3775.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Jonas Mueller and Aditya Thyagarajan. 2016. Siamese
recurrent architectures for learning sentence similar-
ity. In AAAI, pages 2786–2792.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Anne Preller. 2014. From logical to distributional mod-
els. arXiv preprint arXiv:1412.8527.

Patti J Price. 1990. Evaluation of spoken language sys-
tems: The atis domain. In Speech and Natural Lan-
guage: Proceedings of a Workshop Held at Hidden
Valley, Pennsylvania, June 24-27, 1990.



140

Suman V Ravuri and Andreas Stolcke. 2015. Re-
current neural network and lstm models for lexical
utterance classification. In INTERSPEECH, pages
135–139.

Richard Schwartz, Scott Miller, David Stallard, and
John Makhoul. 1997. Hidden understanding models
for statistical sentence understanding. In Acoustics,
Speech, and Signal Processing, 1997. ICASSP-97.,
1997 IEEE International Conference on, volume 2,
pages 1479–1482. IEEE.

Puyang Xu and Ruhi Sarikaya. 2013. Convolutional
neural network based triangular crf for joint in-
tent detection and slot filling. In Automatic Speech
Recognition and Understanding (ASRU), 2013 IEEE
Workshop on, pages 78–83. IEEE.


