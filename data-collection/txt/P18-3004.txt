




































Recursive Neural Network Based Preordering for English-to-Japanese Machine Translation


Proceedings of ACL 2018, Student Research Workshop, pages 21–27
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

21

Recursive Neural Network Based Preordering for English-to-Japanese
Machine Translation

Yuki Kawara† Chenhui Chu‡
†Graduate School of Information Science and Technology, Osaka University

‡Institute for Datability Science, Osaka University
{kawara.yuki,arase}@ist.osaka-u.ac.jp,chu@ids.osaka-u.ac.jp

Yuki Arase†

Abstract

The word order between source and tar-
get languages significantly influences the
translation quality in machine translation.
Preordering can effectively address this
problem.　 Previous preordering methods
require a manual feature design, making
language dependent design costly. In this
paper, we propose a preordering method
with a recursive neural network that learns
features from raw inputs. Experiments
show that the proposed method achieves
comparable gain in translation quality to
the state-of-the-art method but without a
manual feature design.

1 Introduction

The word order between source and target
languages significantly influences the transla-
tion quality in statistical machine translation
(SMT) (Tillmann, 2004; Hayashi et al., 2013;
Nakagawa, 2015). Models that adjust orders
of translated phrases in decoding have been
proposed to solve this problem (Tillmann, 2004;
Koehn et al., 2005; Nagata et al., 2006). However,
such reordering models do not perform well for
long-distance reordering. In addition, their com-
putational costs are expensive. To address these
problems, preordering (Xia and McCord, 2004;
Wang et al., 2007; Xu et al., 2009; Isozaki et al.,
2010b; Gojun and Fraser, 2012; Nakagawa,
2015) and post-ordering (Goto et al., 2012,
2013; Hayashi et al., 2013) models have been
proposed. Preordering reorders source sentences
before translation, while post-ordering reorders
sentences translated without considering the word
order after translation. In particular, preorder-
ing effectively improves the translation quality
because it solves long-distance reordering and

computational complexity issues (Jehl et al.,
2014; Nakagawa, 2015).

Rule-based preordering methods either man-
ually create reordering rules (Wang et al.,
2007; Xu et al., 2009; Isozaki et al., 2010b;
Gojun and Fraser, 2012) or extract reordering
rules from a corpus (Xia and McCord, 2004;
Genzel, 2010). On the other hand, studies in
(Neubig et al., 2012; Lerner and Petrov, 2013;
Hoshino et al., 2015; Nakagawa, 2015) apply
machine learning to the preordering problem.
Hoshino et al. (2015) proposed a method that
learns whether child nodes should be swapped at
each node of a syntax tree. Neubig et al. (2012)
and Nakagawa (2015) proposed methods that con-
struct a binary tree and reordering simultaneously
from a source sentence. These methods require
a manual feature design for every language
pair, which makes language dependent design
costly. To overcome this challenge, methods
based on feed forward neural networks that do
not require a manual feature design have been
proposed (de Gispert et al., 2015; Botha et al.,
2017). However, these methods decide whether
to reorder child nodes without considering the
sub-trees, which contains important information
for reordering.

As a preordering method that is free of man-
ual feature design and makes use of information in
sub-trees, we propose a preordering method with
a recursive neural network (RvNN). RvNN cal-
culates reordering in a bottom-up manner (from
the leaf nodes to the root) on a source syntax
tree. Thus, preordering is performed consid-
ering the entire sub-trees. Specifically, RvNN
learns whether to reorder nodes of a syntax tree1

with a vector representation of sub-trees and
syntactic categories. We evaluate the proposed

1In this paper, we used binary syntax trees.



22

method for English-to-Japanese translations us-
ing both phrase-based SMT (PBSMT) and neu-
ral MT (NMT). The results confirm that the pro-
posed method achieves comparable translation
quality to the state-of-the-art preordering method
(Nakagawa, 2015) that requires a manual feature
design.

2 Preordering with a Recursive Neural
Network

We explain our design of the RvNN to conduct
preordering after describing how to obtain gold-
standard labels for preordering.

2.1 Gold-Standard Labels for Preordering
We created training data for preordering by label-
ing whether each node of the source-side syntax
tree has reordered child nodes against a target-
side sentence. The label is determined based
on Kendall’s τ (Kendall, 1938) as in (Nakagawa,
2015), which is calculated by Equation (1).

τ =
4
∑|y|−1

i=1

∑|y|
j=i+1 δ(yi < yj)

|y|(|y| − 1)
− 1,(1)

δ(x) =

{
1 (x is true),

0 (otherwise),

where y is a vector of target word indexes that are
aligned with source words. The value of Kendall’s
τ is in [−1, 1]. When it is 1, it means the se-
quence of y is in a complete ascending order,
i.e., target sentence has the same word order with
the source in terms of word alignment. At each
node, if Kendall’s τ increases by reordering child
nodes, an “Inverted” label is assigned; otherwise,
a “Straight” label, which means the child nodes
do not need to be reordered, is assigned. When
a source word of a child node does not have an
alignment, a “Straight” label is assigned.

2.2 Preordering Model
RvNN is constructed given a binary syntax tree. It
predicts the label determined in Section 2.1 at each
node. RvNN decides whether to reorder the child
nodes by considering the sub-tree. The vector of
the sub-tree is calculated in a bottom-up manner
from the leaf nodes. Figure 1 shows an example
of preordering of an English sentence “My parents
live in London.” At the VP node corresponding to
“live in London,” the vector of the node is calcu-
lated by Equation (2), considering its child nodes

VP
PP

My parents live in London

My parents liveinLondon

PRP$ NNS VBP IN NNP

NP

S

Figure 1: Preordering an English sentence “My
parents live in London” with RvNN (Nodes with a
horizontal line mean “Inverted”).

correspond to “live” and “in London.”

p = f([pl;pr]W + b), (2)

s = pWs + bs, (3)

where f is a rectifier, W ∈ R2λ×λ is a weight ma-
trix, pl and pr are vector representations of the left
and right child nodes, respectively. [·; ·] denotes
the concatenation of two vectors. Ws ∈ Rλ×2 is a
weight matrix for the output layer, and b,bs ∈ Rλ
are the biases. s ∈ R2 calculated by Equation (3)
is a weight vector for each label, which is fed into
a softmax function to calculate the probabilities of
the “Straight” and “Inverted” labels.

At a leaf node, a word embedding calculated by
Equations (4) and (5) is fed into Equation (2).

e = xWE , (4)

pe = f(eWl + bl), (5)

where x ∈ RN is a one-hot vector of an input
word with a vocabulary size of N , WE ∈ RN×λ
is an embedding matrix, and bl ∈ Rλ is the bias.
The loss function is the cross entropy defined by
Equation (6).

L(θ) = − 1
K

K∑
k=1

∑
n∈T

lnk log p(l
n
k ; θ), (6)

where θ is the parameters of the model, n is the
node of a syntax tree T , K is a mini batch size, and
lnk is the label of the n-th node in the k-th syntax
tree in the mini batch.

With the model using POS tags and syntactic
categories, we use Equation (7) instead of Equa-
tion (2).

p = f([pl;pr; et]Wt + bt), (7)



23

where et represents a vector of POS tags or syn-
tactic categories, Wt ∈ R3λ×λ is a weight matrix,
and bt ∈ Rλ is the bias. et is calculated in the
same manner as Equations (4) and (5), but the in-
put is a one-hot vector of the POS tags or syntactic
categories at each node. λ is tuned on a develop-
ment set, whose effects are investigated in Section
3.2.

3 Experiments

3.1 Settings

We conducted English-to-Japanese transla-
tion experiments using the ASPEC corpus
(Nakazawa et al., 2016). This corpus provides 3M
sentence pairs as training data, 1, 790 sentence
pairs as development data, and 1, 812 sentence
pairs as test data. We used Stanford CoreNLP2

for tokenization and POS tagging, Enju3 for
parsing of English, and MeCab4 for tokenization
of Japanese. For word alignment, we used
MGIZA.5 Source-to-target and target-to-source
word alignments were calculated using IBM
model 1 and hidden Markov model, and they were
combined with the intersection heuristic following
(Nakagawa, 2015).

We implemented our RvNN preordering model
with Chainer.6 The ASPEC corpus was created
using the sentence alignment method proposed in
(Utiyama and Isahara, 2007) and was sorted based
on the alignment confidence scores. In this pa-
per, we used 100k sentences sampled from the top
500k sentences as training data for preordering.
The vocabulary size N was set to 50k. We used
Adam (Kingma and Ba, 2015) with a weight de-
cay and gradient clipping for optimization. The
mini batch size K was set to 500.

We compared our model with the state-of-the-
art preordering method proposed in (Nakagawa,
2015), which is hereafter referred to as BTG.
We used its publicly available implementation,7

and trained it on the same 100k sentences as our
model.

We used the 1.8M source and target sentences
as training data for MT. We excluded part of the
sentence pairs whose lengths were longer than

2http://stanfordnlp.github.io/CoreNLP/
3http://www.nactem.ac.uk/enju/
4http://taku910.github.io/mecab/
5http://github.com/moses-smt/giza-pp
6http://chainer.org/
7http://github.com/google/topdown-btg-preordering

1 2 3 4 5

Epoch

6.8

6.9

7.0

7.1

7.2

7.3

7.4

7.5

7.6

T
ra

in
lo

ss

13.46

13.48

13.50

13.52

13.54

13.56

13.58

D
ev

lo
ss

train loss

dev loss

Figure 2: Learning curve of our preordering
model.

Node dimensions 100 200 500
w/o preordering 22.73

w/o tags and categories 24.63 24.95 25.02
w/ tags and categories 25.22 25.41 25.38

Table 1: BLEU scores with preordering by our
model and without preordering under different λ
settings (trained on a 500k subset of the training
data).

50 words or the source to target length ratio ex-
ceeded 9. For SMT, we used Moses.8 We trained
the 5-gram language model on the target side of
the training corpus with KenLM.9 Tuning was
performed by minimum error rate training (Och,
2003). We repeated tuning and testing of each
model 3 times and reported the average of scores.
For NMT, we used the attention-based encoder-
decoder model of (Luong et al., 2015) with 2-layer
LSTM implemented in OpenNMT.10 The sizes
of the vocabulary, word embedding, and hidden
layer were set to 50k, 500, and 500, respectively.
The batch size was set to 64, and the number of
epochs was set to 13. The translation quality was
evaluated using BLEU (Papineni et al., 2002) and
RIBES (Isozaki et al., 2010a) using the bootstrap
resampling method (Koehn, 2004) for the signifi-
cance test.

3.2 Results
Figure 2 shows the learning curve of our preorder-
ing model with λ = 200.11 Both the training and

8http://www.statmt.org/moses/
9http://github.com/kpu/kenlm

10http://opennmt.net/
11The learning curve behaves similarly for different λ val-

ues.



24

Avogadro  ’s  hypothesis  (  1811  )  contributed  to  the  development  in  since  then

Figure 4: Example of a syntax tree with a parse-error (the phrase “(1811)” was divided in two phrases by
mistake). Our preordering result was affected by such parse-errors. (Nodes with a horizontal line means
“Inverted”.)

PBSMT NMT
BLEU RIBES BLEU RIBES

w/o preordering 22.88 64.07 32.68 81.68
w/ BTG 29.51 77.20 28.91 79.58
w/ RvNN 29.16 76.39 29.01 79.63

Table 2: BLEU and RIBES scores on the test set.
(All models are trained on the entire training cor-
pus of 1.8M sentence pairs.) Numbers in bold in-
dicate the best systems and the systems that are
statistically insignificant at p < 0.05 from the best
systems.

−1 0 1
Kendall’s τ

0.00

0.05

0.10

0.15

0.20

pr
op

or
ti

on

tau of w/o preordering

tau of preordering with BTG

tau of preordering with RvNN

Figure 3: Distribution of Kendall’s τ in the train-
ing data without preordering, preordering by BTG,
and preordering by our RvNN.

the development losses decreased until 2 epochs.
However, the development loss started to increase
after 3 epochs. Therefore, the number of epochs
was set up to 5, and we chose the model with the
lowest development loss. The source sentences
in the translation evaluation were preordered with
this model.

Next, we investigated the effect of λ. Table
1 shows the BLEU scores with different λ val-
ues, as well as the BLEU score without preorder-

ing. In this experiment, PBSMT was trained with
a 500k subset of training data, and the distortion
limit was set to 6. Our RvNNs consistently out-
performed the plain PBSMT without preordering.
The BLEU score improved as λ increased when
only word embedding was considered. In addi-
tion, RvNNs involving POS tags and syntactic cat-
egories achieved even higher BLEU scores. This
result shows the effectiveness of POS tags and
syntactic categories in reordering. For these mod-
els, setting λ larger than 200 did not contribute to
the translation quality. Based on these, we further
evaluated the RvNN with POS tags and syntactic
categories where λ = 200.

Table 2 shows BLEU and RIBES scores of the
test set on PBSMT and NMT trained on the en-
tire training data of 1.8M sentence pairs. The dis-
tortion limit of SMT systems trained using pre-
ordered sentences by RvNN and BTG was set to 0,
while that without preordering was set to 6. Com-
pared to the plain PBSMT without preordering,
both BLEU and RIBES increased significantly
with preordering by RvNN and BTG. These scores
were comparable (statistically insignificant at p <
0.05) between RvNN and BTG,12 indicating that
the proposed method achieves a translation quality
comparable to BTG. In contrast to the case of PB-
SMT, NMT without preordering achieved a signif-
icantly higher BLEU score than NMT models with
preordering by RvNN and BTG. This is the same
phenomenon in the Chinese-to-Japanese transla-
tion experiment reported in (Sudoh and Nagata,
2016). We assume that one reason is the isola-
tion between preordering and NMT models, where
both models are trained using independent opti-
mization functions. In the future, we will investi-
gate this problem and consider a model that unifies

12The p-value for BLEU and RIBES were 0.068 and
0.226, respectively.



25

Preordered examples
Source sentence because of the embedding heterostructure, current leakage around the threshold was minimal.
BTG of the embedding heterostructure because, the threshold around current leakage minimal was.
RvNN embedding heterostructure the of because, around threshold the current leakage minimal was.

Translation examples by PBSMT
Reference 埋込みヘテロ構造のため、しきい値近くでの漏れ電流は非常に小さかった。

(embedding heterostructure of because, threshold around leakage very minimal.)
w/o preordering 埋込みヘテロ構造のため、漏れ電流のしきい値付近では最低であった。

(embedding heterostructure of because, leakage threshold around minimal.)
BTG　 の埋込みヘテロ構造のため、このしきい値付近での漏れ電流の最小であった。

(of embedding heterostructure of because, the threshold around leakage minimal.)
RvNN 埋込みヘテロ構造のため、周辺のしきい値の電流漏れは認められなかった。

(embedding heterostructure of because, around threshold leakage recognized not.)

Table 3: Example where preordering improves translation. (Literal translations are given in the paren-
thesis under the Japanese sentences.)

Preordered examples
Source sentence avogadro’s hypothesis (1811) contributed to the development in since then.
BTG avogadro’s hypothesis (1811) the then since in development to contributed .
RvNN avogadro’s hypothesis (1811 then since in to development the contributed).

Translation examples by PBSMT
Reference Avogadroの仮説 (1811)は，以後の発展に貢献した。

(Avogadro’s hypothesis (1811), since then development to contributed.)
w/o preordering Avogadroの仮説 (1811)の開発に貢献し以後である。

(Avogadro’s hypothesis (1811) development to contributed since then.)
BTG Avogadroの仮説 (1811)以後の発展に貢献した。

(Avogadro’s hypothesis (1811) since then development to contributed.)
RvNN Avogadroの仮説 (1811以降のこれらの開発に貢献した。　

(Avogadro’s hypothesis (1811 since then these development to contributed.)

Table 4: Example of a parse-error disturbed preordering in our method. (Literal translations are given in
the parenthesis under the Japanese sentences.)

preordering and translation in a single model.

Figure 3 shows the distribution of Kendall’s τ
in the original training data as well as the dis-
tributions after preordering by RvNN and BTG.
The ratio of high Kendall’s τ largely increased in
the case of RvNN, suggesting that the proposed
method learns preordering properly. Furthermore,
the ratio of high Kendall’s τ by RvNN is more than
that of BTG, implying that preordering by RvNN
is better than that by BTG.

We also manually investigated the preordering
and translation results. We found that our model
improved both. Table 3 shows a successful pre-
ordering and translation example on PBSMT. The
word order is notably different between source and
reference sentences. After preordering, the word
order between the source and reference sentences
became the same. Because RvNN depends on
parsing, sentences with a parse-error tended to fail
in preordering. For example, the phrase “(1811)”
in Figure 4 was divided in two phrases by mistake.
Consequently, preordering failed. Table 4 shows
preordering and translation examples for the sen-
tence in Figure 4. Compared to the translation

without preordering, the translation quality after
preordering was improved to deliver correct mean-
ing.

4 Conclusion

In this paper, we proposed a preordering method
without a manual feature design for MT. The ex-
periments confirmed that the proposed method
achieved a translation quality comparable to the
state-of-the-art preordering method that requires
a manual feature design. As a future work, we
plan to develop a model that jointly parses and pre-
orders a source sentence. In addition, we plan to
integrate preordering into the NMT model.

Acknowledgement

This work was supported by NTT communica-
tion science laboratories and Grant-in-Aid for Re-
search Activity Start-up #17H06822, JSPS.

References
Jan A. Botha, Emily Pitler, Ji Ma, Anton Bakalov,

Alex Salcianu, David Weiss, Ryan McDonald, and



26

Slav Petrov. 2017. Natural language processing
with small feed-forward networks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 2879–2885,
Copenhagen, Denmark.

Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of the International Con-
ference on Computational Linguistics (COLING),
pages 376–384, Beijing, China.

Adrià de Gispert, Gonzalo Iglesias, and Bill Byrne.
2015. Fast and accurate preordering for SMT us-
ing neural networks. In Proceedings of the Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT), pages 1012–
1017, Denver, Colorado.

Anita Gojun and Alexander Fraser. 2012. Determin-
ing the placement of German verbs in English–to–
German SMT. In Proceedings of the Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL), pages 726–735, Avi-
gnon, France.

Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012.
Post-ordering by parsing for Japanese-English sta-
tistical machine translation. In Proceedings of the
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 311–316, Jeju Is-
land, Korea.

Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2013.
Post-ordering by parsing with ITG for Japanese-
English statistical machine translation. ACM Trans-
actions on Asian Language Information Processing,
12(4):17:1–17:22.

Katsuhiko Hayashi, Katsuhito Sudoh, Hajime Tsukada,
Jun Suzuki, and Masaaki Nagata. 2013. Shift-
reduce word reordering for machine translation. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 1382–1386, Seattle, Washington, USA.

Sho Hoshino, Yusuke Miyao, Katsuhito Sudoh, Kat-
suhiko Hayashi, and Masaaki Nagata. 2015. Dis-
criminative preordering meets Kendall’s τ maxi-
mization. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
and International Joint Conference on Natural Lan-
guage Processing (ACL-IJCNLP), pages 139–144,
Beijing, China.

Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 944–952, Cambridge, USA.

Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple re-
ordering rule for SOV languages. In Proceedings

of the Workshop on Statistical Machine Translation
and MetricsMATR, pages 244–251, Uppsala, Swe-
den.

Laura Jehl, Adrià de Gispert, Mark Hopkins, and Bill
Byrne. 2014. Source-side preordering for transla-
tion using logistic regression and depth-first branch-
and-bound search. In Proceedings of the Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL), pages 239–248,
Gothenburg, Sweden.

M. G. Kendall. 1938. A new measure of rank correla-
tion. Biometrika, 30(1/2):81–93.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of the International Conference for Learning Repre-
sentations (ICLR), San Diego, USA.

Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 388–395,
Barcelona, Spain.

Philipp Koehn, Amittai Axelrod, Alexandra Birch-
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In Proceedings of the International Workshop on
Spoken Language Translation (IWSLT), pages 68–
75, Pittsburgh, USA.

Uri Lerner and Slav Petrov. 2013. Source-side clas-
sifier preordering for machine translation. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
513–523, Seattle, USA.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1412–1421, Lis-
bon, Portugal.

Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,
and Kazuteru Ohashi. 2006. A clustered global
phrase reordering model for statistical machine
translation. In Proceedings of the International
Conference on Computational Linguistics and An-
nual Meeting of the Association for Computational
Linguistics (COLING-ACL), pages 713–720, Syd-
ney, Australia.

Tetsuji Nakagawa. 2015. Efficient top-down BTG
parsing for machine translation preordering. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics and Interna-
tional Joint Conference on Natural Language Pro-
cessing (ACL-IJCNLP), pages 208–218, Beijing,
China.



27

Toshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi-
moto, Masao Utiyama, Eiichiro Sumita, Sadao
Kurohashi, and Hitoshi Isahara. 2016. ASPEC:
Asian scientific paper excerpt corpus. In Proceed-
ings of the International Conference on Language
Resources and Evaluation (LREC), pages 2204–
2208, Portorož, Slovenia.

Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 843–
853, Jeju Island, Korea.

Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 160–167, Sapporo,
Japan.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 311–318, Philadel-
phia, USA.

Katsuhito Sudoh and Masaaki Nagata. 2016. Chinese-
to-Japanese patent machine translation based on
syntactic pre-ordering for WAT 2016. In Proceed-
ings of the Workshop on Asian Translation (WAT),
pages 211–215, Osaka, Japan.

Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the
Association for Computational Linguistics (HLT-
NAACL), pages 101–104, Boston, Massachusetts,
USA.

Masao Utiyama and Hitoshi Isahara. 2007. A
Japanese-English patent parallel corpus. In Pro-
ceedings of the Machine Translation Summit XI,
pages 475–482, Copenhagen, Denmark.

Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 737–745, Prague, Czech
Republic.

Fei Xia and Michael McCord. 2004. Improving a
statistical MT system with automatically learned
rewrite patterns. In Proceedings of the International
Conference on Computational Linguistics (COL-
ING), pages 508–514, Geneva, Switzerland.

Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve

SMT for subject-object-verb languages. In Pro-
ceedings of the Human Language Technologies: An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL), pages 245–253, Boulder, USA.


