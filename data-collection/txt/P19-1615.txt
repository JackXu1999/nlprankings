



















































Careful Selection of Knowledge to Solve Open Book Question Answering


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6120–6129
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

6120

Careful Selection of Knowledge to solve Open Book Question Answering

Pratyay Banerjee∗ and Kuntal Kumar Pal∗ and Arindam Mitra∗ and Chitta Baral
Department of Computer Science, Arizona State University

pbanerj6,kkpal,amitra7,chitta@asu.edu

Abstract

Open book question answering is a type of nat-
ural language based QA (NLQA) where ques-
tions are expected to be answered with respect
to a given set of open book facts, and common
knowledge about a topic. Recently a challenge
involving such QA, OpenBookQA, has been
proposed. Unlike most other NLQA tasks
that focus on linguistic understanding, Open-
BookQA requires deeper reasoning involving
linguistic understanding as well as reasoning
with common knowledge. In this paper we
address QA with respect to the OpenBookQA
dataset and combine state of the art language
models with abductive information retrieval
(IR), information gain based re-ranking, pas-
sage selection and weighted scoring to achieve
72.0% accuracy, an 11.6% improvement over
the current state of the art.

1 Introduction

Natural language based question answering
(NLQA) not only involves linguistic understand-
ing, but often involves reasoning with various
kinds of knowledge. In recent years, many NLQA
datasets and challenges have been proposed, for
example, SQuAD (Rajpurkar et al., 2016), Trivi-
aQA (Joshi et al., 2017) and MultiRC (Khashabi
et al., 2018), and each of them have their own
focus, sometimes by design and other times
by virtue of their development methodology.
Many of these datasets and challenges try to
mimic human question answering settings. One
such setting is open book question answering
where humans are asked to answer questions in
a setup where they can refer to books and other
materials related to their questions. In such a
setting, the focus is not on memorization but, as
mentioned in Mihaylov et al. (2018), on “deeper
understanding of the materials and its application

∗ These authors contributed equally to this work.

to new situations (Jenkins, 1995; Landsberger,
1996).” In Mihaylov et al. (2018), they propose
the OpenBookQA dataset mimicking this setting.

Question: A tool used to identify the percent
chance of a trait being passed down has how
many squares ? (A) Two squares (B) Four
squares (C) Six squares (D) Eight squares
Extracted from OpenBook:
a punnett square is used to identify the percent
chance of a trait being passed down from a
parent to its offspring.
Retrieved Missing Knowledge:
Two squares is four.
The Punnett square is made up of 4 squares
and 2 of them are blue and 2 of them are
brown, this means you have a 50% chance of
having blue or brown eyes.

Table 1: An example of distracting retrieved knowl-
edge

The OpenBookQA dataset has a collection of
questions and four answer choices for each ques-
tion. The dataset comes with 1326 facts represent-
ing an open book. It is expected that answering
each question requires at least one of these facts.
In addition it requires common knowledge. To ob-
tain relevant common knowledge we use an IR
system (Clark et al., 2016) front end to a set of
knowledge rich sentences. Compared to reading
comprehension based QA (RCQA) setup where
the answers to a question is usually found in the
given small paragraph, in the OpenBookQA setup
the open book part is much larger (than a small
paragraph) and is not complete as additional com-
mon knowledge may be required. This leads to
multiple challenges. First, finding the relevant
facts in an open book (which is much bigger than
the small paragraphs in the RCQA setting) is a



6121

challenge. Then, finding the relevant common
knowledge using the IR front end is an even bigger
challenge, especially since standard IR approaches
can be misled by distractions. For example, Ta-
ble 1 shows a sample question from the Open-
BookQA dataset. We can see the retrieved miss-
ing knowledge contains words which overlap with
both answer options A and B. Introduction of such
knowledge sentences increases confusion for the
question answering model. Finally, reasoning in-
volving both facts from open book, and common
knowledge leads to multi-hop reasoning with re-
spect to natural language text, which is also a chal-
lenge.

We address the first two challenges and make
the following contributions in this paper: (a) We
improve on knowledge extraction from the Open-
Book present in the dataset. We use semantic tex-
tual similarity models that are trained with differ-
ent datasets for this task; (b) We propose natural
language abduction to generate queries for retriev-
ing missing knowledge; (c) We show how to use
Information Gain based Re-ranking to reduce dis-
tractions and remove redundant information; (d)
We provide an analysis of the dataset and the lim-
itations of BERT Large model for such a question
answering task.

The current best model on the leaderboard of
OpenBookQA is the BERT Large model (Devlin
et al., 2018). It has an accuracy of 60.4% and does
not use external knowledge. Our knowledge selec-
tion and retrieval techniques achieves an accuracy
of 72%, with a margin of 11.6% on the current
state of the art. We study how the accuracy of the
BERT Large model varies with varying number of
knowledge facts extracted from the OpenBook and
through IR.

2 Related Work

In recent years, several datasets have been pro-
posed for natural language question answer-
ing (Rajpurkar et al., 2016; Joshi et al., 2017;
Khashabi et al., 2018; Richardson et al., 2013;
Lai et al., 2017; Reddy et al., 2018; Choi et al.,
2018; Tafjord et al., 2018; Mitra et al., 2019) and
many attempts have been made to solve these chal-
lenges (Devlin et al., 2018; Vaswani et al., 2017;
Seo et al., 2016).

Among these, the closest to our work is the
work in (Devlin et al., 2018) which perform QA
using fine tuned language model and the works of

(Sun et al., 2018; Zhang et al., 2018) which per-
forms QA using external knowledge.

Related to our work for extracting missing
knowledge are the works of (Ni et al., 2018; Musa
et al., 2018; Khashabi et al., 2017) which respec-
tively generate a query either by extracting key
terms from a question and an answer option or
by classifying key terms or by Seq2Seq models to
generate key terms. In comparison, we generate
queries using the question, an answer option and
an extracted fact using natural language abduction.

The task of natural language abduction for nat-
ural language understanding has been studied for
a long time (Norvig, 1983, 1987; Hobbs, 2004;
Hobbs et al., 1993; Wilensky, 1983; Wilensky
et al., 2000; Charniak and Goldman, 1988, 1989).
However, such works transform the natural lan-
guage text to a logical form and then use formal
reasoning to perform the abduction. On the con-
trary, our system performs abduction over natural
language text without translating the texts to a log-
ical form.

3 Approach

Our approach involves six main modules: Hy-
pothesis Generation, OpenBook Knowledge Ex-
traction, Abductive Information Retrieval, Infor-
mation Gain based Re-ranking, Passage Selection
and Question Answering. A key aspect of our ap-
proach is to accurately hunt the needed knowledge
facts from the OpenBook knowledge corpus and
hunt missing common knowledge using IR. We
explain our approach in the example given in Ta-
ble 2.

Question: A red-tailed hawk is searching for
prey. It is most likely to swoop down on what?
(A) a gecko
Generated Hypothesis :
H : A red-tailed hawk is searching for prey. It
is most likely to swoop down on a gecko.
Retrieved Fact from OpenBook:
F : hawks eat lizards
Abduced Query to find missing knowledge:
K : gecko is lizard
Retrieved Missing Knowledge using IR:
K : Every gecko is a lizard.

Table 2: Our approach with an example for the correct
option

In Hypothesis Generation, our system generates



6122

Figure 1: Our approach

a hypothesis Hij for the ith question and jth an-
swer option, where j ∈ {1, 2, 3, 4}. In Open-
Book Knowledge Extraction, our system retrieves
appropriate knowledge Fij for a given hypothe-
sis Hij using semantic textual similarity, from the
OpenBook knowledge corpus F. In Abductive In-
formation Retrieval, our system abduces missing
knowledge from Hij and Fij. The system for-
mulates queries to perform IR to retrieve miss-
ing knowledge Kij. With the retrieved Kij, Fij,
Information Gain based Re-ranking and Passage
Selection our system creates a knowledge passage
Pij. In Question Answering, our system uses Pij
to answer the questions using a BERT Large based
MCQ model, similar to its use in solving SWAG
(Zellers et al., 2018).

3.1 Hypothesis Generation

Our system creates a hypothesis for each of the
questions and candidate answer options as part of
the data preparation phase as shown in the exam-
ple in Table 2. The questions in the OpenBookQA
dataset are either with wh word or are incomplete
statements. To create hypothesis statements for
questions with wh words, we use the rule-based
model of Demszky et al. (2018). For the rest of
the questions, we concatenate the questions with
each of the answers to produce the four hypothe-
ses. This has been done for all the training, test
and validation sets.

3.2 OpenBook Knowledge Extraction

To retrieve a small set of relevant knowledge facts
from the knowledge corpus F, a textual similarity
model is trained in a supervised fashion on two
different datasets and the results are compared.
We use the large-cased BERT (Devlin et al., 2018)
(BERT Large) as the textual similarity model.

3.2.1 BERT Model Trained on STS-B
We train it on the semantic textual similarity (STS-
B) data from the GLUE dataset (Wang et al.,

2018). The trained model is then used to retrieve
the top ten knowledge facts from corpus F based
on the STS-B scores. The STS-B scores range
from 0 to 5.0, with 0 being least similar.

3.2.2 BERT Model Trained on OpenBookQA
We generate the dataset using the gold Open-
BookQA facts from F for the train and validation
set provided. To prepare the train set, we first
find the similarity of the OpenBook F facts with
respect to each other using the BERT model
trained on STS-B dataset. We assign a score
5.0 for the gold F̂i fact for a hypothesis. We
then sample different facts from the OpenBook
and assign the STS-B similarity scores be-
tween the sampled fact and the gold fact F̂i as the
target score for that fact Fij and Hij. For example:

Hypothesis : Frilled sharks and angler fish live
far beneath the surface of the ocean, which is
why they are known as Deep sea animals.
Gold Fact : deep sea animals live deep in the
ocean : Score : 5.0
Sampled Facts :
coral lives in the ocean : Score : 3.4
a fish lives in water : Score : 2.8

We do this to ensure a balanced target score is
present for each hypothesis and fact. We use this
trained model to retrieve top ten relevant facts for
each Hij from the knowledge corpus F.

3.3 Natural Language Abduction and IR
To search for the missing knowledge, we need
to know what we are missing. We use “abduc-
tion” to figure that out. Abduction is a long
studied task in AI, where normally, both the ob-
servation (hypothesis) and the domain knowledge
(known fact) is represented in a formal language
from which a logical solver abduces possible ex-
planations (missing knowledge). However, in our
case, both the observation and the domain knowl-
edge are given as natural language sentences from
which we want to find out a possible missing
knowledge, which we will then hunt using IR.
For example, one of the hypothesis Hij is “A red-
tailed hawk is searching for prey. It is most likely
to swoop down on a gecko.”, and for which the
known fact Fij is “hawks eats lizards”. From this
we expect the output of the natural language ab-
duction system to be Kij or “gecko is a lizard”.
We will refer to this as “natural language abduc-
tion”.



6123

For natural language abduction, we propose
three models, compare them against a baseline
model and evaluate each on a downstream ques-
tion answering task. All the models ignore stop
words except the Seq2Seq model. We describe the
three models and a baseline model in the subse-
quent subsections.

3.3.1 Word Symmetric Difference Model
We design a simple heuristic based model defined
as below:

Kij = (Hij∪Fij)\(Hij∩Fij) ∀j ∈ {1, 2, 3, 4}

where i is the ith question, j is the jth option,Hij ,
Fij , Kij represents set of unique words of each in-
stance of hypothesis, facts retrieved from knowl-
edge corpus F and abduced missing knowledge of
validation and test data respectively.

3.3.2 Supervised Bag of Words Model
In the Supervised Bag of Words model, we select
words which satisfy the following condition:

P (wn ∈ Kij) > θ

where wn ∈ {Hij ∪ Fij}. To elaborate, we learn
the probability of a given word wn from the set
of words in Hij ∪ Fij belonging to the abduced
missing knowledge Kij . We select those words
which are above the threshold θ.

To learn this probability, we create a train-
ing and validation dataset where the words simi-
lar (cosine similarity using spaCy) (Honnibal and
Montani, 2017) to the words in the gold miss-
ing knowledge K̂i (provided in the dataset) are
labelled as positive class and all the other words
not present in K̂i but in Hij ∪ Fij are labelled as
negative class. Both classes are ensured to be bal-
anced. Finally, we train a binary classifier using
BERT Large with one additional feed forward net-
work for classification. We define value for the
threshold θ using the accuracy of the classifier on
validation set. 0.4 was selected as the threshold.

3.3.3 Copynet Seq2Seq Model
In the final approach, we used the copynet se-
quence to sequence model (Gu et al., 2016) to gen-
erate, instead of predict, the missing knowledge
given, the hypothesis H and knowledge fact from
the corpus F. The intuition behind using copynet
model is to make use of the copy mechanism to
generate essential yet precise (minimizing distrac-
tors) information which can help in answering the

question. We generate the training and validation
dataset using the gold K̂i as the target sentence,
but we replace out-of-vocabulary words from the
target with words similar (cosine similarity us-
ing spaCy) (Honnibal and Montani, 2017) to the
words present in Hij ∪Fij . Here, however, we did
not remove the stopwords. We choose one, out of
multiple generated knowledge based on our model
which provided maximum overlap score, given by

overlap score =

∑
i count((Ĥi ∪ Fi) ∩Ki)∑

i count(K̂i)

where i is the ith question, Ĥi being the set of
unique words of correct hypothesis, Fi being the
set of unique words from retrieved facts from
knowledge corpus F, Ki being the set of unique
words of predicted missing knowledge and K̂i be-
ing the set of unique words of the gold missing
knowledge .

3.3.4 Word Union Model
To see if abduction helps, we compare the above
models with a Word Union Model. To extract the
candidate words for missing knowledge, we used
the set of unique words from both the hypothesis
and OpenBook knowledge as candidate keywords.
The model can be formally represented with the
following:

Kij = (Hij ∪ Fij) ∀j ∈ {1, 2, 3, 4}

3.4 Information Gain based Re-ranking
In our experiments we observe that, BERT QA
model gives a higher score if similar sentences are
repeated, leading to wrong classification. Thus,
we introduce Information Gain based Re-ranking
to remove redundant information.

We use the same BERT Knowledge Extrac-
tion model Trained on OpenBookQA data (section
3.2.2), which is used for extraction of knowledge
facts from corpus F to do an initial ranking of the
retrieved missing knowledge K. The scores of this
knowledge extraction model is used as relevancy
score, rel. To extract the top ten missing knowl-
edge K, we define a redundancy score, redij , as
the maximum cosine similarity, sim, between the
previously selected missing knowledge, in the pre-
vious iterations till i, and the candidate missing
knowledge Kj . If the last selected missing knowl-
edge is Ki, then

redij(Kj) = max(redi−1,j(Kj), sim(Ki,Kj))



6124

rank score = (1− redi,j(Kj)) ∗ rel(Kj)

For missing knowledge selection, we first take
the missing knowledge with the highest rel score.
From the subsequent iteration, we compute the
redundancy score with the last selected missing
knowledge for each of the candidates and then
rank them using the updated rank score. We se-
lect the top ten missing knowledge for each Hij.

3.5 Question Answering
Once the OpenBook knowledge facts F and miss-
ing knowledge K have been extracted, we move
onto the task of answering the questions.

3.5.1 Question-Answering Model
We use BERT Large model for the question an-
swering task. For each question, we create a pas-
sage using the extracted facts and missing knowl-
edge and fine-tune the BERT Large model for the
QA task with one additional feed-forward layer for
classification. The passages for the train dataset
were prepared using the knowledge corpus facts,
F. We create a passage using the top N facts, sim-
ilar to the actual gold fact F̂i, for the train set. The
similarities were scored using the STS-B trained
model (section 3.2.1). The passages for the train-
ing dataset do not use the gold missing knowledge
K̂i provided in the dataset. For each of our ex-
periments, we use the same trained model, with
passages from different IR models.

The BERT Large model limits passage length to
be lesser than equal to 512. This restricts the size
of the passage. To be within the restrictions we
create a passage for each of the answer options,
and score for all answer options against each pas-
sage. We refer to this scoring as sum score, defined
as follows:

For each answer options, Aj , we create a pas-
sage Pj and score against each of the answer op-
tions Ai. To compute the final score for the an-
swer, we sum up each individual scores. If Q is
the question, the score for the answer is defined as

Pr(Q,Ai) =
4∑

j=1

score(Pj , Q,Ai)

where score is the classification score given by the
BERT Large model. The final answer is chosen
based on,

A = argmax
A

Pr(Q,Ai)

3.5.2 Passage Selection and Weighted Scoring
In the first round, we score each of the answer
options using a passage created from the selected
knowledge facts from corpus F. For each ques-
tion, we ignore the passages of the answer options
which are in the bottom two. We refer to this as
Passage Selection. In the second round, we score
for only those passages which are selected after
adding the missing knowledge K.

We assume that the correct answer has the high-
est score in each round. Therefore we multiply
the scores obtained after both rounds. We refer
to this as Weighted Scoring. We define the com-
bined passage selected scores and weighted scores
as follows :

Pr(F, Q,Ai) =

4∑
j=1

score(Pj , Q,Ai)

where Pj is the passage created from extracted
OpenBook knowledge, F. The top two pas-
sages were selected based on the scores of
Pr(F, Q,Ai).

Pr(F ∪K, Q,Ai) =
4∑

k=1

δ ∗ score(Pk, Q,Ai)

where δ = 1 for the top two scores and δ = 0 for
the rest. Pk is the passage created using both the
facts and missing knowledge. The final weighted
score is :

wPr(Q,Ai) = Pr(F, Q,Ai)∗Pr(F∪K, Q,Ai)

The answer is chosen based on the top weighted
scores as below:

A = argmax
A

wPr(Q,Ai)

4 Experiments

4.1 Dataset and Experimental Setup
The dataset of OpenBookQA contains 4957 ques-
tions in the train set and 500 multiple choice ques-
tions in validation and test respectively. We train
a BERT Large based QA model using the top ten
knowledge facts from the corpus F, as a passage
for both training and validation set. We select the
model which gives the best score for the validation
set. The same model is used to score the validation
and test set with different passages derived from
different methods of Abductive IR. The best Ab-
ductive IR model, the number of facts from F and
K are selected from the best validation scores for
the QA task.



6125

F Any Passage Correct Passage Accuracy(%)
N TF-IDF Trained STS-B TF-IDF Trained STS-B TF-IDF Trained STS-B
1 228 258 288 196 229 234 52.6 63.6 59.2
2 294 324 347 264 293 304 57.4 66.2 60.6
3 324 358 368 290 328 337 59.2 65.0 60.2
5 350 391 398 319 370 366 61.6 65.4 62.8
7 356 411 411 328 390 384 59.4 65.2 61.8

10 373 423 420 354 405 396 60.4 65.2 59.4

Table 3: Compares (a) The number of correct facts that appears across any four passages (b) The number of correct
facts that appears in the passage of the correct hypothesis (c) The accuracy for TF-IDF, BERT model trained on
STS-B dataset and BERT model trained on OpenBook dataset. N is the number of facts considered.

4.2 OpenBook Knowledge Extraction

Question: .. they decide the best way to save
money is ? (A) to quit eating lunch out (B)
to make more phone calls (C) to buy less with
monopoly money (D) to have lunch with friends
Knowledge extraction trained with STS-B:
using less resources usually causes money to be
saved
a disperser disperses
each season occurs once per year
Knowledge extraction trained with Open-
BookQA:
using less resources usually causes money to be
saved
decreasing something negative has a positive
impact on a thing
conserving resources has a positive impact on
the environment

Table 3 shows a comparative study of our three
approaches for OpenBook knowledge extraction.
We show, the number of correct OpenBook knowl-
edge extracted for all of the four answer options
using the three approaches TF-IDF, BERT model
trained on STS-B data and BERT model Trained
on OpenBook data. Apart from that, we also show
the count of the number of facts present precisely
across the correct answer options. It can be seen
that the Precision@N for the BERT model trained
on OpenBook data is better than the other models
as N increases.

The above example presents the facts retrieved
from BERT model trained on OpenBook which
are more relevant than the facts retrieved from
BERT model trained on STS-B. Both the mod-
els were able to find the most relevant fact, but
the other facts for STS-B model introduce more
distractors and have lesser relevance. The impact
of this is visible from the accuracy scores for the

QA task in Table 3 . The best performance of the
BERT QA model can be seen to be 66.2% using
only OpenBook facts.

4.3 Abductive Information Retrieval

We evaluate the abductive IR techniques at differ-
ent values for number of facts from F and number
of missing knowledge K extracted using IR. Fig-
ure 2 shows the accuracy against different com-
binations of F and K , for all four techniques of
IR prior to Information gain based Re-ranking. In
general, we noticed that the trained models per-
formed poorly compared to the baselines. The
Word Symmetric Difference model performs bet-
ter, indicating abductive IR helps. The poor per-
formance of the trained models can be attributed
to the challenge of learning abductive inference.

For the above example it can be seen, the
pre-reranking facts are relevant to the question but
contribute very less considering the knowledge
facts retrieved from the corpus F and the correct
answer. Figure 3 shows the impact of Information
gain based Re-ranking. Removal of redundant
data allows the scope of more relevant informa-
tion being present in the Top N retrieved missing
knowledge K.

Question: A red-tailed hawk is searching for
prey. It is most likely to swoop down on what?
(A) an eagle (B) a cow (C) a gecko (D) a deer
Fact from F : hawks eats lizards
Pre-Reranking K :
red-tail hawk in their search for prey
Red-tailed hawks soar over the prairie and
woodlands in search of prey.
Post-Reranking K:
Geckos - only vocal lizards.
Every gecko is a lizard.



6126

Figure 2: Accuracy v/s Number of facts from F - num-
ber of facts from K, without Information Gain based
Re-ranking for 3 abductive IR models and Word Union
model. 1

Figure 3: Accuracy v/s Number of facts from F - num-
ber of facts from K, with Information Gain based Re-
ranking for 3 abductive IR models and Word Union
model. 1

4.4 Question Answering

Table 4 shows the incremental improvement on
the baselines after inclusion of carefully selected
knowledge.

Passage Selection and Weighted Scoring are
used to overcome the challenge of boosted pre-
diction scores due to cascading effect of errors in
each stage.

Question: What eat plants? (A) leopards (B)
eagles (C) owls (D) robin
Appropriate extracted Fact from F :
some birds eat plants
Wrong Extracted Fact from F :
a salamander eats insects
Wrong Retrieved Missing Knowledge:
Leopard geckos eat mostly insects

For the example shown above, the wrong an-
swer leopards had very low score with only the

Solver Accuracy (%)
Leaderboard
Guess All (“random”) 25.0
Plausible Answer Detector 49.6
Odd-one-out Solver 50.2
Question Match 50.2
Reading Strategies 55.8
Model - BERT-Large (SOTA)
Only Question (No KB) 60.4
Model - BERT-Large (Our)
F - TF-IDF 61.6
F - Trained KE 66.2
F ∪K 70.0
F ∪K with Weighted Scoring 70.4
F ∪K with Passage Selection 70.8
F ∪K with Both 72.0
Oracle - BERT-Large
F gold 74.4
F ∪K gold 92.0

Table 4: Test Set Comparison of Different Compo-
nents. Current state of the art (SOTA) is the Only Ques-
tion model. K is retrieved from Symmetric Difference
Model. KE refers to Knowledge Extraction.

facts extracted from knowledge corpus F. But in-
troduction of missing knowledge from the wrong
fact from F boosts its scores, leading to wrong
prediction. Passage selection helps in removal of
such options and Weighted Scoring gives prefer-
ence to those answer options whose scores are rel-
atively high before and after inclusion of missing
knowledge.

5 Analysis & Discussion

5.1 Model Analysis
BERT Question Answering model: BERT per-
forms well on this task, but is prone to distractions.
Repetition of information leads to boosted predic-
tion scores. BERT performs well for lookup based
QA, as in RCQA tasks like SQuAD. But this poses
a challenge for Open Domain QA, as the extracted
knowledge enables lookup for all answer options,
leading to an adversarial setting for lookup based
QA. This model is able to find the correct answer,
even under the adversarial setting, which is shown
by the performance of the sum score to select the
answer after passage selection.

Symmetric Difference Model This model im-
proves on the baseline Word Union model by 1-

1No Passage Selection and Weighted Scoring.



6127

2%. The improvement is dwarfed because of in-
appropriate domain knowledge from F being used
for abduction. The intersection between the inap-
propriate domain knowledge and the answer hy-
pothesis is ∅, which leads to queries which are
exactly same as the Word Union model.

Supervised learned models The supervised
learned models for abduction under-perform. The
Bag of Words and the Seq2Seq models fail to
extract keywords for many F − H pairs, some-
times missing the keywords from the answers. The
Seq2Seq model sometimes extracts the exact miss-
ing knowledge, for example it generates “some
birds is robin” or “lizard is gecko”. This shows
there is promise in this approach and the poor per-
formance can be attributed to insufficient train data
size, which was 4957 only. A fact verification
model might improve the accuracy of the super-
vised learned models. But, for many questions,
it fails to extract proper keywords, copying just a
part of the question or the knowledge fact.

5.2 Error Analysis
Other than errors due to distractions and failed IR,
which were around 85% of the total errors, the er-
rors seen are of four broad categories.

Temporal Reasoning: In the example 2 shown
below, even though both the options can be
considered as night, the fact that 2:00 AM is
more suitable for the bats than 6:00 PM makes it
difficult to reason. Such issues accounted for 5%
of the errors.

Question: Owls are likely to hunt at?
(A) 3:00 PM (B) 2:00 AM (C) 6:00 PM (D)
7:00 AM

Negation: In the example shown below, a
model is needed which handles negations specif-
ically to reject incorrect options. Such issues
accounted for 1% of the errors.

Question: Which of the following is not an in-
put in photosynthesis? (A) sunlight (B) oxygen
(C) water (D) carbon dioxide

Conjunctive Reasoning: In the example as
shown below, each answer options are partially
correct as the word “ bear” is present. Thus
a model has to learn whether all parts of the
answer are true or not, i.e Conjunctive Reasoning.
Logically, all answers are correct, as we can see

2Predictions are in italics, Correct answers are in Bold.

an “or”, but option (A) makes more sense. Such
issues accounted for 1% of the errors.

Question: Some berries may be eaten by (A)
a bear or person (B) a bear or shark (C) a bear
or lion (D) a bear or wolf

Qualitative Reasoning: In the example shown
below, each answer options would stop a car but
option (D) is more suitable since it will stop the
car quicker. A deeper qualitative reasoning is
needed to reject incorrect options. Such issues
accounted for 8% of the errors.

Question: Which of these would stop a car
quicker? (A) a wheel with wet brake pads (B)
a wheel without brake pads (C) a wheel with
worn brake pads (D) a wheel with dry brake
pads

6 Conclusion

In this work, we have pushed the current state
of the art for the OpenBookQA task using sim-
ple techniques and careful selection of knowledge.
We have provided two new ways of performing
knowledge extraction over a knowledge base for
QA and evaluated three ways to perform abductive
inference over natural language. All techniques
are shown to improve on the performance of the
final task of QA, but there is still a long way to
reach human performance.

We analyzed the performance of various com-
ponents of our QA system. For the natural lan-
guage abduction, the heuristic technique performs
better than the supervised techniques. Our anal-
ysis also shows the limitations of BERT based
MCQ models, the challenge of learning natu-
ral language abductive inference and the multiple
types of reasoning required for an OpenBookQA
task. Nevertheless, our overall system improves
on the state of the art by 11.6%.

7 Acknowledgement

We thank NSF for the grant 1816039 and DARPA
for partially supporting this research.

References
Eugene Charniak and Robert Goldman. 1988. A logic

for semantic interpretation. In Proceedings of the
26th annual meeting on Association for Compu-



6128

tational Linguistics, pages 87–94. Association for
Computational Linguistics.

Eugene Charniak and Robert P Goldman. 1989. A
semantics for probabilistic quantifier-free first-order
languages, with particular application to story un-
derstanding. In IJCAI, volume 89, pages 1074–
1079. Citeseer.

Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-
tau Yih, Yejin Choi, Percy Liang, and Luke Zettle-
moyer. 2018. Quac: Question answering in context.
arXiv preprint arXiv:1808.07036.

Peter Clark, Oren Etzioni, Tushar Khot, Ashish Sab-
harwal, Oyvind Tafjord, Peter Turney, and Daniel
Khashabi. 2016. Combining retrieval, statistics, and
inference to answer elementary science questions.
In Thirtieth AAAI Conference on Artificial Intelli-
gence.

Dorottya Demszky, Kelvin Guu, and Percy Liang.
2018. Transforming question answering datasets
into natural language inference datasets. arXiv
preprint arXiv:1809.02922.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 1631–1640. Association for Computational
Linguistics.

Jerry R Hobbs. 2004. Abduction in natural language
understanding. Handbook of pragmatics, pages
724–741.

Jerry R Hobbs, Mark E Stickel, Douglas E Appelt, and
Paul Martin. 1993. Interpretation as abduction. Ar-
tificial intelligence, 63(1-2):69–142.

Matthew Honnibal and Ines Montani. 2017. spacy 2:
Natural language understanding with bloom embed-
dings, convolutional neural networks and incremen-
tal parsing. To appear.

Tony Jenkins. 1995. Open Book Assessment in Com-
puting Degree Programmes. Citeseer.

Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1601–1611. Associa-
tion for Computational Linguistics.

Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,
Shyam Upadhyay, and Dan Roth. 2018. Looking

beyond the surface: A challenge set for reading com-
prehension over multiple sentences. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long Papers), volume 1, pages 252–262.

Daniel Khashabi, Tushar Khot, Ashish Sabharwal, and
Dan Roth. 2017. Learning what is essential in ques-
tions. In Proceedings of the 21st Conference on
Computational Natural Language Learning (CoNLL
2017), pages 80–89.

Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard Hovy. 2017. Race: Large-scale reading
comprehension dataset from examinations. In Pro-
ceedings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing, pages 785–
794. Association for Computational Linguistics.

J Landsberger. 1996. Study guides and strategies.

Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Sabharwal. 2018. Can a suit of armor conduct elec-
tricity? a new dataset for open book question an-
swering. In EMNLP.

Arindam Mitra, Peter Clark, Oyvind Tafjord, and
Chitta Baral. 2019. Declarative question answering
over knowledge bases containing natural language
text with answer set programming.

Ryan Musa, Xiaoyan Wang, Achille Fokoue, Nicholas
Mattei, Maria Chang, Pavan Kapanipathi, Bassem
Makni, Kartik Talamadupula, and Michael Wit-
brock. 2018. Answering science exam questions
using query rewriting with background knowledge.
arXiv preprint arXiv:1809.05726.

Jianmo Ni, Chenguang Zhu, Weizhu Chen, and Ju-
lian McAuley. 2018. Learning to attend on es-
sential terms: An enhanced retriever-reader model
for scientific question answering. arXiv preprint
arXiv:1808.09492.

Peter Norvig. 1983. Frame activated inferences in a
story understanding program. In IJCAI, pages 624–
626.

Peter Norvig. 1987. Inference in text understanding.
In AAAI, pages 561–565.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2383–2392. Asso-
ciation for Computational Linguistics.

Siva Reddy, Danqi Chen, and Christopher D Manning.
2018. Coqa: A conversational question answering
challenge. arXiv preprint arXiv:1808.07042.

Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. Mctest: A challenge dataset for
the open-domain machine comprehension of text.

https://doi.org/10.18653/v1/P16-1154
https://doi.org/10.18653/v1/P16-1154
https://doi.org/10.18653/v1/P17-1147
https://doi.org/10.18653/v1/P17-1147
https://doi.org/10.18653/v1/P17-1147
https://doi.org/10.18653/v1/D17-1082
https://doi.org/10.18653/v1/D17-1082
Http://www.studygs.net/tsttak7.htm.
https://doi.org/10.18653/v1/D16-1264
https://doi.org/10.18653/v1/D16-1264


6129

In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
193–203.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2016. Bidirectional attention
flow for machine comprehension. arXiv preprint
arXiv:1611.01603.

Kai Sun, Dian Yu, Dong Yu, and Claire Cardie. 2018.
Improving machine reading comprehension with
general reading strategies. CoRR, abs/1810.13441.

Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau
Yih, and Ashish Sabharwal. 2018. Quarel: A dataset
and models for answering questions about qualita-
tive relationships. arXiv preprint arXiv:1811.08048.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Alex Wang, Amapreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: A multi-task benchmark and analysis platform
for natural language understanding. arXiv preprint
arXiv:1804.07461.

Robert Wilensky. 1983. Planning and understanding:
A computational approach to human reasoning.

Robert Wilensky, David N Chin, Marc Luria, James
Martin, James Mayfield, and Dekai Wu. 2000. The
berkeley unix consultant project. In Intelligent Help
Systems for UNIX, pages 49–94. Springer.

Rowan Zellers, Yonatan Bisk, Roy Schwartz, and
Yejin Choi. 2018. Swag: A large-scale adversarial
dataset for grounded commonsense inference. arXiv
preprint arXiv:1808.05326.

Yuyu Zhang, Hanjun Dai, Kamil Toraman, and
Le Song. 2018. Kgˆ 2: Learning to reason science
exam questions with contextual knowledge graph
embeddings. arXiv preprint arXiv:1805.12393.


