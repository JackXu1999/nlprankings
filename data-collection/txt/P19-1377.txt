
























































A Spreading Activation Framework for Tracking Conceptual Complexity of Texts


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3878–3887
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3878

A Spreading Activation Framework
for Tracking Conceptual Complexity of Texts

Ioana Hulpuş1, Sanja Štajner2 and Heiner Stuckenschmidt1
1Data and Web Science Group, University of Mannheim, Germany

2 Symanto Research, Nürnberg, Germany
{ioana,heiner}@informatik.uni-mannheim.de

sanja.stajner@symanto.net

Abstract

We propose an unsupervised approach for as-
sessing conceptual complexity of texts, based
on spreading activation. Using DBpedia
knowledge graph as a proxy to long-term
memory, mentioned concepts become acti-
vated and trigger further activation as the text
is sequentially traversed. Drawing inspira-
tion from psycholinguistic theories of reading
comprehension, we model memory processes
such as semantic priming, sentence wrap-up,
and forgetting. We show that our models cap-
ture various aspects of conceptual text com-
plexity and significantly outperform current
state of the art.

1 Introduction

Reading comprehension has long been linked to
processes over semantic memory, such as seman-
tic priming through spreading activation (Ander-
son, 1981; Collins and Loftus, 1975; Neely, 1991;
Gulan and Valerjev, 2010). While psycholinguis-
tic literature abounds in research and demonstra-
tion of such processes (Just and Carpenter, 1980;
Kutas and Hillyard, 1984; Carroll and Slowiaczek,
1986), there is a gap in understanding if they can
be modeled in an automated way for capturing
the cognitive load required by texts. At the same
time, the recent advances in the publication of en-
cyclopedic knowledge graphs provide an unprece-
dented opportunity for modeling human knowl-
edge at scale.

We focus on conceptual complexity which, as
opposed to lexical and syntactic complexity (Vaj-
jala and Meurers, 2014; Ambati et al., 2016), has
received very little attention so far. Conceptual
complexity accounts for the background knowl-
edge necessary to understand mentioned concepts
as well as the implicit connections that the reader
has to access between the mentioned concepts in

order to fully understand a text. It plays an im-
portant role in making texts accessible to children,
non-native speakers, as well as people with low lit-
eracy levels or intellectual disabilities (Arfé et al.,
2017). Apart from being one of the main fac-
tors for understanding the story, conceptual com-
plexity also influences the readers’ interest in the
text: readers who lack relevant background knowl-
edge have difficulties in understanding conceptu-
ally complex texts (Arfé et al., 2017; Benjamin,
2012), while high-knowledge readers need some
obstacles (more conceptual complexity) to main-
tain their interest (Arfé et al., 2017; Benjamin,
2012; Kalyuga et al., 2003). Therefore, correctly
estimating conceptual complexity of a text, and of-
fering a reader a text of an appropriate cognitive
load, is of utmost importance for: (1) ensuring cor-
rect understanding of a text; (2) maintaining the
readers’ interest; and (3) promoting deeper-level
processing and enhancing the readers knowledge.

In this paper, we are building on top of the psy-
cholinguistic findings that words are recognized
faster if preceded by words related in meaning (se-
mantic priming) (Gulan and Valerjev, 2010), and
we adopt spreading activation theory as one of the
main theories that tries to explain how priming oc-
curs. Specifically, we introduce a framework that
considers sequential text reading and models two
simultaneous processes: (i) a spreading activation
process that runs over long-term memory (approx-
imated by the knowledge graph), activates con-
cepts and transfers them to working memory, and
(ii) a process that tracks concepts and their acti-
vation in working memory and subjects them to
forgetting. We use the activation values of con-
cepts in working memory at different points in the
text in order to assess the amount of priming trig-
gered by the text. Our hypothesis is that the higher
these activation values (more priming), the lower
the conceptual complexity.



3879

We validate our framework through extensive
experiments, and show that the models we propose
on top of it outperform state-of-the-art measures
that aim to predict conceptual complexity.

2 Related Work

In spite of its real-world importance, automatic as-
sessment of conceptual complexity of texts has not
received much attention so far. A few approaches
have been proposed, but most of them are either
not freely available, or have not been tested on
large corpora (see (Benjamin, 2012) for the ex-
tensive list of approaches and their shortcomings).
DeLite (vor der Brück et al., 2008) software and
Coh-Metrix (Graesser et al., 2004), for example,
do not have any features related to conceptual clar-
ity, which would measure ambiguity, vagueness,
and abstractness of a concept, or the level of nec-
essary background knowledge. From this perspec-
tive, the work of Štajner and Hulpuş (2018) is the
only work that attempts to automatically measure
conceptual complexity of texts. They propose a
supervised method using a set of graph-based fea-
tures over DBpedia knowledge graph. In our ex-
periments, we use these features as state-of-the-art
for comparison with our approach.

In the cognitive science domain, the work most
related to ours is in the direction of capturing
knowledge in cognitive architectures (Lieto et al.,
2018). Salvucci (2014) proposes the use of DBpe-
dia as a source of declarative knowledge to be inte-
grated with the ACT-R cognitive architecture (An-
derson and Lebiere, 1998). They implement a
very basic spreading activation model for scoring
facts in the knowledge base for answering natural
language, factual questions such as “What is the
population of Philadelphia?”. Several other ap-
proaches have been proposed for extending ACT-
R with knowledge and reasoning (Ball et al., 2004;
Oltramari and Lebiere, 2012), but none of them
aim to assess the complexity of texts.

With respect to spreading activation, it has
long been adopted as a methodology for informa-
tion retrieval (Crestani, 1997), used for document
summarization (Nastase, 2008), document simi-
larity (Syed et al., 2008), as well as cross-domain
recommendation (Heitmann and Hayes, 2016),
among others. Nevertheless, there is no prior at-
tempt to apply spreading activation to the recently
developed encyclopedic knowledge graphs with
the purpose of modeling reading comprehension.

This paper fills in this gap and shows that pair-
ing spreading activation with other working mem-
ory processes (such as forgetting) can result in
models that accurately assess conceptual complex-
ity of a document.

3 Framework for Unsupervised
Assessment of Conceptual Complexity

Our framework tracks the activation of concepts
in working memory during reading processes. We
consider an encyclopedic knowledge graph, DB-
pedia1, as a proxy to long-term memory over
which spreading activation processes run and
bring concepts into the working memory. Text
is processed sequentially, and each mention of a
DBpedia concept triggers a tide of spreading acti-
vation over the DBpedia knowledge graph. Once
brought into working memory, the activated con-
cepts are subject to a forgetting process which de-
cays their activation as the text is being read. At
the same time, concepts in working memory accu-
mulate more activation as they are repeated, or as
related concepts are mentioned.

We track the cumulative activation (CA) of the
mentioned concepts at different points in time: at
encounter (AE), at the end of sentences (AEoS)
and at the end of paragraphs (AEoP). We use
these values to estimate the conceptual complex-
ity of texts, under the overarching hypothesis that
a higher activation of text concepts in working
memory indicates more accessible texts.

3.1 Spreading Activation over DBpedia
For the spreading activation (SA) process, we
exploit the graph structure of DBpedia. Each
DBpedia concept is a node in the knowledge
graph (KG). Each triple <s, p, o> (short
from <subject, predicate, object>) whose sub-
ject and object are DBpedia concepts, becomes a
typed relation (or typed edge), that we denote with
s

p−→ o. This way, the knowledge base is repre-
sented as a graph KG = (V,E, T, τ), where V
is the set of concepts, E is the set of directed re-
lations between the concepts and τ : E → T as-
signs a type in T to each edge in E. We denote by
ρ(x) ⊂ E the set of all relations of node x ∈ V ,
and by nr(x) ∈ V the neighbour of x through re-
lation r ∈ E. We denote by A(p)(c) the amount
of activation node c has after pulse p, by A(p)out(c)
the amount of activation node c outputs at pulse

1http://dbpedia.org



3880

p and A(p)in (c) the amount of activation that flows
into node c at pulse p.

The core idea common to all SA models in lit-
erature is that concepts become active and fire,
spreading their activation to their neighbors in
KG, who in turn fire and activate their neighbors
and so on, until preset termination conditions are
met. Therefore, the SA process consists of multi-
ple iterations called pulses.

In our model, a SA process is triggered when-
ever a concept is mentioned in the text (the seed
concept), by setting its activation to 1.0, and that
of all other nodes in V to 0.0. Formally, the initial
conditions are A(0)(seed) = 1.0 and A(0)(i) =
0.0, ∀i ∈ V, i 6= seed. Then at pulse 1, the seed
fires and the SA process starts.

Formally, a SA model must define three func-
tions: the output function, the input function
and an activation function (Berthold et al., 2009;
Crestani, 1997). In the following, we describe how
we define these functions in order to study concep-
tual complexity of text.

The output function defines how much activa-
tion is output by a concept at pulse p+1, given its
activation at current pulse p. To define this func-
tion, we use a distance decay parameter α, which
decays the activation going out of each node expo-
nentially with respect to p. Furthermore, our out-
put function limits the concepts that fire to those
concepts whose activation surpasses a given firing
threshold β for the first time. Hence, α and β con-
trol the number of activated concepts and the in-
tensity of their activation, providing potential for
personalization according to memory capacity of
the target audience.

A
(p+1)
out (c) = α · fβ(A(p)(c)); (1)

where fβ(x) = x if x ≥ β; 0 otherwise.
The input function aggregates the amount of

activation that flows into a node (called target
node) given the activations flowing out of their
neighbours (called source nodes). Drawing inspi-
ration from spreading activation theory in cogni-
tive science (Collins and Quillian, 1969; Collins
and Loftus, 1975), we define accessibility of a tar-
get concept given a source concept based on how
strong is the semantic relation between them, as
well as by how familiar the target concept is to
the reader. We define the strength of the seman-
tic relation between two nodes as its exclusivity,
introduced by Hulpuş et al (2015) and proven by

Zhu and Iglesias (2017) to be particularly effec-
tive for computing semantic relatedness. Regard-
ing the user’s familiarity with the target concept,
in absence of user data we approximate it by the
popularity of the target concept computed as the
normalized node degree as pop(c) = log(D(c))log(|V |−1) ,
where D(c) denotes the number of neighbors of
concept c.

Formally, given the relation s
p−→ o, the acces-

sibility scores of its endpoints s and o are com-
puted as shown in Formula 2.

acc(o, s
p−→ o) = excl(s p−→ o) · pop(o)

acc(s, s
p−→ o) = excl(s p−→ o) · pop(s)

(2)

Consequently, although the edges of the KG
are directed, activation can flow in both directions
over the same edge. For example, given the rela-
tion Accordion isA−→ Musical instrument, the
mention of Accordion will activate the concept
Musical instrument, and vice-versa.

We can therefore generalize our notation, so that
given a concept c and one of its relations (incom-
ing or outgoing), r, c’s accessibility over the rela-
tion r is defined as accr(c) = excl(r) · pop(c).

To make sure that the total amount of activation
received from a concept by its neighbours, equals
the amount of activation it outputs, we normalize
the accessibility value as in Formula 3.

accr(c) =
accr(c)∑

r′∈ρ(c)
accr′(nr′ ◦ nr(c))

; (3)

Finally, the input function is defined in For-
mula 4.

A
(p+1)
in (c) =

∑
r∈ρ(c)

A
(p+1)
out (nr(c)) · accr(c) (4)

The activation function is the function that
computes the activation of a concept after the
pulse p + 1, given its activation at time p and its
incoming activation at p+ 1. Formally,

A(p+1)(c) = A(p)(c) +A
(p+1)
in (c) (5)

In order to avoid cycles in which concepts keep
activating each other, we constrain the process so
that a concept can only fire in the first pulse after
its activation overpasses β.



3881

After firing, concepts become burned and al-
though during the future pulses they can receive
activation, they cannot fire again. When there
are no more unburnt concepts with an activation
higher than β in the graph, the SA process finishes.
The activations resulted after this process are the
activations that the nodes have after the last pulse,
denoted in the following by SA(·).

3.2 The Working Memory Model
At the beginning of a text, the working memory
(WM) is considered empty. As the text is be-
ing read, the concepts activated through SA are
brought into WM with an activation computed as
a function of their SA activation φ(SA(c)).

The WM keeps track of all activated concepts
and aggregates the activation that they achieve
from different SA processes. Furthermore, a for-
getting process takes place on top of WM, that is
triggered at every word. We therefore use words
as time unit in our WM model. The forgetting
process decays the activations of all concepts in
WM with a preset decay factor at every encoun-
tered word (γw), and additionally at every end of
sentence (γs) and at every end of paragraph (γp).
Therefore, given the words at indices i and j,
(i < j), in paragraphs pi and pj (pi ≤ pj) and
sentences si and sj (si ≤ sj) respectively, we de-
note the decay that occurs in the interval of time
between the two words as γi,j and compute it as in
Equation 6.

γi,j = γw
j−i · γssj−si · γppj−pi . (6)

We define cumulative activation, CA(i)(c) of a
concept c as its activation in the WM at time of
reading word i. It is defined recursively as it con-
sists of the cumulative activation that the concept
has at time i − 1 and that has been subject to for-
getting, together with the activation φ(SA(i)(c))
that it receives as a result from the SA process that
takes place at time i (see Equation 7).

CA(i)(c) = γi−1,iCA
(i−1)(c) + φ(SA(i)(c))

=
i∑

k=0

γk,iφ(SA
(k)(c))

(7)
We illustrate this process with an example (Ta-

ble 1) which shows, after the given text having
been linked to DBpedia, the seed concepts cor-
responding to each mention and the set of text

concepts activated by the seed concepts. Fig-
ure 1 shows the evolution of concepts’ activation
in WM, e.g. the concept db:Shelf (storage) be-
comes active when it is mentioned, with an acti-
vation of 1.0. We compute the activations in Fig-
ure 1 by defining the function φ as a constant func-
tion in which all concepts that become active in
the SA process receive an activation of 1 in WM.
We denote this function as φ1. In this example,
we use values 0.85 and 0.7 for word (γw) and sen-
tence decay (γs), respectively. The forgetting pro-
cess is also illustrated as, unless reactivated, the
CA scores decrease with every token, and the de-
crease is stronger after each sentence. The figure
also shows how the concepts’ CAs get adjusted
every time they are activated by mentioned con-
cepts. For example, at the time “instruments” is
mentioned, the concepts db:Musical instrument,
and db:Accordion increase their existing CAs, and
db:Band (rock and pop) becomes active in WM.

3.3 Estimating Conceptual Text Complexity

One of the hypotheses that we want to test is
that our framework can capture the forward prim-
ing phenomenon. We therefore hypothesize that
in simpler texts, target concepts already exist in
WM before they are explicitly mentioned in the
text. In other words, the higher CA(c) at the en-
counter of concept c, the easier it is to comprehend
the concept c and connect it to its context. Con-
sidering concept ci is the concept encountered in
text at time i, its activation at encounter (AE) is
CA(i−1)(ci), hence its CA at the time of the word
that precedes it.

AE(ci) = CA
(i−1)(ci) (8)

Furthermore, the psycholinguistic theory of
backward semantic priming states that concepts
can actually receive activation from concepts men-
tioned afterwards, in a way explaining their previ-
ous occurrence. To account for this, concepts keep
accumulating CA in WM after they are mentioned.
More over, in the psycholinguistic literature the
end of sentences have been proven to trigger a
wrapping up process (Just and Carpenter, 1980),
in which the information of the sentence is being
reviewed. Based on these insights, we hypothe-
size that in simpler texts, the concepts exhibit a
higher CA at the end of the sentences / paragraphs
they occur in, than in more conceptually complex
texts. Formally, given a sentence s, and denoting

db:Shelf_(storage)
db:Musical_instrument
db:Accordion
db:Band_(rock_and_pop)


3882

Mention Seed Concept Activated text concepts
shelves db:Shelf (storage) db:Shelf (storage)
accordions db:Accordion db:Accordion, db:Musical instrument
instruments db:Musical instrument db:Musical instrument, db:Accordion, db:Band (rock and pop)
pictures db:Image db:Image
Irish db:Irish people db:Irish people, db:The Pogues
band db:Band (rock and pop) db:Band (rock and pop), db:Musical instrument, db:The Pogues
The Pogues db:The Pogues db:The Pogues, db:Accordion, db:Irish people, db:Musical instrument, db:Band (rock and pop)
wall db:Wall db:Wall

Table 1: Example of text linked to DBpedia, together with the text concepts activated through spreading activation.
(Text: The 2 shelves hold a selection of accordions and other instruments for sale. Pictures of the Irish band The
Pogues hang on the wall.). db: stands for the DBpedia namespace http://dbpedia.org/resource/

0

0.5

1

1.5

2

2.5

http://dbpedia.org/page/Shelf_(storage) http://dbpedia.org/page/Accordion http://dbpedia.org/page/Musical_instrument

http://dbpedia.org/page/Image http://dbpedia.org/page/Irish_people http://dbpedia.org/page/Band_(rock_and_pop)

http://dbpedia.org/page/The_Pogues http://dbpedia.org/page/Wall

Figure 1: The change of CA in WM for the concepts in Table 1 as the text is sequentially traversed.

the index of its last word as eos(s), we can define
the sentence wrap-up activation (AEoS) of any
concept c that is mentioned in s as in Formula 9.
The paragraph wrap-up activation(AEoP ) is de-
fined similarly.

AEoSs(c) = CA
(eos(s))(c);

AEoPp(c) = CA
(eop(p))(c);

(9)

Therefore, each concept mention in the text pro-
duces three CA scores: activation at encounter
(AE), activation at the end of the sentence it oc-
curs in (AEoS), and activation at the end of the
paragraph it occurs in (AEoP ). Table 2 presents
the scores of the defined CAs for the example in
Table 1. Scores for AE are seen in Figure 1 on
the word just before the target mention. Scores for
AEoS are seen on the last word of the correspond-
ing sentence, and scores for AEoP are seen at the
end of the text.

For assessing the conceptual complexity of a
given document D that has been linked to the
knowledge base KG, resulting in m concept men-
tions, we propose to compute the activations of the
mentioned concepts and take the inverse of their

average as in Equation 10.

con comp(D) =
m∑m

i=1 activation(ci)
(10)

where ci is the concept that mention i is linked
to, and activation(ci) is a placeholder for any
linear combination of the AE(ci), AEoS(ci) and
AEoP (ci).

4 Experiments

4.1 Dataset
As ground truth, we use Newsela corpus which
provides English news text on five complexity lev-
els, the original story, and four manually simpli-
fied versions, gradually simplified by trained hu-
man editors under high quality control (Xu et al.,
2015). As the target audience are children and sec-
ond language learners, and texts are intended to
maintain readers’ interest, texts are not only sim-
plified at a linguistic level but also at a cognitive
level.

We report our experiments on 200 randomly
sampled original texts from the English Newsela
corpus, and for each of them, their four corre-
sponding simplifications resulting in 1000 docu-
ments. All texts have been linked to DBpedia us-
ing KanDis (Hulpuş et al., 2015).

http://dbpedia.org/resource/


3883

Mention shelves accordions instruments pictures Irish band The Pogues wall
Concept db:Shelf (storage) db:Accordion db:Musical instrument db:Image db:Irish people db:Band (rock and pop) db:The Pogues db:Wall
AE 0 0 0.72 0 0 0.26 1.57 0
AEoS 0.20 1.17 1.17 0.20 0.84 0.98 1.21 1
AEoP 0.02 0.66 1.04 0.20 0.84 0.98 1.21 1

Table 2: AE, AEoS and AEoP scores for the mentions from the example in Table 1.

4.2 SA Model Settings
Settings of the output function. To explore
our output function, we study how α (the graph
decay) and β (the firing threshold) influence
the performance of the models. We imple-
mented models for α taking values from the set
{0.25, 0.5, 0.75} and β taking values from the set
{0.0025, 0.005, 0.0075, 0.01}, excluding the α =
0.75 and β = 0.0025 combination because the
activated sub-graph becomes computationally too
expensive.

Settings of the input function. To explore our
input function, we implemented four accessibil-
ity computation schemes that we name according
to the exclusivity and popularity factors (Excl-
Pop) being used or not:

(No-No): acc(o, s p→ o) = acc(s, s p→ o) = 1.0;

(Yes-No): acc(o, s p→ o) = acc(s, s p→ o) =
excl(s

p→ o);

(No-Yes): acc(o, s p→ o) = pop(o) and
acc(s, s

p→ o) = pop(s);

(Yes-Yes): following Equation 2.

We transmit the intuition behind the
input/output-function settings by reporting
the average number of activated KG nodes per
SA process over a sample of 1579 SA processes
(triggered by 100 texts: 20 titles on all 5 levels).
The results, shown in Table 3, indicate that
exclusivity dramatically reduces the number of
activated concepts. This is because exclusiv-
ity gives preference to less common relations,
directing the activation in the graph to the few
concepts that are strongly related to the seed. At
the same time, the use of popularity in the absence
of exclusivity has the opposite effect because
popularity gives preference to the nodes with high
degrees. When both exclusivity and popularity
are used, only the high degree concepts that have
very specific relations to the seed are activated.

With respect to the output function parameters,
as expected, more concepts are activated as α de-
creases and as β increases.

Output function Input function settings (Excl-Pop)
β α Yes-Yes Yes-No No-Yes No-No
0.0025 0.5 1,245 4,448 135,381 115,935
0.0025 0.25 1,106 2,977 95,142 75,491
0.005 0.75 1,003 3,428 108,086 90,082
0.005 0.5 1,002 2,576 85,895 68,318
0.005 0.25 979 2,190 51,190 34,921
0.0075 0.75 935 2,424 79,858 65,182
0.0075 0.5 917 2,111 60,840 45,175
0.0075 0.25 911 1,893 30,561 19,652
0.01 0.75 903 2,016 61,280 47,664
0.01 0.5 897 1,807 45,065 31,839
0.01 0.25 897 1,535 20,864 13,916

Table 3: Number of activated nodes in different SA set-
tings.

4.3 WM Settings
We experimented with multiple definitions for the
φ function, and report values for two definitions,
φA and φ1 as shown below:

φA(SA(c)) =

{
SA(c) if SA(c) < 1.0
pop(c) if SA(c) = 1.0

φ1(SA(c)) = 1 if SA(c) > 0.0

φA uses the activations computed in the SA pro-
cess, except for the seed concept where it uses its
popularity score. This ensures that concepts men-
tioned in text become active in WM according to
their popularity. φA is therefore sensitive to the ac-
tual SA scores, and to the popularity of mentioned
concepts. On the contrary, φ1 is only sensitive to
changes in the set of activated concepts.

We investigated six parameter combinations for
the values of the token, sentence and paragraph de-
cay factors (< γw, γs, γp >):

no forgetting: <1, 1, 1>;

no paragraph transfer: <1, 1, 0> - there is no
forgetting within a paragraph, but complete
forgetting takes place between paragraphs;

no sentence transfer : <1, 0, 0> - there is no
forgetting within a sentence, but complete
forgetting takes place between sentences;

weak decay: <0.995, 0.9, 0.8> - the CA of a
concept drops by one order of magnitude ev-
ery 6 paragraphs of original texts (assuming



3884

20 words per sentence, and 4 sentences per
paragraph) and every 8 paragraphs for sim-
ple texts (assuming 12 words per sentence, 4
sentences per paragraph)2;

medium decay: <0.85, 0.7, 0.5> - the CA de-
cays one order of magnitude every 2 original
sentences, and every 3 simple sentences re-
spectively;

strong decay: <0.75, 0.5, 0.25> - the CA decays
one order of magnitude with every original
sentence, and with every 2 simple sentences.

Given the described SA and WM models, we
implemented a total of 264 models in our frame-
work.

4.4 State-of-the-art Measures
We compare our system to the graph-based met-
rics proposed by Štajner and Hulpuş (2018), as
well as the baseline (ent/men) that computes the
number of unique concepts per mention. For com-
pleteness, we briefly describe these features:

PageRank represents the average of the PageR-
ank scores computed over the knowledge
graph for all the mentioned concepts in the
text;

PairDistSent and PairDistPar compute the av-
erage shortest path distance over the knowl-
edge graph between all pairs of concepts
mentioned in a sentence or paragraph respec-
tively, averaged over all sentences or para-
graphs respectively;

PairSemRelSent and PairSemRelPar are simi-
lar to the previous two measures, but in-
stead of shortest path distances, they com-
pute the exclusivity-based semantic related-
ness (Hulpuş et al., 2015);

DensitySent and DensityPar compute the aver-
age density of the subgraphs extracted such
that they connect all the pairs of concepts
mentioned in the sentences or paragraphs re-
spectively, by paths of at most 4 hops;

ConnCompSent and ConnCompPar are com-
puted using the same subgraphs as those ex-
tracted in the previous measures, but comput-

2The numbers of 20 words per normal sentence and 12
words per simple sentence are taken from published statistics
on the dataset we use (Xu et al., 2015).

ing the number of connected components av-
eraged over sentences or paragraphs respec-
tively.

All state-of-the-art features were computed over
the same knowledge-graph (DBpedia) and using
the same entity linker, KanDis (Hulpuş et al.,
2015). Therefore, there are no biases stemming
from the choice of those two in the comparison of
our models with the state of the art.

4.5 Tasks and Evaluation Metrics
For each of our models we calculated 4 scores,
by plugging into Equation 10 the values for AE,
AEoS, AEoP , and All (the sum of previous
three). Based on these scores, we test our models
on two tasks: (i) Ranking five versions of the same
news story according to their conceptual complex-
ity; (ii) Identifying the conceptually simpler of two
versions of the same news story.

In the ranking task, we compare our mod-
els’ ranking of the five versions to the ground
truth ranking, by computing their Kendall’s Tau-
b (Kendall, 1948), which calculates the difference
between the number of concordant and discordant
pairs, out of the number of all pairs, while also
handling ties. Generally, Kendall Tau values range
between −1 and 1, with 1 being obtained when
there is perfect agreement between the compared
rankings, −1 when one ranking is the reverse of
the other, and 0 when the two compared rankings
are independent. Hence for a random ranking we
would expect Kendall Tau-b results close to 0.

In the second task, we calculate accuracy as the
percentage of text pairs in which the simpler ver-
sion was predicted as less conceptually complex
by our models. In this task, the scores can range
from 0 to 1, with a value of 0.5 for random picks.

5 Results and Discussion

We present our results starting with the WM set-
tings because variations in these settings lead to
the highest variations in the results.

5.1 Impact of WM Settings
Table 4 presents the average Kendall Tau-b scores
for the six WM decay settings, four types of acti-
vation scores and two φ functions.

The first conclusion that stands out from this ta-
ble is that there is a certain sweet spot when the
WM decay is strong or medium, in which AEoS
performs substantially better than all other scores



3885

WM decay φ
1 φA

AE AEoS AEoP All AE AEoS AEoP All
strong decay -.15 .74 .34 .58 -.06 .76 .40 .64
medium decay -.08 .77 .36 .60 .03 .79 .41 .66
weak decay -.08 -.11 -.15 -.12 .16 .19 .10 .17
no forgetting -.08 -.11 -.08 -.09 .16 .15 .19 .17
no paragraph transfer .01 -.19 -.01 -.06 .19 .12 .20 .17
no sentence transfer -.44 -.46 NA NA -.28 -.05 NA NA

Table 4: Kendall Tau-b scores averaged over the 200
titles for all models with corresponding reading decay.

φ α β
exclusivity-popularity

y-y y-n n-y n-n
φA any any .80 .80 .79 .79

φ1

0.50 0.0025 .81 .74 .70 .72
0.25 0.0025 .81 .75 .72 .74
0.75 0.005 .82 .76 .72 .73
0.50 0.005 .82 .76 .74 .75
0.25 0.005 .82 .76 .75 .76
0.75 0.0075 .82 .77 .75 .76
0.50 0.0075 .82 .77 .76 .76
0.25 0.0075 .82 .78 .76 .76
0.75 0.01 .82 .77 .76 .76
0.50 0.01 .82 .78 .77 .76
0.25 0.01 .82 .79 .77 .77

Table 5: Kendall Tau-b scores of the AEoS measures
computed with WM medium decay setting averaged
over all 200 titles.

in all other settings. If the WM decay is either too
strong (no sentence transfer) or too weak (no for-
getting, weak decay and no paragraph transfer), all
models perform poorly.

The second finding that is revealed by this table
is that AE achieves very poor results across all
WM settings. On the one hand, this indicates that
our experiments are not able to confirm the for-
ward semantic priming hypothesis. On the other
hand, given the good results of AEoS, our exper-
iments confirm the backwards priming hypothesis
and sentence wrap-up.

5.2 Impact of the SA Settings
Table 5 shows the influence of the graph settings
parameters in the ranking task. We focus on the
best performing settings from Table 4, which mea-
sures AEoS using WM medium decay.

Input function. Among all the SA settings, the
definition of accessibility has the most influence.
Our results show that the use of both exclusivity
and popularity leads toAEoS scores that best cor-
relate with our ground truth complexity levels.

Output function. The choice of α and β pa-
rameters makes no noticeable difference for φA,
while it makes a statistically significant differ-
ence3 for φ1. In the latter case, the best results are

3Statistically significant difference refers to a 0.001 level

obtained when α = 0.25 and β = 0.01, which cor-
responds to the setting which activates the smallest
DBpedia subgraph (Table 3).

A somehow unexpected finding that has a great
impact on SA parameter selection is that the big-
ger the activated DBpedia subgraph, the worse the
results. This indicates that allowing the activa-
tion to spread through more of KG, might result
in more noise. Consequently, controlling the flow
of activation through relation and concept rele-
vance scoring dramatically reduces the activated
network, while improving the results.

5.3 Results on Pairwise Text Comparison

The pairwise comparison task provides insight on
the models’ ability to discriminate between two
versions of the same news story. The results of
the models with a medium WM decay and with
the combination of α and β at the opposite sides
of the proposed spectrum are shown in Table 6 for
both tasks, together with the results of the state of
the art and the baseline (ent/men).

The first observation is that our models distin-
guish almost perfectly between very complex and
very simple versions of the same text (0−4, 1−4,
0−3). Also, generally they significantly outper-
form the baseline and state-of-the-art measures.
However, our models perform close to random
on distinguishing between the two most complex
versions of the same title (0−1), the only setting
in which they are outperformed by some state-
of-the-art features and the baseline. Manual in-
spection indicates that the simplification that takes
place between the two levels mostly involves sen-
tence /paragraph splitting (syntactical simplifica-
tion) which, as a side effect can have the decrease
in the number of connected components, favour-
ing ConnCompPar and ConnCompSent measures.

The results of the best model using φ1 surpass
the results of the best model using φA, particu-
larly for the close level pairs (1−2, 2−3 and 3−4),
which are generally harder to distinguish (paired t-
test at 0.001 level of significance). This indicates
that the fact that a concept is activated by SA is
more relevant than the actual amount of activa-
tion, particularly for capturing subtle differences
in texts.

of significance using paired t-test, whenever mentioned.



3886

Model Level pairs
Type α β Exc. Pop. 0-1 0-2 0-3 0-4 1-2 1-3 1-4 2-3 2-4 3-4 Kendall Tau-b
φA any any yes yes .64 .88 .97 1 .88 .97 .99 .87 .95 .80 .80
φ1 0.5 0.0025 no no .54 .83 .94 .95 .86 .93 .96 .83 .92 .82 .72
φ1 0.25 0.01 no no .58 .84 .95 .99 .85 .94 .98 .86 .95 .86 .77
φ1 0.5 0.0025 no yes .52 .83 .94 .95 .86 .92 .96 .83 .91 .82 .70
φ1 0.25 0.01 no yes .58 .86 .95 .99 .87 .94 .98 .87 .93 .87 .77
φ1 0.5 0.0025 yes no .54 .85 .94 .97 .87 .95 .98 .85 .91 .82 .74
φ1 0.25 0.01 yes no .58 .88 .96 1 .86 .96 .99 .88 .95 .85 .79
φ1 0.5 0.0025 yes yes .59 .89 .97 1 .90 .97 .99 .89 .97 .87 .81
φ1 0.25 0.01 yes yes .61 .89 .98 1 .92 .97 .99 .90 .97 .88 .82

(Štajner and Hulpuş, 2018)

ent/men .67 .76 .83 .82 .71 .80 .79 .71 .72 .54 .47
PageRank .50 .53 .57 .57 .62 .58 .55 .53 .55 .57 .12
PairDistSent .58 .64 .65 .67 .58 .66 .64 .63 .59 .50 .23
PairSemRelSent .56 .62 .68 .77 .56 .69 .75 .63 .71 .55 .24
DensitySent .61 .69 .68 .72 .60 .63 .66 .51 .56 .58 .25
ConnCompSent .67 .71 .83 .83 .60 .72 .74 .68 .73 .56 .41
PairDistPar .58 .70 .77 .84 .60 .76 .80 .67 .78 .60 .42
PairSemRelPar .60 .74 .87 .88 .70 .83 .88 .77 .83 .71 .56
DensityPar .59 .64 .57 .64 .57 .56 .62 .56 .62 .56 .19
ConnCompPar .69 .64 .74 .76 .61 .66 .62 .65 .62 .52 .22
SeedDegree .53 .51 .59 .55 .58 .55 .50 .53 .54 .58 .12

Table 6: Accuracies of the pairwise comparison task, and the Kendall Tau-b correlations for the AEoS scores of
our models for medium WM decay, and for the state-of-the-art measures. Level 0 is the original text, while level 4
is the simplest version. Any signifies that the reported results were the same for all parameter choices.

6 Conclusion

We introduced a framework for tracking the con-
ceptual complexity of texts during sequential read-
ing, by mimicking human memory processes
such as forward and backward semantic priming
through spreading activation, sentence wrap-up
and forgetting, and implemented a series of unsu-
pervised models within it.

Our results confirmed the hypothesis that texts
are simpler when the concepts therein are highly
active at the end of their corresponding sentences.
From the SA perspective, we showed that mea-
sures that account for relevance of relations and
nodes make a significant impact, and that targeted
search in the close proximity of the seeds performs
best. Finally, our models strongly outperform the
state-of-the-art measures in automatic assessment
of conceptual complexity.

References
Bharat Ram Ambati, Siva Reddy, and Mark Steedman.

2016. Assessing relative sentence complexity using
an incremental ccg parser. In Proceedings of the
2016 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1051–1057.

John Robert Anderson and Christian Lebiere. 1998.
The atomic components of thought. Lawrence Erl-
baum Associates.

Jonathan Anderson. 1981. Analysing the readability of
English and non-English texts in the classroom with

Lix. In Proceedings of the Annual Meeting of the
Australian Reading Association.

Barbara Arfé, Lucia Mason, and Inmaculada Fajardo.
2017. Simplifying informational text structure for
struggling readers. Reading and Writing.

Jerry Ball, Stuart Rodgers, and Kevin Gluck. 2004. In-
tegrating act-r and cyc in a large-scale model of lan-
guage comprehension for use in intelligent agents.
In AAAI Workshop, pages 19–25.

Rebekah George Benjamin. 2012. Reconstructing
Readability: Recent Developments and Recommen-
dations in the Analysis of Text Difficulty. Educa-
tional Psychology Review, 24(1):63–88.

Michael R Berthold, Ulrik Brandes, Tobias Kötter,
Martin Mader, Uwe Nagel, and Kilian Thiel. 2009.
Pure spreading activation is pointless. In The 17th
ACM SIGSPATIAL International Conference on Ad-
vances in Geographic Information Systems-GIS’09,
pages 1915–1918.

Tim vor der Brück, Sven Hartrumpf, and Hermann
Helbig. 2008. A readability checker with super-
vised learning using deep indicators. Informatica,
32(4):429–435.

Patrick Carroll and Maria L. Slowiaczek. 1986. Con-
straints on semantic priming in reading: A fixation
time analysis. Memory & Cognition, 14(6):509–
522.

Allan M. Collins and Elizabeth F. Loftus. 1975. A
spreading activation theory of semantic processing.
Psychological Review, 82:407–428.

Allan M. Collins and M. Ross Quillian. 1969. Re-
trieval time from semantic memory. Journal of Ver-
bal Learning and Verbal Behavior, 8(2):240 – 247.

https://doi.org/10.18653/v1/N16-1120
https://doi.org/10.18653/v1/N16-1120
https://doi.org/10.1016/S1364-6613(98)01250-9
https://doi.org/10.1007/s11145-017-9785-6
https://doi.org/10.1007/s11145-017-9785-6
https://doi.org/10.1007/s10648-011-9181-8
https://doi.org/10.1007/s10648-011-9181-8
https://doi.org/10.1007/s10648-011-9181-8
https://doi.org/10.1145/1645953.1646264
https://doi.org/10.3758/BF03202522
https://doi.org/10.3758/BF03202522
https://doi.org/10.3758/BF03202522
https://doi.org/10.1037/0033-295X.82.6.407
https://doi.org/10.1037/0033-295X.82.6.407
https://doi.org/10.1016/S0022-5371(69)80069-1
https://doi.org/10.1016/S0022-5371(69)80069-1


3887

Fabio Crestani. 1997. Application of Spreading Acti-
vation Techniques in Information Retrieval. Artifi-
cial Intelligence Review, pages 453–482.

Arthur C. Graesser, Danielle S. McNamara, Max M.
Louwerse, and Zhiqiang Cai. 2004. Coh-Metrix:
Analysis of text on cohesion and language. Behav-
ior Research Methods, Instruments, & Computers,
36(2):193–202.

Tanja Gulan and Pavle Valerjev. 2010. Semantic and
related types of priming as a context in word recog-
nition. Review of psychology, 17(1):53–58.

Benjamin Heitmann and Conor Hayes. 2016. Semstim:
Exploiting knowledge graphs for cross-domain rec-
ommendation. In 2016 IEEE 16th International
Conference on Data Mining Workshops (ICDMW),
pages 999–1006.

Ioana Hulpuş, Narumol Prangnawarat, and Conor
Hayes. 2015. Path-based semantic relatedness on
linked data and its use to word and entity disam-
biguation. In The Semantic Web - ISWC 2015, pages
442–457, Cham. Springer International Publishing.

Marcel Adam Just and Patricia A. Carpenter. 1980. A
theory of reading:from eye fixations to comprehen-
sion. Psychological Review, 87(4).

Slava Kalyuga, Paul Ayres, Paul Chandler, and John
Sweller. 2003. The expertise reversal effect. Jour-
nal of Educational Psychology, 38:23–31.

Maurice G. Kendall. 1948. Rank correlation methods.
Griffin, London.

Marta Kutas and Steven A. Hillyard. 1984. Brain po-
tentials during reading reflect word expectancy and
semantic association. Nature, 307(161).

Antonio Lieto, Christian Lebiere, and Alessandro
Oltramari. 2018. The knowledge level in cognitive
architectures: Current limitations and possible de-
velopments. Cognitive Systems Research, 48:39 –
55. Cognitive Architectures for Artificial Minds.

Vivi Nastase. 2008. Topic-driven multi-document
summarization with encyclopedic knowledge and
spreading activation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’08, pages 763–772, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

James H. Neely. 1991. Semantic priming effects in
visual word recognition: A selective review of cur-
rent findings and theories. In D. Besner and G. W.
Humphreys, editors, Basic processes in reading: Vi-
sual word recognition, pages 265–335. Lawrence
Erlbaum Associates, Hillsdale.

Alessandro Oltramari and Christian Lebiere. 2012.
Pursuing artificial general intelligence by leverag-
ing the knowledge capabilities of act-r. In Artificial
General Intelligence, pages 199–208, Berlin, Hei-
delberg. Springer Berlin Heidelberg.

Dario D. Salvucci. 2014. Endowing a cognitive archi-
tecture with world knowledge. In Proceedings of
the Annual Meeting of the Cognitive Science Soci-
ety, volume 36.

Sanja Štajner and Ioana Hulpuş. 2018. Automatic
assessment of conceptual text complexity using
knowledge graphs. In Proceedings of the 27th In-
ternational Conference on Computational Linguis-
tics, pages 318–330. Association for Computational
Linguistics.

Zareen Syed, Tim Finin, and Anupam Joshi. 2008.
Wikipedia as an ontology for describing documents.
In Proceedings of the Second International Confer-
ence on Weblogs and Social Media. AAAI Press.

Sowmya Vajjala and Detmar Meurers. 2014. Assessing
the relative reading level of sentence pairs for text
simplification. In Proceedings of the EACL 2014,
pages 288–297.

Wei Xu, Chris Callison-Burch, and Courtney Napoles.
2015. Problems in Current Text Simplification Re-
search: New Data Can Help. Transactions of the As-
sociaton for Computational Linguistics, 3:283–297.

Ganggao Zhu and Carlos. A. Iglesias. 2017. Com-
puting semantic similarity of concepts in knowledge
graphs. IEEE Transactions on Knowledge and Data
Engineering, 29(1):72–85.

https://doi.org/10.1023/A:1006569829653
https://doi.org/10.1023/A:1006569829653
https://doi.org/10.3758/BF03195564
https://doi.org/10.3758/BF03195564
https://doi.org/10.1109/ICDMW.2016.0145
https://doi.org/10.1109/ICDMW.2016.0145
https://doi.org/10.1109/ICDMW.2016.0145
https://doi.org/10.1007/978-3-319-25007-6_26
https://doi.org/10.1007/978-3-319-25007-6_26
https://doi.org/10.1007/978-3-319-25007-6_26
https://doi.org/10.1037/0033-295X.87.4.329
https://doi.org/10.1037/0033-295X.87.4.329
https://doi.org/10.1037/0033-295X.87.4.329
https://doi.org/10.1207/S15326985EP3801_4
http://gso.gbv.de/DB=2.1/CMD?ACT=SRCHA&SRT=YOP&IKT=1016&TRM=ppn+18489199X&sourceid=fbw_bibsonomy
https://doi.org/10.1038/307161a0
https://doi.org/10.1038/307161a0
https://doi.org/10.1038/307161a0
https://doi.org/10.1016/j.cogsys.2017.05.001
https://doi.org/10.1016/j.cogsys.2017.05.001
https://doi.org/10.1016/j.cogsys.2017.05.001
http://dl.acm.org/citation.cfm?id=1613715.1613812
http://dl.acm.org/citation.cfm?id=1613715.1613812
http://dl.acm.org/citation.cfm?id=1613715.1613812
https://doi.org/10.1007/978-3-642-35506-6_21
https://doi.org/10.1007/978-3-642-35506-6_21
https://escholarship.org/uc/item/6hg94304
https://escholarship.org/uc/item/6hg94304
http://aclweb.org/anthology/C18-1027
http://aclweb.org/anthology/C18-1027
http://aclweb.org/anthology/C18-1027
https://www.aclweb.org/anthology/E14-1031
https://www.aclweb.org/anthology/E14-1031
https://www.aclweb.org/anthology/E14-1031
https://doi.org/10.1162/tacl_a_00139
https://doi.org/10.1162/tacl_a_00139
https://doi.org/10.1109/TKDE.2016.2610428
https://doi.org/10.1109/TKDE.2016.2610428
https://doi.org/10.1109/TKDE.2016.2610428

