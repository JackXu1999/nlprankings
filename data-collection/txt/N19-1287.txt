



















































Document-Level Event Factuality Identification via Adversarial Neural Network


Proceedings of NAACL-HLT 2019, pages 2799–2809
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

2799

Document-Level Event Factuality Identification via
Adversarial Neural Network

Zhong Qian1, Peifeng Li1 2, Qiaoming Zhu1 2 and Guodong Zhou1 2
1School of Computer Science and Technology, Soochow University, Suzhou, China

2AI Research Institute, Soochow University, Suzhou, China
qianzhongqz@163.com, {pfli, qmzhu, gdzhou}@suda.edu.cn

Abstract

Document-level event factuality identification
is an important subtask in event factuality
and is crucial for discourse understanding in
Natural Language Processing (NLP). Previous
studies mainly suffer from the scarcity of suit-
able corpus and effective methods. To solve
these two issues, we first construct a corpus
annotated with both document- and sentence-
level event factuality information on both En-
glish and Chinese texts. Then we present an
LSTM neural network based on adversarial
training with both intra- and inter-sequence at-
tentions to identify document-level event fac-
tuality. Experimental results show that our
neural network model can outperform various
baselines on the constructed corpus.

1 Introduction

Document-level event factuality identification is
the task of deciding the commitment of relevant
sources towards the factual nature of an event, and
to determine whether an event is a fact, a possi-
bility, or an impossible situation from the view of
document. Identifying document-level factuality
of events requires comprehensive understanding
of documents. As illustrated in Figure 1 where
events are in bold, the event “reach” (including
its other forms) have various factuality values in
different sentences. For example, in paragraph 2,
“reach” is impossible/CT- according to the nega-
tive word “denied”, while in paragraph 3, “reach”
is possible/PS+ due to the speculative word “may”.
The main contents of this document is “Mexico de-
nied that they will reach an agreement with the
U.S. on the new trade deal”, and the document-
level factuality of the event “reach” is CT-.

Document-level event factuality identification
is fundamental for document-level NLP applica-
tions, such as machine reading comprehension,
which aims to have machines read a text passage

According to Politico.com, it is said the United States
will reach(CT+) an agreement with Mexico on the new
trade deal that will replace North American Free Trade
Agreement (NAFTA) before December, 2017.

However, Mexican Economy Minister Ildefonso Gua-
jardo denied that they plan to reach(CT-) any agreement
with the U.S. on the trade deal talks.

“We are not going to sacrifice the quality of an
agreement because of pressure of time. We will keep en-
gaged.” he said. Just two days ago, Guajardo said the
two sides may reach(PS+) an agreement within hours.

The government has not been informed that any agree-
ment will be reached(CT-) yet, said another two Mexi-
can officials.

During the past few weeks, the U.S. has been ne-
gotiating with Mexico on the new trade deal and has
achieved much progress. Thus, some media speculate
that they will possibly reach(PS+) an agreement. But
now it seems that the negotiations will continue before
they can get a good deal.

(Time: November, 2017)
(Document-level factuality of the event “reach” is CT-.)

Figure 1: An example document with both sentence-
and document-level event factuality.

and then answer questions about the text. Accord-
ing to the document in Figure 1, the answer of the
following question should be “No”, which is con-
sistent with the document-level factuality of the
event “reach” (CT-):

Q: Does the U.S. reach an agreement with Mex-
ico on the new trade deal before December 2017?

A: No.
Previous studies mostly reported on sentence-

level event factuality identification tasks. On one
hand, due to the scarcity of document-level event
factuality corpus, these studies only considered the
corpora annotated with sentence-level event factu-
ality information, such as ACE 20051, LU (Diab
et al., 2009), FactBank (Saurı́ and Pustejovsky,
2009), and UDS-IH2 (Rudinger et al., 2018).

On the other hand, previous studies only con-

1https://catalog.ldc.upenn.edu/LDC2006T06



2800

sidered information within sentences, using rules
(Saurı́, 2008; Saurı́ and Pustejovsky, 2012), ma-
chine learning models (de Marneffe et al., 2012;
Werner et al., 2015; Baly et al., 2018), and com-
binations of them (Qian et al., 2015; Stanovsky
et al., 2017) for modeling. Neural network models
have also recently been used for the sentence-level
event factuality identification (He et al., 2017;
Rudinger et al., 2018; Qian et al., 2018). Ac-
cording to Figure 1, document-level event factu-
ality can not be deduced from each sentence-level
factuality separately, but depends on the compre-
hensive semantic information of sentences. How-
ever, no suitable model for document-level task
has been proposed yet.

To solve the issues above, this paper focuses
on document-level event factuality identification.
Our contributions can be summarized as follows.

1) We construct a document-level event factual-
ity corpus, i.e. DLEF, on both English and Chi-
nese texts. To our best knowledge, this is the first
document-level event factuality corpus. The statis-
tics on the corpora and the experimental results
show that our corpus can sufficiently reflect lin-
guistic characteristics of news texts, and provide
adequate support on resource for research.

2) We propose an LSTM neural network with
both intra- and inter-sequence attentions to iden-
tify document-level event factuality, and consider
dependency paths from speculative and negative
cues to the event and sentences containing the
event as features. Due to the diversity of vari-
ous contents of the texts in DLEF corpus, we em-
ploy Adversarial Training to improve the robust-
ness of our model. Experimental results show that
our model is superior to various baselines. The
corpus and code of this paper will be released at
https://github.com/qz011/dlef.

2 Corpus Annotation

This section introduces our Document-Level
Event Factuality (DLEF) corpus, including the
source, detailed guidelines for both document- and
sentence-level event factuality, and the main statis-
tics of the corpus.

2.1 Source

News texts contain sufficient speculative and neg-
ative information that is significant for event factu-
ality identification, and usually focus on one event
with a specific topic. Moreover, FactBank (Saurı́

+ - u
CT CT+ CT- CTu
PS PS+ PS- (NA)
U (NA) (NA) Uu

Table 1: Event factuality values.

and Pustejovsky, 2009), the sentence-level event
factuality corpus, is also based on news texts.

Therefore, we choose news texts in both English
and Chinese to construct our corpus. The English
corpus consists of 1727 documents from January
2017 to January 2018, among which 1506 docu-
ments are from China Daily2, and 221 documents
are from Sina Bilingual News3. The Chinese cor-
pus consists of 4649 documents from Sina News4.
These news documents cover various topics, e.g.,
politics, economy, culture, military, and society,
which can reflect the heterogeneity of language in
news texts.

2.2 Factuality Values
Saurı́ (2008) employed modality and polarity to
describe event factuality values. Modality con-
veys the certainty degree of events, such as cer-
tain (CT), probable (PR), and possible (PS), while
polarity expresses whether the event happened, in-
cluding positive(+) and negative(-).

We use the factuality values in Table 1 accord-
ing to Saurı́ (2008). Both PR and PS are specula-
tive values and share similar certainty degrees in
our corpus, and are merged into PS . U/u means
underspecified. PSu and U+/- are not applicable
(NA) and are not considered. Although CTu is
applicable, neither document-level nor sentence-
level event can be annotated as CTu in our corpus.

2.3 Annotation Guidelines
We adopt the definition of events proposed by
TimeML (Pustejovsky et al., 2003) and consider
the events that can be critical for computing the
factuality. To ensure that the task is meaningful,
we focus on the events that have various types of
sentence-level factuality values. If there is more
than one suitable event in a document, we anno-
tate them separately.

First, the annotation of document-level event
factuality is based on the definition, i.e., deter-
mining the factuality of an event from the view of

2http://www.chinadaily.com.cn
3http://roll.edu.sina.com.cn/english/syxw/index.shtml
4http://news.sina.com.cn

https://github.com/qz011/dlef


2801

the document requires to understand the semantic
of the document, including various sentence-level
event factuality.

Second, sentence-level event factuality is essen-
tial for document-level task, which makes sense
when document- and sentence-level factuality of
events have different values. Therefore, we anno-
tate the sentence-level event factuality as follows:

CT- events are negated by negative cues. For
example, the events “enter” and “merger” are gov-
erned by negative cues “impossible” and “denied”
in sentence S1 and S2, respectively.

(S1) He said that the loss made it impossible for
them to enter the semifinals.

(S2) Sinopec responded to National Business
Daily, and denied the rumors of a merger with
PetroChina.

PS+ events (e.g. “improve” and “fallen”) are
governed by speculative cues (e.g., “impossible”
and “denied”), just as illustrated in sentence S4
and S5.

(S4) We think that further investigation may
help to improve the treatment of people with simi-
lar infections.

(S5) The missing parts may have fallen during
the flight of the plane.

PS- events are governed by both speculative and
negative cues. Different from CT-, PS- means
incompletely negation. For example, the PS-
event “noticed” is governed by the speculative cue
“probably” and the negative cue “not” in sentence
S6, and “fall” is modified by the cues “may” and
“not” in sentence S7.

(S6) The bus driver had probably not noticed
the truck early enough.

(S7) Oil prices may not fall sharply due to the
strong global demand.

Uu events can appear in questions (e.g., “con-
sidering” in sentence S8) and in the inten-
sional contexts with underspecified semantics
(e.g., “raises” in sentence S9):

(S8) Is France considering to leave EU?
(S9) The US dollar’s declination can not be re-

versed even if the Federal Reserve raises rates
three times.

CT+ events are factual and do not meet the
above conditions.

2.4 Statistics

The task is trivial if most documents have only one
type of sentence-level factuality value, and in this

Corpus Docs n=1 n=2 n>3

English

CT- 162 97 20
PS+ 93 157 24
PS- 2 6 4
Uu 5 6 1

CT+ 1022 119 9
Total 1284 385 58

Chinese

CT- 491 612 239
PS+ 321 425 102
PS- 9 11 16
Uu 8 5 7

CT+ 2061 290 52
Total 2890 1343 416

Table 2: Statistics of the documents in DLEF corpus
with n types of sentence-level event factuality values.

case, document-level factuality probably shares
the same value. To understand the usefulness of
document-level event factuality identification and
DLEF corpus, we launched the statistics of doc-
uments with n different types of sentence-level
event factuality values shown in Table 2. From
the table we can find that for English corpus there
are 41.94% CT- and 66.06% PS+ documents with
different sentence-level event factuality values, but
these CT+ documents only cover 11.13%. While
for Chinese corpus, these CT- and PS+ documents
cover 63.41% and 62.15%, but these CT+ docu-
ments only make up 14.23%.

Table 2 indicates that sentence-level factuality
usually agrees with document-level factuality in
CT+ documents, making them straightforward to
be identified. However, in those non-CT+ doc-
uments with non-factual document-level values,
sentence-level factuality is likely to have differ-
ent values from documents, making them more
difficult to be identified. In general, English and
Chinese corpus have 25.64% and 37.84% docu-
ments with different sentence-level event factual-
ity values, indicating this corpus is suitable for the
document-level event factuality identification.

Table 3 shows the statistics of the DLEF cor-
pus. CT+ document-level events are in the major-
ity, because information reported by news texts is
usually real.

Kappa (Cohen, 1960) is employed to mea-
sure the inter-annotator agreement of annotating
document- and sentence-level event factuality be-
tween the two independent annotators who an-
notate the entire corpus, just as shown in Table
4. These two annotators are postgraduate stu-



2802

Corpus Statistics

English

Documents

CT- 279/16.16%
PS+ 274/15.87%
PS- 12/0.69%
Uu 12/0.69%

CT+ 1150/66.59%
Total 1727

Sentence-
Level
Events

CT- 662/11.52%
PS+ 574/9.99%
PS- 37/6.44%
Uu 71/1.24%

CT+ 4401/76.61%
Total 5745

Avg. Len. of Sentences 14.73
Avg. Len. of Documents 467.25

Chinese

Documents

CT- 1342/28.87%
PS+ 848/18.24%
PS- 36/0.77%
Uu 20/0.43%

CT+ 2403/51.69%
Total 4649

Sentence-
Level
Events

CT- 3923/20.69%
PS+ 2879/15.18%
PS- 123/0.65%
Uu 555/2.93%

CT+ 11482/60.55%
Total 18962

Avg. Len. of Sentences 29.00
Avg. Len. of Documents 716.38

Table 3: Statistics of DLEF corpus. The units of length
of English and Chinese texts are tokens and Chinese
characters, respectively.

Corpus Value Sent-Level Doc-Level

English

All 0.81 0.91
CT- 0.82 0.89
PS+ 0.77 0.87
CT+ 0.84 0.93

Chinese

All 0.82 0.81
CT- 0.83 0.82
PS+ 0.79 0.78
CT+ 0.83 0.84

Table 4: Inter-annotator agreement of event factuality.

dents who major in NLP. In addition, the Kappa of
events on English and Chinese corpus are 0.83 and
0.85, respectively. All the Kappa values are larger
than 0.75, proving the effectiveness and meaning-
fulness of our DLEF corpus.

3 Adversarial Neural Network for
Document-Level Event Factuality
Identification

This section describes the LSTM neural network
for document-level event factuality identification
in detail. As shown in Figure 2, to extract feature
representations of events from the view of docu-
ments, we consider both intra- and inter-sequence
attention for dependency paths and sentences. In
addition, due to the diversity of contents of doc-

softmax(W1he+b1)

Sentences
(S0, S1, …, Sj-1)

output

Input 
Layer

Softmax 
Layer

Dependency Syntactic Paths
(P0, P1, …, Pi-1)

LSTM_1
(Intra-Sequence)

Embedding 
Layer

LSTM 
Layer

Inter-
Sequence 
Attention 

Layer

P0
P1

P2
Pi-1

S0
S1

S2
S3

Sj-1

0        1       2       i-1

LSTM_2
(Intra-Sequence)

0      1      2      3       j-1

he

αT αT

hsp hss

Figure 2: Neural network architecture for document-
level event factuality identification.

uments in DLEF corpus, we consider adversarial
training to ensure the robustness of our model.

3.1 Input Features

For our task, we use the specified events that have
been annotated, and utilize the Chinese cues in
CNeUn corpus (Zou et al., 2015) and the English
cues in BioScope corpus (Vincze et al., 2008) that
also considers multi-word cues, e.g., rule out. We
do not use any annotated sentence-level event fac-
tuality. For one event, we consider all the sen-
tences containing it, and mainly employ the fol-
lowing two features in our model:

1) Syntactic Features: Previous studies (Saurı́
and Pustejovsky, 2012; de Marneffe et al., 2012)
have proved the effectiveness of dependency trees
on event factuality identification tasks. Hence, we
employ the dependency paths from speculative or
negative cues to the event as syntactic features.

2) Semantic Features: We use the sentences
containing the event as semantic features.

In addition, we also consider the above features
in contexts of each sentence containing the event
as the input, and set the windows size as 3, i.e.,
one sentence before and after the current one. If
adjacent sentences contain speculative or negative
cues, the dependency path is the concatenation of
the path from the cue to the root and the path from
the root to the event (Quirk and Poon, 2017).



2803

3.2 LSTM with Two Attention Layers
A dependency path or sentence can be represented
as X0 according to the embedding table. We em-
ploy LSTM with hidden units nh to model the se-
quences from both directions to produce the for-
ward hidden sequence

−→
H , the backward hidden

sequence
−→
H , and the output sequence H =

−→
H +←−

H . We adopt the attention mechanism to capture
the most important information from H , and ob-
tain the output h:

Hm = tanh(H) (1)

α = softmax(vTH) (2)

h = tanh(HαT ) (3)

where v ∈ Rnh is the parameter. One event can
have k sequences X0,X1, . . . ,Xk−1, whose rep-
resentation is Hs = h0,h1, . . . ,hk−1 according
to the above equations, where Hs ∈ Rk×nh . To
extract the feature representation hs ∈ Rnh from
the k sequences, we utilize an inter-sequence at-
tention mechanism that is computed as:

Hms = tanh(Hs) (4)

αs = softmax(v
T
sHms) (5)

hs = tanh(Hsα
T
s ) (6)

where vs ∈ Rnh is the parameter. Suppose that an
event has i dependency paths P0,P1, . . . ,Pi−1,
and appears in j sentences S0,S1, . . . ,Sj−1.
Considering that dependency paths and sentences
contain syntactic and semantic information, re-
spectively, we employ two LSTM neural networks
defined above to learn vector representations hsp
and hss of dependency paths and sentences, and
concatenate them into the feature representation of
the event he:

he = hsp ⊕ hss (7)

where ⊕ is the concatenation operator. Finally, he
is fed into the softmax layer to compute the prob-
ability of the factuality values of the event:

o = softmax(W1he + b1) (8)

where W1 ∈ Rc×dim(he) and b1 ∈ Rc are param-
eters, and c = 5 is the number of categories of
factuality values (CT+, CT-, PS+, PS-, Uu). The
objective function of the proposed neural network
is designed as:

LD(θ) = −
1

m

m−1∑
i=0

log p(y
(i)
j |x

(i), θ) (9)

where y(i) is the golden label of the instance x(i)

and p(y(i)j |x(i)) is the probability, m is the number
of instances, and θ is the parameter set to learn.
This model with TWO attention layers is denoted
as Att 2 in the next section.

3.3 Adversarial Training
As described in Section 2, documents in DLEF
corpus cover various topics. To improve the ro-
bustness of our model, we consider Adversarial
Training. Similar to previous work (Miyato et al.,
2016; Wu et al., 2017), we add a small adversarial
perturbation eadv to word embeddings, and em-
ploy the following objective function:

Ladv(X|θ) = L(X + eadv|θ) (10)
eadv = argmax

‖e‖6�
L(X + e|θ̂) (11)

where θ̂ is a fixed copy value of the current θ and
X is the input. Due to the intractable nature in the
computation of Eq. (11), Goodfellow et al. (2014)
proposed Eq. (12) to linear L(X|θ) near X to
approximate Eq. (11):

eadv = �g/‖g‖ (12)
g = ∇TL(X|θ̂) (13)

where T is the embedding table.

4 Experiments

We introduce the experimental settings and the
baselines, finally presenting the experimental re-
sults and analysis in detail.

4.1 Experimental Settings
The PS- and Uu documents only cover 1.39% and
1.20% in our English and Chinese corpus, respec-
tively. Therefore, we mainly focus on the perfor-
mance of CT+, CT-, and PS+.

For fair comparison, we perform 10-fold cross-
validation on English and Chinese corpora, re-
spectively. In addition to Precision, Recall, and
F1-measure for each category of factuality value,
we consider macro- and micro-averaging to ob-
tain the overall performance of all the categories
of factuality values. The hidden units of LSTM
are set as nh = 50. We initialize word embed-
dings via Word2Vec (Mikolov et al., 2013), setting
the dimensions as d0 = 100, and fine-tuning them
during training. SGD with momentum is applied
to optimize our models.



2804

Att 2 and Att 2+AT are the models proposed
in Section 3 that consider the contexts, i.e., one
sentence before and after the current sentence con-
taining the event as the input. Compared to Att 2,
Att 2+AT considers Adversarial Training (AT, the
same below). We also consider the following base-
lines for the comparison with our models:

MaxEntVote is a maximum entropy model that
only considers the view of AUTHOR (de Marn-
effe et al., 2012). We use maximum entropy
model to identify sentence-level event factuality,
and consider voting mechanism, i.e., choose the
value committed by the most sentences as the
document-level factuality value. We also consider
other machine learning models, e.g. Lee et al.
(2015), but obtain lower micro-/macro-averaged
F1 on English (59.38/33.36) and Chinese corpus
(53.91/43.20).

SentVote identifies sentence-level event factu-
ality, and does not consider inter-sequence atten-
tion in the model proposed in Section 3. Similar
to MaxEntVote model, voting mechanism is used
to identify document-level event factuality in this
SentVote model.

MP 2 considers Max-Pooling instead of atten-
tion compared with Att 2.

Att 1 considers only intra-sequence attention,
but not the inter-sequence attention. For an event,
we concatenate its i dependency paths and j sen-
tences into one path and one sentence as the input,
respectively.

4.2 Results and Analysis

4.2.1 Architecture of Neural Networks

Table 5 presents the performances of our mod-
els and baselines. MaxEntVote gives relatively
lower results than other models, especially on CT-
and PS+. SentVote models are better than Max-
EntVote, but still obtain lower results than Att 2,
which can prove that inter-sequence attention is
more useful than voting. Max-pooling only se-
lects the most active information for each dimen-
sion of features, While attention takes into account
all the features and assigns weights for them ac-
cording their degrees of importance. Hence, Att 2
gets better results than MP L2. Att 1 only consid-
ers the intra-sequence attention and obtains lower
results than Att 2, which proves the effectiveness
of inter-sequence attention. Att 2 and Att 2+AT
achieve better results than other baselines. Com-
pared to Att 2, Att 2+AT considers the adversarial

perturbation and training that can alleviate over-
fitting. Therefore, Att 2+AT is superior to Att 2,
which can prove the effectiveness of adversarial
training.

On both English and Chinese corpora, the per-
formance of CT+ is better than those of PS+ and
CT-. On one hand, it is easier to identify CT+
documents due to their majority. On the other
hand, most news texts hardly contain bogus and
false contents. Therefore, in most CT+ docu-
ments, sentence-level factuality values are consis-
tent with the document-level value, just as in S10.
However, in PS+ and CT- documents with non-
CT+ document-level values, sentence-level factu-
ality values have different viewpoints with the cor-
responding document, varying among CT-, PS+,
and CT+, making the task more difficult, e.g., S11.

(S10) India successfully tested(CT+) a super-
sonic missile, capable of destroying an incoming
ballistic missile at low altitude. ...... The test(CT+)
was carried out from a test range in Odisha , offi-
cial sources said.

(S11) Argentine navy said it had not
contacted(CT-) the SAN Juan submarine. ...
... Some media previously said the navy may
have received signals from the submarine and
contacted(PS+) it.

4.2.2 Input of Neural Networks
For Att 2+AT, we also investigate the effects of
contexts of the sentences containing events as the
input on the performance. The results is given in
Table 6, which shows that contexts can improve
the performance more significantly on the Chinese
corpus than the English corpus. We find that in
the Chinese corpus these sentences are commonly
in the same paragraph and have a strong seman-
tic coherence. Therefore, information in adjacent
sentences can contribute to the identification of the
document-level factuality of the events in the cur-
rent sentences.

Sentences S12 and S13 are adjacent sentences
in one paragraph. The document-level factual-
ity value of the event “provided” in S12 is CT-.
However, the sentence-level value of “provided”
is PS+. If we consider S13, the negative cue “de-
nied” can lead to the correct document-level fac-
tuality value of “provided”. While in the English
corpus, similar sentences are much fewer, because
paragraphs in most English news texts only con-
tain one or two sentences, and sentences in dif-
ferent paragraphs share less semantic correlation



2805

Corpus Systems CT- PS+ CT+ Micro-A Macro-A

English

MaxEntVote 58.17 35.89 75.14 68.42 56.40
SentVote 70.22 57.85 83.98 78.06 70.68
MP 2 70.57 56.39 83.72 77.65 70.23
Att 1 65.25 53.65 79.18 73.23 66.03
Att 2 73.88 59.29 88.59 81.84 73.92
Att 2+AT 76.87 62.14 89.84 83.56 76.28

Chinese

MaxEntVote 62.44 58.29 72.22 67.72 64.32
SentVote 72.66 58.39 80.68 74.70 70.58
MP 2 74.34 65.17 78.91 75.22 72.81
Att 1 68.82 49.78 81.89 71.12 67.28
Att 2 81.41 73.35 86.58 82.79 80.45
Att 2+AT 83.35 74.06 87.52 84.03 81.64

Table 5: F1-measures of baselines and our model.

Corpus Systems CT- PS+ CT+ Micro-A Macro-A

English

Att 2+AT 76.87 62.14 89.84 83.56 76.28
w/o CTX +0.95 +0.47 -0.80 -0.31 +0.21
w/o Dpath (Only Sent) -16.49 -8.28 -4.88 -7.08 -9.88
w/o Sent (Only Dpath) -20.79 -12.63 -19.25 -19.20 -17.56

Chinese

Att 2+AT 83.35 74.06 87.52 84.03 81.64
w/o CTX -3.05 -2.62 -1.03 -1.84 -2.23
w/o Dpath (Only Sent) -10.31 -7.23 -7.80 -8.49 -8.44
w/o Sent (Only Dpath) -17.53 -11.02 -14.79 -15.19 -14.44

Table 6: F1-measures of Att 2+AT with different input features.

than those in the same paragraph. Hence, perfor-
mance improvement is less when considering ad-
jacent sentences in the English corpus.

(S12) 外界质疑在竞标过程中，墨西哥政
府为相关企业提提提供供供了“有利位置”。 (It is
doubted that the Mexican government provided
“vantage points” for the enterprises involved dur-
ing the bidding process.)

(S13)墨西哥外交部在7日对此予以回应，否
认了这种说法。 (The Mexican Foreign Ministry
responded and denied the rumor on 7th.)

If we consider more adjacent sentences, e.g.,
two sentences before and after the current sen-
tence, however, the results will be a bit lower. The
micro-/macro-averaged F1 on English and Chi-
nese corpus are 81.20/75.65 and 82.57/80.91, re-
spectively. We think the reason is that some sen-
tences are far away from the current sentence and
have little effect on the current event, and consid-
ering more contexts may also lead to overfitting.

Moreover, we explore the effects of considering
only dependency path (Dpath) and only sentence
(Sent) in Table 6. Att 2+AT achieves the best re-
sults when considering both paths and sentences

as input, proving that both of them are effective
features for our model. Att 2+AT obtains higher
performance with only sentences than only paths
as input, meaning that Att 2+AT is mainly benefi-
cial from sentences that can offer semantic infor-
mation. Error analysis shows that documents with
incorrect identified values contains sentences with
more speculative or negative cues:

(S14) When asked if it might be arson, authori-
ties said that no fire raiser has been found now, but
the possibility of artificial arson should not been
ruled out.

S14 contains speculative cues “if ”, “might”,
“possibility” and negative cues “no”, “not”, “ruled
out”. It is difficult to identify whether the events
are governed by the cues when only considering
the dependency paths and ignoring the semantic
information offered by sentences. S14 can demon-
strate the importance of semantic features.

4.2.3 Documents with Different
Sentence-Level Event Factuality Values

As mentioned in Section 2.4, the document-level
task becomes trivial if most documents have only
one category of sentence-level factuality value that



2806

Corpus n CT- PS+ CT+ Micro-A Macro-A

English
n=1 85.63 64.46 94.91 91.36 81.67
n>2 58.33 65.79 56.01 60.91 60.04

Chinese
n=1 84.45 76.73 93.44 89.85 84.87
n>2 82.68 69.91 58.51 73.22 70.37

Table 7: F1-measures of Att 2+AT on the documents with n types of sentence-level factuality values.

Corpus Level CT- PS+ CT+ Micro-A Macro-A

English
Sentence 72.05 59.68 91.14 85.96 74.29

Document (with Joint Opt) 75.46 62.80 88.65 82.89 75.64
Document (w/o Joint Opt) 76.87 62.14 89.84 83.56 76.28

Chinese
Sentence 74.20 68.88 87.73 81.98 76.94

Document (with Joint Opt) 83.30 73.74 87.40 83.83 81.48
Document (w/o Joint Opt) 83.35 74.06 87.52 84.03 81.64

Table 8: F1-measures of Att 2+AT with joint optimization.

is the same as document-level value. Table 7
shows the performance of Att 2+AT on the docu-
ments with n different types of sentence-level fac-
tuality values. The micro- and macro-averaged F1
of n>2 are lower than those of n=1, indicating
that the factuality of documents that have different
types of sentence-level factuality are more difficult
to identify due to the interference from sentence-
level values.

We notice that in the Chinese corpus, the per-
formance of CT- is much higher than that of PS+
and CT+ when n>2. According to the analysis
on the Chinese corpus, we find that most CT- doc-
uments are usually used to deny the rumors, i.e.,
those sentence-level events whose factuality val-
ues are not CT-. Therefore, the sentence-level CT-
events are often in the topic sentences of the docu-
ments and dominate among sentences, which can
contribute to the better results of document-level
CT- events in Chinese corpus.

4.2.4 Joint Optimization Model

Because document-level event factuality is related
with sentence-level factuality information, we also
consider the joint optimization model for them.
For sentence-level task, we use the LSTM neural
network in Section 3 and only consider the current
sentence, i.e., do not consider information in ad-
jacent sentences and the inter-sequence attention
layer. The objective of document- and sentence-
level task are denoted as LD(θ) and LS(θ), and
the objective of our joint optimization model is:

LJ(θ) = εLD(θ) + (1− ε)LS(θ) (14)

where ε=0.6 is the trade-off. The performance
of both sentence-level and document-level event
factuality identification is shown in Table 8.
The micro-/macro-averaged F1 of joint optimiza-
tion model on English and Chinese corpus are
82.89/75.64 and 83.83/81.48, respectively. Al-
though document-level event factuality is based on
the factuality information in sentences, sentence-
level factuality value of an event only depends on
the current sentence, and is likely to have a differ-
ent value compared to the current document-level
factuality. Therefore, the joint model can not im-
prove the performance of document-level task.

5 Related Work

Researchers have studied document-level tasks in
many NLP applications, e.g., sentiment analysis
(Xu et al., 2016; Dou, 2017), named entity recog-
nition (Luo et al., 2018), and machine transla-
tion (Born et al., 2017). But related studies on
event factuality are limited to the sentence-level
task. Diab et al. (2009) and Prabhakaran et al.
(2010) presented studies of belief annotation and
tagging, and classified predicate events into Com-
mitted Belief (CB), Non-CB or Not Applicable
using a supervised framework. For factuality as-
sessment, Lee et al. (2015) employed dependency
features, while Stanovsky et al. (2017) consid-
ered deep linguistic information, such as modality
classes, syntactic re-ordering with PropS tree an-
notation structure (Lotan et al., 2013). Baly et al.
(2018) considered a set of features and predicted
the factuality of reporting and bias of news media.



2807

Saurı́ (2008) and Saurı́ and Pustejovsky (2012)
proposed a rule-based model to identify event fac-
tuality on FactBank. de Marneffe et al. (2012)
used a machine learning model and Qian et al.
(2015) utilized a two-step framework combining
machine learning and rule-based approaches on
FactBank. In addition to FactBank, Prabhakaran
et al. (2015) proposed a ongoing framework for a
larger corpus based on LU, and Cao et al. (2013)
constructed a Chinese corpus annotated with event
factuality based on ACE 2005. However, no previ-
ous work annotated a document-level corpus. We
construct DLEF corpus with document-level event
factuality for the first time.

Some studies focused on document-level event
identification task. Choubey et al. (2018) de-
signed a rule-based classifier to identify central
events according to event coreference relations.
Liu et al. (2018) utilized a kernel-based neural
model that captured semantic relations between
discourse units for event salience identification.
However, they did not consider the document-
level event factuality. To our best knowledge, this
paper is the first work on document-level event
factuality identification task.

Previous studies (He et al., 2017; Rudinger
et al., 2018; Qian et al., 2018) have tried neural
network models on sentence-level factuality iden-
tification. Recent research has shown that neu-
ral networks with multi-level attention can extract
meaningful information from heterogeneous input
and improve the performance of NLP tasks, e.g.,
discourse relation (Liu and Li, 2016), relation clas-
sification (Wang et al., 2016), and question an-
swering (Yu et al., 2017). Moreover, to improve
the robustness of neural networks, related stud-
ies considered adversarial perturbation and train-
ing on text classification (Miyato et al., 2016) and
relation extraction (Wu et al., 2017). This paper is
in line in proposing an adversarial neural network
with both intra- and inter-sequence attention.

6 Conclusion

We investigated document-level event factuality
identification task by constructing a corpus an-
notated with document- and sentence-level event
factuality based on both English and Chinese
texts. To identify document-level event factual-
ity, we proposed an LSTM neural network with
both intra- and inter-sequence attention, and con-
sider adversarial training to improve the robust-

ness. Experimental results showed that document-
level event identification on our DLEF corpus is
useful, and our adversarial training model outper-
forms several baselines. To our knowledge, this is
the first paper for the document-level event factu-
ality identification.

In the future work, we will consider to de-
tect events and their sentence-level and document-
level factuality with a joint framework, and we
will also continue to expand the scale of our DLEF
corpus.

Acknowledgments

The authors would like to thank the three anony-
mous reviewers for their comments on this pa-
per. This work was partially supported by national
Natural Science Foundation of China (NSFC)
via Grant Nos. 61836007, 6177235, 61773276,
61673290.

References
Ramy Baly, Georgi Karadzhov, Dimitar Alexandrov,

James R. Glass, and Preslav Nakov. 2018. Predict-
ing factuality of reporting and bias of news media
sources. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, Brussels, Belgium, October 31 - November 4,
2018, pages 3528–3539.

Leo Born, Mohsen Mesgar, and Michael Strube. 2017.
Using a graph-based coherence model in document-
level machine translation. In Proceedings of Dis-
coMT@EMNLP 2017, pages 26–35.

Yuan Cao, Qiaoming Zhu, and Peifeng Li. 2013.
The construction of chinese event factuality cor-
pus. Journal of Chinese Information Processing,
27(6):38–45.

Prafulla Kumar Choubey, Kaushik Raju, and Ruihong
Huang. 2018. Identifying the most dominant event
in a news article by mining event coreference re-
lations. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT, New Orleans, Louisiana,
USA, June 1-6, 2018, Volume 2 (Short Papers),
pages 340–345.

Jacob Cohen. 1960. A coefficient of agreement for
nominal scales. Educational & Psychological Mea-
surement, 20(1):37–46.

Mona T. Diab, Lori S. Levin, Teruko Mitamura, Owen
Rambow, Vinodkumar Prabhakaran, and Weiwei
Guo. 2009. Committed belief annotation and tag-
ging. In Proceedings of the Third Linguistic Anno-
tation Workshop, LAW 2009, pages 68–73.

https://aclanthology.info/papers/D18-1389/d18-1389
https://aclanthology.info/papers/D18-1389/d18-1389
https://aclanthology.info/papers/D18-1389/d18-1389
https://aclanthology.info/papers/W17-4803/w17-4803
https://aclanthology.info/papers/W17-4803/w17-4803
https://aclanthology.info/papers/N18-2055/n18-2055
https://aclanthology.info/papers/N18-2055/n18-2055
https://aclanthology.info/papers/N18-2055/n18-2055
http://www.aclweb.org/anthology/W09-3012
http://www.aclweb.org/anthology/W09-3012


2808

Zi-Yi Dou. 2017. Capturing user and product informa-
tion for document level sentiment analysis with deep
memory network. In Proceedings of EMNLP 2017,
pages 521–526.

Ian J. Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2014. Explaining and harnessing adver-
sarial examples. CoRR, abs/1412.6572.

Tianxiong He, Peifeng Li, and Qiaoming Zhu. 2017.
Identifying chinese event factuality with convolu-
tional neural networks. In Proceedings of CLSW
2017, pages 284–292.

Kenton Lee, Yoav Artzi, Yejin Choi, and Luke Zettle-
moyer. 2015. Event detection and factuality assess-
ment with non-expert supervision. In Proceedings
of EMNLP 2015, pages 1643–1648.

Yang Liu and Sujian Li. 2016. Recognizing implicit
discourse relations via repeated reading: Neural net-
works with multi-level attention. In Proceedings of
EMNLP 2016, pages 1224–1233.

Zhengzhong Liu, Chenyan Xiong, Teruko Mitamura,
and Eduard H. Hovy. 2018. Automatic event
salience identification. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, Brussels, Belgium, October 31 -
November 4, 2018, pages 1226–1236.

Amnon Lotan, Asher Stern, and Ido Dagan. 2013.
Truthteller: Annotating predicate truth. In Human
Language Technologies: Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, Proceedings, June 9-14, 2013,
Westin Peachtree Plaza Hotel, Atlanta, Georgia,
USA, pages 752–757.

Ling Luo, Zhihao Yang, Pei Yang, Yin Zhang, Lei
Wang, Hongfei Lin, and Jian Wang. 2018. An
attention-based bilstm-crf approach to document-
level chemical named entity recognition. Bioinfor-
matics, 34(8):1381–1388.

Marie-Catherine de Marneffe, Christopher D. Man-
ning, and Christopher Potts. 2012. Did it happen?
the pragmatic complexity of veridicality assessment.
Computational Linguistics, 38(2):301–333.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed repre-
sentations of words and phrases and their composi-
tionality. In Proceedings of NIPS 2013, pages 3111–
3119.

Takeru Miyato, Andrew M. Dai, and Ian J.
Goodfellow. 2016. Virtual adversarial training
for semi-supervised text classification. CoRR,
abs/1605.07725.

Vinodkumar Prabhakaran, Tomas By, Julia Hirschberg,
Owen Rambow, Samira Shaikh, Tomek Strza-
lkowski, Jennifer Tracey, Michael Arrigo, Ru-
payan Basu, Micah Clark, Adam Dalton, Mona
Diab, Louise Guthrie, Anna Prokofieva, Stephanie

Strassel, Gregory Werner, Yorick Wilks, and Janyce
Wiebe. 2015. A new dataset and evaluation for be-
lief/factuality. In Proceedings of the Fourth Joint
Conference on Lexical and Computational Seman-
tics, pages 82–91, Denver, Colorado. Association for
Computational Linguistics.

Vinodkumar Prabhakaran, Owen Rambow, and
Mona T. Diab. 2010. Automatic committed belief
tagging. In COLING 2010, pages 1014–1022.

James Pustejovsky, José M. Castaño, Robert Ingria,
Roser Saurı́, Robert J. Gaizauskas, Andrea Set-
zer, Graham Katz, and Dragomir R. Radev. 2003.
Timeml: Robust specification of event and temporal
expressions in text. In New Directions in Question
Answering, Papers from 2003 AAAI Spring Sympo-
sium, pages 28–34, Stanford University, Stanford,
CA, USA.

Zhong Qian, Peifeng Li, Yue Zhang, Guodong Zhou,
and Qiaoming Zhu. 2018. Event factuality identifi-
cation via generative adversarial networks with aux-
iliary classification. In Proceedings of the Twenty-
Seventh International Joint Conference on Artificial
Intelligence, IJCAI 2018, July 13-19, 2018, Stock-
holm, Sweden., pages 4293–4300.

Zhong Qian, Peifeng Li, and Qiaoming Zhu. 2015. A
two-step approach for event factuality identification.
In IALP 2015, pages 103–106.

Chris Quirk and Hoifung Poon. 2017. Distant super-
vision for relation extraction beyond the sentence
boundary. In Proceedings of EACL 2017, pages
1171–1182.

Rachel Rudinger, Aaron Steven White, and Benjamin
Van Durme. 2018. Neural models of factuality. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 731–744. Associa-
tion for Computational Linguistics.

Roser Saurı́. 2008. A Factuality Profiler for Eventu-
alities in Text. Ph.D. thesis, Brandeis University,
Waltham, MA, USA.

Roser Saurı́ and James Pustejovsky. 2009. Factbank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227–268.

Roser Saurı́ and James Pustejovsky. 2012. Are you
sure that this happened? assessing the factuality de-
gree of events in text. Computational Linguistics,
38(2):261–299.

Gabriel Stanovsky, Judith Eckle-Kohler, Yevgeniy
Puzikov, Ido Dagan, and Iryna Gurevych. 2017. In-
tegrating deep linguistic features in factuality pre-
diction over unified datasets. In Proceedings of ACL
2017, pages 352–357.

https://aclanthology.info/papers/D17-1054/d17-1054
https://aclanthology.info/papers/D17-1054/d17-1054
https://aclanthology.info/papers/D17-1054/d17-1054
http://arxiv.org/abs/1412.6572
http://arxiv.org/abs/1412.6572
https://doi.org/10.1007/978-3-319-73573-3_25
https://doi.org/10.1007/978-3-319-73573-3_25
http://aclweb.org/anthology/D/D15/D15-1189.pdf
http://aclweb.org/anthology/D/D15/D15-1189.pdf
http://aclweb.org/anthology/D/D16/D16-1130.pdf
http://aclweb.org/anthology/D/D16/D16-1130.pdf
http://aclweb.org/anthology/D/D16/D16-1130.pdf
https://aclanthology.info/papers/D18-1154/d18-1154
https://aclanthology.info/papers/D18-1154/d18-1154
http://aclweb.org/anthology/N/N13/N13-1091.pdf
https://doi.org/10.1093/bioinformatics/btx761
https://doi.org/10.1093/bioinformatics/btx761
https://doi.org/10.1093/bioinformatics/btx761
https://doi.org/10.1162/COLI_a_00097
https://doi.org/10.1162/COLI_a_00097
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality
http://arxiv.org/abs/1605.07725
http://arxiv.org/abs/1605.07725
http://www.aclweb.org/anthology/S15-1009
http://www.aclweb.org/anthology/S15-1009
http://aclweb.org/anthology/C/C10/C10-2117.pdf
http://aclweb.org/anthology/C/C10/C10-2117.pdf
http://www.timeml.org/publications/timeMLpubs/IWCS-v4.pdf
http://www.timeml.org/publications/timeMLpubs/IWCS-v4.pdf
https://doi.org/10.24963/ijcai.2018/597
https://doi.org/10.24963/ijcai.2018/597
https://doi.org/10.24963/ijcai.2018/597
https://doi.org/10.1109/IALP.2015.7451542
https://doi.org/10.1109/IALP.2015.7451542
https://aclanthology.info/papers/E17-1110/e17-1110
https://aclanthology.info/papers/E17-1110/e17-1110
https://aclanthology.info/papers/E17-1110/e17-1110
https://doi.org/10.18653/v1/N18-1067
https://doi.org/10.1007/s10579-009-9089-9
https://doi.org/10.1007/s10579-009-9089-9
https://doi.org/10.1162/COLI_a_00096
https://doi.org/10.1162/COLI_a_00096
https://doi.org/10.1162/COLI_a_00096
https://doi.org/10.18653/v1/P17-2056
https://doi.org/10.18653/v1/P17-2056
https://doi.org/10.18653/v1/P17-2056


2809

Veronika Vincze, György Szarvas, Richárd Farkas,
György Móra, and János Csirik. 2008. The bio-
scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(S-11).

Linlin Wang, Zhu Cao, Gerard de Melo, and Zhiyuan
Liu. 2016. Relation classification via multi-level at-
tention cnns. In Proceedings of ACL 2016, pages
1298–1307.

Gregory Werner, Vinodkumar Prabhakaran, Mona
Diab, and Owen Rambow. 2015. Committed belief
tagging on the factbank and lu corpora: A compara-
tive study. In Proceedings of the Second Workshop
on Extra-Propositional Aspects of Meaning in Com-
putational Semantics (ExProM 2015), pages 32–40,
Denver, Colorado. Association for Computational
Linguistics.

Yi Wu, David Bamman, and Stuart J. Russell. 2017.
Adversarial training for relation extraction. In Pro-
ceedings of EMNLP 2017, pages 1778–1783.

Jiacheng Xu, Danlu Chen, Xipeng Qiu, and Xuan-
jing Huang. 2016. Cached long short-term memory
neural networks for document-level sentiment clas-
sification. In Proceedings of EMNLP 2016, pages
1660–1669.

Dongfei Yu, Jianlong Fu, Tao Mei, and Yong Rui.
2017. Multi-level attention networks for visual
question answering. In Proceedings of CVPR 2017,
pages 4187–4195.

Bowei Zou, Qiaoming Zhu, and Guodong Zhou. 2015.
Negation and speculation identification in chinese
language. In Proceedings of ACL-IJCNLP 2015,
pages 656–665.

https://doi.org/10.1186/1471-2105-9-S11-S9
https://doi.org/10.1186/1471-2105-9-S11-S9
https://doi.org/10.1186/1471-2105-9-S11-S9
http://aclweb.org/anthology/P/P16/P16-1123.pdf
http://aclweb.org/anthology/P/P16/P16-1123.pdf
http://www.aclweb.org/anthology/W15-1304
http://www.aclweb.org/anthology/W15-1304
http://www.aclweb.org/anthology/W15-1304
https://aclanthology.info/papers/D17-1187/d17-1187
http://aclweb.org/anthology/D/D16/D16-1172.pdf
http://aclweb.org/anthology/D/D16/D16-1172.pdf
http://aclweb.org/anthology/D/D16/D16-1172.pdf
https://doi.org/10.1109/CVPR.2017.446
https://doi.org/10.1109/CVPR.2017.446
http://aclweb.org/anthology/P/P15/P15-1064.pdf
http://aclweb.org/anthology/P/P15/P15-1064.pdf

