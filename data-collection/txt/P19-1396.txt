



















































Leveraging Meta Information in Short Text Aggregation


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4042–4049
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

4042

Leveraging Meta Information in Short Text Aggregation

He Zhao† Lan Du†∗ Guanfeng Liu‡ Wray Buntine†
†Faculty of Information Technology

Monash University, Australia
ethanhezhao@gmail.com, {lan.du,wray.buntine}@monash.edu

‡Department of Computing
Macquarie University, Australia
guanfeng.liu@mq.edu.au

Abstract

Analysing topics in short texts (e.g., tweets
and new headings) is a challenging task be-
cause short texts often contain insufficient
word co-occurrence information, which is im-
portant to learn good topics in conventional
topic topics. To deal with the insufficiency,
we propose a generative model that aggregates
short texts into clusters by leveraging the asso-
ciated meta information. Our model can gen-
erate more interpretable topics as well as doc-
ument clusters. We develop an effective Gibbs
sampling algorithm favoured by the fully lo-
cal conjugacy in the model. Extensive exper-
iments demonstrate that our model achieves
better performance in terms of document clus-
tering and topic coherence.

1 Introduction

Texts generated on the internet (e.g., tweets, news
headlines and product reviews) are usually short,
which means that each individual document con-
tains insufficient word co-occurrence information.
Many existing topic models like Latent Dirichlet
Allocation (LDA) (Blei et al., 2003) and its vari-
ants infer topics purely based on the word occur-
rence information, which often results in degraded
performance and makes those models incapable of
learning from short texts.

Recently, many research efforts have been de-
voted to analysing short texts. A common strategy
is to aggregate short texts into clusters and then ap-
ply topic models to those clusters. The clusters are
expected to aggregate the word co-occurrence in-
formation of the assigned documents. One widely-
used option is known as self-aggregation, where
we can aggregate short texts according to the con-
textual information. For example, the contextual
information of a document can be encoded by its
topics so that the topic assignments can be used for

∗Corresponding author

aggregation. This line of research includes models
such as SATM (Quan et al., 2015), LTM (Li et al.,
2018a), and PTM (Zuo et al., 2016a). On the other
hand, many short texts, likes tweets, often come
with meta information (meta-info for short, also
known as meta-data or side information), such
as authors, categories, hashtags, timestamps, etc.
Therefore, another popular option is to aggregate
short texts according to their meta-info. For exam-
ple, we can assume that tweets published by the
same users (Hong and Davison, 2010; Zhao et al.,
2011) or with the same hashtags (Mehrotra et al.,
2013) are likely to discuss similar topics. Those
tweets can be aggregated into the same clusters.

Although the above two aggregation schemes
have yielded prominent results on short text anal-
ysis, there is still space for improvement. For ex-
ample, in tweet analysis: if we ignore the associ-
ated meta-info as in the self-aggregation scheme,
we may lose important information; on the other
hand, it may not be a perfect idea to simply aggre-
gate the tweets according to one kind of its meta-
info such as hashtags, because the amounts of the
tweets in different hashtags may differ largely and
the diversity of the tweets labelled by one hashtag
can be dramatic. In this paper, we are interested
in developing a principle way of incorporating the
meta-info directly into the generative process of
a self-aggregation model, so that we can take ad-
vantage of both aggregation schemes in one in-
tegrated model. Here we present the Meta-Info
Guided Aggregation (MIGA) model, a new self-
aggregation model whose aggregation process is
guided by the meta-info associated with each in-
dividual short text. Specifically, MIGA aggregates
short texts according to two factors: whether those
texts have similar content and whether they share
similar meta-info. The proposed model assumes
that the more short texts share the meta-info and
discuss similar topics, the more likely they are as-



4043

signed to the same cluster. Moreover, MIGA auto-
matically balances the two factors in a principled
way. The flexibility in the framework of MIGA
also allows us to leverage hierarchical meta-info
and/or the pre-trained word embeddings to further
improve the model performance.

2 Related Work

In addition to the aforementioned aggregation
or pooling based models, another popular re-
search direction for short text topic modelling
is using word correlations or embedding to en-
hance topic models. For example, Biterm Topic
Model (BTM) (Yan et al., 2013) and Relational
BTM (Li et al., 2018b) enrich each document
with word pairs (i.e., biterms). Instead of us-
ing all the word pairs, Yang et al. (2015a) con-
siders pre-trained phrases. While Word Network
Topic Model (WNTM) (Zuo et al., 2016b) lever-
ages a network of word co-occurrences. Yang
et al. (2018) uses document-level co-occurrence
patterns. With word embeddings, Latent Fea-
ture LDA (LFLDA) (Nguyen et al., 2015) mixes
a Dirichlet-Multinomial model with a softmax
function of word embeddings; Word-Topic Mix-
ture (WTM) model (Fu et al., 2016) combines
the idea of LFLDA and Topical Word Embed-
ding (TWE) model (Liu et al., 2015); Gaussian
LDA (GLDA) (Das et al., 2015) directly generates
word embeddings from Gaussian distributions;
Xun et al. (2016) uses an alternative background
model to complement Gaussian topics in GLDA.
GPUDMM (Li et al., 2016), GPUPDMM (Li
et al., 2017) and SeaNMF (Shi et al., 2018)
utilise word semantic relations computed from
pre-trained word embeddings. MetaLDA (Zhao
et al., 2017c, 2018a), WEIFTM (Zhao et al.,
2017b), and WEDTM (Zhao et al., 2018c) lever-
age binary and real-valued word embeddings in
the topic-word distributions, respectively. Without
using external word embeddings, DirBN (Zhao
et al., 2018b) can be viewed as a self-aggregation
model which aggregates the word co-occurrence
information with a multi-layer structure.

The proposed model, MIGA, falls into the cate-
gory of aggregation based models. Compared with
others in this line, the major novelty of MIGA
is that it considers both meta-info and content of
short texts in the aggregation process, while ex-
isting models only take one factor into account.
For short text models with word embeddings, they

!"

#",%&%,'
∀) ∀* ∀+

,- ." /"

0'
∀)

1",2

∀3
4",2 56

∀7

8

9-

Figure 1: Graphical model of MIGA.

may face problems when the contextual informa-
tion of the external word embeddings is not con-
sistent with the contextual information in the tar-
get corpus. For example, the word embeddings
trained on large general corpora may not be suit-
able for a specialised target corpus. Compared
with those models, MIGA does not rely on exter-
nal information of words. Moreover, if applicable,
MIGA can also be flexibly extended with hierar-
chical meta-info and word embeddings.

3 The Proposed Model1

Given a set of D short documents, the exist-
ing self-aggregation methods such as PTM (Zuo
et al., 2016a) assume that each document d ∈
{1, · · · , D} belongs to one of M latent clusters.
Each cluster accumulates the word counts from
the assigned documents and contains more suf-
ficient word co-occurrence information than an
individual document. To generate the ith (i ∈
{1, · · · , Nd}) word wd,i in document d with Nd
words, we first sample d’s cluster assignment cd =
m ∈ {1, · · · ,M} according to its doc-cluster dis-
tribution, i.e., ψd ∈ RM+ ; and then we sample a
topic zd,i ∈ {1, · · · ,K} for word wd,i from the
cluster-topic distribution θcd ∈ RK+ . Given zd,i,
wd,i is sampled from the topic-word distribution
φzd,i ∈ R

V
+, where V is the size of the vocabulary

in the target corpus.
Different from the existing self-aggregation

methods, which impose an uninformative prior on
ψd, our model draws it from a document-specific
prior πd ∈ RM+ , constructed from d’s meta-info.
Assume that there are L unique labels2 in a cor-
pus and the labels of document d are encoded in a
binary vector fd ∈ {0, 1}L, where fd,l = 1 indi-
cates d has label l. This encoding method allows a

1The inference algorithm of the proposed model is elabo-
rated in the appendix.

2Hereafter, we use “labels” and “meta-info” interchange-
ably, even though our model is able to incorporate any meta-
info in discrete formats.



4044

document to have multiple labels. Figure 1 shows
the full generative process of MIGA, which is also
described as follows:

1. For each latent cluster m:
(a) For each label l: λl,m ∼ Ga(µ0, µ0)
(b) Draw: θm ∼ DirK(α)

2. For each topic k, draw φk ∼ DirV (β0)
3. For each document d:

(a) For each cluster m, compute: πd,m =∏L
l=1(λl,m)

fd,l

(b) Draw: ψd ∼ DirM (πd)
(c) Draw the cluster assignment: cd ∼

CatM (ψd)
4. For each word i in document d:

(a) Draw topic: zd,i ∼ CatK(θcd)
(b) Draw word: wd,i ∼ CatV (φzd,i)

where Ga(·, ·) is the gamma distribution with the
shape and rate parameters; DirK(·) is the K di-
mensional Dirichlet distribution; CatK(·) is the K
dimensional categorical distribution.

The main idea of MIGA is the meta-info guided
aggregation, where instead of putting an uninfor-
mative prior on ψd, MIGA constructs an infor-
mative document-specific Dirichlet prior with pa-
rameter πd computed from the document’s labels.
Specifically, in Step 3a above, λl,m captures the
correlations between label l and cluster m. If doc-
ument d has label l, i.e., fd,l = 1, λl,m contributes
to πd,m, which is the prior of ψd,m. This shows
how the meta-info influences the probability of as-
signing a document to a cluster. Moreover, in our
model, meta-info only contributes to the prior and
the actual value of ψd,m is eventually determined
by both the prior and the evidence (i.e., the content
of d), according to Bayes’ theorem. The incor-
poration of meta-info in the Dirichlet prior of our
model is related to the ones in Zhao et al. (2017a,
2018d), but theirs work in different domains.

Leveraging hierarchical meta-info: MIGA can
be extended to accommodate hierarchical meta-
info (e.g., an academic paper labelled with
tags “computer science→machine learning→deep
learning”). Let us consider a two-layer hierarchy,
where the L document labels (i.e., the first-layer
labels) are further categorised into a set of L′ su-
per classes (i.e., the second-layer labels). Note
that one document label is allowed to belong to
multiple super classes and f ′l,l′ ∈ {0, 1} is used to
denote whether or not a first-layer label l belongs

Dataset D V avg.Nd L
Tweets (Mehrotra et al., 2013) 87,638 24,884 11 6

Patents4 13,588 3,745 9
L′ = 3
L = 10

Web Snippets (Li et al., 2016) 12,237 10,052 15 8
Stackoverflow (Xu et al., 2015) 18,287 2,458 5 20

20Newsgroups5 10,020 2000 28
L′ = 6
L = 20

Table 1: Statistics of the datasets. D, V , avg.Nd, and
L stand for the number of documents, the vocabulary
size, the average document length, and the number of
unique labels (L′ is the number of unique labels in the
second-layer, if available), respectively.

to a second-layer label l′. The general idea here
is that instead of drawing λl,m from an uninfor-
mative gamma prior as in our original model, we
draw it from a prior distribution informed by the
second-layer labels, as follows:

λ′l′,m ∼ Ga(µ0, µ0), (1)

λl,m ∼ Ga

(
L′∏
l′=1

(λ′l′,m)
f ′
l,l′ , µ0

)
, (2)

where λ′l′,m captures the correlation between la-
bels at the two layers. Thus, the information of the
second-layer labels will be propagated down to the
assignment process of the documents.

Leveraging word embeddings: MIGA can be
extended to incorporate word embeddings to guide
the generation of latent topics. Following the ap-
proach introduced in Zhao et al. (2017c), we draw
φk ∼ DirV (βk), where where βk ∈ RV+ is com-
puted with a log-linear model of word embed-
dings, similar to Step 3a in the generative process.

4 Experiments

We evaluate the performance of MIGA on docu-
ment clustering and topic coherence, with several
advances in short text topic modelling3. We also
provide a set of qualitative analysis to demonstrate
the interpretability of our model.

The details of the datasets used in the experi-
ments are shown in Table 1. The hyper-parameter
settings of the proposed model are as follows: For
MIGA, we set β0 = 0.01, µ0 = 1.0, and im-
posed gamma prior on each entry of α; for the

3We also conducted text classification experiments with
the compared models but MIGA did not show significant im-
provements over MetaLDA.

4Collected from https://www.lens.org.
5http://qwone.com/˜jason/20Newsgroups/.

A subset with the most frequent 2000 vocabulary words and
the documents with less than 50 words was used.

https://www.lens.org
http://qwone.com/~jason/20Newsgroups/


4045

Dataset Tweets Patents Web Snippets Stackoverflow 20Newsgroups
#Clusters 100 200 500 100 200 500 100 200 500 100 200 500 100 200 500

KMeans + TFIDF - - - 0.36 0.44 0.51 0.49 0.60 0.73 0.19 0.24 0.32 0.29 0.31 0.40
KMeans + LDA 0.69 0.70 0.72 0.50 0.52 0.57 0.69 0.71 0.76 0.27 0.30 0.34 0.40 0.43 0.50

MetaLDA 0.70 0.70 0.64 0.57 0.55 0.49 0.77 0.82 0.82 0.31 0.28 0.28 0.39 0.44 0.40
GPUDMM 0.59 0.62 0.63 0.36 0.37 0.41 0.69 0.71 0.72 0.17 0.18 0.23 0.25 0.24 0.31

PTM 0.84 0.83 0.84 0.53 0.53 0.55 0.61 0.68 0.77 0.21 0.23 0.26 0.30 0.31 0.34
MIGA 0.92 0.93 0.92 0.57 0.59 0.61 0.71 0.77 0.80 0.31 0.34 0.34 0.44 0.43 0.48

(a) Purity
Dataset Tweets Patents Web Snippets Stackoverflow 20Newsgroups

#Clusters 100 200 500 100 200 500 100 200 500 100 200 500 100 200 500
KMeans + TFIDF - - - 0.16 0.20 0.23 0.29 0.34 0.39 0.10 0.15 0.23 0.30 0.30 0.34
KMeans + LDA 0.26 0.25 0.25 0.21 0.22 0.25 0.35 0.35 0.36 0.14 0.18 0.24 0.30 0.33 0.38

MetaLDA 0.29 0.26 0.23 0.25 0.24 0.19 0.41 0.42 0.41 0.17 0.18 0.21 0.30 0.33 0.32
MetaLDA-eb 0.29 0.26 0.23 0.22 0.23 0.17 0.40 0.38 0.31 0.15 0.14 0.16 0.31 0.32 0.25
GPUDMM 0.21 0.23 0.24 0.10 0.12 0.15 0.36 0.38 0.38 0.06 0.10 0.17 0.15 0.17 0.25

PTM 0.39 0.36 0.35 0.23 0.24 0.25 0.29 0.33 0.38 0.11 0.13 0.16 0.24 0.26 0.28
MIGA 0.47 0.45 0.43 0.26 0.27 0.29 0.35 0.39 0.40 0.18 0.19 0.22 0.33 0.34 0.38

(b) NMI

Table 2: Purity and NMI for document clustering. The best scores are in boldface.

other models, we used their best settings reported
in their papers unless otherwise specified. The
number of MCMC iterations for training was set
to 2000 for all the models.

Document Clustering: To measure the clus-
tering performance we used purity and Normal-
ized Mutual Information (NMI), which are com-
monly used metrics for clustering (Manning et al.,
2008). We compared MIGA with PTM (Zuo
et al., 2016a), MetaLDA (Zhao et al., 2017c)6,
GPUDMM (Li et al., 2016, 2017), as well as
KMeans with different document features. The
experiment was run as follows: 80% documents
in each dataset were used in training, purity was
computed on the remaining documents; For Met-
aLDA and MIGA, the labels of the test docu-
ments were excluded in testing for fair compar-
ison; For PTM and MIGA, we set K = 50 and
varied M , and the cluster assignments were used
in computing the two scores; For LDA, MetaLDA,
and GPUDMM, we used the topic with the largest
weight as the cluster assignment, i.e., the number
of topics equals to the number of clusters, follow-
ing Nguyen et al. (2015); KMeans with two kinds
of document features (V dimensional TFIDF vec-
tors and K = 50 dimensional document-topic
vectors extracted from LDA) served as the base-
lines. Table 2 shows the purity and NMI scores7.

6Original MetaLDA is able to use both document meta-
info and word embeddings. Here we used its variant only
with document meta-info.

7The scores of KMeans + TFIDF on Tweets are not re-
ported because it exceeds the memory of our machine.

MIGA outperforms the other models on Tweets,
Patents, and Stackoverflow, which are relatively
shorter than the other datasets. This demonstrates
our model’s effectiveness on clustering short texts.

Topic Coherence: Topic coherence measures
the semantic coherence in the most signifi-
cant (top) words in a topic, which is another
commonly-used metric for topic models. Here we
used the Normalized Pointwise Mutual Informa-
tion (NPMI) (Aletras and Stevenson, 2013; Lau
et al., 2014) to calculate a topic coherence score
of the top 10 words of each topic8. Follow-
ing Yang et al. (2015b), to eliminate rare topics,
we report the scores over the top 50% topics with
the largest number of words (i.e., for a topic k,
we can count the number of words that are as-
signed to it:

∑D
d=1

∑Nd
i=1 1zd,i=k). It is known that

word embeddings are able to significantly improve
topic coherence. Therefore, in this experiment,
following Zhao et al. (2017c), we used the word
embeddings binarised from the pre-trained 50-
dimensional GloVe word embeddings (Pennington
et al., 2014) in MIGA and MetaLDA, denoted as
MIGA-eb and MetaLDA-eb, respectively. For all
the models, we setK = 50. For PTM, MIGA, and
MIGA-eb, we report the best scores with M vary-
ing from 100 to 3000. Table 4 shows the NPMI
scores, where in general, among the models with-
out word embeddings, MIGA outperforms the oth-
ers on most datasets. Moreover, word embeddings

8We used the Palmetto package (http://palmetto.
aksw.org) with a large Wikipedia dump to compute NPMI.

http://palmetto.aksw.org
http://palmetto.aksw.org


4046

comp
1. use problem using running program
2. fax university internet mail phone
3. thanks help edu advance appreciated

comp.graphics
1. graphics color screen bit mode
2. card drivers driver video windows
3. software support looking products product

comp.sys.ibm.pc.hardware
1. drive disk hard drives floppy
2. mb card controller ide scsi
3. mhz board speed port problem

comp.sys.mac.hardware
1. monitor pc mouse systems box
2. card modem software internal meg
3. like buy looking price new good power want low need

comp.windows.x
1. windows motif application server widget
2. ftp file program package format
3. windows printer font version text

Cluster
Num

Documents Topic

1

purchase store cat outstanding shipping crucial memory
upgrades ram sdram ddr service pricing com

computer
memory
virtual
cache
apple

manufacture flash graphics crucial memory upgrades cards usb
storage ram media

computer selection online memory upgrades ram ddr
upgrade specialist

2

reviews box deal processors shipping computers
pricegrabber tax customers prices retail com

digital
camera

electronics
reviews
apple

home store shop manufacturer accessories downloads
product online apple ipods

reviews forums discussion photography camera
articles news digital review

3

country experience altavista languages search
comprehensive web

programming
java
code

language
source

browser documentation hall resources lists faqs apl jhu
programming compiler tutorials edu sites java books download

introduction programming math textbook downloading
on-line edu java

Table 3: Left: Topics related to the hierarchical labels in 20Newsgroups. We started with a second-layer label
“comp” and found its most related topics by first selecting the most related clusters (by ranking λ′l′,m) then selecting
the most related topics (by ranking θm,k). Next, we looked at the first-layer labels (marked in italic) associated
with l′, i.e. f ′l,l′ = 1 and then found the most related topics in a similar way. Right: Clusters, documents, and
topics for the label of “Computers” on Web Snippets, discovered by MIGA with K = 100,M = 500. We first
selected the most related clusters to “Computers” by ranking λl,m, and then selected the most related documents
and the most related topic in each cluster by ranking πd,m and θm,k, respectively.

Datasets Tweets Patents
Web

Snippets
Stack

overflow
20News
groups

MetaLDA 0.0020 0.0371 0.0501 -0.0767 -0.0146
MetaLDA-eb 0.0062 0.0480 0.0513 -0.0690 -0.0174
GPUDMM -0.0182 0.0000 0.0368 -0.0992 -0.0535

PTM 0.0073 0.0432 0.0408 -0.0675 -0.0282
MIGA -0.0001 0.0494 0.0544 -0.0430 -0.0178

MIGA-eb -0.0031 0.0497 0.0786 -0.0214 -0.0018

Table 4: NPMI for topic coherence. The best scores are
in boldface.

further help improve topic coherence of MIGA-eb.
It is noteworthy that MIGA and MIGA-eb do not
improve NPMI over PTM on Tweets. There are
two possible factors: the labels of the tweets are
not informative enough for MIGA to learn better
clusters and the vocabulary of this dataset consists
many slangs and abbreviations, which are not in-
cluded in the corpus used for calculating NPMI.

Qualitative Analysis: The left sub-table of Ta-
ble 3 shows the relations between the hierarchical
labels and topics discovered by MIGA in 20News-
groups. One can see that the associated top-
ics of the second-layer document label, “comp”,
are more general ones, describing several gen-
eral aspects of computers, while the topics associ-
ated with the first-layer labels are relatively more
specific. For example, the associated topics of
“comp.sys.ibm.pc.hardware” are specific ones de-
scribing different aspects of computer hardware.
The right sub-table shows the relations between
clusters, documents, and topics discover by MIGA

in Web Snippets. It can be observed that the doc-
uments in Web Snippets labelled with “Comput-
ers” are quite diverse, which can be further clus-
tered into the ones related to “hardware”, “dig-
ital products”, “programming language”, and so
on. Therefore, simply aggregating those docu-
ments into one cluster as in previous meta-info ag-
gregation models may not be appropriate. MIGA
can discover fine-grained latent clusters, each of
which focuses on different aspects of “Computers”
and can intuitively be interpreted by its top topic.

5 Conclusion

We have presented a new aggregation framework,
MIGA, for short text topic analysis. MIGA is
able to aggregate short text documents into latent
clusters by leveraging meta-info. MIGA takes ad-
vantages of previous models which perform ag-
gregation according to either content or meta-info
in short texts. The proposed framework can be
easily extended with hierarchical meta-info and
word embeddings. The experimental results have
shown that MIGA achieves improved performance
on document clustering, topic coherence, as well
as appealing interpretability. For future study,
we would like to investigate how to automati-
cally learn the number of latent clusters with non-
parametric Bayesian methods.



4047

References
Nikolaos Aletras and Mark Stevenson. 2013. Evaluat-

ing topic coherence using distributional semantics.
In International Conference on Computational Se-
mantics, pages 13–22.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet allocation. JMLR, pages
993–1022.

W. Buntine and M. Hutter. 2012. A Bayesian view
of the Poisson-Dirichlet process. arXiv preprint
arXiv:1007.0296v2 [math.ST].

Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015.
Gaussian LDA for topic models with word embed-
dings. In ACL, pages 795–804.

Xianghua Fu, Ting Wang, Jing Li, Chong Yu, and
Wangwang Liu. 2016. Improving distributed word
representation and topic model by word-topic mix-
ture model. In ACML, pages 190–205.

Liangjie Hong and Brian D Davison. 2010. Empirical
study of topic modeling in Twitter. In Workshop on
social media analytics, pages 80–88.

Jey Han Lau, David Newman, and Timothy Baldwin.
2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality.
In EACL, pages 530–539.

Chenliang Li, Yu Duan, Haoran Wang, Zhiqian Zhang,
Aixin Sun, and Zongyang Ma. 2017. Enhancing
topic modeling for short texts with auxiliary word
embeddings. ACM Transactions on Information
Systems, 36(2):11.

Chenliang Li, Haoran Wang, Zhiqian Zhang, Aixin
Sun, and Zongyang Ma. 2016. Topic modeling for
short texts with auxiliary word embeddings. In SI-
GIR, pages 165–174.

Ximing Li, Changchun Li, Jinjin Chi, and Jihong
Ouyang. 2018a. Short text topic modeling by ex-
ploring original documents. Knowledge and Infor-
mation Systems, 56(2):443–462.

Ximing Li, Ang Zhang, Changchun Li, Lantian Guo,
Wenting Wang, and Jihong Ouyang. 2018b. Rela-
tional biterm topic model: Short-text topic model-
ing using word embeddings. The Computer Journal,
62(3):359–372.

Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong
Sun. 2015. Topical word embeddings. In AAAI,
pages 2418–2424.

Christopher D Manning, Prabhakar Raghavan, and
Hinrich Schütze. 2008. Introduction to Information
Retrieval. Cambridge University Press, Cambridge.

Rishabh Mehrotra, Scott Sanner, Wray Buntine, and
Lexing Xie. 2013. Improving LDA topic models for
microblogs via tweet pooling and automatic label-
ing. In SIGIR, pages 889–892.

Dat Quoc Nguyen, Richard Billingsley, Lan Du, and
Mark Johnson. 2015. Improving topic models with
latent feature word representations. TACL, pages
299–313.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In EMNLP, pages 1532–1543.

Xiaojun Quan, Chunyu Kit, Yong Ge, and Sinno Jialin
Pan. 2015. Short and sparse text topic modeling via
self-aggregation. In IJCAI, pages 2270–2276.

Tian Shi, Kyeongpil Kang, Jaegul Choo, and Chan-
dan K Reddy. 2018. Short-text topic modeling
via non-negative matrix factorization enriched with
local word-context correlations. In WWW, pages
1105–1114.

Jiaming Xu, Peng Wang, Guanhua Tian, Bo Xu, Jun
Zhao, Fangyuan Wang, and Hongwei Hao. 2015.
Short text clustering via convolutional neural net-
works. In NAACL-HLT, pages 62–69.

Guangxu Xun, Vishrawas Gopalakrishnan, Fenglong
Ma, Yaliang Li, Jing Gao, and Aidong Zhang. 2016.
Topic discovery for short texts using word embed-
dings. In ICDM, pages 1299–1304.

Xiaohui Yan, Jiafeng Guo, Yanyan Lan, and Xueqi
Cheng. 2013. A biterm topic model for short texts.
In WWW, pages 1445–1456. ACM.

Shansong Yang, Weiming Lu, Dezhi Yang, Liang Yao,
and Baogang Wei. 2015a. Short text understand-
ing by leveraging knowledge into topic model. In
NAACL, pages 1232–1237.

Yang Yang, Feifei Wang, Junni Zhang, Jin Xu, and
S Yu Philip. 2018. A topic model for co-occurring
normal documents and short texts. World Wide Web,
21(2):487–513.

Yi Yang, Doug Downey, and Jordan Boyd-Graber.
2015b. Efficient methods for incorporating knowl-
edge into topic models. In EMNLP, pages 308–317.

He Zhao, Lan Du, and Wray Buntine. 2017a. Leverag-
ing node attributes for incomplete relational data. In
ICML, pages 4072–4081.

He Zhao, Lan Du, and Wray Buntine. 2017b. A
word embeddings informed focused topic model. In
ACML, pages 423–438.

He Zhao, Lan Du, Wray Buntine, and Gang Liu. 2017c.
MetaLDA: A topic model that efficiently incorpo-
rates meta information. In ICDM, pages 635–644.

He Zhao, Lan Du, Wray Buntine, and Gang Liu. 2018a.
Leveraging external information in topic modelling.
Knowledge and Information Systems, pages 1–33.

He Zhao, Lan Du, Wray Buntine, and Mingyuan Zhou.
2018b. Dirichlet belief networks for topic structure
learning. In NeurIPS, pages 7966–7977.



4048

He Zhao, Lan Du, Wray Buntine, and Mingyuan Zhou.
2018c. Inter and intra topic structure learning with
word embeddings. In ICML, pages 5887–5896.

He Zhao, Piyush Rai, Lan Du, and Wray Buntine.
2018d. Bayesian multi-label learning with sparse
features and labels, and label co-occurrences. In
AISTATS, pages 1943–1951.

Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He,
Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. 2011.
Comparing twitter and traditional media using topic
models. In ECIR, pages 338–349.

M. Zhou and L. Carin. 2015. Negative binomial pro-
cess count and mixture modeling. TPAMI, pages
307–320.

Yuan Zuo, Junjie Wu, Hui Zhang, Hao Lin, Fei Wang,
Ke Xu, and Hui Xiong. 2016a. Topic modeling of
short texts: A pseudo-document view. In SIGKDD,
pages 2105–2114.

Yuan Zuo, Jichang Zhao, and Ke Xu. 2016b. Word net-
work topic model: A simple but general solution for
short and imbalanced texts. Knowledge and Infor-
mation Systems, 48(2):379–398.

A Appendix on the details of the
inference algorithm

We first denote the set of all the words in the tar-
get corpus, the set of the topic assignments for all
the words, the set of the cluster assignments for all
the documents, the set of the labels of all the docu-
ments asW , Z , C, and F , respectively. Given the
generative process of MIGA, the joint probability
of the model is as follows:

Pr(W,Z, C,− | F) =
D∏
d

Nd∏
i

Pr(zd,i | θcd) Pr(wd,i | φzd,i) ·

D∏
d

Pr(cd | ψd) Pr(ψd | πd) ·
K∏
k

Pr(φk | β0) ·

M∏
m

Pr(θm | α) ·
L∏
l

M∏
m

Pr(λl,m). (3)

A.1 Sampling cluster assignment cd:

Now we extract the related terms to the cluster as-
signments C in Eq. (3) to have:

Pr(C,−) ∝
D∏
d

Pr(cd | ψd) Pr(ψd | πd) ·

D∏
d

Nd∏
i

Pr(zd,i | θcd). (4)

Given the probability density functions of
Dirichlet and categorical distributions, Eq. (4) can
be written as:

Pr(C,−) ∝
D∏
d

Γ(πd,cd + 1)Γ(πd,·)

Γ(πd,cd)Γ(πd,· + 1)
·

M∏
m

Γ(α·)

Γ(α· + nclusterm,· )

K∏
k

Γ(αk + n
cluster
m,k )

Γ(αk)

(5)

∝
D∏
d

πd,cd
πd,·

·

M∏
m

Γ(α·)

Γ(α· + nclusterm,· )

K∏
k

Γ(αk + n
cluster
m,k )

Γ(αk)
.

(6)

Given Eq. (6), the conditional probability for
Gibbs sampling of cd can be derived by:

Pr(cd = m | −) ∝
Pr(C,W,Z)

Pr(C¬d,W¬d,Z¬d)

=
πd,m
πd,·

Γ(α· + n
cluster¬d
m,· )

Γ(α· + nclusterm,· )

K∏
k

Γ(αk + n
cluster
m,k )

Γ(αk + n
cluster¬d
m,k )

=
πd,m
πd,·

∏K
k

[∏ndocd,k
j (αk + n

cluster¬d
m,k + j − 1)

]
∏Nd
i (α· + n

cluster¬d
m,· + i− 1)

,

(7)

where ndocd,k =
∑Nd

i I(zd,i=k) and I(·) is the indica-
tor function; nclusterm,k =

∑D
d

∑Nd
i I(zd,i=k&cd=m);

πd,· =
∑M

m πd,m; n
cluster
m,· =

∑K
k n

cluster
m,k ;

ncluster¬dm,· = n
cluster
m,· − Nd; ncluster¬dm,k = nclusterm,k −

ndocd,k; α· =
∑K

k αk.

A.2 Sampling topic assignment zd,i:

The sampling of zd,i is similar to the LDA model:

Pr(zd,i = k | −) ∝ (αk + nclustercd,k )
β0 + n

topic
k,v

β0 ∗ V + ntopick,·
,

(8)

where ntopick,v =
∑D

d

∑Nd
i I(wd,i=v&zd,i=k) and

n
topic
k,· =

∑V
v n

topic
k,v .

A.3 Sampling λl,k:

As λl,k is used to construct πd, which is the prior
of ψd, according to Eq. (3), we have:

Pr(ψd | πd) ∝
Γ(πd,·)

Γ(πd,· + 1)

M∏
m

πd,m. (9)

According to Zhao et al. (2017c), if we intro-
duce qd ∼ Beta(πd,·, 1), Eq. (9) can be augmented



4049

as:

Pr(πd | −) ∝
M∏
m

(qd)
πd,mπd,m. (10)

Recall that πd,m =
∏L
l=1(λl,m)

fd,l , we can ac-
tually extract the terms related to λl,m to get:

Pr(λl,m | −) ∝

e
−λl,m

∑D
d:fd,l=1

πd,m
λl,m

log 1
qd · (λl,m)gl,m ,

(11)

where gl,m =
∑D

d Ifd,l=1&cd=m.
Given the above equation, we can sample λl,m

from its gamma posterior:
λl,m ∼ Ga(µ, ν),
µ = µ0 + gl,m,

ν = µ0 −
D∑

d:fd,l=1

πd,m
λl,m

log qd. (12)

A.4 Sampling λ′l′,k for MIGA with
hierarchical meta-info:

To incorporate the second-layer labels, λ′l′,m can
be sampled similarly to λl,k as follows:

λ′l′,m ∼ Ga(µ′, ν ′),

µ′ = µ0 +
L∑

l:f ′
l,l′=1

xl,m,

ν ′ = µ0 +
L∑

l:f ′
l,l′=1

log
yl,m + µ0

µ0
, (13)

where yl,m =
∑D

d:fd,l=1
πd,m
λl,m

log 1qd and xl,m ∼

CRT(
∏L′
l′=1(λ

′
l′,m)

f ′
l,l′ , gl,m). Here, h ∼

CRT(n, r) stands for the Chinese Restaurant Table
distribution (Zhou and Carin, 2015) that generates
the number of tables h seated by n customers in a
Chinese restaurant process with the concentration
parameter r (Buntine and Hutter, 2012).

A.5 Overall Inference Algorithm for MIGA:
The inference algorithm for MIGA is shown in Al-
gorithm 1.

Algorithm 1 Gibbs sampling algorithm for MIGA
Require: {wd,i}d,i, {fd,l}d,l, K, M , α, β0, µ0,

MaxIteration
Ensure: {cd}d, {zd,i}d,i, {λl,m}l,m

1: Randomly initialise all the latent variables ac-
cording to the generative process

2: for iter ← 1 to MaxIteration do
3: / ∗ Sample the cluster assignments ∗ /
4: for d← 1 to D do
5: Sample cd by Eq. (7)
6: Update nclusterm,k
7: end for
8: / ∗ Sample the topics for words ∗ /
9: for d← 1 to D do

10: for i← 1 to Nd do
11: For wd,i = v, sample zd,i by Eq. (8)
12: Update nclusterm,k , n

doc
d,k , n

topic
k,v

13: end for
14: end for
15: / ∗ Sample λl,m ∗ /
16: for all l,m do
17: Sample λl,m by Eq. (12)
18: Recompute πd,m =

∏L
l=1(λl,m)

fd,l

19: end for
20: end for


