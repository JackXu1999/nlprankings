



















































Authorship Identification in Bengali Literature: a Comparative Analysis


Proceedings of COLING 2012: Demonstration Papers, pages 41–50,
COLING 2012, Mumbai, December 2012.

Authorship Identi�ation in Bengali Literature: aComparative Analysis
Tanmoy ChakrabortyDepartment of Computer Siene & EngineeringIndian Institute of Tehnology, KharagpurIndiaits_tanmoy�se.iitkgp.ernet.inAbstratStylometry is the study of the unique linguisti styles and writing behaviors of indi-viduals. It belongs to the ore task of text ategorization like authorship identi�ation,plagiarism detetion et. Though reasonable number of studies have been onduted inEnglish language, no major work has been done so far in Bengali. In this work, We willpresent a demonstration of authorship identi�ation of the douments written in Bengali.We adopt a set of �ne-grained stylisti features for the analysis of the text and use them todevelop two di�erent models: statistial similarity model onsisting of three measures andtheir ombination, and mahine learning model with Deision Tree, Neural Network andSVM. Experimental results show that SVM outperforms other state-of-the-art methodsafter 10-fold ross validations. We also validate the relative importane of eah stylistifeature to show that some of them remain onsistently signi�ant in every model used inthis experiment.Keywords: Stylometry, Authorship Identi�ation, Voabulary Rihness, MahineLearning.

41



1 IntrodutionStylometry is an approah that analyses text in text mining e.g., novels, stories, dramasthat the famous author wrote, trying to measure the author's style, rhythm of his pen, sub-jetion of his desire, prosody of his mind by hoosing some attributes whih are onsistentthroughout his writing, whih plays the linguisti �ngerprint of that author. Authorshipidenti�ation belongs to the subtask of Stylometry detetion where a orrespondene be-tween the prede�ned writers and the unknown artiles has to be established taking intoaount various stylisti features of the douments. The main target in this study is tobuild a deision making system that enables users to predit and to hoose the right au-thor from a spei� anonymous authors' artiles under onsideration, by hoosing variouslexial, syntati, analytial features alled as stylisti markers. Wu inorporate two mod-els�(i) statistial model using three well-established similarity measures- osine-similarity,hi-square measure, eulidean distane, and (ii) mahine learning approah with DeisionTree, Neural Network and Support Vetor Mahine (SVM).The pioneering study on authorship attributes identi�ation using word-length his-tograms appeared at the very end of nineteen entury (Malyutov, 2006). Afterthat, a number of studies based on ontent analysis (Krippendor�, 2003), omputa-tional stylisti approah (Stamatatos et al., 1999), exponential gradient learn algorithm(Argamon et al., 2003), Winnow regularized algorithm (Zhang et al., 2002), SVM basedapproah (Pavele et al., 2007) have been proposed for various languages like English, Por-tuguese (see (Stamatatos, 2009) for reviews). As a beginning of Indian language Stylometryanalysis, (Chanda et al., 2010) started working with handwritten Bengali texts to judgeauthors. (Das and Mitra, 2011) proposed an authorship identi�ation task in Bengali us-ing simple n-gram token ounts. Their approah is restritive when onsidering authors ofthe same period and same genre. The texts we have hosen are of the same genre and ofthe same time period to ensure that the suess of the learners would infer that texts anbe lassi�ed only on the style, not by the proli� disrimination of text genres or distinttime of writings. We have ompared our methods with the onventional tehnique alledvoabulary rihness and the existing method proposed by (Das and Mitra, 2011) in Ben-gali. The observation of the e�et of eah stylisti feature over 10-ross validations relieson that fat that some of them are inevitable for authorship identi�ation task espeiallyin Bengali, and few of the rare studied features ould aelerate the performane of thismapping task.2 Proposed MethodologyThe system arhiteture of the proposed stylometry detetion system is shown in Figure 1.In this setion, we brie�y desribe di�erent omponents of the system arhiteture andthen analytially present the set of stylisti features.2.1 Textual analysisBasi pre-proessing before atual textual analysis is required so that stylisti markersare learly viewed to the system for further analysis. Token-level markers disussed inthe next subsetion are extrated from this pre-proessed orpus. Bengali Shallow parser1has been used to separate the sentene and the hunk boundaries and to identify parts-of-1http://ltr.iiit.a.in/analyzer/bengali
42



Bengali Corpus                     Cleaned Corpus
                       Pre−processing

                      Parsed Corpus

           Bengali
            Shallow

         Parser
         Feature Extraction

Training
Model

Testing
Model

                  Classification Model

              Classification

Author R Author A Author O Figure 1: System arhiteturespeeh of eah token. From this parsed text, hunk-level and ontext-level markers are alsodemarated.2.2 Stylisti features extrationStylisti features have been proposed as more reliable style markers than for example, word-level features sine the stylisti markers are sometime not under the onsious ontrol ofthe author. To allow the seletion of the linguisti features rather than n-gram terms,robust and aurate text analysis tools suh as lemmatizers, part-of-speeh (POS) taggers,hunkers et are needed. We have used the Shallow parser, whih gives a parsed outputof a raw input orpus. The stylisti markers whih have been seleted in this experimentare disussed in Table 1. Most of the features desribed in Table 1 are self-explanatory.However, the problem ours when identifying keywords (KW) from the artiles of eahauthor whih serve as the representative of that author. For this, we have identi�ed top �ftyhigh frequent words (sine we have tried to generate maximum distint and non-overlappedset of keywords) exluding stop-words in Bengali for eah author using TF ∗ IDF method.Note that, all the features are normalized to make the system independent of doumentlength.2.3 Building lassi�ation modelThree well-known statistial similarity based metris namely Cosine-Similarity (COS), Chi-Square measure (CS) and Eulidean Distane (ED) are used to get their individual e�eton lassifying douments, and their ombined e�ort (COM) has also been reported. Formahine-learning model, we inorporate three di�erent modules: Deision Trees (DT)2,Neural Networks (NN)3 and Support Vetor Mahine (SVM). For training and lassi�ationphases of SVM, we have used YamCha4 toolkit and TinySVM- 0.075 lassi�er respetivelywith pairwise multi-lass deision method and the polynomial kernel.2See5 pakage by Quinlan, http://www.rulequest.om/see5-info.html3Neuroshell � the ommerial software pakage, http://www.neuroshell.om/4http://hasen-org/ taku/software/yamha/5http://l.aist-nara.a.jp/taku-ku/software/TinySVM
43



No. Feature Explanation Normalization
TokenLevel

1. L(w) Average length of the word Avg. len.(word)/ Max len.(word)Intersetion of the keywords2. KW (R) of Author R and the test |KW (doc)⋂KW (R)|doumentIntersetion of the keywords3. KW (A) of Author A and the test |KW (doc)⋂KW (A)|doumentIntersetion of the keywords4. KW (O) of Author O and the test |KW (doc)⋂KW (O)|doument5. HL Hapex Legomena (No of ount(HL)/ount(word)words with frequeny=1)6. Pun. No of puntuations ount(pun)/ount(word)
PhraseLevel 7. NP Deteted Noun Phrase ount(NP)/ount of all phrase8. VP Deteted Verb Phrase ount(VP)/ount of all phrase9. CP Deteted Conjunt Phrase ount(CP)/ount of all phrase10. UN Deteted unknown word ount(POS)/ount of all phrase11. RE Deteted redupliations ount(RDP+ECHO)/ount ofand eho words all phrase
ContextLeve
l 12. Dig Number of the dialogs Count(dialog)/ No. ofsentenes13. L(d) Average length of the dialog Avg. words per dialog/ No. ofsentenes14. L(p) Average length of the Avg. words per para/ No. ofparagraph sentenesTable 1: Seleted features used in the lassi�ation model3 Experimental Results3.1 CorpusResoure aquisition is one of the hallenging obstales to work with eletronially resoureonstrained languages like Bengali. However, this system has used 150 stories in Bengaliwritten by the noted Indian Nobel laureate Rabindranath Tagore6. We hoose this domainfor two reasons: �rstly, in suh writings the idiosynrati style of the author is not likelyto be overshadowed by the harateristis of the orresponding text-genre; seondly, inthe previous researh (Chakaraborty and Bandyopadhyay, 2011), the author has workedon the orpus of Rabindranath Tagore to explore some of the stylisti behaviors of hisdouments. To di�erentiate them from other authors' artiles, we have seleted 150 artilesof Sarat Chandra Chottopadhyay and 150 artiles7 of a group of other authors (exludingprevious two authors) of the same time period. We divide 100 douments in eah luster fortraining and validation purpose and rest for testing. The statistis of the entire dateset istabulated in Table 2. Statistial similarity based measures use all 100 douments for makingrepresentatives the lusters. In mahine learning models, we use 10-fold ross validationmethod disussed later for better onstruting the validation and testing submodules. Thisdemonstration fouses on two topis: (a) the e�ort of many authors on feature seletion6http://www.rabindra-rahanabali.nltr.org7http://banglalibrary.evergreenbangla.om/

44



and learning and (b) the e�ort of limited data in authorship detetion.Clusters Authors No. of douments No. of tokens No. of unique tokensRabindranathCluster 1 Tagore 150 6,862,580 4,978,672(Author R)Sarat ChandraCluster 2 Chottopadyhay 150 4,083,417 2,987,450(Author A)Cluster 3 Others 150 3,818,216 2,657,813(Author O)Table 2: Statistis of the used dataset3.2 Baseline system (BL)In order to set up a baseline system, we use traditional lexial-based methodology alledvoabulary rihness (VR) (Holmes, 2004) whih is basially the type-token ratio (V/N),where V is the size of the voabulary of the sample text and N is the number of tokenswhih forms the simple text. By using nearest-neighbor algorithm, the baseline system triesto map eah of the testing douments to one author. We have also ompared our approahwith the state-of-the-art method proposed by (Das and Mitra, 2011). The results of thebaseline systems are depited using onfusion matries in Table 3.Voabulary rihness (VR) (Das and Mitra, 2011)R A O e(error) in % R A O e(error) in %R 26 14 10 48% 31 9 10 38%A 17 21 12 58% 18 30 2 40%O 16 20 14 72% 10 6 34 32%Avg. error 56% Avg. error 36.67%Table 3: Confusion matries of two baseline system (orret mappings are italiized diago-nally).3.3 Performanes of two di�erent modelsThe onfusion matries in Table 4 desribe the auray of the statistial measures and theresults of their ombined voting. The auray of the majority voting tehnique is 67.3%whih is relatively better than others. Sine the attributes tested are ontinuous, all thedeision trees are onstruted using the fuzzy threshold parameter, so that the knife-edgebehavior for deision trees is softened by onstruting an interval lose to the threshold. Forneural network, many strutures of the multilayer network were experimented with beforewe ame up with our best network. Bakpropogation feed forward networks yield the bestresult with the following arhiteture: 14 input nodes, 8 nodes on the �rst hidden layer, 6nodes on the seond hidden layer, and 6 output nodes (to at as error orreting odes).Two output nodes are allotted to a single author (this inreases the Hamming distanebetween the lassi�ations - the bit string that is output with eah bit orresponding toone author in the lassi�ation- of any two authors, thus dereasing the possibility ofmislassi�ation). Out of 100 training samples, 30% are used in the validation set whihdetermines whether over-�tting has ourred and when to stop training. It is worth noting
45



that the reported results are the average of 10-fold ross validations. We will disuss theomparative results of individual ross validation phase in the next setion. Table 5 reportsthe error rate of individual model in three onfusion matries. At a glane, mahine learningapproahes espeially SVM (83.3% auray) perform tremendously well ompared to theother models. Statistial similarity modelsCosine similarity Chi-square measure Eulidean distane Majority voting(COS) (CS) (ED) (COM)R A O e(%) R A O e(%) R A O e(%) R A O e(%)R 30 12 8 40 34 9 7 32 27 15 8 46 34 7 9 28A 15 27 8 46 14 30 6 40 18 26 6 48 11 32 7 36O 12 9 29 42 9 8 33 34 17 6 27 46 6 11 33 34Avg. error 42.7 Avg. error 35.3 Avg. error 46.6 Avg. error 32.7Table 4: Confusion matries of statistial similarity measures on test set.Mahine Learning modelsDeision Tree Neural Networks Support Vetor MahineR A O e(%) R A O e(%) R A O e(%)R 35 8 6 28 38 9 3 24 44 3 3 12A 7 37 6 26 10 35 5 30 8 40 2 20O 6 5 39 22 9 5 36 28 2 7 41 18Avg. error 25.3 Avg. error 27.3 Avg. error 16.7Table 5: Confusion matries of mahine learning models on test set (averaged over 10-foldross validations).3.4 Comparative analysisThe performane of any mahine learning tool highly depends on the population and di-vergene of training samples. Limited dataset an overshadowed the intrinsi produtivityof the tool. Beause of the lak of large number of dataset, we divide the training datarandomly into 10 sets and use 10-fold ross validation tehnique to prevent over�tting foreah mahine learning model. The boxplot in Figure 2(a) reports the performane of eahmodel on 10-fold ross validation phrase with mean auray and variane. In three ases,sine the nothes in the box plots overlap, we an onlude, with ertain on�dene, thatthe true medians do not di�er. The outliers are marked separately with the dotted points.The di�erene between lower and upper quartiles in SVM is omparatively smaller thanthe others that shows relative low variane of auraies in di�erent iterations.We also measure the pairwise agreement in mapping three types of authors using Cohen'sKappa oe�ient (Cohen, 1960). In Figure 2(b), the high orrelation between Deision Treeand Neural Network models, whih is onsiderably high ompared to the others signi�esthat the e�ets of both of these models in author-doument mapping task are reasonablyidential and less e�ient ompared to SVM model.As a pioneer of studying di�erent mahine learning models in Bengali authorship task, itis worth measuring the relative importane of individual feature in eah learning modelthat gets some features high privilege and helps in feature ranking. We have dropped eah
46



           
 

 

 

   
 

 

 

  

0.65

0.7

0.75

A
vg

. i
nt

er
−

m
od

el
 a

gr
ee

m
en

t

(b)

72

74

76

78

80

82

84

A
ve

ra
ge

 a
cc

ur
ac

y 
(in

 %
)

(a)

DT vs. SVMDT vs. NNSVMDT NN vs. SVMNNFigure 2: (a) Boxplot of average auray (in %) of three mahine learning modules on10-fold ross validations; (b) pair-wise average inter-model agreement of the models usingCohen's Kappa measure.
L(w) KW(R) KW(A) KW(O) HL Punc. NP VP CP UN RE Dig L(d) L(p)  

 

60

 

 

60

 

60

 

Dropped features

V
ar

ia
tio

n 
of

 a
cc

ur
ac

y

 

 

DT
SVM
NN

Figure 3: (Color online) Average auray after deleting features one at a time (the mag-nitude of the error bar indiates the di�erene of the auraies before and after droppingone feature for eah mahine learning model).feature one by one and pointed out its relative impat on auray over 10-fold ross vali-dations. The points against eah feature in the line graphs in Figure 3 show perentage ofauray when that feature is dropped, and the magnitude of the orresponding error barmeasures the di�erene between �nal auray (when all features present) and aurayafter dropping that feature. All models rely on the high importane of length of the wordin this task. All of them also reah to the ommon onsensus of the importane of KW(R),KW(A), KW(O), NP and CP. But few of the features typially re�et unpreditable signa-tures in di�erent models. For instane, length of the dialog and unknown word ount showlarger signi�ane in SVM, but they are not so signi�ant in other two models. Similarharateristis are also observed in Deision tree and Neural network models.Finally, we study the responsibility of individual authors for produing erroneous results.Figure 4 depits that almost in every ase, the system has little overestimated the authorsof douments as author R. It may our due to the aquisition of douments beause thedouments in luster 2 and luster 3 are not so diverse and well-strutured as the doumentsof Rabindranath Tagore. Developing appropriate orpus for this study is itself a separate
47



researh area speially when dealing with learning modules, and it takes huge amount oftime. The more the fous will be on this language, the more we expet to get divergeorpus of di�erent Bengali writers.
VR COS CS ED COM DT NN SVM

0

10

20

30

40

50

Different models

E
rr

or
 r

at
e

 

 

Author R
Author A
Author O

Figure 4: (Color online) Error analysis: perentage of error ours due to wrong identi�edauthors.4 Conlusion and Future workThis paper attempts to demonstrate the mehanism to reognize three authors in Bengaliliterature based on their style of writing (without taking into aount the author's pro�le,genre or writing time). We have inorporated both statistial similarity based measures andthree mahine learning models over same feature sets and ompared them with the baselinesystem. All of the mahine learning models espeially SVM yield a signi�antly higherauray than other models. Although the SVM yielded a better numerial performane,and are onsidered inherently suitable to apture an intangible onept like style, thedeision trees are human readable making it possible to de�ne style. While more featuresould produe additional disriminatory material, the present study proves that arti�ialintelligene provides stylometry with exellent lassi�ers that require fewer and relevantinput variables than traditional statistis. We also showed that the signi�ane of the usedfeatures in authorship identi�ation task are relative to the used model. This preliminarystudy is the journey to reveal the intrinsi style of writing of the Bengali authors basedupon whih we plan to build more robust, generi and diverge authorship identi�ationtool.ReferenesArgamon, S., ari¢, M., and Stein, S. S. (2003). Style mining of eletroni messages formultiple authorship disrimination: �rst results. In KDD '03: Proeedings of the ninthACM SIGKDD international onferene on Knowledge disovery and data mining, pages475�480. ACM.Chakaraborty, T. and Bandyopadhyay, S. (2011). Inferene of �ne-grained attributes ofbengali orpus for stylometry detetion. pages 79�83.Chanda, S., Franke, K., Pal, U., and Wakabayashi, T. (2010). Text independent writeridenti�ation for bengali sript. In Proeedings of the 2010 20th International Confer-
48



ene on Pattern Reognition, ICPR '10, pages 2005�2008, Washington, DC, USA. IEEEComputer Soiety.Cohen, J. (1960). A Coe�ient of Agreement for Nominal Sales. Eduational and Psy-hologial Measurement, 20(1):37�46.Das, S. and Mitra, P. (2011). Author identi�ation in bengali literary works. In Proeed-ings of the 4th international onferene on Pattern reognition and mahine intelligene,PReMI'11, pages 220�226, Berlin, Heidelberg. Springer-Verlag.Holmes, D. (2004). Review: Attributing authorship: An introdution. LLC, 19(4):528�530.Krippendor�, K. (2003). Content Analysis: An Introdution to Its Methodology. SAGEPubliations.Malyutov, M. B. (2006). General theory of information transfer and ombinatoris. hap-ter Authorship attribution of texts: a review, pages 362�380. Springer-Verlag, Berlin,Heidelberg.Merriam, T. (1998). Heterogeneous authorship in early shakespeare and the problem ofhenry v. Literary and Linguisti Computing, 13(1):15�27.Pavele, D., Justino, E. J. R., and Oliveira, L. S. (2007). Author identi�ation using sty-lometri features. Inteligenia Arti�ial, Revista Iberoameriana de Inteligenia Arti�ial,11(36):59�66.Rudman, J. (1997). The State of Authorship Attribution Studies: Some Problems andSolutions. Computers and the Humanities, 31(4):351�365.Stamatatos, E. (2009). A survey of modern authorship attribution methods. J. Am. So.Inf. Si. Tehnol., 60(3):538�556.Stamatatos, E., Fakotakis, N., and Kokkinakis, G. (1999). Automati authorship attribu-tion. In Proeedings of the ninth onferene on European hapter of the Assoiation forComputational Linguistis, pages 158�164. Assoiation for Computational Linguistis.Vapnik, V. N. (1995). The nature of statistial learning theory. Springer-Verlag New York,In., New York, NY, USA.Zhang, T., Damerau, F., and Johnson, D. (2002). Text hunking based on a generalizationof winnow. Journal of Mahine Learning Researh, 2:615�637.

49




