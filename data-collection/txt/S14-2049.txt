



















































haLF: Comparing a Pure CDSM Approach with a Standard Machine Learning System for RTE


Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 300–304,
Dublin, Ireland, August 23-24, 2014.

haLF: Comparing a Pure CDSM Approach with a Standard
Machine Learning System for RTE

Lorenzo Ferrone
University of Rome “Tor Vergata”

Via del Politecnico 1
00133 Roma, Italy

lorenzo.ferrone@gmail.com

Fabio Massimo Zanzotto
University of Rome “Tor Vergata”

Via del Politecnico 1
00133 Roma, Italy

fabio.massimo.zanzotto@uniroma2.it

Abstract

In this paper, we describe our sub-
mission to the Shared Task #1. We
tried to follow the underlying idea of
the task, that is, evaluating the gap
of full-fledged recognizing textual en-
tailment systems with respect to com-
positional distributional semantic mod-
els (CDSMs) applied to this task. We
thus submitted two runs: 1) a sys-
tem obtained with a machine learning
approach based on the feature spaces
of rules with variables and 2) a sys-
tem completely based on a CDSM that
mixes structural and syntactic infor-
mation by using distributed tree ker-
nels. Our analysis shows that, under
the same conditions, the fully CDSM
system is still far from being competi-
tive with more complex methods.

1 Introduction

Recognizing Textual Entailment is a largely
explored problem (Dagan et al., 2013). Past
challenges (Dagan et al., 2006; Bar-Haim et
al., 2006; Giampiccolo et al., 2007) explored
methods and models applied in complex and
natural texts. In this context, machine learn-
ing solutions show interesting results. The
Shared Task #1 of SemEval instead wants to
explore systems in a more controlled textual
environment where the phenomena to model
are clearer. The aim of the Shared Task is to
study how RTE systems built upon composi-
tional distributional semantic models behave

This work is licenced under a Creative Commons At-
tribution 4.0 International License. Page numbers and
proceedings footer are added by the organizers. License
details: http://creativecommons.org/licenses/by/
4.0/

with respect to the above tradition. We tried
to capture this underlying idea of the task.

In this paper, we describe our submission
to the Shared Task #1. We tried to fol-
low the underlying idea of the task, that is,
evaluating the gap of full-fledged recognizing
textual entailment systems with respect to
compositional distributional semantic models
(CDSMs) applied to this task. We thus sub-
mitted two runs: 1) a system obtained with a
machine learning approach based on the fea-
ture spaces of rules with variables (Zanzotto
et al., 2009) and 2) a system completely based
on a CDSM that mixes structural and syntac-
tic information by using distributed tree ker-
nels (Zanzotto and Dell’Arciprete, 2012). Our
analysis shows that, under the same condi-
tions, the fully CDSM system is still far from
being competitive with more complete meth-
ods.

The rest of the paper is organized as follows.
Section 2 describes the full-fledged recognizing
textual entailment system that is used for com-
parison. Section 3 introduces a novel composi-
tional distributional semantic model, namely,
the distributed smoothed tree kernels, and the
way this model is applied to the task of RTE.
Section 4 describes the results in the challenge
and it draws some preliminary conclusions.

2 A Standard full-fledged Machine
Learning Approach for RTE

For now on, the task of recognizing textual en-
tailment (RTE) is defined as the task to decide
if a pair p = (a, b) like:

(“Two children are lying in the snow and are
making snow angels”, “Two angels are
making snow on the lying children”)

is in entailment, in contradiction, or neutral.
As in the tradition of applied machine learn-

300



ing models, the task is framed as a multi-
classification problem. The difficulty is to de-
termine the best feature space on which to
train the classifier.

A full-fledged RTE systems based on ma-
chine learning that has to deal with natural
occurring text is generally based on:

• some within-pair features that model the
similarity between the sentence a and the
sentence b

• some features representing more complex
information of the pair (a, b) such as rules
with variables that fire (Zanzotto and
Moschitti, 2006)

In the following, we describe the within-pair
feature and the syntactic rules with variable
features used in the full-fledged RTE system.

As the second space of features is generally
huge, the full feature space is generally used in
kernel machines where the final kernel between
two instances p1 = (a1, b1) and p2 = (a2, b2) is:

K(p1, p2) = FR(p1, p2) +
+ (WTS(a1, b1) ·WTS(a2, b2) + 1)2

where FR counts how many rules are in com-
mon between p1 and p2 and WTS computes a
lexical similarity between a and b. In the fol-
lowing sections we describe the nature ofWTS
and of FR

2.1 Weighted Token Similarity (WTS)

This similarity model was first defined bt Cor-
ley and Mihalcea (2005) and since then has
been used by many RTE systems. The model
extends a classical bag-of-word model to a
Weighted-Bag-of-Word (wbow) by measuring
similarity between the two sentences of the
pair at the semantic level, instead of the lexical
level.

For example, consider the pair: “Os-
cars forgot Farrah Fawcett”, “Farrah Fawcett
snubbed at Academy Awards”. This pair is
redundant, and, hence, should be assigned
a very high similarity. Yet, a bag-of-word
model would assign a low score, since many
words are not shared across the two sen-
tences. wbow fixes this problem by match-
ing ‘Oscar’-‘Academy Awards’ and ‘forgot’-
‘snubbed’ at the semantic level. To provide

these matches, wbow relies on specific word
similarity measures over WordNet (Miller,
1995), that allow synonymy and hyperonymy
matches: in our experiments we specifically
use Jiang&Conrath similarity (Jiang and Con-
rath, 1997).

2.2 Rules with Variables as Features

The above model alone is not sufficient to
capture all interesting entailment features as
the relation of entailment is not only related
to the notion of similarity between a and b.
In the tradition of RTE, an interesting feature
space is the one where each feature represents
a rule with variables, i.e. a first order rule
that is activated by the pairs if the variables
are unified. This feature space has been
introduced in (Zanzotto and Moschitti, 2006)
and shown to improve over the one above.
Each feature 〈fr1, fr2〉 is a pair of syntactic
tree fragments augmented with variables.
The feature is active for a pair (t1, t2) if the
syntactic interpretations of t1 and t2 can
be unified with < fr1, fr2 >. For example,
consider the following feature:

〈

S
PPP���

NP X VP
HH��

VBP

bought

NP Y
,

S
PPP���

NP X VP
HH��

VBP
owns

NP Y
〉

This feature is active for the pair (“GM bought
Opel”,“GM owns Opel”), with the variable
unification X = “GM ” and Y = “Opel”. On
the contrary, this feature is not active for the
pair (“GM bought Opel”,“Opel owns GM ”) as
there is no possibility of unifying the two vari-
ables.

FR(p1, p2) is a kernel function that counts
the number of common rules with variables
between p1 and p2. Efficient algorithms for
the computation of the related kernel func-
tions can be found in (Moschitti and Zanzotto,
2007; Zanzotto and Dell’Arciprete, 2009; Zan-
zotto et al., 2011).

301



S(t) = {
S:booked::v

QQ��
NP VP

,
VP:booked::v

ZZ��
V NP

,
NP:we::p

PRP
,

S:booked::v
ZZ��

NP

PRP

VP , . . . ,

VP:booked::v
HH��

V

booked

NP

DT NN

, . . . }

Figure 1: Subtrees of the tree t in Figure 2 (a non-exhaustive list.)

3 Distributed Smoothed Tree
Kernel: a Compositional
Distributional Semantic Model
for RTE

The above full-fledged RTE system, although
it may use distributional semantics, is not a
model that applies a compositional distribu-
tional semantic model as it does not explic-
itly transform sentences in vectors, matrices,
or tensors that represent their meaning.

We here propose a model that can be con-
sidered a compositional distributional seman-
tic model as it transforms sentences into ma-
trices that are then used by the learner as fea-
ture vectors. Our model is called Distributed
Smoothed Tree Kernel (Ferrone and Zanzotto,
2014) as it mixes the distributed trees (Zan-
zotto and Dell’Arciprete, 2012) representing
syntactic information with distributional se-
mantic vectors representing semantic informa-
tion. The computation of the final matrix for
each sentence is done compositionally.

S:booked::v````̀
     

NP:we::p

PRP:we::p

We

VP:booked::vXXXX
����

V:booked::v

booked

NP:flight::n
PPP���

DT:the::d

the

NN:flight::n

flight

Figure 2: A lexicalized tree.

3.1 Notation

Before describing the distributed smoothed
trees (DST) we introduce a formal way to de-
note constituency-based lexicalized parse trees,
as DSTs exploit this kind of data structures.
Lexicalized trees are denoted with the letter t
and N(t) denotes the set of non terminal nodes
of tree t. Each non-terminal node n ∈ N(t)
has a label ln composed of two parts ln =
(sn, wn): sn is the syntactic label, while wn is
the semantic headword of the tree headed by

n, along with its part-of-speech tag. Termi-
nal nodes of trees are treated differently, these
nodes represent only words wn without any
additional information, and their labels thus
only consist of the word itself (see Fig. 2).
The structure of a DST is represented as fol-
lows: Given a tree t, h(t) is its root node and
s(t) is the tree formed from t but considering
only the syntactic structure (that is, only the
sn part of the labels), ci(n) denotes i-th child
of a node n. As usual for constituency-based
parse trees, pre-terminal nodes are nodes that
have a single terminal node as child.

Finally, we use
→
wn ∈ Rk to denote the distri-

butional vector for word wn, whereas T repre-
sents the matrix of a tree t encoding structure
and distributional meaning.

3.2 The Method in a Glance

We describe here the approach in a few sen-
tences. In line with tree kernels over struc-
tures (Collins and Duffy, 2002), we introduce
the set S(t) of the subtrees ti of a given lexi-
calized tree t. A subtree ti is in the set S(t) if
s(ti) is a subtree of s(t) and, if n is a node in
ti, all the siblings of n in t are in ti. For each
node of ti we only consider its syntactic label
sn, except for the head h(ti) for which we also
consider its semantic component wn (see Fig.
1). The functions DSTs we define compute the
following:

DST (t) = T =
∑

ti∈S(t)
Ti

where Ti is the matrix associated to each sub-
tree ti. The similarity between two text frag-
ments a and b represented as lexicalized trees
ta and tb can be computed using the Frobenius
product between the two matrices Ta and Tb,
that is:

〈Ta,Tb〉F =
∑

tai ∈S(ta)
tbj∈S(tb)

〈Tai ,Tbj〉F (1)

302



We want to obtain that the product 〈Tai ,Tbj〉F
approximates the dot product between the
distributional vectors of the head words

(〈Tai ,Tbj〉F ≈ 〈
→

h(tai ),
→

h(tbj)〉) whenever the syn-
tactic structure of the subtrees is the same
(that is s(tai ) = s(t

b
j)), and 〈Tai ,Tbj〉F ≈ 0 oth-

erwise. This property is expressed as:

〈Tai ,Tbj〉F ≈ δ(s(tai ), s(tbj)) · 〈
→

h(tai ),
→

h(tbj)〉 (2)

To obtain the above property, we define

Ti =
→

s(ti)
→

wh(ti)
>

where
→

s(ti) are distributed tree fragment
(Zanzotto and Dell’Arciprete, 2012) for the
subtree t and

→
wh(ti) is the distributional

vector of the head of the subtree t. Dis-
tributed tree fragments have the property

that
→

s(ti)
→

s(tj) ≈ δ(ti, tj). Thus, given the
important property of the outer product
that applies in the Frobenius product:

〈→a→w>,
→
b
→
v
>〉F = 〈→a ,

→
b 〉 · 〈→w,→v 〉. we have that

Equation 2 is satisfied as:

〈Ti,Tj〉F = 〈
→

s(ti),
→

s(tj)〉 · 〈 →wh(ti),
→

wh(tj)〉
≈ δ(s(ti), s(tj)) · 〈 →wh(ti),

→
wh(tj)〉

It is possible to show that the overall com-
positional distributional model DST (t) can be
obtained with a recursive algorithm that ex-
ploit vectors of the nodes of the tree.

The compositional distributional model is
then used in the same learning machine used
for the traditional RTE system with the fol-
lowing kernel function:

K(p1, p2) =
〈DST (a1), DST (a2)〉+ 〈DST (b1), DST (b2)〉+

+(WTS(a1, b1) ·WTS(a2, b2) + 1)2

4 Results and Conclusions

For the submission we used the java ver-
sion of LIBSVM (Chang and Lin, 2011).
Distributional vectors are derived with
DISSECT (Dinu et al., 2013) from a
corpus obtained by the concatenation of
ukWaC (wacky.sslmit.unibo.it), a mid-
2009 dump of the English Wikipedia

Model Accuracy (3-ways)
DST 69.42

full-fledged RTE System 75.66
Max 84.57
Min 48.73

Average 75.35

Table 1: Accuracies of the two systems on the
test set, together with the maximum, mini-
mum and average score for the challenge.

(en.wikipedia.org) and the British Na-
tional Corpus (www.natcorp.ox.ac.uk), for a
total of about 2.8 billion words. The raw co-
occurrences count vectors were transformed
into positive Pointwise Mutual Information
scores and reduced to 300 dimensions by
Singular Value Decomposition. This setup
was picked without tuning, as we found it
effective in previous, unrelated experiments.

We parsed the sentence with the Stanford
Parser (Klein and Manning, 2003) and ex-
tracted the heads for use in the lexicalized
trees with Collins’ rules (Collins, 2003).

Table 1 reports our results on the textual en-
tailment classification task, together with the
maximum, minimum and average score for the
challenge. The first observation is that the
full-fledged RTE system is still definitely bet-
ter than our CDSM system. We believe that
the main reason is that the DST cannot en-
code variables which is an important aspect
to capture when dealing with textual entail-
ment recognition. This is particularly true
for this dataset as it focuses on word order-
ing and on specific and recurrent entailment
rules. Our full-fledged system scored among
the first 10 systems, slightly above the over-
all average score, but our pure CDSM system
is instead ranked within the last 3. We think
that a more in-depth comparison with other
fully CDSM systems will give us a better in-
sight on our model and will also assess more
realistically the quality of our system.

References

Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and
Idan Szpektor. 2006. The second pascal recog-
nising textual entailment challenge. In Proceed-
ings of the Second PASCAL Challenges Work-

303



shop on Recognising Textual Entailment. Venice,
Italy.

Chih-Chung Chang and Chih-Jen Lin. 2011.
LIBSVM: A library for support vector ma-
chines. ACM Transactions on Intelligent Sys-
tems and Technology, 2:27:1–27:27. Soft-
ware available at http://www.csie.ntu.edu.
tw/~cjlin/libsvm.

Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels
over discrete structures, and the voted percep-
tron. In Proceedings of ACL02.

Michael Collins. 2003. Head-driven statistical
models for natural language parsing. Comput.
Linguist., 29(4):589–637.

Courtney Corley and Rada Mihalcea. 2005. Mea-
suring the semantic similarity of texts. In Proc.
of the ACL Workshop on Empirical Modeling
of Semantic Equivalence and Entailment, pages
13–18. Association for Computational Linguis-
tics, Ann Arbor, Michigan, June.

Ido Dagan, Oren Glickman, and Bernardo
Magnini. 2006. The pascal recognising tex-
tual entailment challenge. In Quionero-Candela
et al., editor, LNAI 3944: MLCW 2005, pages
177–190. Springer-Verlag, Milan, Italy.

Ido Dagan, Dan Roth, Mark Sammons, and
Fabio Massimo Zanzotto. 2013. Recognizing
Textual Entailment: Models and Applications.
Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool Publishers.

Georgiana Dinu, Nghia The Pham, and Marco
Baroni. 2013. DISSECT: DIStributional SE-
mantics Composition Toolkit. In Proceedings
of ACL (System Demonstrations), pages 31–36,
Sofia, Bulgaria.

Lorenzo Ferrone and Fabio Massimo Zanzotto.
2014. Towards syntax-aware compositional dis-
tributional semantic models. In Proceedings of
Coling 2014. COLING, Dublin, Ireland, Aug 23–
Aug 29.

Danilo Giampiccolo, Bernardo Magnini, Ido Da-
gan, and Bill Dolan. 2007. The third pas-
cal recognizing textual entailment challenge. In
Proceedings of the ACL-PASCAL Workshop on
Textual Entailment and Paraphrasing, pages 1–
9. Association for Computational Linguistics,
Prague, June.

Jay J. Jiang and David W. Conrath. 1997. Seman-
tic similarity based on corpus statistics and lex-
ical taxonomy. In Proc. of the 10th ROCLING,
pages 132–139. Tapei, Taiwan.

Dan Klein and Christopher D. Manning. 2003.
Accurate unlexicalized parsing. In Proceedings
of the 41st Annual Meeting on Association for

Computational Linguistics - Volume 1, ACL ’03,
pages 423–430, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

George A. Miller. 1995. WordNet: A lexical
database for English. Communications of the
ACM, 38(11):39–41, November.

Alessandro Moschitti and Fabio Massimo Zan-
zotto. 2007. Fast and effective kernels for re-
lational learning from texts. In Proceedings of
the International Conference of Machine Learn-
ing (ICML). Corvallis, Oregon.

Fabio Massimo Zanzotto and Lorenzo
Dell’Arciprete. 2009. Efficient kernels for
sentence pair classification. In Conference
on Empirical Methods on Natural Language
Processing, pages 91–100, 6-7 August.

F.M. Zanzotto and L. Dell’Arciprete. 2012. Dis-
tributed tree kernels. In Proceedings of Interna-
tional Conference on Machine Learning, pages
193–200.

Fabio Massimo Zanzotto and Alessandro Mos-
chitti. 2006. Automatic learning of textual en-
tailments with cross-pair similarities. In Pro-
ceedings of the 21st Coling and 44th ACL, pages
401–408. Sydney, Australia, July.

Fabio Massimo Zanzotto, Marco Pennacchiotti,
and Alessandro Moschitti. 2009. A machine
learning approach to textual entailment recog-
nition. NATURAL LANGUAGE ENGINEER-
ING, 15-04:551–582.

Fabio Massimo Zanzotto, Lorenzo Dell’Arciprete,
and Alessandro Moschitti. 2011. Efficient graph
kernels for textual entailment recognition. Fun-
damenta Informaticae, 107(2-3):199 – 222.

304


