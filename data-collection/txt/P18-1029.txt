



















































Zero-shot Learning of Classifiers from Natural Language Quantification


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 306–316
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

306

Zero-shot Learning of Classifiers from Natural Language Quantification

Shashank Srivastava Igor Labutov Tom Mitchell
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA

ssrivastava@cmu.edu ilabutov@cs.cmu.edu tom.mitchell@cmu.edu

Abstract

Humans can efficiently learn new concepts
using language. We present a framework
through which a set of explanations of a
concept can be used to learn a classifier
without access to any labeled examples.
We use semantic parsing to map explana-
tions to probabilistic assertions grounded
in latent class labels and observed attributes
of unlabeled data, and leverage the differen-
tial semantics of linguistic quantifiers (e.g.,
‘usually’ vs ‘always’) to drive model train-
ing. Experiments on three domains show
that the learned classifiers outperform pre-
vious approaches for learning with limited
data, and are comparable with fully super-
vised classifiers trained from a small num-
ber of labeled examples.

1 Introduction

As computer systems that interact with us in nat-
ural language become pervasive (e.g., Siri, Alexa,
Google Home), they suggest the possibility of let-
ting users teach machines in language. The abil-
ity to learn from language can enable a paradigm
of ubiquitous machine learning, allowing users to
teach personalized concepts (e.g., identifying ‘im-
portant emails’ or ‘project-related emails’) when
limited or no training data is available.

In this paper, we take a step towards solving
this problem by exploring the use of quantifiers to
train classifiers from declarative language. For il-
lustration, consider the hypothetical example of
a user explaining the concept of an “important
email” through natural language statements (Fig-
ure 1). Our framework takes a set of such natural
language explanations describing a concept (e.g.,
“emails that I reply to are usually important”) and
a set of unlabeled instances as input, and produces

Figure 1: Supervision from language can enable
concept learning from limited or even no labeled
examples. Our approach assumes the learner has
sensors that can extract attributes from data, such
as those listed in the table, and language that can
refer to these sensors and their values.

a binary classifier (for important emails) as output.
Our hypothesis is that language describing con-
cepts encodes key properties that can aid statistical
learning. These include specification of relevant
attributes (e.g., whether an email was replied to),
relationships between such attributes and concept
labels (e.g., if a reply implies the class label of
that email is ‘important’), as well as the strength of
these relationships (e.g., via quantifiers like ‘often’,
‘sometimes’, ‘rarely’). We infer these properties
automatically, and use the semantics of linguis-
tic quantifiers to drive the training of classifiers
without labeled examples for any concept. This
is a novel scenario, where previous approaches in
semi-supervised and constraint-based learning are
not directly applicable. Those approaches require
manual pre-specification of expert knowledge for
model training. In our approach, this knowledge is
automatically inferred from noisy natural language
explanations from a user.

Our approach is summarized in the schematic
in Figure 2. First, we map the set of natural lan-
guage explanations of a concept to logical forms



307

Figure 2: Our approach to Zero-shot learning from
Language. Natural language explanations on how
to classify concept examples are parsed into for-
mal constraints relating features to concept labels.
The constraints are combined with unlabeled data,
using posterior regularization to yield a classifier.

that identify the attributes mentioned in the explana-
tion, and describe the information conveyed about
the attribute and the concept label as a quantitative
constraint. This mapping is done through semantic
parsing. The logical forms denote quantitative con-
straints, which are probabilistic assertions about
observable attributes of the data and unobserved
concept labels. Here the strength of a constraint
is assumed to be specified by a linguistic quanti-
fier (such as ‘all’, ‘some’, ‘few’, etc., which reflect
degrees of generality of propositions). Next, we
train a classification model that can assimilate these
constraints by adapting the posterior regularization
framework (Ganchev et al., 2010).

Intuitively, this can be seen as defining an op-
timization problem, where the objective is to find
parameter estimates for the classifier that do not
simply fit the data, but also agree with the human
provided natural language advice to the greatest ex-
tent possible. Since logical forms can be grounded
in a variety of sensors and external resources, an
explicit model of semantic interpretation concep-
tually allows the framework to subsume a flexible
range of grounding behaviors. The main contribu-
tions of this work are:

1. We introduce the problem of zero-shot learn-
ing of classifiers from language, and present
an approach towards this.

2. We develop datasets for zero-shot classifi-
cation from natural descriptions, exhibiting

tasks with various levels of difficulty.
3. We empirically show that coarse probability

estimates to model linguistic quantifiers can
effectively supervise model training across
three domains of classification tasks.

2 Related Work

Many notable approaches have explored incorpo-
ration of background knowledge into the training
of learning algorithms. However, none of them ad-
dresses the issue of learning from natural language.
Prominent among these are the Constraint-driven
learning (Chang et al., 2007a), Generalized Expec-
tation (Mann and McCallum, 2010) and Posterior
Regularization (Ganchev et al., 2010) and Bayesian
Measurements (Liang et al., 2009) frameworks. All
of these require domain knowledge to be manually
programmed in before learning. Similarly, Prob-
abilistic Soft Logic (Kimmig et al., 2012) allows
users to specify rules in a logical language that can
be used for reasoning over graphical models. More
recently, multiple approaches have explored few-
shot learning from perspective of term or attribute-
based transfer (Lampert et al., 2014), or learning
representations of instances as probabilistic pro-
grams (Lake et al., 2015).

Other work (Lei Ba et al., 2015; Elhoseiny et al.,
2013) considers language terms such as colors and
textures that can be directly grounded in visual
meaning in images. Some previous work (Srivas-
tava et al., 2017) has explored using language ex-
planations for feature space construction in concept
learning tasks, where the problem of learning to in-
terpret language, and learning classifiers is treated
jointly. However, this approach assumes availabil-
ity of labeled data for learning classifiers. Also
notable is recent work by Andreas et al. (2017),
who propose using language descriptions as param-
eters to model structure in learning tasks in multiple
settings. More generally, learning from language
has also been previously explored in tasks such
as playing games (Branavan et al., 2012), robot
navigation (Karamcheti et al., 2017), etc.

Natural language quantification has been studied
from multiple perspectives in formal logic (Barwise
and Cooper, 1981), linguistics (Löbner, 1987; Bach
et al., 2013) and cognitive psychology (Kurtzman
and MacDonald, 1993). While quantification has
traditionally been defined in set-theoretic terms in
linguistic theories1, our approach joins alternative

1e.g., ‘some A are B’ ⇔ A ∩B 6= ∅



308

perspectives that represent quantifiers probabilis-
tically (Moxey and Sanford, 1993; Yildirim et al.,
2013). To the best of our knowledge, this is the
first work to leverage the semantics of quantifiers
to guide statistical learning models.

3 Learning Classifiers from Language

Our approach relies on first mapping natural lan-
guage descriptions to quantitative constraints that
specify statistical relationships between observable
attributes of instances and their latent concept la-
bels (Step 1 in Figure 2). These quantitative con-
straints are then imbued into the training of a classi-
fier by guiding predictions from the learned models
to concur with them (Step 2). We use semantic
parsing to interpret sentences as quantitative con-
straints, and adapt the posterior regularization prin-
ciple for our setting to estimate the classifier param-
eters. Next, we describe these steps in detail. Since
learning in this work is largely driven by the seman-
tics of linguistic quantifiers, we call our approach
Learning from Natural Quantification, or LNQ.

3.1 Mapping language to constraints

A key challenge in learning from language is con-
verting free-form language to representations that
can be reasoned over, and grounded in data. For
example, a description such as ‘emails that I re-
ply to are usually important’ may be converted to
a mathematical assertion such as P (important |
replied : true) = 0.7’, which statistical methods
can reason with. Here, we argue that this process
can be automated for a large number of real-world
descriptions. In interpreting statements describing
concepts, we infer the following key elements:

1. Feature x, which is grounded in observed at-
tributes of the data. For our example, ‘emails
replied to’ can refer to a predicate such as
replied:true, which can be evaluated in con-
text of emails to indicate the whether an email
was replied to. Incorporating compositional rep-
resentations enables more complex reasoning.
e.g., ‘the subject of course-related emails usu-
ally mentions CS100’ can map to a composite
predicate like ‘isStringMatch(field:subject,
stringVal(‘CS100’))’ , which can be evaluated
for different emails to reflect whether their sub-
ject mentions ‘CS100’. Mapping language to ex-
ecutable feature functions has been shown to be
effective (Srivastava et al., 2017). For sake of sim-
plicity, here we assume that a statement refers to a

single feature, but the method can be extended to
handle more complex descriptions.
2. Concept label y, specifying the class of in-
stances a statement refers to. For binary classes,
this reduces to examples or non-examples of a con-
cept. For our running example, y corresponds to
the positive class of important emails.
3. Constraint-type asserted by the statement. We
argue that most concept descriptions belong to one
of three categories shown in Table 2, and these con-
stitute our vocabulary of constraint types for this
work. For our running example (‘emails that I reply
to are usually important’), the type corresponds to
P (y | x), since the syntax of the statement indi-
cates an assertion conditioned on the feature indi-
cating whether an email was replied to. On the
other hand, an assertion such as ‘I usually reply
to important emails’ indicates an assertion condi-
tioned on the set important emails, and therefore
corresponds to the type P (x | y).
4. Strength of the constraint. We assume this to
be specified by a quantifier. For our running ex-
ample, this corresponds to the adverb ‘usually’. In
this work, by quantifier we specifically refer to
frequency adverbs (‘usually’,‘rarely’, etc.) and fre-
quency determiners (‘few’, ‘all’, etc.).2 Our thesis
is that the semantics of quantifiers can be leveraged
to make statistical assertions about relationships
involving attributes and concept labels. One way
to do this might be to simply associate point esti-
mates of probability values, suggesting the fraction
of truth values for assertions described with these
quantifiers. Table 1 shows probability values we
assign to some common frequency quantifiers for
English. These values were set simply based on
the authors’ intuition about their semantics, and
do not reflect any empirical distributions. See Fig-
ure 8 for empirical distributions corresponding to
some linguistic quantifiers in our data. While these
probability values maybe inaccurate, and the se-
mantics of these quantifiers may also change based
on context and the speaker, they can still serve as
a strong signal for learning classifiers since they
are not used as hard constraints, but serve to bias
classifiers towards better generalization.

We use a semantic parsing model to map state-
ments to formal semantic representations that spec-
ify these aspects. For example, the statement
‘Emails that I reply to are usually important’ is

2This is a significantly restricted definition, and does not
address non-frequency determiners (e.g.,‘the’, ‘only’, etc. ) or
mass quantifiers (e.g. ‘a lot’, ‘little’), among other categories.



309

Frequency quantifier Probability
all, always, certainly, definitely 0.95
usually, normally, generally,
likely, typically

0.70

most, majority 0.60
often, half 0.50
many 0.40
sometimes, frequently, some 0.30
few, occasionally 0.20
rarely, seldom 0.10
never 0.05

Table 1: Probability values we assign to com-
mon linguistic quantifiers (hyper-parameters for
method)

mapped to a logical form like (x→replied:true
y→positive type:y|x quant:usually).

3.1.1 Semantic Parser components
Given a descriptive statement s, the parsing prob-
lem consists of predicting a logical form l that best
represents its meaning. In turn, we formulate the
probability of the logical form l as decomposing
into three component factors: (i) probability of
observing a feature and concept labels lxy based
on the text of the sentence, (ii) probability of the
type of the assertion ltype based on the identified
feature, concept label and syntactic properties of
the sentence s, and (iii) identifying the linguistic
quantifier, lquant, in the sentence.

P (l | s) = P (lxy | s) P (ltype | lxy, s) P (lquant | s)

We model each of the three components as fol-
lows: by using a traditional semantic parser for the
first component, training a Max-Ent classifier for
the constraint-type for the second component, and
looking for an explicit string match to identify the
quantifier for the third component.
Identifying features and concept labels, lxy: For
identifying the feature and concept label men-
tioned in a sentence, we presume a linear score
S(s, lxy) = wTψ(s, lxy) indicating the goodness
of assigning a partial logical form, lxy, to a sen-
tence s. Here, ψ(s, lxy) ∈ Rn are features that can
depend on both the sentence and the partial logical
form, and w ∈ Rn is a parameter weight-vector for
this component. Following recent work in semantic
parsing (Liang et al., 2011), we assume a loglinear
distribution over interpretations of a sentence.

P (lxy | s) ∝ wTψ(s, lxy)
Provided data consisting of statements labeled
with logical forms, the model can be trained via
maximum likelihood estimation, and be used to
predict interpretations for new statements. For
training this component, we use a CCG semantic
parsing formalism, and follow the feature-set
from Zettlemoyer and Collins (2007), consisting
of simple indicator features for occurrences
of keywords and lexicon entries. This is also
compatible with the semantic parsing formalism in
Srivastava et al. (2017), whose data (and accom-
panying lexicon) are also used in our evaluation.
For other datasets with predefined features, this
component is learned easily from simple lexicons
consisting of trigger words for features and labels.3

This component is the only part of the parser
that is domain-specific. We note that while this
component assumes a domain-specific lexicon
(and possibly statement annotated with logical
forms), this effort is one-time-only, and will
find re-use across the possibly large number of
concepts in the domain (e.g., email categories).

Identifying assertion type, ltype: The principal
novelty in our semantic parsing model is in iden-
tifying the type of constraint asserted by a state-
ment. For this, we train a MaxEnt classifier, which
uses positional and syntactic features based on the
text-spans corresponding to feature and concept
mentions to predict the constraint type. We extract
the following features from a statement:
1. Boolean value indicating whether the text-span
corresponding to the feature x precedes the text
span for the concept label y.
2. Boolean value indicating if sentence is in pas-
sive (rather than active) voice, as identified by the
occurrence of nsubjpass dependency relation.
3. Boolean value indicating whether head of the
text-span for x is a noun, or a verb.
4. Features indicating the occurrence of condi-
tional tokens (‘if’, ‘then’ and ‘that’) preceding or
following text-spans for x and y.
5. Features indicating presence of a linguistic
quantifier in a det or an advmod relation with
syntactic head of x or y.

Since the constraint type is determined by
syntactic and dependency parse features, this

3We also identify whether a feature x is negated, through
the existence of a neg dependency relation with the head of
its text-span. e.g., Important emails are usually not deleted



310

Type Example description Conversion to Expectation Constraint
P (y | x) Emails that I reply to are usually important E[Iy=important,reply(x):true]− pusually × E[Ireply(x):true] = 0
P (x | y) I often reply to important emails E[Iy=important,reply(x):true]− poften × E[Iy=important] = 0
P (y) I rarely get important emails Same as P (y|x0), where x0 is a constant feature

Table 2: Common constraint-types, and their representation as expectations over feature values

component does not need to be retrained for
new domains. In this work, we trained this
classifier based on a manually annotated set of 80
sentences describing classes in the small UCI Zoo
dataset (Lichman, 2013), and used this model for
all experiments.

Identifying quantifiers, lquant: Multiple linguis-
tic quantifiers in a sentence are rare, and we simply
look for the first occurrence of a linguistic quanti-
fier in a sentence, i.e. P (lquant|s) is a deterministic
function. We note that many real world descrip-
tions of concepts lack an explicit quantifier. e.g.,
‘Emails from my boss are important’. In this work,
we ignore such statements for the purpose of train-
ing. Another treatment might be to models these
statements as reflecting a default quantifier, but
we do not explore this direction here. Finally, the
decoupling of quantification from logical represen-
tation is a key decision. At the cost of linguistic
coarseness, this allows modeling quantification ir-
respective of the logical representation (lambda
calculus, predicate-argument structures, etc.).

3.2 Classifier training from constraints
In the previous section, we described how indi-
vidual explanations can be mapped to probabilis-
tic assertions about observable attributes (e.g., the
statement ‘Emails that I reply to are usually impor-
tant’ may map to P (y = important | replied =
true) = pusually). Here, we describe how a set
of such assertions can be used in conjunction with
unlabeled data to train classification models.

Our approach relies on having predictions from
the classifier on a set of unlabeled examples (X =
{x1 . . . xn}) agree with human-provided advice (in
form of constraints). The unobserved concept la-
bels (Y = {y1 . . . yn}) for the unlabeled data con-
stitute latent variables for our method. The train-
ing procedure can be seen as iteratively inferring
the latent concept labels for unlabeled examples
so as to agree with the human advice, and updat-
ing the classification models by taking these labels
as given. While there are multiple approaches for
training statistical models with constraints on latent

variables, here we use the Posterior Regularization
(PR) framework. The PR objective can be used to
optimize a latent variable model subject to a set of
constraints, which specify preferences for values
of the posterior distributions pθ(Y | X).
JQ(θ) = L(θ)−minq∈QKL(q | pθ(Y |X))
Here, the set Q represents a set of preferred pos-

terior distributions over latent variables Y , and is
defined as Q := {qX(Y ) : Eq[φ(X,Y )] ≤ b}.
The overall objective consists of two components,
representing how well does a model θ explain the
data (likelihood term L(θ)), and how far it is from
the set Q (KL-divergence term).

In our case, each parsed statement defines a
probabilistic constraint. The conjunction of all
such constraints defines Q (representing models
that exactly agree with human-provided advice).
Thus, optimizing the objective reflects a tension
between choosing models that increase data
likelihood, and emulating language advice.

Converting to PR constraints: The set of con-
straints that PR can handle can be characterized
as bounds on expected values of functions (φ) of
X and Y (or equivalently, from linearity of expec-
tation, as linear inequalities over expected values
of functions of X and Y ). To use the framework,
we need to ensure that each constraint type in our
vocabulary can be expressed in such a form.

Following the plan in Table 2, each constraint
type can be converted in an equivalent form
Eq[φ(X,Y )] = b, compatible with PR. In partic-
ular, each of these constraint types in our vocab-
ulary can be expressed as equations about expec-
tation values of joint indicator functions of label
assignments to instances and their attributes. To ex-
plain, consider the assertion P (y = important |
replied : true) = pusually. The probability on
the LHS can be expressed as the empirical fraction∑

i E[Iyi=important,replied:true]∑
i E[Ireplied:true]

, which leads to the lin-
ear constraints seen in Table 2 (expected values
in the table hide summations over instances for
brevity). Here, I denote indicator functions. Thus,
we can incorporate probability constraints into our



311

adaptation of the PR scheme.
Learning and Inference: We choose a loglinear
parameterization for the concept classifier.

pθ(yi | xi) ∝ exp(yθTx)
The training of the classifier follows the modified
EM procedure described in Ganchev et al. (2010).
As proposed in the original work, we solve a re-
laxed version of the optimization that allows slack
variables, and modifies the PR objective with a L2
regularizer. This allows solutions even when the
problem is over-constrained, and the setQ is empty
(e.g. due to contradictory advice).

J ′(θ, q) = L(θ)−KL(q|pθ(Y |X))
− λ ||Eq[φ(X,Y )]− b||2

The key step in the training is the computation of
the posterior regularizer in the E-step.
argmin

q
KL(q | pθ) + λ ||Eq[φ(X,Y )]− b||2

This objective is strictly convex, and all constraints
are linear in q. We follow the optimization proce-
dure from Bellare et al. (2009), whereby the min-
imization problem in the E-step can be efficiently
solved through gradient steps in the dual space. In
the M-step, we update the model parameters for the
classifier based on label distributions q estimated
in the E-step. This simply reduces to estimating the
parameters θ for the logistic regression classifier,
when class label probabilities are known. In all
experiments, we run EM for 20 iterations and use
a regularization coefficient of λ = 0.1.

4 Datasets

For evaluating our approach, we created datasets of
classification tasks paired with descriptions of the
classes, as well as used some existing resources. In
this section, we summarize these steps.
Shapes data: To experiment with our approach
in a wider range of controlled settings, part of our
evaluation focuses on synthetic concepts. For this,
we created a set of 50 shape classification tasks that
exhibit a range of difficulty, and elicited language
descriptions spanning a variety of quantifier ex-
pressions. The tasks require classifying geometric
shapes with a set of predefined attributes (fill color,
border, color, shape, size) into two concept-labels
(abstractly named ‘selected shape’, and ‘other’).
The datasets were created through a generative pro-
cess, where features xi are conditionally indepen-
dent given the concept-label. Each feature’s con-
ditional distribution is sampled from a symmetric

(a) Statement generation task

(b) Concept Quiz

Figure 3: Shapes data: Mechanical Turk tasks for
(a) collecting concept descriptions, and (b) human
evaluation from concept descriptions

Dirichlet distribution, and varying the concentra-
tion parameter α allows tuning the noise level of
the generated datasets (quantified via their Bayes
Optimal accuracy4). A dataset is then generated
by sampling from these conditional distributions.
We sample a total of 50 such datasets, consisting
of 100 training and 100 test examples each, where
each example is a shape and its assigned label.

For each dataset, we then collected statements
from Mechanical Turk workers that describe the
concept. The task required turkers to study a sam-
ple of shapes presented on the screen for each of
the two concept-labels (see Figure 3(a)). They were
then asked to write a set of statements that would
help others classify these shapes without seeing the
data. In total, 30 workers participated in this task,
generating a mean of 4.3 statements per dataset.
Email data: Srivastava et al. (2017) provide a
dataset of language explanations from human users
describing 7 categories of emails, as well as 1030
examples of emails belonging to those categories.
While this work uses labeled examples, and focuses

4This is the accuracy of a theoretically optimal classifier,
which knows the true distribution of the data and labels



312

Shapes:
If a shape doesn’t have a blue border, it is prob-
ably not a selected shape.
Selected shapes occasionally have a yellow fill.
Emails:
Emails that mention the word ’meet’ in the sub-
ject are usually meeting requests
Personal reminders almost always have the
same recipient and sender
Birds:
A specimen that has a striped crown is likely to
be a selected bird.
Birds in the other category rarely ever have
dagger-shaped beaks

Table 3: Examples of explanations for each domain

Figure 4: Statement generation task for Birds data

on mapping natural language explanations (∼30
explanations per email category) to compositional
feature functions, we can also use statements in
their data for evaluating our approach. While lan-
guage quantifiers were not studied in the original
work, we found about a third of the statements in
this data to mention a quantifier.
Birds data: The CUB-200 dataset (Wah et al.,
2011) contains images of birds annotated with
observable attributes such as size, primary color,
wing-patterns, etc. We selected a subset of the data
consisting of 10 species of birds and 53 attributes
(60 examples per species). Turkers were shown
examples of birds from a species, and negative
examples consisting of a mix of birds from other

Approach Avg Accuracy Labels Descriptions
LNQ 0.751 no yes
Bayes Optimal 0.831 – –
FLGE+ 0.659 no yes
FLGE 0.598 no yes
LR 0.737 yes no
Random 0.524 – –
Ablation:
LNQ (coarse quant) 0.679 no yes
LNQ (no quant) 0.545 no yes
Human:
Human teacher 0.802 yes writes
Human learner 0.734 no yes

Table 4: Classification performance on Shapes
datasets (averaged over 50 classification tasks).

species, and were asked to describe the classes
(similar to the Shapes data, see Figure 4). During
the task, users also had access to a table enumerat-
ing groundable attributes they could refer to. In all,
60 workers participated, generating 6.1 statements
on average.

5 Experiments

Incorporating constraints from language has not
been addressed before, and hence previous ap-
proaches for learning from limited data such as
Mann and McCallum (2010); Chang et al. (2007b)
would not directly work for this setting. Our base-
lines hence consist of extended versions of previous
approaches that incorporate output from the parser,
as well as fully supervised classifiers trained from
a small number of labeled examples.
Classification performance: The top section in
Table 4 summarizes performance of various clas-
sifiers on the Shape datasets, averaged over all 50
classification tasks. FLGE+ refers to a baseline

Figure 5: LNQ vs Bayes Optimal Classifier perfor-
mance for Shape datasets. Each dot represents a
dataset generated from a known distribution.



313

that uses the Feature Labeling through General-
ized Expectation criterion, following the approach
in Druck et al. (2008); Mann and McCallum (2010).
The approach is based on labeling features are in-
dicating specific class-labels, which corresponds
to specifiying constraints of type P (y|x)5. While
the original approach (Druck et al., 2008) sets this
value to 0.9, we provide the method the quantita-
tive probabilities used by LNQ. Since the original
method cannot handle language descriptions, we
also provide the approach the concept label y and
feature x as identified by the parser. FLGE rep-
resents the version that is not provided quantifier
probabilities. LR refers to a supervised logistic re-
gression model trained on n = 8 randomly chosen
labeled instances.6 We note that LNQ performs
substantially better than both FLGE+ and LR on
average. This validates our modeling principle for
learning classifiers from explanations alone, and
also suggests value in our PR-based formulation,
which can handle multiple constraint types. We
further note that not using quantifier probabilities
significantly deteriorates FLGE’s performance.

Figure 5 provides a more detailed characteriza-
tion of LNQ’s performance. Each blue dot repre-
sents performance on a shape classification task.
The horizontal axis represents the accuracy of the
Bayes Optimal classifier, and the vertical repre-
sents accuracy of the LNQ approach. The blue line
represents the trajectory for x = y, representing a
perfect statistical classifier in the asymptotic case
of infinite samples. We note that LNQ is effective
in learning competent classifiers for all levels of
hardness. Secondly, except for a small number of
outliers, the approach works especially well for
learning easy concepts (towards the right). From
an error-analysis, we found that a majority of these
errors are due to problems in parsing (e.g., missed
negation, incorrect constraint type) or due to poor
explanations from the teacher (bad grammar, or
simply incorrect information).

Figure 6 shows results for email classification
tasks. In the figure, LN* refers to the approach
in Srivastava et al. (2017), which uses natural lan-
guage descriptions to define compositional features
for email classification, but does not incorporate

5In general, Generalized Expectation can also handle
broader constraint types, similar to Posterior Regularization

6LNQ models are indistinct from LR w.r.t. parametriza-
tion, but trained to maximize a different objective. The choice
of n here is arbitrary, but is roughly twice the number of
explanations for each task in this domain

Figure 6: Classification performance (F1) on Email
data. (LN* Results from Srivastava et al. (2017))

supervision from quantification. For this task, we
found very few of the natural language descriptions
to contain quantifiers for some of the individual
email categories, making a direct comparison im-
practical. Thus in this case, we evaluate methods
by combining supervision from descriptions in ad-
dition to 10 labeled examples (also in line with
evaluation in the original paper). We note that addi-
tionally incorporating quantification (LNQ) consis-
tently improves classification performance across
email categories. On this task, LNQ improves upon
FLGE+ and LN* for 6 of the 7 email categories.

Figure 7 shows classification results on the Birds
data. Here, LR refers to a logistic regression model
trained on n=10 examples. The trends in this case
are similar, where LNQ consistently outperforms
FLGE+, and is competitive with LR.
Ablating quantification: From Table 4, we
further observe that the differential associative
strengths of linguistic quantifiers are crucial for
our method’s classification performance. LNQ (no
quant) refers to a variant that assigns the same
probability value (average of values in Table 1),
irrespective of quantifier. This yields a near ran-
dom performance, which is what we’d expect if the
learning is being driven by the differential strengths
of quantifiers. LNQ (coarse quant) refers to a vari-
ant that rounds assigned quantifier probabilities in
Table 1 to 0 or 1. (i.e., quantifiers such are rarely
get mapped to 0, while always gets mapped to a
probability of 1). While its performance (0.679)
suggests that simple binary feedback is a substan-
tial signal, the difference from the full model in-
dicates value in using soft probabilities. On the
other hand, in a sensitivity study, we found the
performance of the approach to be robust to small
changes in the probability values of quantifiers.
Comparison with human performance: For the
Shapes data, we evaluated human teachers’ own
understanding of concepts they teach by evaluating



314

Figure 7: Classification performance on Birds data

them on a quiz based on predicting labels for exam-
ples from the test set (see Figure 3(b)). Second, we
solicit additional workers that were not exposed to
examples from the dataset, and present them only
with the statements describing that data (created
by a teacher), which is comparable supervision to
what LNQ receives. We then evaluate their perfor-
mance at the same task. From Table 4, we note that
a human teacher’s average performance is signifi-
cantly worse (p < 0.05, Wilcoxon signed-rank test)
than the Bayes Optimal classifier indicating that the
teacher’s own synthesis of concepts is noisy. The
human learner performance is expectedly lower,
but interestingly is also significantly worse than
LNQ. While this might be potentially be caused by
factors such as user fatigue, this might also suggest
that automated methods can be better at reasoning
with constraints than humans in certain scenarios.
These results need to be validated through compre-
hensive experiments in more domains.
Empirical semantics of quantifiers: We can es-
timate the distributions of probability values for
different quantifiers from our labeled data. For this,
we aggregate sentences mentioning a quantifier,
and calculate the empirical value of the (condi-
tional) probability associated with the statement,
leading to a set of probability values for each quan-
tifier. Figure 8 shows empirical distributions of
probability values for six quantifiers. We note that
while a few estimates (e.g., ‘rarely’ and ‘often’)
roughly align with pre-registered beliefs, others are
somewhat off (e.g., ‘likely’ shows a much higher
value), and yet others (e.g., ‘sometimes’) show a
large spread of values to be meaningfully modeled
as point values. LNQ’s performance, inspite of
this, shows strong stability in the approach. We
don’t use these empirical probabilities in experi-
ments, (instead of pre-registered values), so as not
to tune the hyperparameters to a specific dataset.

Figure 8: Empirical probability distributions for
six quantifiers (Shapes data). Plots show Beta dis-
tributions with Method-of-Moment estimates. Red
bars correspond to values from Table 1

Such estimates would not be available for a new
task without labeled data. Further, using labeled
data for estimating these probabilities, and then us-
ing the learned model for predicting labels would
constitute overfitting, biasing evaluation.

6 Discussion and Future Work

Our approach is surprisingly effective in learning
from free-form language. However, it does not ad-
dress linguistic issues such as modifiers (e.g., very
likely), nested quantification, etc. On the other
hand, we found no instances of nested quantifi-
cation in the data, suggesting that people might
be primed to use simpler language when teaching.
While we approximate quantifier semantics as abso-
lute probability values, they may vary significantly
based on the context, as shown by cognitive studies
such as Newstead and Collis (1987). Future work
can model how these parameters can be adapted
in a task specific way (e.g., cases such as cancer
prediction where base rates are small), and pro-
vide better models of quantifier semantics. e.g., as
distributions, rather than point values.

Our approach is a step towards the idea of using
language to guide learning of statistical models.
This is an exciting direction, which contrasts with
the predominant theme of using statistical learning
methods to advance the field of NLP. We believe
that language may have as much to help learning,
as statistical learning has helped NLP.

Acknowledgments

This research was supported by the CMU - Yahoo!
InMind project. The authors would also like to
thank the anonymous reviewers for helpful com-
ments and suggestions.



315

References
Jacob Andreas, Dan Klein, and Sergey Levine.

2017. Learning with latent language. CoRR
abs/1711.00482. http://arxiv.org/abs/1711.00482.

Elke Bach, Eloise Jelinek, Angelika Kratzer, and Bar-
bara BH Partee. 2013. Quantification in natural lan-
guages, volume 54. Springer Science & Business
Media.

Jon Barwise and Robin Cooper. 1981. Generalized
quantifiers and natural language. Linguistics and
philosophy 4(2):159–219.

Kedar Bellare, Gregory Druck, and Andrew McCallum.
2009. Alternating projections for learning with ex-
pectation constraints. In Proceedings of the Twenty-
Fifth Conference on Uncertainty in Artificial Intelli-
gence. AUAI Press, pages 43–50.

SRK Branavan, David Silver, and Regina Barzilay.
2012. Learning to win by reading manuals in a
monte-carlo framework. Journal of Artificial Intel-
ligence Research 43:661–704.

Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007a.
Guiding semi-supervision with constraint-driven
learning. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics. Association for Computational Linguis-
tics, Prague, Czech Republic, pages 280–287.
http://www.aclweb.org/anthology/P07-1036.

Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007b.
Guiding semi-supervision with constraint-driven
learning. In ACL. pages 280–287.

Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using gener-
alized expectation criteria. In Proceedings of the
31st annual international ACM SIGIR conference on
Research and development in information retrieval.
ACM, pages 595–602.

Mohamed Elhoseiny, Babak Saleh, and Ahmed Elgam-
mal. 2013. Write a classifier: Zero-shot learning us-
ing purely textual descriptions. In The IEEE Inter-
national Conference on Computer Vision (ICCV).

Kuzman Ganchev, Jennifer Gillenwater, Ben Taskar,
et al. 2010. Posterior regularization for structured
latent variable models. Journal of Machine Learn-
ing Research 11(Jul):2001–2049.

Siddharth Karamcheti, Edward C Williams, Dilip Aru-
mugam, Mina Rhee, Nakul Gopalan, Lawson LS
Wong, and Stefanie Tellex. 2017. A tale of two
draggns: A hybrid approach for interpreting action-
oriented and goal-oriented instructions. arXiv
preprint arXiv:1707.08668 .

Angelika Kimmig, Stephen H. Bach, Matthias
Broecheler, Bert Huang, and Lise Getoor. 2012.
A short introduction to probabilistic soft logic.
In NIPS Workshop on Probabilistic Programming:
Foundations and Applications.

Howard S Kurtzman and Maryellen C MacDonald.
1993. Resolution of quantifier scope ambiguities.
Cognition 48(3):243–279.

Brenden M Lake, Ruslan Salakhutdinov, and Joshua B
Tenenbaum. 2015. Human-level concept learning
through probabilistic program induction. Science
350(6266):1332–1338.

Christoph H Lampert, Hannes Nickisch, and Stefan
Harmeling. 2014. Attribute-based classification for
zero-shot visual object categorization. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence 36(3):453–465.

Jimmy Lei Ba, Kevin Swersky, Sanja Fidler, and Rus-
lan Salakhutdinov. 2015. Predicting deep zero-shot
convolutional neural networks using textual descrip-
tions. In The IEEE International Conference on
Computer Vision (ICCV).

Percy Liang, Michael I Jordan, and Dan Klein. 2009.
Learning from measurements in exponential fami-
lies. In Proceedings of the 26th annual international
conference on machine learning. ACM, pages 641–
648.

Percy Liang, Michael I Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1. Association
for Computational Linguistics, pages 590–599.

M. Lichman. 2013. UCI machine learning repository.
http://archive.ics.uci.edu/ml.

Sebastian Löbner. 1987. Quantification as a major
module of natural language semantics. Studies in
discourse representation theory and the theory of
generalized quantifiers 8:53.

Gideon S Mann and Andrew McCallum. 2010. Gener-
alized expectation criteria for semi-supervised learn-
ing with weakly labeled data. Journal of machine
learning research 11(Feb):955–984.

Linda M Moxey and Anthony J Sanford. 1993. Prior
expectation and the interpretation of natural lan-
guage quantifiers. European Journal of Cognitive
Psychology 5(1):73–91.

Stephen E Newstead and Janet M Collis. 1987. Con-
text and the interpretation of quantifiers of frequency.
Ergonomics 30(10):1447–1462.

Shashank Srivastava, Igor Labutov, and Tom Mitchell.
2017. Joint concept learning and semantic parsing
from natural language explanations. In Proceed-
ings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 1528–1537.
http://aclweb.org/anthology/D17-1161.

C. Wah, S. Branson, P. Welinder, P. Perona, and S. Be-
longie. 2011. The Caltech-UCSD Birds-200-2011
Dataset. Technical report.

http://arxiv.org/abs/1711.00482
http://arxiv.org/abs/1711.00482
http://www.aclweb.org/anthology/P07-1036
http://www.aclweb.org/anthology/P07-1036
http://www.aclweb.org/anthology/P07-1036
http://archive.ics.uci.edu/ml
http://archive.ics.uci.edu/ml
http://aclweb.org/anthology/D17-1161
http://aclweb.org/anthology/D17-1161
http://aclweb.org/anthology/D17-1161


316

Ilker Yildirim, Judith Degen, Michael K Tanenhaus,
and T Florian Jaeger. 2013. Linguistic variability
and adaptation in quantifier meanings. In CogSci.

Luke S Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed ccg grammars for parsing to
logical form. In EMNLP-CoNLL. pages 678–687.


