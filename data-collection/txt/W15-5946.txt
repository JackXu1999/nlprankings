



















































Proceedings of the...


D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 316–324,
Trivandrum, India. December 2015. c©2015 NLP Association of India (NLPAI)

Triangulation of Reordering Tables: An Advancement Over Phrase Table
Triangulation in Pivot-Based SMT

Deepak Patil, Harshad Chavan, Pushpak Bhattacharyya
Indian Institute of Technology Bombay, India

{deepakcp,harshadpc,pb}@cse.iitb.ac.in

Abstract

Triangulation in Pivot-Based Statistical
Machine Translation(SMT) is a very effec-
tive method for building Machine Trans-
lation(MT) systems in case of scarcity of
the parallel corpus. Phrase Table Triangu-
lation helps in such a resource constrained
setting by inducing new phrase pairs with
the help of a pivot. However, it does
not explore the possibility of extracting re-
ordering information through the use of
pivot. This paper presents a novel method
for triangulation of reordering tables in
Pivot Based SMT. We show that the use
of a pivot can help in extracting better re-
ordering information and can assist in im-
proving the quality of the translation. With
a detailed example, we show that triangu-
lation of reordering tables also improves
the lexical choices a system makes during
translation. We observe a BLEU score im-
provement of 1.06 for Marathi to English
MT system with Hindi as a pivot, and also
significant improvements in 8 other trans-
lation systems by using this method.

1 Introduction

Pivot-Based Statistical Machine Transla-
tion(SMT) is a crucial and a well known
methodology for building Machine Transla-
tion(MT) Systems for language pairs that are not
so rich in terms of available resources. Low or
no availability of parallel corpus for a language
pair is one of the main reasons behind following
a Pivot-Based approach. Pivot-Based approach
makes use of a parallel corpus from source
language to some language other than the target
language, which is known as a “Pivot language”
or simply a “Pivot”, and a parallel corpus from the
pivot language to the target language. Key idea

behind using a pivot is, using one or more pivot
languages to improve the quality of translation
by making use of additional information that
the pivot induces. This additional information is
mainly in the form of a new set of phrase pairs that
are extracted with the help of the pivot language.
This method of extracting new phrase pairs in
Pivot-Based SMT is known as the Triangulation
method.

Current approaches and methods that discuss
triangulation generally focus on the triangulation
of the phrase tables from source language(source)
to pivot language(pivot) and pivot language to tar-
get language(target). This improves the translation
quality because of the newly added phrase pairs,
but the reordering information for these newly
added phrase pairs is not present in the reorder-
ing table. The focus of our work is to explore the
possibility of extracting the reordering informa-
tion for newly added phrase pairs by making use
of the pivot language. To the best of our knowl-
edge this is the first work that discusses the ap-
proaches for triangulation of reordering tables and
shows significant improvements.

To begin with, Section 2 discusses the work re-
lated to Pivot-Based SMT and the Triangulation
method. We then provide a background for our ap-
proaches for reordering table triangulation in Sec-
tion 3. Section 4 discusses the mathematics in-
volved in these approaches. We describe our ap-
proaches viz. Table Based approach and Count
Based approach in detail, in Section 5. Section
6 explains the experimental setup. Results of our
experiments are shown in Section 7 along with the
discussion of those results. We conclude our work
in Section 8 and also point out a few directions for
future work.

2 Related Work

Use of a pivot in machine translation has been ex-
tensively studied by many researchers. Wang et al.316



(2006) demonstrated the use of pivot languages for
extracting better word alignments when a source-
target parallel corpus is either unavailable or is
very small in size. Wu and Wang (2007) proposed
the entire formulation for a pivot based approach
and triangulation of phrase tables and showed that
a pivot can be used for inducing new phrase pairs
that were not extracted while building a phrase ta-
ble from the training corpus. This forms a basis
for a valid speculation that, pivot may also prove
to be useful in extracting reordering information.

Several strategies have been proposed for the
use of a pivot. Utiyama and Isahara (2007) pro-
posed two strategies, a phrase translation strat-
egy that is similar to the triangulation method
and a sentence translation strategy that makes use
of two different MT systems - one from source-
pivot and other from pivot-target. They showed
through their experiments that the phrase trans-
lation strategy performs significantly better than
the sentence translation strategy. Wu and Wang
(2009) additionally introduced a Synthetic Method
for Pivot based SMT, that synthesizes the source-
target corpus from source-pivot and pivot-target
corpora. It was found through their experiments
that the Triangulation Method outperforms the Sy-
thetic method as well. This encouraged us to focus
our work on the triangulation approach.

Nakov and Ng (2012) showed that merging the
phrase tables by giving priority to the original ta-
ble that is extracted from a direct parallel cor-
pus(direct table) and using additional features is a
good strategy. In case of an interpolation, it refers
to giving higher weight values to the translation
probabilities from a direct source-pivot phrase ta-
ble. Dabre et al. (2015) studied the use of more
than one pivots simultaneously. They also men-
tioned that the use of pivot for reordering would
be an interesting problem.

The reordering model on which we focus in
our work is a lexicalized reordering model dis-
cussed by Koehn et al. (2005). Lexicalized re-
ordering was first proposed by Tillmann (2004).
Similar approach was suggested by Ohashi et al.
(2005). Mirkin (2014) proposed several ways for
incrementally updating the lexicalized reordering
model when new training data is introduced and
described a way to combine new and existing re-
ordering models. To the best of our knowledge, no
work has been done on triangulation of reordering
tables in specific.

3 Background for Our Approaches

Our aim is to generate a source language to target
language reordering table given a source language
to pivot language reordering table and a pivot lan-
guage to target language reordering table. This is
called as Triangulation of Reordering tables or Re-
ordering Table Triangulation. In this paper, we
propose two approaches for triangulation of re-
ordering tables. Both of them make use of reorder-
ing tables, which have a peculiar format in Moses,
a statistical machine translation system (Koehn et
al., 2007). Before going forward with the descrip-
tion of our approaches, we explain the concepts of
reordering orientations and give a short overview
of the structure of a reordering table in Moses.

3.1 Reordering Orientations: Monotone,
Swap and Discontinuous

Moses implements reordering using three kinds of
orientations - Monotone, Swap and Discontinu-
ous (Koehn et al., 2005; Koehn, 2009). For each
phrase and its corresponding translation, there are
three possible orientations.

Let S1, S2 be phrases in source sentence, T1, T2
be their corresponding translations in the target
sentence and T1 be a phrase that immediately pre-
cedes T2 in the target sentence. Then for the
phrase pair (S2, T2) the orientation is :

• Monotone, if S1 is a phrase that immediately
precedes S2 in the source sentence.

Figure 1: Monotone Orientation

• Swap, if S1 is a phrase that immediately suc-
ceeds S2 in the source sentence.

Figure 2: Swap Orientation
317



• Discontinuous, if S1 and S2 are not adjacent
to each other in the source sentence.

Figure 3: Discontinuous Orientation

Let us consider sentences from English-Marathi
language pair shown in Figure 4.

Figure 4: Example depicting all orientations

As per the definitions discussed above, the ori-
entation exhibited by the phrase pair (Ram, rAm )
is a monotone since, the translation of the previous
phrase of rAm i.e. ‘After the school’ is also the
previous phrase of ‘Ram’. Similarly by definition,
(plays, K��to) has a swap orientation and (in the
ground, m{dAnAt ) has a discontinuous orientation.

3.2 Structure of a Reordering Table in
MOSES

As discussed in subsection 3.1, for each phrase
pair, there are three reordering orientations pos-
sible. A typical reordering table in Moses consists
of all such phrase pairs and their respective prob-
ability values for monotone, swap and discontinu-
ous orientations.

Following is a sample entry from English-
Marathi reordering table.
By looking |||pAEhSyAvr |||0.2 0.2 0.6 0.2 0.2 0.6

The first three values are the probability val-
ues of a phrase pair (By looking, pAEhSyAvr )
being a monotone, swap and discontinuous re-
spectively, with respect to the previous phrase of
pAEhSyAvr (pAhilyAvara) in the target sentence.
The next three values are the probability values
of a phrase pair (By looking, pAEhSyAvr ) being
a monotone, swap and discontinuous respectively,

with respect to the next phrase of pAEhSyAvr in
the target sentence.

Reordering table is used during decoding to
score the candidate translations. Scoring a partic-
ular candidate translation for reordering, involves
scoring each phrase with respect to its previous
and next phrase. This along with the Language
Model, boosts up the scores assigned to better or-
dered candidate translations.

3.3 Objective of the Approach

Our objective is to determine the probability val-
ues discussed in Subsection 3.2, for phrase pairs
that are newly extracted by phrase table triangu-
lation, given the source-pivot and pivot-target re-
ordering tables.

We propose two approaches for doing this. Our
first approach only assumes the availability of two
reordering tables mentioned above, whereas the
second approach assumes the availability of a mul-
tilingual parallel corpus that is used for training, in
addition to the two reordering tables.

4 Mathematical Formulation

For a language pair L1 − L2, (A, B) is a phrase
pair if phrase A from L1 translates to phrase B
from L2. We will use the notation O(A → B)
to denote the orientation of a phrase pair (A, B).
The possible values of O are {M,S,D}.
Where,
M(A→ B) : (A, B) has a Monotone orientation
S(A→ B) : (A, B) has a Swap orientation
D(A→ B) : (A, B) has a Discontinuous orientation

For the discussion in this section, let X,Y and Z
be the phrases from source, pivot and target lan-
guages respectively which are all translations of
each other. In other words, (X, Y) is phrase pair
from source to pivot, (Y, Z) is a phrase pair from
pivot to target and (X, Z) is a phrase pair from
source to target.

Let us assume that (X, Z) is a phrase pair that
was not originally present in the phrase table and
was induced during the phrase table triangulation
process. Then our task is to determine the proba-
bility values P [M(X → Z)], P [S(X → Z)] and
P [D(X → Z)] in order to include an entry for
phrase pair (X, Z) in the reordering table.

There can be more than one pivot phrases Y
for a particular phrase pair (X, Z). There are
also multiple combinations of source-pivot and
pivot-target orientations possible for (X, Y) and318



(Y, Z) respectively.
Marginalizing over these variables we get,

P [M(X → Z)] =
∑
Y

O(X→Y )
O(Y→Z)

P [M(X→ Z), O(X→ Y ), O(Y→ Z)]

(1)

Applying the chain rule, we get,

P [M(X → Z)] =
∑
Y

O(X→Y )
O(Y→Z)

P [M(X→ Z)|O(X→ Y ), O(Y→ Z)].
P [O(X→ Y ), O(Y→ Z)]

(2)

Since, O(X → Y ) and O(Y → Z) are indepen-
dent of each other, applying this independence
assumption on second term, we get

P [M(X → Z)] =
∑
Y

O(X→Y )
O(Y→Z)

P [M(X→ Z)|O(X→ Y ), O(Y→ Z)].
P [O(X → Y )].P [O(Y→ Z)]

(3)

P [O(X → Y )] and P [O(Y → Z)] are the
reordering probability values from source-pivot
and pivot-target and are directly available from the
reordering tables that are used for triangulation.
The first term i.e.

P [M(X→ Z)|O(X→ Y ), O(Y→ Z)]
called a “Multiplicative Factor” henceforth, is

a probability that for a particular orientation of
O(X → Y ) and O(Y → Z), source-target orien-
tation for (X, Z) is a Monotone, in case of the cal-
culation above. We worked out the mathematics
for probability of (X, Z) being a Monotone. Sim-
ilar calculations can be performed for determin-
ing the Swap and Discontinuous probability val-
ues. Therefore, similar kind of terms will exist for
Swap and Discontinuous as well. The value for
the Multiplicative Factor is not directly available
to us. For calculating this value we propose two
approaches in this paper.

5 Description of the Approaches

For calculating the Multiplicative Factor in Equa-
tion 3 we can follow one of the two paths, one
is called a Table Based Approach and the other is
called Count Based Approach.

5.1 Table Based Approach
This approach only assumes the availability of two
reordering tables: a source language to pivot lan-
guage reordering table and a pivot language to tar-
get language reordering table. Based on all possi-
ble source-pivot and pivot-target orientations, we
designed an “Orientation Table” of all possible
source-target orientations, which is shown in Ta-
ble 1.

M S D
Monotone (M) M S, D S, D

Swap (S) S M, D M, D
Discontinuous (D) D M ,S, D M, S, D

Table 1: Orientation Table

The rows of the Orientation Table(Oi ′s) rep-
resent different orientations for a source-pivot
phrase pair and the columns(Oj ′s) represent dif-
ferent orientation for a pivot-target phrase pair,
where the pivot phrase is common. A cell which is
in row Oi and column Oj lists all possible orien-
tations from source-target when source-pivot ori-
entation is Oi and pivot-target orientation is Oj .
For example, assume that S, P, T are phrases from
source, pivot and target languages respectively and
are translations of each other. If the source-pivot
orientation is a monotone for phrase pair (S, P) and
the pivot-target orientation is swap for phrase pair
(P, T), then for phrase pair (S, T) the orientation
can either be a Swap or a Discontinuous. We stud-
ied numerous examples from the most general sce-
narios as well as from specific language pairs, in
order to create this table.

We use this table for determining the Multi-
plicative Factor by looking up into the appropri-
ate cell of the table while doing the computation.
We also assume that if there are more than one
orientations in a particular cell then all of them
are equally likely. We have observed that there
might be some wrong translations in the training
data because of which the orientations which are
not present in a particular cell may also be exhib-
ited by some phrases in some situations, although,
such occurrences are few in number. To factor
in for such discrepancies, we assume that orien-
tations present in the cells of the above table are
the “most probable” ones and other orientations
are also possible with a very small probability.

For example, for a phrase pair (S, P) the orien-
tation is monotone and for (P, T) the orientation is319



En
-G

u-
H

i

En
-H

i-G
u

En
-H

i-M
r

H
i-G

u-
En

H
i-G

u-
M

r

H
i-P

a-
En

M
r-

G
u-

H
i

M
r-

G
u-

En

M
r-

H
i-E

n

DIR-DIR 25.97 16.03 10.12 29.87 33.69 29.78 41.66 16.72 16.77
INTER-DIR 25.83 17.37 13.11 30.01 36.09 29.86 42.58 17.97 19.04
INTER-TRI 25.08 16.94 12.57 28.54 36.13 28.38 42.47 17.70 19.09

INTER-INTER 26.30 17.71 13.19 30.52 36.28 30.39 42.67 18.55 20.10
IMPROVEMENT 0.47 0.34 0.08 0.51 0.19 0.53 0.09 0.58 1.06

Table 2: BLEU Scores for Count Based Approach
(IMPROVEMENT row shows improvement in INTER-INTER system over INTER-DIR system)

swap, then the multiplicative factor while calculat-
ing probability of (S, T) being a monotone i.e.
P [M(S → T )|M(S → P ), S(P → T )] can be
assigned a small value, say 0.1, since monotone is
not listed in the cell.
The other two orientations are assumed to be
equally likely, so in this case
P [S(S → T )|M(S → P ), S(P → T )] = 0.45
and
P [D(S → T )|M(S → P ), S(P → T )] = 0.45

5.2 Count Based Approach

This approach assumes the availability of two
reordering tables and the source-pivot-target
multilingual parallel corpus that is used for
training. By using the definition of probability,
the Multiplicative Factor in equation (3) can be
written as
P [M(S → T )|O(S → P ), O(P → T )]

=
count[M(S → T ), O(S → P ), O(P → T )]

count[O(S → P ), O(P → T )]
Here the numerator is the count of the num-

ber of occurrences where phrase pair (S, T) has a
monotone orientation when phrase pair (S, P) has
a particular orientation O1 and (P, T) has an ori-
entation O2. The denominator is the count of the
number of occurrences where phrase pair (S, P)
has a particular orientation O1 and (P, T) has an
orientation O2.

Basic idea in the Count Based Approach is
to extract these kind of counts from the corpus
for each source-target phrase pair. For extracting
these counts, say for a phrase pair (S, T), we need
to look at the phrases which immediately precede
or succeed phrase T in a target sentence and the
position of their translations with respect to the
position of phrase S in the source sentence. The

counts have to be extracted for each phrase pair
and have to be stored separately in order to lookup
while doing the computations for reordering table
triangulation using this approach. This approach
seems more intuitive since we use the parallel cor-
pus for getting the count values and therefore we
are likely to get more accurate values for the mul-
tiplicative factor.

6 Experimental Setup

We performed several experiments with both the
approaches discussed in section 5. The parallel
corpus used for the experiments was a Health and
Tourism domain multilingual parallel corpus (Jha,
2010) which was divided into a training corpus of
46000 sentences, a tuning corpus of 500 sentences
and a test corpus of 2000 sentences. The reason
behind using a small sized corpus for our experi-
ments is that, often in a realistic scenario, obtain-
ing a large sized multilingual parallel corpus is dif-
ficult.

The tool used for translation was Moses 3.0
(Koehn et al., 2007). It was used for train-
ing, tuning(MERT based) and testing the trans-
lation systems. We experimented with four
kinds of MT systems for 9 different source-
pivot-target combinations from the language
set {English(En), Hindi(Hi), Marathi(Mr), Gu-
jarati(Gu), Punjabi(Pa)}, mentioned in Table 2.
The differences in these systems were in the
phrase table and reordering table which each sys-
tem uses. A ‘Direct Phrase table’ and a ‘Direct
Reordering Table’ are the tables that are obtained
directly from source-target parallel corpus after
training. A ‘Triangulated Phrase Table’ and a ‘Tri-
angulated Reordering Table’ are the tables that are
obtained through the process of triangulation. An
‘Interpolated Phrase Table’ is a phrase table ob-320



Figure 5: Sample Translation from English-Hindi-Marathi INTER-DIR MT System

tained by linear interpolation of values from Direct
Phrase Table and Triangulated Phrase Table. In-
terpolation of reordering tables can be performed
either by linear interpolation or by Fill-up interpo-
lation (Dabre et al., 2015) of Direct and Triangu-
lated Reordering Tables. With this terminology in
place, below is a small description of each system.

• DIR-DIR : This system uses a Direct Phrase
Table and a Direct Reordering Table.

• INTER-DIR : This system uses an Interpo-
lated Phrase Table and a Direct Reordering
table.

• INTER-TRI : This system uses an Interpo-
lated Phrase Table and a Triangulated Re-
ordering table.

• INTER-INTER : This system uses an Inter-
polated Phrase Table and an Interpolated Re-
ordering table.

7 Results and Discussion

We performed experiments for both Table Based
and Count Based approaches. The improvements
were observed from the quantitative point of view
- in the BLEU scores (Papineni et al., 2002) -
as well as from the qualitative point of view - in
the reordering of the translation that a system pro-
duces and better lexical choice in the translation.

7.1 Improvements in BLEU Scores
For the table based approach, we found an im-
provement of 0.48 in the BLEU score from 17.98
for INTER-DIR system to 18.46 for INTER-
INTER system, for Marathi to English transla-
tion when Gujarati is used as a pivot. Small im-
provements were also observed for other systems,

like Hindi-Gujarati-English (0.4), Hindi-Punjabi-
English (0.33) and Hindi-Gujarati-Marathi (0.16).
The improvements in other five systems were not
as significant as these. This was an expected pat-
tern in the results since, the Table Based approach
does not actually look at the training data while
calculating the Multiplicative Factor. In this ap-
proach, Multiplicative Factor is calculated based
on the Orientation Table which is training data in-
dependent. Therefore, the approach was expected
to perform better in some situations than others.

On the other hand, the Count Based approach
extracts the counts for orientations from the train-
ing corpus. Thus the calculated Multiplicative
Factor is more accurate as compared to the Table
Based approach. So we expected the Count Based
approach to outperform the Table Based and pro-
duce an improvement in the BLEU score for most
of the source-pivot-target language combinations.
This is indeed the case and the results for Count
Based approach are shown in Table 2. The im-
provements vary with language combinations be-
cause of the difference in the quality of the counts
that are extracted for each combination.

The improvement from DIR-DIR system to
INTER-DIR system is because of the phrase ta-
ble triangulation and addition of newly extracted
phrase pairs in the phrase table. Therefore, in or-
der to quantify the improvement in BLEU score
that is solely achieved by our approach, we con-
sider INTER-DIR system as our baseline for all
experiments. We can see from Table 2, that sig-
nificant improvements are achieved by INTER-
INTER systems over INTER-DIR systems in case
of most language combinations. INTER-TRI
system does not outperform INTER-DIR system
since, it uses only the Triangulated Reordering ta-321



Figure 6: Sample Translation from English-Hindi-Marathi INTER-INTER MT System

ble which mostly contains entries for only newly
extracted phrase pairs. INTER-INTER systems
perform better as expected, since knowledge from
both, Direct and Triangulated Reordering tables is
combined and used in these systems.

7.2 Qualitative Improvement in Reordering

Let us closely look at an actual translation exam-
ple from the test corpus to clearly understand, how
calculating reordering probability values for newly
added phrase pairs helps in improving the quality
of translation. It must be noted that, although the
reordering information is not available for newly
added phrase pairs in INTER-DIR system, they
may still end up being a part of the best possible
translation because of other factors involved such
as Phrase Translation Probability and Language
Model. For example, a newly induced phrase
pair may have higher phrase translation probabil-
ity than other phrase pairs, which results in that
phrase being a part of the best candidate transla-
tion. This phenomenon is responsible for the im-
provement that is achieved by INTER-DIR system
over DIR-DIR system.

Figure 5(a) shows the translation example
from English-Hindi-Marathi INTER-DIR system
and Figure 6(a) shows the translation example
from English-Hindi-Marathi INTER-INTER sys-
tem. From the phrase tables, we observed that
the phrase pair (of the waist, k\br�QyA) was added
through triangulation of phrase tables. So, in
INTER-DIR system, there are no reordering prob-
ability values available for this phrase pair, but in
the INTER-INTER system, the reordering table
has an entry for this phrase pair with reordering
probability values. Figure 6(b) shows that reorder-
ing table entry for this phrase pair.

The translation produced by INTER-DIR sys-
tem shown in Figure 5(a), will also be produced
as a candidate translation by INTER-INTER sys-
tem, but the INTER-INTER system discards it to
choose the translation shown in Figure 6(a) as the
best translation, since the reordering probability
values for the new phrase pair play a vital role.
If we look at figure 5(b), the reordering probabil-
ity values for (the waist, k\br ) favour discontinu-
ous orientation with respect to the previous phrase
and monotone orientation with respect to the next
phrase. But the translation that is generated has
a discontinuous orientation for the phrase k\br
with respect to both previous and next phrases.
Since the generated candidate is not in accordance
with the values from reordering table, the candi-
date gets a relatively lesser score.

On the other hand, if we look at Figure 6(b),
the reordering probability values for (of the waist,
k\br�QyA) favour discontinuous orientation with
respect to the previous phrase and swap orienta-
tion with respect to the next phrase. The transla-
tion that is generated as a candidate, shown in Fig-
ure 6(a), also has discontinuous orientation with
respect to the previous phrase and swap orienta-
tion with respect to the next phrase, for the phrase
k\br�QyA. Since the candidate is in accordance
with the values from reordering table, the candi-
date gets a higher score as compared to the earlier
candidate. There are also other factors like trans-
lation model and language model that contribute
to the computation, but in this example the contri-
bution by reordering probabilities is distinctly ev-
ident. We can see that the inclusion of reordering
information for newly added phrase pair has also
induced better lexical choices in the translation.

This leads to the translation shown in Figure322



6(a) being selected as the best translation. It can
be easily observed that the translation is a better
one in terms of reordering. This shows that our ap-
proach works well to improve the quality of trans-
lation.

Following is another example from Marathi-
Hindi-English system that shows qualitative
improvement in the reordering of phrases in the
output translation.

Input:
K� p afÄ as� nhF z`ZAlA aApl� vjn jA-t
vAVt�.
(khuupa ashakta asuunahI rugNAlA Apale vajana
jAsta vATate .)

English Equivalent (Reference):
Patient thinks his / her weight to be too much even
on being very weak .

Output of INTER-DIR System:
Despite weight of the patient has to be extremely
weak .

Output of INTER-INTER System:
despite being very weak patient feels more your
weight .

It is evident from the example that the output
of INTER-INTER system is better ordered than
the output of INTER-DIR system. Except for the
word “your”, the meaning of the source sentence
is better conveyed by the output of INTER-INTER
system.

8 Conclusion and Future Work

The issue of data scarcity has been addressed ef-
fectively by Phrase Table triangulation. In this
paper we went a step further and proposed two
new approaches for the triangulation of reorder-
ing tables viz. Table Based approach and Count
Based approach. We conducted experiments with
both these approaches on 9 different translation
systems, each using a different combination of
source-pivot-target languages and found signifi-
cant improvements in the BLEU scores for most of
the systems. We also discussed actual translation
examples that showed a qualitative improvement
in the reordering of the output, thereby vindicating
the fact that better reordering information can be
extracted through a pivot. Examples also showed
that the lexical choices made by the system im-

proved when our approach was used. We also ob-
served that Count Based approach performed bet-
ter than Table Based approach for most of the lan-
guage combinations.

The focus of our work was on finding the re-
ordering probability values for phrase pairs that
are newly added to the phrase table through the use
of a pivot. The question that remains is, whether a
pivot can help beyond extracting these reordering
values. Another interesting question is whether
this approach scales well to a scenario where more
than one pivots are used simultaneously. We will
try to focus on these points in the future.

Acknowledgement

We would like to thank Mr. Anoop Kunchukut-
tan and Mr. Rohit More from Centre For Indian
Language Technology, IIT Bombay for their valu-
able inputs and suggestions and for the insightful
discussions on the topic.

References
Raj Dabre, Fabien Cromieres, Sadao Kurohashi, and

Pushpak Bhattacharyya. 2015. Leveraging small
multilingual corpora for smt using many pivot lan-
guages. In Proceedings of the 2015 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 1192–1202, Denver, Colorado,
May–June. Association for Computational Linguis-
tics.

Girish Nath Jha. 2010. The tdil program and the indian
language corpora initiative (ilci). In Proceedings of
the Seventh Conference on International Language
Resources and Evaluation (LREC 2010). European
Language Resources Association (ELRA).

Philipp Koehn, Amittai Axelrod, Alexandra Birch,
Chris Callison-Burch, Miles Osborne, David Talbot,
and Michael White. 2005. Edinburgh system de-
scription for the 2005 iwslt speech translation eval-
uation. In IWSLT, pages 68–75.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the ACL on
interactive poster and demonstration sessions, pages
177–180. Association for Computational Linguis-
tics.

Philipp Koehn. 2009. Statistical machine translation.
Cambridge University Press.323



Shachar Mirkin. 2014. Incrementally updating the smt
reordering model. In Proceedings of The 28th Pa-
cific Asia Conference on Language, Information and
Computing (PACLIC), Phuket, Thailand.

Preslav Nakov and Hwee Tou Ng. 2012. Improv-
ing statistical machine translation for a resource-
poor language using related resource-rich languages.
Journal of Artificial Intelligence Research, pages
179–222.

Kazuteru Ohashi, Kazuhide Yamamoto, Kuniko Saito,
and Masaaki Nagata. 2005. Nut-ntt statistical ma-
chine translation system for iwslt 2005. In IWSLT,
pages 118–123.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004: Short Papers, pages
101–104. Association for Computational Linguis-
tics.

Masao Utiyama and Hitoshi Isahara. 2007. A compari-
son of pivot methods for phrase-based statistical ma-
chine translation. In HLT-NAACL, pages 484–491.
Citeseer.

Haifeng Wang, Hua Wu, and Zhanyi Liu. 2006. Word
alignment for languages with scarce resources us-
ing bilingual corpora of other language pairs. In
Proceedings of the COLING/ACL on Main confer-
ence poster sessions, pages 874–881. Association
for Computational Linguistics.

Hua Wu and Haifeng Wang. 2007. Pivot language ap-
proach for phrase-based statistical machine transla-
tion. Machine Translation, 21(3):165–181.

Hua Wu and Haifeng Wang. 2009. Revisiting pivot
language approach for machine translation. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP: Volume 1-Volume 1, pages 154–162. Asso-
ciation for Computational Linguistics.

324


