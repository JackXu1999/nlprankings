















































Interaction-Aware Topic Model for Microblog Conversations through Network Embedding and User Attention


Proceedings of the 27th International Conference on Computational Linguistics, pages 1398–1409
Santa Fe, New Mexico, USA, August 20-26, 2018.

1398

Interaction-Aware Topic Model for Microblog Conversations through
Network Embedding and User Attention

Ruifang He1,2,∗, Xuefei Zhang1,2,∗, Di Jin1,2,†, Longbiao Wang1,2, Jianwu Dang1,2,3, Xiangang Li4
1School of Computer Science and Technology, Tianjin University, Tianjin, China
2Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin, China

3Japan Advanced Institute of Science and Technology, Ishikawa, Japan
4AI Labs, Didi Chuxing, Beijing, China

{rfhe,jindi,longbiao wang}@tju.edu.cn,zhangxuefei29@163.com
jdang@jaist.ac.jp,lixiangang@didichuxing.com

Abstract

Traditional topic models are insufficient for topic extraction in social media. The existing meth-
ods only consider text information or simultaneously model the posts and the static characteristics
of social media. They ignore that one discusses diverse topics when dynamically interacting with
different people. Moreover, people who talk about the same topic have different effects on the
topic. In this paper, we propose an Interaction-Aware Topic Model (IATM) for microblog con-
versations by integrating network embedding and user attention. A conversation network linking
users based on reposting and replying relationship is constructed to mine the dynamic user be-
haviours. We model dynamic interactions and user attention so as to learn interaction-aware edge
embeddings with social context. Then they are incorporated into neural variational inference for
generating the more consistent topics. The experiments on three real-world datasets show that
our proposed model is effective.

1 Introduction

The prosperity of microblog platforms, such as Twitter1 and Sina Weibo2 brings the large scale, noisy
and user-generated short posts. Automatically extracting topics in social media aims to reveal thematic
information of the underlying collection, which can be used in summarization (Zhuang et al., 2016),
hashtag recommendation (Li et al., 2016b), response generation (Xing et al., 2017) and so on.

The conventional topic models, like Latent Dirichlet Allocation (LDA) (Blei et al., 2003), infer the
hidden topics based on word co-occurrences in documents. They could not be directly transferred in
social media due to the data sparsity and the noise of short texts.

The existing relevant researches can be roughly categorized into: (1) Methods exploit the content of
short texts by aggregation strategy (Mehrotra et al., 2013) or modeling word-pair co-occurrence (Yan et
al., 2013). (2) Models incorporating with word embedding try to deeply understand the posts by repre-
sentation learning (Sridhar, 2015; Hu and Tsujii, 2016), yet they ignore the social context of microblog
messages. Essentially, social media content and network structures influence each other (Bi et al., 2014),
the only content analysis is insufficient. (3) (Li et al., 2016a; Chen and Ren, 2017) take into account the
content and static characteristics of network structures to deduce topics. However, they ignore dynamic
user behaviours.

Actually, a user may talk about various topics when interacting with diverse neighbours in a social
network. For instance, Fig. 1(a) shows two conversation trees, where [U0] communicates with [U1] and
[U2] about the topic of United Kingdom European Union membership referendum, while [U0] talks a
game with [U3]. Hence, we need to infer topics according to the interactions between users. Moreover,
although [U0] and [U1] argue about the same topic, [U0] contains salient content in topic description,
e.g., UK, EU, referendum while the reply of [U1] is nothing but simply response to [U0], e.g., Yes, agree.

This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://
creativecommons.org/licenses/by/4.0/
* Indicates equal contribution.
†Corresponding author.

1https://twitter.com
2https://weibo.com



1399

Theresa May: "The UK's never felt 
totally at home in the EU" 

Me: "I've never felt less at home in 
the UK since the referendum"

Yes, exactly. 
And I can't 

believe how 
quickly it's 
changed.

Think it's the views of a 
nasty minority who have 
been able to trick ppl bcs

of a lack of strong 
political leadership.

Feel like we've regressed a lot. My 
pops observed that it's somewhat 
reminiscent of the 1970s shortly 

after he arrived here.

Good luck to 
everyone playing 

tonight! 

Who's 
ready for 

Friday 
Night 

Lights!?! 
We are!!!!

I Love 
this 
game

[UO]

[U1] [U2]

[U0]

[U3]

[U0][U4]

Game Brexit Referendum

(a) Two conversation trees. [Ui]: the i-th user; Arrow
lines: reposting/replying relations.

• [U0]: Theresa May: “The UK‘s never felt
totally at home in the EU”. Me: “I’ve
never felt less at home in the UK since
the referendum”. I Love this game

• [U1]:Yes, exactly. And I can't agree more.

• [U2]:Think it's the views of a nasty
minority who have been able to trick
ppl bcs of a lack of strong political
leadership.

• [U3]:Good luck to everyone playing
tonight!

• [U4]:Who's ready for Friday Night
Lights!?! We are!!!

[U3]

[U0]

[U1] [U2]

[U4]

(b) An example of a conversation network. The green box
shows the topic about a game; The red one exhibits the
topic of Brexit referendum.

Figure 1: A conversation network is transformed from two conversation trees.

Thus, it is intuitive that during the topic extraction of United Kingdom European Union membership
referendum, [U0] is more important than [U1]. The dynamic interactions and diverse user attention
on the topic provide us heuristic insights for topic extraction.

Note that conversation trees can be transformed into conversation networks through user relations,
as illustrated in the Fig. 1. Inspired by network embedding and social user behaviour researches, we
propose an Interaction Aware Topic Model (IATM) integrating dynamic interactions and various effects
of users on the topic. The contributions are as follows.

• We propose a novel Interaction-Aware Topic Model (IATM) for microblog conversations, which
simultaneously considers social media content and network topology.

• Our method models dynamic interactions and various effects of users, and encodes them as
interaction-aware edge embeddings through network embedding and user attention.

• The deeper edge representations are employed in neural variational inference for generating the
more consistent topics.

• The experiments verify that the proposed IATM is effective. Dynamic user behaviours are useful
for topic extraction of short texts in social media.

2 Related Work

Previous researches for topic extraction in social media can be mainly classified into two aspects.

2.1 Focusing on Content Information

They depend on pure content to generate document-topic distribution and topic-word distribution. (1)
Aggregation Strategy Based. To solve the data sparsity short texts, some use different strategies to
heuristically aggregate microblog messages based on authorship (Hong and Davison, 2010; Zhao et al.,
2011) or hashtags (Mehrotra et al., 2013; Tang et al., 2013) before applying traditional topic model. Self-
Aggregation based Topic Model (SATM) (Quan et al., 2015) combines short texts aggregation and topic
induction into a unified model. Others use Biterm Topic Model (BTM) (Yan et al., 2013) and RNN-IDF
based Biterm Short-text Topic Model (RIBSTM) (Lu et al., 2017) modeling biterm co-occurrence in the
whole corpus to enhance topic discovery. (2) Word Embedding Based. Since word embeddings have
been shown their ability to form clusters of conceptually similar words in the embedding space (Mikolov
et al., 2013c), Gaussian Mixture Topic Model (GMTM) (Sridhar, 2015) and Latent Concept Topic Model
(LCTM) (Hu and Tsujii, 2016) utilize word embeddings to improve topic generation. However, since
social media content and network structures influence with each other, only focusing on content is insuf-
ficient.



1400

2.2 Integrating Content and Network Structure information

This kind of researches considers social content and network structures together, including followship
and repostship. (1) Followship Based. Lim et al.(2013) jointly model the text and user-follower net-
work during topic inference. Yet the appearance of Zombie Fans purchased by those who would like to
increase their influence, disturbs the followship network. (2) Repostship Based. LeadLDA (Li et al.,
2016a) generates words according to topic dependencies derived from conversation trees by differentiat-
ing messages from leader and follower. Leader is the post that contains salient and new information in
topic description while follower is the post that simply responds to its reposted or replied messages. Chen
and Ren (2017) propose ForumLDA to infer topics by distinguishing response messages from relevant
and irrelevant to the original post.

However, due to the zombie fans in followship network, or exploring the static characteristics in re-
postship network, these methods ignore the dynamic user behaviours latent in reciprocal interactions
between users. Therefore we emphasize on exploring whether user interactions and user attention can
help topic generation in social media.

2.3 Network Embedding and Neural Variational Inference

The further researches (Perozzi et al., 2014; Tu et al., 2017) about network embedding make it possible
to suitably model the dynamic user behaviours. Meanwhile, comparing with topic models based on
word embedding, Miao et al. (2017) and Srivastava and Sutton (2017) apply topic model with neural
variational inference (Kingma and Welling, 2014) on traditional long documents, which consistently
produces the better topics. A small change in the document will produce a small change in topics. Yet
they have not taken into account sparse and noisy texts and network structures in social media.

Therefore, we model dynamic user behaviours and content information as interaction-aware edge
embeddings, which combine network embedding and user attention. Further, they are incorporated into
variational auto-encoder for topic extraction in social media, which can produce the more consistent and
precise topics.

3 Interaction-Aware Topic Model

The large scale short and noisy texts in social media bring the more serious data sparseness and incon-
sistency in topics. Yet the dynamic user behaviours also provide us opportunities. Here, we propose two
hypotheses and explore whether they are useful for topic extraction.

H1: Dynamic Interactions. One discusses various topics with different people.
H2: User Attention. Users who debate the same topic have diverse effects on the topic.
We need to construct a conversation network, shown in Fig. 1(b), transformed from conversation trees.

3.1 Conversation Network

Conversation networks are transformed from conversation trees through user relations, which are ex-
hibited in Fig. 1(b). We first give the basic notations and definitions of a conversation network. Given
G = (V,E, T ), where V is the set of vertices, E ⊆ V × V is the edges between vertices and T denotes
text information of vertices. Each vertex v ∈ V represents a user and every edge e(u,v) ∈ E stands for
the reposting or replying relationship between two vertices (u, v). To solve the sparsity of short texts, we
aggregate all the messages posted by the same user, including the original and the reposting messages3.
Text information of the unique vertex v ∈ V is the messages Mv = (w1, w2, ..., wn), where n is the
count of words in Mv.

In order to illustrate the proposed IATM model, we firstly give two definitions:
D1: Edge Embedding. The edge embedding e for every edge e(u,v) is acquired by concatenating the

vertex embeddings u and v. It encodes user representations with respect to various neighbours.
D2: Interaction-aware Edge Embedding. It models the various effects of u and v on a topic via user

attention mechanism, and makes edge embedding as an interaction-aware style.

3Both of replying and reposting messages are called reposting messages.



1401

3.2 Model

vu u v

Convolution Convolution

Mutual Attention

SG

Attention

Model 
Dynamic 
Interactions

Model User 
Attention

Topics 
Generation

Figure 2: The IATM framework.

The proposed IATM framework is shown in Fig. 2. The generative process of IATM mainly includes
three parts: (1) model dynamic interactions, (2) model user attention and (3) topics generation. A basis
of three parts is that each word is represented as a low dimensional, continuous and real-valued vector,
also known as word embedding (Mikolov et al., 2013c). Given text information of a vertex v, we take
the embedding wi ∈ Rd

′
for each word to obtain embedding sequence as Sv = (w1,w2, ...,wn). Here,

d′ indicates the dimension of word embeddings.

3.2.1 Model Dynamic Interactions

Corresponding to the assumption H1, a specific vertex should have its own diverse points with distinct
neighbours, which leads to different edge embeddings.

To make full use of network structures and associated text information, we encode each vertex v as a
concatenation of a structure-based embedding v(s) and a text-based embedding v(t), and get the vertex
embedding v = v(s) ⊕ v(t)(v ∈ Rd).

Structure-based Embedding. We adopt a neural language model (SkipGram, or SG for short)
(Mikolov et al., 2013a) for v(s). To maximize the probability of a node’s neighbourhood co-occurrence,
we define the objective function of structure-based embeddings as follows

Ls = − log
∑

−k≤j≤k
p(v

(s)
i+j |v

(s)
i ) (1)

where v(s)i is corresponding to the i-th vertex, the window size is k, and p(v
(s)
i+j |v

(s)
i ) is defined using



1402

the softmax fuction

p(v
(s)
i+j |v

(s)
i ) =

exp((v
(s)
i+j)

T
v
(s)
i )∑|V |

t=1 exp((v
(s)
t )

T
v
(s)
i )

. (2)

In this way, nodes with similar neighbours share the similar structure-based embeddings.
Text-based Embedding. To discover the thematic information of the vertex pair in an edge, we utilize

mutual attention (Santos et al., 2016; Tu et al., 2017) to obtain text-based embeddings, which allows the
pooling operation to be aware of the topic of an edge e(u,v). To some extent, content information from a
vertex can directly affect the text-based embedding of the other vertex, and vice versa.

Convolutional neural network (CNN) has gained great performance on the textual information encod-
ing (Chen et al., 2015; Wang et al., 2017). Given the embedding sequence Sv, we conduct convolution
operation over Sv within the i-th window as follows

xi = C · (Sv)i:i+l−1 + b (3)

where C ∈ Rd×(l×d′) is a convolution matrix, b is the bias vector and window size is l. The same
operation is also on Su. The outputs of convolution, P ∈ Rd×m and Q ∈ Rd×n where m and n mean
the length of Su and Sv respectively, are as the input of mutual attention layer to compute the correlation
matrix F.

F = relu(PTAQ)(F ∈ Rm×n) (4)

A ∈ Rd×d is a matrix to be learned by the neural network and we emlpoy relu as activation function.
Note that, the element Fi,j ∈ F is the pair-wise correlation score of the hidden vectors Pi and Qj .

Then, we employ the mean-pooling along rows and columns of F to generate pooling vectors by

g
(p)
i = mean(Fi,1, ...,Fi,n) g

(q)
j = mean(F1,j , ...,Fm,j). (5)

The pooling vectors of P and Q are obtained as

g(p) = (g
(p)
1 , ..., g

(p)
m )

T g(q) = (g
(q)
1 , ..., g

(q)
n )

T . (6)

Next, the softmax function is operated on g(p) and g(q) to get the mutual attentive vectors a(p) and a(q).
For instance, the i-th element of mutual attentive vector a(p) is computed as

a
(p)
i =

exp(g
(p)
i )∑m

t=1 exp(g
(p)
t )

. (7)

Finally, we get the text-based embeddings u(t) and v(t) by u(t) = Pa(p) and v(t) = Qa(q). The
objective function of text-based embeddings is as

Lt(e) = α log p(v
(t)|u(t)) + β log p(v(t)|u(s)) + γ log p(v(s)|u(t)) (8)

where α, β and γ control the weights of corresponding parts. Similarly, we employ softmax function
for calculating the conditional probabilities in Eq. (8) as in Eq. (2).

Edge Embedding. Here, we obtain the vertex embeddings u and v by u = u(s) ⊕ u(t) and v =
v(s) ⊕ v(t). In this way, we operate e = u ⊕ v to get the edge embedding e ∈ R2d for e(u,v), which is
dynamic context-aware.

3.2.2 Model User Attention
As referred before, we assume that users who discuss the identical topic may take different effects on
the topic. Attention mechanism is designed for mining the different significance of users on the topic.
Since edge embedding is incorporated by the vertex-pair embeddings in an edge and each vertex denotes
a user, the attention of users on the topic are transformed into the user attention vector a(e) ∈ R2d. a(e)



1403

is computed by conducting softmax function on the edge embedding e. To formulate, the i-th element
of a(e) is computed as

a
(e)
i =

exp(ei)∑2d
t=1 exp(et)

. (9)

After that, through combining topic information of the vertex-pair in an edge with user attention on
the topic, we obtain interaction-aware edge embedding i ∈ R2d by using element-wise product as i =
(e1a

(e)
1 , ..., e2da

(e)
2d ).

3.2.3 Topics Generation
Neural variational inference (Miao et al., 2017) approximates the posterior of a generative model with
a variational distribution parameterized by a neural network, which consistently generates the better
topics. With respect to social media, we input the interaction-aware edge embedding, which is encoded
with dynamic user behaviours and content information, into variational auto-encoder. To formulate,
suppose d̂ is a document, w is a word in d̂ and the number of topics is K. Here, we adopt neural
variational inference to infer the multinomial document-topic distribution θd̂ = (p(t1|d̂), ..., p(tK |d̂))
and topic-word distribution φw = (p(w|t1), ..., p(w|tK)), where ti is the i-th topic.

Document-topic distribution. Precisely, given the interaction-aware edge embedding i, we first en-
code it to a hidden space as

henc = relu(W
(ih)i+ b(ih)) (10)

where W(ih) and b(ih) are the parameters of the neural network. Then the Gaussian parameters µd̂ and
σ2
d̂

can be obtained as

µd̂ = W
(hµ)henc + b

(hµ) log(σ2
d̂
) = W(hσ)henc + b

(hσ). (11)

The latent semantic vector z ∈ RK can be calculated using the reparameterization trick as z = µd̂+�×σd̂
where � ∼ N (µ0, σ20) is the prior Gaussian distribution. The hyper-parameters µ0 and σ20 is set to a zero
mean and unit variance Gaussian. Here we pass the Gaussian random vector z through the softmax
function to parameterize the multinomial document-topic distribution θd̂.

Topic-word distribution. The conventional topic models compute the conditional probability p(w|d̂)
as

p(w|d̂) = φw × (θd̂)
T . (12)

Therefore, we compute the topic-word distribution φw as the parameter of neural network by

hdec = softmax(φw × (θd̂)
T ). (13)

Thereafter, a new interaction-aware edge embedding i′ is generated as

i′ = relu(W(hi)hdec + b
(hi)) (14)

where W(hi) and b(hi) are the parameters of neural network.
The objective for this part is as

Lθ,φ(e) = Eq(θ,z|i)[log p(i|z, θ, φ)]−DKL[q(θ, z|i)||p(θ|µd̂, σ
2
d̂
)] (15)

where the variational distribution q(θ, z|i) approximates the true posterior p(θ|µd̂, σ
2
d̂
) through Kullback-

Leibler divergence.

3.3 Model Training
We need to minimize the overall objective function as

L = Ls +
∑
e∈E

(Lt(e) + Lθ,φ(e)). (16)

The conditional probability exploiting softmax function is computationally expensive according to E-
q. (1) and Eq. (8). Therefore, we employ negative sampling (Mikolov et al., 2013b) and Adam (Kingma
and Ba, 2015) to optimize the overall objective function. In order to prevent overfitting, we also employ
dropout (Srivastava et al., 2014) during the generation of document-topic and topic-word distribution.



1404

4 Experiments

4.1 Datasets

We obtain the datasets shown in Tab. 1 based on the original microblog corpus used by Li et al., (2016a).
They collected the posts of 50 frequent hashtags during May 1 -July 31, 2014 through Sina Weibo
hashtag-search API4. Then they split the whole corpus into 3 datasets and each month represented a
datasets. We further deal with the original datasets as follows: 1) Remove the posts whose length is less
than 3 words or that have no poster username; 2) Filter users who have no reposting or replying relation-
ship; 3) Aggregate all the original and the reposting posts from the same user to form text information of
a user vertex.

Month #Users #Reposting

May 8907 10435
June 19293 25962
July 16990 20971

Table 1: Statistics of Datasets.

4.2 Evaluation Metrics

In previous work, a popular metric for topic model, perplexity, is evaluated based on the likelihood of
held-out documents. Nonetheless, Chang et al. (2009) have proved that higher likelihood of held-out
documents doesn’t necessarily correspond to human perception of semantically coherent topics. Instead-
ly, we follow (Mimno et al., 2011) to calculate the coherence score of a topic given the top N words
as

C = 1
K
·
K∑
k=1

N∑
i=2

i−1∑
j=1

log
D(wki , w

k
j ) + 1

D(wkj )
(17)

where wki refers to the i-th word in topic k ranked by topic distribution over words, D(w
k
i , w

k
j ) stands

for the count of documents where wki and w
k
j co-occur,and D(w

k
j ) indicates the number of documents

that contain word wkj . In this paper, a document is the aggregated text of a user.

4.3 Baselines

To validate whether two assumptions mentioned in Section 3 are useful for topic extraction, we compare
the proposed IATM with the following state-of-the-art baselines and its two variants:

Text Analysis Only:
(1) SATM (Quan et al., 2015) combines the aggregation of short texts and topic inference into an

unified model.
(2) BTM (Yan et al., 2013) directly models the word co-occurrence patterns in the whole corpus.
(3) LCTM (Hu and Tsujii, 2016) uses word embeddings pretrained by a log-linear word2vec model5

to deduce topics for tackling the data sparsity.
Text and Stucture Analysis:
(4) LeadLDA (Li et al., 2016a) generates words according to topic dependencies derived from con-

versation trees.
(5) ForumLDA (Chen and Ren, 2017) models the generation of topics by discriminating response

messages relevant or irrelevant to the original post.
Two Variants:
(6) IATM (-interaction) is the IATM without dynamic interactions between users, which just takes

text information into account and also uses neural variational inference to deduce topics.

4http://open.weibo.com/wiki/2/search/topics
5https://code.google.com/archive/p/word2vec/



1405

(7) IATM (-user attention) considers the interactions between users, but not the various effects of
users on the topic.

4.4 Experiment Settings

The hyperparameters of SATM, BTM, LCTM, LeadLDA and ForumLDA are set according to the best
hyperparameters reported in their original papers. We run Gibbs samplings (in SATM, BTM, LCT-
M, LeadLDA and ForumLDA) with 1000 iterations to ensure convergence. For IATM (-interaction) and
IATM (-user attention), the parameter settings are kept the same as IATM. Here, we set the vertex embed-
ding dimension d as 200. We take the hyperparameters which achieve the best performance on datasets
via an small grid search over combinations of the initial learning rate [0.001, 0.0001], α ∈ [0.1, 1],
β ∈ [0.1, 1] and γ ∈ [0.1, 1]. Finally, learning rate is set as 0.001, α as 1.0, β as 0.3 and γ as 0.3. The
other parameters are initialized by randomly sampling from normal distribution N (1.0, 0.32).

4.5 Performance Evaluation

Model K = 50 K = 100N = 10 N = 15 N = 20 N = 10 N = 15 N = 20
SATM -76.10 -190.38 -341.22 -76.07 -190.48 -341.46
BTM -79.27 -181.95 -326.77 -79.58 -183.00 -319.28

LCTM -70.91 -165.37 -296.36 -58.65 -140.10 -261.40
LeadLDA -53.91 -138.53 -258.38 -58.15 -141.34 -261.65

ForumLDA -55.76 -129.57 -231.90 -55.84 -132.23 -236.89
IATM (-interaction) -60.33 -145.98 -274.08 -64.26 -152.74 -280.40

IATM (-user attention) -57.77 -126.06 -240.66 -59.57 -131.79 -233.54
IATM -43.34 -112.64 -228.27 -47.32 -121.46 -219.96

Table 2: Coherence scores on May. Higher is better.

Model K = 50 K = 100N = 10 N = 15 N = 20 N = 10 N = 15 N = 20
SATM -31.04 -24.30 -58.39 -29.74 -22.02 -55.00
BTM -78.79 -179.99 -321.74 -75.77 -176.13 -315.43

LCTM -91.72 -208.75 -367.76 -81.88 -181.57 -323.16
LeadLDA -63.54 -150.18 -278.19 -72.07 -169.80 -309.40

ForumLDA -78.22 -140.46 -229.62 -82.33 -160.46 -258.72
IATM (-interaction) -74.61 -180.84 -340.83 -67.29 -161.30 -301.66

IATM (-user attention) -69.75 -147.53 -234.73 -61.34 -148.01 -280.06
IATM -46.69 -113.09 -213.61 -59.11 -133.96 -225.48

Table 3: Coherence scores on June. Higher is better.

Model K = 50 K = 100N = 10 N = 15 N = 20 N = 10 N = 15 N = 20
SATM -128.28 -254.45 -431.88 -128.68 -254.74 -432.01
BTM -73.26 -172.43 -313.12 -76.10 -176.67 -320.70

LCTM -72.78 -160.08 -275.58 -63.56 -137.36 -238.31
LeadLDA -70.40 -157.83 -268.23 -59.75 -130.83 -226.62

ForumLDA -89.16 -215.47 -396.20 -89.96 -213.59 -386.65
IATM (-interaction) -93.59 -224.17 -409.84 -91.96 -233.86 -396.22

IATM (-user attention) -61.60 -144.75 -251.46 -57.00 -127.57 -227.81
IATM -50.75 -119.48 -212.26 -46.80 -110.27 -204.35

Table 4: Coherence scores on July. Higher is better.



1406

Following (Li et al., 2016a; Yan et al., 2013), we set the number of topics to 50 (K = 50) and 100
(K = 100). K = 50 is to match the count of hashtags and K = 100 is much larger than the real number
of topics. As shown in Tab. 2, 3 and 4, we evaluate the top N = 10, 15, 20 words of K = 50 and
K = 100. From these tables, we have the following overall observations:

(1) What call for special attention is that, SATM exhibits the unstable performance on various datasets.
Specifically, SATM performs poorly on May, the worst on July and the best on June. It may be ascribed to
its heavy reliance on the number of pseudo-documents. On the contrary, IATM has a stable performance
and is only outperformed by SATM on June.

(2) Topic models that analyze text and structures perform better than the ones with only text excep-
t SATM on June. It indicates that considering texts and structures together is necessary due to their
reciprocal influence in a social network.

(3) Our proposed IATM achieves crucial improvement in comparison with all the baselines on May
and July, and it gives the greater coherence scores than the other baselines expect SATM on June. The
reasons are two-fold: 1) It effectively identifies topics via simultaneously considering social media con-
tent and network topology. 2) It further produces interaction-aware edge embedding as a deeper edge
representation combined dynamic user interactions and user attention.

Assumptions K = 50, N = 20 K = 100, N = 20

Dynamic Interactions 27.32% 22.13%
User Attention 9.91% 11.87%

Table 5: The effects of two hypotheses by comparing IATM with its two variants.

To further evaluate the effectiveness of two assumptions, we compare IATM with its two variants,
observations are as follows:

(4) IATM (-interaction) gives the worst coherence scores. The reason is that it has no consideration of
social context. After introducing dynamic interactions, IATM (-user attention) obtains the considerable
improvement. Moreover, IATM outperforms IATM (-user attention) on three datasets whatever different
number of topics. It demonstrates the effectiveness of dynamic interactions (H1) and user attention (H2).

(5) For each component, we compute the average growth percentage under top N = 20 words of
K = 50 and K = 1006 in comparison of IATM and two variants. Seen from Tab. 5, two hypotheses are
both useful for topic extraction in social media, and dynamic interactions modeling has greater influence
than user attention.

4.6 Case Study

To get an intuitive understanding of extracted topics, we design an experiment to visualize the top 10
words about “MH17 Crash” induced by the different models when K = 50, shown in Fig. 3. Due
to “MH17 Crash” is included in the July dataset, LCTM gives the highest coherence score among the
models of text analysis only. Then we choose LCTM, LeadLDA and ForumLDA as the competitors of
our proposed method due to the limited space. Note that, individual words accompany with diverse font
sizes. The larger the font size is, the more relevant the word is to the topic. We have the following
observations from Fig. 3:

(1) As for LCTM based on word embeddings, “Malaysian Media” is the top one key word and “killed”,
“crash”, “sad” and “Ukraine” is correlated to the topic. However, it mistakenly groups “Argentina lose
the World Cup”, which co-occurs with “sad” and “Argentina”. Compared with LeadLDA, ForumLDA
and IATM, LCTM performs the worst. It further testifies that the analysis of texts and structures in a
social network is necessary due to their relevance in social media.

(2) With respect to LeadLDA, which distinguishes every message from leader and follower, “crash”
is the top one key word and “killed”, “Ukraine”, “shoot down” and “Malaysia” is related to the topic.
However, “bus” is falsely aggregated, which is relevant to the bus explosion in Guangzhou. Maybe it is

6The ones under other settings are not shown due to the limited space.



1407

马媒(Malaysian Media)
难过(sad)

主页(home page)

飞机(airplane)

信号(signal)

阿根廷(Argentina)

坠毁(crash)

遇难(killed)置顶(top)

乌克兰(Ukraine)

(a) LCTM.

坠毁(crash)
香港(Hong Kong)

遇难(killed)
乌克兰(Ukraine)

马来西亚(Malaysia)

击落(shoot down)

普京(Vladimir Putin)

飞机(airplane)
小米(Xiaomi)

公交车(bus)

(b) LeadLDA.

坠毁(crash)

国籍(nationality)

马航(Malaysia Airlines)

乌克兰(Ukraine)

马来西亚(Malaysia)

鹿晗(Lu Han)

婴儿(baby)

喜欢(like)

电影(movie)

支持(support)

(c) ForumLDA.

马航(Malaysia Airlines)
乌克兰(Ukraine)

小米(Xiaomi)

俄罗斯(Russia)

击落(shoot down)

客机(airliner)

支持(support)

遇难(killed)
坠毁(crash)

马来西亚(Malaysia)

(d) IATM.

Figure 3: Word clouds describing “MH17 Crash” of different models. Each word cloud represents the
similar topic generated by the corresponding model with top 10 words. English translations for the
original Chinese words are inside the brackets.

because it wrongly recognizes the reposting message about the bus explosion in Guangzhou as a leader
message.

(3) ForumLDA differentiates reposting messages from relevant and irrelevant response posts. It gener-
ates the top one key word “crash”, and “Malaysia Airlines”, “Ukraine”, “Malaysia” and “nationality” are
correlated to the topic. However, “Lu Han” and “movie” about a movie starring Lu Han, are mistakenly
mixed. Perhaps it is because no prior knowledge is given to ensure the validity of relevance distinction.

(4) The top words generated by IATM, “Malaysia Airlines”, “Malaysia”, “crash” and “Ukrain”, are
related to the topic. Moreover, some detailed information “airliner” is also produced. Nevertheless, the
generated “Xiaomi” is not relevant to the topic. We check the corpus and find that there are lots of posts
tagged one hashtag, but expressing the other topic. For instance, someone makes product promotion of
“Xiaomi” by utilizing the hot event “MH17 Crash”. This belongs to a spam message.

5 Conclusion and Future Work

We propose an Interaction-Aware Topic Model (IATM) for microblog conversations through integrating
dynamic interactions and various user attention. Our method not only makes full use of content and
structure information in social media, but also keeps the better consistency of generated topics by vari-
ational auto-encoder. We model user behaviours through transforming conversation trees into network.
Further interaction-aware embeddings produced by adding diverse effects of different users on a topic,
encode the content and structure information of two neighbour users. This helps to incorporate the con-
versation context, and is easy to absorb user attention. Then it is plugged in neural variational inference
to generate topics. Experiments on three real-world microblog datasets demonstrate that the proposed
IATM is effective.

Yet lots of people would like to utilize hot events to have a product promotion and post some irrelevant
messages. These spams damagingly affect the performance of IATM. In the future, we will unify spam
posts separation during topics deduction.

Acknowledgments
The work was supported by National Natural Science Foundation of China (61472277, 61772361,
61771333). We would like to thank anonymous reviewers for the valuable and detailed comments and
suggestions.

References
Bin Bi, Yuanyuan Tian, Yannis Sismanis, Andrey Balmin, and Junghoo Cho. 2014. Scalable topic-specific influ-

ence analysis on microblogs. In Proceedings of the 7th ACM international conference on Web search and data
mining, pages 513–522.



1408

David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of Machine
Learning Research, 3(Jan):993–1022.

Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L Boyd-Graber, and David M Blei. 2009. Reading tea leaves:
How humans interpret topic models. In Advances in Neural Information Processing Systems, pages 288–296.

Chaotao Chen and Jiangtao Ren. 2017. Forum latent dirichlet allocation for user interest discovery. Knowledge-
Based Systems, 126:1–7.

Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, Jun Zhao, et al. 2015. Event extraction via dynamic multi-
pooling convolutional neural networks. In Proceedings of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages
167–176.

Liangjie Hong and Brian D Davison. 2010. Empirical study of topic modeling in twitter. In Proceedings of the
1st Workshop on Social Media Analytics, pages 80–88.

Weihua Hu and Junichi Tsujii. 2016. A latent concept topic model for robust topic inference using word embed-
dings. In The 54th Annual Meeting of the Association for Computational Linguistics, pages 380–386.

Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. Proceedings of the Inter-
national Conference on Learning Representations.

Diederik P Kingma and Max Welling. 2014. Auto-encoding variational bayes. In Proceedings of the International
Conference on Learning Representations.

Jing Li, Ming Liao, Wei Gao, Yulan He, and Kam-Fai Wong. 2016a. Topic extraction from microblog posts
using conversation structures. In Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics.

Yang Li, Ting Liu, Jing Jiang, and Liang Zhang. 2016b. Hashtag recommendation with topical attention-based
lstm. In Proceedings of the 26th International Conference on Computational Linguistics.

Kar Wai Lim, Changyou Chen, and Wray L. Buntine. 2013. Twitter-Network Topic Model: A full Bayesian
treatment for social network and text modeling. In Advances in Neural Information Processing Systems: Topic
Models Workshop, pages 1–5.

Hengyang Lu, Lu-Yao Xie, Ning Kang, Chong-Jun Wang, and Jun-Yuan Xie. 2017. Don’t forget the quantifiable
relationship between words: Using recurrent neural network for short text topic discovery. In Proceedings of
the 31st AAAI Conference on Artificial Intelligence, pages 1192–1198.

Rishabh Mehrotra, Scott Sanner, Wray Buntine, and Lexing Xie. 2013. Improving lda topic models for microblogs
via tweet pooling and automatic labeling. In Proceedings of the 36th International ACM SIGIR Conference on
Research and Development in Information Retrieval, pages 889–892.

Yishu Miao, Edward Grefenstette, and Phil Blunsom. 2017. Discovering discrete latent topics with neural varia-
tional inference. In Proceedings of the 34th International Conference on Machine Learning.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations
in vector space. CoRR, abs/1301.3781, abs/1312.6114.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of
words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages
3111–3119.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word
representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, volume 13, pages 746–751.

David Mimno, Hanna M Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. Optimiz-
ing semantic coherence in topic models. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing, pages 262–272.

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In
Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
pages 701–710.



1409

Xiaojun Quan, Chunyu Kit, Yong Ge, and Sinno Jialin Pan. 2015. Short and sparse text topic modeling via
self-aggregation. In Proceedings of the 24th International Joint Conference on Artificial Intelligence, pages
2270–2276.

Cicero Dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. 2016. Attentive pooling networks. CoRR, ab-
s/1602.03609.

Vivek Kumar Rangarajan Sridhar. 2015. Unsupervised topic modeling for short texts using distributed representa-
tions of words. In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,
pages 192–200.

Akash Srivastava and Charles Sutton. 2017. Autoencoding variational inference for topic models. Proceedings of
the International Conference on Learning Representations.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout:
a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–
1958.

Jian Tang, Ming Zhang, and Qiaozhu Mei. 2013. One theme in all views: modeling consensus topics in multiple
contexts. In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pages 5–13.

Cunchao Tu, Han Liu, Zhiyuan Liu, and Maosong Sun. 2017. Cane: Context-aware network embedding for
relation modeling. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,
volume 1, pages 1722–1731.

Xuepeng Wang, Kang Liu, and Jun Zhao. 2017. Handling cold-start problem in review spam detection by jointly
embedding texts and behaviors. In Proceedings of the 55th Annual Meeting of the Association for Computa-
tional Linguistics, volume 1, pages 366–376.

Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang, Ming Zhou, and Wei-Ying Ma. 2017. Topic aware neural
response generation. In Proceedings of the 31st AAAI Conference on Artificial Intelligence, pages 3351–3357.

Xiaohui Yan, Jiafeng Guo, Yanyan Lan, and Xueqi Cheng. 2013. A biterm topic model for short texts. In
Proceedings of the 22nd International Conference on World Wide Web, pages 1445–1456.

Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He, Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. 2011.
Comparing twitter and traditional media using topic models. In Proceedings of the 33rd European Conference
on Information Retrieval, pages 338–349.

Hao Zhuang, Rameez Rahman, Xia Hu, Tian Guo, Pan Hui, and Karl Aberer. 2016. Data summarization with
social contexts. In Proceedings of the 25th ACM International on Conference on Information and Knowledge
Management, pages 397–406.


