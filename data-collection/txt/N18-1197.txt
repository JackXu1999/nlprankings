



















































Learning with Latent Language


Proceedings of NAACL-HLT 2018, pages 2166–2179
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Learning with Latent Language

Jacob Andreas Dan Klein Sergey Levine
Computer Science Division

University of California, Berkeley
{jda,klein,svlevine}@eecs.berkeley.edu

Abstract

The named concepts and compositional opera-
tors present in natural language provide a rich
source of information about the abstractions
humans use to navigate the world. Can this
linguistic background knowledge improve the
generality and efficiency of learned classifiers
and control policies? This paper aims to
show that using the space of natural language
strings as a parameter space is an effective
way to capture natural task structure. In
a pretraining phase, we learn a language
interpretation model that transforms inputs
(e.g. images) into outputs (e.g. labels) given
natural language descriptions. To learn a new
concept (e.g. a classifier), we search directly
in the space of descriptions to minimize
the interpreter’s loss on training examples.
Crucially, our models do not require language
data to learn these concepts: language is
used only in pretraining to impose structure
on subsequent learning. Results on image
classification, text editing, and reinforcement
learning show that, in all settings, models with
a linguistic parameterization outperform those
without.1

1 Introduction

The structure of natural language reflects the struc-
ture of the world. For example, the fact that it is
easy for humans to communicate the concept left
of the circle but comparatively difficult to com-
municate mean saturation of the first five pixels
in the third column reveals something about the
abstractions we find useful for interpreting and
navigating our environment (Gopnik and Meltzoff,
1987). In machine learning, efficient automatic
discovery of reusable abstract structure remains a
major challenge. This paper investigates whether

1Code and data are available at https://github.com/
jacobandreas/l3.

0.0

0.9

0.8

truetrue
true

true

concept 
learning:

evaluation:

there is a  
green square

a gray square is 
above a square

a red cross is 
below a square

0.2

a red cross is 
below a square

Figure 1: Example of our approach on a binary image
classification task. We assume access to a pretrained
language interpretation model that outputs the proba-
bility that an image matches a given description. To
learn a new visual concept, we search in the space of
natural language descriptions to maximize the interpre-
tation model’s score (top). The chosen description can
be used with the interpretation model to classify new
images (bottom). Figures best viewed in color.

background knowledge from language can pro-
vide a useful scaffold for acquiring it. We specif-
ically propose to use language as a latent param-
eter space for few-shot learning problems of all
kinds, including classification, transduction and
policy search. We aim to show that this linguis-
tic parameterization produces models that are both
more accurate and more interpretable than direct
approaches to few-shot learning.

Like many recent frameworks for multitask-
and meta-learning, our approach consists of three
phases: a pretraining phase, a concept-learning
phase, and an evaluation phase. Here, the product
of pretraining is a language interpretation model
that maps from descriptions to predictors (e.g. im-
age classifiers or reinforcement learners). Our the-
sis is that language learning is a powerful, general-
purpose kind of pretraining, even for tasks that do
not directly involve language.

New concepts are learned by searching directly
in the space of natural language strings to mini-

2166



?.???

(a) language learning

a white cross is left 
of a yellow shape

truetrue
true false

x(�i)

y(�i)

x(c)

y(c)

x(e)

y(e)

w(�i)

truetrue
true

(b) concept learning (c) evaluation

???

Figure 2: Formulation of the learning problem. Ul-
timately, we care about our model’s ability to learn a
concept from a small number of training examples (b)
and successfully generalize it to held-out data (c). In
this paper, concept learning is supported by a language
learning phase (a) that makes use of natural language
annotations on other learning problems. These annota-
tions are not provided for the real target task in (b–c).

mize the loss incurred by the language interpre-
tation model (Figure 1). Especially on tasks that
require the learner to model high-level composi-
tional structure shared by training examples, natu-
ral language hypotheses serve a threefold purpose:
they make it easier to discover these compositional
concepts, harder to overfit to few examples, and
easier for humans to understand inferred patterns.

Our approach can be implemented using a stan-
dard kit of neural components, and is simple
and general. In a variety of settings, we find
that the structure imposed by a natural-language
parameterization is helpful for efficient learning
and exploration. The approach outperforms both
multitask- and meta-learning approaches that map
directly from training examples to outputs by way
of a real-valued parameterization, as well as ap-
proaches that make use of natural language anno-
tations as an additional supervisory signal rather
than an explicit latent parameter. The natural lan-
guage concept descriptions inferred by our ap-
proach often agree with human annotations when
they are correct, and provide an interpretable de-
bugging signal when incorrect. In short, by equip-
ping models with the ability to “think out loud”
when learning, they become both more compre-
hensible and more accurate.

2 Background

Suppose we wish to solve an image classifica-
tion problem like the one shown in Figure 2b–c,
mapping from images x to binary labels y. One
straightforward way to do this is to solve a learn-

ing problem of the following form:

argmin
η ∈H

∑

(x, y)

L(f(x; η), y) , (1)

where L is a loss function and f is a richly-
parameterized class of models (e.g. convolutional
networks) indexed by η (e.g. weight matrices) that
map from images to labels. Given a new image x′,
f(x′; η) can be used to predict its label.

In the present work, we are particularly in-
terested in few-shot learning problems where the
number of (x, y) pairs is small—on the order of
five or ten examples. Under these conditions, di-
rectly solving Equation 1 is a risky proposition—
any model class powerful enough to capture the
true relation between inputs and outputs is also
likely to overfit. For few-shot learning to be suc-
cessful, extra structure must be supplied to the
learner. Existing approaches obtain this struc-
ture by either carefully structuring the hypothe-
sis space or providing the learner with alternative
training data. The approach we present in this pa-
per combines elements of both, so we begin with
a review of existing work.

(Inductive) program synthesis approaches
(e.g. Gulwani, 2011) reduce the effective size of
the hypothesis class H by moving the optimiza-
tion problem out of the continuous space of weight
vectors and into a discrete space of formal pro-
gram descriptors (e.g. regular expressions or Pro-
log queries). Domain-specific structure like ver-
sion space algebras (Lau et al., 2003) or type
systems (Kitzelmann and Schmid, 2006) can be
brought to bear on the search problem, and the
bias inherent in the syntax of the formal language
provides a strong prior. But while program syn-
thesis techniques are powerful, they are also lim-
ited in their application: a human designer must
hand-engineer the computational primitives nec-
essary to compactly describe every learnable hy-
pothesis. While reasonable for some applications
(like string editing), this is challenging or impos-
sible for others (like computer vision).

An alternative class of multitask learning ap-
proaches (Caruana, 1998) import the relevant
structure from other learning problems rather than
defining it manually (Figure 2a, top). Since we
may not know a priori what set of learning prob-
lems we ultimately wish to evaluate on, it is use-
ful to think of learning as taking places in three
phases:

2167



1. a pretraining (or “meta-training”) phase that
makes use of various different datasets i with
examples {(x(`i)1 , y

(`i)
1 ), . . . , (x

(`i)
n , y

(`i)
n )}

(Figure 2a)

2. a concept-learning phase in which the
pretrained model is adapted to fit data
{(x(c)1 , y

(c)
1 ), . . . , (x

(c)
n , y

(c)
n )} for a specific

new task (Figure 2b)

3. an evaluation phase in which the learned
concept is applied to a new input x(e) to pre-
dict y(e) (Figure 2c)

In these approaches, learning operates over two
collections of parameters: shared parameters η
and task-specific parameters θ. In pretraining,
multitask approaches find:

argmin
η ∈Ra, θ(`i) ∈Rb

∑

i, j

L
(
f(x

(`i)
j ; η, θ

(`i)), y
(`i)
j

)
. (2)

At concept learning time, they solve for:

argmin
θ(c) ∈Rb

∑

j

L
(
f(x

(c)
j ; η, θ

(c)), y
(c)
j

)
(3)

on the new dataset, then make predictions for new
inputs using f(x(e); η, θ(c)).

Closely related meta-learning approaches (e.g.
Schmidhuber, 1987; Santoro et al., 2016; Vinyals
et al., 2016) make use of the same data, but col-
lapse the inner optimization over θ(c) and predic-
tion of y(e) into a single learned model.

3 Learning with Language

In this work, we are interested in developing a
learning method that enjoys the benefits of both
approaches. In particular, we seek an intermediate
language of task representations that, like in pro-
gram synthesis, is both expressive and compact,
but like in multitask approaches is learnable di-
rectly from training data without domain engineer-
ing. We propose to use natural language as this
intermediate representation. We call our approach
learning with latent language (L3).

Natural language shares many structural advan-
tages with the formal languages used in synthesis
approaches: it is discrete, has a rich set of com-
positional operators, and comes equipped with a
natural description length prior. But it also has a
considerably more flexible semantics. And cru-
cially, plentiful annotated data exists for learning
this semantics: we cannot hand-write a computer

program to recognize a small dog, but we can learn
how to do it from image captions. More basically,
the set of primitive operators available in language
provides a strong prior about the kinds of abstrac-
tions that are useful for natural learning problems.

Concretely, we replace the pretraining phase
above with a language-learning phase. We as-
sume that at language-learning time we have ac-
cess to natural-language descriptions w(`i) (Fig-
ure 2a, bottom). We use these w as parameters, in
place of the task-specific parameters θ—that is, we
learn a language interpretation model f(x; η, w)
that uses shared parameters η to turn a description
w into a function from inputs to outputs. For the
example in Figure 2, f might be an image rating
model (Socher et al., 2014) that outputs a scalar
judgment y of how well an image xmatches a cap-
tion w.

Because these natural language parameters are
observed at language-learning time, we need only
learn the real-valued shared parameters η used for
their interpretation (e.g. the weights of a neural
network that implements the image rating model):

argmin
η ∈Ra

∑

i, j

L
(
f(x

(`i)
j ; η, w

(`i)), y
(`i)
j

)
. (4)

At concept-learning time, conversely, we solve
only the part of the optimization problem over nat-
ural language strings:

argmin
w(c) ∈Σ∗

∑

j

L
(
f(x

(c)
j ; η, w

(c)), y
(c)
j

)
. (5)

This last step presents something of a chal-
lenge. When solving the corresponding optimiza-
tion problem, synthesis techniques can exploit the
algebraic structure of the formal language, while
end-to-end learning approaches take advantage of
differentiability. Here we can’t do either—the lan-
guage of strings is discrete, and any structure in
the interpretation function is wrapped up inside
the black box of f . Inspired by related techniques
aimed at making synthesis more efficient (Devlin
et al., 2017), we use learning to help us develop an
effective optimization procedure for natural lan-
guage parameters.

In particular, we simply use the language-
learning datasets, consisting of pairs (x(`i)j , y

(`i)
j )

and descriptions wi, to fit a reverse proposal
model, estimating:

argmaxλ
∑

i log q(wi|x
(`i)
1 , y

(`i)
1 , . . . , x

(`i)
n , y

(`i)
n ;λ)

2168



true

a white shape is 
left of a yellow 

semicircle

true true

true true

Figure 3: The few-shot image classification task.
Learners are shown four positive examples of a visual
concept (left) and must determine whether a fifth image
matches the pattern (right). Natural language annota-
tions are provided during language learning but must
be inferred for concept learning.

where q provides a (suitably normalized) approx-
imation to the distribution of descriptions given
task data. In the running example, this pro-
posal distribution is essentially an image caption-
ing model (Donahue et al., 2015). By sampling
from q, we expect to obtain candidate descriptions
that are likely to obtain small loss. But our ulti-
mate inference criterion is still the true model f :
at evaluation time we perform the minimization in
Equation 5 by drawing a fixed number of samples,
selecting the hypothesis w(c) that obtains the low-
est loss, and using f(x(e); η, w(c)) to make pre-
dictions.

What we have described so far is a generic pro-
cedure for equipping collections of related learn-
ing problems with a natural language hypothesis
space. In Sections 4 and 5, we describe how this
procedure can be turned into a concrete algorithm
for supervised classification and sequence predic-
tion. In Section 6, we describe how to extend these
techniques to reinforcement learning.

4 Few-shot Classification

We begin by investigating whether natural lan-
guage can be used to support high-dimensional
few-shot classification. Our focus is on visual
reasoning tasks like the one shown in Figure 3.
In these problems, the learner is presented with
four images, all positive examples of some visual
concept like a blue shape near a yellow triangle,
and must decide whether a fifth, held-out image
matches the same concept. These kinds of rea-
soning problems have been well-studied in visual
question answering settings (Johnson et al., 2017;
Suhr et al., 2017). Our version of the problem,
where the input and output feature no text data,
but an explanation must be inferred, is similar to

the visual reasoning problems proposed by Raven
(1936) and Bongard (1968).

To apply the recipe in Section 2, we need to
specify an implementation of the interpretation
model f and the proposal model q. We begin
by computing representations of input images x.
We start with a pre-trained 16-layer VGGNet (Si-
monyan and Zisserman, 2014). Because spatial in-
formation is important for these tasks, we extract a
feature representation from the final convolutional
layer of the network. This initial featurization is
passed through two fully-connected layers to form
a final image representation, as follows:

x VGG-16 FC ReLU FC rep( )x

We define interpretation and proposal models:2

f(x;w) = σ
(
rnn-encode(w)>rep(x)

)

q(w | {xj}) = rnn-decode
(
w | 1n

∑
j
rep(xj)

)

The interpretation model f outputs the probabil-
ity that x is assigned a positive class label, and is
trained to maximize log-likelihood. Because only
positive examples are provided in each language
learning set, the proposal model q can be defined
in terms of inputs alone. Details regarding training
hyperparameters, RNN implementations, etc. may
be found in Appendix A.

Our evaluation aims to answer two questions.
First, does the addition of language to the learning
process provide any benefit over ordinary multi-
task or meta-learning? Second, is it specifically
better to use language as a hypothesis space for
concept learning rather than just an additional sig-
nal for pretraining? We use several baselines to
answer these questions:

1. Multitask: a multitask baseline in which
the definition of f above is replaced by
σ(θ>i rep(x)) for task-specific parameters θi
that are optimized during both pretraining
and concept-learning.

2. Meta: a meta-learning baseline in which f is
defined by σ([ 1n

∑
j rep(xj)]

>rep(x)).3

2Suppressing shared parameters η and λ for clarity.
3Many state-of-the-art approaches to meta-learning for

classification (e.g. Snell et al., 2017) are not well-defined
for possibly-overlapping evaluation classes with only positive
examples provideded. Here we have attempted to provide a
robust implementation that is as close as possible to the other
systems under evaluation.

2169



3. Meta+Joint: as in Meta, but the pretraining
objective includes an additional term for pre-
dicting q (discarded for concept learning).

We report results on a dataset derived from
the ShapeWorld corpus of Kuhnle and Copestake
(2017). In this dataset the held-out image matches
the target concept 50% of the time. In the val-
idation and test folds, half of learning problems
feature a concept that also appears in the language
learning set (but with different exemplar images),
while the other half feature both new images and
a new concept. Images contain two or three dis-
tractor shapes unrelated to the objects that define
the target concept. Captions in this dataset were
generated from DMRS representations using an
HPS grammar (Copestake et al., 2016). (Our re-
maining experiments use human annotators.) The
dataset contains a total of 9000 pretraining tasks
and 1000 of each validation and test tasks. More
dataset statistics are provided in Appendix B.

Results are shown in Table 1. It can be seen
that L3 provides consistent improvements over
the baselines, and that these improvements are
present both when identifying new instances of
previously-learned concepts and when discover-
ing new ones. Some example model predictions
are shown in Figure 4. The model often succeeds
in making correct predictions, even though its in-
ferred descriptions rarely match the ground truth.
Sometimes this is because of inherent ambiguity
in the description language (Figure 4a), and some-
times because the model is able to rule out candi-
dates on the basis of partial captions alone (Fig-
ure 4b, where it is sufficient to recognize that the

Model Val (old) Val (new) Val Test

Random 50 50 50 50
Multitask 64 49 57 59
Meta 63 62 62 64
Meta+Joint 63 69 66 64
L3 (ours) 70 72 71 70

L3 (oracle) 77 80 79 78

Table 1: Evaluation on image classification. Val
(old) and Val (new) denote subsets of the validation set
that contain respectively previously-used and novel vi-
sual concepts. L3 consistently outperforms alternative
learning methods based on multitask learning, meta-
learning, and meta-learning jointly trained to predict
descriptions (Meta+Joint). The last row shows results
when the model is given a ground-truth concept de-
scription rather than having to infer it from examples.

target concept involves a circle). More examples
are provided in Appendix C.

5 Programming by Demonstration

Next we explore whether the same technique can
be applied to tasks that involve more than binary
similarity judgments. We focus on structured pre-
diction: specifically a family of string processing
tasks. In these tasks, the model is presented with
examples of five strings transformed according to
some rule; it must then apply an appropriate trans-
formation to a sixth (Figure 5). Learning proceeds
as in the previous section, with:

rep(x, y) = rnn-encode([x, y])

f(y | x;w) =
rnn-decode

(
y | [rnn-encode(x), rnn-encode(w)]

)

q(w | {(xj , yj)}) =
rnn-decode

(
w | 1n

∑
j
rep(xj , yj)

)

Baselines are analogous to those for classification.
While string editing tasks of the kind shown

in Figure 5 are popular in both the programming
by demonstration literature (Singh and Gulwani,
2012) and the semantic parsing literature (Kush-
man and Barzilay, 2013), we are unaware of any
datasets that support both learning paradigms at
the same time. We have thus created a new dataset
of string editing tasks by (1) sampling random reg-
ular transducers, (2) applying these transducers to
collections of dictionary words, and (3) showing
the collected examples to Mechanical Turk users

a blue cross is above 
a pentagon

a cyan pentagon is to 
the right of a 
magenta shape

false

true

(a)

(b)

(c)

examples true description true label

pred. description pred. label

a square is above a 
red cross

a red cross is below 
a square

true

true

a circle is above a 
yellow circle

a cyan circle is to 
the left of a 

rectangle

false

false

Figure 4: Example predictions for image classifica-
tion. The model achieves high accuracy even though
predicted descriptions rarely match the ground truth.
High-level structure like the presence of certain shapes
or spatial relations is consistently recovered.

2170



Model Val Test

Identity 18 18
Multitask 54 50
Meta 66 62
Meta+Joint 63 59
L3 80 76

Table 2: Results for string editing. The reported num-
ber is the percentage of cases in which the predicted
string exactly matches the reference. L3 is the best per-
forming model; using language data for joint training
rather than as a hypothesis space provides little benefit.

and asking them to provide a natural language ex-
planation with their best guess about the under-
lying rule. The dataset thus features both multi-
example learning problems, as well as structured
and unstructured annotations for each target con-
cept. There are 3000 tasks for language learning
and 500 tasks for each of validation and testing
(Appendix B). Annotations are included in the
code release for this paper.

Results are shown in Table 2. In these experi-
ments, all models that use descriptions have been
trained on the natural language supplied by human
annotators. While we did find that the Meta+Joint
model converges considerably faster than all the
others, its final performance is somewhat lower
than the baseline Meta model. As before, L3 out-
performs alternative approaches for learning di-
rectly from examples with or without descriptions.

Because all of the transduction rules in this
dataset were generated from known formal de-
scriptors, these tasks provide an opportunity to
perform additional analysis comparing natural lan-
guage to more structured forms of annotation
(since we have access to ground-truth regular ex-
pressions) and more conventional synthesis-based
methods (since we have access to a ground-truth
regular expression execution engine). We addi-
tionally investigate the effect of the number of

warding 
curved 
uranium 
pedaled 
drum

warying 
curved 
uranium 
peyaled 
drum

replace d before a 
vowel with y

s/d([aeiou])/y\1/g

chided

chiyed

Figure 5: Example string editing task. Learners are
presented with five examples of strings transformed ac-
cording to some rule (left), and must apply an appropri-
ate transformation to a sixth string (right). Language-
learning annotations (center) may take the form of ei-
ther natural language or regular expressions.

change any n  
to a c

replace all n s 
with c

loocies

loocies

replace consonant - 
vowel pairings with n

replace pairs of 
letters consisting of a 
consonant followed by a 

vowel with an n

ntnynd

ntnnd

(a)

(b)

(c)

examples true description true output

pred. description pred. output

emboldens 
kisses 
loneliness 
vein 
dogtrot

emboldecs 
kisses 
locelicess 
veic 
dogtrot

loonies

mapper 
concluding 
excuse 
effete 
contracting

npnr 
nncnnng 
exnn 
efnn 
nntncnng

betrayed

plummest 
bereaving 
eddied 
struggles 
evils

plummesti 
bereavinti 
eddieti 
struggleti 
evilti

change the last 
letter of the word 

into t i

replace the last letter 
of the word with t i

mistrialti

mistrialti

mistrials
Figure 6: Example predictions for string editing.

samples drawn from the proposal model. These
results are shown in Table 3.

A few interesting facts stand out. Under the
ordinary evaluation condition (with no ground-
truth annotations provided), language-learning
with natural language data is actually better than
language-learning with regular expressions. This
might be because the extra diversity helps the
model determine the relevant axes of variation and
avoid overfitting to individual strings. Allowing
the model to do its own inference is also better
than providing ground-truth natural language de-
scriptions, suggesting that it is actually better at
generalizing from the relevant concepts than our
human annotators (who occasionally write things
like I have no idea for the inferred rule). Unsur-
prisingly, with ground truth REs (which unlike the
human data are always correct) we can do bet-
ter than any of the models that require inference.
Coupling our inference procedure with an oracle
RE evaluator, we essentially recover the synthesis-
based approach of Devlin et al. (2017). Our find-
ings are consistent with theirs: when an exact exe-
cution engine is available, there is no reason not to
use it. But we can get almost 90% of the way there

Annotations Samples Oracle1 100 Ann. Eval.

None (Meta) 66 – – –
Natural language 66 80 75 –
Regular expressions 60 76 88 90

Table 3: Inference and representation experiments for
string editing. Italicized numbers correspond to entries
in Table 2. Allowing the model to use multiple samples
rather than the 1-best decoder output substantially im-
proves performance. The full model does better with
inferred natural language descriptions than either regu-
lar expressions or ground-truth natural language.

2171



with an execution model learned from scratch. Ex-
amples of model behavior are shown in Figure 6;
more may be found in Appendix D.

6 Policy Search

The previous two sections examined supervised
settings where the learning signal comes from few
examples but is readily accessible. In this section,
we move to a set of reinforcement learning prob-
lems, where the learning signal is instead sparse
and time-consuming to obtain. We evaluate on a
collection of 2-D treasure hunting tasks. These
tasks require the agent to discover a rule that de-
termines the location of buried treasure in a large
collection of environments of the kind shown in
Figure 7. To recover the treasure, the agent must
navigate (while avoiding water) to its goal loca-
tion, then perform a DIG action. At this point
the episode ends; if the treasure is located in the
agent’s current position, it receives a reward, oth-
erwise it does not. In every task, the treasure has
consistently been buried at a fixed position relative
to some landmark (in Figure 7 a heart). Both the
offset and the identity of the target landmark are
unknown to the agent, and the location of the land-
mark varies across maps. Indeed, there is noth-
ing about the agent’s observations or action space
to suggest that landmarks and offsets are even the
relevant axes of variation across tasks: only the
language reveals this structure.

The interaction between language and learning
in these tasks is rather different from the super-
vised settings. In the supervised case, language
serves mostly as a guard against overfitting, and

Figure 7: Example treasure hunting task: the agent
is placed in a random environment and must collect a
reward that has been hidden at a consistent offset with
respect to some landmark. At language-learning time
only, natural language instructions and expert policies
are provided. The agent must both learn primitive nav-
igation skills, like avoiding water, as well as the high-
level structure of the reward functions for this domain.

can be generated conditioned on a set of pre-
provided concept-learning observations. Here,
agents are free to interact with the environment as
much as they need, but receive observations only
during interaction. Thus our goal here will be to
build agents that can adapt quickly to new environ-
ments, rather than requiring them to immediately
perform well on held-out data.

Why should we expect L3 to help in this setting?
In reinforcement learning, we typically encourage
our models to explore by injecting randomness
into either the agent’s action space or its under-
lying parameterization. But most random policies
exhibit nonsensical behaviors; as a result, it is in-
efficient both to sample in the space of network
weights and to perform policy optimization from
a random starting point. Our hope is that when pa-
rameters are chosen from within a structured fam-
ily, a stochastic search in this structured space will
only ever consider behaviors corresponding to a
reasonable final policy, and in this way discover
good behavior faster than ordinary RL.

Here the interpretation model f describes a pol-
icy that chooses actions conditioned on the cur-
rent environment state and a linguistic parameter-
ization. As the agent initially has no observations
at all, we simply design the proposal model to gen-
erate unconditional samples from a prior over de-
scriptions. Taking x to be an agent’s current obser-
vation of the environment state, we define a state
representation network and models:

x FC tanh FC rep( )xtanh

f(a | x; w) ∝ rnn-encode(w)> Wa rep(x)
q(w) = rnn-decode(w)

This parameterization assumes a discrete action
space, and assigns to each action a probability pro-
portional to a bilinear function of the encoded de-
scription and world state. f is an instruction fol-
lowing model of a kind well-studied in natural lan-
guage processing (Branavan et al., 2009); the pro-
posal model allows it to generate its own instruc-
tions without external direction. To learn, we sam-
ple a fixed number of descriptions w from q. For
each description, we sample multiple rollouts of
the policy it induces to obtain an estimate of its av-
erage reward. Finally, we take the highest-scoring
description and fine-tune its induced policy.

At language-learning time, we assume access
to both natural language descriptions of these tar-

2172



20 40 60 80 100
Timestep (×1000)

−1.0
−0.5

0.0

0.5

1.0

1.5

2.0

2.5

3.0

A
ve

ra
ge

re
w

ar
d

L3

Multitask

Scratch

Figure 8: Treasure hunting reward obtained by each
learning algorithm across multiple evaluation environ-
ments, after language learning has already taken place
(bands show 95% confidence intervals for mean per-
formance). Multitask learns an embedding for each
task, while Scratch trains on every task individually.
L3 rapidly discovers high-scoring policies in most envi-
ronments. Dashed line indicates the end of the concept-
learning phase; subsequent performance comes from
fine-tuning. The max reward for this task is 3.

get locations provided by human annotators, as
well as expert policies for navigating to the lo-
cation of the treasure. The multitask model we
compare to replaces these descriptions with train-
able task embeddings.4 The learner is trained
from task-specific expert policies using DAgger
(Ross et al., 2011) during the language-learning
phase, and adapts to individual environments us-
ing “vanilla” policy gradient (Williams, 1992) dur-
ing the concept learning phase.

The environment implementation and linguistic
annotations are in this case adapted from a natural
language navigation dataset originally introduced
by Janner et al. (2017). In our version of the prob-
lem (Figure 7), the agent begins each episode in a
random position on a randomly-chosen map and
must attempt to obtain the treasure. Relational
concepts describing target locations are reused
between language learning and concept-learning
phases, but the environments themselves are dis-
tinct. For language learning the agent has access
to 250 tasks, and is evaluated on an additional 50.

Averaged learning curves for held-out tasks are
shown in Figure 8. As expected, reward for the L3

model remains low during the initial exploration
period, but once a description is chosen the score

4In RL, the contribution of L3 is orthogonal to that of
meta-learning—one could use a technique like RL2 (Duan
et al., 2016) to generate candidate descriptions more effi-
ciently, or MAML (Finn et al., 2017) rather than zero-shot
reward as the training criterion for the interpretation model.

improves rapidly. Immediately L3 achieves bet-
ter reward than the multitask baseline, though it
is not perfect; this suggests that the interpretation
model is somewhat overfit to the pretraining envi-
ronments. After fine-tuning even better results are
rapidly obtained. Example rollouts are visualized
in Appendix E. These results show that the model
has used the structure provided by language to
learn a better representation space for policies—
one that facilitates sampling from a distribution
over interesting and meaningful behaviors.

7 Other Related Work

This is the first approach we are aware of to frame
a general learning problem as optimization over
a space of natural language strings. However,
many closely related ideas have been explored in
the literature. String-valued latent variables are
widely used in language processing tasks rang-
ing from morphological analysis (Dreyer and Eis-
ner, 2009) to sentence compression (Miao and
Blunsom, 2016). Natural language annotations
have been used in conjunction with training ex-
amples to guide the discovery of logical descrip-
tions of concepts (Ling et al., 2017; Srivastava
et al., 2017), and used as an auxiliary loss for
training (Frome et al., 2013), analogously to the
Meta+Joint baseline in this paper. Structured
language-like annotations have been used to im-
prove learning of generalizable structured poli-
cies (Oh et al., 2017; Andreas et al., 2017; Denil
et al., 2017). Finally, natural language instruc-
tions available at concept-learning time (rather
than language-learning time) have been used to
provide side information to reinforcement learners
about high-level strategy (Branavan et al., 2011),
environments (Narasimhan et al., 2017) and explo-
ration (Harrison et al., 2017).

8 Conclusion

We have presented an approach for learning in a
space parameterized by natural language. Using
simple models for representation and search in this
space, we demonstrated that our approach outper-
forms standard baselines on classification, struc-
tured prediction and reinforcement learning tasks.
We believe that these results suggest the following
general conclusions:

Language encourages compositional general-
ization. Standard deep learning architectures are
good at recognizing new instances of familiar

2173



concepts, but not always at generalizing to new
ones. By forcing decisions to pass through a lin-
guistic bottleneck in which the underlying com-
positional structure of concepts is explicitly ex-
pressed, stronger generalization becomes possible.

Language simplifies structured exploration.
Natural language scaffolding provides dramatic
advantages in problems like reinforcement learn-
ing that require exploration: models with latent
linguistic parameterizations can limit exploration
to a class of behaviors that are likely a priori to be
goal-directed and interpretable.

And generally, language can help learning. In
multitask settings, it can even improve learning on
tasks for which no language data is available at
training or test time. While some of these advan-
tages are also provided by techniques built on top
of formal languages, natural language is at once
more expressive and easier to obtain than formal
supervision. We believe this work hints at broader
opportunities for using naturally-occurring lan-
guage data to improve machine learning for tasks
of all kinds.

Acknowledgments

JA is supported by a Facebook graduate fellow-
ship.

References
Jacob Andreas, Dan Klein, and Sergey Levine. 2017.

Modular multitask reinforcement learning with pol-
icy sketches. In Proceedings of the International
Conference on Machine Learning.

Mikhail Moiseevich Bongard. 1968. The recognition
problem. Technical report.

S.R.K. Branavan, Harr Chen, Luke S. Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.

S.R.K. Branavan, David Silver, and Regina Barzilay.
2011. Learning to win by reading manuals in a
Monte-Carlo framework. In Proceedings of the Hu-
man Language Technology Conference of the Asso-
ciation for Computational Linguistics.

Rich Caruana. 1998. Multitask learning. In Learning
to learn, Springer.

Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259 .

Ann A Copestake, Guy Emerson, Michael Wayne
Goodman, Matic Horvat, Alexander Kuhnle, and
Ewa Muszynska. 2016. Resources for building ap-
plications with dependency minimal recursion se-
mantics. In Language Resources and Computation.

Misha Denil, Sergio Gómez Colmenarejo, Serkan
Cabi, David Saxton, and Nando de Freitas.
2017. Programmable agents. arXiv preprint
arXiv:1706.06383 .

Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju,
Rishabh Singh, Abdel-rahman Mohamed, and Push-
meet Kohli. 2017. RobustFill: Neural program
learning under noisy I/O. In Proceedings of the In-
ternational Conference on Machine Learning.

Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadar-
rama, Marcus Rohrbach, Subhashini Venugopalan,
Kate Saenko, and Trevor Darrell. 2015. Long-term
recurrent convolutional networks for visual recogni-
tion and description. In Proceedings of the Confer-
ence on Computer Vision and Pattern Recognition.

Markus Dreyer and Jason Eisner. 2009. Graphical
models over multiple strings. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.

Yan Duan, John Schulman, Xi Chen, Peter L Bartlett,
Ilya Sutskever, and Pieter Abbeel. 2016. RL2:
Fast reinforcement learning via slow reinforcement
learning. arXiv preprint arXiv:1611.02779 .

Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.
Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of the International
Conference on Machine Learning.

Andrea Frome, Greg Corrado, Jonathon Shlens, Samy
Bengio, Jeffrey Dean, MarcAurelio Ranzato, and
Tomas Mikolov. 2013. Devise: A deep visual-
semantic embedding model. In Advances in Neural
Information Processing Systems.

Alison Gopnik and Andrew Meltzoff. 1987. The de-
velopment of categorization in the second year and
its relation to other cognitive and linguistic develop-
ments. Child Development .

Sumit Gulwani. 2011. Automating string processing
in spreadsheets using input-output examples. ACM
SIGPLAN Notices 46(1).

Brent Harrison, Upol Ehsan, and Mark O Riedl. 2017.
Guiding reinforcement learning exploration using
natural language. arXiv preprint arXiv:1707.08616
.

Michael Janner, Karthik Narasimhan, and Regina
Barzilay. 2017. Representation learning for
grounded spatial reasoning. Transactions of the As-
sociation for Computational Linguistics .

Robin Jia and Percy Liang. 2016. Data recombina-
tion for neural semantic parsing. arXiv preprint
arXiv:1606.03622 .

2174



Justin Johnson, Bharath Hariharan, Laurens van der
Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross
Girshick. 2017. CLEVR: A diagnostic dataset for
compositional language and elementary visual rea-
soning. Proceedings of the Conference on Computer
Vision and Pattern Recognition .

Diederik Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of the International Conference on Learning Repre-
sentations.

Emanuel Kitzelmann and Ute Schmid. 2006. Induc-
tive synthesis of functional programs: An explana-
tion based generalization approach. Journal of Ma-
chine Learning Research 7.

Alexander Kuhnle and Ann Copestake. 2017.
Shapeworld-a new test methodology for multi-
modal language understanding. arXiv preprint
arXiv:1704.04517 .

Nate Kushman and Regina Barzilay. 2013. Using se-
mantic unification to generate regular expressions
from natural language. In Proceedings of the An-
nual Meeting of the North American Chapter of the
Association for Computational Linguistics.

Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in ccg grammar induction for semantic pars-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.

Tessa Lau, Steven A Wolfman, Pedro Domingos, and
Daniel S Weld. 2003. Programming by demonstra-
tion using version space algebra. Machine Learning
53(1).

Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word
problems. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.

Yishu Miao and Phil Blunsom. 2016. Language as a
latent variable: Discrete generative models for sen-
tence compression. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing.

Karthik Narasimhan, Regina Barzilay, and Tommi
Jaakkola. 2017. Deep transfer in reinforcement
learning by language grounding. arXiv preprint
arXiv:1708.00133 .

Junhyuk Oh, Satinder Singh, Honglak Lee, and Push-
meet Kohli. 2017. Zero-shot task generalization
with multi-task deep reinforcement learning. In Pro-
ceedings of the International Conference on Ma-
chine Learning.

John C Raven. 1936. Mental tests used in genetic stud-
ies: The performance of related individuals on tests
mainly educative and mainly reproductive. Unpub-
lished masters thesis, University of London .

Stéphane Ross, Geoffrey J Gordon, and Drew Bagnell.
2011. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. In In-
ternational Conference on Artificial Intelligence and
Statistics.

Adam Santoro, Sergey Bartunov, Matthew Botvinick,
Daan Wierstra, and Timothy Lillicrap. 2016. Meta-
learning with memory-augmented neural networks.
In Proceedings of the International Conference on
Machine Learning.

Jurgen Schmidhuber. 1987. Evolutionary principles in
self-referential learning. On learning how to learn.
Diploma thesis, Institut f. Informatik, Tech. Univ.
Munich .

K Simonyan and A Zisserman. 2014. Very deep convo-
lutional networks for large-scale image recognition.
arXiv preprint arxiv:1409.1556 .

Rishabh Singh and Sumit Gulwani. 2012. Learning se-
mantic string transformations from examples. In In-
ternational Conference on Very Large Databases.

Jake Snell, Kevin Swersky, and Richard S Zemel. 2017.
Prototypical networks for few-shot learning. arXiv
preprint arXiv:1703.05175 .

Richard Socher, Andrej Karpathy, Quoc V Le, Christo-
pher D Manning, and Andrew Y Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions of
the Association for Computational Linguistics 2.

Shashank Srivastava, Igor Labutov, and Tom Mitchell.
2017. Joint concept learning and semantic parsing
from natural language explanations. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.

Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi.
2017. A corpus of natural language for visual rea-
soning. In 55th Annual Meeting of the Association
for Computational Linguistics, ACL.

Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray
Kavukcuoglu, and Daan Wierstra. 2016. Matching
networks for one shot learning. In Advances in Neu-
ral Information Processing Systems.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning 8(3-4).

2175



A Model and Training Details

In all models, RNN encoders and decoders use
gated recurrent units (Cho et al., 2014).

Few-shot classification Models are trained with
the ADAM optimizer (Kingma and Ba, 2015) with
a step size of 0.0001 and batch size of 100. The
number of pretraining iterations is tuned based on
subsequent concept-learning performance on the
development set. Neural network hidden states,
task parameters, and word vectors are all of size
512. 10 hypotheses are sampled during for each
evaluation task in the concept-learning phase.

Programming by demonstration Training as
in the classification task, but with a step size of
0.001. Hidden states are of size 512, task param-
eters of size 128 and word vectors of size 32. 100
hypotheses are sampled for concept learning.

Policy search DAgger (Ross et al., 2011) is
used for pre-training and vanilla policy gradient
(Williams, 1992) for concept learning. Both learn-
ing algorithms use ADAM with a step size of 0.001
and a batch size of 5000 samples. For imitation
learning, rollouts are obtained from the expert pol-
icy on a schedule with probability 0.95t (for t the
current epoch). For reinforcement learning, a dis-
count of 0.9 is used. Because this dataset con-
tains no development data, pretraining is run un-
til performance on the pretraining tasks reaches a
plateau. Hidden states and task embeddings are
of size 64. 100 hypotheses are sampled for con-
cept learning, and 1000 episodes (divided evenly
among samples) are used to estimate hypothesis
quality before fine-tuning.

B Dataset Information

ShapeWorld This is the only fully-synthetic
dataset used in our experiments. Each scene fea-
tures 4 or 5 non-overlapping entities. Descriptions
refer to spatial relationships between pairs of enti-
ties identified by shape, color, or both. There are
8 colors and 8 shapes. The total vocabulary size is
only 30 words, but the dataset contains 2643 dis-
tinct captions. Descriptions are on average 12.0
words long.

Regular expressions Annotations were col-
lected from Mechanical Turk users. Each user
was presented with the same task as the learner in
this paper: they observed five strings being trans-
formed, and had to predict how to transform a

sixth. Only after they correctly generated the held-
out word were they asked for a description of the
rule. Workers were additionally presented with
hints like “look at the beginning of the word” or
“look at the vowels”. Descriptions are automati-
cally preprocessed to strip punctuation and ensure
that every character literal appears as a single to-
ken.

The regular expression data has a vocabulary of
1015 rules and a total of 1986 distinct descriptions.
Descriptions are on average 12.3 words in length
but as long as 46 words in some cases.

Navigation The data used was obtained from
Janner et al. (2017). We created our own variant of
the dataset containing collections of related tasks.
Beginning with the “local” tasks in the dataset, we
generated alternative goal positions at fixed offsets
from landmarks as described in the main section
of this paper. Natural-language descriptions were
selected for each task collection from the human
annotations provided with the dataset. The vocab-
ulary size is 74 and the number of distinct hints
446. The original action space for the environment
is also modified slightly: rather than simply reach-
ing the goal cell (achieved with reasonably high
frequency by a policy that takes random moves),
we require the agent to commit to an individual
goal cell and end the episode with a special DIG
action.

Data augmentation Due to their comparatively
small size, a data augmentation scheme (Jia and
Liang, 2016) is employed for the regular expres-
sion and navigation datasets. In particular, wher-
ever a description contains a recognizable entity
name (i.e. a character literal or a landmark name),
a description template is extracted. These tem-
plates are then randomly swapped in at training
time on other examples with the same high-level
semantics. For example, the description replace
first b with e is abstracted to replace first CHAR1
with CHAR2, and can subsequently be specialized
to, e.g., replace first c with d. This templating
is easy to implement because we have access to
ground-truth structured concept representations at
training time. If these were not available it would
be straightforward to employ an automatic tem-
plate induction system (Kwiatkowski et al., 2011)
instead.

2176



C Examples: ShapeWorld

(Examples in this and the following appendices were not cherry-picked.)

Positive examples:

True description:
a red ellipse is to the right of an ellipse

Inferred description:
a red shape is to the right of a red semicircle

Input: True label:
true

Pred. label:
true

a shape is below a white ellipse

a white shape is to the left of a yellow ellipse

false

true

a magenta triangle is to the left of a magenta pentagon

a magenta triangle is to the left of a pentagon

true

true

a green pentagon is to the right of a yellow shape

a green shape is to the right of a red semicircle

false

false

a red circle is above a magenta semicircle

a green triangle is above a red circle

false

true

a white ellipse is to the left of a green cross

a green cross is to the right of a white ellipse

true

true

2177



D Examples: Regular Expressions

Example in:
mediaeval
paneling
wafer
conventions
handsprings

Example out:
ilediaeval
ilaneling
ilafer
ilonventions
ilandsprings

Human description:
leading consonant si replaced with i l

Inferred description:
first consonant of a word is replaced with i l

Input:
chaser

True out:
ilhaser

Pred. out:
ilhaser

uptakes
pouching
embroidery
rebelliousness
stoplight

uptakes
punuching
embrunidery
rebelliunusness
stunplight

replace every o with u n

change all o to u n

regulation

regulatiunn

regulatinun

fluffiest
kidnappers
matting
griping
disagreements

fluffiest
kidnappers
eeatting
griping
disagreeeeents

the leter m is replaced by ee

change every m to ee

chartering

chartering

chartering

clandestine
limning
homes
lifeblood
inflates

clandqtine
limning
homq
lifqlood
inflatq

e

where e appears , replace it
and the following letter with q

gratuity

gratuity

gratuity

fruitlessly
sandier
washers
revelries
dewlaps

fruitlessly
sandier
washemu
revelrimu
dewlamu

if the word ends with an s , replace
the last two letters with m u

change last to m u if consonant

prompters

promptemu

promptemu

ladylike
flintlocks
student
surtaxes
bedecks

ladylike
flintlocknl
studennl
surtaxenl
bedecknl

ending consonant is replaced with n l

drop last two and add n l

initials

initialnl

initialnl

porringer
puddling
synagog
curtseying
monsieur

porringeer
puddlinge
synageoge
curtseyinge
monsieur

add e next to letter g

when a letter is preceded by a g ,
e is added after that letter

rag

rage

rage

trivializes
tried
tearfully
hospitalize
patronizing

trivializes
tried
gxarfully
gxspitalize
gxtronizing

replace the 1st 2 letters of the word with a g x
if the word begins with a consonant then a vowel

if the second letter is a vowel , replace the
first two letters with g x

landlords

gxndlords

gxndlords

microseconds
antiviral
flintlock
appreciable
stricter

microsecnyr
antiviral
flintloyr
appreciabyr
stricter

replace consonants with y r

the last two letters are replaced by y r

exertion

exertion

exertiyr

2178



E Examples: Navigation

White breadcrumbs show the path taken by the agent.

Human description:
move to the star

Inferred description:
reach the star cell

reach square one right of triangle

reach cell to the right of the triangle

reach cell on left of triangle

reach square left of triangle

reach spade

go to the spade

left of the circle

go to the cell to the left of the circle

reach cell below the circle

reach cell below circle

2179


