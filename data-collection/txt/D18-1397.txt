



















































A Study of Reinforcement Learning for Neural Machine Translation


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3612–3621
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

3612

A Study of Reinforcement Learning for Neural Machine Translation

Lijun Wu1∗, Fei Tian2, Tao Qin2, Jianhuang Lai1 and Tie-Yan Liu2
1School of Data and Computer Science, Sun Yat-sen University

2Microsoft Research
wulijun3@mail2.sysu.edu.cn; stsljh@mail.sysu.edu.cn;

{fetia,taoqin,tyliu}@microsoft.com

Abstract

Recent studies have shown that reinforcemen-
t learning (RL) is an effective approach for
improving the performance of neural machine
translation (NMT) system. However, due
to its instability, successfully RL training is
challenging, especially in real-world systems
where deep models and large datasets are lever-
aged. In this paper, taking several large-scale
translation tasks as testbeds, we conduct a sys-
tematic study on how to train better NMT mod-
els using reinforcement learning. We provide
a comprehensive comparison of several impor-
tant factors (e.g., baseline reward, reward shap-
ing) in RL training. Furthermore, to fill in the
gap that it remains unclear whether RL is still
beneficial when monolingual data is used, we
propose a new method to leverage RL to fur-
ther boost the performance of NMT system-
s trained with source/target monolingual da-
ta. By integrating all our findings, we obtain
competitive results on WMT14 English- Ger-
man, WMT17 English-Chinese, and WMT17
Chinese-English translation tasks, especial-
ly setting a state-of-the-art performance on
WMT17 Chinese-English translation task.

1 Introduction

Recently, neural machine translation (NMT) (Bah-
danau et al., 2015; Hassan et al., 2018; Wu et al.,
2016; He et al., 2017; Xia et al., 2016, 2017; Wu
et al., 2018b,a) has become more and more popular
given its superior performance without the demand
of heavily hand-crafted engineering efforts. It is
usually trained to maximize the likelihood of each
token in the target sentence, by taking the source
sentence and the preceding (ground-truth) target
tokens as inputs. Such training approach is referred
as maximum likelihood estimation (MLE) (Scholz,
1985). Although easy to implement, the token-level

∗This work was conducted at Microsoft Research Asia.

objective function during training is inconsisten-
t with sequence-level evaluation metrics such as
BLEU (Papineni et al., 2002).

To address the inconsistency issue, reinforce-
ment learning (RL) methods have been adopted to
optimize sequence-level objectives. For example,
policy optimization methods such as REINFORCE
(Ranzato et al., 2016; Wu et al., 2017b) and actor-
critic (Bahdanau et al., 2017) are leveraged for
sequence generation tasks including NMT. In ma-
chine translation community, a similar method is
proposed with the name ‘minimum risk training’
(Shen et al., 2016). All these works demonstrate
the effectiveness of RL techniques for NMT mod-
els (Wu et al., 2016).

However, effectively applying RL to real-world
NMT systems has not been fulfilled by previous
works. First, most of, if not all, previous works
verified their methods based on shallow recurrent
neural network (RNN) models. However, to obtain
state-of-the-art (SOTA) performance, it is essential
to leverage recently derived deep models (Gehring
et al., 2017; Vaswani et al., 2017), which are much
more powerful.

Second, it is not easy to make RL practically ef-
fective given quite a few widely acknowledged lim-
itations of RL method (Henderson et al., 2018) such
as high variance of gradient estimation (Weaver and
Tao, 2001), and objective instability (Mnih et al.,
2013). Therefore, several tricks are proposed in
previous works. However, it remains unclear, and
no agreement is achieved on how to use these tricks
in machine translation. For example, baseline re-
ward method (Weaver and Tao, 2001) is suggested
in (Ranzato et al., 2016; Nguyen et al., 2017; Wu
et al., 2016) but not leveraged in (He and Deng,
2012; Shen et al., 2016).

Third, large-scale datasets, especially monolin-
gual datasets are shown to significantly improve
translation quality (Sennrich et al., 2015a; Xia et al.,



3613

2016) with MLE training, while it remains nearly
empty on how to combine RL with monolingual
data in NMT.

In this paper, we try to fulfill these gaps and s-
tudy how to practically apply RL to obtain strong
NMT systems with quite competitive, even state-
of-the-art performance. Several comprehensive s-
tudies are conducted on different aspects of RL
training to figure out how to: 1) set efficient re-
wards; 2) combine MLE and RL objectives with
different weights, which aims to stabilize the train-
ing procedure; 3) reduce the variance of gradient
estimation.

In addition, given the effectiveness of leveraging
monolingual data in improving translation quali-
ty, we further propose a new method to combine
the strength of both RL training and source/target
monolingual data. To the best of our knowledge,
this is the first work that tries to explore the power
of monolingual data when training NMT model
with RL method.

We obtain some useful findings through the ex-
periments on WMT17 Chinese-English (Zh-En),
WMT17 English-Chinese (En-Zh) and WMT14
English-German (En-De) translation tasks. For in-
stance, multinomial sampling is better than beam
search in reward computation, and the combination
of RL and monolingual data significantly enhances
the NMT model performance. Our main contribu-
tions are summarized as follows.

• We provide the first comprehensive study on
different aspects of RL training, such as how
to setup reward and baseline reward, on top of
quite competitive NMT models.

• We propose a new method that effectively
leverages large-scale monolingual data, from
both the source and target side, when training
NMT models with RL.

• Combined with several of our findings and
method, we obtain the SOTA translation quali-
ty on WMT17 Zh-En translation task, surpass-
ing strong baseline (Transformer big model +
back translation) by nearly 1.5 BLEU points.
Furthermore, on WMT14 En-De and WMT17
En-Zh translation tasks, we can also obtain
strong competitive results.

We hope that our studies and findings will ben-
efit the community to better understand and lever-
age reinforcement learning for developing strong

NMT models, especially in real-world scenarios
faced with deep models and large amount of train-
ing data (including both parallel and monolin-
gual data). Towards this end, we open source all
our codes/dataset at https://github.com/
apeterswu/RL4NMT to provide a clear recipe
for performance reproduction.

2 Background

In this section, we first introduce the attention-
based sequence-to-sequence learning framework
for neural machine translation (NMT), and then
introduce the basis of applying reinforcement learn-
ing to training NMT models.

2.1 Neural Machine Translation
Typical NMT models are based on the encoder-
decoder framework with attention mechanism.
The encoder first maps a source sentence x =
(x1, x2, ..., xn) to a set of continuous represen-
tations z = (z1, z2, ..., zn). Given z, the de-
coder then generates a target sentence y =
(y1, y2, ..., ym) of word tokens one by one. At each
decoding step t of model training, the probability
of generating a token yt is maximized conditioned
on x and y<t = (y1, ..., yt−1). Given N training
sentence pairs {xi, yi}Ni=1, maximum likelihood es-
timation (MLE) is usually adopted to optimize the
model, and the training objective is defined as:

Lmle =
N∑
i=1

log p(yi|xi)

=

N∑
i=1

m∑
t=1

log p(yit|yi1, ..., yit−1, xi),

(1)

where m is the length of sentence yi.
Among all the encoder-decoder models, the re-

cently proposed Transformer (Vaswani et al., 2017)
architecture achieves the best translation quality so
far. The main difference between Transformer and
previous RNNSearch (Bahdanau et al., 2015) or
ConvS2S (Gehring et al., 2017) is that Transformer
relies entirely on self-attention (Lin et al., 2017) to
compute representations of source and target side
sentences, without using recurrent or convolutional
operations.

2.2 Training NMT with Reinforcement
Learning

As aforementioned, reinforcement learning (RL) is
leveraged to bridge the gap between training and

https://github.com/apeterswu/RL4NMT
https://github.com/apeterswu/RL4NMT


3614

inference of NMT, by directly optimizing the e-
valuation measure (e.g., BLEU) at training time.
Specifically, NMT model can be viewed as an a-
gent, which interacts with the environment (the pre-
vious words y<t and the context vector z available
at each step t). The parameters of the agent define
a policy, i.e., a conditional probability p(yt|x, y<t).
The agent will pick an action , i.e., a candidate
word out from the vocabulary, according to the pol-
icy. A terminal reward is observed once the agent
generates a complete sequence ŷ. The reward for
machine translation is the BLEU (Papineni et al.,
2002) score, denoted as R(ŷ, y), which is defined
by comparing the generated ŷ with the ground-truth
sentence y. Note that here the reward R(ŷ, y) is
the sentence-level reward, i.e., a scalar for each
complete sentence ŷ. The goal of the RL training
is to maximize the expected reward:

Lrl =

N∑
i=1

Eŷ∼p(ŷ|xi)R(ŷ, y
i)

=

N∑
i=1

∑
ŷ∈Y

p(ŷ|xi)R(ŷ, yi),
(2)

where Y is the space of all candidate transla-
tion sentences, which is exponentially large due
to the large vocabulary size, making it impossi-
ble to exactly maximize Lrl. In practice, REIN-
FORCE (Williams, 1992) is usually leveraged to
approximate the above expectation via sampling ŷ
from the policy p(y|x), leading to the objective as
maximizing:

L̂rl =
N∑
i=1

R(ŷi, yi), ŷi ∼ p(y|xi),∀i ∈ [N ]. (3)

Throughout the paper we will use REINFORCE
as our policy optimization method for RL training.

3 Strategies for RL Training

Although training NMT with RL can fill in the gap
between training objectives and evaluation metrics,
it is not easy to successfully put RL training into
practice. A key challenge is that RL methods are
highly unstable and inefficient, due to the noise in
gradient estimation and reward computation. To
our best knowledge, currently there is no consen-
sus, or even a systematic study on how to configure
different setups for RL training to avoid such prob-
lems, especially for training deep NMT models on
large scale datasets. We therefore aim to shed light

on practical applications of RL for NMT training.
For this purpose, we provide a comprehensive re-
view of several important methods to stabilize RL
training process in this section.

3.1 Reward Computation
It is critical to set up appropriate rewards for RL
training, i.e., the R(ŷ, y) in Eqn. (3). There are
two important aspects to consider in configuring the
reward R(ŷ, y): how to sample training instance ŷ
and whether to use reward shaping.

Generate ŷ There are two strategies to sample ŷ
for computing the BLEU reward R(ŷ, y). The first
one is beam search (Sutskever et al., 2014), it is a
breadth-first search method that maintains a “beam”
of the top-K scoring candidates (prefix hypothe-
sis sentences) at each generation step. Then, for
each candidate sentence in the beam,K most likely
words are appended, resulting in a pool of K ×K
new candidates. Out from this pool, the top-K
translations with largest probabilities are selected,
and the beam search process continues. The second
strategy is multinomial sampling (Chatterjee and
Cancedda, 2010), which produces each word one
by one through multinomial sampling over the mod-
el’s output distribution. Both sampling strategies
terminate the expansion of a candidate sentence
when an ‘end of sentence’ (<EOS>) token is met.

The choice of different sampling strategies re-
flects the exploration-exploitation dilemma. Beam
search strategy generates more accurate ŷ by ex-
ploiting the probabilistic space output via curren-
t NMT model, while multinomial sampling pays
more attention to explore more diverse candidates.

Whether to Use Reward Shaping From Eqn.
(3) we can see that for the entire sequence ŷ, there
is only one terminal reward R(ŷ, y) available for
model training. Note that the agent needs to take
tens of actions (with the number depending on the
length of ŷ) to generate a complete sentence ŷ, but
only one reward is available for all those actions.
Consequently, RL training is inefficient due to the
sparsity of rewards, and the model updates each
token in the training sentence with the same reward
value without distinction. Reward shaping (Ng
et al., 1999) is a strategy to overcome this shortcom-
ing. In reward shaping, intermediate reward at each
decoding step t is imposed and denoted as rt(ŷt, y).
Bahdanau et al. (2017) sets up the intermediate re-
ward as rt(ŷt, y) = R(ŷ1...t, y) − R(ŷ1...t−1, y),
where R(ŷ1...t, y) is defined as the BLEU score



3615

of ŷ1...t with respect to y. Note that we have
R(ŷ, y) =

∑m
t=1 rt(ŷt, y), where m is the length

of ŷ. During RL training, the cumulative reward∑m
τ=t rτ (ŷτ , y) is used to update the policy at time

step t. It is verified that using the shaped reward rt
instead of awarding the whole score R(ŷ, y) does
not change the optimal policy (Ng et al., 1999).

3.2 Variance Reduction of Gradient
Estimation

As mentioned before, the REINFORCE algorithm
suffers from high variance in gradient estimation,
mainly caused by using single sample ŷ to estimate
the expectation. To reduce the variance, Ranzato
et al. (2016) subtracts an average reward from the
returned reward at each time step t, and the actual
reward used to update the policy is

R(ŷ, y)− r̂t, (4)

where r̂t is the estimated average reward at step t,
named as baseline reward (Weaver and Tao, 2001).
Together with reward shaping, the updated reward
becomes

∑m
τ=t rτ (ŷτ , y)− r̂t at step t.

Intuitively speaking, a baseline reward r̂t is es-
tablished, which either encourages a word choice
ŷt if the induced reward R satisfies R > r̂t, or
discourages it if R < r̂t. Here R is either the
terminal reward R(ŷ, y) or the cumulative reward∑m

τ=t rτ (ŷτ , y). Such estimated baseline reward
r̂t is designed to decrease the high variance of the
gradient estimator.

In practice, the baseline reward r̂t can be ob-
tained through different approaches. For exam-
ple, one may sample multiple sentences and use
the mean terminal reward for these sentences as
baseline reward. In our work, we adopt the func-
tion learning approach, using simple network (e.g.,
multi-layer perceptron) to build the learning func-
tion, which is the same as used in (Ranzato et al.,
2016; Bahdanau et al., 2017).

3.3 Combine MLE and RL Objectives

The last important strategy we would like to men-
tion is the combination of MLE training objective
with RL objective, which is assumed to further sta-
bilize RL training process (Wu et al., 2016; Li et al.,
2017; Wu et al., 2017a).

A simple way is to linearly combine the MLE
(Eqn. (1)) and RL (Eqn. (3)) objectives as follows:

Lcom = α ∗ Lmle + (1− α) ∗ L̂rl, (5)

where α is the hyperparamter controlling the trade-
off between MLE and RL objectives. We will em-
pirically evaluate how different values of α impact
the final translation accuracy.

4 RL Training with Monolingual Data

Previous works typically conduct RL training with
only bilingual data for NMT. Monolingual data has
been proved to be able to significantly improve
the performance of NMT systems (Sennrich et al.,
2015a; Xia et al., 2016; Cheng et al., 2016). It
remains an open problem whether it is possible to
combine the benefits of RL training and monolin-
gual data such that even more competitive results
can be obtained. In this section we provide several
solutions for combination and will study them in
next section. Note that all the settings discussed in
this section are semi-supervised learning, i.e., both
bilingual and monolingual data are available.

4.1 With Source-Side Monolingual Data

We first provide a solution to RL training with
source-side monolingual data. As shown in Eqn.
(3), in RL training we need to calculate the re-
ward signal R(ŷ, y) for each generated sentence
ŷ, and therefore the reference sentence y seems to
be a must-have, which unfortunately is missing for
source-side monolingual data.

We tackle this challenge via generating pseu-
do target reference y by bootstrapping with the
model itself. Apparently, for the source-side mono-
lingual data, the pseudo target reference y should
have good translation quality. Therefore, for each
source-side monolingual sentence, we use the N-
MT model trained from the bilingual data to beam
search a target sentence and treat it as the pseu-
do target reference y. Afterwards ŷ is obtained
via multinomial sampling to calculate the reward.
Although multinomial sampling is usually not as
good as sampling via beam search, the combination
of beam search (to get the pseudo target reference
sentence) and the multinomial sampling (to gener-
ate the action sequence of the agent) achieves good
exploration-exploitation trade-off, since the pseudo
target reference exploits the accuracy of current
NMT model while ŷ achieves better exploration.

4.2 With Target-Side Monolingual Data

For a target-side monolingual sentence, its source
sentence x is missing, and consequently ŷ is un-
available since it is sampled based on x. We tackle



3616

this challenge via back translation (Sennrich et al.,
2015a). We first train a reverse NMT model from
the target language to the source language with
bilingual data. For each target-side monolingual
sentence, using the reverse NMT model, we back
translate it to get its pseudo source sentence x. We
then pair the target monolingual data and its back-
translated sentence as a pseudo bilingual sentence
pair, which can be used for RL training in the same
way as the genuine bilingual sentence pairs.

4.3 With both Source-Side and Target-Side
Monolingual Data

A natural extension of previous discussions is to
combine both the source-side and target-side mono-
lingual data for RL training. We consider two com-
binations, the sequential method and the unified
method. The former one sequentially leverages the
source-side and target-side monolingual data for
RL training. Specifically, we first train an MLE
model using the bilingual data and source-side (or
target-side) monolingual data; based on this MLE
model, we then use REINFORCE for training with
target-side (or source-side) monolingual data. For
unified approach, we pack the paired data out from
three domains together: the genuine bilingual data,
the source monolingual data with its pseudo target
references (introduced in subsection 4.1), and the
target monolingual data with its back-translated
samples (introduced in subsection 4.2). Then we
treat the combined data as normal bilingual data on
which the NMT model is trained via MLE or RL
principles. Our goal is to investigate the model per-
formance with different training data and find the
best recipe of how to use these data in RL training.
More details are introduced in next section.

5 Experiments

In this section, we provide a systematic study on
aforementioned RL training strategies and the solu-
tions of leveraging monolingual data. The RL train-
ing strategies are evaluated on bilingual dataset-
s from three translation tasks, WMT14 English-
German (En-De), WMT17 English-Chinese (En-
Zh) and WMT17 Chinese-English (Zh-En), and we
further conduct the experiments to leverage mono-
lingual data in WMT17 Zh-En translation.

5.1 Experimental Settings

For the bilingual datasets, WMT17 (Bojar et al.,
2017) En-Zh 1 and WMT17 Zh-En use the same
dataset, which contains about 24M sentences pairs,
including CWMT Corpus 2017 and UN Parallel
Corpus V1.0. The Jieba2 segmenter is used to per-
form Chinese word segmentation. We use byte
pair encoding (BPE) (Sennrich et al., 2015b) to pre-
process the source and target sentences, forming
source-side and target-side dictionary with 40, 000
and 37, 000 types, respectively. We use the news-
dev2017 as the dev set and newstest2017 as the
test set. For the WMT14 En-De dataset, it con-
tains about 4.5M training pairs, newstest2012 and
newstest2013 are concatenated as the dev set and
newstest2014 acts as test set. Same as (Vaswani
et al., 2017), we also perform BPE to process the
En-De dataset, the shared source-target vocabulary
contains about 37, 000 tokens.

For the monolingual dataset on Zh-En translation
task, similar to (Sennrich et al., 2017), the Chinese
monolingual data comes from LDC Chinese Gi-
gaword (4th edition) and the English monolingual
data comes from News Crawl 2016 articles. After
preprocessing (e.g., language detection and filter-
ing sentences with more than 80 words), we keep
4M Chinese sentences and 7M English sentences.

We adopt the Transformer model with trans-
former big setting as defined in (Vaswani et al.,
2017) for Zh-En and En-Zh translations, which
achieves SOTA translation quality in several oth-
er datasets. For En-De translation, we utilize the
transformer base v1 setting. These settings are ex-
actly same as used in the original paper, except
we set the layer prepostprocess dropout for Zh-En
and En-Zh translation to be 0.05. The optimizer
used for MLE training is Adam (Kingma and Ba,
2015) with initial learning rate is 0.1, and we fol-
low the same learning rate schedule in (Vaswani
et al., 2017). During training, roughly 4, 096 source
tokens and 4, 096 target tokens are paired in one
mini batch. Each model is trained using 8 NVIDI-
A Tesla M40 GPUs. For RL training, the model
is initialized with parameters of the MLE model
(trained with only bilingual data), and we continue
training it with learning rate 0.0001. Same as (Bah-
danau et al., 2017), to calculate the BLEU reward,
we start all n-gram counts from 1 instead of 0 and

1http://www.statmt.org/wmt17/
translation-task.html

2https://github.com/fxsjy/jieba

http://www.statmt.org/wmt17/translation-task.html
http://www.statmt.org/wmt17/translation-task.html
https://github.com/fxsjy/jieba


3617

Training Strategy En-De En-Zh Zh-En
MLE 27.02 34.12 24.29

RL (beam + terminal) 27.06 34.25 24.42
RL (multinomial + terminal) 27.22 34.46 24.70

RL (beam + shaping) 27.04 34.28 24.47
RL (multinomial + shaping) 27.23 34.47 24.72

Table 1: Results of different strategies for reward com-
putation. ‘beam’ refers to ‘beam search and ‘multino-
mial’ to ‘multinomial sampling’. While generating ŷ
through beam search, we use width 4. ‘shaping’ refers
to using reward shaping and ‘terminal’ refers not.

multiply the resulting score by the length of the
target reference sentence. For inference, we use
beam search with width 6. We run each setting
for at least 5 times and report the averaged case
sensitive BLEU scores3 (Papineni et al., 2002) on
test set. The test set BLEU is chosen via the best
configuration based on the validation set.

5.2 Results of of RL Training Strategies

We first evaluate different strategies for RL training,
based only on bilingual datasets from previously
introduced three translation tasks.

Reward Computation As reviewed in subsec-
tion 3.1, for reward computation, we need to con-
sider how to sample ŷ and whether to use reward
shaping.

The results are shown in Table 1, where “RL”
stands for RL training with the REINFORCE algo-
rithm. We also report the performance of the pre-
trained NMT model with the MLE loss. From the
table, an interesting finding is that ŷ sampled via
beam search strategy is worse than that by multi-
nomial sampling, with a gap of roughly 0.2-0.3
BLEU points on the test set (with significant test
score ρ < 0.05). We therefore conjecture that ex-
ploration is more important than exploitation in
reward computing: multinomial sampling brings
more data diversity to the training of NMT mod-
el, while sentences generated by beam search are
usually very similar to each other. Furthermore,
we find that there is no big difference between the
leverage of reward shaping or terminal reward, with
only slightly better performance of reward shaping.
We therefore use multinomial sampling and reward
shaping in later experiments.

3Calculated by SacréBLEU toolkit, which produces exact-
ly the same evaluation result as that in WMT17 Zh-En cam-
paign. https://github.com/awslabs/sockeye/
tree/master/contrib/sacrebleu

Training Strategy En-De En-Zh Zh-En
RL 27.23 34.47 24.72

RL (baseline function) 27.25 34.43 24.73

Table 2: Results of variance reduction of gradient esti-
mation.

 = 0  = 0.1  = 0.3  = 0.5  = 0.7  = 0.9
24

26

28

30

32

34

36

BL
EU

 S
co

re

27.23 27.28 27.48 27.37 27.25 27.20

34.47 34.50 34.63 34.56 34.44 34.40

24.72 24.89 25.04 24.87 24.71 24.65

En-De
En-Zh
Zh-En

Figure 1: Results of different weights α to combine
MLE and RL objectives.

Variance Reduction of Gradient Estimation
Next we evaluate the strategies for reducing vari-
ance of gradient estimation (see section3.2). We
want to know whether the baseline reward is nec-
essary. To compute the baseline reward, similar to
(Ranzato et al., 2016; Bahdanau et al., 2017), we
build a two-layer MLP regressor with Relu (Nair
and Hinton, 2010) activation units. The function
takes the hidden states from decoder as input, and
the parameters of the regressor are trained to mini-
mize the mean squared loss of Eqn. (4). We first
pre-train the baseline function for 20k steps/mini-
batches, and then jointly train NMT model (with
RL) and the baseline reward function.

Table 2 shows that the learning of baseline re-
ward does not help RL training. This contradicts
with previous observations (Ranzato et al., 2016),
and seems to suggest that the variance of gradient
estimation in NMT is not as large as we expected.
The reason might be that the probability mass on
the target-side language space induced by the NMT
model is highly concentrated, making the sampled
ŷ representative enough in terms of estimating the
expectation. Therefore, for the economic perspec-
tive, it is not necessary to add the additional steps
of using baseline reward on RL training for NMT.

Combine MLE and RL Objectives As shown
in Eqn. (5), the hyperparameter α controls the
trade-off between MLE and RL objectives. For
comparison, we set α to be [0, 0.1, 0.3, 0.5, 0.7,
0.9] in our experiments. The results are presented
in Figure 1.

https://github.com/awslabs/sockeye/tree/master/contrib/sacrebleu
https://github.com/awslabs/sockeye/tree/master/contrib/sacrebleu


3618

[Data] (Objective) Valid Test
[B] (MLE) 22.32 24.29

[B] (MLE) + [B] (RL) 22.87 25.04
[B] (MLE) + [Ms] (RL) 23.03 25.22

[B & Ms] (MLE) 24.31 25.31
[B & Ms] (MLE) + [B & Ms] (RL) 24.58 25.60

Table 3: Results with source monolingual data. “B” de-
notes bilingual data, “Ms” denotes source-side mono-
lingual data, “&” denotes data combination.

The results show that combining the MLE ob-
jective with the RL objective achieves better per-
formance (27.48 for En-De, 34.63 for En-Zh and
25.04 for Zh-En with α = 0.3). This indicates that
MLE objective is helpful to stabilize the training
and improve the model performance, as we expect-
ed. However, further increasing α does not bring
more gain. The best trade-off between MLE and
RL objectives in our experiment is α = 0.3. There-
fore, we set α = 0.3 in the following experiments.

5.3 Results of RL Training with Monolingual
Data

In this subsection, we report the results on both
valid and test set of RL training using bilingual
and monolingual data in Zh-En translation. From
Table 3 to Table 6, “RL” denotes the model trained
with RL using multinomial sampling, reward shap-
ing, no baseline reward, and combined objective,
based on the observations in the last subsection.
“B” denotes bilingual data, “Ms” denotes source-
side monolingual data and “Mt” denotes target-side
monolingual data, “&” denotes data combination.

With Source-Side Monolingual Data As dis-
cussed before, we use beam search with beam
width 4 to sample the pseudo target sentence y
for each monolingual sentence x. We consider sev-
eral settings for RL training: 1) only source-side
monolingual data; 2) the combination of bilingual
and source-side monolingual data. We first train
an MLE model using the augmented dataset com-
bining the genuine bilingual data with the pseudo
bilingual data generated from the monolingual da-
ta, and then perform RL training on this combined
dataset. The results are shown in Table 3.

With Target-Side Monolingual Data For
target-side monolingual data, we first pre-train a
translation model from English to Chinese 4, and
use it to back translate target-side monolingual

4The BLEU score of the En-Zh model is 34.12.

[Data] (Objective) Valid Test
[B] (MLE) 22.32 24.29

[B] (MLE) + [B] (RL) 22.87 25.04
[B] (MLE) + [Mt] (RL) 22.96 25.15

[B & Mt] (MLE) 24.14 25.24
[B & Mt] (MLE) + [B & Mt] (RL) 24.41 25.58

Table 4: Results with target monolingual data. “B” de-
notes bilingual data, “Mt” denotes target-side monolin-
gual data, “&” denotes data combination.

[Data] (Objective) Valid Test
[B & Ms] (MLE) 24.31 25.31

[B & Ms] (MLE) + [B & Ms] (RL) 24.58 25.60
[B & Ms] (MLE) + [Mt] (RL) 24.61 25.72

[B & Mt] (MLE) 24.14 25.24
[B & Mt] (MLE) + [B & Mt] (RL) 24.41 25.58

[B & Mt] (MLE) + [Ms] (RL) 24.75 25.92

Table 5: Results of sequential approach for monolin-
gual data. “B” denotes bilingual data, “Ms” denotes
source-side monolingual data and “Mt” denotes target-
side monolingual data, “&” denotes data combination.

[Data] (Objective) Valid Test
[B & Ms & Mt] (MLE) 25.58 26.13
+ [B & Ms & Mt] (RL) 25.90 26.73

Table 6: Results of unified approach for monolingual
data. “+” means to initialize the RL model using above
MLE model, which is trained on the combination of
bilingual data, source-side monolingual data and target-
side monolingual data.

sentence y to get pseudo source sentence x.
Similarly, we consider several settings for RL
training: 1) only target-side monolingual data; 2)
the combination of bilingual data and target-side
monolingual data. We train an MLE model
using both the genuine and the generated pseudo
bilingual data, and then perform RL training on
this data. The results are presented in Table 4.

From Table 3 and 4, we have several observa-
tions. First, monolingual data helps RL training,
improving BLEU score from 25.04 to 25.22 (ρ <
0.05) in Table 3. Second, when we only add mono-
lingual data for RL training, the model achieves
similar performance compared to MLE training
with bilingual and monolingual data (e.g., 25.15 vs.
25.24 (ρ < 0.05) in Table 4).

With both Source-Side and Target-Side Mono-
lingual Data We have two approaches to use
both source-side and target-side monolingual da-
ta, as described in subsection 4.3. The results are
reported in Table 5 and Table 6.

From Table 5, we can observe that the sequen-



3619

System Architecture BLEU
Existing end-to-end NMT systems

Vaswani et al. (2017) Transformer 24.29
Sennrich et al. (2015a) Transformer + Target Monolingual Data (i.e., back translation) 25.24
SougouKnowing Stacked LSTM model + Reranking 24.00
SougouKnowing-ensemble Stacked LSTM model + Reranking + Ensemble 26.40

Our end-to-end NMT
this work Transformer + RL 25.04

Transformer + Source Monolingual Data 25.31
Transformer + Source Monolingual Data + RL 25.60
Transformer + Target Monolingual Data 25.24
Transformer + Target Monolingual Data + RL 25.58
Transformer + Source & Target Monolingual Data 26.13
Transformer + Source & Target Monolingual Data + RL 26.73

Table 7: Comparisons of different competitive end-to-end NMT systems. SougouKnowing results come from
http://matrix.statmt.org/matrix/systems_list/1878.

tial training of monolingual data can benefit the
model performance. Taking the last three rows as
an example, the BLEU score of the MLE model
trained on the combination of bilingual data and
target-side monolingual data is 25.24; based on this
model, RL training using the source-side monolin-
gual data further improves the model performance
by 0.7 (ρ < 0.01) BLEU points. From Table 6, we
can observe on top of a quite strong MLE baseline
(26.13), through the unified RL training, we can
still improve the test set by 0.6 points to 26.73 (ρ <
0.01), which shows the effectiveness of combining
source/target monolingual data and reinforcement
learning.

5.4 Comparison with Other Models

At last, as a summary of our empirical result-
s, we compare several representataive end-to-end
NMT systems to our work in Table 7, which
includes the Transformer (Vaswani et al., 2017)
model, with/without back-translation (Sennrich
et al., 2015a) and the best NMT system in
WMT17 Chinese-English translation challenge5

(SougouKnowing-ensemble). The results clear-
ly show that after combing both source-side and
target-side monolingual data with RL training, we
obtain the state-of-the-art BLEU score 26.73, even
surpassing the best ensemble model in WMT17
Zh-En translation challenge.

6 Related Work

Our work is mainly related with the literature of us-
ing reinforcement learning to directly optimize the
evaluation measure for neural machine translation.
Several representative works are (Ranzato et al.,

5http://matrix.statmt.org/matrix/
systems_list/1878

2016; Shen et al., 2016; Bahdanau et al., 2017). In
(Ranzato et al., 2016), the authors propose to train
a neural translation model with the objective grad-
ually shifting from maximizing token-level likeli-
hood to optimizing the sentence-level BLEU score.
Shen et al. (2016) proposes to adopt minimum risk
training (Goel and Byrne, 2000) to minimize the
task specific expected loss (i.e., induced by BLEU
score) on NMT training data. Instead of the RE-
INFORCE (Williams, 1992) algorithm used in the
above two works, Bahdanau et al. (2017) further
optimizes the policy by actor-critic algorithm. Wu
et al. (2016) introduces a simple RL based method
to optimize the stacked LSTM model for NMT,
achieving better BLEU scores on English-French
translation but not on English-German. Edunov
et al. (2017) presents a comparative study of sev-
eral classical structural prediction losses for NMT
model, which also includes sequence-level loss but
not exactly the same as RL.

Our work is also related with the research works
that leverage monolingual data for improving N-
MT models (Zhang and Zong, 2016; Sennrich et al.,
2015a; Wang et al., 2018; Xia et al., 2016; Cheng
et al., 2016). Zhang and Zong (2016) exploits the
source-side monolingual data in NMT. Sennrich
et al. (2015a) proposes back-translation method
to leverage target-side monolingual data for NMT.
Xia et al. (2016) formulates the machine transla-
tion as a communication game, which leverages
the power of two directional translation models and
source/target monolingual data. Cheng et al. (2016)
proposes a similar semi-supervised approach. How-
ever, none of these works have explored the power
of monolingual data in the context of training NMT
model with reinforcement learning.

http://matrix.statmt.org/matrix/systems_list/1878
http://matrix.statmt.org/matrix/systems_list/1878
http://matrix.statmt.org/matrix/systems_list/1878


3620

7 Conclusion

In this work, we presented a study of how to ef-
fectively train NMT models using reinforcement
learning. Different RL strategies were evaluated
in German-English, English-Chinese and Chinese-
English translation tasks on large-scale bilingual
datasets. We found that (1) multinomial sampling
is better than beam search, (2) several previous
tricks such as reward shaping and baseline reward
does not make significant difference, and (3) the
combination of the MLE and RL objectives is im-
portant. In addition, we explored the source/target
monolingual data for RL training. By combing the
power of RL and monolingual data, we achieve the
state-of-the-art BLEU score on WMT17 Chinese-
English translation task. We hope that our study
and results can benefit the community and bring
some insights on how to train deep NMT models
with reinforcement learning and big data.

References
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,

Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. 2017. An actor-critic
algorithm for sequence prediction. Fifth Internation-
al Conference on Learning Representations.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. Third International
Conference on Learning Representations.

Ondřej Bojar, Rajen Chatterjee, Christian Federman-
n, Yvette Graham, Barry Haddow, Shujian Huang,
Matthias Huck, Philipp Koehn, Qun Liu, Varvara Lo-
gacheva, Christof Monz, Matteo Negri, Matt Post,
Raphael Rubino, Lucia Specia, and Marco Turchi.
2017. Findings of the 2017 conference on machine
translation. In Proceedings of the Second Confer-
ence on Machine Translation, Volume 2: Shared
Task Papers, pages 169–214, Copenhagen, Denmark.
Association for Computational Linguistics.

Samidh Chatterjee and Nicola Cancedda. 2010. Mini-
mum error rate training by sampling the translation
lattice. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 606–615. Association for Computational Lin-
guistics.

Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Semi-
supervised learning for neural machine translation.
meeting of the association for computational linguis-
tics, pages 1965–1974.

Sergey Edunov, Myle Ott, Michael Auli, David Grangi-
er, and Marc’Aurelio Ranzato. 2017. Classical struc-

tured prediction losses for sequence to sequence
learning. Proceedings of NAACL-HLT 2018.

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N Dauphin. 2017. Convolutional
sequence to sequence learning. Proceedings of the
34th international conference on machine learning.

Vaibhava Goel and William J Byrne. 2000. Minimum
bayes-risk automatic speech recognition. Computer
Speech & Language, 14(2):115–135.

Hany Hassan, Anthony Aue, Chang Chen, Vishal
Chowdhary, Jonathan Clark, Christian Federman-
n, Xuedong Huang, Marcin Junczys-Dowmunt,
William Lewis, Mu Li, et al. 2018. Achieving hu-
man parity on automatic chinese to english news
translation. arXiv preprint arXiv:1803.05567.

Di He, Hanqing Lu, Yingce Xia, Tao Qin, Liwei Wang,
and Tieyan Liu. 2017. Decoding with value network-
s for neural machine translation. In Advances in
Neural Information Processing Systems, pages 178–
187.

Xiaodong He and Li Deng. 2012. Maximum expect-
ed bleu training of phrase and lexicon translation
models. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 292–301. Association
for Computational Linguistics.

Peter Henderson, Riashat Islam, Philip Bachman,
Joelle Pineau, Doina Precup, and David Meger.
2018. Deep reinforcement learning that matters.
Thirthy-Second AAAI Conference On Artificial Intel-
ligence.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. Third Interna-
tional Conference on Learning Representations.

Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, and
Dan Jurafsky. 2017. Adversarial learning for neural
dialogue generation. Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos
Santos, Mo Yu, Bing Xiang, Bowen Zhou, and
Yoshua Bengio. 2017. A structured self-attentive
sentence embedding. Fifth International Confer-
ence on Learning Representations.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Alex Graves, Ioannis Antonoglou, Daan Wierstra,
and Martin Riedmiller. 2013. Playing atari with
deep reinforcement learning. Advances in neural in-
formation processing systems, workshop.

Vinod Nair and Geoffrey E Hinton. 2010. Rectified
linear units improve restricted boltzmann machines.
In Proceedings of the 27th international conference
on machine learning, pages 807–814.



3621

Andrew Y Ng, Daishi Harada, and Stuart Russell.
1999. Policy invariance under reward transforma-
tions: Theory and application to reward shaping. In
ICML, volume 99, pages 278–287.

Khanh Nguyen, Hal Daumé III, and Jordan Boyd-
Graber. 2017. Reinforcement learning for bandit
neural machine translation with simulated human
feedback. Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic e-
valuation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2016. Sequence level train-
ing with recurrent neural networks. Fourth Interna-
tional Conference on Learning Representations.

FW Scholz. 1985. Maximum likelihood estimation.
Encyclopedia of statistical sciences.

Rico Sennrich, Alexandra Birch, Anna Currey, Ulrich
Germann, Barry Haddow, Kenneth Heafield, An-
tonio Valerio Miceli Barone, and Philip Williams.
2017. The university of edinburgh’s neural mt sys-
tems for wmt17. In Proceedings of the Second Con-
ference on Machine Translation, pages 389–399. As-
sociation for Computational Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015a. Improving neural machine translation mod-
els with monolingual data. Proceedings of the 54th
Annual Meeting of the Association for Computation-
al Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015b. Neural machine translation of rare words
with subword units. Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics.

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Minimum
risk training for neural machine translation. meet-
ing of the association for computational linguistics,
pages 1683–1692.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural network-
s. In Advances in neural information processing sys-
tems, pages 3104–3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 6000–6010.

Yijun Wang, Yingce Xia, Li Zhao, Jiang Bian, Tao Qin,
Guiquan Liu, and T Liu. 2018. Dual transfer learn-
ing for neural machine translation with marginal
distribution regularization. In Proceedings of the
Thirty-Second AAAI Conference on Artificial Intel-
ligence.

Lex Weaver and Nigel Tao. 2001. The optimal reward
baseline for gradient-based reinforcement learning.
In Proceedings of the Seventeenth conference on Un-
certainty in artificial intelligence, pages 538–545.
Morgan Kaufmann Publishers Inc.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. In Reinforcement Learning, pages
5–32. Springer.

Lijun Wu, Xu Tan, Di He, Fei Tian, Tao Qin, Jianhuang
Lai, and Tie-Yan Liu. 2018a. Beyond error propaga-
tion in neural machine translation: Characteristics
of language also matter. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing.

Lijun Wu, Fei Tian, Li Zhao, Jianhuang Lai, and Tie-
Yan Liu. 2018b. Word attention for sequence to se-
quence text understanding. In AAAI.

Lijun Wu, Yingce Xia, Li Zhao, Fei Tian, Tao Qin,
Jianhuang Lai, and Tie-Yan Liu. 2017a. Adversar-
ial neural machine translation. arXiv preprint arX-
iv:1704.06933.

Lijun Wu, Li Zhao, Tao Qin, Jianhuang Lai, and Tie-
Yan Liu. 2017b. Sequence prediction with unlabeled
data by reward function learning. IJCAI-17, pages
3098–3104.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey, Max-
im Krikun, Yuan Cao, Qin Gao, Klaus Macherey,
et al. 2016. Google’s neural machine translation sys-
tem: Bridging the gap between human and machine
translation. arXiv preprint arXiv:1609.08144.

Yingce Xia, Di He, Tao Qin, Liwei Wang, Nenghai Yu,
Tieyan Liu, and Weiying Ma. 2016. Dual learning
for machine translation. neural information process-
ing systems, pages 820–828.

Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin,
Nenghai Yu, and Tie-Yan Liu. 2017. Deliberation
networks: Sequence generation beyond one-pass de-
coding. In Advances in Neural Information Process-
ing Systems, pages 1784–1794.

Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ing source-side monolingual data in neural machine
translation. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1535–1545.


