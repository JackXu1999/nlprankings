



















































Convolutional Interaction Network for Natural Language Inference


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1576–1585
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1576

Convolutional Interaction Network for Natural Language Inference

Jingjing Gong‡, Xipeng Qiu∗‡, Xinchi Chen‡, Dong Liang§ †, Xuanjing Huang‡
‡ Shanghai Key Laboratory of Intelligent Information Processing, Fudan University

‡ School of Computer Science, Fudan University
§ State Key Laboratory of Information Security, Institute of Information Engineering CAS

{jjgong15, xpqiu, xinchichen13, xjhuang}@fudan.edu.cn,liangdong@iie.ac.cn

Abstract

Attention-based neural models have achieved
great success in natural language inference
(NLI). In this paper, we propose the Con-
volutional Interaction Network (CIN), a gen-
eral model to capture the interaction between
two sentences, which can be an alternative
to the attention mechanism for NLI. Specif-
ically, CIN encodes one sentence with the
filters dynamically generated based on an-
other sentence. Since the filters may be de-
signed to have various numbers and sizes, CIN
can capture more complicated interaction pat-
terns. Experiments on three very large datasets
demonstrate CIN’s efficacy.

1 Introduction

Natural language inference (NLI) is a pivotal and
challenging natural language processing (NLP)
task. The goal of NLI is to identify the logical
relationship (entailment, neutral, or contradiction)
between a premise and a corresponding hypothe-
sis. Generally, NLI is also related to many other
NLP tasks under the paradigm of semantic match-
ing of two texts, such as question answering Hu
et al. (2014); Wan et al. (2016) and information
retrieval Liu et al. (2015), and so on. An essential
challenge is to capture the semantic relevance of
two sentences. Due to the semantic gap (or lexical
chasm) problem, natural language inference is still
a challenging problem.

Recently, deep learning is raising a substan-
tial interest in natural language inference and has
achieved some great progresses Hu et al. (2014);
Parikh et al. (2016); Chen et al. (2017a). To
model the complicated semantic relationship be-
tween two sentences, previous models heavily uti-
lize various attention mechanism Bahdanau et al.

∗Corresponding Author.
†Contribution during internship at Fudan University.

(2014); Vaswani et al. (2017) to build the interac-
tion at different granularity (word, phrase and sen-
tence level), such as ABCNN Yin et al. (2016),
Attention LSTM Rocktäschel et al. (2015), bi-
directional attention LSTM Chen et al. (2017a),
and so on. While attention is very successful in
natural language inference, its mechanism is quite
simple and can be regarded as a weighted sum
of the target vectors. This paradigm results in a
lack of flexibility in more complicated interaction
model.

In this paper, we propose a new interaction
module, called Convolutional Interaction Network
(CIN), which can serve as an alternative module
of attention mechanism. Specifically, CIN utilizes
convolutional neural network to extract the valued
features (or representations) from sentences. In
the case of NLI, whether a feature of one sentence
being important depends on another sentence. In-
spired by the idea of using one network to gen-
erate the parameters of another network Ha et al.
(2016a); De Brabandere et al. (2016), we intro-
duce a filter generation network to dynamically
generate convolutional filters. Each sentence is
convolved by a dynamically generated filter by
another sentence. Thus, the convolved features
of one sentence can be regarded as context-aware
representations under the influence of another sen-
tence.

The contributions of this paper can be summa-
rized as follows.

1. CIN is a new interaction model, invented as
an alternative module to the attention model.
CIN can also capture both the intra- or inter-
interactions of two sentences.

2. Compared to attention model, CIN is more
general and flexible to capture the compli-
cated interaction. As discussed in Section



1577

3.3, the attention model is approximately
equivalent to a special case of CIN.

3. We perform extensive empirical studies on
three very large datasets. Experiment results
demonstrate that our proposed architecture is
effective for natural language inference.

2 Attentive Interaction for Natural
Language Inference

Currently, the dominative method for natural lan-
guage inference is to use attention mechanism to
model the interaction between two sentence.

Given two input sentences x =
[x1, x2, · · · , xm] and y = [y1, y2, · · · , yn]
with length m and n respectively, we first encode
them into two vectorial sequences

X = [x1,x2, · · · ,xm] ∈ Rd×m, (1)
Y = [y1,y2, · · · ,yn] ∈ Rd×n. (2)

The encoder usually consists of one or several
CNN/RNN layers to get the context-aware token
representations.

To capture the interaction between two sen-
tence, various neural attentions can be used, such
as sentence2word attention Rocktäschel et al.
(2015), word2word attention Parikh et al. (2016);
Chen et al. (2017a).

Word2word Attentive Interaction The
word2word attention captures the dependency
between two words xi and yj from the concerned
two sentences respectively. The word2word
attention computes a similarity matrix M , in
which each element mi,j is the alignment score
between xi and yj .

mi,j = f(xi,yj), 1 ≤ i ≤ m, 1 ≤ j ≤ n, (3)

where f is a score function.
There are two most prevalent attention func-

tions: multiplicative attention and additive atten-
tion. Multiplicative attention is:

f(xi,yj) = x
T
i yj . (4)

Additive attention computes a compatibility func-
tion by a feed-forward network with a single hid-
den layer.

f(xi,yj) = w
>σ(W1xi + W2yj + b), (5)

where w, W1, W2 and b are learnable parame-
ters.

While these two kinds of attentions have similar
performance, the multiplicative attention is more
popular in practice since it requires less computa-
tion power and have less memory demand with op-
timized matrix multiplication. With multiplicative
attention, we can compute the mimic representa-
tions for both X and Y .

X̄ = Y softmax(Y TX) ∈ Rd×m, (6)
Ȳ = Xsoftmax(XTY ) ∈ Rd×n, (7)

where softmax(·) is column-wise normalization
function. Each vector x̄i ∈ X̄ is called as
mimic vector, which is a weighted summation of
{yj}nj=1. Intuitively, the mimic vector x̄i provides
the related information of token xi extracted from
sentence Y .

Prediction After interaction, a prediction mod-
ule is used to aggregate the interaction informa-
tion and extract the fix-length representation of
two sentences. Finally, the final sentence repre-
sentations are fed into a feed-forward network to
predict the relationship between two sentences.

3 Convolutional Interaction Network

In this section, we propose a new interaction
method by utilizing dynamic convolutional filters,
called Convolutional Interaction Network (CIN).
CIN can serve as an alternative module of atten-
tion mechanism.

We first briefly introduce how the convolution
works over text sequence, then describe the pro-
posed model and its connection to attention model.

3.1 Convolution over Sequence
Convolution is an effective operation in deep neu-
ral networks, which convolves the input with a set
of filters to extract non-linear compositional fea-
tures. Although originally designed for computer
vision, convolutional models have subsequently
shown to be effective for NLP and have achieved
excellent performance in sentence modeling Kim
(2014); Kalchbrenner et al. (2014), and other tra-
ditional NLP tasks Hu et al. (2014); Zeng et al.
(2014); Gehring et al. (2017).

Given a sentence representation X =
[x1,x2, · · · ,xm] ∈ Rd×m, a convolutional
filter W (f) ∈ Rd×kd, the convolution process is
defined as

x′i = f
(
W (f)[xi−[k/2], · · · ,xi+[k/2]] + b(f)

)
, (8)



1578

Filters

Convolutional 
Interaction Network

Filter Generation
Network

Filter Generation
Network

Wx Wy

Figure 1: Convolutional Interaction Network. ⊗
denotes the convolution operation.

where f(·) is a non-linear activation function, such
as ReLU, k indicates the size of convolution win-
dow, and b(f) ∈ Rd is a bias vector.

The convolution can be abbreviated as

X ′ = f(W (f) ⊗X) ∈ Rd×m, (9)

where ⊗ denotes the convolutional operation. To
ensure the output of convolution has equal length
as to the input, we pad [k2 ] zero vectors on both
sides of the input.

3.2 Convolutional Interaction Network
Convolution is very effective when it comes to
extracting useful features from a sentence. But
for NLI, whether a word (or feature) being im-
portant in one sentence depends on another sen-
tence. Therefore, a better convolution operation
should have the ability to extract substantial fea-
tures from one sentence according to another sen-
tence. Thus, the convolutional filter should be
dynamically changeable. Inspired by Jia et al.
(2016); Ha et al. (2016b), we propose a filter gen-
eration network (FGN) to generate a dynamical
filter, which is used to extract the context-aware
information.

Given two sentences x, y, and their representa-
tions X = [x1,x2, · · · ,xm] ∈ Rd×m and Y =
[y1,y2, · · · ,yn] ∈ Rd×n, the filter for each sen-
tence is generated according to the other sentence
by

W (f)x = FGN(X) ∈ Rd×τd, (10)
W (f)y = FGN(Y ) ∈ Rd×τd, (11)

where τ is the width of filter, FGN(·) is the filter
generation network. A detailed implementation of
FGN is presented in Section 3.4.

Now we can convolve the two sentences with
the generated filters.

X̄ = f(W (f)y ⊗X) ∈ Rd×m, (12)

Ȳ = f(W (f)x ⊗ Y ) ∈ Rd×n, (13)

where the attained matrix X̄ and Ȳ can be re-
garded as the context-aware representation of sen-
tences x and y, depending on each other.

Figure 1 gives an illustration of CIN.

3.3 Connection to Attentive Interaction
CIN is more general than attention model. Assum-
ing that we set k = 1 and FGN to be a function of
FGN(X) = XXT, Eq. (12) and (13) of CIN can
be written as

X̄ = (Y Y T)X = Y (Y TX), (14)

Ȳ = (XXT)Y = X(XTY ). (15)

Compared to Eq. (6) and (7), under the above
assumption, CIN is equivalent to the word2word
multiplicative attention model without softmax
normalization.

3.4 An Implementation of Filter Generation
Network (FGN)

To generate the dynamic filters, the key factor
is how to choose the filter generation network
FGN(·) in Eq. (10) and (11). Although many so-
phisticated networks can be employed, we give an
simple implementation in this paper.

For ease of presentation, we only describe how
we generate dynamical filter according to sentence
x. The same procedure is utilized for sentence y.

Firstly, we summarize the information of sen-
tence x with an over-time k-max pooling on X ,

Ux = ReLU(Wu ⊗X) ∈ Rd×m, (16)
z1:kx = k-max(Ux) ∈ Rd×k, (17)

where Ux is a non-linear transformation of X by
convolution filter Wu ∈ Rd×d. The idea of k-max
pooling is to capture the most important features
(the k highest values) from sentence X .

Then we generate k filters W jx for j = 1, · · · , k
by

W jx = ReLU
(
Pdiag(zjx)Q+B

)
, (18)



1579

where P ∈ R
d
k
×d, Q ∈ Rd×τd and B ∈ R

d
k
×τd

are learnable parameters.
The final filter is obtained by concatenating the

k generated filters,

W (f)x = [W
1
x ;W

2
x ; · · · ;W kx ] ∈ Rd×τd. (19)

Similar to x, we can also obtain the dynamic
filters W (f)y according to the sentence y.

4 Incorporating CIN into a Deep
Network Architecture for NLI

Our overall network architecture for NLI is based
on a successful model proposed by Chen et al.
(2017a). The major difference is that we use CIN
to capture the interaction, instead of bi-directional
attention.

The network architecture consists of three com-
ponents: (1) an encoding layer; (2) convolutional
interaction layers; (3) a prediction layer. Figure 2
gives an illustration.

4.1 Encoding Layer

The input of natural language inference task is a
pair of sentences x and y. Since each word in a
sentence is a symbol that can not be directly pro-
cessed by neural networks, we need first map each
word to a d dimensional embedding vector.

Thus, the two sentences are mapped to two ma-
trix Ex ∈ Rde×m and Ey ∈ Rde×n respectively.
We also use syntactical and lexical information
such as part of speech tagging information, ex-
act match feature and character representation. In
this paper, exact match value of each word is set
to 1(default to be 0) if the word concerned share
the same stem or lemma with any word in coun-
terpart sentence. Character representation of the
word is obtained using a convolution neural net-
work followed by a max pooling along sequence
length dimension as same as Kim (2014). The fi-
nal representation of word is a concatenation of
word embedding, character encoded vector, POS
tagging embedding and exact match feature. Both
character embedding and POS tagging embedding
are randomly initialized. All embeddings are up-
dated during training.

We use bi-directional LSTM (BiLSTM)
Hochreiter and Schmidhuber (1997) to in-
corporate the forward and backward context
information of sequence. Thus, we can get the

avg max avg max

FNN

p(·|X,Y)

En
co

d
in

g
La

ye
r

C
o

n
vo

lu
ti

o
n

al
 In

te
ra

ct
io

n
La

ye
rs

P
re

d
ic

ti
o

n
La

ye
r

Nx

fusion fusion

Filter Generation
Network

Filter Generation
Network

Wx Wy

Figure 2: The overall network architecture for nat-
ural language inference. The Nx means the num-
ber of the stacking interaction layers.

phrase-level encoding of two input sentences,

X = [Bi-LSTM(Ex);Ex], (20)

Y = [Bi-LSTM(Ey);Ey], (21)

where X ∈ Rd×m and Y ∈ Rd×n are the phrase-
level encoding representation of sentence x and y
respectively.

4.2 Convolutional Interaction Layers

In the interaction layers, we use our proposed CIN
to model the interaction between two sentences.

We first dynamically generate context-aware fil-
ters W (f)x and W

(f)
y based on the sentence encod-

ings X and Y respectively, which are further used
for both intra-sentence and inter-sentence interac-
tion.

Intra-Sentence Interaction The intra-sentence
convolutional interaction is to convolve one sen-
tence by the filter generated by itself.

Xintra = f(W
(f)
x ⊗X), (22)

Yintra = f(W
(f)
y ⊗ Y ), (23)

The role of the intra-sentence convolutional in-
teraction is the same as self-attention Shen et al.
(2017), which is also very useful in NLI.

Inter-Sentence Interaction The inter-sentence
interaction takes filters generated from the coun-



1580

terpart sentence to convolve the inputs.

Xinter = f(W
(f)
x ⊗ Y ), (24)

Yinter = f(W
(f)
y ⊗X), (25)

The inter-sentence convolutional interaction
plays a role similar to the cross-attention between
two sentences.

Fusion Layer After CIN, we can fuse two kinds
of context-aware representations of each sentence.
For sentence x, the Xintra and Xinter represent
the extracted features under consideration of in-
formation of itself and sentence y respectively.

To efficiently utilizeXintra andXinter, a fusion
layer is used. We use the comparing operation pro-
posed in Chen et al. (2017a) to fuse the two kinds
of representation. Let ui and vi be intra and in-
ter attentive vector of the i-th word in sentence x,
a heuristic and effective composition operator is
used to combine two vectors.

x̄
(c)
i = [ui;vi;ui − vi;ui � vi; |ui − vi|], (26)

x
(c)
i = ReLU(Wcx̄

c
i + Wxxi + bc), (27)

Thus, we can obtain two fused representations
X(c) and Y (c) for two sentences, which are fur-
ther fed into the prediction layer or another stacked
interaction layer. The interaction layers can be
stacked for Nx times to capture the complicated
matching information.

4.3 Prediction Layer
After interaction layers, an aggregation layer is
employed to aggregate the two sequences of fus-
tion vectors X(c) and Y (c) into a fixed-length
matching vectors. The aggregation component
usually consists of another BiLSTM layer and a
following pooling layer. We then perform max
pooling over time for both X(c) and Y (c) to get
two fix representation vector for two sentences, p
and h:

p = max(X(c)), (28)

h = max(Y (c)), (29)

where the functions max is the max pooling oper-
ations over time steps.

Finally, the pooled vectors are composed as one
relation vector and fed into a feed-forward net-
work to predict the relationship between two sen-
tences. Specially, the two-layer feed-forward net-
work has one hidden layers with tanh activation

Train Dev Test Len(P) Len(H) Vocab

SNLI 549K 9.8K 9.8K 14 8 36K
MultiNLI1 392K 9.8K 9.8K 22 11 85K
MultiNLI2 392K 9.8K 9.8K 22 11 85K

Quora 384K 10K 10K 12 12 107K

Table 1: Statistics of three datasets: SNLI,
MultiNLI, Quora. Len(P) and Len(H) refer to
the average length of two sentences. MultiNLI1

and MultiNLI2 indicate the in-domain and cross-
domain datasets.

and softmax output layer in our experiments.

m = [p;h;p− h;p ∗ h; |p− h|], (30)
p(·|x, y) = FNN(m). (31)

5 Training

Given a trainset {x(i), y(i), t(i)}Ni=1, the objective
is to minimize a cross entropy loss J (θ):

J (θ)=− 1
N

∑
i

log p(t(i)|x(i), y(i))+λ||θ||22, (32)

where θ represents all the connection weights.
We use the Adam optimizer Kingma and Ba

(2014) with an initial learning rate of 0.0004. De-
fault L2 regularization λ is set to 10−6. To avoid
overfitting, dropout is applied after each fully con-
nected, recurrent or convolutional layer.

Initialization We take advantage of pre-trained
word embeddings such as Glove Pennington et al.
(2014) to transfer more knowledge from vast un-
labeled data. For the words that don’t appear in
Glove, we randomly initialize their embeddings
from a normal distribution with mean 0.0 and stan-
dard deviation 0.1.

The network weights are initialized with Xavier
normalization Glorot and Bengio (2010) to main-
tain the variance of activations throughout the for-
ward and backward passes. Biases are uniformly
set to zero when the network is constructed.

5.1 Datasets

To make quantitative evaluation, our model was
evaluated on three well known datasets: Stan-
ford Natural Language Inference dataset (SNLI),
MultiNLI dataset and Quora Question pair dataset
(Quora). Detailed statistical information of these
datasets is shown in Table 1.



1581

Models Train Test

Handcrafted features (Bowman et al., 2015) 99.7 78.2

LSTM with attention (Rocktäschel et al., 2015) 85.3 83.5
Match-LSTM (Wang and Jiang, 2016) 92.0 86.1
Decomposable attention model (Parikh et al., 2016) 90.5 86.8
BiMPM (Zhiguo Wang, 2017) 90.9 87.5
NTI-SLSTM-LSTM (Munkhdalai and Yu, 2017) 88.5 87.3
Re-read LSTM (Sha et al., 2016) 90.7 87.5
DIIN (Gong et al., 2017) 91.2 88.0
ESIM (Chen et al., 2017a) 92.6 88.0
CIN 93.2 88.0

ESIM (Chen et al., 2017a) (Ensemble) 93.5 88.6
BiMPM (Zhiguo Wang, 2017) (Ensemble) 93.2 88.8
DIIN (Gong et al., 2017) (Ensemble) 92.3 88.9
CIN (Ensemble) 94.3 89.1

Table 2: Performance on SNLI dataset.

SNLI The SNLI corpus Bowman et al. (2015)
consists of 570,152 sentence pairs. Each sentence
pair is labeled as one of entailment, contradiction
and neutral relation.

MultiNLI Orgnized the same as SNLI,
MultiNLI corpus Williams et al. (2017) is another
dataset for NLI, it contains 433,000 sentence
pairs. Like SNLI, each pair is labeled with one
of entailment, contradiction and neutral label.
Difference between MultiNLI and SNLI is that,
MultiNLI have in-domain test set and develop-
ment set as well as an out-of-domain test and
development set.

Quora The Quora Question pair dataset have
over 400k question pairs, each question pair is as-
signed with a binary label to indicate if the pair are
paraphrase to each other. We evaluate our model
on the data which was previously partitioned by
Zhiguo Wang (2017)

5.2 Overall Results
We use the accuracy to evaluate the performance
of our convolutional interaction network (CIN)
and other models on SNLI, MultiNLI and Quora.

SNLI Table 2 shows the results of different
models on the train set and test set of SNLI. The
first row gives a baseline model with handcrafted
features presented by Bowman et al. (2015). All
the other models are attention-based neural net-
works. Wang and Jiang (2016) exploits the long

short-term memory (LSTM) for NLI. Parikh et al.
(2016) uses attention to decompose the problem
into subproblems that can be solved separately.
Chen et al. (2017a) incorporates the chain LSTM
and tree LSTM jointly. Zhiguo Wang (2017) pro-
poses a bilateral multi-perspective matching for
NLI.

In Table 2, the second block gives the single
models. As we can see, our proposed model
CIN achieves 88.0% in accuracy on SNLI test set.
Compared to the previous work, CIN obtains com-
petitive performance.

To further improve the performance of NLI sys-
tems, researchers have built ensemble models. En-
semble systems obtained the best performance on
SNLI. Our ensemble model obtains 89.1% in ac-
curacy and outperforms the current state-of-the-art
model.

Overall, single model of CIN performs compet-
itively well and outperforms the previous models
on ensemble scenarios for the natural language in-
ference task.

MultiNLI Table 3 shows the performance of
different models on MultiNLI. The original aim of
this dataset is to evaluate the quality of sentence
representations. Recently this dataset is also used
to evaluate the interaction model involving atten-
tion mechanism.

The first line of Table 3 gives a baseline model
without interaction. The second block of Table 3
gives the attention-based models. The proposed



1582

Models Match Mismatch

BiLSTM
(Williams et al., 2017)

67.0 67.6

InnerAtt
(Balazs et al., 2017)

72.1 72.1

ESIM
(Chen et al., 2017a)

72.3 72.1

Gated-Att BiLSTM
(Chen et al., 2017b)

73.2 73.6

ESIM
(Chen et al., 2017a)

76.3 75.8

CIN 77.0 77.6

Table 3: Performance on MultiNLI test set.

Models Test

Siamese-CNN 79.60
Multi-Perspective CNN 81.38
Siamese-LSTM 82.58
Multi-Perspective-LSTM 83.21
L.D.C 85.55
BiMPM (Zhiguo Wang, 2017) 88.17

CIN 88.62

Table 4: Performance on Quora question pair
dataset.

model, CIN, achieves the accuracies of 77.0% and
77.6% on the match and mismatch test sets respec-
tively. The results show that our model outper-
forms the other models.

Quora Table 4 shows the performance of differ-
ent models on the Quora test set. The baselines
on Table 4 are all implemented in Zhiguo Wang
(2017). The Siamese-CNN model and Siamese-
LSTM model encode sentences with CNN and
LSTM respectively, and then predict the re-
lationship between them based on the cosine
similarity. Multi-Perspective-CNN and Multi-
Perspective-LSTM are transformed from Siamese-
CNN and Siamese-LSTM respectively by replac-
ing the cosine similarity calculation layer with
their multi-perspective cosine matching function.
The L.D.C is a general compare-aggregate frame-
work that performs word-level matching followed
by a aggregation of convolution neural networks.
As we can see, our model outperforms the base-

Models Dev Test

CIN 88.6 88.0
Remove whole interaction 85.6 85.1
Remove intra-attention 88.1 87.7

Table 5: Ablation experiment on SNLI dataset.

Premise

(1) A girl playing a violin along with a group of people
(2) A girl playing a violin along with a group of people

Hypothesis

(1) A girl is playing an instrument .
(2) A girl is playing an instrument .

Table 6: Gradient visualization of premise and hy-
pothesis. (1) Gradient scale of X , Y on encoding
layer. (2) Gradient scale ofX(c), Y (c) on first CIN
layer. Darker color corresponds to a higher scale
of gradient, and implies a higher contribution to
the final prediction.

lines and achieve 88.62% in the test sets of Quora
corpus.

5.3 Model Ablation

To better understand the performance of our
model, we analyze the effect of each key compo-
nent of the proposed model. As illustrated in Table
5, the first row is the full CIN model. By dropping
convolutional interaction layers, the performance
decreases to 85.1% on the test set, which indi-
cate the interaction information is crucial for NLI.
By just dropping intra-attention layer, the perfor-
mance decreases to 87.7% on the test set. Accord-
ing to the results, all of the components positively
contribute to the final performance.

5.4 Case Study

To give an intuitive understanding of how our
model works, we give an analysis on the follow-
ing case from the test set.

Premise: A girl playing a violin along with a
group of people.
Hypothesis: A girl is playing an instrument.
Label: Entailment.
The visualization results are produced from

model with two stacked CINs. X , Y is the hid-
den states at encoding layer, and X(c), Y (c) is the
hidden states at first CIN layer. For a hidden state



1583

A gi
rl

is pl
ay

in
g

an in
st

ru
m

en
t

(a)

A
girl

playing
a

violin
along

with
a

group
of

people

A gi
rl

is pl
ay

in
g

an in
st

ru
m

en
t

(b)
0.06

0.12

0.18

0.24

0.30

Figure 3: A visualization of word to word correla-
tion. Darker color correspond to a higher correla-
tion. (a) Correlation of XTY at encoder layer. (b)
Correlation of X(c)

T
Y (c) at first CIN layer.

xi of word xi, we can calculate its gradient scale
|| ∂J∂xi ||

2 to show its contribution to final prediction.
Table 6 shows the gradient scales of hidden

states of each word in the encoding layer and the
first CIN layer. As we can see, some phrases (like
playing a violin and playing an instrument) in-
stead of isolated words (like violin and instrument)
become more focused after a CIN layer. It implies
CIN could capture some higher level patterns.

Figure 3 gives a visualization of correlations
of hidden states of two sentences. (a) shows the
correlations after the encoding layer, the same
words are most correlated. This is because em-
bedding layer and encoding layer are shared be-
tween premise and hypothesis. (b) shows the cor-
relations after the first CIN layer, the correlation
exists between phrases {playing a violin vs. play-
ing an instrument}, instead of the same words.
The interaction layer connects playing in Premise
to Hypothesis instrument, and connects playing in
Hypothesis to Premise violin. Thus, the correla-
tion between instrument in Hypothesis and violin
in Premise are boosted, as we know these are im-
portant to reasoning.

6 Related Work

There are mainly two threads of work related to
ours.

One thread of work is using attention-based
model for natural language inference (NLI). NLI
has been widely investigated for many years. Ben-

efiting from the development of deep learning and
the availability of large-scale annotated datasets,
deep neural models have achieved great success.
Rocktäschel et al. (2015) firstly use LSTM with
attention for text matching task. Wang and Jiang
(2016) use word-by-word attention to exploit the
word-level match. Parikh et al. (2016) propose a
new framework to model the relationship between
two sentences using interact-compare-aggregate
architecture. Chen et al. (2017a) incorporates the
chain LSTM and tree LSTM jointly. Zhiguo Wang
(2017) use self-attention mechanism to capture
contextual information from the whole sentence.

Unlike the above models, we use an alternative
model to capture the complicate interaction infor-
mation of two sentences.

Another thread of work is the idea of using one
network to generate the parameters of another net-
work. De Brabandere et al. (2016) proposed the
dynamic filter network to implicitly learn a variety
of filtering operations. Ha et al. (2016a) proposed
the model hypernetwork, which uses a small net-
work to generate the weights for a larger network.

Unlike these models, our dynamical filter is em-
ployed for interaction. Therefore, a filter genera-
tion function is proposed to capture the related in-
tra and inter dependent information of a sentence
pair.

7 Conclusion

In this paper, we propose an alternative interaction
model, Convolutional Interaction Network (CIN),
for natural language inference. CIN utilizes the
dynamic convolutional filters to model the inter-
action between two sentences. Specifically, each
sentence is convolved by dynamical filters gener-
ated based on another sentence. CIN is more gen-
eral and flexible since the filters may have various
numbers and sizes, thereby capturing more com-
plicated interaction patterns. Experiments on three
very large datasets demonstrate the efficacy of our
proposed model.

In future work, we hope to improve the extensi-
bility of CIN and apply it to other NLP tasks, such
as machine comprehension.

Acknowledgments

We would like to thank the anonymous review-
ers for their valuable comments. We would
like to thank the anonymous reviewers for their
valuable comments. The research work is



1584

supported by Shanghai Municipal Science and
Technology Commission (No. 17JC1404100
and 16JC1420401), and National Natural Sci-
ence Foundation of China (No. 61672162 and
61751201), Fundamental Theory and Cutting-
Edge Technology Research Program of Institute
of Information Engineering, CAS (Grant No.
Y7Z0281102).

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Jorge A Balazs, Edison Marrese-Taylor, Pablo Loy-
ola, and Yutaka Matsuo. 2017. Refining raw sen-
tence representations for textual entailment recogni-
tion via attention. arXiv preprint arXiv:1707.03103.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei,
Hui Jiang, and Diana Inkpen. 2017a. Enhanced
LSTM for natural language inference. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 1657–1668.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017b. Recurrent neural
network-based sentence encoder with gated atten-
tion for natural language inference. arXiv preprint
arXiv:1708.01353.

Bert De Brabandere, Xu Jia, Tinne Tuytelaars, and Luc
Van Gool. 2016. Dynamic filter networks. In Neural
Information Processing Systems (NIPS).

Jonas Gehring, Michael Auli, David Grangier, De-
nis Yarats, and Yann N. Dauphin. 2017. Convo-
lutional sequence to sequence learning. In Pro-
ceedings of the 34th International Conference on
Machine Learning, ICML 2017, Sydney, NSW, Aus-
tralia, 6-11 August 2017, pages 1243–1252.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In International conference on artificial
intelligence and statistics, pages 249–256.

Yichen Gong, Heng Luo, and Jian Zhang. 2017. Natu-
ral language inference over interaction space. arXiv
preprint arXiv:1709.04348.

David Ha, Andrew Dai, and Quoc V Le. 2016a. Hy-
pernetworks. arXiv preprint arXiv:1609.09106.

David Ha, Andrew Dai, and Quoc V Le. 2016b. Hy-
pernetworks. arXiv preprint arXiv:1609.09106.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences.
In Advances in Neural Information Processing Sys-
tems.

Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and
Luc Van Gool. 2016. Dynamic filter networks.
In Advances in Neural Information Processing Sys-
tems 29: Annual Conference on Neural Information
Processing Systems 2016, December 5-10, 2016,
Barcelona, Spain, pages 667–675.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng,
Kevin Duh, and Ye-Yi Wang. 2015. Representation
learning using multi-task deep neural networks for
semantic classification and information retrieval. In
NAACL.

Tsendsuren Munkhdalai and Hong Yu. 2017. Neural
tree indexers for text understanding. In Proceed-
ings of the conference. Association for Computa-
tional Linguistics. Meeting, volume 1, page 11. NIH
Public Access.

Ankur P Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. arXiv preprint
arXiv:1606.01933.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Tim Rocktäschel, Edward Grefenstette, Karl Moritz
Hermann, Tomáš Kočiskỳ, and Phil Blunsom. 2015.
Reasoning about entailment with neural attention.
arXiv preprint arXiv:1509.06664.

Lei Sha, Baobao Chang, Zhifang Sui, and Sujian Li.
2016. Reading and thinking: Re-read lstm unit
for textual entailment recognition. In Proceedings
of COLING 2016, the 26th International Confer-
ence on Computational Linguistics: Technical Pa-
pers, pages 2870–2879.



1585

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2017. Disan:
Directional self-attention network for rnn/cnn-
free language understanding. arXiv preprint
arXiv:1709.04696.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, 4-9 Decem-
ber 2017, Long Beach, CA, USA, pages 6000–6010.

Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu,
Liang Pang, and Xueqi Cheng. 2016. A deep ar-
chitecture for semantic matching with multiple po-
sitional sentence representations. In AAAI.

Shuohang Wang and Jing Jiang. 2016. Learning natu-
ral language inference with lstm. In Proceedings of
the North American Chapter of the Association for
Computational Linguistics.

Adina Williams, Nikita Nangia, and Samuel R Bow-
man. 2017. A broad-coverage challenge corpus for
sentence understanding through inference. arXiv
preprint arXiv:1704.05426.

Wenpeng Yin, Hinrich Schütze, Bing Xiang, and
Bowen Zhou. 2016. ABCNN: attention-based con-
volutional neural network for modeling sentence
pairs. TACL, 4:259–272.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via con-
volutional deep neural network. In Proceedings of
COLING, pages 2335–2344.

Radu Florian Zhiguo Wang, Wael Hamza. 2017. Bilat-
eral multi-perspective matching for natural language
sentences. In Proceedings of the Twenty-Sixth Inter-
national Joint Conference on Artificial Intelligence,
pages 4144–4150.


