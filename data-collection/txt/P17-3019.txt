



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics- Student Research Workshop, pages 114–119
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-3019

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics- Student Research Workshop, pages 114–119
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-3019

Fast Forward through Opportunistic Incremental
Meaning Representation Construction

Petr Babkin
Rensselaer Polytechnic Institute

Troy, New York
babkip@rpi.edu

Sergei Nirenburg
Rensselaer Polytechnic Institute

Troy, New York
nirens@rpi.edu

Abstract

One of the challenges semantic parsers
face involves upstream errors originating
from pre-processing modules such as ASR
and syntactic parsers, which undermine
the end result from the get go. We report
the work in progress on a novel incremen-
tal semantic parsing algorithm that sup-
ports simultaneous application of indepen-
dent heuristics and facilitates the construc-
tion of partial but potentially actionable
meaning representations to overcome this
problem. Our contribution to this point is
mainly theoretical. In future work we in-
tend to evaluate the algorithm as part of a
dialogue understanding system on state of
the art benchmarks.

1 Introduction

The versatility of human language comprehension
overshadows countless transient failures happen-
ing under the hood. At various points during a
conversation we are bound to tolerate error and
uncertainty: coming either from the speech itself
(e.g., disfluencies, omissions) or resulting from
our own mistakes and deficiencies as a hearer (e.g.,
misinterpretation of an ambiguous utterance due
to incomplete knowledge, etc.). The entire pro-
cess is an ever recurring cycle of gap filling, error
recovery and proactive re-evaluation. Better yet,
some of the arising issues we choose not to re-
solve completely — leaving room for underspeci-
fication and residual ambiguity, or abandoning al-
together. Similarly, we are highly selective with
respect to material to attend to, glancing over the
bits deemed of only minor relevance or redundant.

We believe incrementality and opportunism to
be key to achieving this type of behavior in a com-
putational NLU system. Incremental construction
of meaning representations allows for their contin-

uous refinement and revision with gradually accu-
mulating evidence. In addition, it gives the sys-
tem an opportunity to make meta-level decisions
of whether to pursue analysis of further material
and/or to a greater depth; or to satisfice with a par-
tial meaning representation built so far — thereby
reducing the amount of work and avoiding poten-
tially problematic material.

By opportunism we refer to the ability to simul-
taneously engage all available sources of decision
making heuristics (morphological, syntactic, se-
mantic, and pragmatic knowledge) as soon as their
individual preconditions are met. On one hand,
this provides a degree of independence among the
knowledge sources such that a failure of any one of
them does not necessarily undermine the success
of the process as a whole. On the other hand, op-
portunistic application of heuristics facilitates the
construction of partial or underspecified meaning
representations when their complete versions are
infeasible to obtain.

As a step towards incorporating these princi-
ples in an NLU system, we present a novel oppor-
tunistic incremental algorithm for semantic pars-
ing. The rest of the paper is organized as follows.
In the following section, we will introduce our
approach while providing necessary background.
Next, we will work through an example derivation,
highlighting some of the features of the algorithm.
Then, in the discussion section, we will position
this work in the context of related work in the field,
as well as within our own research agenda. Fi-
nally, we will conclude with the progress to date
and plans for the evaluation.

2 Approach

2.1 Framework

Our description will be grounded within the
theoretico-computational framework of Ontologi-
cal Semantics (OntoSem) — a cognitively inspired

114

https://doi.org/10.18653/v1/P17-3019
https://doi.org/10.18653/v1/P17-3019


and linguistically motivated knowledge-rich ac-
count of NLU and related cognitive processes
(McShane and Nirenburg, 2016).

The goal of the algorithm we are presenting in
this paper is to build meaning representations of
text (TMRs) in an incremental and opportunistic
fashion. As a source of building blocks for the
TMRs we will use the OntoSem lexicon and on-
tology. The lexicon provides a bridge between
surface forms (e.g., word lemmas, multi-word ex-
pressions, or phrasals) and their corresponding se-
mantic types (called concepts), while the ontology
encodes the relationships among concepts, such as
case roles defined on them. In addition, OntoSem
provides accounts of modality, reference, tempo-
ral relations, and speech acts, which become part
of the search space for our algorithm. A mean-
ing representation in OntoSem is denoted as a col-
lection of numbered frames with key-value entries
called slots. An example of such a TMR corre-
sponding to the natural language utterance “Apply
pressure to the wound!” is shown below:

REQUEST-ACTION-1
agent HUMAN-1
theme APPLY-1
beneficiary HUMAN-2

APPLY-1
agent HUMAN-2
instrument PRESSURE-1
theme WOUND-INJURY-1

The root of this TMR is an instance of the
REQUEST-ACTION speech act evoked by the im-
perative mood of the utterance. The agent (re-
quester) of this speech act is the implied speaker of
the utterance. The beneficiary (or patient, in other
semantic formalisms) is the implied addressee of
this message. Next, the theme of the speech act
(the action being requested) is an instance of the
APPLY event defined in the ontology. The agent
of the action requested is specified to be the same
as the addressee. The instrument is specified to
be an instance of PRESSURE. Finally, the theme
(or target) of the requested action references some
known instance of a WOUND-INJURY, either in-
troduced earlier in text, or contained in the com-
mon ground of the interlocutors.

2.2 Algorithm

A TMR can be represented as a (possibly rooted)
graph G = (V,E), where the set of vertices V rep-
resents semantic instances and the set of edges E

denotes case role linkings among them. We can
thus formulate our problem generally as that of
producing a pair (V,E) for an input utterance u
as a sequence of tokens 〈u1, u2, ..., un〉, so as to
maximize some score: argmax(V,E) score(V,E).

The algorithm operates in two phases: the for-
ward pass generates candidate solutions and the
backward pass extracts the best scoring solution.
The forward pass of the algorithm proceeds itera-
tively: by performing a series of incremental op-
erations: next and link, which we define in order.
The basic data structure used is an abstract set S
of items p0 through pn.

next is defined as an incremental operation
consuming one or more tokens from the input ut-
terance 〈u1, u2, ..., un〉 and returning zero or more
items p to be added to the set S.

link operation is defined as accepting a set Si
and returning a new set Si+1 and a bipartite graph
defined by (Si, Si+1, Li+1,i), where Li+1,i is a set
of edges between the two sets-partitions.
The following snippet provides a high level de-
scription of the execution of the forward pass of
the algorithm.

function EXPAND(〈u1, u2, ..., un〉)
Si ← ∅
initialize lattice C with tuple 〈Si, ∅, ∅〉
while NO-ACTIONABLE-TMR(C) do

if MORE-INPUT-NEEDED(C) then
Si ← Si∪NEXT(〈u1, u2, ..., un〉)

else
C ← C ∪ {LINK(Si)}
Si ← Si+1
if FIXED-POINT(C) then

return ∅
end if

end if
end while
return C

end function

Expand in the name of the function refers to the
fact that the lattice data structure used to store
the candidate solutions is repeatedly expanded by
the two incremental operations we just introduced.
The next operation expands the width of the lat-
tice by adding new elements to the set Si, which
can be thought of as a “layer” of the lattice. The
link operation, expands the depth of the lattice by
creating a new “layer” and connecting it to the el-
ements of the previous layer. Intuitively, the pro-
cess can be visualized diagonally as alternating

115



horizontal and vertical expansions. The entire lat-
tice can be thought of as a metaphorical “loom”,
on which meaning representations are “woven”.
The depth of the lattice is the depth of nesting
in the corresponding meaning representation tree
with a linear upper bound. In practice, however,
we expect it to approach O(log|U |) as most of the
nesting occurs in multiple-argument instances.

Figure 1: A schematic depiction of the incremen-
tal operations next (a), link (b), and the resulting
lattice containing a solution (c).

The following conditionals are employed to con-
trol the execution flow of the main algorithm:
1. NO-ACTIONABLE-TMR captures high-level
desiderata for execution continuation. In the cur-
rent implementation the algorithm simply requires
that the TMR is headed by an event and that its
as well as its constituents’ core roles (e.g., agent,
theme) are filled; and that it spans all components
of the input. Work is ongoing to operationalize
pragmatic considerations for actionability, includ-
ing whether a partial TMR is coherent with pre-
ceding discourse or is consistent with a known
script, that could warrant early halting.
2. MORE-INPUT-NEEDED is triggered when
there are no components to link despite the TMR
being non-actionable.
3. FIXED-POINT basically means that further it-
eration does not produce any novel interpretations.

The subsequent solution extraction phase
amounts to simply extracting TMR candidates
from the lattice via depth-first traversal and rank-
ing them by the cumulative scores of constituent
nodes and edges. It should be noted that this pro-
cedure operates on a considerably reduced sub-
set of the original search space as it effectively
chooses among a limited set of viable candidates
produced during the forward pass. Going a level
deeper in our description, we will now turn to the
implementation details of the incremental opera-
tions next and link.

The next operation translates the next word to-
ken (or a group of tokens) from the input utter-
ance into corresponding semantic type that are
then combined together into a TMR. It is currently
realized as a basic lexical look-up by lemma,
with greedy matching for multi-word expressions
and non-semantically loaded tokens are skipped.
For polysemous words, semantic representations
for all variants are considered. While the On-
toSem lexicon specifies a battery of syntactic and
morphological constraints imposed on each word
sense (e.g., syntactic dependencies they are ex-
pected to have — cf. McShane and Nirenburg
(2016) for an in-depth overview), their application
is deferred until the scoring stage as word sense
disambiguation is pursued jointly, as part of mean-
ing representation construction by the algorithm,
rather than as a pre-processing stage.

The link operation is the core TMR-building
routine of the algorithm.

function LINK(Si)
Si+1 ← Si; Li+i,i ← ∅
for all argument taking pj in Si+1 do

Rj ←CASE-ROLES(pj)
for all F in ALIGN(Si, Rj) do

F ⇔ {pk ∈ Si|SAT(pk, rj,k ∈ Rj)}
yield〈j, k, rj,k,SCORE(pj , pk, rj,k)〉

end for
end for
return (Si, Si+1, Li+1,i)

end function

On each application, it considers a superposition
of TMR fragments of depth 1 formed by the ele-
ments of the two adjacent partitions Si and Si+i.
The set Lii+1 is used to store edges connecting
the elements of the two sets. For each argument-
taking instance pj in Si+1, the function attempts to
find alignments from roles Rj to instances F ⊂ Si
that would satisfy their semantic constraints.

For each such alignment, the function yields the
resulting edges as tuples containing the end point
indices along with the corresponding case role la-
bel and link score. In addition to the already men-
tioned ranking of candidate meaning representa-
tions during the solution extraction phase, link
scores can also be used to drive beam search dur-
ing the forward pass to prune the search space
on the fly. Scores are currently assigned based
on a) how closely the filler matches the role’s
type constraint (inverse ontological distance) and
b) whether the role matches the expected syntactic

116



dependency specified for it in the lexicon. A large
part of future work will involve incorporation of
pragmatic heuristics including those based on co-
reference, coherence (e.g., prefer the role agent if
a filler in question took on the that role in a series
of events in preceding discourse), and knowledge
of scripts, all of which we hypothesize especially
crucial in situations with unreliable syntax.

The computational complexity of this operation
is a little harder to estimate since it depends on
variable numbers of word senses and correspond-
ing case roles. Treating them as constant factors
gives us quadratic worst-case complexity in the
length of the utterance. A more accurate estimate
can be obtained but the key point is that the over-
lapping representation of hypotheses with itera-
tive deepening eliminates the branching factor of
exhaustive permutation and results in polynomial
rather than exponential order of complexity.

2.3 A Worked Example

We will now proceed with the derivation of the
meaning representation for the example sentence.

Although the OntoSem lexicon specifies more fine
grain senses of the words used, to keep the exam-
ple simple, we will only consider a subset. Our
description will follow the derivation in Table 1.
1. The algorithm initializes with the empty sets of
semantic instances S and case roles links L.
2. Both NO-ACTIONABLE-TMR and MORE-
INPUT-NEEDED conditions are triggered, invok-
ing the next operation. The first token “Apply”
has an instrumental sense e.g., “apply paint to the
wall”, but to introduce some degree of ambigu-
ity we will also consider a phrasal sense as in “he
needs to apply himself”, having a meaning of the
modality of type EFFORT.
3. At the next step, the conditions are triggered
again since neither of the current head event can-
didates have their roles specified. The next to-
ken “pressure” instantiates two possible interpre-
tations: literal, physical PRESSURE, and figura-
tive i.e. “to be pressured to do something” trans-
lating into a modality of type POTENTIAL.
4. The currently disjoint TMR components
(p1, p2, p3, p4) can now be connected via case

# Op uz ∆Si,∆Lii+1 C
1 init ∅ ∅
2 next Apply p1:APPLY, p2:EFFORT p1 p2

3-4 next

link

pressure p3:PRESSURE, p4:POTENTIAL

l14, l
2
4, l

4
2:scope, l

3
1: instrument, theme

p1 p2 p3 p4

p1 p2 p3 p4

5-6 next

link

wound p5:WOUND-INJURY

l51:theme

p1 p2 p3 p4 p5

p1 p2 p3 p4 p5

7-8 next

link

! p6:REQUEST-ACTION

l16, l
2
6, l

4
6:theme

p1 p2 p3 p4 p5 p6

p1 p2 p3 p4 p5 p6

9 link n/a L23 ← L12 \ {l∗1}
L12 ← L12 \ {l∗6}

p1 p2 p3 p4 p5 p6

p1 p2 p3 p4 p5 p6

p1 p2 p3 p4 p5 p6

10 extract n/a p6:REQUEST-ACTION

p1:APPLY

p5:WOUND-INJURYp3:PRESSURE

instrument
theme

scope

p1 p2 p3 p4 p5

p1 p2 p4

p2 p4 p6

Table 1: Example TMR derivation. The columns respectively indicate: operation performed, current
token, instantiated concept or linked semantic role, and the visual representation of the solution lattice.

117



roles, which triggers the link operation, result-
ing in four compound TMR fragments: (AP-
PLY instrument/theme PRESSURE), (EFFORT
scope POTENTIAL), (POTENTIAL scope AP-
PLY), (POTENTIAL scope EFFORT). The first
interpretation has a role ambiguity to be resolved
during the solution extraction phrase.
5. Neither of the TMR fragments are complete,
still having unspecified roles, thereby triggering
the next operation. The following token “wound”
translates into p8:WOUND-INJURY concept1.
6. During the next iteration of linking
p8:WOUND-INJURY is employed as a theme of
p1:APPLY resulting in the (APPLY (instrument
PRESSURE) (theme WOUND-INJURY)) TMR
fragment.
7. The last token “!” confirms the imperative
mood of the utterance, signaling the instantiation
of the REQUEST-ACTION speech act.
8. The three TMR heads are then linked to the
theme slot of of the REQUEST-ACTION instance.
9. Since none of the current fragments alone
spans all of the components of the input despite
their aggregate coverage, they need to be com-
bined together through the creation of a new layer
producing nested TMR fragments. Certain links
have been pruned as not leading to novel solutions.
10.The termination condition is fulfilled as there
now exists a complete TMR candidate accounting
for all of the lexemes observed in the input. It is
shown in the table both as a tree and as a high-
lighted fragment of the solution lattice.

3 Discussion & Related Work

Syntax plays a formidable role in the understand-
ing of meaning. In a number of theories of se-
mantics, important aspects such as case role align-
ment are determined primarily based on some
syntactico-semantic interface such as in Lexical
Functional Grammars (Ackerman, 1995), Com-
binatory Categorial Grammars (Steedman and
Baldridge, 2011), or others. Some approaches
have even gone as far as to cast the entire prob-
lem of semantic parsing as that of decorating a de-
pendency parse (May, 2016). However, such tight
coupling of semantics with syntax has its down-
sides. First, linguistic variation in the expression
of the same meaning needs to be accounted for ex-
plicitly (e.g., by creating dedicated lexical senses

1Although a verbal sense is also conceivable, we will as-
sume it has been ruled out by morphology.

or production rules to cover all possible realiza-
tions). Second and more importantly, the feasi-
bility of a semantic interpretation becomes con-
ditional on the well-formedness of the language
input and the correctness of the corresponding
syntactic parse. This requirement seems unnec-
essarily strong considering a) human outstanding
ability to make sense of ungrammatical and frag-
mented speech (Kong et al., 2014) and b) still con-
siderably high error rates of ASR transcription and
parsing (Roark et al., 2006).

The algorithm we present, by contrast, operates
directly over the meaning representation space,
while relying on heuristics of arbitrary provenance
(syntax, reference, coherence, etc.) in its scor-
ing mechanism for disambiguation and to steer the
search process towards the promising region of the
search space. This allows it to focus on exploring
conceptually plausible interpretations while being
less sensitive to upstream noise. The algorithm
is opportunistic as the overlapping lattice repre-
sentation allows it to simultaneously pursue multi-
ple hypotheses, while the explicit evaluation of the
candidates is deferred until a promising solution is
found.

Incremental semantic parsing has come back
to the forefront of NLU tasks (Peldszus and
Schlangen, 2012), (Zhou et al., 2016). Ac-
counting for incrementality helps to make dia-
logues more human-like by accommodating spo-
radic asynchronous feedback as well as correc-
tions, wedged-in questions, etc. (Skantze and
Schlangen, 2009). In addition, incrementality of
meaning representation construction coupled with
sensory grounding has been shown to help prune
the search space and reduce ambiguity during se-
mantic parsing (Kruijff et al., 2007). The pre-
sented algorithm is incremental as it proceeds via a
series of operations gradually extending and deep-
ening the lattice structure containing candidate so-
lutions. The algorithm constantly re-evaluates its
partial solutions and potentially produces an ac-
tionable TMR without exhaustive search over the
entire input.

The design of the algorithm was influenced by
the classical blackboard architecture (Carver and
Lesser, 1992). Blackboard architectures offer sig-
nificant benefits: they develop solutions incremen-
tally by aggregating evidence; employ different
sources of knowledge simultaneously; and enter-
tain multiple competing or cooperating lines of

118



reasoning concurrently. Some implementation de-
tails of our algorithm were inspired by the well-
known Viterbi algorithm: e.g., forward and back-
ward passes, solution scoring (Viterbi, 1967); as
well as the relaxed planning graph heuristic from
autonomous planning: namely, the lattice repre-
sentation of partial solutions and the repeated si-
multaneous application of operators (Blum and
Furst, 1995).

4 Conclusion & Future Work

The opportunistic and incremental algorithm we
presented in this paper has the potential to improve
the flexibility and robustness of meaning repre-
sentation construction in the face of ASR noise,
parsing errors, and unexpected input. This ca-
pability is essential for NLU results to approach
true human capability (Peldszus and Schlangen,
2012). We implemented the algorithm and inte-
grated it with non-trivial subsets of the current
OntoSem’s domain-general lexicon (9354 out of
17198 senses) and ontology (1708 concept real-
izations out of 9052 concepts) (McShane et al.,
2016). Once fully integrated and tuned, we plan
to formally evaluate the performance of the algo-
rithm on a portion of the SemEval 2016 AMR
parsing shared task dataset (May, 2016) while
measuring the impact of parsing errors on the
end TMR quality. In addition, it would be in-
teresting to empirically quantify the utility of our
proposed pragmatic heuristics in the domain of a
task-oriented dialogue such as the Dialogue State
Tracking Challenge (Williams et al., 2013).

Acknowledgements

This research was supported in part by Grant
N00014-16-1-2118 from the U.S. Office of Naval
Research and by the Cognitive and Immersive
Systems Laboratory collaboration between Rens-
selaer Polytechnic Institute and IBM Research.
We would also like to thank the three anonymous
reviewers for their valuable feedback.

References
Farrell Ackerman. 1995. Lexical functional grammar.

In Jef Verschuren, Jan-Ola Östman, and Jan Blom-
maert, editors, Handbook of Pragmatics, pages 346–
350. John Benjamins, Amsterdam.

Avrim L. Blum and Merrick L. Furst. 1995. Fast plan-
ning through planning graph analysis. Artificial In-
telligence, 90(1):1636–1642.

Norman Carver and Victor Lesser. 1992. The Evolu-
tion of Blackboard Control Architectures.

Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A. Smith. 2014. A Dependency Parser for
Tweets.

Geert-Jan Kruijff, Pierre Lison, Trevor Benjamin, Hen-
rik Jacobsson, and N. Hawes. 2007. Incremental,
multi-level processing for comprehending situated
dialogue in human-robot interaction. In Language
and Robots: Proceedings from the Symposium (Lan-
gRo’2007). University of Aveiro, 12.

Jonathan May. 2016. SemEval-2016 Task 8: Meaning
Representation Parsing.

Marjorie McShane and Sergei Nirenburg. 2016. Deci-
sions for semantic analysis in cognitive systems.

Marjorie McShane, Sergei Nirenburg, and Stephen
Beale. 2016. Language understanding with onto-
logical semantics.

Andreas Peldszus and David Schlangen. 2012. In-
cremental Construction of Robust but Deep Seman-
tic Representations for Use in Responsive Dialogue
Systems. In Eva Hajiov, editor, Proceedings of the
Coling Workshop on Advances in Discourse Analy-
sis and its Computational Aspects.

Brian Roark, Mary Harper, Eugene Charniak, Bonnie
Dorr, Mark Johnson, Jeremy G. Kahn, Yang Liu,
Mari Ostendorf, John Hale, Anna Krasnyanskaya,
Matthew Lease, Izhak Shafran, Matthew Snover,
Robin Stewart, and Lisa Yung. 2006. Sparseval:
Evaluation metrics for parsing speech.

Gabriel Skantze and David Schlangen. 2009. Incre-
mental dialogue processing in a micro-domain. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, EACL ’09, pages 745–753, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Mark Steedman and Jason Baldridge, 2011. Combina-
tory Categorial Grammar, pages 181–224. Wiley-
Blackwell.

Andrew Viterbi. 1967. Error bounds for convolutional
codes and an asymptotically optimum decoding al-
gorithm. IEEE Transactions on Information Theory,
13(2):260–269, April.

Jason Williams, Antoine Raux, Deepak Ramach, and
Alan Black. 2013. The dialog state tracking chal-
lenge. In In Proceedings of the 14th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue SIGDIAL.

Junsheng Zhou, Feiyu Xu, Hans Uszkoreit, Weiguang
Qu, Ran Li, and Yanhui Gu. 2016. AMR parsing
with an incremental joint model. In EMNLP.

119


	Fast Forward Through Opportunistic Incremental Meaning Representation Construction

