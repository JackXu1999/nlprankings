



















































State Gradients for RNN Memory Analysis


Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 344–346
Brussels, Belgium, November 1, 2018. c©2018 Association for Computational Linguistics

344

State Gradients for RNN Memory Analysis

Lyan Verwimp, Hugo Van hamme, Vincent Renkens, Patrick Wambacq
ESAT – PSI, KU Leuven
Kasteelpark Arenberg 10
3001 Heverlee, Belgium

{firstname}.{lastname}@kuleuven.be

Abstract

We present a framework for analyzing what
the state in RNNs remembers from its input
embeddings. We compute the gradients of the
states with respect to the input embeddings
and decompose the gradient matrix with Sin-
gular Value Decomposition to analyze which
directions in the embedding space are best
transferred to the hidden state space, charac-
terized by the largest singular values. We ap-
ply our approach to LSTM language models
and investigate to what extent and for how
long certain classes of words are remembered
on average for a certain corpus. Additionally,
the extent to which a specific property or re-
lationship is remembered by the RNN can be
tracked by comparing a vector characterizing
that property with the direction(s) in embed-
ding space that are best preserved in hidden
state space.

1 Introduction

Recurrent neural networks (RNNs) are the cur-
rent state of the art in many speech and language
technology applications, but they are often called
‘black-box’ models since it is hard for humans
to interpret what exactly the network has learned.
We present a framework to investigate what the
states of RNNs remember from their input and
for how long. We apply our approach to the cur-
rent state of the art in language modeling, long
short-term memory (Hochreiter and Schmidhuber,
1997) (LSTM) LMs (Sundermeyer et al., 2012),
but it can be applied to other types of RNNs too
and to other models with continuous word repre-
sentations as input.

2 Average memory of the RNN

Our framework is inspired by backpropagation,
but instead of computing the gradient of the loss,
we compute the gradient of the state with respect

to the input embedding, the ‘state gradient’, to
capture the influence of the input on the state. To
examine how long input words are remembered by
the RNN, we calculate the gradient with a certain
delay – with respect to the input word embedding
a few time steps earlier. The gradient matrix Ḡτ
(averaged over all time steps), where τ is a certain
delay, is decomposed with Singular Value Decom-
position (SVD):

Ḡτ = U Σ V
T = σ1 u1 v

T
1 +σ2 u2 v

T
2 +. . . (1)

We can interpret V as directions in the embed-
ding space, Σ as the extent to which the directions
in the embedding space can be found in the hid-
den state space and U as corresponding directions
in the hidden state space. Hence, the directions
with the largest singular values (SVs) (lowest in-
dex) are directions in embedding space that are
best remembered by the RNN.

In order to investigate how well the RNN re-
members on a corpus level, we can track the
largest SV or the sum of all SVs with respect to
the delay τ . For an LM trained on Penn Treebank,
we observe an exponential decay of the SVs with
respect to the delay: much of the information that
is present in the cell state about a specific word is
quickly forgotten. However, on average, some in-
formation is still remembered even after process-
ing more than 20 words. The ratio of the largest
SV with respect to the sum of all SVs becomes
larger as the delay increases, indicating that the
memory becomes more selective.

We can also compare the SVs based on gra-
dient matrices averaged over specific classes of
words or individual words. We observe for exam-
ple that pronouns have a larger effect on the cell
state than other parts-of-speech for a delay of 0,
which makes sense because they determine which
verb conjugation should follow.



345

3 Tracking a specific property

We can also track whether a specific relationship
encoded in the input embedding is remembered by
the RNN. It has been shown that relationships be-
tween word embeddings can be characterized as
vector offsets (Mikolov et al., 2013). We compare
a vector characterizing a specific property to the
directions in the embedding space that are best re-
membered (the directions in VT corresponding to
the largest SVs), to see if and how well the prop-
erty is remembered in the hidden state.

Firstly, we define a specific property as the dif-
ference between the averaged embeddings for the
classes separated by that property:

da−b = ēa − ēb (2)

where ēa and ēb are the result of averaging all em-
beddings of words belonging to classes a and b
respectively. In order to check whether this defi-
nition makes sense for a specific property, we first
test whether the embeddings of the two classes are
linearly separable by training a linear classifier.

We propose two methods to investigate the ex-
tent to which a property is remembered. Firstly,
we can compare d with Hn, which is the sub-
space of the embedding space spanned by the di-
rections that are best remembered, the n largest
right-singular vectors. To be able to do this, we
make the orthogonal projection of d onHn:

y = projHn d = Vn V
T
n d (3)

where Vn is the matrix containing the n first
columns of V. Assuming d is normalized to unit
length, we can calculate the cosine similarity be-
tween y and d as follows:

cos(d,y) =
dT Vn V

T
n d

‖d‖ ‖Vn VTn d‖
=
∥∥VTn d∥∥ (4)

The cosine similarity between d andHn is a mea-
sure of how close d is to the top n directions that
are best remembered in the RNN state.

A second option is comparing d with the direc-
tion in embedding space that is best remembered.
To do this, we multiply d with the average gradi-
ent matrix:

r =
∥∥Ḡτ × d∥∥ (5)

If d would be the embedding direction that is best
remembered in the state, then it would be equal to

0 5 10 15 20 25 30
0

0.2

0.4

0.6

0.8

1

delay τ

m
/c
o
s(
d
,H

5
)

Figure 1: m (full lines) and cos(d,H5) (dotted lines)
for sg-pl (blue) and common-proper (green) nouns with
respect to the delay for a PTB LM. Gray lines: σ1 /

∑
σ

(full) and
∑5

n=1 σn /
∑
σ (dotted).

v1 and r would be equal to σ1. Hence, in order to
get a relative measure of how well the difference
between two classes is remembered, we compare r
with σ1 and obtain a ‘extent to which the property
is remembered, relative to the property that is best
remembered’, or the ‘relative memory’ m:

m =
r

σ1
(6)

In Figure 1, we plot m and cos(d,H5) for
the properties singular-plural (sg-pl) noun and
common-proper (cm-pr) noun. Prior experiments
with a linear classifier showed that these properties
can be characterized as a difference vector. Ac-
cording to both measures the sg-pl distinction is
slightly better remembered for a delay of 0, while
for the other delays the cm-pr distinction is better
remembered. In all plots, there is a sharp decrease
after a delay of 1 or 2, indicating that the proper-
ties seem mostly important on the short term. We
also plot the ratio of σ1 and the sum of the 5 largest
SVs with respect to the sum of all SVs (gray lines).
Notice that if τ increases, the ratio increases too,
which confirms our observation in section 2 that
the memory becomes more selective over time.

4 Conclusion

We analyze the memory of an RNN by comput-
ing the gradients of its state with respect to its in-
put. The state gradient matrix is decomposed with
SVD, and the resulting singular values and direc-
tions with the highest singular values are inspected
to investigate for how long and how well the RNN
remembers its input.



346

References
Sepp Hochreiter and Jürgen Schmidhuber. 1997.

Long short-term memory. Neural Computation,
9(8):1735–1780.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL), pages 746–751.

Martin Sundermeyer, Ralf Schlüter, and Hermann Ney.
2012. LSTM Neural Networks for Language Mod-
eling. In INTERSPEECH, pages 1724–1734.


