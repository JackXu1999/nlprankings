



















































Event Ordering with a Generalized Model for Sieve Prediction Ranking


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 843–853,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Event Ordering with a Generalized Model for Sieve Prediction Ranking

Bill McDowell
The Pennsylvania State University
forkunited@gmail.com

Nathanael Chambers
United States Naval Academy
nchamber@usna.edu

Alexander G. Ororbia II
The Pennsylvania State University

ago109@psu.edu

David Reitter
The Pennsylvania State University

reitter@psu.edu

Abstract

This paper improves on several aspects of
a sieve-based event ordering architecture,
CAEVO (Chambers et al., 2014), which
creates globally consistent temporal rela-
tions between events and time expressions.
First, we examine the usage of word em-
beddings and semantic role features. With
the incorporation of these new features, we
demonstrate a 5% relative F1 gain over our
replicated version of CAEVO. Second, we
reformulate the architecture’s sieve-based
inference algorithm as a prediction rerank-
ing method that approximately optimizes a
scoring function computed using classifier
precisions. Within this prediction rerank-
ing framework, we propose an alternative
scoring function, showing an 8.8% relative
gain over the original CAEVO. We further
include an in-depth analysis of one of the
main datasets that is used to evaluate tem-
poral classifiers, and we show that in spite
of the density of this corpus, there is still
a danger of overfitting. While this paper
focuses on temporal ordering, its results
are applicable to other areas that use sieve-
based architectures.

1 Introduction

Narratives that describe a series of events rarely do
so in order. Basic rules of journalism dictate that
important information leads a news report, and ac-
cordingly, algorithms that re-order events chrono-
logically need to combine a wealth of contextual,
rhetorical, and commonsense information.

Most research on event ordering aims to pro-
duce only partial orderings of event mentions

and time expressions (Bethard and Martin, 2007;
Cheng et al., 2007; UzZaman and Allen, 2010;
Llorens et al., 2010; Bethard, 2013). In the past,
labeled corpora used for training and evaluation
contained only small subsets of pairs of events
and times. Some of these corpora, like Time-
bank, have annotations restricted to salient, eas-
ily labeled pairs within the same document. Other
more recent data sets contain annotations that form
timelines of events that involve common entities
(Pustejovsky et al., 2003; Minard et al., 2015).
Due to the lack of consistency of annotations
across event pairs, it is difficult to use these cor-
pora in accurately measuring the practical perfor-
mance of event ordering algorithms.

Richer datasets are becoming available that pro-
vide more complete event orderings which in-
clude logically implied relations that are less ev-
ident from local text features. In particular, the
TimeBank-Dense corpus provides a significantly
more dense and complete set of annotations, al-
lowing for the evaluation of methods that make
use of broad contextual information across many
event pairs (Cassidy et al., 2014). One method that
has been developed to leverage such information
is CAEVO—a sieve-based architecture that made
the first effort toward dense event ordering (Cham-
bers et al., 2014). This method maintains tran-
sitivity constraints across independent predictions
from several specialized classifiers. More specifi-
cally, the architecture runs a series of “sieve” clas-
sifiers with their predictions ranked in order by
precision using a held-out dataset. The higher pre-
cision classifiers are ranked more highly in the se-
ries, and predictions are expanded by transitivity
rules (e.g. if event e1 is before e2, and e2 is be-
fore e3, then e1 is before e3) after each individual
classifier generates its predictions. The high den-

843



sity of the constructed prediction graph allows the
transitivity rules to generate accurate predictions
for links that would otherwise be difficult to pre-
dict from the text.

This paper proposes improvements to CAEVO
with respect to (1) feature engineering within ma-
chine learned sieves, (2) generalization of the
sieve-based architecture to facilitate higher per-
forming sieve prediction rerankings, and (3) the
leveraging of unlabeled data. First, for our fea-
ture engineering improvements, we are motivated
by the fact that TimeBank-Dense contains a rel-
atively small training sample, and so we extend
the feature sets for the architecture’s machine
learned classifiers to include features that encode
lexical information about events in relatively low
dimensional spaces based on word embeddings
(Mikolov et al., 2013) and semantic role labeling
(SRL) annotations (Gildea and Jurafsky, 2002).
Second, our generalization of the sieve-based ar-
chitecture allows us to experiment with alterna-
tive methods for establishing the precedence rank-
ing of sieve predictions. Furthermore, we iden-
tify an approximate upper bound on any rank-
ing method’s performance. Lastly, in our exper-
iments with unlabeled data, we analyze the effect
of changing the density of the architecture’s pre-
diction graph. Our hypothesis is that increasing
the number of predictions on unlabeled data will
increase performance on labeled data through the
application of CAEVO’s transitivity constraints.

Our extensions produce new state-of-the-art re-
sults on the original test split of TimeBank-Dense
(8.8% F1 increase). Beyond this, we describe al-
ternative evaluations on other splits of the data in
order to analyze the effect of the common small
sizes of temporal corpora like TimeBank-Dense.
This analysis is critical for future work in tempo-
ral ordering, and sheds further light on previous
work’s results.

2 Related Work

Early work on event ordering focused on develop-
ing machine-learned classifiers that label the tem-
poral relations between small subsets of pairs of
events within documents using lexical and syntac-
tical features (Bethard and Martin, 2007; Cheng
et al., 2007; UzZaman and Allen, 2010; Llorens
et al., 2010; Bethard, 2013). Later work lever-
aged information across pairwise predictions by
imposing transitivity constraints using techniques

like integer linear programming and Markov logic
networks (Bramsen et al., 2006; Chambers and Ju-
rafsky, 2008; Tatu and Srikanth, 2008; Yoshikawa
et al., 2009). CAEVO followed these and other
hybrid rule-based approaches (D’Souza and Ng,
2013), but with the transitivity constraints yielding
larger gains in performance for the more complete
temporal graph constructed on the TimeBank-
Dense corpus (Cassidy et al., 2014; Chambers
et al., 2014).

The TimeBank-Dense corpus provides a signifi-
cantly more dense and complete set of annotations
compared to previous corpora.1 TimeBank-Dense
extends a subset of the original TimeBank cor-
pus with annotations for (almost) all event-time,
time-time, and event-time pairs across consecu-
tive sentences, as well as relations to the doc-
ument creation time. This dense corpus facili-
tated the evaluation of CAEVO—a sieve-based ar-
chitecture which maintains transitivity constraints
across independent predictions from several spe-
cialized classifiers.

Recent work has focused on the construction
of timelines of related events, using SRL an-
notations to determine which events are related
through common actors (Laparra et al., 2015). In
addition, other work has outperformed the orig-
inal CAEVO with a 2.2% relative F1 gain on
TimeBank-Dense using word embedding features
within a stacked ensemble of event-event, event-
time, and event-creation-time logistic regression
classifiers (Mirza and Tonelli, 2016). We draw
inspiration from this recent work by incorporat-
ing SRL and word embedding features into the
machine-learned CAEVO sieves.

The CAEVO architecture is itself inspired by
the sieve-based architectures that have been suc-
cessfully applied to event and entity coreference
as well as spatial relation extraction tasks (Lee
et al., 2012, 2013; D’Souza and Ng, 2015). Years
since CAEVO’s introduction, a coreference sieve
architecture still achieves top performance (Lee
et al., 2017). The key idea behind these archi-
tectures is to combine information from several
classifiers by assigning precedence to predictions
according to the reliability of the classifier from
which they originate. A precision-ranked series of

1The new corpus is the result of several TempEval com-
petitions (Verhagen et al., 2007, 2010; UzZaman et al., 2012)
which prompted efforts to develop more complete event or-
dering annotations (Bramsen et al., 2006; Kolomiyets et al.,
2012; Do et al., 2012).

844



“sieve” classifiers generate predictions, and pre-
dictions from the more reliable sieves earlier in the
series inform the predictions of the less reliable
sieves later in the series. Generally, the predic-
tions from a highly-ranked sieve can inform a low-
ranked sieve in several ways, but within CAEVO,
predictions from early classifiers are coupled via
transitive inference rules to generate an expanding
set of predictions that override output from less re-
liable classifiers later on in the series. In the next
section, we describe a more generic view of this
architecture which will motivate alternative meth-
ods for assigning precedence to predictions from
the collection of classifiers.

3 Generalizing Sieve Architectures

Sieve architectures are used in many areas such as
entity coreference, relation extraction, and tempo-
ral ordering. A core contribution of this paper is a
generalization of how these models score and rank
their decisions. Like other sieve models, CAEVO
uses a precision-ranked series of classifiers (i.e.
“sieves”) coupled with transitivity constraints to
provide a solution to the event ordering task. How-
ever, prior work has not investigated alternatives to
the coarse-grained precision-based rankings pro-
vided by the architecture. This section gives a
generalized formal view on this precision-ranked
setup, and Section 4.3.1 describes our experiments
with new alternatives to traditional sieve archi-
tectures that are available within our generalized
view.

Informally speaking, a sieve architecture ap-
plies a sequence of classifiers that each make
their own independent labeling decisions, and the
architecture resolves conflicts between these de-
cisions by assigning precedence to those which
have higher estimated precision. The architecture
estimates the precision for each sieve on a de-
velopment set of data, and associates all predic-
tions from a given sieve with this precision esti-
mate. These precision scores determine an over-
all ranking to predictions within the final system.
When labeling a new test document, the archi-
tecture chooses predictions from all higher preci-
sion sieves over predictions from lower precision
sieves. But this common ranking of predictions is
coarse-grained, so this paper proposes other ways
of ordering the classifier predictions. Figure 1 il-
lustrates the difference between prediction rank-
ings from traditional sieve architectures and our

P=.82

P=.75

P=.72

a b c

Figure 1: Sieve classifier decisions as ranked in a
sieve architecture: (a) three sieves with their preci-
sions, (b) each sieve’s decisions ranked as in a tra-
ditional system, (c) a potential ranking influenced
by precision, but not strictly bound to it.

alternatives. The middle column shows the strict
prediction ordering given by traditional sieve sys-
tems, but the fuzzy ordering in the right column is
possible within our proposed alternative architec-
tures. Section 4.3.1 explores this in depth.

We now formally define a typical sieve architec-
ture (in terms of the temporal ordering domain).
Consider the set of event mentions E, time
expressions T , and temporal relation types L =
{BEFORE, AFTER, INCLUDES, INCLUDED, SIMULTANEOUS, VAGUE}.
We desire an architecture that encodes functions
fee : E × E → L, fet : E × T → L, and
ftt : T × T → L which accurately classify
relations between event-event, event-time, and
time-time pairs, respectively. The gold-standard
annotations within our corpora are logically
consistent, so we can assume that the true event
orderings induced by the functions fee, fet, and
ftt conform to the transitivity constraints given in
Table 1.

Algorithm 1 depicts a generalized view of the
CAEVO architecture which encodes approxima-
tions to the desired fee, fet, and ftt labeling func-
tions (and this view also applies to other typical
sieve systems). The algorithm combines predic-
tions from a set of sieve classifiers F̂ that pro-
vide partial approximations to fee, fet, and ftt
within restricted syntactic contexts. As described
by Chambers et al. (2014), F̂ contains both rule
based and machine-learned classifiers. In this pa-

845



Constraints
BEFORE(o1, o2), BEFORE(o2, o3)→ BEFORE(o1, o3)

BEFORE(o1, o2), INCLUDES(o2, o3)→ BEFORE(o1, o3)
BEFORE(o1, o2), SIMULTAN(o2, o3)→ BEFORE(o1, o3)
INCLUDED(o1, o2), BEFORE(o2, o3)→ BEFORE(o1, o3)

INCLUDED(o1, o2), INCLUDED(o2, o3)→ INCLUDED(o1, o3)
INCLUDED(o1, o2), SIMULTAN(o2, o3)→ INCLUDED(o1, o3)

INCLUDED(o1, o2), AFTER(o2, o3)→ AFTER(o1, o3)
INCLUDES(o1, o2), INCLUDES(o2, o3)→ INCLUDES(o1, o3)
INCLUDES(o1, o2), SIMULTAN(o2, o3)→ INCLUDES(o1, o3)

SIMULTAN(o1, o2), BEFORE(o2, o3)→ BEFORE(o1, o3)
SIMULTAN(o1, o2), INCLUDED(o2, o3)→ INCLUDED(o1, o3)
SIMULTAN(o1, o2), INCLUDES(o2, o3)→ INCLUDES(o1, o3)
SIMULTAN(o1, o2), SIMULTAN(o2, o3)→ SIMULTAN(o1, o3)

SIMULTAN(o1, o2), AFTER(o2, o3)→ AFTER(o1, o3)
AFTER(o1, o2), INCLUDES(o2, o3)→ AFTER(o1, o3)
AFTER(o1, o2), SIMULTAN(o2, o3)→ AFTER(o1, o3)

AFTER(o1, o2), AFTER(o2, o3)→ AFTER(o1, o3)
BEFORE(o1, o2)→ AFTER(o2, o1)
AFTER(o1, o2)→ BEFORE(o2, o1)

INCLUDES(o1, o2)→ INCLUDED(o2, o1)
INCLUDED(o1, o2)→ INCLUDES(o2, o1)
SIMULTAN(o1, o2)→ SIMULTAN(o2, o1)

VAGUE(o1, o2)→ VAGUE(o2, o1)

Table 1: Transitivity and symmetry constraints
in C from Equation 1 and Algorithm 1. In this
list, every constraint applies to events and/or times
o1, o2 and o3. We abbreviate “SIMULTANEOUS”
with “SIMULTAN” due to space constraints.

per, our experiments focus on the machine learned
sieves that give within-sentence event-event pre-
dictions (EEWS), within-sentence event-time pre-
dictions (ETWS), within-syntactic dominance re-
lation event-event predictions (EED), and event to
document creation time relations (EDCT).

Given a set of unlabeled data points D ⊆ (E ∪
T )× (E ∪T ), Algorithm 1 uses the sieves in F̂ to
construct a set of predictions F̂D = {(d, f̂(d), f̂) |
d ∈ D, f̂ ∈ F̂} where each prediction is indexed
with its associated sieve f̂ . The algorithm then
sorts and partitions F̂D according to a prediction
scoring function s : (D × L × F̂ ) → R. Finally,
the returned set of predictions R is constructed by
iteratively adding predictions from F̂D in descend-
ing order (with respect to s) while applying con-
straints C. C consists of the transitive rules (de-
picted in Table 1) along with the constraint that
prior predictions in R cannot be overwritten by

later predictions. The rules C are applied at each
iteration by extending the current predictions with
those implied by transitivity.

Algorithm 1 Sieve Inference
1: function SIEVEINFERENCE
2: Input F̂ := learned and rule-based sieves
3: Input D := data to classify
4: Input s := prediction scoring function
5: Input C := constraint application function
6: F̂D ← {(d, f̂(d), f̂) | d ∈ D, f̂ ∈ F̂}
7: P ← F̂D sorted and partitioned by s
8: R← {}
9: for i := 1 to |P | do

10: R← C(Pi ∪R)
return R

One of the weaknesses of CAEVO (and other
sieve-based systems) addressed in this paper is
that its scoring function, s(d, f̂(d), f̂), is simply
the precision of f̂ as measured on held-out data.
All predictions made by f̂ must have the same
ranking score (see Figure 1 again). This coarse-
ranking is likely to be sub-optimal relative to rank-
ings based on other scoring functions.

We can motivate improvements to Algorithm 1
by viewing it as a greedy approximation to the op-
timization problem which chooses a set of scored
predictions according to:

R = arg max
S⊆F̂D

(∑
p∈S

s(p)
)

subject to C (1)

Given the view that CAEVO is providing a so-
lution to the objective in Equation 1 using Algo-
rithm 1, it is straightforward to see possible direc-
tions for improvement. Namely, the architecture
can be improved through changes to the sieves F̂ ,
the scoring methods s, the constraints C, the data
D, and the underlying greedy approximation algo-
rithm. Intuitively, if we want Equation 1 to give a
highly accurate set of predictions, then we should
pick an F̂ to contain more accurate classifiers 2,
an s which ranks correct predictions above incor-
rect predictions, and a large set D which enables C
to propagate precise labels from easy-to-classify
data samples onto hard-to-classify data samples.
Notably, CAEVO’s rigid choice of scoring func-
tion s to be the precision of f̂ only allows s to
give a coarse-grained scoring, which can score in-
correct predictions higher than correct predictions.

2This includes possibility that one or more f̂ ∈ F̂ could
be parameterized by more complex function approximators,
such as neural networks (LeCun et al., 2015).

846



Furthermore, CAEVO’s sieve-inference in Algo-
rithm 1 is greedy, and other methods like inte-
ger linear programming (ILP) which provide bet-
ter solutions to Equation 1 might yield more accu-
rate predictions. Lastly, CAEVO limits D to only
contain labeled evaluation data without taking ad-
vantage of constraints imposed across unlabeled
event pairs. These observations motivate several
of the extensions we describe and experiment with
below.

4 Models and Experiments

In our experiments, we replicate CAEVO, add new
features to the sieves, modify the scoring func-
tion, and include larger amounts of related unla-
beled data to further constrain the predictions. Un-
less otherwise noted, results are computed using
the original train-dev-test split of the TimeBank-
Dense and original CAEVO experiments (Cham-
bers et al., 2014; Cassidy et al., 2014).

4.1 Replication

We replicate the CAEVO architecture within a
more generic framework with the aim of substan-
tiating and extending the CAEVO results from
Chambers et al. (2014). The replication process al-
lows us to validate the robustness of the originally
published results while determining their sensitiv-
ity to various parameter settings.

We reconstruct features within an alternative
feature engineering pipeline, and ensure that the
feature matrices match those from the original sys-
tem. During this process, we observed two issues
in the original system. First, features based on
gold-standard event “tense”, “aspect”, and “class”
were not included in the machine-learned mod-
els that produced the reported results even though
they were described in the original paper (they ap-
pear to have been inadvertently configured off). In
light of this, we leave these features out of our
replicated architecture, but add them into the re-
vised architecture in our feature engineering ex-
periments. Second, the EEWS, EED, and ETWS
sieves in CAEVO used a minimum feature oc-
currence cutoff of 2 across training data whereas
EDCT used a cutoff of 1. We experiment with dif-
ferent settings of these values in the next section.
Other minor bugs and the details of replication are
described in the appendix.

The R column of Table 2 gives micro-averaged

accuracies3 for the four machine-learned sieves
and the full replicated architecture. These accu-
racies reproduce the original CAEVO results up to
less than 1% discrepancy in accuracy due to ver-
sion differences between the machine-learning li-
braries and minor bugs in the original system. 4

4.2 Rich Feature Engineering
We extend our CAEVO replication with additional
knowledge of event attributes, word embeddings,
and SRL labels for each of the machine learned
sieves.

4.2.1 Event Attributes
As noted above, the original CAEVO paper had
reported the use of gold-standard TimeML tense,
aspect, class, polarity, and modality event attribute
features, but close inspection of the architecture
suggests that these features had been left out when
computing the final results. We experimented with
adding features computed from these attributes
into each of the machine-learned classifiers. For
each event in a given event-event or event-time
pair, we extend the feature vector with indicators
for possible values of each event attribute. Also,
for each event-event pair, we extend the feature
vector with indicators of whether the event at-
tributes are equal for the source and target (e.g.
equal tense), as well as features representing the
conjunction of each attribute across source and tar-
get (e.g. for the tense attribute, one of the indica-
tors is PAST-FUTURE, which is for a pair contain-
ing a past tensed event and a future tensed event).

The F1 scores computed on the TimeBank-
Dense test-set with the additional event attribute
features are given in the Ev column of Ta-
ble 2. Each machine-learned sieve increases in
F1, but the overall architecture decreases slightly.
This highlights the non-monotonic relationship
between the performance of individual sieves and
the performance of the overall architecture.

4.2.2 Semantic Role Labeling
We compute additional features from annota-
tions generated using the mate-tools SRL system
(Björkelund et al., 2009). Specifically, for a given
pair, we compute features representing SRL pred-
icates of the events as well as their conjunction.

3The micro-averaged accuracies are equivalent to micro-
averaged F1 scores computed on data for which some label is
output by a classifier.

4The results for the rule-based sieves are not shown, but
they match the original system exactly.

847



Sieve R Ev SRL W2V R+ F+
EDCT .524 .547 .511 .524 .553 .553
ETWS .414 .450 .414 .450 .443 .480
EEWS .442 .466 .424 .450 .450 .456
EED .428 .500 .435 .473 .488 .466
Full .502 .495 .493 .504 .520 .527

Table 2: Micro-averaged accuracies on the
TimeBank-Dense test-set for machine-learned
sieves and the fully replicated architecture with
various feature extensions. Results are given for
our baseline CAEVO replication (R) and exten-
sions with gold-standard event attribute features
(Ev), SRL features (SRL), word embedding fea-
tures (W2V), all new features (R+), and all new
features with new feature count cutoffs (F+).

Also, we compute the shortest path between a pair
within the undirected graph formed by the SRL
predicates and arguments (i.e. where there are
nodes for predicate spans and argument spans, and
there is an edge between two spans if one is the ar-
gument of the other). As shown in Table 2 under
the SRL column, these features only give a minor
improvement in micro-averaged F1 for the EED
sieve, but hurt performance of the other sieves and
the architecture on the TimeBank-Dense test set.
However, we believe this may be due to over-
fitting, as we observe 3% and 4% gains for the
ETWS and EED sieves with these features on the
development set.

4.2.3 Word Embeddings
Given that recent work has shown improvements
using word embeddings (using log-linear neural
language models such as the Skip-Gram architec-
ture), we extend feature vectors with the word
vectors representing events and their similarity.
Following Mirza and Tonelli (2016), we use the
three million 300-dimensional word2vec vectors
5 pre-trained on part of the Google News dataset
(Mikolov et al., 2013). For each token span cor-
responding to either an event mention or time ex-
pression in a given pair datum, we extend the fea-
ture vector with normalized sums of word vectors
computed from tokens of the span. In addition, we
include the cosine similarity between the vectors
for the events in a pair, as well as a vector rep-
resenting the normalized difference between the
pair’s vectors. Micro-averaged F1 scores on the
TimeBank-Dense test set with these word embed-

5Pre-trained word vectors can be retrieved from https:
//code.google.com/archive/p/word2vec/.

ding features are given in the W2V column of Ta-
ble 2. The ETWS and EED sieves show improve-
ments of more than 3%, and the EEWS shows a
gain of about 1%. However, the F1 score for the
overall architecture remains nearly the same.

4.2.4 Full Extension
We extend the machine learned sieves with the full
set of event attributes, SRL, and word embedding
features as described above. As shown under the
R+ column of Table 2, this yields a 2% gain in
micro-averaged F1 for the overall architecture as
well as gains for each individual sieve. Also, Sec-
tion 4.1 mentioned that the feature count cutoffs
in R and R+ are set to 1 for EDCT and 2 for all
remaining machine-learned sieves. For simplic-
ity, we set the cutoff to 1 across all sieves in F+,
yielding a 4% improvement in ETWS and minor
gains in EEWS and the full system over R+. Over-
all, our feature engineering efforts give F+ a 5%
relative gain (2.5% absolute) over the replicated
CAEVO architecture (R).

4.3 Modifying Sieve Inference
This section proposes new inference methods for
sieve architectures by varying the scoring func-
tion s and adding unlabeled data to D from Al-
gorithm 1 and Equation 1 in Section 3. This is a
core contribution that can benefit not just tempo-
ral ordering, but also other sieve systems applied
to other NLP tasks.

4.3.1 Alternative Scoring Methods
In the original CAEVO architecture’s implemen-
tation of Algorithm 1 from Section 3, the score
s(d, f̂(d), f̂) is computed as the precision of the
sieve f̂ on the development set. This greedy scorer
s gives a coarse-grain ranking of sieve predictions,
assigning equal precedence to all predictions from
a given sieve f̂ . Intuitively, if we want to produce
a higher accuracy architecture, then we should
adjust the scoring function s to score all correct
predictions more highly than all incorrect predic-
tions6. CAEVO’s use of f̂ precision in comput-
ing s is a coarse-grained heuristic in line with this
goal, but there are better choices.

Ideal Scorer In the best case, the F+* column
of Table 3 shows the micro-averaged F1 (equiv-
alent to accuracy) when s scores a prediction as

6Note that while such a choice of s should produce good
performance, this performance is not necessarily optimal un-
der the transitivity constraints.

848



Data V CAEVO (R) F+ F+L F+S F+LU F+*
Dev .378 .481 .485 .490 .481 .491 .585
Test .403 .502 .527 .546 .521 .541 .642

Table 3: Micro-averaged F1 scores on the original TimeBank-Dense train-dev-test split for several
versions of the sieve architecture. Results are given for the VAGUE majority baseline (V), the CAEVO
replication (R), the architecture with the extended feature set (F+), and varying inference methods un-
der the extended feature set. The varying inference methods include an alternative prediction scoring
function s computed by precision of each sieve on each relation label (F+L), s computed by precision
multiplied by classifier probability estimates (F+S), s computed by precision on each label with extra
unlabeled data (F+LU), and s computed to produce near-optimal ordering (F+*).

s(d, f̂(d), f̂) = 1 if f̂(d) is the correct label
for datum d, and 0 otherwise. This near-optimal
choice of s in F+* gives a 10% gain over F+,
suggesting a large room for improvement by re-
ranking sieve predictions rather than improving
the accuracy of the individual sieves. This sug-
gests that architecture performance will increase
by improving the estimates of the prediction con-
fidence encoded by s, rather than improving the
predictions themselves.

New Scorers We thus consider several alterna-
tives for s. First, we attempted estimating s by
training a reranking logistic regression model to
predict whether f̂(d) is the correct label for d
within prediction (d, f̂(d), f̂). This approach did
not improve performance over other simpler ap-
proaches (possibly due to the small size of rerank-
ing training data), and so we only report results
for the simpler approaches. In one approach, mo-
tivated by the observation that precision varies
across relation labels, we compute s(d, f̂(d), f̂) as
the precision of f̂ for predictions with label f̂(d)
on the dev data. This sieve-label precision ap-
proach improves F1 over F+ as shown in the F+L
column of Table 3. In a second approach, we com-
pute s(d, f̂(d), f̂) as the precision of f̂ multiplied
by the probability assigned to f̂(d) by the logis-
tic regression model employed by f̂ . According to
the F+S column of Table 3, this approach does not
show improvement over F+.

4.3.2 Leveraging Unlabeled Data
CAEVO uses Algorithm 1 to draw inferences
about a data set D. In the original implementa-
tion, this set contained only the gold-standard la-
beled evaluation pairs within two sentence win-
dows. However, if D were expanded with other
unlabeled data points outside of two sentence win-
dows (for which it is easy to predict labels with

high precision), the transitivity constraints in C
might generate further high precision predictions
on the labeled data. Interestingly, this gives the ar-
chitecture the property that making a larger num-
ber of predictions on a logically connected set of
data can lead to higher overall performance on
subsets of that data. Given this observation, we ap-
ply the F+L version of the architecture to all pairs
of events and times within a document. The result-
ing F1 scores given this expansion of D with the
unlabeled TimeBank-Dense pairs are shown under
column F+LU of Table 3. Unfortunately, these
scores show no improvement over the scores un-
der column F+L which suggests that the architec-
ture did not draw high precision inferences from
the unlabeled data to labeled data. This may be
due to the lack of sieves tuned specifically to make
between-sentence unlabeled data predictions, or it
may be due to an inherent difficulty in making
these predictions over the labeled within-sentence
and consecutive sentence predictions.

5 Deep Dive into the Data

One of the difficulties facing the temporal order-
ing community is sparse data. This has been an
issue since the original TimeBank Corpus, and the
TimeBank-Dense Corpus had data expansion as
one of its core goals. However, we argue that
data sparsity is still a problem, and previous work
tends not to explore different test sets, potentially
misidentifying positive and negative results spe-
cific to particular splits of the data. This issue
seems especially relevant due to the small size of
the TimeBank-Dense data (only 5 documents in
dev and 9 documents in test for the original split
(Chambers et al., 2014)).

The underlying question is whether new re-
sults present a significant improvement upon older
ones. We consider multiple cross-validation splits

849



of the data to get some sense about the answer
to this question. We chose this approach over
null-hypothesis significance testing due to limi-
tations induced by small sample size in conjunc-
tion with the dependencies between predictions
arising through the transitivity constraints. These
two issues render it difficult to make a hard de-
termination of significance in a way that does not
violate hypothesis testing assumptions. Instead,
our cross-validation splits give a weak qualitative
sense of the generalizability of our methods.

Our cross validation setup consisted of four
splits of the 36 documents with 5 documents per
test set and 4 documents per development set for
each split. Table 4 shows the micro-averaged F1
scores of the architectures described in the pre-
vious sections on each of these splits. Unsur-
prisingly, the results show that some architecture
scores were boosted while others lessened on these
alternative splits. Notably, the added features in
F+ still make consistent gains over the CAEVO
replication R, and the ideal scorer F+* makes con-
sistent large gains (as high as 18% on Fold 3, and
a low of 6% on Fold 1). However, F+L performs
well on the development sets but does not im-
prove performance on the test sets. We hypoth-
esize that F+L overfits to the development sets
due to their small size and the small number of
predictions available to compute sieve-label preci-
sions. The consistently large gains of F+L on the
dev sets suggest that the method will achieve high
performance as long as the precision estimates in
s are accurate, but the method requires more de-
velopment data than F+ to compute accurate esti-
mates without overfitting due to the large number
of sparsely distributed sieve-label combinations.

This extra analysis helps to highlight the poten-
tial for overfitting. We hope this encourages future
work to bear this in mind, and to also present re-
sults across multiple tests. Extra evaluations like
those in this section are often unexciting, but we
argue that it is of utmost importance that they
are conducted. This paper could have ended at
the previous section’s top test set results, but we
hope the reader sees extra value in the deep analy-
sis of one’s results.

6 Discussion

In the above experiments, we successfully repli-
cated the CAEVO system from Chambers et al.
(2014), and then proposed a generalization of the

sieve-based architecture that enabled several new
extensions and improvements. With the injection
of new features, we improve the overall system
with a 5% relative gain in F1. Furthermore, our
generalized version of CAEVO’s sieve architec-
ture allowed us to score and rank predictions based
on both label and sieve precision, raising the F1
results to an 8.8% relative improvement over our
replicated CAEVO (under F+L in Table 3). We
consider these results a new state-of-the-art on the
TimeBank-Dense corpus. In addition, the large
gains using a near-optimal scoring function (un-
der F+* in Table 3) suggest that future work might
make substantial progress by building further al-
ternative prediction scoring methods.

We also perform an in-depth analysis of our
improvements on alternative splits of the data.
Through this analysis, we find that our feature
engineering results are robust. More interest-
ingly, while the F+L scoring method gives in-
creased F1 on the original TimeBank-Dense split
and all cross-validation dev sets, it does not yield
improved performance on our alternative cross-
validation test sets. This analysis suggests signifi-
cant improvement for F+L over the original archi-
tecture, but with possible overfitting label-specific
precision estimates on our small amount of devel-
opment data.

Finally, we presented the first experiments that
leveraged unlabeled data. These experiments gave
negative results, but we believe future research
might see improvements through inference over
unlabeled data by (1) improving the precision
of unlabeled data predictions (through the incor-
poration of precise between-sentence prediction
sieves), (2) increasing the density of the unlabeled
data (e.g. by including easy-to-predict cross-
document links between related events), (3) in-
creasing the number of constraints across the data
through the incorporation of sieves for additional
tasks like event and entity coreference (coref), or
(4) increasing the size of the data for more reliable
evaluation and training. With respect to (4), we
hypothesize that although Timebank-Dense con-
tains more temporal relations than other tempo-
ral corpora, it is still small in size. Our hope for
future work is to extend the data set with more
dense annotations, but spread across a larger num-
ber of document contexts, such that different scor-
ing and inference methods may be robustly trained
and evaluated.

850



Split Data V CAEVO (R) F+ F+L F+S F+LU F+*

Fold 0
Dev .385 .574 .571 .596 .593 .596 .676
Test .400 .503 .535 .534 .530 .534 .632

Fold 1
Dev .435 .519 .537 .592 .543 .592 .663
Test .312 .443 .503 .501 .506 .501 .558

Fold 2
Dev .450 .522 .542 .555 .521 .516 .684
Test .462 .528 .540 .507 .541 .506 .706

Fold 3
Dev .436 .536 .562 .575 .535 .573 .664
Test .470 .484 .497 .500 .500 .497 .682

Table 4: Micro-averaged F1 scores on four cross-fold validation train-dev-test splits of TimeBank-Dense
for the sieve architectures defined in Table 3. The new features in F+ make conistent gains across folds,
and the ideal scorer F+* demonstrates consistently large room for improvement using alternative scoring
methods. The F+L model still performs well on the dev sets, but it gives no performance gains on test
sets. This suggests the danger of overfitting the scoring functions s within sieve architectures, as the
sieve-label precision scores use in F+L were computed over a small, four document dev set in each fold.

In sum, we present a new state-of-the-art event
ordering model. Furthermore, we propose a gen-
eralized approach to classifier ranking that is ap-
plicable to all sieve architectures (not just tempo-
ral ordering). Instead of producing coarse-grained
ranking of classifier predictions, our proposal se-
lects more fine-grained, higher performing predic-
tion rerankings. In addition, we show temporal or-
dering gains using SRL and word embedding fea-
tures. The code for our event-ordering architec-
tures and experiments is publicly available7. We
hope that this work will encourage further efforts
in dense event ordering research.

Acknowledgments

We thank Jesse Dodge and Noah Smith for ad-
vising and support with preliminary experiments
that gave several negative results prior to the work
in this paper, helping to prune likely dead-end re-
search directions. We also thank the RTW group at
CMU for allowing us to use their library reposito-
ries. The work of authors BM and DR was funded
under NSF grant SES-1528409. NC was partially
supported by a grant from the Office of Naval Re-
search.

References
Steven Bethard. 2013. Cleartk-timeml: A minimalist

approach to tempeval 2013. In Second Joint Con-
ference on Lexical and Computational Semantics (*
SEM). volume 2, pages 10–14.

7https://github.com/forkunited/
CAEVO-plus

Steven Bethard and James H Martin. 2007. Cu-tmp:
Temporal relation classification using syntactic and
semantic features. In Proceedings of the 4th Inter-
national Workshop on Semantic Evaluations. Asso-
ciation for Computational Linguistics, pages 129–
132.

Anders Björkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning: Shared Task.
Association for Computational Linguistics, pages
43–48.

Philip Bramsen, Pawan Deshpande, Yoong Keok Lee,
and Regina Barzilay. 2006. Inducing temporal
graphs. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics,
pages 189–198.

Taylor Cassidy, Bill McDowell, Nathanel Chambers,
and Steven Bethard. 2014. An annotation frame-
work for dense event ordering. Technical report,
DTIC Document.

Nathanael Chambers, Taylor Cassidy, Bill McDowell,
and Steven Bethard. 2014. Dense event ordering
with a multi-pass architecture. Transactions of the
Association for Computational Linguistics 2:273–
284.

Nathanael Chambers and Dan Jurafsky. 2008. Jointly
combining implicit constraints improves temporal
ordering. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Association for Computational Linguistics, pages
698–706.

Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2007. Naist. japan: Temporal relation iden-
tification using dependency parsed tree. In Proceed-
ings of the 4th International Workshop on Seman-

851



tic Evaluations. Association for Computational Lin-
guistics, pages 245–248.

Quang Xuan Do, Wei Lu, and Dan Roth. 2012. Joint
inference for event timeline construction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning. Association
for Computational Linguistics, pages 677–687.

Jennifer D’Souza and Vincent Ng. 2013. Classifying
temporal relations with rich linguistic knowledge.
In HLT-NAACL. pages 918–927.

Jennifer D’Souza and Vincent Ng. 2015. Sieve-based
spatial relation extraction with expanding parse
trees. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing. pages 758–768.

Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics 28(3):245–288.

Oleksandr Kolomiyets, Steven Bethard, and Marie-
Francine Moens. 2012. Extracting narrative time-
lines as temporal dependency structures. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics: Long Papers-
Volume 1. Association for Computational Linguis-
tics, pages 88–97.

Egoitz Laparra, Itziar Aldabe, and German Rigau.
2015. Document level time-anchoring for timeline
extraction. Volume 2: Short Papers page 358.

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
2015. Deep learning. Nature 521(7553):436–444.

Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan
Jurafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics 39(4):885–916.

Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning. Asso-
ciation for Computational Linguistics, pages 489–
500.

Heeyoung Lee, Mihai Surdeanu, and Dan Jurafsky.
2017. A scaffolding approach to coreference reso-
lution integrating statistical and rule-based models.
Natural Language Engineering pages 1–30.

Hector Llorens, Estela Saquete, and Borja Navarro.
2010. Tipsem (english and spanish): Evaluating crfs
and semantic roles in tempeval-2. In Proceedings of
the 5th International Workshop on Semantic Eval-
uation. Association for Computational Linguistics,
pages 284–291.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .

Anne-Lyse Minard, Manuela Speranza, Eneko
Agirre, Itziar Aldabe, Marieke van Erp, Bernardo
Magnini, German Rigau, Ruben Urizar, and Fon-
dazione Bruno Kessler. 2015. Semeval-2015 task
4: Timeline: Cross-document event ordering. In
Proceedings of the 9th International Workshop
on Semantic Evaluation (SemEval 2015). pages
778–786.

Paramita Mirza and Sara Tonelli. 2016. On the con-
tribution of word embeddings to temporal relation
classification. In The 26th International Conference
on Computational Linguistics. pages 2818–2828.

James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al. 2003. The timebank corpus. In Corpus
linguistics. volume 2003, page 40.

Marta Tatu and Munirathnam Srikanth. 2008. Experi-
ments with reasoning for temporal relations between
events. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume 1.
Association for Computational Linguistics, pages
857–864.

Naushad UzZaman and James F Allen. 2010. Trips and
trios system for tempeval-2: Extracting temporal in-
formation from text. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation. Associ-
ation for Computational Linguistics, pages 276–283.

Naushad UzZaman, Hector Llorens, James Allen, Leon
Derczynski, Marc Verhagen, and James Pustejovsky.
2012. Tempeval-3: Evaluating events, time ex-
pressions, and temporal relations. arXiv preprint
arXiv:1206.5333 .

Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal
relation identification. In Proceedings of the 4th In-
ternational Workshop on Semantic Evaluations. As-
sociation for Computational Linguistics, pages 75–
80.

Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th international
workshop on semantic evaluation. Association for
Computational Linguistics, pages 57–62.

Katsumasa Yoshikawa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009. Jointly identi-
fying temporal relations with markov logic. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP: Volume 1-Volume 1. Association for Com-
putational Linguistics, pages 405–413.

852



A Replication Details

While replicating CAEVO, we did not find any
major issues that significantly change the results
reported in the original paper. However, we found
the following minor bugs: (1) bias features are in-
cluded in the EEWS, EED, and ETWS machine-
learned sieves but not in EDCT, (2) the computa-
tion of dependency path features does not always
compute the shortest paths, and (3) code that com-
putes token paths is specified to only compute for
paths with length less than 4, but does not do this
correctly. In our replicated version, we remove
each of these bugs.

We also noticed the following quirks in the orig-
inal system:

• Features based on gold-standard event tense,
aspect, and class were not included in the
machine-learned models that produced the
reported results even though they were de-
scribed in the original CAEVO paper.

• The EEWS, EED, and ETWS sieves were
trained using feature matrices with a mini-
mum feature occurrence count of 2 across
training data whereas EDCT has a minimum
feature occurrence count of 1. We know of
no motivation for setting this parameter dif-
ferently for the EDCT sieve, but resetting it
to 2 within EDCT drops its performance to
below the “All Vague” baseline sieve, result-
ing it from it being effectively removed from
the system, and yielding a 5% drop in perfor-
mance. The sensitivity of the overall system’s
performance to this parameter setting high-
lights the importance of using enough data to
acquire accurate precision estimates to deter-
mine the prediction scoring.

• The EED sieve had a lower precision estimate
than EEWS, but EEWS makes predictions on
a superset of the event pairs for which EED
makes predictions. This means that EED has
no functional relevance with respect to the
performance of the original architecture.

The replication process also revealed the sen-
sitivity of the results to the details of feature en-
gineering and feature selection. Overall, the pro-
cess confirmed that minor flaws and oddities will
likely remain in complicated architectures like
CAEVO after they have been documented, and it

can be worthwhile to repeatedly inspect and repli-
cate these systems to ensure that they function as
specified.

853


