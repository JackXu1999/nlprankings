



















































Identifying and Reducing Gender Bias in Word-Level Language Models


Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 7–15
Minneapolis, Minnesota, June 3 - 5, 2019. c©2017 Association for Computational Linguistics

7

Identifying and Reducing Gender Bias in Word-Level Language Models

Shikha Bordia1
sb6416@nyu.edu

Samuel R. Bowman1,2,3
bowman@nyu.edu

1Dept. of Computer Science
New York University

251 Mercer St
New York, NY 10012

2Center for Data Science
New York University

60 Fifth Avenue
New York, NY 10011

3Dept. of Linguistics
New York University
10 Washington Place
New York, NY 10003

Abstract

Many text corpora exhibit socially problematic
biases, which can be propagated or amplified
in the models trained on such data. For ex-
ample, doctor cooccurs more frequently with
male pronouns than female pronouns. In this
study we (i) propose a metric to measure gen-
der bias; (ii) measure bias in a text corpus and
the text generated from a recurrent neural net-
work language model trained on the text cor-
pus; (iii) propose a regularization loss term for
the language model that minimizes the pro-
jection of encoder-trained embeddings onto
an embedding subspace that encodes gender;
(iv) finally, evaluate efficacy of our proposed
method on reducing gender bias. We find this
regularization method to be effective in re-
ducing gender bias up to an optimal weight
assigned to the loss term, beyond which the
model becomes unstable as the perplexity in-
creases. We replicate this study on three train-
ing corpora—Penn Treebank, WikiText-2, and
CNN/Daily Mail—resulting in similar conclu-
sions.

1 Introduction

Dealing with discriminatory bias in training data
is a major issue concerning the mainstream im-
plementation of machine learning. Existing bi-
ases in data can be amplified by models and the
resulting output consumed by the public can in-
fluence them, encourage and reinforce harmful
stereotypes, or distort the truth. Automated sys-
tems that depend on these models can take prob-
lematic actions based on biased profiling of indi-
viduals. The National Institute for Standards and
Technology (NIST) evaluated several facial recog-
nition algorithms and found that they are systemat-
ically biased based on gender (Ngan and Grother,
2015). Algorithms performed worse on faces la-
beled as female than those labeled as male.

Models automating resume screening have also
proved to have a heavy gender bias favoring male
candidates (Lambrecht and Tucker, 2018). Such
data and algorithmic biases have become a grow-
ing concern. Evaluation and mitigation of biases
in data and models that use the data has been a
growing field of research in recent years.

One natural language understanding task vul-
nerable to gender bias is language modeling. The
task of language modeling has a number of prac-
tical applications, such as word prediction used in
onscreen keyboards. If possible, we would like
to identify the bias in the data used to train these
models and reduce its effect on model behavior.

Towards this pursuit, we aim to evaluate the ef-
fect of gender bias on word-level language models
that are trained on a text corpus. Our contributions
in this work include: (i) an analysis of the gen-
der bias exhibited by publicly available datasets
used in building state-of-the-art language models;
(ii) an analysis of the effect of this bias on re-
current neural networks (RNNs) based word-level
language models; (iii) a method for reducing bias
learned in these models; and (iv) an analysis of the
results of our method.

2 Related Work

A number of methods have been proposed for
evaluating and addressing biases that exist in
datasets and the models that use them. Recasens
et al. (2013) studies the neutral point of view
(NPOV) edit tags in the Wikipedia edit histories
to understand linguistic realization of bias. Ac-
cording to their study, bias can be broadly cat-
egorized into two classes: framing and episte-
mological. While the framing bias is more ex-
plicit, the epistemological bias is implicit and
subtle. Framing bias occurs when subjective or
one-sided words are used. For example, in the



8

Training Corpus

Generated TextCross Entropy Loss

T
raining W

ord L
evel L

anguage M
odel

Bias in Generated Text

Generated Text 
with 

Regularization

Cross Entropy Loss
+ λ(N.B)

T
raining W

ord L
evel L

anguage M
odel

Bias in Training Corpus
Bias after Regularization

Figure 1: Word level language model is a three layer LSTM model. λ controls the importance of minimizing bias
in the embedding matrix.

sentence—“Usually, smaller cottage-style houses
have been demolished to make way for these Mc-
Mansions.”, the word McMansions has a neg-
ative connotation towards large and pretentious
houses. Epistemological biases are entailed, as-
serted or hedged in the text. For example, in
the sentence—“Kuypers claimed that the main-
stream press in America tends to favor liberal
viewpoints,” the word claimed has a doubtful ef-
fect on Kuypers statement as opposed to stated
in the sentence—“Kuypers stated that the main-
stream press in America tends to favor liberal
viewpoints.” It may be possible to capture both of
these kinds of biases through the distributions of
co-occurrences. In this paper, we deal with iden-
tifying and reducing gender bias based on words
co-occurring in a context window.

Bolukbasi et al. (2016) propose an approach to
investigate gender bias present in popular word
embeddings, such as word2vec (Mikolov et al.,
2013). They construct a gender subspace using a
set of binary gender pairs. For words that are not
explicitly gendered, the component of the word
embeddings that project onto this subspace can be
removed to debias the embeddings in the gender
direction. They also propose a softer variation
that balances reconstruction of the original em-
beddings while minimizing the part of the embed-
dings that project onto the gender subspace. We
use the softer variation to debias the embeddings
while training our language model.

Zhao et al. (2017) look at gender bias in the con-
text of using structured prediction for visual object
classification and semantic role labeling. They ob-

serve gender bias in the training examples and that
their model amplifies the bias in its predictions.
They impose constraints on the optimization to re-
duce bias amplification while incurring minimal
degradation in their model’s performance.

Word embeddings can capture the stereotypi-
cal bias in human generated text leading to biases
in NLP Applications. Caliskan et al. (2017) con-
duct Word Embedding Association Test (WEAT).
It is based on the hypothesis that word embeddings
closer together in high dimensional space are se-
mantically closer. They find strong evidence of
social biases in pretrained word embeddings.

Rudinger et al. (2018) introduce Winogender
schemas1 and evaluate three coreference resolu-
tion systems—rule-based, statistical and neural
systems. They find that these systems’ predictions
strongly prefer one gender over the other for occu-
pations.

Font and Costa-Jussà (2019) study the impact
of gender debiasing techniques by Bolukbasi et al.
(2016) and Zhao et al. (2018) in machine trans-
lation. They find these methods to be effective,
and even a noted BLEU score improvement for the
debiased model. Our work is closely related but
while they use debiased pretrained embeddings,
we train the word embeddings from scratch and
debias them while the language model is trained.

May et al. (2019) extend WEAT to state-of-
the-art sentence encoders: the Sentence Encoder
Association Test (SEAT). They show that these
tests can provide an evidence for presence of bias.

1It is Winograd Schema-style coreference dataset consist-
ing of pair of sentences that differ only by a gender pronoun



9

However, the cosine similarity between sentences
can be an inadequate measure of text similarity
in sentences. In this paper, we attempt to mini-
mize the cosine similarity between word embed-
dings and gender direction.

Gonen and Goldberg (2019) conduct experi-
ments using the debiasing techniques proposed by
Bolukbasi et al. (2016) and Zhao et al. (2018).
They show that bias removal techniques based on
gender direction are inefficient in removing all as-
pects of bias. In a high dimensional space, spa-
tial distribution of the gender neutral word embed-
dings remain almost same after debiasing. This
enables a gender-neutral classifier to still pick up
the cues that encode other semantic aspects of
bias. We use softer variation of the debiasing
method proposed by Bolukbasi et al. (2016) and
attempt to measure the debiasing effect from the
minimal changes in the embedding space.

3 Methods

We first examine the bias existing in the datasets
through qualitative and quantitative analysis of
trained embeddings and cooccurrence patterns.
We then train an LSTM word-level language
model on a dataset and measure the bias of the
generated outputs. As shown in Figure 1, we then
apply a regularization procedure that encourages
the embeddings learned by the model to depend
minimally on gender. We debias the input and the
output embeddings individually as well as simul-
taneously. Finally, we assess the efficacy of the
proposed method in reducing bias.

We observe that when both input and output em-
beddings are debiased together, the perplexity of
the model shoots up by a much larger number than
the input or the output embeddings debiased indi-
vidually. We report our results when only input
embeddings are debiased. This method, however,
does not limit the model to capture other forms of
bias being learned in other model parameters or
output embeddings.

The code implementing our methods can be
found in our GitHub repository.2

3.1 Datasets and Text Preprocessing

We compare the model on three datasets–Penn
Treebank (PTB), WikiText-2 and CNN/Daily
Mail. The first two have been used in language
modeling for a long time. We include CNN/Daily

2https://github.com/BordiaS/language-model-bias

Mail dataset in our experiments as it contains a
more diverse range of topics.

PTB Penn Treebank comprises of articles rang-
ing from scientific abstracts, computer manuals,
etc. to news articles. In our experiments, we
observe that PTB has a higher count of male
words than female words. Following prior lan-
guage modeling work, we use the Penn Treebank
dataset (PTB; Marcus et al., 1993) preprocessed
by Mikolov et al. (2010).

WikiText-2 WikiText-2 is twice the size of the
PTB and is sourced from curated Wikipedia ar-
ticles. It is more diverse and therefore has a
more balanced ratio of female to male gender
words than PTB. We use preprocessed WikiText-2
(Wikitext-2; Merity et al., 2016).

CNN/Daily Mail This dataset is curated from a
diverse range of news articles on topics like sports,
health, business, lifestyle, travel etc. This dataset
has an even more balanced ratio of female to male
gender words and thus, relatively less biased than
the above two. However, this does not mean that
the use of pronouns is not biased. This dataset
was released as part of a summarization dataset by
Hermann et al. (2015), and contains 219,506 arti-
cles from the newspaper the Daily Mail. We sub-
sample the sentences by a factor of 100 in order
to make the dataset more manageable for experi-
ments.

3.2 Word-Level Language Model
We use a three-layer LSTM word-level language
model (AWD-LSTM; Merity et al., 2018) with
1150 hidden units implemented in PyTorch.3

These models have an embedding size of 400 and
a learning rate of 30.

We use a batch size of 80 for Wikitext-2 and
40 for PTB. Both are trained for 750 epochs.
The PTB baseline model achieves a perplexity
of 62.56. For WikiText-2, the baseline model
achieves a perplexity of 67.67.

For CNN/Daily Mail, we use a batch size of 80
and train it for 500 epochs. We do early stopping
for this model. The hyperparameters are chosen
through a systematic trial and error approach. The
baseline model achieves a perplexity of 118.01.

All three baseline models achieve reasonable
perplexities indicating them to be good proxies for
standard language models.

3https://github.com/salesforce/awd-lstm-lm



10

Fixed Context Infinite Context
λ µ σ β µ σ β Ppl.

train 0.83 1.00 3.81 4.65
0.0 0.74 0.91 0.40 2.23 2.90 0.38 62.56

0.001 0.69 0.88 0.34 2.43 2.98 0.35 62.69
0.01 0.63 0.81 0.31 2.56 3.40 0.36 62.83

0.1 0.64 0.82 0.33 2.30 3.09 0.24 62.48
0.5 0.70 0.91 0.39 2.91 3.76 0.38 62.5
0.8 0.76 0.96 0.45 3.43 4.06 0.26 63.36
1.0 0.84 0.94 0.38 2.42 3.02 -0.30 62.63

Table 1: Experimental results for Penn Treebank and generated text for different λ values

Fixed Context Infinite Context
λ µ σ β µ σ β Ppl.

train 0.80 1.00 3.70 4.60
0.0 0.70 0.84 0.29 3.48 4.29 0.15 67.67

0.001 0.69 0.84 0.27 2.32 3.12 0.16 67.84
0.01 0.61 0.79 0.20 1.88 2.69 0.14 67.78

0.1 0.65 0.82 0.24 2.26 3.11 0.06 67.89
0.5 0.70 0.88 0.31 2.25 3.17 0.20 69.07
0.8 0.65 0.84 0.28 2.07 2.98 0.18 69.36
1.0 0.74 0.92 0.27 2.32 3.21 -0.08 69.56

Table 2: Experimental results for WikiText-2 and generated text for different λ values

Fixed Context Infinite Context
λ µ σ β µ σ β Ppl.

train 0.72 0.94 0.77 1.05
0.0 0.51 0.68 0.22 0.43 0.59 0.29 118.01
0.1 0.38 0.52 0.19 0.85 1.38 0.22 116.49
0.5 0.34 0.48 0.14 0.79 1.31 0.20 116.19
0.8 0.40 0.56 0.19 0.96 1.57 0.23 121.00
1.0 0.62 0.83 0.21 1.71 2.65 0.31 120.55

Table 3: Experimental results for CNN/Daily Mail and generated text for different λ values

3.3 Quantifying Biases

For numeric data, bias can be caused simply by
class imbalance, which is relatively easy to quan-
tify and fix. For text and image data, the com-
plexity in the nature of the data increases and it
becomes difficult to quantify. Nonetheless, defin-
ing relevant metrics is crucial in assessing the bias
exhibited in a dataset or in a model’s behavior.

3.3.1 Bias Score Definition
In a text corpus, we can express the probability of
a word occurring in context with gendered words
as follows:

P (w|g) = c(w, g)/Σic(wi, g)
c(g)/Σic(wi)

where c(w, g) is a context window and g is a set of
gendered words that belongs to either of the two
categories: male or female. For example, when
g = f , such words would include she, her, woman

etc. w is any word in the corpus, excluding stop
words and gendered words. The bias score of a
specific word w is then defined as:

biastrain(w) = log

(
P (w|f)
P (w|m)

)
This bias score is measured for each word in the

text sampled from the training corpus and the text
corpus generated by the language model. A posi-
tive bias score implies that a word cooccurs more
often with female words than male words. For an
infinite context, the words doctor and nurse would
cooccur as many times with a female gender as
with male gender words and the bias scores for
these words will be equal to zero.

We conduct two sets of experiments where we
define context window c(w, g) as follows:

Fixed Context In this scenario, we take a fixed
context window size and measure the bias scores.



11

We generated bias scores for several context win-
dow sizes in the range (5, 15). For a context size k,
there are k words before and k words after the tar-
get word w for which the bias score is being mea-
sured. Qualitatively, a smaller context window
size has more focused information about the tar-
get word. On the other hand, a larger window size
captures topicality (Levy and Goldberg, 2014). By
choosing an optimal window of k = 10, we give
equal weight of 5% to the ten words before and the
ten words after the target word.

Infinite Context In this scenario, we take an in-
finite window of context with weights diminish-
ing exponentially based on the distance between
the target word w and the gendered word g. This
method emphasizes on the fact that the nearest
word has more information about the target word.
The farther the context gets away from a word,
the less information it has about the word. We
give 5% weight to the words adjacent to the target
word as in Fixed Context but reduce the weights
of the words following by 5% and 95% to the rest;
this applied recursively gives a base of 0.95. This
method of exponential weighting instead of equal
weighting adds to the stability of the measure.

3.3.2 Bias Reduction Measures

To evaluate debiasing of each model, we measure
the bias for the generated corpus.

biasλ(w) = log(
P (w|f)
P (w|m)

)

To estimate the amplification or reduction of the
bias, we fit a univariate linear regression model
over bias scores of context words w as follows:

biasλ(w) = β ∗ biastrain(w) + c

where β is the scaled amplification measure rela-
tive to the training data. Reducing β implies debi-
asing the model.

We also look at the distribution of the bias by
evaluating mean absolute bias and deviation in
bias scores for each context word in each of the
generated corpora.

µλ = mean(abs(biasλ));σλ = stdev(biasλ)

We take the mean of absolute bias score as the
word can be biased in either of the two directions.

3.4 Model Debiasing
Machine learning techniques that capture patterns
in data to make coherent predictions can uninten-
tionally capture or even amplify the bias in data
(Zhao et al., 2017). We consider a gender sub-
space present in the learned embedding matrix in
our model as introduced in the Bolukbasi et al.
(2016) paper. We train these embeddings on the
word level language model instead of using the
debiased pretrained embeddings (Font and Costa-
Jussà, 2019). We conduct experiments for the
three cases where we debias—input embeddings,
output embeddings, and both the embeddings si-
multaneously.

Let w ∈ SW be a word embedding correspond-
ing to a word in the word embedding matrix W .
Let

Di, . . . , Dn ⊂ SW
be the defining sets4 that contain gender-opposing
words, e.g. man and woman. The defining sets
are designed separately for each corpus since cer-
tain words may not appear in another corpus. We
consider it a defining set if both gender-opposing
words occur in the training corpus.

If ui, vi are the embeddings corresponding to
the words man and woman, then {ui,vi} = Di.
We consider the matrix C which is defined as a
stack of difference vectors between the pairs in the
defining sets. We have:

C =

(
u1−v1

2 )
...

(un−vn2 )

 = UΣV
The difference between the pairs encodes the

gender information corresponding to the gender
pair. We then perform singular value decomposi-
tion on C, obtaining UΣV . The gender subspace
B is then defined as the first k columns (where k
is chosen to capture 50% of the variation) of the
right singular matrix V :

B = V1:k

Let N be the matrix consisting of the embed-
dings for which we would like the corresponding
words to exhibit unbiased behavior. If we want
the embeddings in N to have minimal bias, then
its projection onto the gender subspace B should
be small in terms its the squared Frobenius norm.

4See the supplement for corpus-wise defining sets



12

Target Word λ Sample From Generated Text

0.0 “she was put on her own machine to raise money for her own wedding <unk> route which saw her
crying and <unk> down a programme today . effects began by bottom of her marrow the <unk>”

crying 0.5 “he <unk> in the americas with the <unk> which can spread a <unk> circumcision ceremony
made last month . as he <unk> his mother s <unk> crying to those that”

1.0 “he discovered peaceful facebook remains when he was caught crying officers but was arrested after
they found the crash hire a man <unk> brown shocked his brother <unk> over”

0.0 “camilla said she talked to anyone and had previously left her love of two young children . it all
comes with her family in conviction of her son s death . it s been fragile . the <unk> and retail boy
that was rik s same maker identified genuinely <unk> attacked all”

fragile 0.5 “his children at nearby children s hospital in <unk> and went <unk> years after he was arrested on
<unk> bail . she spent relaxed weeks in prison after being sharply in fragile <unk> while she was
jailed and strangled when she was born in <unk> virginia”

1.0 “could they possibly have a big barrier to jeff <unk> and <unk> my son all intelligence period that
will contain the east country s world from all in the world the truth is when we moved clear before
the split twenty days earlier that day . none of the distributed packs on the website can never <unk>
re able to <unk> it the second time so that fitting fragile <unk> are and less the country is <unk>
. it came as it was once <unk> million lead jobs mail yorkshire . adoption of these first product is
ohio but it is currently almost impossible for the moon to address and fully offshore hotly ”

0.0 “mr <unk> worked traditions at the squadron base in <unk> rbs to marry the us government .he
referring to the mainland them in february <unk> he kept communist leadership from undergoing”

leadership 0.5 “obama s first wife janet had a chance to run the opposition for a superbowl event for charity the
majority of the south african people s travel stage <unk> leadership while it was married off christ-
mas”

1.0 “the woman s lungs and drinking the ryder of his daughters s leadership morris said businesses .
however being of his mouth around wiltshire and burn talks from the hickey s <unk> employees”

0.0 “his legs and allegedly killed himself by suspicious points . in the latest case after an online page he
left prisoner in his home in <unk> near <unk> manhattan on saturday when he was struck in his
car operating in <unk> bay smoking <unk> and <unk> <unk> when he had”

prisoner 0.5 “it is something that the medicines can target prisoner and destroy <unk> firms in the uk but i hope
that there are something into the on top getting older people who have more branded them as poor .”

1.0 “the ankle follows a worker <unk> her <unk> prisoner she died this year before now an profile
which clear her eye borrowed for her organ own role . it was a huge accident after the drugs she had”

Table 4: Generated text comparison for CNN/Daily Mail for different λ values

Therefore, to reduce the bias learned by the em-
bedding layer in the model, we can add the follow-
ing bias regularization term to the training loss:

LB = λ‖NB‖2F

where λ controls the importance of minimizing
bias in the embedding matrix W (from which N
and B are derived) relative to the other compo-
nents of the model loss. The matrices N and C
are updated each iteration during the model train-
ing.

We input 2000 random seeds in the language
model as starting points to start word generation.
We use the previous words as an input to the lan-
guage model and perform multinomial selection to
generate up the next word. We repeat this up to
500 times. In total, we generate 106 tokens for all
three datasets for each λ and measure the bias.

4 Experiments

4.1 Model

After achieving the baseline results, we run exper-
iments to tune λ as hyperparameter. We report an
in-depth analysis of bias measure on the models
with debiased input embeddings.

4.2 Results and Text Examples

We calculate the measures stated in Section 3.3 for
the three datasets and the generated corpora using
the corresponding RNN models. The results are
shown in Tables 1, 2 and 3. We see that the µ con-
sistently decline as we increase λ until a point, be-
yond which the model becomes unstable. So there
is a scope of optimizing the λ values. The detailed
analysis is presented in Section 4.3

Table 4 shows excerpts around selected target
words from the generated corpora to demonstrate
the effect of debiasing for different values of λ.
We highlight the words crying and fragile that are
typically associated with feminine qualities, along



13

with the words leadership and prisoners that are
stereotyped with male identity. These biases are
reflected in the generated text for λ = 0. We no-
tice increased mention of the less probable gen-
der in the subsequent generated text with debias-
ing (λ = 0.5, 1.0). For fragile, the generated text
at λ = 1.0 has reduced the mention of stereotyped
female words but had no mentions of male words;
resulting in a large chunk of neutral text. Simi-
larly, in prisoners, the generated text for λ = 0.5
has no gender words.

However, these are small snippets and the bias
scores presented in the supplementary table quan-
tifies the distribution of gender words around the
target word in the entire corpus. These target
words are chosen as they are commonly perceived
gender biases and in our study, they show promi-
nent debiasing effect.5

4.3 Analysis and Discussion

We consider a text corpus to be biased when it has
a skewed distribution of words cooccuring with
one gender vs another. Any dataset that has such
demographic bias can lead to (potentially unin-
tended) social exclusion (Hovy, 2015). PTB and
WikiText-2 consist of news articles related to busi-
ness, science, politics, and sports. These are all
male dominated fields. However, CNN/Daily Mail
consists of articles across diverse set of categories
like entertainment, health, travel etc. Among the
three corpora, Penn Treebank has more frequent
mentions of male words with respect to female
words and CNN/Daily Mail has the least.

As defined, bias score of zero implies perfectly
neutral word, any value higher/lower implies fe-
male/male bias. Therefore, the absolute value of
bias score signifies presence of bias. Overall bias
in a dataset can be estimated as the average of ab-
solute bias score (µ). The aggregated absolute bias
scores µ of the three datasets—Penn Treebank,
WikiText-2, and CNN/Daily Mail—are 0.83, 0.80,
and 0.72 respectively. Higher µ value in this mea-
sure means on-an-average the words in the entire
corpus are more gender biased. As per the Tables
1, 2, and 3, we see that the µ consistently decline
as we increase λ until a point, beyond which the
model becomes unstable. So there is a scope of
optimizing the λ values.

The second measure we evaluated is the stan-
dard deviation (σ) of the bias score distribution.

5For more examples, refer to the supplement

Less biased dataset should have the bias score con-
centrating closer to zero and hence lower σ value.
We consistently see that, with the initial increase
of λ, there is a decrease in σ of the bias score dis-
tribution.

The final measure to evaluate debiasing is com-
parison of bias scores at individual word level. We
regress the bias scores of the words in generated
text against their bias scores in the training corpus
after removing the outliers. The slope of regres-
sion β signifies the amplification or dampening ef-
fect of the model relative to the training corpus.
Unlike the previous measures, this measure gives
clarity at word level bias changes. A drop in β sig-
nifies reduction in bias and vice versa. A negative
β signifies inversion in bias assuming there are no
other effects of the loss term. In our experiments,
we observe β to increase with higher values of λ
possibly due to instability in model and none of
those values go beyond 1.

We observe that corpus level bias scores like µ,
σ are less effective measures to study efficacy of
debiasing techniques because they fail to track the
improvements at word level. Instead, we recom-
mend a word level score comparison like β to eval-
uate robustness of debiasing at corpus level.

To choose the context window in a more ro-
bust manner, we take exponential weightings to
the cooccurrences. The results for aggregated av-
erage of absolute bias and standard deviation show
the same pattern as in fixed context window.

As shown in the results above, we see that
the standard deviation (σ), absolute mean (µ) and
slope of regression (β) reduce for smaller λ rel-
ative to those in training data and then increase
with λ to match the variance in the original cor-
pus. This holds for the experiments conducted
with fixed context window as well as with expo-
nential weightings.

5 Conclusion

In this paper, we quantify and reduce gender bias
in word level language models by defining a gen-
der subspace and penalizing the projection of the
word embeddings onto that gender subspace. We
device a metric to measure gender bias in the train-
ing and the generated corpus.

In this study, we quantify corpus level bias
in two different metrics—absolute mean (µ) and
standard deviation (σ). However, for evaluating
debiasing effects, we propose a relative metric (β)



14

to study the change in bias scores at word level in
generated text vs. training corpus. To calculate
β, we conduct an in-depth regression analysis of
the word level bias measures in the generated text
corpus over the same for the training corpus.

Although we found mixed results on amplifica-
tion of bias as stated by Zhao et al. (2017), the de-
biasing method shown by Bolukbasi et al. (2016)
was validated with the use of novel and robust
bias measure designed in this paper. Our proposed
methodology can deal with distribution of words
in a vocabulary in word level language model and
it targets one way to measure bias, but it’s highly
likely that there is significant bias in the debiased
models and data, just not bias that we can detect
on this measure. It can be concluded different bias
metrics show different kinds of bias (Gonen and
Goldberg, 2019).

We additionally observe a perplexity bias trade-
off as a result of the additional bias regularization
term. In order to reduce bias, there is a compro-
mise on perplexity. Intuitively, as we reduce bias
the perplexity is bound to increase due to the fact
that, in an unbiased model, male and female words
will be predicted with an equal probability.

6 Acknowledgements

We are grateful to Yu Wang and Jason Cramer for
helping to initiate this project, to Nishant Subra-
mani for helpful discussion, and to our reviewers
for their thoughtful feedback. Bowman acknowl-
edges support from Samsung Research.

References
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou,

Venkatesh Saligrama, and Adam T Kalai. 2016.
Man is to computer programmer as woman is to
homemaker? Debiasing word embeddings. In D. D.
Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and
R. Garnett, editors, Advances in Neural Information
Processing Systems 29, pages 4349–4357. Curran
Associates, Inc.

Aylin Caliskan, Joanna J. Bryson, and Arvind
Narayanan. 2017. Semantics derived automatically
from language corpora contain human-like biases.
Science, 356(6334):183–186.

Joel Escudé Font and Marta R. Costa-Jussà. 2019.
Equalizing gender biases in neural machine trans-
lation with word embeddings techniques. CoRR,
abs/1901.03116.

Hila Gonen and Yoav Goldberg. 2019. Lipstick on a
pig: Debiasing methods cover up systematic gender

biases in word embeddings but do not remove them.
In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Association for Computational Linguistics.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In C. Cortes, N. D.
Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,
editors, Advances in Neural Information Processing
Systems 28, pages 1693–1701. Curran Associates,
Inc.

Dirk Hovy. 2015. Demographic factors improve clas-
sification performance. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), volume 1, pages 752–762.

Anja Lambrecht and Catherine E Tucker. 2018. Al-
gorithmic bias? an empirical study into appar-
ent gender-based discrimination in the display of
stem career ads. Social Science Research Network
(SSRN).

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), vol-
ume 2, pages 302–308.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313–330.

Chandler May, Alex Wang, Shikha Bordia, Samuel R.
Bowman, and Rachel Rudinger. 2019. On measur-
ing social bias in sentence encoders. In Proceed-
ings of the 2019 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies. Associa-
tion for Computational Linguistics.

Stephen Merity, Nitish Shirish Keskar, and Richard
Socher. 2018. Regularizing and optimizing LSTM
language models. In International Conference on
Learning Representations.

Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843.

Tomas Mikolov, Martin Karafiát, Lukás Burget, Jan
Cernocký, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045–1048. ISCA.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf
http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf
https://doi.org/10.1126/science.aal4230
https://doi.org/10.1126/science.aal4230
http://arxiv.org/abs/1901.03116
http://arxiv.org/abs/1901.03116
http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf
http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf
https://openreview.net/forum?id=SyyGPP0TZ
https://openreview.net/forum?id=SyyGPP0TZ


15

Mei Ngan and Patrick Grother. 2015. Face recogni-
tion vendor test (FRVT) performance of automated
gender classification algorithms. US Department
of Commerce, National Institute of Standards and
Technology.

Marta Recasens, Cristian Danescu-Niculescu-Mizil,
and Dan Jurafsky. 2013. Linguistic models for an-
alyzing and detecting biased language. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 1650–1659.

Rachel Rudinger, Jason Naradowsky, Brian Leonard,
and Benjamin Van Durme. 2018. Gender bias in
coreference resolution. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, New Orleans, Louisiana.
Association for Computational Linguistics.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. 2017. Men also like
shopping: Reducing gender bias amplification using
corpus-level constraints. In EMNLP, pages 2979–
2989. Association for Computational Linguistics.

Jieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-
Wei Chang. 2018. Learning gender-neutral word
embeddings. In Proceedings of the 2018 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 4847–4853. Association for Com-
putational Linguistics.

http://aclweb.org/anthology/D18-1521
http://aclweb.org/anthology/D18-1521

