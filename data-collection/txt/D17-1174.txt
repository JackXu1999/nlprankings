



















































Neural Discontinuous Constituency Parsing


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1666–1676
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Neural Discontinuous Constituency Parsing

Miloš Stanojević and Raquel G. Alhama
Institute for Logic, Language and Computation (ILLC)

University of Amsterdam
{m.stanojevic, rgalhama}@uva.nl

Abstract

One of the most pressing issues in dis-
continuous constituency transition-based
parsing is that the relevant information for
parsing decisions could be located in any
part of the stack or the buffer. In this pa-
per, we propose a solution to this prob-
lem by replacing the structured percep-
tron model with a recursive neural model
that computes a global representation of
the configuration, therefore allowing even
the most remote parts of the configura-
tion to influence the parsing decisions. We
also provide a detailed analysis of how
this representation should be built out of
sub-representations of its core elements
(words, trees and stack). Additionally, we
investigate how different types of swap or-
acles influence the results. Our model is
the first neural discontinuous constituency
parser, and it outperforms all the previ-
ously published models on three out of
four datasets while on the fourth it obtains
second place by a tiny difference.

1 Introduction

Research on constituency parsing has been mostly
concentrated on projective trees, which can be
modeled with Context-Free Grammars (CFGs).
One of the main reasons for this is that modeling
non-projective trees often requires richer gram-
mar formalisms, which in practice implies slower
runtime. For instance, the parsing algorithms
for binary LCFRS—the most prominent grammar-
based approach to parsing non-projective con-
stituency trees—have computational complexity
O(n3k), where k is the fan-out of the grammar.
For this reason, researchers turned to faster ap-
proximate methods. Approximations can be done

in two ways: either on the types of structures that
are predicted or on the parsing algorithm.

The first approach approximates discontinu-
ous constituency structures with simpler structures
for which more efficient algorithms exist. This
method works as a pipeline: it converts the input to
a simpler formalism, parses with it, and then con-
verts it back. Relevant examples are the parsers
by Hall and Nivre (2008) and Fernández-González
and Martins (2015), who convert discontinuous
constituents to dependencies, and Versley (2016),
who also applied a conversion but in this case to
the projective constituency trees.

The second approach—approximation on the
parsing algorithm—consists of an approximate
search for the most probable parse. This is
analogous to the search done by transition-based
parsers, which greedily search through the space
of all possible parses, resulting in very fast mod-
els. The first transition-based discontinuous con-
stituency parser of this sort was presented in Vers-
ley (2014), and it consists of a shift-reduce parser
that handles discontinuities with swap transitions.
This parser was very similar to dependency parsers
with swap transitions (Nivre, 2009; Nivre et al.,
2009), but unlike its dependency equivalents, it did
not exhibit higher accuracy. Later work on discon-
tinuous transition-based parsing was largely fo-
cused on finding alternative transitioning systems
to handle discontinuity. Maier (2015) and Maier
and Lichte (2016) proposed new types of swap op-
erations (CompoundSwap and SkipShift) to make
the transition sequences shorter—and therefore
easier to learn. Coavoux and Crabbé (2017) went
even further by modifying not only the transitions
but the whole configuration structure by introduc-
ing an additional stack.

Over the years the transitioning system has seen
some progress, but the learning model has re-
mained the same : a sparse linear model trained

1666



with structured perceptron and early update strat-
egy (Collins, 2002; Collins and Roark, 2004;
Huang et al., 2012). This model requires heavy
feature engineering and has a limited capacity in
modeling interaction between the features.

Maier and Lichte (2016) argue that one of the
biggest problems of transition based systems is
precisely their greedy search, because they can-
not recover from the bad decisions made in ear-
lier parsing steps. Some researchers try to account
for this problem by increasing the beam size, but
there is a limit on how much the beam can be in-
creased while remaining efficient for practical use
(Coavoux and Crabbé, 2017).

The solution we propose is to use a probabilis-
tic model that exploits the information from the
whole configuration structure when making the
decision for the next action. This can be achieved
by using recurrent neural models that allow in-
formation to flow all the way from the individual
characters, up trough the words, POS tags, sub-
trees, stack and buffer until the final configura-
tion representation. Thanks to using a neural net-
work model, which removes the need for feature
engineering, we can concentrate on the question
of which representations are more relevant for the
model at each step of the flow. Thus, we reflect on
how alternative representations should impact the
task, and we report their relative contribution in an
ablation study.

In our work, we also reduce the number of swap
transitions by trying to postpone them as much
as possible, in a style similar to the lazy-swap
used in Nivre et al. (2009) —albeit with an even
lower number of swaps. This change influences
the model indirectly by introducing a helpful in-
ductive bias.

Our model gets state-of-the-art results on Ne-
gra, Negra-30 and TigerSPMRL datasets, and on
the TigerHN achieves the second best published
result. To the best of our knowledge this is the
first work that uses neural networks in the context
of discontinuous constituency parsing.

2 Transition System

We base our transitioning system on the shift-
promote-adjoin transitions proposed in Cross and
Huang (2016), because they remove the need
for explicit binarization. Transition-based parsers
consist of two components: a configuration that
represents a parsing state and a set of transitions

between configurations.
The configuration consists of two data struc-

tures: a stack S that contains all the constituents
built so far, and a buffer B of words that remain
to be processed. The initial configuration consists
of a buffer filled with words and an empty stack—
presented as the axiom in Figure 1. The objec-
tive is to find a sequence of transitions that lead
to a goal state in which the buffer is empty and
the stack contains only one constituent with the
ROOT label. The shift transition moves the first
element from the buffer to the top of the stack. The
pro(X) transition “promotes” the topmost element
of the stack: it replaces it with a tree that has non-
terminal X and the topmost element of the stack as
its only child, which also becomes its head con-
stituent. The adjx transition adjoins the second
topmost element of the stack as a leftmost child of
the topmost element of the stack. The adjy tran-
sition is a mirror transition of the adjx.

The transitions described so far are enough
to handle projective constituency structures, and
have been used with success for this task in Cross
and Huang (2016). To make the parser able to
process discontinuous constituents we need an ad-
ditional transition that allows for constituents that
are far apart on the stack to become close, so that
they can be adjoined into a new constituent. For
this we use the swap transition from Nivre (2009).
This transition takes the second topmost element
from the stack and puts it back to the buffer. To
prevent infinite loops of shift-swap transitions, we
put a constraint that swap can be applied only
to constituents that have not been swapped be-
fore. To do this we use the linear ordering of con-
stituents<ind based on the position of the leftmost
word in their yield (Maier and Lichte, 2016).

2.1 Oracle

In the case of non-projective parsing, the extrac-
tion of the oracle is not trivial because there can be
many possible oracles that would derive the same
tree. Therefore it is common practice to use some
heuristic to extract only one of the possible ora-
cles.

To construct the oracle, we start with the initial
configuration and apply the first transition whose
conditions are satisfied. We keep applying transi-
tions to the resulting configurations until the goal
is reached. The transitions are determined as fol-
lows: first, we apply adjx, adjy or pro(X) if

1667



axiom
〈

[], [w1, w2, ..., wn]
〉

shift

〈
S, x|B 〉〈
S|x, B 〉

pro(X)

〈
S|t, B 〉〈

S|X(t), B 〉
adjx

〈
S|t|X(t1 . . . tk), B

〉〈
S|X(t, t1 . . . tk), B

〉
adjy

〈
S|X(t1 . . . tk)|t, B

〉〈
S|X(t1 . . . tk, t), B

〉
swap

〈
S|t1|t2, B

〉〈
S|t2, t1|B

〉 t1 <ind t2
goal

〈
ROOT, �

〉
Figure 1: Transition System

one of those produces a constituent that is found
in the tree; in case of failure, we check the condi-
tion for applying swap, which varies depending on
the type of oracle, as we define next. If all these
checks fail then a shift transition is performed.

2.1.1 Eager Oracle
Nivre (2009) introduced swap transitions with a
very simple oracle. We can define the swapping
condition for the extraction of the Eager oracle
transition sequence as:

s1 <G s0 (1)

where s0 and s1 are the topmost and second top-
most elements of the stack respectively, and <G
is the projective ordering of the nodes in the tree.
That ordering can be computed by visiting the
nodes in the tree in the postorder traversal.

This is the technique that has been used in most
previous proposals on discontinuous constituency
parsing (Maier, 2015; Maier and Lichte, 2016).

2.1.2 Lazy Oracle
Eager swapping strategy produces a large number
of swap transitions which makes them difficult to
predict. For this reason, Nivre et al. (2009) intro-
duced a lazy-swap operation that postpones swap-
ping by having an additional condition during the
construction of an oracle. This technique was used
successfully in Versley (2014) to improve over the
eager swapping baseline. As an example, in Fig-
ure 2a word w1 should shift and swap many times

to get to word w5 in order to construct constituent
C. In contrast, a Lazy oracle would postpone
swapping until constituent B is built so that only
one swap operation over node B would be enough
for word w1 to get to word w5.

In order to define that condition in the context
of discontinuous constituency parsing, we need to
define a few other terms. First of all, we call a
projective constituent any constituent that yields
a continuous span of words (marked with blue
color in Figure 2). Note that a projective con-
stituent might contain non-projective constituents
as its descendants. A fully projective constituent
is a constituent that is projective and whose de-
scendants are all projective (marked with red in
Figure 2). Finally, a maximal fully projective con-
stituent is a fully projective constituent whose par-
ent is not a fully projective constituent (marked
green in Figure 2). Finally, we define a func-
tion MPC(x) that returns the closest maximally
projective constituent that is ascendant of a con-
stituent x if there is one; otherwise, it returns x.

The condition for the lazy swap can now be ex-
pressed as:

s1 <G s0 ∧ MPC(s0) 6= MPC(b0) (2)
where s0 and b0 are the topmost elements of the
stack and buffer, respectively. This means that
we do not allow swap to penetrate into maximally
projective constituents, so swapping can be de-
layed until the maximally projective constituent
has been built.

2.1.3 Lazier Oracle
The standard Lazy swap strategy helps in cases
where MPC constituents exist, like in Figure 2a.
But in cases like Figure 2b there are no MPC con-
stituents (except for words), so Lazy would not
show any improvement over Eager. Still, even
in this case it is visible that swapping w1 should
be postponed until B is built. We introduce an
oracle strategy called Lazier that implements the
heuristic of postponing swapping over projective
constituents.1

Let a function CPC(x) return the closest pro-
jective constituent ascendant of a constituent x.
The condition for swap operation can now be ex-
pressed with:

s1 <G s0 ∧ CPC(s0) = CPC(s1) (3)
1The same intuition is followed in the Barriers strategy of

Versley (2014).

1668



w1 w2 w3 w4 w5

A

B

C

D

(a) Tree with Maximal Fully Projective Node B

w1 w2 w3 w4 w5

A

B

C

D

(b) Tree without any Maximal Fully Projective Nodes (other
than words themselves)

Figure 2: Example tree structures

This constraint prevents swap from allowing con-
stituents to escape their closest projective ances-
tor. If we know that the swap operation can be
performed, i.e. if s1 <G s0, it is easy to show
that in that case CPC(s0) = CPC(s1) =⇒
MPC(s0) 6= MPC(b0) or in other words that
Lazy is a special case of Lazier. There are are
only two cases to consider about CPC(s0): case
a) s0 and CPC(s0) are separated by a non-empty
sequence of non-projective constituents and case
b) s0 is the immediate child of CPC(s0). In
case a) from definition of maximal projective con-
stituents follows that MPC(s0) = s0 and there-
fore MPC(s0) 6= MPC(b0) since s0 and b0 are
non-overlapping. In case b) we need to consider
two possible options: b1) CPC(s0) is fully pro-
jective and b2) CPC(s0) is not fully projective.
Case b1) is not possible because it leads to con-
tradiction with original condition s1 <G s0. Case
b2) leads again to MPC(s0) = s0 and by that
MPC(s0) 6= MPC(b0).

3 Model

As mentioned before, our goal is to have a model
that can have a global representation of the parsing
state. In order to define this global representation
of the configuration, we first need to analyze what
are the proper representations of its subparts.

3.1 How to Represent Terminal Nodes?

The representations induced by neural networks
are continuous vectors that encode the information
that is relevant for the loss function. The initial
nodes in the computation graph are often embed-

dings that represent the atomic inputs in the model.
In our model, the embedding of a terminal node is
computed by concatenating the following four em-
beddings and then applying the affine transforma-
tion to compress the result into a smaller vector:

• a trained word embedding
• a trained POS tag embedding
• a pre-trained word embedding
• a trained character embedding of the word
Trained embeddings (both word and POS tag

embedding) are automatically trained by our
model to better suit the task that we are solv-
ing. The usage of pre-trained embeddings has
become standard in neural parsing models: these
presentations are helpful because they bring addi-
tional contextual information from a bigger non-
annotated corpora. The embeddings that we use in
this work are the ones distributed with the Polyglot
package (Al-Rfou et al., 2013).

The character embedding representation of a
word is computed by composing the representa-
tions of each character in the word form. This can
be useful to recover some of the morphological
features present in the word, such as suffixes or
prefixes. We compose character embeddings by
running a bi-directional LSTM (Bi-LSTM) over
the characters (Ling et al., 2015; Ballesteros et al.,
2015).

The embeddings composed in this way express
the properties of a word, but they ignore the con-
text in which the word appears in the actual sen-

1669



tence. To address this we compute the final rep-
resentation of the word by running a separate Bi-
LSTM model over the initial vectors of the termi-
nals in the same way as done by Kiperwasser and
Goldberg (2016) and Cross and Huang (2016).

3.2 How to Represent Non-Terminal Nodes?
During the parsing process we need to produce
the representations of the full subtrees that are
going to be placed on the stack. In the depen-
dency parsing literature, many approaches for rep-
resenting dependency subtrees use the represen-
tation of the head word. If the representation of
the head word is computed using a model that
takes context into account, such as Bi-LSTM mod-
els, then this simple architecture can give good
results (Kiperwasser and Goldberg, 2016; Cross
and Huang, 2016). However, we believe that this
is not the right approach for discontinuous con-
stituency parsing. The reason is that, for the parser
to know to which constituents it should attach the
current constituent, it needs to know which argu-
ments have already been attached and which ones
are missing. In other words, even if the head of
two different constituents is the same, their rep-
resentation should be different because they have
different requirements.

To address this we use a “composition function”
approach where we recursively compute the rep-
resentation of the constituent. Recursive neural
networks (RecNN) (Goller and Küchler, 1996) are
one way of accomplishing this. Dyer et al. (2015)
use RecNN to compute the representation of the
subtrees in the dependency structure. We adapt
this model to our case in the following way. For bi-
nary constituents (i.e. outputs of adjx and adjy)
the composition function takes the representation
of the head constituent hhead, the representation
of the complement hcomp and one single bit that
represents the directionality of the ex in the ad-
joining operation (0 for adjx and 1 for adjy).
The resulting hnew representation is computed as
follows:

hnew = tanh(Wadj [hhead;hcomp; ex] + badj)

Here, semi-colon (;) represents vector concatena-
tion, and Wadj and badj are the weight matrix and
the bias vector that are trained together with the
rest of the model, to optimize the desired loss
function.

The transition pro(X) also creates new trees and
its composition function can be seen as a function

of a Simple RNN model:

hnew = tanh(Wpro[hhead; ent] + bpro)

Here ent is the embedding for the non-terminal to
which constituent gets promoted. Wpro and bpro
are again the weight matrix and the bias vector
whose values are estimated during training.

Simple RNN models have been shown to suffer
from vanishing gradient problem, and for that rea-
son they have been largely replaced with LSTM
models (Hochreiter and Schmidhuber, 1997). The
same holds for recursive neural network mod-
els. Le and Zuidema (2016) have shown that,
for deep and complex hierarchical structures, the
models that have a memory akin to the memory
in LSTM are much more robust towards the van-
ishing gradient problem. Thus, in our work we
use the Tree-LSTM neural architecture from Tai
et al. (2015), but the alternative recursive version
of LSTM by Le and Zuidema (2015) could be used
as well.

In the Tree-LSTM model each constituent is
represented by the hidden state h and the mem-
ory cell c. The composition function for the
binary constituents with representations hhead,
chead, hcomp and ccomp computes the new repre-
sentations hnew and cnew in the following way:

fhead = σ(W
(f)
11 hhead +W

(f)
12 hcomp + b

(f)
x )

fcomp = σ(W
(f)
21 hhead +W

(f)
22 hcomp + b

(f)
x )

i = σ(W (i)1 hhead +W
(i)
2 hcomp + b

(i)
x )

o = σ(W (o)1 hhead +W
(o)
2 hcomp + b

(o)
x )

u = tanh(W (u)1 hhead +W
(u)
2 hcomp + b

(u)
x )

cnew = i� u+ fhead � chead + fcomp � ccomp
hnew = o� tanh(cnew)

All the W matrices and the bias vectors b are
trained parameters of the composition function.
For each equation above there is an alternative
equation that instead of bias bx uses bias by.
Which equation/bias will be used depends on the
directionality of the adjoining operation.

For the promote transition, since it creates only
one unary node, we can use almost the same com-

1670



putation as in the standard LSTM:

f = σ(W (f)hhead +W
(f)
nt ent + b

(f))

i = σ(W (i)hhead +W
(i)
nt ent + b

(i))

o = σ(W (o)hhead +W
(o)
nt ent + b

(o))

u = tanh(W (u)hhead +W
(u)
nt ent + b

(u))
cnew = i� u+ f � chead
hnew = o� tanh(cnew)

The main difference from the standard LSTM is
that here we additionally use the information from
the non-terminal embedding ent to which the con-
stituent is promoted.

3.3 How to Represent a Configuration?

We have covered how to represent syntactic ob-
jects (terminal and non-terminal nodes) that are
stored in the stack and the buffer, but we still need
to decide how to combine these representations to
make a final decision about the next transition.

One possibility is to first find a suitable repre-
sentation for the stack and the buffer individually,
concatenate these representations and then apply a
multi-layer perceptron (MLP) to produce the prob-
abilities for the next action.

The stack and the buffer can be seen as the same
type of data structure: the buffer can be interpreted
as a stack that is filled by pushing the words in
a sentence from the last to the first. Therefore,
we can use same approach for modeling stack and
buffer.

The most common approach for representing a
stack structure in transition based parsers (both in
perceptron and neural models) is to take the rep-
resentations of the first few top constituents on
the top of the stack. Thus, this approach assumes
that only the top of the stack and buffer are rel-
evant for deciding the next action. Even though
this assumption seems reasonable in the context
of continuous constituency parsing, for discon-
tinuous parsing it can be very harmful because
the constituents that we want to merge might be
very far from each other in the stack, as argued in
(Maier and Lichte, 2016).

In our work, we explore an alternative model
that could address this problem; namely, the
Stack-LSTM model proposed in (Dyer et al.,
2015). This model consists of an LSTM that pro-
cesses the whole stack as a sequence, to obtain in
this way a representation of the stack that includes

all of its elements. This approach gave good re-
sults on continuous dependency parsing, but its
properties should be even more important for dis-
continuous parsing, since it allows to keep in the
stack a representation of all the constituents.

Given the stack hstack and buffer hbuffer repre-
sentations computed by Stack-LSTMs, we com-
pute the configuration representation hconf by
concatenating these vectors and then applying an
affine transformation followed by aReLU(·) non-
linearity:

hconf = ReLU(Wconf [hstack;hbuffer] + bconf )

This vector representation encodes the whole con-
figuration: the information flow passes trough ev-
ery character, every POS tag, every constituent in
the stack and in the buffer. From this vector rep-
resentation we can compute the probability of the
transition z from the set of possible transitions Z
by applying one final softmax layer:

p(z|hconf ) = exp(w
T
z hconf + bzi)∑

zi∈Z exp(w
T
zihconf + bzi)

The probability of the whole sequence of transi-
tions is defined as the product of the probabilities
of its transitions:

p(z|w) =
|z|∏
i=1

p(zi|hconf i)

The parameters are optimized for maximum like-
lihood of the oracle sequence of transitions.

4 Experiments

We empirically test the performance of our parser
on two German constituency treebanks: Negra and
Tiger. The preprocessing applied to these tree-
banks follows the same methods used in other dis-
continuous constituency parsing literature, as de-
scribed in Maier (2015) and implemented in the
tree-tools software2.

We use two different versions of the Negra tree-
bank. The first version is filtered for the sen-
tences up to 30 words, in order to remain compa-
rable to previous grammar-based models; the sec-
ond version includes sentences of all lengths. As
for the Tiger treebank, we use two different splits:
TigerHN (Hall and Nivre, 2008) and TigerSPMRL

2https://github.com/wmaier/treetools

1671



name value
trained word embedding dim. 100
pretrained word embedding dim. 64
POS embedding dim. 20
character embedding dim. 100
character Bi-LSTM layers 1
word Bi-LSTM layers 2
(non-)terminal node repr. dim. 40
configuration repr. dim. 100
stack LSTM dim. 100
stack LSTM layers 2
buffer LSTM dim. 100
buffer LSTM layers 2
optimizer Adam
optimizer parameter b1 0.9
optimizer parameter b2 0.999
beam size 16

Table 1: Hyper-parameters of the model

#swaps jump size
Eager 43.17 ±49.21 1.00 ± 0.0
Lazy 10.96 ± 9.96 6.88 ±5.54
Lazier 5.40 ± 3.05 10.03 ±11.05

Table 2: Average number of swaps and jump sizes
per sentence

(Maier, 2015). We evaluated the model with the
evaluation module of discodop3 parser.

Our model is implemented with DyNet (Neu-
big et al., 2017) and the code is available at
https://github.com/stanojevic/
BadParser. The concrete hyper-parameters of
our model are shown in Table 1. We optimize the
parameters with Adam optimizer on the training
set, for 10 iterations with 100 random restarts, and
we do model selection on the validation set for
the F-score. During test time we use beam search
with beam of size 16.

We conducted the development of our model on
the TigerHN train and development sets. First we
will analyze the effect of different model design
decisions and then we show the results over the
test set. The development set scores on TigerHN
are shown in Table 3.

4.1 Which oracle is better?

The results in Table 3 show that the Eager ora-
cle works better than Lazy for discontinuous con-
stituents, but for continuous constituents (and over
all constituents on average) Lazy works better.
This can be explained by Lazy being very conser-
vative about swaps: since their number is signifi-

3https://github.com/andreasvc/
disco-dop

10 20 30 40 50 60 70 80
Sentence Length

0

50

100

150

200

M
ea

n 
nu

m
be

r o
f s

wa
ps

Eager
Lazy
Lazier

(a) Mean number of swaps per sentence length.

10 20 30 40 50 60 70 80
Sentence Length

10

0

10

20

30

40

50

M
ea

n 
siz

e 
of

 th
e 

sw
ap

 ju
m

p

Eager
Lazy
Lazier

(b) Mean size of the swap jumps per sentence length.

Figure 3: The effect of different swap strategies of
sentences with up to 80 words

cantly reduced, the transition becomes difficult to
predict, and thus the model gives up on predict-
ing swaps and concentrates on the statistics for
the projective operations. In other words, Lazy
predicts swaps only if the statistical evidence for
swaps is high. This can be seen by the contrast
between high precision but very low recall on dis-
continuous constituents.

Eager works in the opposite direction: since
it has observed many swaps it has a strong bias
to predict them, which leads to a high recall but
low precision. Lazier strikes a good balance be-
tween precision and recall on the discontinuous
constituents, and because of that it outperforms
both Eager and Lazy on F-score for both all con-
stituents and discontinuous constituents.

The good result of Lazier cannot be subscribed
only to the shorter transition sequences being eas-
ier to predict, because if it was up to the transition

1672



All Discontinuous
C

om
po

si
tio

n
fu

nc
tio

n

B
i-

L
ST

M
la

ye
rs

St
ac

k-
L

ST
M

O
ra

cl
e

P R F E P R F E
Tree-LSTM 2 Lazier 84.85 83.81 84.33 46.64 55.67 46.54 50.70 37.48

Head 2 Lazier 61.86 53.91 57.61 13.87 16.43 6.63 9.45 3.49
RecNN 2 Lazier 83.57 82.84 83.21 43.55 57.30 38.17 45.82 32.94

Tree-LSTM 0 Lazier 81.31 80.30 80.80 39.85 48.17 33.67 39.64 27.37
Tree-LSTM 1 Lazier 83.86 83.67 83.77 44.46 58.12 42.57 49.15 35.08
Tree-LSTM 2 top3 Lazier 82.29 81.07 81.68 40.64 54.39 31.49 39.89 26.57
Tree-LSTM 2 Lazy 84.80 83.39 84.09 45.65 59.35 36.24 45.00 31.88
Tree-LSTM 2 Eager 83.74 83.04 83.39 43.81 50.15 43.68 46.69 33.49

Table 3: Precision (P), Recall (R), F-score (F) and Exact (E), for our best model and ablated versions.

Negra Negra-All TigerHN TigerSPMRL
All L≤40 All L≤40 All

co
nv

2d
ep Hall and Nivre (2008) - - - 79.93 - -

Fernández-González and
Martins (2015) 82.56

? 81.08 80.52 85.53 84.22 80.62�

L
C

FR
S

van Cranenburgh (2012) - 72.33 71.08 - - -
van Cranenburgh and Bod
(2013) - 76.8 - - - -

Kallmeyer and Maier
(2013) 75.75 - - - - -

Tr
an

si
tio

n-
B

as
ed

Versley (2014) - - - 74.23 - -
Maier (2015) 76.95 - - 79.52 - 74.71
Maier and Lichte (2016) - - - 80.02 - 76.46
Coavoux and Crabbé
(2017) 82.46 82.76 82.16 85.11 84.01 81.60

This work 83.29 83.39 82.87 85.25 84.06 81.64

Table 4: Final results on test set, computed with discodop evaluation module. ?Trained on Negra-All.
�Evaluated with SPRML scripts.

sequences length alone then Lazy would work bet-
ter than Eager on the discontinuous constituents.
The more likely explanation is that Lazier intro-
duces an inductive bias in the model that is use-
ful for generalization, and that allows the model to
generalize better than Eager and Lazy.

We also quantified how many swaps are made
by Eager, Lazy and Lazier. Figure 3 shows the
statistics over the TigerHN training set for differ-
ent sentence lengths; the aggregated statistics over
all sentence lengths can be read in Table 2. We
can observe in Figure 3(a) that, in the case of short
sentences, all the swapping strategies give simi-
lar results, but as sentences get longer the number
of swaps in Eager gets much higher and more un-
stable than lazier alternatives. We found that for
some sentences Lazy and Lazier do with 2 swaps

what Eager does with 126 swaps. Compared to
Lazy, Lazier is much more stable in terms of the
number of swaps, which can be seen by the stan-
dard deviation in Table 2. In Figure 3(b) shows a
similar trend for the size of the jump of swap tran-
sitions. All the swaps of Eager make a jump of
size 1, while the jumps of Lazier can go up to 91
words.

4.2 What is the best word representation?
We have tested whether the representation of a
word based solely on its embeddings is enough
to get good results or, instead, this representation
should be refined by the bi-directional LSTM. Ta-
ble 3 shows that adding layers to the bi-directional
LSTM consistently improves the scores. The dif-
ference between not using a bi-directional LSTM
and using 2 layers of bi-directional LSTM is 3.63

1673



F-score, which is a big margin. Adding a third
layer did not improve scores significantly.

4.3 What is the best composition function?
We have tried three options for composition func-
tions: Head (use only the head word embedding
instead of a composition function), RecNN and
TreeLSTM – all presented in Section 3.2. As we
expected, the head representation alone did not
perform well, which shows that some type of com-
position function is needed. We find that using a
recursive model with a memory cell improves re-
sults by 1.12 F-score, and thus we settle for the
TreeLSTM composition function.

4.4 What is the best configuration
representation?

We tested two configuration representations: the
first one – top3 – takes the 3 topmost elements
from the stack and the buffer as the representa-
tives, while the second one – Stack-LSTM – mod-
els the whole content of the configuration via re-
current neural models. In line with our intuitions,
the Stack-LSTM, thanks to considering the whole
stack and buffer structure instead of only a few el-
ements, outperforms top3 by a margin of 2.65 F-
score points.

4.5 Comparison with other models
We took the version of our model that performed
the best on the TigerHN development set and com-
pared it on the four different datasets (two tree-
banks with two different splits) with other parsers.

In Table 4 we show the results compared to
the other works published on these datasets. Our
parser outperforms all the previously published
models on all datasets except TigerHN, where it
ends up second best after Fernández-González and
Martins (2015). As shown in our previous anal-
ysis, exploring alternative representations of the
different components has allowed us to construct a
better model. We must also notice that, when com-
paring to other models, one influential cause of
the good performance may be the capacity of our
model, provided by the neural architectures. Neu-
ral networks allow modeling relations from input
to output that are much more complex than those
captured by the approaches we compare to, most
of which use linear models based on perceptron or
simple PCFG type of generative models.

We have also tested our model on the predicted
POS tags from TigerSPMRL split, as provided in

TigerSPRML
F1 (spmrl.prm)
≤ 70 All

Versley (2014) 73.90 –
This work 77.25 76.96
F&M (2015) 77.72 77.32
Coavoux&Crabbé (2017) 79.44 79.26
Versley (2016) 79.84 79.50

Table 5: Results on SPMRL data with predicted
tags.

the shared task (Seddah et al., 2013). The results
are shown in Table 5. The biggest strength of our
model—its capacity— is in this case its biggest
weakness: it causes the model parameters to over-
fit the noisy predicted tags during training, be-
cause we have not used any form of regularization.
Model combinations like the one in Versley (2016)
do not suffer from this because they implicitly do
strong regularization. Our model could probably
achieve better results on this dataset with stronger
regularization, which we leave for future research.

5 Conclusion

We have presented the first neural model for
discontinuous constituency parsing that achieves
state-of-the-art results in three out of four standard
datasets for discontinuous parsing with gold POS
tags. Our findings suggest that i) bidirectional
LSTM should be used for refining the represen-
tations of terminals even in the cases when they
are going to be combined by a recursive model,
ii) the performance of the composition function
depends to a big extent on the availability of the
memory cells, to prevent the vanishing gradient,
iii) it is crucial to use all the elements in the stack
and buffer in the decision process instead of just
few elements on the top and iv) Lazier oracle gives
better and more stable results than Eager and Lazy
oracles on both continuous and discontinuous con-
stituents.

Acknowledgments

We are thankful to Andreas van Cranenburgh and
Wolfgang Maier for sharing the data and prepro-
cessing scripts, and to the anonymous reviewers
for suggestions that improved the paper. We are
grateful to the ILLC for providing us with travel-
ing funds to attend the conference.

1674



References
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.

2013. Polyglot: Distributed word representa-
tions for multilingual nlp. In Proceedings of the
Seventeenth Conference on Computational Natu-
ral Language Learning. Association for Computa-
tional Linguistics, Sofia, Bulgaria, pages 183–192.
http://www.aclweb.org/anthology/W13-3520.

Miguel Ballesteros, Chris Dyer, and Noah A. Smith.
2015. Improved transition-based parsing by model-
ing characters instead of words with lstms. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, Lisbon, Portugal,
pages 349–359. http://aclweb.org/anthology/D15-
1041.

Maximin Coavoux and Benoit Crabbé. 2017. Incre-
mental discontinuous phrase structure parsing with
the gap transition. In Proceedings of the 15th Con-
ference of the European Chapter of the Associa-
tion for Computational Linguistics. Association for
Computational Linguistics, Valencia, Spain.

Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10. Associa-
tion for Computational Linguistics, pages 1–8.

Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics. Association for Compu-
tational Linguistics, page 111.

James Cross and Liang Huang. 2016. Incremental
parsing with minimal features using bi-directional
lstm. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 32–37.
http://anthology.aclweb.org/P16-2006.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the 53rd An-
nual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Beijing, China, pages 334–343.
http://www.aclweb.org/anthology/P15-1033.

Daniel Fernández-González and André F. T. Martins.
2015. Parsing as reduction. In Proceedings of
the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers). Association for Compu-
tational Linguistics, Beijing, China, pages 1523–
1533. http://www.aclweb.org/anthology/P15-1147.

Christoph Goller and Andreas Küchler. 1996. Learning
task-dependent distributed representations by back-
propagation through structure. In Proceedings of
the International Conference on Neural Networks.
IEEE, pages 347–352.

Johan Hall and Joakim Nivre. 2008. Parsing discon-
tinuous phrase structure with grammatical functions.
In Proceedings of the 6th International Conference
on Natural Language Processing (GoTAL). pages
169–180.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Association for Computational Linguistics, Strouds-
burg, PA, USA, NAACL HLT ’12, pages 142–151.
http://dl.acm.org/citation.cfm?id=2382029.2382049.

Laura Kallmeyer and Wolfgang Maier. 2013. Data-
driven parsing using probabilistic linear context-
free rewriting systems. Computational Linguistics
39(1):87–119.

Eliyahu Kiperwasser and Yoav Goldberg. 2016.
Simple and accurate dependency parsing
using bidirectional lstm feature represen-
tations. Transactions of the Association
for Computational Linguistics 4:313–327.
https://transacl.org/ojs/index.php/tacl/article/view/885.

Phong Le and Willem Zuidema. 2015. Composi-
tional distributional semantics with long short
term memory. In Proceedings of the Fourth
Joint Conference on Lexical and Computa-
tional Semantics. Association for Computational
Linguistics, Denver, Colorado, pages 10–19.
http://www.aclweb.org/anthology/S15-1002.

Phong Le and Willem Zuidema. 2016. Quantifying
the vanishing gradient and long distance dependency
problem in recursive neural networks and recur-
sive LSTMs. In ACL Workshop on Representation
Learning for NLP.

Wang Ling, Chris Dyer, Alan W Black, Isabel Tran-
coso, Ramon Fermandez, Silvio Amir, Luis Marujo,
and Tiago Luis. 2015. Finding function in form:
Compositional character models for open vocabu-
lary word representation. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Compu-
tational Linguistics, Lisbon, Portugal, pages 1520–
1530. http://aclweb.org/anthology/D15-1176.

Wolfgang Maier. 2015. Discontinuous incremental
shift-reduce parsing. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint

1675



Conference on Natural Language Processing (Vol-
ume 1: Long Papers). Association for Compu-
tational Linguistics, Beijing, China, pages 1202–
1212. http://www.aclweb.org/anthology/P15-1116.

Wolfgang Maier and Timm Lichte. 2016. Discontinu-
ous parsing with continuous trees. In Proceedings of
the Workshop on Discontinuous Structures in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, San Diego, California, pages 47–
57. http://www.aclweb.org/anthology/W16-0906.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji,
Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku-
mar, Chaitanya Malaviya, Paul Michel, Yusuke
Oda, Matthew Richardson, Naomi Saphra, Swabha
Swayamdipta, and Pengcheng Yin. 2017. Dynet:
The dynamic neural network toolkit. arXiv preprint
arXiv:1701.03980 .

Joakim Nivre. 2009. Non-projective dependency
parsing in expected linear time. In Proceed-
ings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Lan-
guage Processing of the AFNLP. pages 351–
359. http://www.aclweb.org/anthology/P/P09/P09-
1040.pdf.

Joakim Nivre, Marco Kuhlmann, and Johan Hall. 2009.
An improved oracle for dependency parsing with on-
line reordering. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies (IWPT).
pages 73–76.

Djamé Seddah, Reut Tsarfaty, Sandra Kübler, Marie
Candito, Jinho D. Choi, Richárd Farkas, Jen-
nifer Foster, Iakes Goenaga, Koldo Gojenola Gal-
letebeitia, Yoav Goldberg, Spence Green, Nizar
Habash, Marco Kuhlmann, Wolfgang Maier, Joakim
Nivre, Adam Przepiórkowski, Ryan Roth, Wolf-
gang Seeker, Yannick Versley, Veronika Vincze,
Marcin Woliński, Alina Wróblewska, and Eric Ville-
monte de la Clergerie. 2013. Overview of the
SPMRL 2013 shared task: A cross-framework
evaluation of parsing morphologically rich lan-
guages. In Proceedings of the Fourth Workshop
on Statistical Parsing of Morphologically-Rich Lan-
guages. Association for Computational Linguis-
tics, Seattle, Washington, USA, pages 146–182.
http://www.aclweb.org/anthology/W13-4917.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term mem-
ory networks. In Proceedings of the 53rd An-
nual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers). Association for Compu-
tational Linguistics, Beijing, China, pages 1556–
1566. http://www.aclweb.org/anthology/P15-1150.

Andreas van Cranenburgh. 2012. Efficient pars-
ing with linear context-free rewriting systems.
In Proceedings of the 13th Conference of the
European Chapter of the Association for Com-
putational Linguistics. Association for Computa-
tional Linguistics, Avignon, France, pages 460–470.
http://www.aclweb.org/anthology/E12-1047.

Andreas van Cranenburgh and Rens Bod. 2013. Dis-
continuous parsing with an efficient and accurate
DOP model. In Proceedings of the 13th Interna-
tional Conference on Parsing Technologies (IWPT).

Yannick Versley. 2014. Experiments with easy-first
nonprojective constituent parsing. In Proceedings
of the First Joint Workshop on Statistical Parsing
of Morphologically Rich Languages and Syntac-
tic Analysis of Non-Canonical Languages. Dublin
City University, Dublin, Ireland, pages 39–53.
http://www.aclweb.org/anthology/W14-6104.

Yannick Versley. 2016. Discontinuity reˆ2-visited:
A minimalist approach to pseudoprojective con-
stituent parsing. In Proceedings of the Work-
shop on Discontinuous Structures in Natural Lan-
guage Processing. Association for Computational
Linguistics, San Diego, California, pages 58–69.
http://www.aclweb.org/anthology/W16-0907.

1676


