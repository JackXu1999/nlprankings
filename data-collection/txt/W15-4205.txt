



















































Reconciling Heterogeneous Descriptions of Language Resources


Proceedings of the 4th Workshop on Linked Data in Linguistics (LDL-2015), pages 39–48,
Beijing, China, July 31, 2015. c©2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing

Reconciling Heterogeneous Descriptions of Language Resources
John P. McCrae, Philipp Cimiano

CIT-EC, Bielefeld University
Bielefeld, Germany

{jmccrae, cimiano}@cit-ec.uni-bielefeld.de
Victor Rodrı́guez Doncel, Daniel Vila-Suero

Jorge Gracia
Universidad Politécnica de Madrid

Madrid, Spain
{vrodriguez, dvila, jgracia}@fi.upm.es

Luca Matteis, Roberto Navigli
University of Rome, La Sapienza

Rome, Italy
{matteis, navigli}@di.uniroma1.it

Andrejs Abele, Gabriela Vulcu
Paul Buitelaar

Insight Centre, National University of Ireland
Galway, Ireland

{andrejs.abele, gabriela.vulcu,
paul.buitelaar}@insight-centre.org

Abstract

Language resources are a cornerstone of
linguistic research and for the develop-
ment of natural language processing tools,
but the discovery of relevant resources re-
mains a challenging task. This is due to
the fact that relevant metadata records are
spread among different repositories and it
is currently impossible to query all these
repositories in an integrated fashion, as
they use different data models and vocab-
ularies. In this paper we present a first at-
tempt to collect and harmonize the meta-
data of different repositories, thus mak-
ing them queriable and browsable in an
integrated way. We make use of RDF
and linked data technologies for this and
provide a first level of harmonization of
the vocabularies used in the different re-
sources by mapping them to standard RDF
vocabularies including Dublin Core and
DCAT. Further, we present an approach
that relies on NLP and in particular word
sense disambiguation techniques to har-
monize resources by mapping values of at-
tributes – such as the type, license or in-
tended use of a resource – into normal-
ized values. Finally, as there are dupli-
cate entries within the same repository as
well as across different repositories, we
also report results of detection of these du-
plicates.

1 Introduction

Language resources are the cornerstone of linguis-
tic research as well as of computational linguistics.
Within NLP, for instance, most tools developed re-
quire a corpus to be trained (e.g. language models,

statistical taggers, statistical parsers, and statisti-
cal machine translation systems) or they require
lexico-semantic resources as background knowl-
edge to perform some task (e.g. word sense dis-
ambiguation). As the number of language re-
sources available keeps growing, the task of dis-
covering and finding resources that are pertinent
to a particular task becomes increasingly difficult.
While there are a number of repositories that col-
lect and index metadata of language resources,
such as META-SHARE (Federmann et al., 2012),
CLARIN (Broeder et al., 2010), LRE-Map (Cal-
zolari et al., 2012), Datahub.io1 and OLAC (Si-
mons and Bird, 2003), they do not provide a com-
plete solution to the discovery problem for two
reasons. First, integrated search over all these dif-
ferent repositories is not possible, as they use dif-
ferent data models, different vocabularies and ex-
pose different interfaces and APIs. Second, these
repositories must strike a balance between quality
and coverage, either opting for coverage at the ex-
pense of quality of metadata, or vice versa.

When collecting metadata from multiple re-
sources, we understand that there are two princi-
pal challenges: property harmonization and du-
plication detection. Harmonization is the chal-
lenge of verifying that there is not only struc-
tural and syntactic interoperability between the re-
sources in that they use the same property, for ex-
ample Dublin Core’s language property, but also
that they use the same value. For example, the fol-
lowing values of the language property are likely
to be equivalent: “French”, “Modern French”,
“français”, “fr”, “fra” and “fre”. It is difficult to
write queries on a dataset if every property has
many equivalent values and thus it is essential to
use a single representation. Secondly, we wish to

1http://datahub.io/

39



detect duplicates that occur either due to the orig-
inal representation or from multiple sources. It
is clear that if a large number of records in fact
describe the same resource then queries for that
resource will return too many resources that may
lead to errors (or annoyance) for users. For exam-
ple, the “Universal Declaration of Human Rights”
is available in 444 languages2 and listing each
translation as a single resource (as the CLARIN
VLO does) does not correctly capture the nature
of the resource. Furthermore, these resources may
not match some queries, such as for example ‘re-
sources in more than one language’, and as such
it is preferable to merge these individual records
into a single complex record.

As the main contribution of this paper, we
present the methods used to harmonize data across
repositories. Due to the different kinds of val-
ues and target taxonomies chosen for each prop-
erty, these methods vary but all are based on state-
of-the-art NLP techniques, including word sense
disambiguation, and make major improvements to
the data quality of our metadata records. Second,
we show indeed that duplicate metadata records
are pervasive and that they occur both within and
across repositories. We then present a simple yet
effective approach to detect duplicates within and
across repositories.

The paper is structured as follows: we give an
overview of work related to harmonization of data
as well as an overview of existing metadata repos-
itories for linguistic data in Section 2. We de-
scribe our metadata collection and schema match-
ing strategy in Section 3. We describe our tech-
niques for metadata harmonization in Section 4.
We describe our methods for duplication detec-
tion in Section 5. The performance of the different
techniques is reported in each of these sections.
We discuss our methodology and approach from a
wider point of view in Section 6.

2 Related Work

Interoperability of metadata is an important prob-
lem in many domains and harmonizing schemas
from different sources has been recognized as a
major challenge (Nilsson, 2010; Khoo and Hall,
2010; Nogueras-Iso et al., 2004). There are dif-
ferent approach to data integration. One approach
consists on mapping data to one monolithic on-

2http://www.ohchr.org/en/udhr/pages/
introduction.aspx

tology that needs to be general enough to ac-
commodate all the data categories from different
sources. While this is appealing as it supports in-
tegrated querying of data, a single ontology can-
not predict all aspects of metadata records, that
all users may wish to record. In contrast, the
linked data approach relies on multiple, standard-
ized smaller and reusable vocabularies, each rep-
resenting a subset of the data. In this line, some
experts have recommended (Brooks and McCalla,
2006):

“A larger set of ontologies sufficient for
particular purposes should be used in-
stead of a single highly constrained tax-
onomy of values.”

In the context of linguistic data, different ap-
proaches have been pursued to collect metadata of
resources. Large consortium-led projects and ini-
tiatives such as the CLARIN projects and META-
NET have attempted to create metadata standards
for representing linguistic data. Interoperability
of the data stemming from these two repositories
is however severely limited due to incompatibili-
ties in their data models. META-SHARE favors a
qualitative approach in which a relatively complex
XML schema is provided to describe metadata of
resources (Gavrilidou et al., 2012). At the same
time, considerable effort has been devoted to en-
suring data quality (Piperidis, 2012). In contrast,
CLARIN does not provide a single schema, but a
set of ‘profiles’ that are described in a schema lan-
guage called the CMDI Component Specification
Language (Broeder et al., 2012). Each institute
describing resources using CMDI can instantiate
the vocabulary to suit their particular needs. Sim-
ilarly, an attempt has been made to catalogue lan-
guage resources by assigning them a single unique
identifier (Choukri et al., 2012).

Other more decentralized approaches are found
in initiatives such as the LRE-Map (Calzolari
et al., 2012) which provides a repository for
researchers who want to submit the resources
accompanying papers submitted to conferences.
Most fields in LRE-Map consist of a text field with
some prespecified options to select and a thorough
analysis of the results has been conducted (Mari-
ani et al., 2014).

Similarly, the Open Linguistics Working
Group (Chiarcos et al., 2012) has been collecting
language resources published as linked data in a

40



Source Records RDF Triples Triples perRecord

META-SHARE 2,442 464,572 190.2
CLARIN VLO 144,570 3,381,736 23.4
Datahub.io 218 10,739 49.3
LRE-Map (LREC 2014) 682 10,650 15.6
LRE-Map (Non-open) 5,030 68,926 13.7
OLAC 217,765 2,613,183 12.0
ELRA Catalogue 1,066 22,580 21.2
LDC Catalogue 714 n/a n/a

Table 1: The sizes of the resources in terms of
number of metadata records and total data size

crowd-sourced repository at Datahub.io, in order
to monitor the Linguistic Linked Data cloud and
produce a diagram showing the status of these
resources.

This clearly shows that the field is very frag-
mented, with different players using different ap-
proaches and most importantly different meta- and
data models, thus impeding the discovery and in-
tegration of linguistic data.

3 Metadata collection and Schema
Matching

In this section we describe the different meth-
ods applied to collect metadata from the different
repositories:

• META-SHARE: For META-SHARE, a
dump of the data was provided by the ILSP
managing node of the META-NET project
in XML format. We developed a custom
script to convert this into the RDF data
model, explicitly aligning data elements to
the Dublin Core metadata vocabulary and
add these as extra RDF triples to the root
of the record. Frequently, these properties
were deeply nested in the XML file and man-
ual analysis was required to detect which in-
stances truly applied to the entire metadata
record.

• CLARIN: For CLARIN, we rely on the OAI-
PMH (Sompel et al., 2004) framework to har-
vest data. The harvested OAI-PMH records
comprise a header with basic information as
well as a download link and a secondary
XML description section that is structured
according to the particular needs of the data
provider. So far, we limit ourselves to collect-
ing only those records that have Dublin Core
properties.

• LRE-Map: For LRE-Map we used the avail-
able RDF/XML data dump3, which contains
submission information from the LREC 2014
conference, as well as data from other confer-
ences, which is not freely available. In the
RDF data, we gathered additional informa-
tion about language resources, including the
title of the paper describing the resource.

• Datahub.io: The data from Datahub.io was
collected by means of the CKAN API4. As
Datahub.io is a general-purpose catalogue we
limited ourselves to extracting only those re-
sources that were of relevance to linguistics.
For this, we used an existing list of rele-
vant categories and tags maintained by the
Working Group on Open Linguistics (Chiar-
cos et al., 2012). The data model used by
Datahub.io is also based on DCAT, so little
adaptation of the data was required.

• OLAC: The Open Language Archives Com-
munity also relies on OAI-PMH to collect
metadata and overlaps significantly with the
CLARIN VLO. Unfortunately the data on
this site is not openly licensed.

• ELRA and LDC Catalogues: These two or-
ganizations sell language resources and their
catalogues are available online. The metadata
records are not themselves openly licensed.

The total size in terms of records and triples
(facts) as well as the average number of triples
per repository are given in Table 1, where we can
see significant differences in size and complexity
of the resources. Note for the rest of this paper
we will concern ourselves only with the openly li-
censed resources.

4 Metadata harmonization

As metadata has been obtained from different
repositories, there are many incompatibilities be-
tween the values used in different resources.
While some repositories ensure high-quality meta-
data in general, we also discovered inconsisten-
cies in the use of values. For instance, while

3http://datahub.io/organization/
institute-for-computational-linguistics-ilc-cnr

4http://datahub.io/api/3/ documented
at http://docs.ckan.org/en/latest/api/
index.html

41



META-SHARE recommends the use of ISO 639-
35 tags for languages, a few data entries use En-
glish names for the language instead of the ISO
code. We describe our approach to data value
normalization below. In this initial harmonization
phase we focused on the key questions of whether
a resource is available, that is the given URL re-
solves, and whether the terms and conditions un-
der which the resource can be used are specified.
Further, we consider three key aspects that users
need to know about resources to help them de-
cide whether the resource matches their needs,
namely: the type of the resource (corpus, lexical
resource, etc.), intended use of the resource and
languages covered. We note that many resources
have multiple values for the same property (e.g.,
language), thus we allow multiple values at the
record level, while still permitting more specific
annotation deeper in the record.

4.1 Availability
In order to enable applications to (re)use language
resources, we should find out if the resources de-
scribed can still be accessed. For this we fo-
cused on the properties which were mapped to
DCAT’s ‘access URL’ property in the previous
section. These ‘access URLs’ are intended to re-
fer to HTML pages containing either download
links or information on how to retrieve and use
the resource. We augment the data with informa-
tion about which links are valid and about the form
of the content returned (e.g. HTML, XML, PDF,
RDF/XML, etc.). Therefore, as we deal with het-
erogeneous sources and repositories, we analyzed
access related characteristics and initially focused
on answering two questions: Is the language re-
source available and accessible on the Web and in
what format?.

To assess the current situation, we crawled and
performed an analysis on a set of 119,290 URLs 6.
Our analysis showed that more than 95% of the
URLs studied corresponded to accessible URLs
(i.e., HTTP Response Code 200 OK), which indi-
cates that in a high number of cases at least some
information is provided to potential consumers of
the resource.

Furthermore, our assessment showed that more
than 66% of the accessible URLs corresponds to
HTML pages, around 10% to RDF/XML docu-

5http://www-01.sil.org/iso639-3/
6Due to crawling restrictions, only 60% of the URLs of

the dataset were actually crawled

Format Resources Percentage
HTML 67,419 66.2%
RDF/XML 9,940 9.8%
JPEG Image 6,599 6.5%
XML (application) 5,626 5.6%
Plain Text 4,251 4.2%
PDF 3,641 3.6%
XML (text) 3,212 3.2%
Zip Archive 801 0.8%
PNG Image 207 0.2%
gzip Archive 181 0.2%

Table 2: The distribution of the 10 most used for-
mats within the analyzed sample of URLs. Note
XML is associated with two MIME types.

ments, and other non-text formats sum up to al-
most 10% of the URLs analyzed (see Table 2). It
is important to note that these results only describe
what was returned by the service, and do not well
reflect the actual format or availability of the data.
For example, the high number of resources return-
ing RDF/XML is mostly due to two CLARIN con-
tributing institutes adopting RDF for their meta-
data.

4.2 Rights

Language resources are generally protected by
copyright laws and they cannot be used against
the terms expressed by the rights holders. These
terms of use declare the actions that are autho-
rized (e.g. derive, distribute) and the applicable
conditions (e.g. attribution, the payment of a fee).
They are an essential requirement for the reuse
of a resource, but their automatic retrieval and
processing is difficult because of the many forms
they may adopt: rights information can appear
either as a textual notice or as structured meta-
data, can consist of a mere reference to a well-
known license (like an Open Data Commons or
Creative Commons license), or it can point to an
institution-specific document in a non-English lan-
guage. These heterogeneous practices prevent the
automated processing of licensing information.

Several challenges are posed for the harmon-
isation of the rights information: first, informa-
tion is often not legally specified but instead vague
statements such as ‘freely available’ are used; sec-
ond, description of specific rights and conditions
of each license requires complex modelling; and
finally, due to the sensitivity of the information,

42



only high precision approaches should be applied.
From the RDF License dataset (Rodriguez-

Doncel et al., 2014) we extracted the title, URI
and abbreviation of the most commonly used li-
censes in different forms, and searched for ex-
act matches normalizing for case, punctuation and
whitespace. This introduced some errors due
to dual-licensing schemes or misleading descrip-
tion were introduced. We manually evaluated all
matching licenses and found 95.8% of the recog-
nised strings were correctly matched. With this
approach we could identify matching licenses for
only 1% of the metadata entries. However, our
observations suggest that this is due to the unin-
formative content for the license attribute. Fur-
thermore, we note that more sophisticated meth-
ods have been shown to improve recall, but they do
this at the cost of precision (Cabrio et al., 2014).

4.3 Usage

The language resource usage indicates the purpose
and application for which the LR was created or
which it has since be used. For META-SHARE we
rely on the 83 values of the useNLPSpecific
property and for LRE-Map we have a more lim-
ited list of 28 suggested values and many more
user-provided free text entries, 3,985 in total (no
other source contained this information). We man-
ually mapped the 28 predefined values in LRE-
Map to one of the 83 values predefined in META-
SHARE. For the user-provided intended usage
values, we developed a matching algorithm that
identifies the corresponding META-SHARE in-
tended use values. First we tokenized the ex-
pressions, then we stemmed the tokens using the
Snowball stemmer (Porter, 2001), and we per-
formed a string inclusion match, i.e. checking
whether META-SHARE usages are included in
the free text entries. For some entries we re-
trieved several matches (e.g. ‘Document Classi-
fication, Text categorisation’ matched both ‘docu-
ment classification’ and ‘text categorisation’), as-
suming that in the case of multiple matches the
union of the intended usages was meant. With this
algorithm we identified 66 matches on a random
sample of 100 user-provided entries and they were
all correct matches. From the remaining 34 un-
matched entries, 16 were empty fields or non spe-
cific e.g. ‘not applicable’, ‘various uses’. Other
16 entries were too general to be mapped to an in-
tended use defined in the META-SHARE vocabu-

lary e.g. ‘testing’, ‘acquisition’. We had one false
negative ‘taggin pos’[sic] and one usage that is
not yet in META-SHARE ‘semantic system eval-
uation’. On this basis we had 98-99% accuracy
on the results. Following the aforementioned al-
gorithm we identified 65% matches on the entire
set of user-entries. We further investigated the re-
maining 35% non-matches and we identified fur-
ther intended use values that are not yet in META-
SHARE vocabulary, e.g. ‘entity linking’, ‘corpus
creation’, which we will suggest as extensions of
the META-SHARE vocabulary.

4.4 Language
To clean the names of languages contained in
metadata records, we aligned to the ISO 639-
3 standard. First we extracted all the language
labels from our records and obtained a total of
833 distinct language labels. Next we leveraged
two resources to map these noisy language la-
bels to standard ISO codes: (i) the official SIL
database7, which contains all the standard ISO
codes and their English names, and (ii) BabelNet8

(Navigli and Ponzetto, 2012), a large multilin-
gual lexico-semantic resource containing, among
others, translations and synonyms of various lan-
guage names along with their ISO codes.

To perform the mapping in an automatic man-
ner, we compared each of the 833 noisy language
labels against the language labels contained in SIL
and BabelNet using two string similarity algo-
rithms: the Dice coefficient string similarity algo-
rithm and the Levenshtein distance string metric.

Table 3 reports an excerpt of the results show-
casing in the first row a match for all cases, in the
second a match for BabelNet but not for SIL, and
in the third a mismatch for all. Furthermore, the
final row reports a mismatch from Levenshtein,
where ‘Turkish, Crimean’ is matched instead.

In order to measure the accuracy of each ap-
proach we tested the mapping algorithms against
a manually annotated dataset containing 100 lan-
guage labels and ISO codes. In Table 4, we
present the accuracy of our methods based on the
number of labels correctly identified (“label accu-
racy”) and the accuracy weighted for the number
of metadata records with that label (“instance ac-
curacy”). The best results are obtained using Ba-
belNet as the source of language labels. Babel-

7http://www-01.sil.org/iso639-3/
download.asp

8http://babelnet.org/

43



Input Expectedoutput
BabelNet

output
SIL

output

dice leven dice leven

Kurdish kur kur kur kur kur
rank – distance 1 0 1 0

label Kurdish Kurdish kurdish Kurdish

Bokmål nob nob nob bok* bdt*
rank – distance 1 0 0.57 3

label bokmål Bokmål bok Bokoto

Ñahñú
(Otomı́)

oto omq* otm* ttf* las*

rank – distance 0.38 8 0.35 7

label
Otomı́
Mangue

Eastern
Otomı́

tuotomb lama(togo)

Turkish
(Türkçe)

tur tur tur tur crh*

rank – distance 0.7 6 0.7 7

label Turkish TürkiyeTürkçesi turkish
Turkish,
Crimean

Table 3: Excerpt output of language mapping.
* indicates mismatches.

Resource LabelAccuracy
Instance
Accuracy

SIL dice coefficient 81% 99.50%
SIL levenshtein 72% 99.42%
BabelNet dice coefficient 91% 99.87%
BabelNet levenshtein 89% 99.85%
SIL + BabelNet
dice coefficient 91% 99.87%
levenshtein 89% 99.85%

Table 4: Accuracy of language mappings

Net is more accurate in matching language labels,
largely because it contains translations, synonyms
and obsolete spellings of most, even rare or di-
alectal, languages. SIL on the other hand only
contains the English representation of each ISO
code, failing to induce certain mappings. Further-
more, the Dice coefficient string similarity algo-
rithm yields more accurate results compared to the
Levenshtein distance metric. We hypothesize that
this is mainly due to the fact that the Dice coeffi-
cient is more lenient compared to the Levensthein
metric as it is insensitive to the order of words.
For instance, using Dice coefficient, the input la-
bel ‘Quechua de Cotahuasi (Arequipa)’ matches
‘Cotahuasi Quechua’ correctly. With the Leven-
shtein algorithm, however, using the same input as
earlier, the label ‘Quechua cajamarquino’ is mis-
takenly matched instead.

Overall, combining BabelNet and SIL yields the
same normalization accuracy as BabelNet alone.

Resource DuplicateTitles
Duplicate

URLs

CLARIN (same contributing institute) 50,589 20
Datahub.io 0 55
META-SHARE 63 967

Table 5: The number of intra-repository duplicate
labels and URLs for resources

Nonetheless, we can observe a slight decrease in
the average distance returned by the Levensthein
algorithm. The addition of a multilingual semantic
database, such as BabelNet, positively affects the
ability to match obsolete names in different lan-
guages.

4.5 Type

The type property is used primarily to describe
the kind of resource being described. For META-
SHARE, we can rely on the structure of resources
to extract one of four primary resource types,
namely, ‘Corpus’, ‘Lexical Conceptual Resource’,
‘Lexical Description’ and ‘Tool Service’. How-
ever, for the other sources considered in this pa-
per the type field permits free text input. In or-
der to enable users to query resources by type we
ran the Babelfy entity linking algorithm (Moro et
al., 2014) to identify entities in the string and then
manually selected elements from this list of enti-
ties that described the kind of resource, such as
‘corpus’. In this way we extracted, 143 categories
for language resources while still ensuring that
syntactic variations were accounted for. The top
10 categories extracted in this way were: ‘Sound’,
‘Corpus’, ‘Lexicon’, ‘Tool’ (software), ‘Instru-
mental Music’9, ‘Service’, ‘Ontology’, ‘Evalua-
tion’, ‘Terminology’ and ‘Translation software’.

5 Duplicate detection

As we are collecting and indexing metadata
records from different repositories, it is possi-
ble to find duplicates, that is records that de-
scribe the same actual resource. In fact, du-
plicate entries did not only occur across reposi-
tories (we dub these inter-repository duplicates)
but also within the same resource (referred to as
intra-repository duplicates). We expand the defi-
nition of inter-repository by noting that CLARIN
is sourced from a number of different contribut-

9These resources are in fact recordings of singing in
under-resourced languages

44



ing institutes and there are duplicates between in-
stitutes, thus we consider links between records
of different CLARIN institutes as inter-repository.
Similarly, there has been no attempt to manage du-
plicates in LRE-Map and so we handle all links
between LRE-Map records as inter-repository.

In order to detect duplicates, we rely on two
properties that should be unique across entries,
that is the title and the ‘access URL’. In Table 5
we show the number of records with duplicate ti-
tles or URLs. Manual inspection of these dupli-
cates yielded the following observations:

META-SHARE META-SHARE contains a num-
ber of duplicate titles. However, these title
duplicates seem to be errors in the export and
can thus be easily corrected.

CLARIN Many resources in CLARIN are de-
scribed across many records. For example, in
CLARIN there may be one different metadata
record for each chapter of a book or recording
within an audio or television collection, or in
at least one case (“The Universal Declaration
of Human Rights”) a record exists for each
language the resource is available in. Thus,
we decided to merge the entries which share
the same title and same contributing institute
in CLARIN.

Datahub.io The creation method of DataHub pre-
vents the creation of different entries with the
same title, so duplicate titles do not occur
in the data. However, we found a number
of entries having the same download URL.
This is due to the fact that different resources
share SPARQL endpoints or download pages,
but the records did not describe the same re-
source and so we did not merge these re-
sources.

Table 6 shows the number of resources with the
same title (Duplicate Titles), same URL (Dupli-
cate URLs) as well as same title and same URL
within and across repositories. We apply the fol-
lowing strategy in handling duplicates:

Intra-repository duplicates As intra-repository
duplicates are mostly either system errors or
series of closely related resources, we simply
merge the corresponding metadata entries. If
a property is one-to-one we take only the first
value.

Duplication Correct Unclear Incorrect
Titles 86 6 8
URLs 95 2 3
Both 99 1 0

Table 7: Precision of matching strategies from a
sample of 100

Property Record Count(As percentage of all records) Triples

Access URL 91,615 (91.6%) 191,006
Language 50,781 (50.7%) 98,267
Type 15,241 (15.2%) 17,894
Rights 3,080 (3.0%) 8915
Usage 3,397 (3.4%) 4,530

Table 8: Number of records and facts harmonized
by our methods

Inter-repository duplicates Inter-repository du-
plicates represent multiple records of the
same underlying resource, they are linked to
one another by the ‘close match’ property.

Note we do not remove duplicates from the
dataset we either combine them into a more struc-
tured record or mark them as deprecated.

We evaluate the precision of this approach on a
sample of 100 inter-repository entries identified as
duplicates according to the above mentioned ap-
proach. We manually classify the matches into
correct, incorrect as well as unclear, if there was
insufficient information to make a decision, the
resources overlapped or were different versions
of each other. Table 7 shows these results. We
see that with 99% precision the method iden-
tifying duplicates if both title and URL match
yields the best results. While the recall is diffi-
cult to assess, an analysis of the data quickly re-
veals that there are many duplicates not detected
using this method. For example, for the Stan-
ford Parser (De Marneffe et al., 2006), we find
metadata records with all of the following titles:
“Stanford Parser”, “Stanford Dependency Parser”,
“Stanford Lexicalized Parser”, “Stanford’s NLP
Parser”, “The Stanford Parser”, “The Stanford
Parser: A Lexicalized Parser”.

6 Discussion

The rapid developments of natural language pro-
cessing technologies in the last few years has re-
sulted in a very large number of language re-
sources being created and made available on the

45



Resource Resource Duplicate Titles Duplicate URLs Both
CLARIN CLARIN (other contributing institute) 1,202 2,884 0
CLARIN Datahub.io 1 0 0
CLARIN LRE-Map 72 64 0
CLARIN META-SHARE 1,204 1,228 28
Datahub.io LRE-Map 59 5 0
Datahub.io META-SHARE 3 0 0
LRE-Map LRE-Map 763 454 359
LRE-Map META-SHARE 91 51 0
All All 3,395 4,686 387

Table 6: Number of duplicate inter-repository records by type

web. In order to enable these resources to be
reused appropriately it is necessary to properly
document resources and make this available as
structured, queriable metadata on the Web. Cur-
rent approaches to metadata collection are ei-
ther curatorial, where dedicated workers maintain
metadata of high quality, such as the approach
employed by META-SHARE. This approach en-
sures metadata quality but is very expensive and
as such it is unlikely that it will be able to han-
dle the vast number of resources published every
year. In contrast, crowd-sourced resources rely
primarily on self-reporting of metadata, and this
approach has a high recall but is very error-prone
and this unreliability can be plainly seen in re-
sources such as LRE-Map. In this paper, we have
aimed to break this dichotomy by aggregating re-
sources from both curated and crowd-sourced re-
sources, and applied natural language processing
techniques to provide a basic level of compliance
among these metadata records, and have achieved
this for a large number of records as summarized
in table 8. In this sense we have considered a small
set of properties that we regard as essential for the
description and discovery of relevant language re-
source, that is: resource type, language, intended
use, and licensing conditions. For the language
property we have shown that it can be harmonized
across repositories with high accuracy by mapping
values to a controlled vocabulary list, although the
data indicated that there were still many languages
which were not covered in the ISO lists. For the
type, rights and usage properties, whose content
is not as limited, it is harder to harmonize but we
were still able to show that in many cases these
results can be connected to known lists of val-
ues. This is important as it would allow for easier
queries of the resource.

Besides harmonizing values of data, we see two
further key aspects to ensure quality of the meta-
data. First, broken links should be avoided as
they are indicators of low curation and low quality.
Thus, we automatically detect such broken URLs
and remove them from the dataset. A second cru-
cial issue is the removal of duplicates, which are
also a sign of low quality.

We have investigated different strategies for de-
tecting duplicates. We observed that the case in
which two metadata records have been provided to
different repositories is common. When integrat-
ing data from different repositories, these entries
become duplicated. In other cases, particularly in
CLARIN, different metadata records are created
for parts of a resource. Genuine duplication likely
affects about 7% of records, underlining the value
of collecting resources from multiple sources. We
further note that it is important to take a high pre-
cision approach to deduplication as the merging
of non-duplicate resources can hide resources en-
tirely from the query. Thus, we have proposed
high-precision methods for detecting such dupli-
cates.

Finally, we note that the data resulting from this
process is available under the Creative Commons
Attribution Non-Commercial ShareALike License
and the data can be queried through a portal, which
is available at URL anonymized. Furthermore,
all code described in this paper is accessible from
a popular open source repository.10

7 Conclusion

We have studied the task of harmonizing records
of language resources that are heterogeneous on
several levels and have shown that the applica-

10To remain anonymous we cannot include URLs for these
resources at this point

46



tion of NLP techniques allows to provide common
metadata that will better enable users to find lan-
guage resources for their specific applications. We
note that this work is still on-going and should be
improved in not only the accuracy and coverage
of harmonization, but also in the number of prop-
erties that are harmonized (authorship and sub-
ject topic are planned). We hope that this new
approach to handling language resource metadata
will better enable users to find language resources
and assist in the creation of new domains of study
in computational linguistics.

Acknowledgments

LingHub was made possible due to significant
help from a large number of people, in partic-
ular we would like to thank the following peo-
ple: Benjamin Siemoneit (Bielefeld University),
Tiziano Flati (University of Rome, La Sapienza),
Martin Brümmer (University of Leipzig), Se-
bastian Hellmann (University of Leipzig), Bet-
tina Klimek (University of Leipzig), Penny
Labropolou (IEA-ILSP), Juli Bakagianni (IEA-
ILSP), Stelios Piperidis (IEA-ILSP), Nicoletta
Calzolari (ILC-CNR), Riccardo del Gratta (ILC-
CNR), Marta Villegas (Pompeu Fabra), Núria Bel
(Pompeu Fabra), Asunción Gómez-Pérez (Univer-
sidad Politécnica de Madrid) and Christian Chiar-
cos (Goethe-University Frankfurt).

This work was funded by the LIDER (”Linked
Data as an enabler of cross-media and multilin-
gual content analytics for enterprises across Eu-
rope”), an FP7 project refererence number 610782
in the topic ICT-2013.4.1: Content analytics and
language technologies.

References
Daan Broeder, Marc Kemps-Snijders, Dieter Van Uyt-

vanck, Menzo Windhouwer, Peter Withers, Peter
Wittenburg, and Claus Zinn. 2010. A data category
registry-and component-based metadata framework.
In Proceedings of the 7th International Conference
on Language Resources and Evaluation, pages 43–
47.

Daan Broeder, Menzo Windhouwer, Dieter Van Uyt-
vanck, Twan Goosen, and Thorsten Trippel. 2012.
CMDI: a component metadata infrastructure. In De-
scribing LRs with metadata: towards flexibility and
interoperability in the documentation of LR work-
shop programme, page 1.

Christopher Brooks and Gord McCalla. 2006. To-
wards flexible learning object metadata. Continu-

ing Engineering Education and Lifelong Learning,
16(1/2):50–63.

Elena Cabrio, Alessio Palmero Aprosio, and Serena
Villata. 2014. These are your rights: A natural
language processing approach to automated RDF li-
censes generation. In The Semantic Web: Trends
and Challenges, pages 255–269. Springer.

Nicoletta Calzolari, Riccardo Del Gratta, Gil Fran-
copoulo, Joseph Mariani, Francesco Rubino, Irene
Russo, and Claudia Soria. 2012. The LRE Map.
Harmonising community descriptions of resources.
In Proceedings of the 8th International Confer-
ence on Language Resources and Evaluation, pages
1084–1089.

Christian Chiarcos, Sebastian Hellmann, and Sebas-
tian Nordhoff. 2012. The Open Linguistics
Working Group of the Open Knowledge Founda-
tion. In Linked Data in Linguistics, pages 153–160.
Springer.

Khalid Choukri, Victoria Arranz, Olivier Hamon, and
Jungyeul Park. 2012. Using the international stan-
dard language resource number: Practical and tech-
nical aspects. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation, pages 50–54.

Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of LREC, volume 6, pages
449–454.

Christian Federmann, Ioanna Giannopoulou, Christian
Girardi, Olivier Hamon, Dimitris Mavroeidis, Sal-
vatore Minutoli, and Marc Schröder. 2012. META-
SHARE v2: An open network of repositories for
language resources including data and tools. In
Proceedings of the 8th International Conference on
Language Resources and Evaluation, pages 3300–
3303.

Maria Gavrilidou, Penny Labropoulou, Elina De-
sipri, Stelios Piperidis, Harris Papageorgiou, Monica
Monachini, Francesca Frontini, Thierry Declerck,
Gil Francopoulo, Victoria Arranz, et al. 2012. The
META-SHARE metadata schema for the description
of language resources. In Proceedings of the 8th In-
ternational Conference on Language Resources and
Evaluation, pages 1090–1097.

Michael Khoo and Catherine Hall. 2010. Merging
metadata: a sociotechnical study of crosswalking
and interoperability. In Proceedings of the 10th
annual joint conference on Digital libraries, pages
361–364. ACM.

Joseph Mariani, Christopher Cieri, Gil Francopoulou,
Patrick Paroubek, and Marine Delaborde. 2014.
Facing the identification problem in language-
related scientific data analysis. In Proceedings of
the 9th International Conference on Language Re-
sources and Evaluation, pages 2199–2205.

47



Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity linking meets word sense disam-
biguation: a unified approach. Transactions of the
Association for Computational Linguistics (TACL),
2:231–244.

Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217–
250.

Mikael Nilsson. 2010. From interoperability to har-
monization in metadata standardization. Ph.D. the-
sis, Royal Institute of Technology.

Javier Nogueras-Iso, F Javier Zarazaga-Soria, Javier
Lacasta, Rubén Béjar, and Pedro R Muro-Medrano.
2004. Metadata standard interoperability: applica-
tion in the geographic information domain. Com-
puters, environment and urban systems, 28(6):611–
634.

Stelios Piperidis. 2012. The META-SHARE language
resources sharing infrastructure: Principles, chal-
lenges, solutions. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation, pages 36–42.

Martin F Porter. 2001. Snowball: A language for stem-
ming algorithms.

Victor Rodriguez-Doncel, Serena Villata, and Asun-
cion Gomez-Perez. 2014. A dataset of RDF li-
censes. In Rinke Hoekstra, editor, Proceedings of
the 27th International Conference on Legal Knowl-
edge and Information System, pages 187–189.

Gary Simons and Steven Bird. 2003. The open lan-
guage archives community: An infrastructure for
distributed archiving of language resources. Liter-
ary and Linguistic Computing, 18(2):117–128.

Herbert van de Sompel, Michael L Nelson, Carl
Lagoze, and Simeon Warner. 2004. Resource har-
vesting within the OAI-PMH framework. D-Lib
Magazine, 10(12).

48


