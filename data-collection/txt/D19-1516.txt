




















































What You See is What You Get: Visual Pronoun Coreference Resolution in Dialogues


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 5123–5132,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

5123

What You See is What You Get:
Visual Pronoun Coreference Resolution in Dialogues

Xintong Yu♥∗, Hongming Zhang♣∗, Yangqiu Song♣, Yan Song♠, and Changshui Zhang♥

♥Institute for Artificial Intelligence, Tsinghua University (THUAI),

Department of Automation, Tsinghua University
♣Department of CSE, The Hong Kong University of Science and Technology

♠Sinovation Ventures

yuxt16@mails.tsinghua.edu.cn, hzhangal@cse.ust.hk, yqsong@cse.ust.hk,

clksong@gmail.com, zcs@mail.tsinghua.edu.cn

Abstract

Grounding a pronoun to a visual object it

refers to requires complex reasoning from var-

ious information sources, especially in conver-

sational scenarios. For example, when peo-

ple in a conversation talk about something all

speakers can see, they often directly use pro-

nouns (e.g., it) to refer to it without previous

introduction. This fact brings a huge chal-

lenge for modern natural language understand-

ing systems, particularly conventional context-

based pronoun coreference models. To tackle

this challenge, in this paper, we formally de-

fine the task of visual-aware pronoun corefer-

ence resolution (PCR) and introduce VisPro,

a large-scale dialogue PCR dataset, to investi-

gate whether and how the visual information

can help resolve pronouns in dialogues. We

then propose a novel visual-aware PCR model,

VisCoref, for this task and conduct compre-

hensive experiments and case studies on our

dataset. Results demonstrate the importance

of the visual information in this PCR case and

show the effectiveness of the proposed model.

1 Introduction

The question of how human beings resolve pro-

nouns has long been an attractive research topic

in both linguistics and natural language process-

ing (NLP) communities, for the reason that pro-

noun itself has weak semantic meaning (Ehrlich,

1981) and the correct resolution of pronouns re-

quires complex reasoning over various informa-

tion. As a core task of natural language under-

standing, pronoun coreference resolution (PCR)

(Hobbs, 1978) is the task of identifying the noun

(phrase) that pronouns refer to. Compared with

the general coreference resolution task, the string-

matching methods are no longer effective for pro-

nouns (Stoyanov et al., 2009), which makes PCR

∗ Equal contribution.

A: What are they doing?

B: Probably celebrating some 

holiday with such a big cake.

A: Can you read the writing on 

it?

B: I can’t tell.
A: What is it behind the cake?

the women

the cakethe statue

Figure 1: An example of a visual-related dialogue. Two

people are discussing the view they can both see. Pro-

nouns and noun phrases referring to the same entity are

marked in same color. The first “it” in the dialogue la-

beled with blue color refers to the object “the big cake”

and the second “it” labeled with green color refers to

the statue in the image.

more challenging than the general coreference res-

olution task.

Recently, great efforts have been devoted into

the coreference resolution task (Raghunathan

et al., 2010; Clark and Manning, 2015, 2016;

Lee et al., 2018) and good performance has been

achieved on formal written text such as newspa-

pers (Pradhan et al., 2012; Zhang et al., 2019a) and

diagnose records (Zhang et al., 2019b). However,

when it comes to dialogues, where more abundant

information is needed, the performance of exist-

ing models becomes less satisfying. The reason

behind is that, different from formal written lan-

guage, correct understanding of spoken language

often requires the support of other information

sources. For example, when people chat with each

other, if they intend to refer to some object that all

speakers can see, they may directly use pronouns

such as “it” instead of describing or mentioning

it in the first place. Sometimes, the object (name

or text description) that pronouns refer to may



5124

not even appear in a conversation, and thus one

needs to ground the pronouns into something out-

side the text, which is extremely challenging for

conventional approaches purely based on human-

designed rules (Raghunathan et al., 2010) or con-

textual features (Lee et al., 2018).

A visual-related dialogue is shown in Figure 1.

Both A and B are talking about a picture, in which

several people are celebrating something. In the

dialogue, the first “it” refers to the “the big cake,”

which is relatively easy for conventional models,

because the correct mention just appears before

the targeting pronoun. However, the second “it”

refers to the statue in the image, which does not

appear in the dialogue at all. Without the support

of the visual information, it is almost impossible to

identify the coreference relation between “it” and

“the statue.”

In this work, we focus on investigating how vi-

sual information can help better resolve pronouns

in dialogues. To achieve this goal, we first cre-

ate VisPro, a large-scale visual-supported PCR

dataset. Different from existing datasets such as

ACE (NIST, 2003) and CoNLL-shared task (Prad-

han et al., 2012), VisPro is annotated based on

dialogues discussing given images. In total, Vis-

Pro contains annotations for 29,722 pronouns ex-

tracted from 5,000 dialogues. Once the dataset

is created, we formally define a new task, visual

pronoun coreference resolution (Visual PCR), and

design a novel visual-aware PCR model VisCoref,

which can effectively extract information from im-

ages and leverage them to help better resolve pro-

nouns in dialogues. Particularly, we align men-

tions in the dialogue with objects in the image and

then jointly use the contextual and visual informa-

tion for the final prediction.

The contribution of this paper is three-folded:

(1) we formally define the task of visual

PCR; (2) we present VisPro, the first dataset

that focuses on PCR in visual-supported di-

alogues; (3) we propose VisCoref, a visual-

aware PCR model. Comprehensive experi-

ments and case studies are conducted to demon-

strate the quality of VisPro and the effective-

ness of VisCoref. The dataset, code, and mod-

els are available at: https://github.com/

HKUST-KnowComp/Visual_PCR.

Figure 2: Example syntax parsing result of the sentence

“A man with a dog is walking on the grass.”

2 The VisPro Dataset

To generate a high-quality and large-scale visual-

aware PCR dataset, we select VisDial (Das et al.,

2017) as the base dataset and invite annotators to

annotate. In VisDial, each image is accompanied

by a dialogue record discussing that image. One

example is shown in Figure 1. In addition, Vis-

Dial also provides a caption for each image, which

brings more information for us to create VisPro1.

In this section, we introduce the details about the

dataset creation in terms of pre-processing, survey

design, annotation, and post-processing.

2.1 Pre-processing

To make the annotation task clear to annotators

and help them provide accurate annotation, we

first extract all the noun phrases and pronouns with

Stanford Parser (Klein and Manning, 2003) and

then provide the extracted noun phrases as candi-

date mentions to annotate on. To avoid the over-

lap of candidate noun phrases, we choose noun

phrases with a height of two in parse trees. One

example is shown in Figure 2. In the syntactic tree

for the sentence “A man with a dog is walking on

the grass,” we choose “A man,” “a dog” and “the

grass” as candidates. If the height of noun phrases

is not limited, then the noun phrase “A man with

a dog” will cover “A man” and “a dog,” leading to

confusion in the options.

Following (Strube and Müller, 2003; Ng, 2005),

we only select third-person personal (it, he, she,

they, him, her, them) and possessive pronouns (its,

his, her, their) as the targeting pronouns. In to-

tal, the VisPro dataset contains 29,722 pronouns of

5,000 dialogues selected from 133,351 dialogues

in VisDial v1.0 (Das et al., 2017). We choose di-

alogues in which the number of pronouns ranges

from four to ten for the following reasons. For

one thing, dialogues with few pronouns are of lit-

1The information contained in the caption only helps pro-
vide noun phrase candidates for workers to annotate and will
not be treated as part of the dialogue.

https://github.com/HKUST-KnowComp/Visual_PCR
https://github.com/HKUST-KnowComp/Visual_PCR


5125

Figure 3: Distribution of pronouns in VisPro.

tle use to the task. For another, dialogues with too

many pronouns often contain repeating pronouns

referring to the same object, which makes the task

too easy. The dialogues selected contain 5.94 pro-

nouns on average. Figure 3 presents the distribu-

tion of different pronouns. From the figure we can

see that “it” and “they” are used most frequently

in the dialogues.

2.2 Survey Design

We divide 29,722 pronouns from 5,000 dialogues

into 3,304 surveys. In each survey, besides the

normal questions, we also include one checkpoint

question to control the annotation quality2. In to-

tal, each survey contains ten questions (nine nor-

mal questions and one checkpoint question). Each

question is about one pronoun.

The survey consists of three parts. We begin

by explaining the task to the annotators, including

how to deal with particular cases such as multi-

word expressions. Then, we present examples to

help the annotators better understand the task. Fi-

nally, we invite them to provide annotations for the

pronouns in the dialogues.

One example of the annotation interface is

shown in Figure 4. The text and the image of the

dialogue are displayed on the left and right panel,

respectively. For each of the targeting pronoun,

the workers are asked to select all the mentions

that it refers to. If any of the noun phrases is se-

lected, the reference type of the pronoun on the

right panel will be set to “noun phrases in text” au-

tomatically, and vice versa. If the concept that the

pronoun refers to is not available in the options,

or the pronoun is not referring to anything in par-

2We design the checkpoint dialogue straightforward and
unambiguous such that any responsive worker can easily pro-
vide the correct annotation.

Figure 4: Annotation interface on MTurk. Workers are

asked to select the anaphoricity type of the highlighted

pronoun in the right panel and all the antecedents of an

anaphoric pronoun on the left panel.

ticular, workers are asked to choose “some con-

cepts not present in text” or “the pronoun is non-

referential” on the right panel accordingly. The

nine normal questions in each survey are consecu-

tive so that pronouns in the same dialogue are dis-

played sequentially.

2.3 Annotation

We employ the Amazon Mechanical Turk plat-

form (MTurk) for our annotations.

We require that our annotators have more than

1,000 approved tasks and a task approval rate more

than 90%. They are also asked to pass a simple

test of pronoun resolution to prove that they un-

derstand the instruction of the task. Based on these

criteria, we identify 186 valid annotators. For each

dialogue, we invite at least four different workers

to annotate. In total, we collect 122,443 annota-

tions at a total cost of USD 3,270.80. We sup-

port the multiple participation of annotators by en-

suring that subsequent surveys are generated with

their previously-unlabelled dialogues.

2.4 Post-processing

Before processing the annotation result, we first

exclude the annotation of workers who fail to an-

swer the checkpoint questions correctly. As a re-

sult, 116,300 annotations are kept, which is 95%

of the overall annotation results. We then decide

the gold mentions that pronouns refer to using the

following procedure:

• Step one: We look into the annotations of each



5126

worker to find out the coreference clusters he

annotates for each dialogue. To achieve this

goal, we merge the intersecting sets of noun

phrase antecedents for pronouns in the same di-

alogue. We observe that annotators often make

the right decision for noun phrases near the

anaphor pronoun, but neglect antecedents far

away. It also happens in the annotation of other

coreference datasets (Chen et al., 2018). There-

fore, we generate clusters from different pro-

nouns in the same dialogue rather than merging

antecedents for each pronoun separately. If an

entity is mentioned and referred to by pronouns

for multiple times in the dialogue, combining

the antecedents for all pronouns could create a

more accurate coreference cluster for the entity.

• Step two: We adjudicate the coreference clus-

ters for the dialogue by majority voting within

all workers.

• Step three: We then decide the anaphoric type of

all pronouns by voting. If a pronoun is consid-

ered to refer to somef noun phrases in the text,

we find out the coreference cluster it belongs to

and choose the noun phrases in the cluster that

precede it as its antecedents.

• Step four: We randomly split the collected

data into train/val/test sets of 4,000/500/500 di-

alogues, respectively.

After collecting the data, we found out that

73.43% of pronouns act as an anaphor to some

noun phrases, 5.67% of pronouns do not have a

suitable antecedent, and the rest 20.90% are not

referential. Among all the pronouns that have

noun phrases as antecedents, 13.45% of them

do not have an antecedent in the dialogue con-

text.3 For anaphoric pronouns, each has 2.06 an-

tecedents on average. In the end, we calculate the

inner-annotator agreement (IAA) to demonstrate

the quality of the resulting dataset. Following con-

ventional approaches (Pradhan et al., 2012), we

use the average MUC score between individual

workers and the adjudication result as the evalu-

ation metric. The final IAA score is 72.4, which

indicates that the workers can clearly understand

our task and provide reliable annotation.

3The antecedent labeled by the worker is provided by the
caption.

is he wearingWhat

Object Detection

manA

+

  

  

  

?

                    
    

Person Pizza

    

Figure 5: Overall structure of the VisCoref model. Text

embedding e is acquired via LSTM and inner-span at-

tention mechanism. Object labels are detected from the

image and the possibility b of each text span referring

to them is calculated. The contextual and visual score

Fc(n, p) and Fv(n, p) are calculated upon the features,
and the final score F (n, p) is their weighted sum.

3 The Task

In this work, we focus on jointly leveraging the

contextual and visual information to resolve pro-

nouns. Thus, we formally define the visual-aware

pronoun coreference resolution as follows:

Given an image I , a dialogue record D which

discusses the content in I , and an external mention

set M, for any pronoun p that appears in D, the

goal is to optimize the following objective:

J =

∑
c∈C e

F (c,p,I,D)

∑
s∈S e

F (s,p,I,D)
, (1)

where F (·) is the overall scoring function of p
refers to c given I and D. c and s denote the

correct mention and the candidate mention, and C

and S denote the correct mention set and the can-

didate mention set, respectively. Note that in the

case where no golden mentions are annotated, the

union of all possible spans in D and M are used

to form S .

4 The Model

The overall framework of the proposed model Vis-

Coref is presented in Figure 5. In VisCoref, we

want to leverage both the contextual and visual in-

formation to resolve pronouns. Thus we split the

scoring function into two parts as follows:

F (n, p) = (1− λvis) · Fc(n, p) + λvis · Fv(n, p),
(2)



5127

where Fc(·) and Fv(·) are the scoring functions
based on contextual and visual information re-

spectively. λvis is the hyper-parameter to control

the importance of visual information in the model.

The details of the two scoring functions are de-

scribed in the following subsections.

4.1 Contextual Scoring

Before computing Fc, we first need to encode the

contextual information into all the candidates and

targeting pronouns through a mention representa-

tion module, which is shown as the dotted box in

Figure 5.

Following (Lee et al., 2018), a standard bidirec-

tional LSTM (BiLSTM) (Hochreiter and Schmid-

huber, 1997) is used to encode each span with at-

tentions (Bahdanau et al., 2015). Assume initial

embeddings of words in a span si are denoted as

x1, ...,xT , and their encoded representation after

BiLSTM as x∗1, ...,x
∗
T , the weighted embeddings

of each span x̂i are obtained by

x̂i =

T∑

t=1

at · xt, (3)

where at is the inner-span attention computed by

at =
eαt

∑T
k=1 e

αk
, (4)

in which αt is obtained by a standard feed-forward

neural network4 αt = NNα(x
∗
t ).

After that, we concatenate the embeddings of

the starting word (x∗start) and the ending word

(x∗end) of each span, as well as its weighted em-

bedding (x̂i) and the length feature (φ(i)) to form
its final representation e:

ei = [x
∗
start,x

∗
end, x̂i, φ(i)]. (5)

On top of the extracted mention representation,

we then compute the contextual score as follows:

Fc(n, p) = NNc([ep, en, ep ⊙ en]), (6)

where [ ] represents the concatenation, ep and en
are the mention representation of the targeting pro-

noun and current candidate mention, and ⊙ indi-

cates the element-wise multiplication.

4We use NN to represent feed-forward neural networks.

4.2 Visual Scoring

In order to align mentions in the text with objects

in the image, the first step of leveraging the visual

information is to recognize the objects from the

picture. We use a object detection module to iden-

tify object labels from each image I , such as “per-

son,” “car,” or “dog.” After that, we convert the

identified labels into vector representations fol-

lowing the same encoding process in 4.1. For each

image, we add a label “null,” indicating that the

pronoun is referring to none of the detected ob-

jects in I . We denote the resulting embeddings as

ec1 , ec2 , ..., ecK , in which ci denotes the detected

labels, and K is the total number of unique labels

in the corresponding image.

After extracting objects from the image, we

would like to know whether the mentions are re-

ferring to them. To achieve this goal, we calculate

the possibility of a mention ni corresponding to

each detected object ck:

βni,ck = NNβ (NNo(eni)⊙NNo(eck)) . (7)

Then we take the softmax of βni,ck as the final pos-

sibility of ni aligned with the object label ck:

bni,ck =
eβni,ck

∑K
l=1 e

βni,cl
. (8)

If ni corresponds to a certain object in I , the

score of that label should be large. Otherwise, the

possibility of “null” should be the largest. There-

fore, we use the maximum of possibility scores

among all K classes except “null”

mi = max
k=1,...,K

bni,ck (9)

as the probability of ni related to some object in I .

Similarly, given two mentions ni and nj , if they

refer to the same detected object ck, then both

bni,ck and bnj ,ck should be large. Thus, we can use

the maximum of their product among all K classes

except “null”

mi,j = max
k=1,...,K

bni,ck · bnj ,ck (10)

as the probability of ni and nj related to the same

object in I .

In the end, we define the visual scoring function

as follows:

Fv(n, p) = NNv([mp,mn,mp ·mn,mp,n]).
(11)



5128

5 The Experiment

In this section, we introduce the implementation

details, experiment setting, and baseline models.

5.1 Experiment Setting

As introduced in Section 2.4, we randomly split

the data into training, validation, and test sets of

4,000, 500, and 500 dialogues, respectively. For

each dialogue, a mention pool of size 30 is pro-

vided for models to detect plausible mentions out-

side the dialogue. The pool contains both men-

tions extracted from the corresponding caption

and randomly selected negative mention samples

from other captions. All models are evaluated

based on the precision (P), recall (R), and F1

score. Last but not least, we split the test dataset

by whether the correct antecedents of the pronoun

appear in the dialogue or not. We denote these two

groups as “Discussed” and “Not Discussed.”

5.2 Implementation Details

Following previous work (Lee et al., 2018), we

use the concatenation of the 300d GloVe embed-

ding (Pennington et al., 2014) and the ELMo (Pe-

ters et al., 2018) embedding as the initial word

representations. Out-of-vocabulary words are

initialized with zero vectors. We adopt the

“ssd resnet 50 fpn coco” model from Tensorflow

detection model zoo5 as the object detection mod-

ule. The size of hidden states in the LSTM module

is set to 200, and the size of the projected embed-

ding for computing similarity between text spans

and object labels is 512. The feed-forward net-

works for contextual scoring and visual scoring

have two 150-dimension hidden layers and one

100-dimension hidden layer, respectively.

For model training, we use cross-entropy as the

loss function and Adam (Kingma and Ba, 2015)

as the optimizer. All the parameters are initialized

randomly. Each mention selects the text span of

the highest overall score among all previous text

spans in the dialogue or the mention pool as its

antecedent, so that all mentions in one dialogue

are clustered into several coreference chains. The

noun phrases in the same coreference cluster as a

pronoun are deemed as the predicted antecedents

of that pronoun. The models are trained with up to

50,000 steps, and the best one is selected based on

its performance on the validation set.

5https://github.com/tensorflow/models/

tree/master/research/object_detection

5.3 Baseline Methods

Since we are the first to proposed a visual-aware

model for pronoun coreference resolution, we

compare our results with existing models of gen-

eral coreference resolution.

• Deterministic model (Raghunathan et al., 2010)

is a rule-based system that aggregates multiple

functions for determining whether two mentions

are coreferent based on hand-craft features.

• Statistical model (Clark and Manning, 2015)

learns upon human-designed entity-level fea-

tures between clusters of mentions to produce

accurate coreference chains.

• Deep-RL model (Clark and Manning, 2016) ap-

plies reinforcement learning to mention-ranking

models to form coreference clusters.

• End-to-end model (Lee et al., 2018) is the state-

of-the-art method of coreference resolution. It

predicts coreference clusters via an end-to-end

neural network that leverages pretrained word

embeddings and contextual information.

Last but not least, to demonstrate the effective-

ness of the proposed model, we also present a vari-

ation of the End-to-end model, which can also use

the visual information, as an extra baseline:

• End-to-end+Visual first extracts features from

images with ResNet-152 (He et al., 2016). Then

it concatenates the image feature with the con-

textual feature in the original End-to-end model

together to make the final prediction.

As the Deterministic, Statistical, and Deep-

RL model are included in the Stanford CoreNLP

toolkit6, we use their released model as baselines.

For the End-to-end model, we also use their re-

leased code7.

6 The Result

The experimental results are shown in Table 1.

Our proposed model VisCoref outperforms all the

baseline models significantly, which indicates that

the visual information is crucial for resolving pro-

nouns in dialogues. Besides that, we also have the

following interesting findings:

6https://stanfordnlp.github.io/

CoreNLP/coref.html
7https://github.com/kentonl/e2e-coref

https://github.com/tensorflow/models/tree/master/research/object_detection
https://github.com/tensorflow/models/tree/master/research/object_detection
https://stanfordnlp.github.io/CoreNLP/coref.html
https://stanfordnlp.github.io/CoreNLP/coref.html
https://github.com/kentonl/e2e-coref


5129

Model
Discussed Not Discussed Overall

P R F1 P R F1 P R F1

Deterministic 61.16 25.54 36.03 17.39 8.26 11.20 56.65 24.01 33.73

Statistical 79.37 26.65 39.90 6.10 1.03 1.77 76.18 24.23 36.76

Deep-RL 72.69 26.97 39.35 32.86 14.46 20.09 68.51 25.93 37.62

End-to-end 89.65 63.69 74.47 67.33 64.76 66.02 86.94 63.79 73.59

End-to-end+Visual 89.78 66.31 76.28 65.17 62.38 63.75 86.91 65.95 74.99

Viscoref 85.95 72.45 78.63 67.04 71.67 69.28 83.78 72.38 77.66

Human 95.03 81.82 87.93 86.18 93.57 89.73 94.02 82.91 88.12

Table 1: Experimental results on VisPro. Precision (P), recall (R) and F1 scores are presented. The best performed

F1 scores are indicated with bold font.

1. For all the conventional models, the “Not Dis-

cussed” pronouns whose antecedents are absent

in dialogues are more challenging than “Dis-

cussed” pronouns whose antecedents appear in

the dialogue context. The reason behind is that

if the correct mentions appear in the near con-

text of pronouns, the information about the cor-

rect mention can be aggregated to the targeting

pronoun through either human-designed rules

or deep neural networks (Bi-LSTM). However,

when the correct mention is not available in the

near context, it is quite challenging for conven-

tional models to understand the dialogue and

correctly ground the pronoun to the object both

speakers can see, as they do not have the sup-

port of visual information.

2. As is shown in the result of the “End-to-

end+Visual” model, simply concatenating the

visual feature to the contextual feature can help

resolve “Discussed” pronouns but may hurt the

performance of the model on “Not Discussed”

pronouns. Different from them, the proposed

Viscoref can improve the resolution of both the

“Discussed” and “Not Discussed” pronouns.

There are mainly two reasons behind: (1) The

visual information in our model is first con-

verted into textual labels and then transformed

into vector representation in the same way as

the dialogue context. Thus the vector space of

contextual and visual information is perfectly

aligned. (2) We introduce a hyper-parameter

λvis to balance the influence of different knowl-

edge resources.

3. Even though our model outperforms all the

baseline methods, we still can observe a huge

Figure 6: Effect of λvis. F1 scores of all categories are

reported.

gap between our model and human being. It in-

dicates that current models still cannot fully un-

derstand the dialogue even with the support of

visual information and further proves the value

and necessity of proposing VisPro.

6.1 Hyper-parameter Analysis

We traverse different weights of visual and con-

textual information from 0 to 1, and the result is

shown in Figure 6. Along with the increase of

λvis, our model puts more weight on the visual in-

formation. As a result, our model can perform bet-

ter. However, when our model focuses too much

on the visual information (when λvis equals to 0.9

or 1), the model overfits to the visual information

and thus performs poorly on the task. To achieve

the balance between the visual and contextual in-

formation, we set λvis to be 0.4.



5130

Correct Antecedent a blue, white and 

red train

Prediction by End-to-end any writing 

Prediction by VisCoref a blue, white and 

red train

Dialogue

A: Any writing or numbers on it?

B: Yes - but it is too small to read.

A: Are there people at the station?

B: There is 1 lonely guy.

Mention pool from corresponding caption

a blue, white and red train; a train station

Mention pool from other captions

a reader, a hard court, men, a pair of 

scissors, legs, a woman skiing, his cellphone, 

a killer, a champagne glass, a pink vase,

a flower arrangement, a toy stroller, etc.

a blue, white 

and red train

people

1 lonely guy

it

any writing

(a)

Correct Antecedent 2 zebras

Prediction by End-to-end the people

Prediction by VisCoref 2 zebras

Dialogue

A: Are they standing on green grass ?

B: No .

A: Are they standing on dirt ?

B: Yes .

A: Are they the only 2 animals you see ?

B: Yes .

Mention pool from corresponding caption

2 zebras, each other, their necks

Mention pool from other captions

the people, a paved road, large vehicles, a 

pedestrian, few people, her hands, a yellow 

motorcycle, the walls, skis and mountains, 

the beds, a black and silver fire, etc.

2 zebras

the people

they

(b)

Figure 7: Randomly selected examples from VisPro. The image, dialogue record, prediction result, and heatmap

of the mention-object similarity are provided. We indicate the target pronoun with the underlined italics font and

the candidate mentions with bold font. Only relevant parts of dialogues are presented. The row of the heatmap

represents mentions in the context, and the column means detected object labels from the image.

6.2 Case Study

To further investigate how visual information can

help solve PCR, we randomly select two examples

and show the prediction results of VisCoref and

End-to-end model in Figure 7.

In the first example in Figure 7(a), given the

pronoun “it,” the End-to-end model picks “any

writing” from the dialogue, while the VisCoref

model chooses “a blue, white and red train” from

the candidate mention sets. Without looking at the

picture, we cannot distinguish between these two

candidates. However, when the picture is taken

into consideration, we observe that there is a train

in the image and thus “a blue, white and red train”

is a more suitable choice, which proves the im-

portance of visual information. A similar situa-

tion happens in Figure 7(b), where the End-to-end

model connects “they” to “the people” but there

is no human being in the image at all. On the

contrary, as VisPro can effectively leverage the vi-

sual information and make the decision that “they”

should refer to “2 zebras.”

7 Related Work

In this section, we introduce the related work

about pronoun coreference resolution and visual-

aware natural language processing problems.

7.1 Pronoun Coreference Resolution

As one core task of natural language understand-

ing, pronoun coreference resolution, the task of

identifying mentions in text that the targeting pro-

noun refers to, plays a vital role in many down-

stream applications in natural language process-

ing, such as machine translation (Guillou, 2012),

summarization (Steinberger et al., 2007) and in-

formation extraction (Edens et al., 2003). Tra-

ditional studies focus on resolving pronouns in

expert-annotated formal textual dataset such as

ACE (NIST, 2003) or OntoNotes (Pradhan et al.,

2012). However, models that perform well on

these datasets might not perform as well in other

scenarios such as dialogues due to the informal

language and the lack of essential information

(e.g., the shared view of two speakers). In this

work, we thus focus on the PCR in dialogues and

show that the information contained in the shared

view can be crucial for understanding the dia-

logues and correctly resolving the pronouns.

7.2 Visual-aware NLP

As the intersection of computer vision (CV) and

natural language processing (NLP), visual-aware

NLP research topics have been popular in both

communities. For instance, image captioning (Xu

et al., 2015) focuses on generating captions for

images, visual question answering (VQA) (Antol

et al., 2015) on answering questions about a im-

age, and visual dialogue (Das et al., 2017) on gen-

erating a suitable response based on images. As

one vital step of all the aforementioned visual-

aware natural language processing tasks (Kottur

et al., 2018), the visual-aware PCR is still unex-

plored. To fill this gap, in this paper, we create

VisPro, which is a large-scale visual-aware PCR



5131

dataset, and introduce VisCoref to demonstrate

how to leverage information hidden in the shared

view to resolve pronouns in dialogues better and

thus understand the dialogues better.

Another related work is the comprehension of

referring expressions (Mao et al., 2016), which is

inferring the object in an image that an expres-

sion describes. However, the task is formulated on

isolated noun phrases specially designed for dis-

criminative descriptions without putting them into

a meaningful context. Instead, our task focuses

on resolving pronouns in dialogues based on im-

ages as the shared view, which enhances the un-

derstanding of dialogues based on the comprehen-

sion of expressions and images.

8 Conclusion

In this work, we formally define the task of visual

pronoun coreference resolution (PCR) and present

VisPro, the first large-scale visual-supported pro-

noun coreference resolution dataset. Different

from conventional pronoun datasets, VisPro fo-

cuses on resolving pronouns in dialogues which

discusses a view that both speakers can see.

Moreover, we also propose VisCoref, the first

visual-aware PCR model that aligns contextual in-

formation with visual information and jointly uses

them to find the correct objects that the targeting

pronouns refer to. Extensive experiments demon-

strate the effectiveness of the proposed model.

Further case studies also demonstrate that jointly

using visual information and contextual informa-

tion is an essential path for fully understanding hu-

man language, especially dialogues.

Acknowledgements

This paper was supported by Beijing Academy

of Artificial Intelligence (BAAI), the Early Ca-

reer Scheme (ECS, No. 26206717) from Research

Grants Council in Hong Kong, and the Tencent AI

Lab Rhino-Bird Focused Research Program.

References

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C. Lawrence Zitnick,
and Devi Parikh. 2015. VQA: visual question an-
swering. In Proceedings of ICCV, pages 2425–
2433.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly

learning to align and translate. In Proceedings of
ICLR.

Hong Chen, Zhenhua Fan, Hao Lu, Alan L. Yuille, and
Shu Rong. 2018. Preco: A large-scale dataset in
preschool vocabulary for coreference resolution. In
Proceedings of EMNLP, pages 172–181.

Kevin Clark and Christopher D. Manning. 2015.
Entity-centric coreference resolution with model
stacking. In Proceedings of ACL, pages 1405–1415.

Kevin Clark and Christopher D. Manning. 2016. Deep
reinforcement learning for mention-ranking coref-
erence models. In Proceedings of EMNLP, pages
2256–2262.

Abhishek Das, Satwik Kottur, Khushi Gupta, Avi
Singh, Deshraj Yadav, José M. F. Moura, Devi
Parikh, and Dhruv Batra. 2017. Visual dialog. In
Proceedings of CVPR, pages 1080–1089.

Richard J. Edens, Helen L. Gaylard, Gareth J. F. Jones,
and Adenike M. Lam-Adesina. 2003. An investiga-
tion of broad coverage automatic pronoun resolution
for information retrieval. In Proceedings of SIGIR,
pages 381–382.

Kate Ehrlich. 1981. Search and inference strategies in
pronoun resolution: an experimental study. In Pro-
ceedings of ACL, pages 89–93.

Liane Guillou. 2012. Improving pronoun translation
for statistical machine translation. In Proceedings
of EACL, pages 1–10.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of CVPR, pages 770–778.

Jerry R Hobbs. 1978. Resolving pronoun references.
Lingua, 44(4):311–338.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of ICLR.

Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL,
pages 423–430.

Satwik Kottur, José M. F. Moura, Devi Parikh, Dhruv
Batra, and Marcus Rohrbach. 2018. Visual corefer-
ence resolution in visual dialog using neural module
networks. In Proceedings of ECCV, pages 160–178.

Kenton Lee, Luheng He, and Luke Zettlemoyer. 2018.
Higher-order coreference resolution with coarse-to-
fine inference. In Proceedings of NAACL-HLT,
pages 687–692.



5132

Junhua Mao, Jonathan Huang, Alexander Toshev, Oana
Camburu, Alan L. Yuille, and Kevin Murphy. 2016.
Generation and comprehension of unambiguous ob-
ject descriptions. In Proceedings of CVPR, pages
11–20.

Vincent Ng. 2005. Supervised ranking for pronoun res-
olution: Some recent improvements. In Proceedings
of AAAI, pages 1081–1086.

US NIST. 2003. The ace 2003 evaluation plan. US
National Institute for Standards and Technology
(NIST), pages 2003–08.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proceedings of EMNLP,
pages 1532–1543.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of NAACL-HLT, pages
2227–2237.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 shared task: Modeling multilingual unre-
stricted coreference in ontonotes. In Proceedings of
EMNLP-CoNLL, pages 1–40.

Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nate Chambers, Mihai Surdeanu, Dan Ju-
rafsky, and Christopher D. Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of EMNLP, pages 492–501.

Josef Steinberger, Massimo Poesio, Mijail A. Kabad-
jov, and Karel Jezek. 2007. Two uses of anaphora
resolution in summarization. Inf. Process. Manage.,
43(6):1663–1680.

Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of ACL, pages 656–664.

Michael Strube and Christoph Müller. 2003. A ma-
chine learning approach to pronoun resolution in
spoken dialogue. In Proceedings of ACL, pages
168–175.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun
Cho, Aaron C. Courville, Ruslan Salakhutdinov,
Richard S. Zemel, and Yoshua Bengio. 2015. Show,
attend and tell: Neural image caption generation
with visual attention. In Proceedings of ICML,
pages 2048–2057.

Hongming Zhang, Yan Song, and Yangqiu Song.
2019a. Incorporating context and external knowl-
edge for pronoun coreference resolution. In Pro-
ceedings of NAACL-HLT, pages 872–881.

Hongming Zhang, Yan Song, Yangqiu Song, and Dong
Yu. 2019b. Knowledge-aware pronoun coreference
resolution. In Proceedings of ACL, pages 867–876.


