



















































Annotating Derivations: A New Evaluation Strategy and Dataset for Algebra Word Problems


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 494–504,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Annotating Derivations: A New Evaluation Strategy and Dataset for
Algebra Word Problems

Shyam Upadhyay1 and Ming-Wei Chang2

1University of Illinois at Urbana-Champaign, IL, USA
2Microsoft Research, Redmond, WA, USA

upadhya3@illinois.edu
minchang@microsoft.com

Abstract

We propose a new evaluation for auto-
matic solvers for algebra word problems,
which can identify mistakes that existing
evaluations overlook. Our proposal is to
evaluate such solvers using derivations,
which reflect how an equation system was
constructed from the word problem. To
accomplish this, we develop an algorithm
for checking the equivalence between two
derivations, and show how derivation an-
notations can be semi-automatically added
to existing datasets. To make our exper-
iments more comprehensive, we include
the derivation annotation for DRAW-1K, a
new dataset containing 1000 general alge-
bra word problems. In our experiments,
we found that the annotated derivations
enable a more accurate evaluation of au-
tomatic solvers than previously used met-
rics. We release derivation annotations for
over 2300 algebra word problems for fu-
ture evaluations.

1 Introduction

Automatically solving math reasoning problems is
a long-pursued goal of AI (Newell et al., 1959;
Bobrow, 1964). Recent work (Kushman et al.,
2014; Shi et al., 2015; Koncel-Kedziorski et al.,
2015) has focused on developing solvers for alge-
bra word problems, such as the one shown in Fig-
ure 1. Developing a solver for word problems can
open several new avenues, especially for online
education and intelligent tutoring systems (Kang
et al., 2016). In addition, as solving word prob-
lems requires the ability to understand and ana-
lyze natural language, it serves as a good test-bed
for evaluating progress towards goals of artificial
intelligence (Clark and Etzioni, 2016).

Am=Bn Cm + Dn = E

      Costs of apple and orange are in ratio 5 : 15 at the Acme Market. 
Mark wanted some fruits so he buys 5 apples and 5 oranges for 100 
dollars. Find cost of each.

5m=15n,5m+5n=100	(m=15,n=5) 	

Solution Equation System Derivation

Figure 1: An algebra word problem with its solution, equa-
tion system and derivation. Evaluating solvers on derivation
is more reliable than evaluating on solution or equation sys-
tem, as it reveals errors that other metric overlook.

An automatic solver finds the solution of a
given word problem by constructing a deriva-
tion, consisting of an un-grounded equation
system1 ({Am = Bn, Cm + Dn = E} in Figure 1)
and alignments of numbers in the text to
its coefficients (blue edges). The deriva-
tion identifies a grounded equation system
{5m = 15n, 5m + 5n = 100}, whose solution can
then be generated to answer the problem. A
derivation precisely describes how the grounded
equation system was constructed from the word
problem by the automatic solver. On the other
hand, the grounded equation systems and the so-
lutions are less informative, as they do not explain
which span of text aligns to the coefficients in the
equations.

While the derivation is clearly the most in-
formative structure, surprisingly, no prior work
evaluates automatic solvers using derivations di-
rectly. To the best of our knowledge, none of the
current datasets contain human-annotated deriva-
tions, possibly due to the belief that the current
evaluation metrics are sufficient and the benefit
of evaluating on derivations is minor. Currently,
the most popular evaluation strategy is to use so-
lution accuracy (Kushman et al., 2014; Hosseini
et al., 2014; Shi et al., 2015; Koncel-Kedziorski et

1Also referred to as a template. We use these two terms
interchangeably.

494



al., 2015; Zhou et al., 2015; Huang et al., 2016),
which computes whether the solution was correct
or not, as this is an easy-to-implement metric. An-
other evaluation strategy was proposed in (Kush-
man et al., 2014), which finds an approximate
derivation from the gold equation system and uses
it to compare against a predicted derivation. We
follow (Kushman et al., 2014) and call this evalu-
ation strategy the equation accuracy. 2

In this work, we argue that evaluating solvers
against human labeled derivation is important. Ex-
isting evaluation metrics, like solution accuracy
are often quite generous — for example, an incor-
rect equation system, such as,

{m+ 5 = n+ 15, m+ n = 15 + 5}, (1)

can generate the correct solution of the word prob-
lem in Figure 1. While equation accuracy appears
to be a stricter metric than solution accuracy, our
experiments show that the approximation can mis-
lead evaluation, by assigning higher scores to an
inferior solver. Indeed, a correct equation system,
(5m = 15n, 5m+5n = 100), can be generated by
using a wrong template, Am = Bn, Am+ An = C,
and aligning numbers in the text to coefficients in-
correctly. We show that without knowing the cor-
rect derivation at evaluation time, a solver can be
awarded for the wrong reasons.

The lack of annotated derivations for word
problems and no clear definition for comparing
derivations present technical difficulties in using
derivation for evaluation. In this paper, we address
these difficulties and for the first time propose to
evaluate the solvers using derivation accuracy. To
summarize, the contributions of this paper are:

• We point out that evaluating using derivations
is more precise compared to existing metrics.
Moreover, contrary to popular belief, there is
a meaningful gap between the derivation ac-
curacy and existing metrics, as it can discover
crucial errors not captured previously.

• We formally define when two derivations are
equivalent, and develop an algorithm that can
determine the same. The algorithm is simple

2Note that an approximation of the derivation is neces-
sary, as there is no annotated derivation. From the brief de-
scription in their paper and the code released by Kushman et
al. (2014), we found that their implementation assumes that
the first derivation that matches the equations and generates
the correct solution is the correct reference derivation against
which predicted derivations are then evaluated.

Word Problem x We are mixing a solu-
tion of 32% sodium and
another solution of 12%
sodium. How many liters
of 32% and 12% solution
will produce 50 liters of a
20% sodium solution?

Textual Numbers Q(x) {321, 121, 322, 122, 50, 20}
Equation System y 32m + 12n = 20 ∗ 50,

m+ n = 50
Solution m = 20, n = 30

Template T Am + Bn = C ∗ D,
m + n = C

Coefficients C(T ) A, B, C, D
Alignments A {321 → A, 121 → B,

50→ C, 20→ D}
EquivTNum {[321, 322], [121, 122]}
Derivation z (T,A)

Table 1: The symbols we used in the paper. Our proposed
annotations are shown in bold. Equivalent textual numbers,
described in EquivTNum, are distinguished with subscripts.

to implement, and can accurately detect the
equivalence even if two derivations have very
different syntactic forms.

• We annotated over 2300 word algebra prob-
lems3 with detailed derivation annotations,
providing high quality labeled semantic
parses for evaluating word problems.

2 Evaluating Derivations

We describe our notation and revisit the notion of
derivation introduced in (Kushman et al., 2014).
We then formalize the notion of derivation equiv-
alence and provide an algorithm to determine it.

Structure of Derivation The word problem in
Table 1 shows our notation, where our proposed
annotations are shown in bold. We denote a word
problem by x and an equation system by y.

An un-grounded equation system (or template)
T is a family of equation systems parameter-
ized by a set of coefficients C(T ) = {ci}ki=1,
where each coefficient ci aligns to a textual num-
ber (e.g., four) in the word problem. We also
refer to the coefficients as slots of the template.
We use (A, B, C, . . .) to represents coefficients and
(m, n, . . .) to represent the unknown variables in
the templates.

LetQ(x) be the set of all the textual numbers in
the problem x, and C(T ) be the coefficients to be
determined in the template T . An alignment is a
set of tuples A = {(q, c) | q ∈ Q(x), c ∈ C(T ) ∪
{�}} aligning textual numbers to coefficient slots,

3available at https://aka.ms/datadraw

495



where a tuple (q, �) indicates that the number q is
not relevant to the final equation system.

Note that there may be multiple semantically
equivalent textual numbers. e.g., in Figure 1, ei-
ther of the 32 can be aligned to coefficient slot A
in the template. These equivalent textual numbers
are marked in the EquivTNum field in the annota-
tion. If two textual numbers q, q′ ∈ EquivTNum,
then we can align a coefficient slot to either q or
q′, and generate a equivalent alignment.

An alignmentA and a template T together iden-
tify a derivation z = (T,A) of an equation sys-
tem. Note that there may be multiple valid deriva-
tions, using one of the equivalent alignments. We
assume there exists a routine Solve(y) that find
the solution of an equation system. We use a Gaus-
sian elimination solver for our Solve routine. We
use hand-written rules and the quantity normalizer
in Stanford CoreNLP (Manning et al., 2014) to
identify textual numbers.

Derivation Equivalence We define two deriva-
tions (T1, A1) and (T2, A2) to be equivalent iff the
corresponding templates T1, T2 and alignments
A1, A2 are equivalent.

Intuitively, two templates T1, T2 are equivalent
if they can generate the same space of equation
systems – i.e., for every assignment of values to
slots of T1, there exists an assignment of values to
slots of T2 such that they generate the same equa-
tion systems. For instance, template (2) and (3)
below are equivalent

m = A + Bn m = C− n (2)

m+ n = A m− Cn = B. (3)
because after renaming (A, B, C) to (B, C, A) re-
spectively in template (2), and algebraic manip-
ulations, it is identical to template (3). We can
see that any assignment of values to correspond-
ing slots will result in the same equation system.

Similarly, two alignments A1 and A2 are equiv-
alent if corresponding slots from each template
align to the same textual number. For the above
example, the alignment {1 → A, 3 → B, 4 → C}
in template (2), and alignment {1 → B, 3 →
C, 4 → A} in template (3) are equivalent. Note
that the alignment {1→ A, 3→ B, 4→ C} for (2)
is not equivalent to {1 → A, 3 → B, 4 → C} in
(3), because it does not respect variable renaming.
Our definition also allows two alignments to be

Algorithm 1 Evaluating Derivation
Input: Predicted (Tp, Ap) and gold (Tg, Ag) derivation
Output: 1 if predicted derivation is correct, 0 otherwise
1: if |C(Tp)| 6= |C(Tg)| then . different # of coeff. slots
2: return 0
3: end if
4: Γ← TEMPLEQUIV(Tp,Tg)
5: if Γ = ∅ then . not equivalent templates
6: return 0
7: end if
8: if ALIGNEQUIV(Γ, Ap, Ag) then . Check alignments
9: return 1

10: end if
11: return 0
12:
13: procedure TEMPLEQUIV(T1, T2)
14: . Note that here |C(T1)| = |C(T2)| holds
15: Γ← ∅
16: for each 1-to-1 mapping γ : C(T1)→ C(T2) do
17: match← True
18: for t = 1 · · ·R do . R : Rounds
19: Generate random vector v
20: A1 ← {(vi → ci)},A2 ← {(vi → γ(ci))}
21: if Solve(T1, A1) 6= Solve(T2, A2) then
22: match← False; break
23: end if
24: end for
25: if match then Γ← Γ ∪ {γ}
26: end for
27: return Γ . Γ 6= ∅ iff the templates are equivalent
28: end procedure
29:
30: procedure ALIGNEQUIV(Γ, A1, A2)
31: for mapping γ ∈ Γ do
32: if following holds true,

(q, c) ∈ A1 ⇐⇒ {(q, γ(c)) or (q′, γ(c))} ∈ A2

33: where (q′, q) ∈ EquivTNum
34: then return 1
35: end if
36: end for
37: return 0
38: end procedure

equivalent, if they use textual numbers in equiva-
lent positions for corresponding slots (as described
by EquivTNum field).

In the following, we carefully explain how tem-
plate and alignment equivalence are determined
algorithmically. Algorithm 1 shows the complete
algorithm for comparing two derivations.

Template Equivalence We propose an approx-
imate procedure TEMPLEQUIV (line 13) that de-
tects equivalence between two templates. The pro-
cedure relies on the fact that under appropriate re-
naming of coefficients, two equivalent templates
will generate equations which have the same solu-
tions, for all possible coefficient assignments.

For two templates T1 and T2, with the same
number of coefficients |C(T1)| = |C(T2)|, we rep-
resent a choice of renaming coefficients by γ, a

496



1-to-1 mapping from C(T1) to C(T2). The two
templates are equivalent if there exists a γ such
that solutions of the equations identified by T1
and T2 are same, for all possible coefficient as-
signments. The TEMPLEQUIV procedure exhaus-
tively tries all possible renaming of coefficients
(line 16), checking if the solutions of the equa-
tion systems generated from a random assignment
(line 19) match exactly. It declares equivalence if
for a renaming γ, the solutions match for R = 10
such random assignments.4 The procedure returns
all renamings Γ of coefficients between two tem-
plates under which they are equivalent (line 27).
We discuss its effectiveness in §3.
Alignment Equivalence The TEMPLEQUIV
procedure returns every mapping γ in Γ under
which the templates were equivalent (line 4).
Recall that γ identifies corresponding slots, c
and γ(c), in T1 and T2 respectively. We describe
alignment equivalence using these mappings.

Two alignmentsA1 andA2 are equivalent if cor-
responding slots (according to γ) align to the same
textual number. More formally, if we find a map-
ping γ such that for each tuple (q, c) in A1 there is
(q, γ(c)) in A2, then the alignments are equivalent
(line 33). We allow for equivalent textual numbers
(as identified by EquivTNum field) to match when
comparing tuples in alignments.

The proof of correctness of Algorithm 1 is
sketched in the appendix. Using Algorithm 1,
we can define derivation accuracy, to be 1 if the
predicted derivation (Tp, Ap) and the reference
derivation (Tg, Ag) are equivalent, and 0 other-
wise.

Properties of Derivation Accuracy By com-
paring derivations, we can ensure that the follow-
ing errors are detected by the evaluation.

Firstly, correct solutions found using incorrect
equations will be penalized, as the template used
will not be equivalent to reference template. Sec-
ondly, correct equation system obtained by an in-
correct template will also be penalized for the
same reason. Lastly, if the solver uses the correct
template to get the correct equation system, but
aligns the wrong number to a slot, the alignment
will not be equivalent to the reference alignment,
and the solver will be penalized too.

4Note that this procedure is a Monte-Carlo algorithm, and
can be made more precise by increasing R. We found mak-
ing R larger than 10 did not have an impact on the empirical
results.

We will see some illustrative examples of above
errors in §5.3. Note that the currently popular eval-
uation metric of solution accuracy will not detect
any of these error types.

3 Annotating Derivations

As none of the existing benchmarks contain
derivation annotations, we decided to augment ex-
isting datasets with these annotations. We also an-
notated DRAW-1K, a new dataset of 1000 general
algebra word problems to make our study more
comprehensive. Below, we describe how we re-
duced annotation effort by semi-automatic gener-
ated some annotations.

Annotating gold derivations from scratch for all
problems is time consuming. However, not all
word problems require manual annotation – some-
times all numbers appearing in the equation sys-
tem can be uniquely aligned to a textual number
without ambiguity. For such problems, the an-
notations are generated automatically.5 We iden-
tify word problems which have at least one align-
ment ambiguity – multiple textual numbers with
the same value, which appears in the equation sys-
tem. A example of such a problem is shown in
Figure 1, where there are three textual numbers
with value 5, which appears in the equation sys-
tem. Statistics for the number of word problems
with such ambiguity is shown in Table 2.

We only ask annotators to resolve such align-
ment ambiguities, instead of annotating the entire
derivation. If more than one alignments are gen-
uinely correct (as in word problem of Table 1), we
ask the annotators to mark both (using the Equiv-
TNum field). This ensures our derivation annota-
tions are exhaustive – all correct derivations are
marked. With the correct alignment annotations,
templates for all problems can be easily induced.

Annotation Effort To estimate the effort re-
quired to annotate derivations, we timed our an-
notators when annotating 50 word problems (all
involved alignment ambiguities). As a control, we
also asked annotators to annotate the entire deriva-
tion from scratch (i.e., only provided with the
word problem and equations), instead of only fix-
ing alignment ambiguities. When annotating from
scratch, annotators took an average of 4 minute per
word problem, while when fixing alignment am-
biguities this time dropped to average of 1 minute

5Annotations for all problems are manually verified later.

497



Dataset DRAW-1K ALG-514 DOLPHIN-L

# problems 1000 514 832
w/ ambiguity 21% 23% 35%
vocab. 2.21k 1.83k 0.33k

Number of Templates

before 329 30 273
after 224 24 203
% reduction 32% 20% 25%

Table 2: Statistics of the datasets. At least 20% of problems
in each dataset had alignment ambiguities that required hu-
man annotations. The number of templates before and after
annotation is also shown (reduction > 20%).

per word problem. We attained a inter-annotator
agreement of 92% (raw percentage agreement),
with most disagreements arising on EquivTNum
field.6

Reconciling Equivalent Templates The num-
ber of templates has been used as a measure of
dataset diversity (Shi et al., 2015; Huang et al.,
2016), however prior work did not reconcile the
equivalent templates in the dataset. Indeed, if
two templates are equivalent, we can replace one
with the other and still generate the correct equa-
tions. Therefore, after getting human judgements
on alignments, we reconcile all the templates us-
ing TEMPLEQUIV as the final step of annotation.

TEMPLEQUIV is quite effective (despite being
approximate), reducing the number of templates
by at least 20% for all datasets (Table 2). We
did not find any false positives generated by the
TEMPLEQUIV in our manual examination. The re-
duction in Table 2 clearly indicates that equivalent
templates are quite common in all datasets, and
number of templates (and hence, dataset diversity)
can be significantly overestimated without proper
reconciliation.

4 Experimental Setup

We describe the three datasets used in our experi-
ments. Statistics comparing the datasets is shown
in Table 2. In total, our experiments involve over
2300 word problems.

Alg-514 The dataset ALG-514 was introduced
in (Kushman et al., 2014). It consists of 514 gen-
eral algebra word problems ranging over a vari-
ety of narrative scenarios (distance-speed, object
counting, simple interest, etc.).

6These were adjudicated on by the first author.

Dolphin-L DOLPHIN-L is the linear-T2 subset
of the DOLPHIN dataset (Shi et al., 2015), which
focuses on number word problems – algebra word
problems which describe mathematical relation-
ships directly in the text. All word problems in
the linear-T2 subset of the DOLPHIN dataset can
be solved using linear equations.

DRAW-1K Diverse Algebra Word (DRAW-1K),
consists of 1000 word problems crawled from
algebra.com. Details on the dataset creation
can be found in the appendix. As ALG-514 was
also crawled from algebra.com, we ensured
that there is little overlap between the datasets.

We randomly split DRAW-1K into train, devel-
opment and test splits with 600, 200, 200 prob-
lems respectively. We use 5-fold cross validation
splits provided by the authors for DOLPHIN-L and
ALG-514.

4.1 Evaluation

We compare derivation accuracy against the fol-
lowing evaluation metrics.

Solution Accuracy We compute solution accu-
racy by checking if each number in the reference
solution appears in the generated solution (disre-
garding order), following previous work (Kush-
man et al., 2014; Shi et al., 2015).

Equation Accuracy An approximation of
derivation accuracy that is similar to the one used
in Kushman et al. (2014). We approximate the
reference derivation z̃ by randomly chosen from
the (several possible) derivations which lead to
the gold y from x. Derivation accuracy is com-
puted against this (possibly incorrect) reference
derivation. Note that in equation accuracy, the
approximation is used instead of annotated deriva-
tion. We include the metric of equation accuracy
in our evaluations to show that human annotated
derivation is necessary, as approximation made by
equation accuracy might be problematic.

4.2 Our Solver

We train a solver using a simple modeling ap-
proach inspired by Kushman et al. (2014) and
Zhou et al. (2015). The solver operates as fol-
lows. Given a word problem, the solver ranks
all templates seen during training, Γtrain, and se-
lects the set of the top-k (we use k = 10) tem-
plates Π ⊂ Γtrain. Next, all possible derivations
D(Π) that use a template from Π are generated

498



Setting Soln. Acc. Eqn. Acc. Deriv. Acc.

ALG-514

TE 76.2 72.7 75.5
TD 78.4 73.9 77.8
TD - TE 2.2 1.2 2.3

DRAW-1K

TE 52.0 48.0 48.0
TD 55.0 48.0 53.0
TD - TE 3.0 0 5.0

DOLPHIN

TE 55.1 50.1 44.2
TD 57.5 36.8 54.9
TD - TE 2.4 -13.3 10.7

Table 3: TE and TD compared using different evaluation met-
rics. Note that while TD is clearly superior to TE due to extra
supervision using the annotations, only derivation accuracy is
able to correctly reflect the differences.

and scored. The equation system ŷ identified by
highest scoring derivation ẑ is output as the pre-
diction. Following (Zhou et al., 2015), we do not
model the alignment of nouns phrases to variables,
allowing for tractable inference when scoring the
generated derivations. The solver is trained using a
structured perceptron (Collins, 2002). We extract
the following features for a (x, z) pair,

Template Features. Unigrams and bigrams of
lemmas and POS tags from the word problem x,
conjoined with |Q(x)| and |C(T )|.
Alignment Tuple Features. For two alignment
tuples, (q1, c1), (q2, c2), we add features indicat-
ing whether c1 and c2 belong to the same equation
in the template or share the same variable. If they
belong to the same sentence, we also add lemmas
of the nouns and verbs between q1 and q2 in x.

Solution Features. Features indicating if the so-
lution of the system identified by the derivation are
integer, negative, non-negative or fractional.

5 Experiments

Are solution and equation accuracy equally ca-
pable as derivation accuracy at distinguishing be-
tween good and bad models? To answer this ques-
tion, we train the solver under two settings such
that one of the settings has clear advantage over
the other, and see if the evaluation metrics reflect
this advantage. The two settings are,

Setting Soln. Acc. Eqn. Acc. Deriv. Acc.

DRAW-1K + Alg-514

TE 32.5 31.5 29.5
TE∗ 60.5 56.0 54.0
TD 62.0 53.0 59.5

TD - TE∗ 1.5 -3.0 5.5

DRAW-1K + Dolphin

TE 41.0 37.5 37.5
TE∗ 58.5 55.5 51.5
TD 60.0 53.0 58.0

TD - TE∗ 1.5 -2.5 6.5

Table 4: When combining two datasets, it is essential to rec-
oncile templates across datasets. Here TE∗ denotes training
on equations after reconciling the templates, while TE simply
combines datasets naively. As TE∗ represents a more appro-
priate setting, we compare TE∗ and TD in this experiment.

TE (TRAIN ON EQUATION) Only the (x,y)
pairs are provided as supervision. Similar to
(Kushman et al., 2014; Zhou et al., 2015), the
solver finds a derivation which agrees with the
equation system and the solution, and trains on it.
Note that the derivation found by the solver may
be incorrect.

TD (TRAIN ON DERIVATION) (x, z) pairs ob-
tained by the derivation annotation are used as su-
pervision. This setting trains the solver on human-
labeled derivations. Clearly, the TD setting is a
more informative supervision strategy than the TE
setting. TD provides the correct template and cor-
rect alignment (i.e. labeled derivation) as super-
vision and is expected to perform better than TE,
which only provides the question-equation pair.

We first present the main results comparing dif-
ferent evaluation metrics on solvers trained using
the two settings.

5.1 Main Results

We compare the evaluation metrics in Table 3. We
want to determine to what degree each evaluation
metric reflects the superiority of TD over TE.

We note that solution accuracy always exceeds
derivation accuracy, as a solver can sometimes get
the right solutions even with the wrong deriva-
tion. Also, solution accuracy is not as sensi-
tive as derivation accuracy to improvements in
the solver. For instance, solution accuracy only
changes by 2.4 on Dolphin-L when comparing
TE and TD, whereas derivation accuracy changes

499



by 10.7 points. We found that the large gap on
Dolphin-L was due to several alignment errors in
the predicted derivations, which were detected by
derivation accuracy. Recall that over 35% of the
problems in Dolphin-L have alignment ambigui-
ties (Table 2). In the TD setting, many of these
errors made by our solver were corrected as the
gold alignment was part of supervision.

Equation accuracy too has several limitations.
For DRAW-1K, it cannot determine which solver
is better and assigns them the same score. Fur-
thermore, it often (incorrectly) considers TD to
be a worse setting than TE, as evident from de-
crease in the scores (for instance, on DOLPHIN-
L). Recall that equation accuracy attempts to ap-
proximate derivation accuracy by choosing a ran-
dom derivation agreeing with the equations, which
might be incorrect.

Study with Combining Datasets With several
ongoing annotation efforts, it is a natural ques-
tion to ask is whether we can leverage multiple
datasets in training to generalize better. In Ta-
ble 4, we combine DRAW-1K’s train split with
other datasets, and test on DRAW-1K’s test split.
DRAW-1K’s test split was chosen as it is the largest
test split with general algebra problems (recall
Dolphin-L contains only number word problems).

We found that in this setting, it was important
to reconcile the templates across datasets. Indeed,
when we simply combine the two datasets in the
TE setting, we notice a sharp drop in performance
(compared to Table 3). However, if we reconciled
all templates and then used the new equations for
training (called TE∗ setting in Table 4), we were
able to see improvements from training on more
data. We suspect difference in annotation style led
to several equivalent templates in the combined
dataset, which got resolved in TE∗. Therefore, in
Table 4, we compare TE∗ and TD settings.7

In Table 4, a trend similar to Table 3 can be
observed – solution accuracy assigns a small im-
provement to TD over TE∗. Derivation accuracy
clearly reflects the fact that TD is superior to TE∗,
with a larger improvement compared to solution
accuracy (eg., 5.5 vs 1.5). Equation accuracy, as
before, considers TD to be worse than TE∗.

Note that this experiment also shows that dif-
ferences in annotation styles across different alge-
bra problem datasets can lead to poor performance

7In TE∗, the model still trains only using equations, with-
out access to derivations. So TD is still better than TE∗.

Dataset Ours KAZB Best Result

ALG-514 76.2 68.7 79.7 (ZDC)
DOLPHIN-L 55.1 37.5 46.3‡ (SWLLR)
DRAW-1K 52.0 43.2 –

Table 5: Comparison of our solver and other state-of-the-art
systems, when trained under TE setting. All numbers are
solution accuracy. See footnote for details on the comparison
to SWLLR.

when combining these datasets naively. Our find-
ings suggest that derivation annotation and tem-
plate reconciliation are crucial for such multi-data
supervision scenarios.

5.2 Comparing Solvers

To ensure that the results in the previous sec-
tion were not an artifact of any limitations of our
solver, we show here that our solver is competi-
tive to other state-of-the-art solvers, and therefore
it is reasonable to assume that similar results can
be obtained with other automatic solvers.

In Table 5, we compare our solver to KAZB, the
system of Kushman et al. (2014), when trained
under the existing supervision paradigm, TE (i.e.,
training on equations) and evaluated using solu-
tion accuracy. We also report the best scores on
each dataset, using ZDC and SWLLR to denote the
systems of Zhou et al. (2015) and Shi et al. (2015)
respectively. Note that our system and KAZB are
the only systems that can process all three datasets
without significant modification, with our solver
being clearly superior to KAZB.

5.3 Case Study

We discuss some interesting examples from the
datasets, to show the limitations of existing met-
rics, which derivation accuracy overcomes.

Correct Solution, Incorrect Equation In the
following example from the DOLPHIN-L dataset,
by choosing the correct template and the wrong
alignments, the solver arrived at the correct solu-
tions, and gets rewarded by solution accuracy.

The sum of 2(q1) numbers is 25(q2). 12(q3)

less than 4(q4) times one(q5) of the numbers is

16(q6) more than twice(q7) the other number.

Find the numbers.

‡SWLLR also had a solver which achieves 68.0, using over
9000 semi-automatically generated rules tailored to number
word problems. We compare to their similarity based solver
instead, which does not use any such rules, given that the rule-
based system cannot be applied to general word problems.

500



Note that there are seven textual numbers
(q1, . . . , q7) in the word problem. We can arrive at
the correct equations ({m + n = 25, 4m − 2n =
16 + 12}), by the correct derivation,

m+ n = q2 q4m− q7n = q6 + q3.

However, the solver found the following deriva-
tion, which produces the incorrect equations
({m+ n = 25, 2m− n = 2 + 12}),

m+ n = q2 q1m− q5n = q7 + q3.

Both the equations have the same solutions (m =
13, n = 12), but the second derivation is clearly
using incorrect reasoning.

Correct Equation, Incorrect Alignment In
such cases, the solver gets the right equation sys-
tem, but derived it using wrong alignment. Solu-
tion accuracy still rewards the solver. Consider the
problem from the DOLPHIN-L dataset,

The larger of two(q1) numbers is 2(q2) more

than 4(q3) times the smaller. Their sum is 67(q4).

Find the numbers.

The correct derivation for this problem is,

m− q3n = q2 m+ n = q4.

However, our system generated the following
derivation, which although results in the exact
same equation system (and thus same solutions),
is clearly incorrect due incorrect choice of ”two”,

m− q3n = q1 m+ n = q4.

Note that derivation accuracy will penalize the
solver, as the alignment is not equivalent to the ref-
erence alignment (q1 and q2 are not semantically
equivalent textual numbers).

Bad Approx. in Equation Accuracy The fol-
lowing word problem is from the ALG-514
dataset:

Mrs. Martin bought 3(q1) cups of coffee

and 2(q2) bagels and spent 12.75(q3) dollars.

Mr. Martin bought 2(q4) cups of coffee and

5(q5) bagels and spent 14.00(q6) dollars. Find

the cost of one(q7) cup of coffee and that of

one(q8) bagel.

The correct derivation is,

q1m+ q2n = q3 q4m+ q5n = q6.

However, we found that equation accuracy used
the following incorrect derivation for evaluation,

q1m+ q2n = q3 q2m+ q5n = q6.

Note while this derivation does generate the cor-
rect equation system and solutions, the derivation
utilizes the wrong numbers and misunderstood the
word problem. This example demonstrates the
needs to evaluate the quality of the word problem
solvers using the annotated derivations.

6 Related Work

We discuss several aspects of previous work in the
literature, and how it relates to our study.

Existing Solvers Current solvers for this task
can be divided into two broad categories based
on their inference approach – template-first and
bottom-up. Template-first approaches like (Kush-
man et al., 2014; Zhou et al., 2015) infer the
derivation z = (T,A) sequentially. They first pre-
dict the template T and then predict alignments
A from textual numbers to coefficients. In con-
trast, bottom-up approaches (Hosseini et al., 2014;
Shi et al., 2015; Koncel-Kedziorski et al., 2015)
jointly infer the derivation z = (T,A). Inference
proceeds by identifying parts of the template (eg.
Am + Bn) and aligning numbers to it ({2 → A,
3 → B}). At any intermediate state during in-
ference, we have a partial derivation, describing
a fragment of the final equation system (2m + 3n).
While our experiments used a solver employing
the template-first approach, it is evident that per-
forming inference in all such solvers requires con-
structing a derivation z = (T,A). Therefore, an-
notated derivations will be useful for evaluating all
such solvers, and may also aid in debugging errors.

Other reconciliation procedures are also dis-
cussed (though briefly) in earlier work. Kush-
man et al. (2014) reconciled templates by using
a symbolic solver and removing pairs with the
same canonicalized form. Zhou et al. (2015) also
reconciled templates, but do not describe how it
was performed. We showed that reconciliation
is important for correct evaluation, for reporting
dataset complexity, and also when combining mul-
tiple datasets.

501



Labeling Semantic Parses Similar to our work,
efforts have been made to annotate semantic
parses for other tasks, although primarily for pro-
viding supervision. Prior to the works of Liang
et al. (2009) and Clarke et al. (2010), seman-
tic parsers were trained using annotated logical
forms (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2005; Wong and Mooney, 2007, inter
alia), which were expensive to annotate. Re-
cently, Yih et al. (2016) showed that labeled se-
mantic parses for the knowledge based question
answering task can be obtained at a cost compa-
rable to obtaining answers. They showed signifi-
cant improvements in performance of a question-
answering system using the labeled parses instead
of answers for training. More recently, by treating
word problems as a semantic parsing task, Upad-
hyay et al. (2016) found that joint learning us-
ing both explicit (derivation as labeled semantic
parses) and implicit supervision signals (solution
as responses) can significantly outperform models
trained using only one type of supervision signal.

Other Semantic Parsing Tasks We demon-
strated that response-based evaluation, which is
quite popular for most semantic parsing prob-
lems (Zelle and Mooney, 1996; Berant et al., 2013;
Liang et al., 2011, inter alia) can overlook rea-
soning errors for algebra problems. A reason for
this is that in algebra word problems there can be
several semantic parses (i.e., derivations, both cor-
rect and incorrect) that can lead to the correct so-
lution using the input (i.e., textual number in word
problem). This is not the case for semantic pars-
ing problems like knowledge based question an-
swering, as correct semantic parse can often be
identified given the question and the answer. For
instance, paths in the knowledge base (KB), that
connect the answer and the entities in the question
can be interpreted as legitimate semantic parses.
The KB therefore acts as a constraint which helps
prune out possible semantic parses, given only the
problem and the answer. However, such KB-based
constraints are unavailable for algebra word prob-
lems.

7 Conclusion and Discussion

We proposed an algorithm for evaluating deriva-
tions for word problems. We also showed how
derivation annotations can be easily obtained by
only involving annotators for ambiguous cases.
We augmented several existing benchmarks with

derivation annotations to facilitate future compar-
isons. Our experiments with multiple datasets
also provided insights into the right approach to
combine datasets – a natural step in future work.
Our main finding indicates that derivation accu-
racy leads to a more accurate assessment of al-
gebra word problem solvers, finding errors which
other metrics overlook. While we should strive
to build such solvers using as little supervision
as possible for training, having high quality anno-
tated data is essential for correct evaluation.

The value of such annotations for evaluation be-
comes more immediate for online education sce-
narios, where such word solvers are likely to be
used. Indeed, in these cases, merely arriving at the
correct solution, by using incorrect reasoning may
prove detrimental for teaching purposes. We be-
lieve derivation based evaluation closely mirrors
how humans are evaluated in schools (by forcing
solvers to show “their work”).

Our datasets with the derivation annotations
have applications beyond accurate evaluation. For
instance, certain solvers, like the one in (Roy and
Roth, 2015), train a relevance classifier to identify
which textual numbers are relevant to solving the
word problem. As we only annotate relevant num-
bers in our annotations, our datasets can provide
high quality supervision for such classifiers. The
datasets can also be used in evaluation test-beds,
like the one proposed in (Koncel-Kedziorski et al.,
2016).

We hope our datasets will open new possibili-
ties for the community to simulate new ideas and
applications for automatic problem solvers.

Acknowledgments

The first author was supported on a grant
sponsored by DARPA under agreement num-
ber FA8750-13-2-0008. We would also like to
thank Subhro Roy, Stephen Mayhew and Chris-
tos Christodoulopoulos for useful discussions and
comments on earlier versions of the paper.

References
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy

Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533–1544, Seattle, Wash-
ington, USA, October. Association for Computa-
tional Linguistics.

502



Daniel G. Bobrow. 1964. A question-answering sys-
tem for high school algebra word problems. In Pro-
ceedings of the October 27-29, 1964, fall joint com-
puter conference, part I, pages 591–614. ACM.

Peter Clark and Oren Etzioni. 2016. My computer is
an honor student but how intelligent is it? standard-
ized tests as a measure of ai. AI Magazine, 37(1):5–
12.

James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world’s response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 18–27, Uppsala, Sweden,
July. Association for Computational Linguistics.

Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1–8. Associ-
ation for Computational Linguistics, July.

Mohammad Javad Hosseini, Hannaneh Hajishirzi,
Oren Etzioni, and Nate Kushman. 2014. Learning
to solve arithmetic word problems with verb catego-
rization. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 523–533, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.

Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin,
and Wei-Ying Ma. 2016. How well do comput-
ers solve math word problems? large-scale dataset
construction and evaluation. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
887–896, Berlin, Germany, August. Association for
Computational Linguistics.

Bo Kang, Arun Kulshreshth, and Joseph J. LaViola Jr.
2016. Analyticalink: An interactive learning envi-
ronment for math word problem solving. In Pro-
ceedings of the 21st International Conference on In-
telligent User Interfaces, pages 419–430. ACM.

Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish
Sabharwal, Oren Etzioni, and Siena Ang. 2015.
Parsing algebraic word problems into equations.
Transactions of the Association for Computational
Linguistics, 3:585–597.

Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate
Kushman, and Hannaneh Hajishirzi. 2016. Mawps:
A math word problem repository. In Proceedings of
the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 1152–1157,
San Diego, California, June. Association for Com-
putational Linguistics.

Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and
Regina Barzilay. 2014. Learning to automatically
solve algebra word problems. In Proceedings of the

52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
271–281, Baltimore, Maryland, June. Association
for Computational Linguistics.

Percy Liang, Michael Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 91–99, Suntec, Sin-
gapore, August. Association for Computational Lin-
guistics.

Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 590–599, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proc. of 52nd Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations.

Allen Newell, John C. Shaw, and Herbert A. Simon.
1959. Report on a general problem-solving pro-
gram. In IFIP Congress, volume 256, page 64.

Subhro Roy and Dan Roth. 2015. Solving general
arithmetic word problems. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1743–1752, Lisbon,
Portugal, September. Association for Computational
Linguistics.

Shuming Shi, Yuehui Wang, Chin-Yew Lin, Xiaojiang
Liu, and Yong Rui. 2015. Automatically solving
number word problems by semantic parsing and rea-
soning. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1132–1142, Lisbon, Portugal, September.
Association for Computational Linguistics.

Shyam Upadhyay, Ming-Wei Chang, Kai-Wei Chang,
and Wen-tau Yih. 2016. Learning from Explicit
and Implicit Supervision Jointly For Algebra Word
Problems. In Proceedings of EMNLP, pages 297–
306.

Yuk Wah Wong and Raymond Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing
with lambda calculus. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 960–967, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.

Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-
Wei Chang, and Jina Suh. 2016. The value of
semantic parse labeling for knowledge base ques-
tion answering. In Proceedings of the 54th Annual

503



Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 201–206,
Berlin, Germany, August. Association for Computa-
tional Linguistics.

J. M. Zelle and R. J. Mooney. 1996. Learning to
Parse Database Queries using Inductive Logic Pro-
ramming. In AAAI.

Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI ’05, Proceedings of the 21st Con-
ference in Uncertainty in Artificial Intelligence, Ed-
inburgh, Scotland, July 26-29, 2005, pages 658–666.

Lipu Zhou, Shuaixiang Dai, and Liwei Chen. 2015.
Learn to solve algebra word problems using
quadratic programming. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 817–822, Lisbon, Portugal,
September. Association for Computational Linguis-
tics.

A Creating DRAW-1K

We crawl over 100k problems from http://
algebra.com. The 100k word problems in-
clude some problems which require solving non-
linear equations (e.g. finding roots of quadratic
equations). We filter out these problems using
keyword matching. We also filter problems whose
explanation do not contain a variable named “x”.
This leaves us with 12k word problems.

Extracting Equations A word problem on
algebra.com is accompanied by a detailed ex-
planation provided by instructors. In our crawler,
we use simple pattern matching rules to extract all
the equations in the explanation. The problems
often have sentences which are irrelevant to solv-
ing the word problem (e.g. “Please help me, I am
stuck.”). During cleaning, the annotator removes
such sentences from the final word problem and
performs some minor editing if necessary.8

1000 problems were randomly chosen from
these pool of 12k problems, which were then
shown to annotators as described earlier to get the
derivation annotations.

B Proof of Correctness (Sketch)

For simplicity, we will assume that EquivTNum is
empty. The proof can easily be extended to handle
the more general situation.

8In some cases, some of the numbers in the text are
rephrased (“10ml” to “10 ml”) in order to allow NLP pipeline
work properly.

Lemma 1. The procedure TEMPLEQUIV returns
Γ 6= ∅ iff templates T1, T2 are equivalent (w.h.p.).
Proof First we prove that with high probability
we are correct in claiming that a γ found by the
algorithm leads to equivalence. Let probability of
getting the same solution even when the template
are not equivalent be �(T1, T2, γ) < 1. The proba-
bility that solution is same for R rounds for T1, T2
which are not equivalent is ≤ �R, which can be
made arbitrarily small by choosing large R. There-
fore, with a large enough R, obtaining Γ 6= ∅ from
TEMPLEQUIV implies there is a γ under which
templates generate equations with the same solu-
tion, and by definition, are equivalent.

Conversely, if templates are equivalent, it im-
plies ∃γ∗ such that under that mapping for any as-
signment, the generated equations have the same
solution. As we iterate over all possible 1-1 map-
pings γ between the two templates, we will find
γ∗ eventually.

Proposition Algorithm 1 returning 1 implies
derivations (Tp, Ap) and (Tg, Ag) are equivalent.

Proof Algorithm returns 1 only if TEMPLEQUIV
found a Γ 6= ∅, and ∃γ ∈ Γ, following holds

(q, c) ∈ Ag ⇐⇒ (q, γ(c)) ∈ Ap
i.e., the corresponding slots aligned to the same
textual number. TEMPLEQUIV found a Γ 6= ∅ im-
plies templates are equivalent (w.h.p). Therefore,
∃γ ∈ Γ such that the corresponding slots aligned
to the same textual number implies the alignments
are equivalent under mapping γ. Together they im-
ply that the derivation was equivalent (w.h.p.).

504


