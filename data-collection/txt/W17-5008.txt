



















































Evaluation of Automatically Generated Pronoun Reference Questions


Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 76–85
Copenhagen, Denmark, September 8, 2017. c©2017 Association for Computational Linguistics

Evaluation of Automatically Generated Pronoun Reference Questions

Arief Yudha Satria and Takenobu Tokunaga
School of Computing

Tokyo Institute of Technology
152-8550 Tokyo, Meguro, Ôokayama, 2-12-1, Japan
satria.a.aa@m.titech.ac.jp take@c.titech.ac.jp

Abstract

This study provides a detailed analysis of
evaluation of English pronoun reference
questions which are created automatically
by machine. Pronoun reference questions
are multiple choice questions that ask test
takers to choose an antecedent of a target
pronoun in a reading passage from four
options. The evaluation was performed
from two perspectives: the perspective of
English teachers and that of English learn-
ers. Item analysis suggests that machine-
generated questions achieve comparable
quality with human-made questions. Cor-
relation analysis revealed a strong cor-
relation between the scores of machine-
generated questions and that of human-
made questions.

1 Introduction

Asking questions has been widely used as a
method to assess the effectiveness of teaching and
learning activities. By asking questions, teach-
ers can get feedback whether students understand
about the teaching materials. In this context, creat-
ing questions becomes an important task in teach-
ing and learning activities. Questions are usu-
ally made by human experts, which demands man-
ual efforts; thus it is time-consuming and expen-
sive. Automatic question generation is a solution
to solve this problem.

Several past studies worked on various kinds
of automatic question generation. Heilman and
Smith (2009) worked on the automatic question
generation for the purpose of reading compre-
hension assessment and practice. Liu and Calvo
(2012) worked on the automatic generation of trig-
ger questions (directive and facilitative) for sup-
porting writing activities. Chali and Hasan (2015)

worked on the automatic generation of all possi-
ble questions given a topic of interest. Serban
et al. (2016) worked on the automatic generation
of questions about an image.

Research on automatic question generation has
been active, yet there are few studies which elab-
orate the detailed evaluation process and in-depth
analysis of the machine-generated questions. QG-
STEC 2010 is the first shared task about question
generation that comprises two subtasks: question
generation from paragraphs and question genera-
tion from sentences (Rus et al., 2010). Human
judges were utilised to evaluate question quality
by considering five criteria: syntactic correctness
and fluency, question type, relevance, ambiguity,
and variety.

Liu and Calvo (2012) evaluated their trigger
question generation system for academic writing
support by comparing machine-generated trigger
questions to human-made trigger questions based
on five aspects: clarity, correctness, relevance,
usefulness for learning concepts, and usefulness to
improve the literature review documents. Twenty-
three students were instructed to write essays and
then to assess the trigger questions if these ques-
tions could improve their essays. Because the
machine-generated trigger questions were created
based on the collected student essays, their anal-
ysis showed that they were effective only for the
collected student essays while the human-made
trigger questions were effective for general essays
as well as the collected essays.

Zhang and VanLehn (2016) employed students
to rate machine-generated questions and human-
made questions based on relevance, fluency, am-
biguity, pedagogy and depth. Araki et al. (2016)
evaluated their question generation system by
judging the questions on three metrics: grammat-
ical correctness, answer existence and inference
steps.

76



On John Black Tuley's land, on Meshach Creek, 6
miles northeast of Tompkinsville, two human
skeletons were found in a small opening, which has
since been known as the Bone Cave. It is a room not
over 10 feet across at any part, in a limestone
conglomerate, and may be of quite recent origin.
Being inconvenient of access, it is not in a position
for residence purposes. The skeletons were probably
those of Indian hunters. They were less than 2 feet
below the surface. The material in which the little
cave is formed will crumble easily in cold weather,
being rather wet from the soil water soaking through
the hill above it.

The word “they” in the passage 
refers to
(A) skeletons
(B) feet
(C) purposes
(D) hunters

1: reading passage

2: target pronoun

3: correct answer

4: distractors

Figure 1: Example of pronoun reference question

Susanti et al. (2017) utilised English teachers
and students to evaluate their question generation
system. English teachers were asked to distin-
guish machine-generated questions from human-
made questions apart. The English teachers also
judged the questions on their usability in a real test
and their difficulties using five scale rating. They
also received suggestions to improve the questions
from the English teachers. Furthermore, students
were asked to answer the machine-generated ques-
tions and human-made questions; their answers
were analysed using item analysis and the analysis
based on Neural Test Theory (Shojima, 2007).

To sum up, the evaluation of automatic question
generation systems in the past research was per-
formed by utilising human judges and students.
In this study, we provide detailed evaluation ex-
periments and analysis of automatically generated
pronoun reference questions. Pronoun reference
questions consist of four components, i.e. a read-
ing passage, a target pronoun, a correct answer,
and three distractors as illustrated in Figure 1. We
focus on pronoun reference questions because they
measure the test taker’s ability to resolve pronoun
in reading passages. We argue that resolving pro-
noun is an important skill for reading comprehen-
sion.

The evaluation target of this study is the English
pronoun reference questions generated by our sys-
tem (Satria and Tokunaga, 2017). To the best of
our knowledge, there is no other system for gen-
erating pronoun reference questions. The system
generates questions from human-written texts by
performing a sentence splitting technique on non-
restrictive relative clauses. The details of the ques-
tion generation system are explained in Section 2.
We evaluate the questions from two different per-
spectives following Susanti et al. (2017). The first

perspective is from English teachers. We argue
that English teachers have the ability to differenti-
ate the good questions from the bad ones because
creating questions is one of the teacher’s respon-
sibilities in the classroom; thus asking English
teachers to judge the quality of machine-generated
questions is reasonable. The second perspective is
from English learners. Good questions can dis-
criminate high proficiency learners from low pro-
ficiency learners. English learners were instructed
to answer the questions and their responses were
used for analysing the characteristics of the ques-
tions.

In what follows, we explain the automatic ques-
tion generation system to be evaluated (Section 2),
followed by the elaboration of the evaluation from
the English teacher perspective (Section 3) and the
English learner perspective (Section 4). We con-
clude the evaluation results and point out the pos-
sible future research direction (Section 5).

2 Generating pronoun reference
questions

Pronoun reference questions such as in Figure 1
ask test takers to identify the antecedent of the
target pronoun in the reading passage; thus the
correct answer can be obtained by employing an
anaphora resolution system to identify the an-
tecedent of the target pronoun. Using this ap-
proach, the performance of the anaphora resolu-
tion system directly affects the quality of the gen-
erated questions. Since the performance of the
state-of-the-arts anaphora resolution system is still
insufficient to be employed for generating pro-
noun reference questions, we proposed to utilise
nonrestrictive relative clauses to obtain pairs of
the correct answer (antecedent) and the target pro-
noun (Satria and Tokunaga, 2017). The core idea

77



of our method is transforming a sentence with a
nonrestrictive relative clause into two sentences
by applying a sentence splitting technique with re-
placing the relative pronoun with a personal pro-
noun. An assumption behind our method is that
the antecedent identification of relative pronouns
is relatively easier than that of personal pronouns
because the antecedents of the relative pronouns
appear in a restricted region in the sentence.

The system receives human-written texts from
Project Gutenberg1 that span several genres (i.e.
science, technology and history) and produces
question components based on the texts. The
question generation process comprises four steps:
correct answer generation, reading passage gen-
eration, target pronoun generation, and distractor
generation.

The nonrestrictive relative clause is vital in our
system because we transform human-written texts
by applying the sentence splitting technique re-
garding nonrestrictive relative clauses to create the
correct answer, the reading passage and the tar-
get pronoun. Nonrestrictive relative clauses are
clauses that do not specify its modifying noun;
they only give additional information to it in-
stead. Thus, they can be detached from their main
clauses. This property allows the sentence split-
ting technique to work most of the cases without
changing the meaning of the texts.

There are cases, however, where the sentence
splitting induces a change of text meaning, mostly
due to the introduced pronoun refers to a differ-
ent antecedent from that referred to by the relative
pronoun in the original sentence. For instance, the
text (2) is derived from the text (1) by extracting
the nonrestrictive relative clause (underlined part)
and replacing the relative pronoun “which” with a
pronoun “it”. The antecedent of “it” in the third
sentence looks to be “legend”, a subject in the pre-
vious sentence. But it should be “knowledge” in
the previous sentence when we look at the original
sentence where “which”, the counterpart of “it” in
(2), obviously refers to “knowledge”. To exclude
such spurious anaphora, we apply the Centering
theory (Brennan et al., 1987; Grosz et al., 1995)
to see the introduced pronoun refers to the same
antecedent as in the original sentence. In this par-
ticular example, the Centering theory tells us that
“legend” in the second sentence of (2) has a higher
status than “knowledge” because the former is a

1https://www.gutenberg.org/

subject and the latter is an element in the prepo-
sitional phrase. Thus “legend” is a more probable
antecedent of “it”, which contradicts the original
sentence of (1).

(1) The church of S. Croce has seen another
strange death of a Pope, that of Sylvester
II. (999-1003), a Frenchman, Gerbert by
name. A legend, related first by car-
dinal Benno in 1099, describes him as
deep in necromantic knowledge, which he
had gathered during a journey through the
Hispano-Arabic provinces.

(2) The church of S. Croce has seen another
strange death of a Pope, that of Sylvester
II. (999-1003), a Frenchman, Gerbert by
name. A legend, related first by car-
dinal Benno in 1099, describes him as
deep in necromantic knowledge. He had
gathered it during a journey through the
Hispano-Arabic provinces.

2.1 Correct answer generation

The identified antecedent of the relative pronoun
is used as a correct answer. To identify the an-
tecedent of the relative pronoun, we employed
both lexical parser and dependency parser. The
lexical parser produces a parse tree of the target
sentence, i.e. a sentence that contains a nonre-
strictive relative clause. The parse tree is traversed
based on hand-made rules (Satria and Tokunaga,
2017) which consider the syntactic attachment and
the linguistic feature, i.e. number. The depen-
dency parser produces a set of dependencies which
include the acl:relc2 dependency relation. If only
both results from the lexical parser together with
hand-made rules and the dependency parser agree
on the antecedent of the relative pronoun, the tar-
get sentence is further processed in the next steps.
The system discards the target sentence which
causes discordance on the antecedent of the rel-
ative pronoun.

2.2 Reading passage and target pronoun
generation

We create a reading passage by splitting a sen-
tence at a nonrestrictive relative clause. Sentence
splitting divides the target sentence into two sen-
tences: the main clause and the relative clause.

2http://universaldependencies.org/docs/en/dep/acl-
relcl.html

78



Table 1: Example of the evaluation table filled by the evaluators

question quality reading passage target pronoun correct answer distractors comments

Q1 2 X X
...

Q2 1 X
...

...
...

...
...

...
...

...

Q60 3
...

When splitting the target sentence, the connection
between two sentences must be maintained in or-
der to retain the sentence meaning. The connec-
tion of those sentences is maintained through the
target pronoun. The system creates the target pro-
noun by replacing the relative pronoun with a per-
sonal pronoun with considering linguistic features.
Because the target pronoun resides in the reading
passage, splitting target sentence and replacing the
relative pronoun with the target pronoun complete
the reading passage generation. For instance, the
text (4) is derived from (3). The underlined non-
restrictive relative clause in (3) is taken out into a
separate sentence and placed after the main clause
in (4). At the same time, the relative pronoun in
the relative clause is replaced with the personal
pronoun “they”. We further confirm that the intro-
duced pronoun “they” surely refers to the subject
in the previous sentence regarding the Centering
theory.

(3) The flowers, which are individually larger
than those of the False Acacia, are of a
beautiful rosy-pink, and produced in June
and July.

(4) The flowers are of a beautiful rosy-pink,
and produced in June and July. They are
individually larger than those of the False
Acacia.

2.3 Distractor generation

Distractor generation comprises the following
three steps.

Candidate generation Since we restrict the an-
tecedent of the pronoun, i.e. the correct answer,
to a noun or a noun phrase, distractors must also
be nouns or noun phrases. The part-of-speech tag-
ger was employed to extract all nouns and noun
phrases in the passage. The incompatible candi-
dates on linguistic features are eliminated from the
distractor candidates.

Coreference chain extraction A coreference
chain consists of a list of expressions that refer to
the same entity in a text. Thus, expressions in the
same coreference chain with the correct answer
are also a possible correct answer. Therefore, they
are eliminated from the distractor candidates.

Candidate ranking Since we need only three
distractors, the distractor candidates are ranked on
the recency principle. Recently mentioned enti-
ties are likely to be maintained in human mem-
ory because they are still fresh; thus those entities
are likely to be referred to by pronouns. More re-
cently mentioned entities are ranked higher than
the less recently mentioned entities. Finally, the
three highest ranked candidates are selected as the
distractors.

3 Evaluation from English teacher
perspective

3.1 Experimental setting
We asked five English teachers3 to evaluate the
quality of 60 machine-generated questions by as-
signing a score of one, two or three to each ques-
tion. The meaning of the scores is described be-
low.

1. problematic, the question is not usable in a
real test. Significant modifications are neces-
sary for real use.

2. acceptable but can be improved, the ques-
tion is usable in a real test as it is, but it can
be further improved.

3. acceptable, the question has no problem to
be used in a real test without any change.

If the question quality is judged to be one
or two, the evaluators must further identify the
problematic question components by checking

3They are non-native English speakers but the TESOL
(http://www.tesol.org) certificate holders.

79



the corresponding columns as shown in Table 1.
The evaluators leave the problematic components
columns empty for acceptable quality questions.
The evaluators may optionally give comments on
problematic components or suggestions to im-
prove the question quality.

Table 2: Distribution of pairwise disagreement

evaluator\score {1,2} {1,3} {2,3}
(A, B) 7 4 28
(A, C) 2 7 14
(A, D) 4 6 24
(A, E) 2 8 20
(B, C) 1 1 28
(B, D) 1 0 28
(B, E) 3 4 30
(C, D) 1 0 27
(C, E) 4 3 22
(D, E) 1 5 19

total 26 38 240

Table 3: Distribution of rating

evaluator\score 1 2 3 total
A 10 18 32 60
B 1 35 24 60
C 1 20 39 60
D 0 18 42 60
E 6 16 38 60

total 18 107 175 300

3.2 Result and discussion
First, we investigated the agreement between the
evaluators by computing the ordinal Krippen-
dorff’s alpha (Krippendorff, 1970); it was 0.05 in-
dicating very low agreement between the evalua-
tors. We further investigated the reason of the low
agreement. We calculated the pairwise disagree-
ment frequency between every pair of the eval-
uators as shown in Table 2. The table indicates
that the disagreement between the judgement “ac-
ceptable but can be improved” and “acceptable”
({2, 3}) is dominant (80%). This fact suggests the
decision on these two categories is highly subjec-
tive. Since they are both acceptable categories, we
recalculated the Krippendorff’s alpha after merg-
ing them into a single category to obtain the value
0.06. The average of the pairwise observation
agreement was 0.89 after merging. Table 3 shows
the distribution of scores judged by each evaluator.
As the table shows, the highly skewed distribution
of judgment can be considered as the main reason

of a very low alpha despite the fairly high obser-
vation agreement.

Table 4: Majority quality scores of 60 questions

majority score frequency

1 0
2 12
3 39

tie 9

total 60

Table 4 shows the distribution of the quality
score calculated by the majority principle. The
majority principle means that when at least three
evaluators rate a same value, that particular value
is defined as the question quality score. Table 5 in-
dicates that there are 39 questions (65%) which the
majority of the evaluators rated “acceptable (3)”.
All nine tie cases get at most two “problematic”
rating, i.e. the “problematic” can not be the ma-
jority. This means all generated questions were
judged “usable in a real test” based on the major-
ity principle.

Table 5: Average quality scores of 60 questions

average score frequency

1.6 1
1.8 1
2.0 4
2.2 8
2.4 7
2.6 22
2.8 13
3.0 4

total 60

Table 5 summarises the average quality scores
of five evaluators with their frequency. Even
though the majority quality is the same, the actual
rating may be different; thus it yields a different
average quality. The question with the score 1.6
gets two ones and three twos. All evaluators agree
that this particular question has an error in the cor-
rect answer. The question with the score 1.8 gets
two ones, two twos and one three. Four evaluators
agree that this particular question has an error in
the correct answer.

Table 6 summarises the comments from the five
evaluators with their frequency. The most com-
mon comments are related to the correct answer.
This tendency is consistent with the component-
wise evaluation of our past research (Satria and

80



Table 6: Evaluator’s comments with frequency

comments frequency

other option could be the correct answer 71
the reading passage is too long 28
the distractors do not distract 18
the distractors are too distracting 11
the reading passage offers little context 6
there are multiple correct answers 5
the reading passage has many technical word 4

(i.e. too difficult)
the correct answer is too obvious 1
the target pronoun is inadequate 1

Tokunaga, 2017). We counted the number of ques-
tions with a checked cell in the “correct answer”
column of the evaluation table (Table 1) to find
80 such cells in total. This number is roughly
the same as that of the comments on correct an-
swers. Among these 80 questions, 12 questions
were rated 1 (problematic) and 68 were rated 2
(acceptable but can be improved). These cases
suggest that the filtering with the Centering theory
should be further improved.

4 Evaluation based on English learner
perspective

The evaluation from the English learner perspec-
tive was conducted to evaluate the behaviour of
machine-generated questions in measuring test
taker’s proficiency.

4.1 Experimental setting

We prepared three sets of questions each of
which contains ten machine-generated questions
(MGQs) and ten human-made questions (HMQs),
in total 20 questions. These 30 HMQs were ran-
domly selected from TOEFL preparation books
while these 30 MGQs were randomly selected
from the set of MGQs which were judged accept-
able on the majority principle in the evaluation by
the English teachers as described in Section 3. The
question sets were created so that the difference of
the average of question difficulty across the ques-
tion sets was minimised. The balance of ques-
tion difficulty among three groups, and between
MGQs and HMQs is important because we cal-
culate the student-wise score correlation between
scores from MGQs and HMQs as explained later
in 4.2.

To balance question difficulty among the ques-
tion sets, we utilised the reading passage diffi-
culty. A question is considered difficult if its read-

Dr.1 M. Aurel9 Stein9, principal2 of1 the1 Oriental7
College1 at1 Lahore9, has1 now1 ready1 for1 publication4
the1 first1 volume2 of1 his1 critical3 edition4 of1 the1
Rajatarangini9, or1 Chronicles8 of1 the1 Kings1 of1
Kashmir9, upon1 which1 he1 has1 been1 engaged3 for1
some1 years1. This1 work1 is1 of1 special1 interest1
as1 being1 almost1 the1 sole4 example1 of1 historical2
literature2 in1 Sanskrit9. It1 was1 written2 by1 the1 poet2
Kalhana9 in1 the1 middle1 of1 the1 twelfth1 century1.

Figure 2: Example of reading passage with word
difficulty level (subscripts correspond to the level)

Table 7: Mean of reading passage difficulty

metric question set MGQ HMQ

average Qs1 2.15 2.14
JACET8000 Qs2 2.13 2.13

Qs3 2.12 2.12

Flesch-Kincaid Qs1 9.9 11.2
grade level Qs2 10.0 9.8

Qs3 9.6 10.7

Flesch-Kincaid Qs1 60.3 46.1
reading ease Qs2 59.9 58.9

Qs3 65.2 50.5

Dale-Chall Qs1 9.0 9.9
readability formula Qs2 9.0 9.1

Qs3 8.9 9.7

ing passage is difficult and vice versa. The read-
ing passage difficulty is calculated based on the
word difficulty in the passages. We employed
JACET8000 (Uemura and Ishikawa, 2004), a list
of 8,000 English words divided into eight levels
of word difficulty based on their word frequency.
Level 1 is the most frequent (i.e. the easiest) while
level 8 is least frequent (i.e. the most difficult).
Words that do not appear in the list are considered
even less frequent than level 8; thus they are con-
sidered to be level 9. To obtain the reading passage
difficulty, we assigned a JACET8000 word diffi-
culty level to every word in the reading passage as
illustrated in Figure 2 and calculated the average
of the difficulty levels. The average of reading pas-
sage difficulty for each question set is presented in
Table 7.

Many metrics to measure text readability have
been proposed in the past, such as Flesch-Kincaid
grade level (Kincaid et al., 1975), Flesch-Kincaid
reading ease (Kincaid et al., 1975) and Dale-Chall
readability formula (Dale and Chall, 1948). The
first two calculate text difficulty with respect to
the number of sentences, words and syllables in
the text. The third one takes into account the dif-
ficulty of each word as well. Table 7 also shows

81



Table 8: TOEIC score of each group

student question TOEIC score number of
group set mean SD students

1 Qs1 561 146 31
2 Qs2 559 123 25
3 Qs3 554 122 25

the mean values of these metrics for each question
set and generation mode, i.e. machine-generated
vs. human-made. Overall, the difficulty of read-
ing passages in every question set is well balanced
against every metric.

Eighty-one Japanese university students (57
first year and 24 second year students) were re-
cruited and divided into three groups, 27 students
for each group, considering their TOEIC scores;
we did our best to minimise the difference of
the score distribution and the mean of the scores
across these three groups. Each student group was
assigned a different question set and instructed to
finish the assigned question set within 30 minutes.

4.2 Result and discussion
Although we made three groups of the same num-
ber of students (27) and assigned a different ques-
tion set to each group, four students mistakenly
worked on a wrong question set. Therefore the
distribution of the number of students in a group
was skewed as shown in Table 8. Table 8 also
shows the average TOEIC score of each group
with a standard deviation (SD).

Table 9: Item difficulty of MGQs and HMQs

MGQ HMQ

mean 0.59 0.60
standard deviation 0.24 0.17
minimum 0.20 0.26
maximum 0.96 0.90

The item analysis investigates the test taker’s re-
sponses to individual question items to evaluate
the quality of those items. It often uses two mea-
sures: the item difficulty and the item discrimina-
tion index. The item difficulty is a proportion of
the number of test takers who answered correctly
to the number of all test takers (Brown, 2013). The
value ranges from 0 to 1 with a larger value repre-
senting an easier item. Table 9 shows the descrip-
tive statistics of the item difficulty of the sets of 30
MGQs and 30 HMQs.

fr
eq

ue
nc

y

0

2

4

6

8

10

item difficulty

[0.
2 -

 0.
3)

[0.
3 -

 0.
4)

[0.
4 -

 0.
5)

[0.
5 -

 0.
6)

[0.
6 -

 0.
7)

[0.
7 -

 0.
8)

[0.
8 -

 0.
9)

[0.
9 -

 1.
0]

MGQ
HMQ

Figure 3: Distribution of item difficulty

Table 9 shows no big difference in mean of the
item difficulty between MGQs and HMQs. This
result suggests that MGQs have similar difficulty
with HMQs. This is consistent with the fact we
maintained the balance of question difficulty be-
tween MGQs and HMQs as explained in Sub-
section 4.1. We also provide the distribution of
the item difficulty of the MGQs and HMQs in
Figure 3. Although the mean is similar between
the MGQs and HMQs as shown in Table 9, Fig-
ure 3 reveals that the distribution of the item diffi-
culty for HMQs is closer to the normal distribution
than that for MGQs. We conducted the Levene’s
test (Levene et al., 1960) to assess the item dif-
ficulty variance homogeneity between MGQs and
HMQs to find that their variances are not homoge-
neous. As we do not care about controlling item
difficulty when generating question items, this is a
natural consequence.

Mexico, 1818. This species, though not hardy enough for
every situation, is yet sufficiently so to stand unharmed as
a wall plant. It grows from 10 feet to 12 feet high, with
deep-green leaves that are hoary on the under sides. The
flowers are bright blue, and produced in June and the fol-
lowing months. They are borne in large, axillary panicles.
In a light, dry soil and sunny position this shrub does well
as a wall plant, for which purpose it is one of the most or-
namental. There are several good nursery forms, of which
the following are amongst the best: C. azureus Albert Pet-
titt, C. azureus albidus, C. azureus Arnddii, one of the best,
C. azureus Gloire de Versailles, and C. azureus Marie Si-
mon.
(A) leaves
(B) sides
(C) flowers← correct answer
(D) months

Figure 4: The easiest question

Figure 4 shows the easiest question item while

82



There are two recesses in the cliff on the opposite side of
the little creek formed by the spring. They are 40 to 50
feet above the water, each with an irregular floor of 20 by
30 feet under shelter of the rock. No solid rock is visible
in front of them, but a projecting ledge appears on either
side about 6 feet below the present average level of the
floor; and this is probably the depth of accumulation at the
front. It seems continuous. It may be less toward the rear.
The cavities are in a stratum which is somewhat shelly and
crumbles easily.
(A) ledge← correct answer
(B) depth
(C) accumulation
(D) front

Figure 5: The most difficult question

fr
eq

ue
nc

y

0

2

4

6

8

discrimination index

[-0
.2 

- 0
.1)

[-0
.1 

- 0
)

[0.
0 -

 0.
1)

[0.
1 -

 0.
2)

[0.
2 -

 0.
3)

[0.
3 -

 0.
4)

[0.
4 -

 0.
5)

[0.
5 -

 0.
6)

[0.
6 -

 0.
7)

[0.
7 -

 0.
8)

[0.
8 -

 0.
9)

MGQ
HMQ

Figure 6: Distribution of item discrimination in-
dex

Figure 5 shows the most difficult one in the MGQs
in which the target pronoun is in bold and the op-
tions are underlined in the reading passage for the
readability purpose. Twenty-four out of 25 stu-
dents answered correctly for the easiest one. This
question item is easy because the subject pronoun
refers to the subject of the previous sentence. Only
five out of 25 students answered correctly for the
most difficult question item. Both extremes are not
preferable in measuring test taker’s proficiency be-
cause too easy items lead to very high scores while
too difficult items lead to very low scores for the
most of test takers.

We calculated the Pearson correlation coeffi-
cient between the JACET8000 based reading pas-
sage difficulty as we defined in Table 7 and the
item difficulty of the MGQs and obtained the value
of 0.56. This result suggests that the reading pas-
sage difficulty can be one of the important factors
for predicting and controlling the item difficulty of
question items.

The item discrimination index is a metric to
measure the discrimination power of question

items (Brown, 2013). The discrimination power
is the ability of question items in discriminating
high-proficiency test takers from low-proficiency
test takers. This metric is vital for language test-
ing because a good test must be able to discrim-
inate test taker’s proficiency precisely. The item
discrimination index of a question item i is com-
puted as follows

IDi =
Ui − Li

n
,

where Ui and Li represent the number of test tak-
ers who correctly answered the question item i in
the high proficiency group and the low proficiency
group respectively, and n denotes the number of
test takers in a group. The groups of high and
low proficiency are defined as the top 27% of the
test takers and bottom 27% of the test takers re-
spectively. The threshold value of 27% is utilised
to maximise two characteristics; those two groups
must be as different as possible to discriminate
clearly, and the number of test takers in each group
must be as large as possible to achieve reliabil-
ity (Popham, 1981; Kelley, 1939).

We computed the item discrimination index for
each question item and the average of them. The
average is 0.33 for the MGQs and 0.37 for the
HMQs. A question item is considered to be ac-
ceptable if its discrimination index is greater than
or equal to 0.2 (Brown, 1983). According to this
criteria, we counted the number of question items
of which the discrimination index is greater than
or equal to 0.2. Out of 30 question items, the
22 MGQs and 24 HMQs items cleared this con-
dition. Figure 6 shows the distribution of the dis-
crimination index. There seems to be no big dif-
ference between the MGQs and HMQs in terms of

The region may be roughly characterized as a vast sandy
plain, arid in the extreme; or rather as two such plains,
separated by a chain of mountains running northwest and
southeast. In the southern part of the reservation this
mountain range is known as the Choiskai mountains, and
here the top is flat and mesa-like in character, dotted with
little lakes and covered with giant pines. They in the sum-
mer give it a park-like aspect. The general elevation of
this plateau is a little less than 9,000 feet above the sea
and about 3,000 feet above the valleys or plains east and
west of it.
(A) plains
(B) mountains
(C) lakes
(D) pines← correct answer

Figure 7: MGQ example with a poor discrimina-
tion index (ID = 0.125)

83



the average discrimination index (0.33 vs. 0.37)
and the number of items clearing the 0.2 crite-
rion (22 vs. 24). Their distribution reveals that
the HMQs shows a slightly better distribution than
the MGQs. However, the MGQs have comparable
discrimination power as the HMQs.

Figure 7 shows an example of MGQ which has
a poor discrimination index, i.e. ID = 0.125.
Three test takers in the high proficiency group and
two test takers in the low proficiency group an-
swered correctly. The distractor “mountains” dis-
tracted test takers in the high proficiency group
very much; thus the number of correctly answered
test takers was almost the same between the two
groups. The potential reason is that “mountains”
appears twice in the text, so it lured the test takers
to choose “mountains”.

To assess the ability of the MGQs in measuring
test taker’s proficiency, we calculated the correla-
tion between the test taker’s score of the MGQs
and other scores including that of the HMQs and
TOEIC scores. We argue that the test taker’s
TOEIC scores provide their true English profi-
ciency. The Pearson correlation coefficient (Pear-
son, 1896) was calculated, presented in Table 10.
The p-value of all the correlation coefficients is
less than 0.05.

Table 10 shows that there is no big difference
between the MGQs and HMQs in terms of the cor-
relation between the test taker’s scores and their
TOEIC scores. Furthermore, the correlation with
the TOEIC Reading scores is stronger than that
with the TOEIC Listening scores. This is a rea-
sonable tendency because the pronoun reference
questions are designed for assessing reading com-
prehension ability.

5 Conclusion

This paper presented the evaluation of auto-
matically generated pronoun reference questions
which ask test takers the antecedent of the spec-
ified pronoun in the reading passage. A pronoun
reference question was automatically generated by
splitting a sentence in a human-written text at a
nonrestrictive relative clause and replacing the rel-
ative pronoun with a personal pronoun.

The evaluation was performed from two differ-
ent perspectives: the English teacher perspective
and the English learner perspective. Automati-
cally generated 60 question items were evaluated
by five English teachers, resulting in that 39 out

Table 10: Peason correlation coefficients between
test taker’s scores

MGQ HMQ

TOEIC Listening 0.56 0.57
TOEIC Reading 0.65 0.68
TOEIC Listening & Reading 0.74 0.77
HMQ 0.61 —

of 60 (65%) question items were considered ac-
ceptable to be used in a real test. We administered
30 MGQs from these acceptable question items to-
gether with 30 HMQs from TOEFL preparation
books to the 81 university students. The analy-
sis results of the test taker’s responses showed that
the MGQs achieved comparable quality with the
HMQs on their item difficulty and item discrimi-
nation index. Furthermore, there was a strong cor-
relation between the MGQ scores and the TOEIC
scores of the same test takers.

Possible future work includes controlling item
difficulty of the generated questions and generat-
ing other types of questions. For instance, our ex-
perimental result suggested that the item difficulty
of the generated questions had a moderate corre-
lation with the reading passage difficulty. Thus,
controlling the passage difficulty might enable us
to control the difficulty of the question items. We
also need to further explore other factors affecting
the item difficulty.

References
Jun Araki, Dheeraj Rajagopal, Sreecharan Sankara-

narayanan, Susan Holm, Yukari Yamakawa, and
Teruko Mitamura. 2016. Generating questions and
multiple-choice answers using semantic analysis of
texts. In Proceedings of COLING 2016, the 26th In-
ternational Conference on Computational Linguis-
tics: Technical Papers. The COLING 2016 Orga-
nizing Committee, Osaka, Japan, pages 1125–1136.
http://aclweb.org/anthology/C16-1107.

Susan E. Brennan, Marilyn W. Friedman, and Carl J.
Pollard. 1987. A centering approach to pro-
nouns. In Proceedings of the 25th Annual Meet-
ing of the Association for Computational Lin-
guistics. Association for Computational Linguis-
tics, Stanford, California, USA, pages 155–162.
https://doi.org/10.3115/981175.981197.

Frederick Gramm Brown. 1983. Principles of educa-
tional and psychological testing. Holt, Rinehart, and
Winston, 3 edition.

James Dean Brown. 2013. Classical test theory. In
Glenn Fulcher and Fred Davidson, editors, The

84



Routledge handbook of language testing, Routledge,
pages 323–335.

Yllias Chali and Sadid A. Hasan. 2015. Towards topic-
to-question generation. Computational Linguistics
41(1):1–20. https://doi.org/10.1162/COLI a 00206.

Edgar Dale and Jeanne S Chall. 1948. A for-
mula for predicting readability: Instructions.
Educational research bulletin pages 37–54.
http://www.jstor.org/stable/1473669.

Barbara J. Grosz, Aravind K. Joshi, and Scott
Weinstein. 1995. Centering: A framework
for modeling the local coherence of discourse.
Computational Linguistics 21(2):203–225.
http://aclweb.org/anthology/J95-2003.

Michael Heilman and Noah A Smith. 2009. Question
generation via overgenerating transformations and
ranking. Technical report, DTIC Document.

Truman L Kelley. 1939. The selection of upper
and lower groups for the validation of test items.
Journal of educational psychology 30(1):17–24.
http://dx.doi.org/10.1037/h0057123.

J Peter Kincaid, Robert P Fishburne Jr, Richard L
Rogers, and Brad S Chissom. 1975. Derivation of
new readability formulas (automated readability in-
dex, fog count and flesch reading ease formula) for
navy enlisted personnel. Technical report, DTIC
Document.

Klaus Krippendorff. 1970. Estimating the re-
liability, systematic error and random er-
ror of interval data. Educational and
Psychological Measurement 30(1):61–70.
https://doi.org/10.1177/001316447003000105.

Howard Levene et al. 1960. Robust tests for equal-
ity of variances. Contributions to probability and
statistics 1:278–292.

Ming Liu and Rafael A. Calvo. 2012. Using infor-
mation extraction to generate trigger questions for
academic writing support. In Stefano A. Cerri,
William J. Clancey, Giorgos Papadourakis, and
Kitty Panourgia, editors, Intelligent Tutoring Sys-
tems: 11th International Conference, ITS 2012,
Chania, Crete, Greece, June 14-18, 2012. Proceed-
ings, Springer Berlin Heidelberg, Berlin, Heidel-
berg, pages 358–367.

Karl Pearson. 1896. Mathematical contributions to
the theory of evolution. iii. regression, heredity, and
panmixia. Philosophical Transactions of the Royal
Society of London. Series A, Containing Papers of a
Mathematical or Physical Character 187:253–318.
http://www.jstor.org/stable/90707.

W.J. Popham. 1981. Modern educational measure-
ment. Englewood Cliff, NJ: Prentice-Hall.

Vasile Rus, Brendan Wyse, Paul Piwek, Mihai Lintean,
Svetlana Stoyanchev, and Cristian Moldovan. 2010.
The first question generation shared task evaluation
challenge. In Proceedings of the 6th International
Natural Language Generation Conference. Associa-
tion for Computational Linguistics, pages 251–257.
http://www.aclweb.org/anthology/W10-4234.

Arief Yudha Satria and Takenobu Tokunaga. 2017.
Automatic generation of english reference ques-
tion by utilising nonrestrictive relative clause. In
Proceedings of the 9th International Conference
on Computer Supported Education - Volume 1:
CSEDU. INSTICC, ScitePress, pages 379–386.
https://doi.org/10.5220/0006320203790386.

Iulian Vlad Serban, Alberto Garcı́a-Durán, Caglar
Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron
Courville, and Yoshua Bengio. 2016. Generat-
ing factoid questions with recurrent neural net-
works: The 30m factoid question-answer corpus.
In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 588–598.
http://www.aclweb.org/anthology/P16-1056.

K Shojima. 2007. Neural test theory. DNC Research
Note 7(02):1–12.

Yuni Susanti, Takenobu Tokunaga, Hitoshi Nishikawa,
and Hiroyuki Obari. 2017. Evaluation of automat-
ically generated english vocabulary questions. Re-
search and Practice in Technology Enhanced Learn-
ing 12(1):11. https://doi.org/10.1186/s41039-017-
0051-y.

Toshihiko Uemura and Shinichiro Ishikawa. 2004.
Jacet 8000 and asia tefl vocabulary initiative. Jour-
nal of Asia TEFL 1(1):333–347.

Lishan Zhang and Kurt VanLehn. 2016. How do
machine-generated questions compare to human-
generated questions? Research and Prac-
tice in Technology Enhanced Learning 11(1):7.
https://doi.org/10.1186/s41039-016-0031-7.

85


