



















































Assessing the Stylistic Properties of Neurally Generated Text in Authorship Attribution


Proceedings of the Workshop on Stylistic Variation, pages 116–125
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Assessing the Stylistic Properties of Neurally Generated Text in
Authorship Attribution

Enrique Manjavacas1, Jeroen De Gussem2, Walter Daelemans1, and Mike Kestemont1

1University of Antwerp, CLiPS, {firstname,lastname}@uantwerpen.be
2Ghent University, Department of History, jedgusse.degussem@ugent.be

Abstract

Recent applications of neural language
models have led to an increased interest
in the automatic generation of natural lan-
guage. However impressive, the evalua-
tion of neurally generated text has so far
remained rather informal and anecdotal.
Here, we present an attempt at the system-
atic assessment of one aspect of the quality
of neurally generated text. We focus on a
specific aspect of neural language genera-
tion: its ability to reproduce authorial writ-
ing styles. Using established models for
authorship attribution, we empirically as-
sess the stylistic qualities of neurally gen-
erated text. In comparison to conventional
language models, neural models generate
fuzzier text that is relatively harder to at-
tribute correctly. Nevertheless, our results
also suggest that neurally generated text
offers more valuable perspectives for the
augmentation of training data.

1 Introduction

In his landmark paper ‘Computing Machinery and
Intelligence‘, Turing (1950) quoted Jefferson’s
‘The Mind of Mechanical Man’ (1949): ‘Not until
a machine can write a sonnet or compose a con-
certo because of thoughts and emotions felt, and
not by the chance fall of symbols, could we agree
that machine equals brain’. Strikingly, these early
pioneers of modern AI considered the conscious
creation of literature as a significant milestone on
the long road towards general AI. In recent years,
the automated generation of text, such as litera-
ture, has received a significant impetus from re-
search in the field of neural language modeling.
A variety of recent studies have demonstrated that
neural language models can be used to synthesize
new (literary) text, even at the character-level.

To a surprising extent, neurally generated text
seems to make an authentic impression on read-
ers, due to its ability to mimic certain properties
of the text on which it was trained, without it de-
grading into in a mere reproduction or patchwork
of verbatim passages in it. In one particularly visi-
ble blog post, Karpathy (2015) demonstrated how
a relatively simple character-level recurrent neural
network, when trained on Shakespeare’s oeuvre,
was able to generate new, artificial text which, cer-
tainly in the eyes of non-experts, undeniably dis-
played some Shakespearean qualities. This blog
has inspired a wide array of other applications –
ranging from cooking recipes (Brewe, 2015) to
Bach’s sonatas (Feynman et al., 2016).

Much of this work has so far been published
in the online blogosphere and the assessment of
the quality of neurally generated text has often re-
mained fairly informal and anecdotal, apart from
a number of more empirically oriented studies,
for instance in the field of hiphop lyric generation
(Potash et al., 2015; Malmi et al., 2015). In this
paper, we report an attempt at a systematic assess-
ment of the properties of neurally generated text
in the context of style-based authorship attribution
in stylometry (Stamatatos, 2009). We address the
following research questions: (1) To which extent
is the text, neurally generated on the basis of a sin-
gle author’s oeuvre, still attributable to the origi-
nal input author? and (2) To which extent is the
neural generation of text useful for training data
augmentation in stylometry, e.g. for authors for
whom little reference data is available?

Below, we first present the model architectures
underlying our text generation, comparing a mod-
ern neural architecture to a more conventional
ngram-based language model. Next, we describe
the Latin data set which we will use (Patrologia
Latina) and discuss our experimental set-up (au-
thorship attribution). We go on to present our at-
tribution results; in the discussion section, we in-

116



terpret and visualize these results. We conclude by
pointing out viable future improvements.

2 Character-Level Text Generation

We approach the task of text generation with
character-level Language Models (LM). In short,
a LM is a probabilistic model of linguistic se-
quences that, at each step in a sequence, assigns
a probability distribution over the vocabulary con-
ditioned on the prefix sequence. More formally, a
LM is defined by Equation 1,

LM(wt) = P (wt|wt−n, wt−(n−1), ..., wt−1) (1)
where n refers to the scope of the model —i.e. the
length of the prefix sequence taken into account to
condition the output distribution at step t. By ex-
tension, a LM defines a generative model of sen-
tences where the probability of a sentence is de-
fined by the following equation:

P (w1, w2, ..., wn) =
n∏
i

P (wt|w1, ..., wt−1) (2)

Given its generative nature, a LM can easily be
used for text generation. We start by sampling
from the output distribution at step t and, then,
we recursively feed back the sampled symbol, to-
gether with any other previous output, to condition
the generative distribution at step t + 1. Equation
3 shows formally the text generation process for a
symbol at step t where w′t−1 is the generated sym-
bol at step t−1 and S refers to any given sampling
method.

w′t = S[P (wt|w′t−n, w′t−(n−1), ..., w′t−1)] (3)
An obvious approach towards sampling is to se-
lect the symbol that maximizes the probability
of the entire generated sequence (argmax decod-
ing). For a large vocabulary (e.g. in the case of a
word-level LM), the search quickly becomes im-
practical and is usually approximated by means of
beam search (including the extreme case of using
a beamsize equal to 1, which corresponds to pick-
ing the most probable symbol at each step). How-
ever, when used for generation, the argmax decod-
ing approach tends to yield repetitive and dull sen-
tences, and eventually runs into dead-end loops.
Therefore, we instead sample from the LM’s out-
put distribution at each step.

The sampling approaches discussed so far at-
tempt to strike a trade-off between variability and

correctness – in the sense of departure from regu-
larities observed in the training data. Beam-search
decoding will tend to generate sentences that are
more formally correct (e.g. more similar to the
sentences observed in the training corpus), while
generating very similar and monotonous output
in the presence of similar histories. Conversely,
multinomial sampling will make the output di-
verge more from the original training data, and
therefore produce a more varied output, but with
a tendency towards more grammatically incorrect
sentences. Focusing on multinomial sampling, the
described trade-off can be operationalized in form
of a parameter τ , mostly referred to as “tempera-
ture", that is in charge of modifying the skewness
of the parameters of the multinomial distribution
to encourage more or less variability in exchange
for potentially less or more formally correct out-
put.1

A further aspect of our LM approach to text
generation is topical variation. In order to ensure
that during generation the LM explores the top-
ical distribution present in the training data, we
implement the following procedure. After having
generated a fixed number of sentences s, a sen-
tence from the LM’s training data is sampled uni-
formly and used to seed the generation of the next
s sentences. Finally, we force the LM to gener-
ate fully terminated sentences by including end-
of-sentence symbols (EOS) during training time
and discarding any output sentence that reaches a
maximum number of characters m without having
generated the EOS symbol – thus, we consider the
generation of a single sentence finished whenever
the EOS symbol is produced and we only gener-
ate sentences with a maximum number of charac-
tersm. This is motivated by the fact that very long
sentences tend to degenerate into poor-quality text.
Our generative system displays a total of 3 genera-
tion hyper-parameters: τ (sampling temperature),
s (reset seed every s sentences) and m (maximum
m characters per sentence).

1Given the multinomial parameters p = {p1, p2, ..., pk}
for a vocabulary size of V , the “freezing” transformation

pτi = p
1
τ
i /

∑V
j p

1
τ
j will flatten the original distribution for

higher values of τ , thereby ensuring more variability in the
output. Conversely, lower values of τ will skew it, thereby
facilitating the outcome of the originally more probable sym-
bol. For τ values approaching 0, we recover the simple
argmax decoding procedure of picking the highest probability
symbol at each step.

117



2.1 Ngram-based Language Model

So far, we have kept the definition of the LM
agnostic with respect to its concrete implemen-
tation. In the current study we compare two
widely-used LM architectures – an ngram-based
LM (NGLM) and a Recurrent Neural Network-
based LM (RNNLM). An NGLM is basically a
conditional probability table for Equation 1 that
is estimated on the basis of the count data for
ngrams of a given length n. Typically, NGLMs
suffer from a data sparsity problem since for a
large enough value of n many possible condition-
ing prefixes will not be observed in the training
data and the corresponding probability distribu-
tion will be missing. To alleviate the sparsity
problem, two techniques—smoothing and back-
off models—can be used that either reserve some
probability mass and evenly redistribute it across
unobserved ngrams (smoothing) or resort back to
a lower-order model to provide an approximation
to the conditional distribution of an unobserved
ngram (backoff models). Here, however, we im-
plement an unsmoothed LM since we only use
the LM for generation, where it is not necessary
to compute probabilities for unseen ngrams. An
unsmoothed NGLM only has one model hyper-
parameter—the ngram order.

2.2 RNN-based Language Model

A RNNLM implements a language model using a
Recurrent Neural Network (RNN) to allow left-to-
right information flow during sequence processing
(Mikolov et al., 2010). As shown in (Bengio et
al., 2003), at a given step t, a RNNLM (Elman,
1990) (i) first computes a distributed representa-
tion wt with dimensionality M of the input xt, (ii)
it then feeds the resulting vector into an RNN layer
that computes a hidden activation ht combining
it with the hidden activation at the previous step
ht−1, and (iii) it projects the hidden activation onto
a space of dimensionality equal to the vocabulary
size V , followed by a softmax function that turns
the output vector into a valid probability distribu-
tion. More formally, given a binary column vec-
tor xt representing the input symbol at step t, we
retrieve its corresponding embedding wt through
wt = Wmxt, where Wm is the embedding matrix
with dimensionality RMxV . The hidden state in
the standard RNN is given by

ht = σ(Wihwi +Whhht−1 + bh) (4)

where Wih and Whh are respectively the input-to-
hidden and hidden-to-hidden projection matrices
with dimensionality RMxH and RHxH , bh is a bias
vector and σ is the sigmoid non-linear function.
Finally, the probability assigned to each entry in
the vocabulary at step t is defined by the softmax

Pt,j =
eot,j∑V
k e

ot,k
(5)

where ot,j is the jth entry in the output vector ot =
Whoht and Who is the hidden-to-output projection
with dimensionality RHxV .

In practice, training an RNN is difficult due to
the vanishing gradient problem (Hochreiter, 1998)
that makes it hard to apply the back-propagation
algorithm for parameter tuning over long se-
quences. Therefore, it is common to implement
the recurrent layer using an enhanced RNN like,
e.g. Long Short-term Memory (LSTM) (Hochre-
iter and Schmidhuber, 1997). An LSTM-based
RNNLM only differs from the previous RNNLM
in the way the hidden activation ht is com-
puted. An LSTM cell incorporates three learnable
gates—an input, forget and output gate—of shape:

it = σ(W iihwt +W
i
hh + b

i
h) (6)

ft = σ(W
f
ihwt +W

f
hh + b

f
h) (7)

ot = σ(W oihwt +W
o
hh + b

o
h) (8)

whereW i,W f andW o are, respectively, the gates
parameters, and a writable memory cell ct that is
updated following

ct = ft�ct−1+it�tanh(W cihwt+W chhht−1+bch)
(9)

(where � is element-wise product and tanh is the
hyperbolic tangent non-linear function). Finally,
the memory cell is combined with the output gate
to yield the hidden activation ht: ht = ot � σ(ct).
As can be seen from the equations, the role of the
gates is to learn to write to and delete from the
memory cell based on the input (Equations 7, 6
and 9), as well as to use the memory cell to com-
pute the hidden activation (Equation 2.2).

A RNNLM has as parameter the embedding
matrix Wm, the hidden-to-output projection Who,
as well as the input-to-hidden and hidden-to-
hidden projections of the RNN/LSTM networks.
Theoretically, what sets a RNNLM apart is that
it consistently displays a much larger context
awareness—because of its ability to carry over in-
formation in the hidden state across very large

118



spans—and that it is therefore able to learn syntac-
tic dependencies and structures from the training
material. This is in stark contrast with a NGLM,
which only reasons on the basis of a very local his-
tory and have little abstractive power.

Importantly, however, it should be emphasized
that most approaches to AA operate on very lo-
cal features, such as lower-order character ngrams
(Stamatatos, 2013; Sapkota et al., 2015; Keste-
mont, 2014). Most state-of-the-art models for
AA indeed depend on document vectors contain-
ing normalized character ngram frequencies (typi-
cally in the range of 2-4), which are fed to a stan-
dard classifier, such as a support-vector machine
with a linear kernel. The fact that the RNNLM
might generate more realistic sentences than the
NGLM does not necessarily entail that it would
have an advantage in AA with respect to a conven-
tional NGLM, which will stay closer to the origi-
nal source documents. An important, if only sec-
ondary, question is therefore whether the use of an
RNNLM in the context of AA would outperform
a conventional NGLM, even if only very local fea-
tures, such as character ngrams, are included in the
model.

3 Experimental setup

3.1 Design

The Patrologia Latina (PL) is a corpus contain-
ing texts of Latin ecclesiastical writers in 221 vol-
umes ranging a time span of 10 centuries, from
Late Antiquity to the High Middle Ages (3rd-
13th century). It was first published in two series
halfway the 19th century by Jacques-Paul Migne,
who mainly based the texts off of 17th and 18th-
century prints. Its digitized version is available
since 1993, and it has remained one of the most
sizable Latin corpora online (±113M words).

Performing this experiment on the PL, and not
on an English corpus, for instance, has been a con-
scious decision to raise the bar. It has been ob-
served that state-of-the-art AA on an inflected lan-
guage such as Latin yields poorer results when it
is reliant on most frequent words (Eder and Ry-
bicki, 2011). Moreover, the Latin that has come
down to us from the 1st century AD onwards is
an institutionalized literary language, hardly a nat-
ural language, showing only far resemblance, or
occasionally no resemblance at all, to the writer’s
mother tongue (Maes, 2009). Tracing stylistic
properties within a heavily formalized language,

and attempting to resuscitate these through gener-
ation, is therefore challenging. An additional ob-
stacle for both language generation as AA is that
many of the PL’s authors cite from similar, author-
itative sources such as the Bible or the church fa-
thers’ precursory texts, thereby having in common
an ecclesiastical vocabulary that could complicate
the detection of stable writing style patterns.

Not all authors in the PL have been equally pro-
lific. These circumstances considerably limit the
set of authors for whom our task is suited (Eder,
2015). We set the condition that our text data in-
clude only texts by authors who dispose of at least
20 authentic, individual documents each. As such
we favored document counts over token counts,
and lexical variety over mere word quantity. A
list of the 18 most prolific authors, their number
of documents and the respective average length of
these documents is given in Fig. 1.

0 20000 40000 60000 80000 100000 120000 140000

Mean words/doc

A.H

H.S

W.S

P.D

B.C

R.M

B

A.M

A.C

A

H.d.S.V

H.C

H.R

R.S.V

T

H.A

R.T

G.I

A
u
th

o
r

0 20 40 60 80 100 120 140
Docs

Figure 1: 18 most prolific Patrologia Latina au-
thors ranked by document count. The bars yield
an average of the document length.

It is not trivial to design an experiment that al-
lows us to study the behavior of generated text in
the context of AA. Fig. 2 shows the experimental
setup which we propose, and in which we attempt
to maximize the comparability of both generated
and authentic data. We start by splitting the full
corpus into two equal-size document collections
(stratified at the author level), α and ω. Only α
will be used to train a LM, which then generates a

119



third collection of synthetic documents. For each
author in α and ω, we aggregate all documents
into a list of sentences per sub-corpus. From these
collections, we create 20 documents containing at
least 5,000 words to create α and ω, through ran-
domly sampling sentences (without replacement)
from the author’s sentence collection. For the
creation of ᾱ, we would also create 20 artificial
5,000-word documents, but this time through sam-
pling new sentences from the LM. This approach
has its limitations, because we limit and balance
the available data to a considerable extent. Fur-
thermore, the sampling procedure implies an un-
derestimation in attribution performance, since it
strips away all supra-sentential information. Nev-
ertheless, this setup guarantees that the authen-
tic and generated corpora are maximally compa-
rable in terms of number of documents, document
length, topical diversity and style mixture—which
is our focus in the present study.

Subsequently, 5 classification experiments are
defined, where we train and and test on different
2-way combinations of the 3 datasets. In a first
pair of experiments, < α,ω > and < ω,α >, we
train and test a classifier on the authentic datasets
to assess the classifier’s performance under natu-
ral conditions. (Note that we apply the classifier
in both directions to account for any directionality
artifacts.) In a third experiment, we train and test a
classifier on the generated data only (< ᾱ, ᾱ >) to
establish to which extent the generated data pre-
serves the data’s stylistic structure at the author
level (i.e. auto-classification). Fourthly, we con-
duct an experiment where we train on the gener-
ated data in ᾱ and test on the authentic data in ω
(< ᾱ, ω >). This allows us to verify whether the
generated documents retain enough stylistic infor-
mation to correctly attribute authentic documents.
Finally, we train a classifier on the authentic data
in ω and test it on ᾱ: this setup (< ω, ᾱ >) allows
to assess whether a classifier, trained on authentic
data is still able to correctly attribute the generated
materials.

In addition, we conduct a final experiment
which can be characterized from the point of view
of self-learning or co-learning (Mihalcea, 2004)—
a semi-supervised learning technique where a core
of training data is expanded with examples from
a related but unlabeled dataset that can be classi-
fied with high confidence by a classifier trained in
the original labeled dataset. In this experiment we

compare the NGLM and RNNLM models with re-
spect to their capacity to boost attribution perfor-
mance by adding synthetic examples to the origi-
nal training set—which might be a valuable strat-
egy for real-life experiments. Specifically, we per-
form attribution on ω using a combination ofα and
ᾱ as training data (< α+ ᾱ, ω >).

Figure 2: Experimental setup. α and ω refer to
50% splits of the full corpus. ᾱ refers to the gen-
erated dataset (cf. dashed line). Each classifier
symbol refers to a classification experiment using
the data at the arrow’s source (first subscript) for
training and the data at the arrow’s target (second
subscript) for testing (note that training only has
to be performed 3 times, one per dataset).

3.2 Language Model Architectures for Text
Generation

In Section 2, the text-generation and model param-
eters were defined. For the present experiments we
generate 20 documents of 5000 words each using
a τ value of 1 and am value estimated on each au-
thor’s dataset. For the RNNLM we reset the seed
(parameter s) every 10 successfully generated sen-
tences, whereas for the NGLM we do it after every
sentence. This asymmetry is motivated by the fact
that NGLM the output distribution of an NGLM
at each step is much more skewed and therefore
sentences generated from the same seed tend to be
be much less varied. For model fitting we set the
NGLM order at 6, which, on a subjective evalua-
tion, seemed a sufficiently large value for the com-
paratively small size of the datasets.

For the RNNLM models the following param-
eter settings were selected. Embedding dimen-

120



sionality M was set to 24, the hidden layer di-
mension was 200 and we stacked up 2 LSTM lay-
ers to encourage the model to learn more abstract
representations. Parameters were chosen based on
common practice and reasonable defaults without
further hyperparameter search. Each model was
trained during 50 epochs using the adaptive variant
of Stochastic Gradient Descent Adam (Kingma
and Ba, 2015) with an initial learning rate of
0.001. We set a small batch size of 50, preferring
stability over speed during training. Moreover, we
clip the gradients before each batch update to a
maximum norm value of 5 to avoid the explod-
ing gradients following (Pascanu et al., 2013) and
truncate the gradient back-propagation after 50 re-
current steps. We also applied 30% dropout af-
ter each recurrent layer following (Zaremba et al.,
2015) to avoid overfitting. For each RNNLM we
held out a validation set using 10% of the data
to monitor and evaluate training. We ensured
that validation perplexity was always lower than
train perplexity. Average validation perplexity was
4.015 with a standard deviation of 0.183.2

3.3 Attribution as Classification
For the AA classification as described in the ex-
perimental setup of section 3, we use a linear
SVM classifier (Diederich, 2003). We extract
shallow linguistic features in the form of Tfidf-
weighted character ngrams (from bigrams to four-
grams) as style markers by which to determine
authorship. Note that the feature extraction of
ngrams in the order of 2 to 4 might have im-
portant repercussions, since NGLM training fully
focuses on capturing that particular distribution,
whereas the more expressive RNNLM models full
sequences. Furthermore, we refrain from using
word-level features such as word ngrams or POS
tags, since this would introduce a further asym-
metry in the comparison given that the RNNLM
can generate unseen words whereas the NGLM
can not. The model accuracy of the SVM is fine-
tuned by searching over different value ranges for
the SVM’s parameters. The number of features is
set to range from 1,000 to 30,000 max features for
each fit, more specifically in the following order:
5,000, 10,000, 15,000 and 30,000 features. For the
C-parameter of the SVM we search over values of
respectively 1, 10, 100 and 1,000.

2All software associated with this paper is avail-
able from https://www.github.com/jedgusse/
project_lorenzo.

Source Experiment F1 P R

Real < α,ω > 0.833 0.818 0.869
< ω,α > 0.811 0.795 0.853

NGLM < α+ ᾱ, ω > 0.814 0.809 0.850
< ᾱ, ω > 0.706 0.744 0.750
< ω, ᾱ > 0.837 0.811 0.881

RNNLM < α+ ᾱ, ω > 0.872 0.878 0.892
< ᾱ, ω > 0.635 0.701 0.658
< ω, ᾱ > 0.724 0.778 0.775

Table 1: Mean F1, Precision (P) and Recall (R)
scores for all classification experiments.

4 Results

4.1 Examples of Generated Language

What follows are two short extracts from the
respective outputs of an NGLM and RNNLM
trained on Augustine (A.H.) (most prolific author
of the dataset, see Table 1), which gives an
anecdotal intuition of how the output of these
language models differs.

Ngram-based LM (ᾱ)

(1) * Sed
Yet

uis
you wish

uenire:
to come

quod
since

postridie,
tomorrow

ascensiones
ascensions

honora
honoured

pastorem,
the shepherd,

nec
and not

sane
completely

reipublicos
republican

idem
the same

testis
witness

et
also

implebitur
will be fulfilled

tamen
nevertheless

mentiendum
to be deceived

sit
it may be

propitiaberis.
you will be enriched.

RNN-based LM (ᾱ)

(2) * Et
And

idam
that same (?)

precepti,
commandment,

siue
be it

ad
towards

sensum
the feeling

noui:
I know:

nonuulde
not enough (?)

sunt
are

enim
after all

Filius
the Son

Domini
of our Lord

substantia,
our substance,

sed
but

non
none

sunt
are there

qui
who

secururum
amongst the untroubled

superbia
through pride

et
also

perrectus
righteous

est,
are,

mortalis
mortal

includendi
by including

estus
fire

que
and

fiumus
we were (?)

propter
because of

illam
this

uideantur.
may they be beheld.

121



< , > < , >
experiment

0.0

0.2

0.4

0.6

0.8

1.0
f1

Real

< , > < , >< + , >
experiment

f1

RNNLM

< , > < , >< + , >
experiment

f1

NGLM

Figure 3: F1 scores for the different combinations of α, ᾱ, and ω.

The extract of RNNLM-generated text as com-
pared to the NGLM demonstrates how the
RNNLM is better at reproducing a syntactic logic
(which moreover makes translation easier). Note,
for instance, how the nominative of the relative
pronoun qui is maintained towards the end of the
subordinate clause in the participle perfect per-
rectus, and even seems to be carried on in the
next clause as opposed to the awkwardly placed
quod in the ngram-based extract. The RNNLM
is also arguably better at positioning the verbs in
the clauses. Compare, for instance, the NGLM’s
dense verbal sequence implebitur tamen mentien-
dum sit propitiaberis. Finally, the RNNLM is
more apt at generating plausible neologisms. Ex-
amples include idam (cfr. idem and quidam), fiu-
mus (cfr. fiemus), secururum (cfr. securus and the
genitive ending -orum and -arum). To a human
reader, the RNNLM produces superficially more
convincing text.

4.2 Attribution results

The results of the attribution experiments are pre-
sented in Table 1 in terms of recall, precision and
F1-scores and the distributions are visualized in
Fig. 3. We focus on the macro-averaged F1-scores
in our discussion, although one should not forget
that the scores vary considerably over individual
authors (cf. Fig. 3). With respect to the authen-
tic data, classifying α on the basis of ω is slightly
more difficult than the reverse direction, which
seems a negligible directionality artifact. When
we use the generated data as training material to
classify authentic material < ᾱ, ω > , we see that
the F1-scores drop significantly for both LMs, al-
though the NGLM seems more robust in this re-

spect. Interestingly, the drop is much less signifi-
cant for the opposite situation, where we train on
authentic material and classify generated material
< ω, ᾱ >. This suggests that enough stylistic
information is preserved in the generated text to
attribute it to the original author, but that this in-
formation in isolation does not suffice to train a
convincing attribution system on. When used in
isolation, the NGLM outperforms the RNNLM in
both setups. However, the situation is clearly dif-
ferent for the augmentation or self-learning setup
(< α+ ᾱ, ω >)—c.f. Section 3—, where we train
an attributor on the combination of α and ᾱ, and
test it on the authentic ω set. Here, we see that
the RNNLM performs better than the NGLM in
the corresponding experiment – the NGLM in fact
even performs worse in this case than in the nor-
mal < α,ω > setup.

4.3 Discussion

To understand the difference in behavior between
both LMs, it is useful to inspect Fig. 4. Here, we
use a Principal Components Analysis (Binongo
and Smith, 1999) to visualize 2500-word samples
for 3 three most prolific authors (Augustine of
Hippo, Honorius of Autun, and Gregory the Great)
using the 150 most common ngrams. We include
a mixture of authentic ω data and generated ᾱ data
for each author, comparing the NGLM and the
RNNLM. The plots shows that NGLM produces
text samples which lie very close in ngram fre-
quencies to the authentic data, whereas the texts
produced by the RNNLM follow a markedly dif-
ferent distribution than ω – this difference is very
outspoken for Augustine, for instance. As might
be expected on the basis of the observation in sec-

122



tion 3.3, the NGLM thus produces data that stays
very close to the original input, whereas RNNLM
yields fuzzier texts, that follow a slightly different
distribution. This explains why it is, for example,
easier to train an attributor on the data generated
by an NGLM than an RNNLM.

0.10 0.05 0.00 0.05 0.10
Principal Component 1: 33.62%

0.075

0.050

0.025

0.000

0.025

0.050

0.075

0.100

0.125

Pr
in

cip
al

 C
om

po
ne

nt
 2

: 1
5.

23
%

Augustinus
Gregorius
Honorius
N-gram-Generated 
Authentic 

0.2

0.1

0.0

0.1

0.2

0.3

0.4
0.3 0.2 0.1 0.0 0.1 0.2 0.3

s 

t 

m 

e 

 e

er

 i

 s
it

qu

in

um

is

a 

ti

ui

i 

us

nt

 a

 p

tu

um 

re

 d

te

at

 c

es

 q qu

us 

et

en

 u

ri

o 

on

st

de

di

ue
r 

is 
ta

an

or

ni

 in

n 

et 

 n

li

am

em

ia

ra

si

ur
iu

 m

ne

 et

se
ic

pe

noun

 et 
it 

d 

su

ci

co

qui

am 

 t

tur
ru

em 

ec

 de

mi
os

 h

im

ur 

ua

cu

 f
in 

ce  no

 in 

tur 

ar

io

uo

 co

 qui

le ut

est

pr

nd

nt 

om

ent

per

id

 di

il

ia 

al

 pr

s e

ca

 es

 su

nu

me

 o

 se

ro po

ct

au

ie

el

ib
tr

ma

ul

 pe

na

re 

que r
do

es 

ter
to

mu

la

st 
c 

itu

ad

as
unt

0.10 0.05 0.00 0.05 0.10 0.15
Principal Component 1: 37.77%

0.10

0.05

0.00

0.05

0.10

Pr
in

cip
al

 C
om

po
ne

nt
 2

: 1
6.

5%

Augustinus
Honorius
Gregorius
RNN-Generated 
Authentic 

0.2

0.1

0.0

0.1

0.2

0.3

0.2 0.1 0.0 0.1 0.2 0.3

s 

t 

m 

e 

 e

er

 s

 i

it

in

qu

a 

um

is

ti

 a

ui

 p

nt

tu

us

 d

i 

re

um 

es

te

 c

et

at

 q qu

us 

en

on

 u

st

ri

de

di

o 

r 

n 

ue

is 

 in

et 

ta
or

an

ni

em

 n

am

 et

li

se

ia

ur

 m si

ne

ra

pe

 et 

ic

iu

d 

co

no

it 

su

un

tur

qui

ci

in  in 

am 

 de
ur 

em 

 t
 co

os

 f

tur 

ecru

mi

est

cu

 qui

ua

io

ce

im

 no

per

ut

uo
ar

om

 h

pr

ent

 di

nd

 es

nt 

id

 se

le

s e

ia 

 su

 pe

me

ca

 pr

poct

st 

 o
ro

al

ib

es 

il
tr

ma

re 

el

na

au
ie

 r

nu

est 

ed

ad

itu

mu

ul

to

 est

la

que

ns

do

Figure 4: PCA plots (1st 2 PCs) for 3 authors us-
ing document vectors representing the normalized
frequencies of the 150 most frequent ngrams (or-
der 2-4) in 2500-word sample. We include a mix-
ture of authentic ω data and generated ᾱ data (top:
NGLM; bottom: RNNLM).

Conversely, our results show that the situation
is different in the data augmentation setup, where
we train an attributor on the combination of α
and ᾱ and test it on the authentic ω set. In this
case, the NGLM performs worse than in the cor-
responding the non-augmented setup, whereas the
performance of the RNNLM sensitively increases.
Arguably, the fuzziness of the RNNLM-generated
data adds an interesting complexity to the original

core of authentic data, which can be exploited by
the classifier.

B.C

B

A

W.S

H.S

A.C

A.H

P.D

A.M

R.M

α
−
ω

Word-level Char-level

B.C

B

A

W.S

H.S

A.C

A.H

P.D

A.M

R.M

ᾱ
N
G
L
M
−
α

R
.M

A
.M P
.D

A
.H

A
.C

H
.S

W
.S A B

B
.C

B.C

B

A

W.S

H.S

A.C

A.H

P.D

A.M

R.M

ᾱ
R
N
N
L
M
−
α

R
.M

A
.M P
.D

A
.H

A
.C

H
.S

W
.S A B

B
.C

0.02 0.00 0.02 0.05 0.00 0.05

Figure 5: Mean-normalized Jaccard similarity
scores between 10 most prolific authors using
word (left column) and character (right column)
bigrams to fourgrams, comparing real data (first
row), RNNLM-synthetic data with real data (sec-
ond row) and NGLM-synthetic data with real data
(third row).

While these results indirectly show that the
RNNLM did not simply overfit on α, it is an in-
teresting question to which extent α and ᾱ dis-
play (lexical) overlap in the case of both LMs. If
the overlap would indeed be larger for the NGLM
than the RNNLM, this would support our interpre-
tation. In Fig. 5, we show mean-normalized, pair-
wise Jaccard similarities for the 10 most prolific
authors in both α and ᾱ for each LM. The dark di-
agonals in the second row of the heatmaps visually
support the observation that the NGLM displays a
much more outspoken overlap between α and ᾱ.
Such an effect is much more faint in the case of

123



the RNNLM and in this respect it remains more
faithful to the real data (first row for α and ω).

5 Conclusion

Our preliminary results confirm that the texts gen-
erated by a traditional NGLM are relatively ‘dull’
and ‘conservative’ in the sense that they stay rela-
tively close to the local distribution of the source
data on which they were trained. Conceptually,
the RNNLM has a clear advantage in terms of
expressiveness and capacity with respect to the
NGLM. In practice, given the small size of AA
datasets, an underfitted RNNLM yield fuzzier ex-
amples, which explains why the NGLM outper-
forms the RNNLM when the classifier is restricted
to the generated data (< ᾱ, ω > and < ω, ᾱ >).
At the same time, the training data augmentation
setup (< α+ ᾱ, ω >) shows that whereas NGLM-
generated data adds comparatively little to the au-
thentic data—reproducing a subset of the origi-
nal feature distribution, as shown in Fig. 5—,
the RNNLM-generated data presents a valuable
data contribution which does result in an abso-
lute increase in attribution performance with re-
spect to the real classification setup < α,ω >. Al-
though further research into the matter is needed,
this clearly suggests that the complexity of the
RNNLM data is useful for training data augmen-
tation, arguably capturing stylistic nuances which
a simpler LM cannot.

In the future, we will explore the flexibility of
the general RNNLM framework to develop gener-
ative architectures that better capture the style of
the training data. In particular, following (Linzen
et al., 2016) we hypothesize that forcing the RNN
to model more linguistic structure—e.g. jointly
modeling words and POS-tags—, should result in
better language generation and better style preser-
vation. Furthermore, we plan on exhaustively test-
ing the capabilities of author-specific generative
models for self-learning in AA, investigating the
effect of adding different amounts of synthetic
data and selectively adding synthetic data based
on the confidence with which it can be correctly
classified by a classifier trained on real data.

Additionally, we would like to investigate
pre-training in out-of-domain data as well as
more compact ways of modelling author-specific
language—such as conditional language models
(Tang et al., 2016)—as means to alleviate under-
fitting of the RNN models on small datasets.

References
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and

Christian Janvin. 2003. A Neural Probabilistic Lan-
guage Model. The Journal of Machine Learning Re-
search, 3:1137–1155.

José Nilo G. Binongo and M. Wilfrid A. Smith. 1999.
The application of principal component analysis to
stylometry. Literary and Linguistic Computing,
14(4):445–466.

Tom Brewe. 2015. Do androids dream of cook-
ing? https://gist.github.com/nylki/
1efbaa36635956d35bcc.

Joachim Diederich. 2003. Authorship attribution
with support vector machines. Applied Intelligence,
19.1(4616):109––123.

Maciej Eder and Jan Rybicki. 2011. Deeper delta
across genres and languages: Do we really need the
most frequent words? Literary and Linguistic Com-
puting, 26(3):315–321.

Maciej Eder. 2015. Taking stylometry to the limits:
Benchmark study on 5,281 texts from "patrologia
latina". In Digital Humanities 2015: Conference
Abstracts, pages 1919–1924.

Jeffrey L. Elman. 1990. Finding structure in time.
Cognitive Science, 14(2):179–211.

Liang Feynman, Mark Gotham, Marcin Tomczak,
Mathew Johnson, and Jaimie Shotton. 2016. The
bachbot challenge. http://bachbot.com/.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long Short-Term Memory. Neural Computation,
9(8):1735–1780.

Sepp Hochreiter. 1998. The Vanishing Gradient Prob-
lem During Learning Recurrent Neural Nets and
Problem Solutions. International Journal of Un-
certainty, Fuzziness and Knowledge-Based Systems,
06(02):107–116.

Geoffrey Jefferson. 1949. The mind of mechanical
man. British Medical Journal, 1(4616):1105–1110.

Andrej Karpathy. 2015. The unreasonable ef-
fectiveness of recurrent neural networks.
http://karpathy.github.io/2015/
05/21/rnn-effectiveness/.

Mike Kestemont. 2014. Function words in author-
ship attribution. from black magic to theory? In
Proceedings of the 3rd Workshop on Computational
Linguistics for Literature, pages 59–66. Association
for Computational Linguistics.

Diederik P. Kingma and Jimmy Lei Ba. 2015.
Adam: a Method for Stochastic Optimization. Inter-
national Conference on Learning Representations
2015, pages 1–15.

124



Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.
2016. Assessing the Ability of LSTMs to Learn
Syntax-Sensitive Dependencies. nov.

Yanick Maes. 2009. Continuity through appropria-
tion? In Jan Papy, Wim Verbaal, and Yanick Maes,
editors, Latinitas Perennis. Volume II: Appropriation
and Latin Literature, chapter 1, pages 1––10. Brill,
Leiden.

Eric Malmi, Pyry Takala, Hannu Toivonen, Tapani
Raiko, and Aristides Gionis. 2015. Dopelearning:
A computational approach to rap lyrics generation.
CoRR, abs/1505.04771.

Rada Mihalcea. 2004. Co-training and self-training
for word sense disambiguation. In CoNLL, pages
33–40.

Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan
Cernocký, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Inter-
speech, number September, pages 1045–1048.

Razvan Pascanu, Tomas Mikolov, and Yoshua Ben-
gio. 2013. On the Difficulties of Training Recurrent
Neural Networks. Icml, (2):1–9.

Peter Potash, Alexey Romanov, and Anna Rumshisky.
2015. GhostWriter: Using an LSTM for Automatic
Rap Lyric Generation. Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, (September):1919–1924.

Upendra Sapkota, Steven Bethard, Manuel Montes,
and Thamar Solorio. 2015. Not all character n-
grams are created equal: A study in authorship at-
tribution. In Proceedings of the 2015 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 93–102, Denver, Colorado, May–
June. Association for Computational Linguistics.

Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Amer-
ican Society For Information Science and Technol-
ogy, (60):538–556.

Efstathios Stamatatos. 2013. On the robustness of au-
thorship attribution based on character n-gram fea-
tures. Journal of Law and Policy, 21(2):421–439.

Jian Tang, Yifan Yang, Sam Carton, Ming Zhang, and
Qiaozhu Mei. 2016. Context-aware Natural Lan-
guage Generation with Recurrent Neural Networks.
arXiv preprint arXiv:1611.09900.

Alan Turing. 1950. Computing machinery and intelli-
gence. Mind, 49(4616):433–460.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2015. Recurrent Neural Network Regularization.
ICLR, pages 1–8.

A Author names with abbreviations

Abbrv Author

H.S Hieronymus Stridonensis
G.I Gregorius I
A.H Augustinus Hipponensis
A.M Ambrosius Mediolanensis
B Beda
H.C Hildebertus Cenomanensis
H.d.S.V Hugo de S- Victore
R.T Rupertus Tuitiensis
W.S Walafridus Strabo
T Tertullianus
P.D Petrus Damianus
H.A Honorius Augustodunensis
H.R Hincmarus Rhemensis
B.C Bernardus Claraevallensis
A Alcuinus
R.M Rabanus Maurus
A.C Anselmus Cantuariensis
R.S.V Richardus S- Victoris

125


