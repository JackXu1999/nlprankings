



















































MTE-NN at SemEval-2016 Task 3: Can Machine Translation Evaluation Help Community Question Answering?


Proceedings of SemEval-2016, pages 887–895,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

MTE-NN at SemEval-2016 Task 3: Can Machine Translation Evaluation
Help Community Question Answering?

Francisco Guzmán, Lluı́s Màrquez and Preslav Nakov
Arabic Language Technologies Research Group

Qatar Computing Research Institute, HBKU
{fguzman,lmarquez,pnakov}@qf.org.qa

Abstract

We present a system for answer ranking
(SemEval-2016 Task 3, subtask A) that is a
direct adaptation of a pairwise neural net-
work model for machine translation evalua-
tion (MTE). In particular, the network incor-
porates MTE features, as well as rich syntac-
tic and semantic embeddings, and it efficiently
models complex non-linear interactions be-
tween them. With the addition of lightweight
task-specific features, we obtained very en-
couraging experimental results, with sizeable
contributions from both the MTE features and
from the pairwise network architecture. We
also achieved good results on subtask C.

1 Introduction

We present a system for SemEval-2016 Task 3 on
Community Question Answering (cQA), subtask A
(English). In that task, we are given a question from
a community forum and a thread of associated text
comments intended to answer the question, and the
goal is to rank the comments according to their ap-
propriateness to the question. Since cQA forum
threads are noisy, as many comments are not an-
swers to the question, the challenge lies in learning
to rank all good comments above all bad ones.1

In this work, we approach subtask A from a novel
perspective: by using notions of machine transla-
tion evaluation (MTE) to decide on the quality of
a comment. In particular, we extend the MTE neu-
ral network framework from Guzmán et al. (2015).

1More detail and examples can be found on the task web-
site (http://alt.qcri.org/semeval2016/task3/) and in the associ-
ated task description paper (Nakov et al., 2016).

We believe that this neural network is interesting for
the cQA problem because: (i) it works in a pairwise
fashion, i.e., given two translation hypotheses and a
reference translation to compare to, the network de-
cides which translation hypothesis is better; this is
appropriate for a ranking problem; (ii) it allows for
an easy incorporation of rich syntactic and semantic
embedded representations of the input texts, and it
efficiently models complex non-linear relationships
among them; (iii) it uses a number of MT evaluation
measures that have not been explored for the cQA
task (e.g., TER, Meteor and BLEU).

The analogy we apply to adapt the neural MTE
architecture to the cQA problem is the following:
given two comments c1 and c2 from the question
thread—which play the role of the two translation
hypotheses—we have to decide whether c1 is a bet-
ter answer than c2 to question q—which plays the
role of the translation reference.

The two tasks seem similar: both reason about the
similarity of two competing texts against a reference
text, to decide which one is better. However, there
are some profound differences. In MTE, the goal is
to decide whether a hypothesis translation conveys
the same meaning as the reference translation. In
cQA, it is to determine whether the comment is an
appropriate answer to the question. Furthermore, in
MTE we can expect shorter texts, which are much
more similar among them. In cQA, the question and
the intended answers might differ significantly both
in length and in lexical content. Thus, it is not clear
a priori whether the MTE network can work well for
cQA. Here, we show that the analogy is convenient,
allowing to achieve competitive results.

887



At competition time, we achieved the sixth best
result on the task from a set of twelve systems. Right
after the competition we introduced some minor im-
provements and extra features, without changing the
fundamental architecture of the network, which im-
proved the MAP result by almost two points. We
also performed a more detailed experimental analy-
sis of the system, checking the contribution of sev-
eral features and parts of the NN architecture. We
observed that every single piece contributes impor-
tant information to achieve the final performance.
While task-specific features are crucial, other as-
pects of the framework are relevant too: syntactic
embeddings, MT evaluation measures, and pairwise
training of the network.

Finally, we used our system for subtask A to solve
subtask C, which asks to find good answers to a new
question that was not asked before in the forum by
reranking the answers to related questions. For the
purpose, we weighted the subtask A scores by the
reciprocal rank of the related questions (following
the order given by the organizers, i.e., the ranking by
Google). Without any subtask C specific addition,
we achieved the fourth best result in the task.

2 Related Work

Recently, many neural network (NN) models have
been applied to cQA tasks: e.g., question-question
similarity (Zhou et al., 2015; dos Santos et al., 2015;
Lei et al., 2016) and answer selection (Severyn and
Moschitti, 2015; Wang and Nyberg, 2015; Shen et
al., 2015; Feng et al., 2015; Tan et al., 2015). Also,
other participants in the SemEval 2016 Task 3 ap-
plied NNs to solve some of the subtasks (Nakov et
al., 2016). However, our goal was different: we
were interested in extending an existing pairwise
NN framework from a different but related problem.

There is also work that uses scores from machine
translation models as a features for cQA (Berger et
al., 2000; Echihabi and Marcu, 2003; Jeon et al.,
2005; Soricut and Brill, 2006; Riezler et al., 2007; Li
and Manandhar, 2011; Surdeanu et al., 2011; Tran
et al., 2015), e.g., a variation of IBM model 1, to
compute the probability that the question is a “trans-
lation” of the candidate answer. Unlike that work,
here we use machine translation evaluation (MTE)
instead of machine translation models.

f(q,c1,c2) 

ψ(q,c1)ψ(q,c2)hq1

hq2

h12

v
xc1

xc2

xq

q

c1

c2

sentences  embeddings pairwise nodes pairwise features

output layer

Figure 1: Overall architecture of the NN.

Another relevant work is that of Madnani et al.
(2012), who applied MTE metrics as features for
paraphrase identification. However, here we have
a different problem: cQA. Moreover, instead of us-
ing MTE metrics as features, we port an entire MTE
framework to the cQA problem.

3 Neural Model for Answer Ranking

The NN model we use for answer ranking is de-
picted in Figure 1. It is a direct adaptation of the
feed-forward NN for MTE described in (Guzmán et
al., 2015). Technically, we have a binary classifica-
tion task with input (q, c1, c2), which should output
1 if c1 is a better answer to q than c2, and 0 oth-
erwise.2 The network computes a sigmoid function
f(q, c1, c2) = sig(w

T
v φ(q, c1, c2)+ bv), where φ(x)

transforms the input x through the hidden layer, wv
are the weights from the hidden layer to the output
layer, and bv is a bias term.

We first map the question and the comments to
a fixed-length vector [xq,xc1 ,xc2 ], using syntactic
and semantic embeddings. Then, we feed this vector
as input to the neural network, which models three
types of interactions, using different groups of nodes
in the hidden layer. There are two evaluation groups
hq1 and hq2 that model how good each comment ci
is to the question q. The input to these groups are the
concatenations [xq,xc1 ] and [xq,xc2 ], respectively.
The third group of hidden nodes h12, which we call
similarity group, models how close c1 and c2 are. Its
input is [xc1 ,xc2 ]. This might be useful as highly
similar comments are likely to be comparable in ap-
propriateness, irrespective of whether they are good
or bad answers in absolute terms.

2In this work, we do not learn to predict ties.

888



In summary, the transformation φ(q, c1, c2) =
[hq1 ,hq2 ,h12] can be written as follows:

hqi = g(Wqi[xq,xci ] + bqi), i = 1, 2

h12 = g(W12[xc1 ,xc2 ] + b12),

where g(.) is a non-linear activation function (ap-
plied component-wise), W ∈ RH×N are the associ-
ated weights between the input layer and the hidden
layer, and b are the corresponding bias terms. We
use tanh as an activation function, rather than sig,
to be consistent with how parts of our input vectors
(the word embeddings) are generated.

The model further allows to incorporate exter-
nal sources of information in the form of skip
arcs that go directly from the input to the out-
put, skipping the hidden layer. These arcs repre-
sent pairwise similarity feature vectors between q
and either c1 or c2. In these feature vectors, we
encode MT evaluation measures (e.g., TER, Me-
teor, and BLEU), cQA task-specific features, etc.
See Section 4.3 for details about the features im-
plemented as skip arcs. In the figure, we indi-
cate these pairwise external feature sets as ψ(q, c1)
and ψ(q, c2). When including the external fea-
tures, the activation at the output is f(q, c1, c2) =
sig(wTv [φ(q, c1, c2), ψ(q, c1), ψ(q, c2)] + bv).

4 Learning Features

We experiment with three kinds of features: (i) in-
put embeddings, (ii) features motivated by previ-
ous work on Machine Translation Evaluation (MTE)
(Guzmán et al., 2015) and (iii) task-specific features,
mostly proposed by participants in the 2015 edition
of the task (Nakov et al., 2015).

4.1 Embedding Features
We use the following vector-based embeddings of
(q, c1, c2) as input to the NN:

• GOOGLE VEC: We use the pre-trained, 300-
dimensional embedding vectors, which Tomas
Mikolov trained on 100 billion words from
Google News (Mikolov et al., 2013).

• SYNTAX VEC: We parse the entire ques-
tion/comment text using the Stanford neural
parser (Socher et al., 2013), and we use the fi-
nal 25-dimensional vector that is produced in-
ternally as a by-product of parsing.

Moreover, we use the above vectors to calculate
pairwise similarity features. More specifically, given
a question q and a pair of comments c1 and c2 for
it, we calculate the following features: ψ(q, c1) =
cos(q, c1) and ψ(q, c2) = cos(q, c2).

4.2 MTE features
MTFEATS (in MTE-NN-improved only). We use
(as skip-arc pairwise features) the following six ma-
chine translation evaluation features, to which we re-
fer as MTFEATS, and which measure the similarity
between the question and a candidate answer:

• BLEU: This is the most commonly used mea-
sure for machine translation evaluation, which
is based on n-gram overlap and length ratios
(Papineni et al., 2002).

• NIST: This measure is similar to BLEU, and
is used at evaluation campaigns run by NIST
(Doddington, 2002).

• TER: Translation error rate; it is based on the
edit distance between a translation hypothesis
and the reference (Snover et al., 2006).

• METEOR: A measure that matches the hy-
pothesis and the reference using synonyms and
paraphrases (Lavie and Denkowski, 2009).

• PRECISION: measure, originating in informa-
tion retrieval.

• RECALL: another measure coming from infor-
mation retrieval.

BLEUCOMP. Following (Guzmán et al., 2015), we
further use as features various components that are
involved in the computation of BLEU: n-gram pre-
cisions, n-gram matches, total number of n-grams
(n=1,2,3,4), lengths of the hypotheses and of the
reference, length ratio between them, and BLEU’s
brevity penalty. We will refer to the set of these fea-
tures as BLEUCOMP.

4.3 Task-specific features
QL VEC (in MTE-NN-improved only). Similarly
to the GOOGLE VEC, but on task-specific data, we
train word vectors using WORD2VEC on all available
cQA training data (Qatar Living) and use them as
input to the NN.

889



QL+IWSLT VEC (in MTE-NN-{primary, con-
trastive1/2} only). We also use trained word vectors
on the concatenation of the cQA training data and
the English portion of the IWSLT data, which con-
sists of TED talks (Cettolo et al., 2012) and is thus
informal and somewhat similar to cQA data.
TASK FEAT. We further extract various task-
specific skip-arc features, most of them proposed for
the 2015 edition of the task (Nakov et al., 2015).
This includes some comment-specific features:

• number of URLs/images/emails/phone num-
bers;

• number of occurrences of the string thank;3

• number of tokens/sentences;

• average number of tokens;

• type/token ratio;

• number of nouns/verbs/adjectives/adverbs/pro-
nouns;

• number of positive/negative smileys;

• number of single/double/triple exclamation/in-
terrogation symbols;

• number of interrogative sentences (based on
parsing);

• number of words that are not in word2vec’s
Google News vocabulary.4

And also some question-comment pair features:

• question to comment count ratio in terms
of sentences/tokens/nouns/verbs/adjectives/ad-
verbs/pronouns;

• question to comment count ratio of words that
are not in word2vec’s Google News vocabulary.

We also have two meta features:

• is the person answering the question the one
who asked it;

• reciprocal rank of the comment in the thread.
3When an author thanks somebody, this post is typically a

bad answer to the original question.
4Can detect slang, foreign language, etc., which would indi-

cate a bad answer.

5 Experiments and Results

Below we explain which part of the available data
we used for training, as well as our basic settings.
Then, we present in detail our experiments and the
evaluation results.

5.1 Data and Settings

We experiment with the data from SemEval-2016
Task 3 (Nakov et al., 2016). The task offers
a higher quality training dataset TRAIN-PART1,
which includes 1,412 questions and 14,110 answers,
and a lower-quality TRAIN-PART2 with 382 ques-
tions and 3,790 answers. We train our model on
TRAIN-PART1 with hidden layers of size 3 for 63
epochs with minibatches of size 30, regularization
of 0.0015, and a decay of 0.0001, using stochastic
gradient descent with adagrad (Duchi et al., 2011);
we use Theano (Bergstra et al., 2010) for learning.
We normalize the input feature values to the [−1; 1]
interval using minmax, and we initialize the network
weights by sampling from a uniform distribution as
in (Bengio and Glorot, 2010). We train the model
using all pairs of good and bad comments, ignoring
ties. At test time we get the full ranking by scoring
all possible pairs, and accumulating the scores at the
comment level.

We evaluate the model on TRAIN-PART2 after
each epoch, and ultimately we keep the model that
achieves the highest Kendall’s Tau (τ ); in case of
a tie, we prefer the parameters from a later epoch.
We selected the above parameter values on the DEV
dataset (244 questions and 2,440 answers) using the
full model, and we use them for all experiments be-
low, where we evaluate on the official TEST dataset
(329 questions and 3,270 answers).

For evaluation, we use mean average precision
(MAP), which is the official evaluation measure. We
further report scores using average recall (AvgRec),
mean reciprocal rank (MRR), Precision (P), Recall
(R), F-measure (F1), and Accuracy (Acc). Note that
the first three are ranking measures, to which we
directly give our ranking scores. However, the lat-
ter four measures require Good vs. Bad categorical
predictions. We generate them based on the ranking
scores using a threshold: if the score is above 0.95
(chosen on the DEV set), we consider the comment
to be Good, otherwise it is Bad.

890



5.2 Contrastive Runs
We submitted two contrastive runs, which differ
from the general settings above as follows:

• MTE-NN-contrastive1: a different network ar-
chitecture with 50 units in the hidden layer (in-
stead of 3 for each of hq1,hq2,h12) and higher
regularization (0.03, i.e., twenty times bigger).
On the development data, it performed very
similarly to those for the primary run, and we
wanted to try a bigger NN.

• MTE-NN-contrastive2: the same architecture
as the primary but different training. We
put together TRAIN-PART1 and DEV and ran-
domly split them into 90% for training and
10% for model selection. The idea here was
to have some training examples from devel-
opment, which was supposed to be a cleaner
dataset (and so more similar to the test set).

5.3 Official Results
Table 1 shows the results for our submissions for
subtask A. Our primary submission was ranked sixth
out of twelve teams on MAP. Note, however, that it
was third on MRR and F1. It is also 3 and 14 points
above the average and the worst systems, respec-
tively, and well above the baselines. Both our con-
trastive submissions performed slightly better, but
neither of them is strong enough to change the over-
all ranking if we had chosen one of them as primary.

For subtask C, we multiplied (i) our scores for
subtask A for the related question by (ii) the given
reciprocal rank of the related question in the list of
related questions. That is, we did not try to ad-
dress question-question similarity (subtask B). We
achieved 4th place with a MAP of 49.38, which is
well above the baseline of 40.36. Our contrastive2
run performed slightly better at 49.49.

5.4 Post-submission Analysis on the Test Set
After the competition, we produced a refined version
of the system (MTE-NN-improved) where the set-
tings changed as follows: (i) using QL VEC instead
of QL+IWSLT VEC, (ii) adding MTFEATS to the
set of features, (iii) optimizing accuracy instead of
Kendall’s tau, (iv) training for 100 epochs instead of
63, and (v) regularization of 0.005 instead of 0.0015.

System MAP AvgRec MRR ∆MAP
MTE-NN-improved 78.20 88.01 86.93
−TASK FEATS 72.91 84.06 78.73 -5.29
−COMMENT RANK 76.08 86.41 84.42 -2.12
−SAME AUTHOR 76.60 86.75 83.71 -1.60
−QL VEC 75.83 86.57 83.90 -2.37
−GOOGLE VEC 76.96 87.66 84.72 -1.24
−SYNTAX VEC 77.65 87.65 85.85 -0.55
−COSINES 76.97 87.28 85.03 -1.23
−MTFEATS 77.75 87.76 86.01 -0.45
−BLEUCOMP 77.83 87.85 86.32 -0.37

Table 2: Ablation study of our improved system on
the test data.

Note that the training and development set re-
mained unchanged. MTE-NN-improved showed
notable improvements on the DEV set over our pri-
mary submission. In Table 2, we present the results
on the TEST set. To gain additional insight about the
contribution of various features and feature groups
to the performance of the overall system, we also
present the results of an ablation study where we re-
moved different feature groups one by one. For this
purpose, we study ∆MAP, i.e., the absolute change in
MAP when the feature or feature group is excluded
from the full system. Not surprisingly, the most im-
portant turn out to be the TASK FEATS (contributing
over 5 MAP points) as they handle important infor-
mation sources that are not available to the system
from other feature groups, e.g., the reciprocal rank
of the comment in the comment thread, which alone
contributes 2.12 MAP points, and the feature check-
ing whether the person who asked the question is
the one who answered, which contributes 1.60 MAP
points. Next in terms of importance come word
embeddings, QL VEC (contributing over 2 MAP
points), trained on text from the target forum, Qatar-
Living. Then come the GOOGLE VEC (contributing
over 1 MAP point), which are trained on 100 billion
words, and thus are still useful even in the presence
of the domain-specific QL VEC, which are in turn
trained on four orders of magnitude less data. In-
terestingly, the MTE-motivated SYNTAX VEC vec-
tors contribute half a MAP point, which shows the
importance of modeling syntax for this task. Next,
we can see that using just the vectors is not enough,
and adding cosines as pairwise features for the three
kinds of vectors contributes over one MAP point.

891



Submission MAP AvgRec MRR P R F1 Acc
1 SemEval 1st 79.191 88.821 86.421 76.961 55.308 64.365 75.112

MTE-NN-improved 78.20 88.01 86.93 57.08 76.75 65.47 67.09
2 SemEval 2nd 77.662 88.053 84.934 75.562 58.846 66.162 75.541
3 SemEval 3rd 77.583 88.142 85.212 74.134 53.0510 61.848 73.395
4 SemEval 4th 77.284 87.525 84.096 70.466 63.364 66.721 74.314
5 SemEval 5th 77.165 87.984 84.695 74.433 56.737 64.394 74.503

MTE-NN-contrastive2 76.98 86.98 85.50 58.71 70.28 63.97 67.83
MTE-NN-contrastive1 76.86 87.03 84.36 55.84 77.35 64.86 65.93

6 MTE-NN-primary 76.446 86.747 84.973 56.289 76.221 64.753 66.278
. . . . . . . . . . . . . . . . . . . . . . . . . . .

SemEval Average 73.54 84.61 81.54 64.80 57.03 58.77 68.45
. . . . . . . . . . . . . . . . . . . . . . . . . . .
12 SemEval 12th 62.2412 75.4112 70.5812 50.2811 53.509 51.8410 59.6011

Baseline 1 (IR) 59.53 72.60 67.83 — — — —
Baseline 2 (random) 52.80 66.52 58.71 40.56 74.57 52.55 45.26
Baseline 3 (all ‘true’) — — — 40.64 100.00 57.80 40.64
Baseline 4 (all ‘false’) — — — — — — 59.36

Table 1: Comparison to the official results on SemEval-2016 Task 3, subtask A. The first column shows
the rank of the primary runs with respect to the official MAP score. The subindices in the results columns
show the rank of the primary runs with respect to the evaluation measure in the respective column.

Finally, the two MTE features, MTFEATS and
BLEUCOMP, together contribute 0.8 MAP points.
It is interesting that the BLEU components man-
age to contribute on top of the MTFEATS, which
already contain several state-of-the-art MTE mea-
sures, including BLEU itself. This is probably be-
cause the other features we have do not model n-
gram matches directly.

We further used the output of our MTE-NN-
improved system to generate predictions for subtask
C, as explained above. This yielded improvements
from 49.38 to 49.87 on MAP, from 55.44 to 56.08
on AvgRec, and from 51.56 to 52.16 on MRR.

6 Conclusion

We have explored the applicability of machine trans-
lation evaluation metrics to answer ranking in com-
munity Question Answering, a seemingly very dif-
ferent task (compared to MTE). In particular, with
ranking in mind, we have adopted a pairwise neural
network architecture, which incorporates MTE fea-
tures, as well as rich syntactic and semantic embed-
dings of the input texts that are non-linearly com-
bined in the hidden layer.

Our post-competition improvements have shown
state-of-the-art performance (Guzmán et al., 2016),
with sizeable contribution from both the MTE fea-
tures and from the network architecture. This is an
encouraging result as it was not a priori clear that an
MTE approach would work well for cQA.

In future work, we plan to incorporate fine-tuned
word embeddings as in the SemanticZ system (Mi-
haylov and Nakov, 2016b), and information from
entire threads (Nicosia et al., 2015; Barrón-Cedeño
et al., 2015; Joty et al., 2015; Joty et al., 2016). We
also want to add more knowledge sources, e.g., as
in the SUper Team system (Mihaylova et al., 2016),
including veracity, sentiment, complexity, troll user
features as inspired by (Mihaylov et al., 2015a; Mi-
haylov et al., 2015b; Mihaylov and Nakov, 2016a),
and PMI-based goodness polarity lexicons as in the
PMI-cool system (Balchev et al., 2016).

We further plan to explore the application of our
NN architecture to subtasks B and C, and to study
the interactions among the three subtasks in order to
solve the primary subtask C. Furthermore, we would
like to try a similar neural network for other seman-
tic similarity problems, such as textual entailment.

892



Acknowledgments

This research was performed by the Arabic Lan-
guage Technologies (ALT) group at the Qatar Com-
puting Research Institute (QCRI), HBKU, part of
Qatar Foundation. It is part of the Interactive sYs-
tems for Answer Search (Iyas) project, which is de-
veloped in collaboration with MIT-CSAIL.

References

Daniel Balchev, Yasen Kiprov, Ivan Koychev, and Preslav
Nakov. 2016. PMI-cool at SemEval-2016 Task 3: Ex-
periments with pmi and goodness polarity lexicons for
community question answering. In Proceedings of the
10th International Workshop on Semantic Evaluation,
SemEval ’16, San Diego, California, USA.

Alberto Barrón-Cedeño, Simone Filice, Giovanni
Da San Martino, Shafiq Joty, Lluı́s Màrquez, Preslav
Nakov, and Alessandro Moschitti. 2015. Thread-level
information for comment classification in community
question answering. In Proceedings of the 53rd
Annual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference
on Natural Language Processing, ACL-IJCNLP ’15,
pages 687–693, Beijing, China.

Yoshua Bengio and Xavier Glorot. 2010. Understanding
the difficulty of training deep feedforward neural net-
works. In Proceedings of the 13th International Con-
ference on Artificial Intelligence and Statistics, vol-
ume 9 of AISTATS ’10, pages 249–256, Chia Laguna
Resort, Sardinia, Italy.

Adam Berger, Rich Caruana, David Cohn, Dayne Freitag,
and Vibhu Mittal. 2000. Bridging the lexical chasm:
Statistical approaches to answer-finding. In Proceed-
ings of the 23rd Annual International ACM Confer-
ence on Research and Development in Information Re-
trieval, SIGIR ’00, pages 192–199, Athens, Greece.

James Bergstra, Olivier Breuleux, Frédéric Bastien, Pas-
cal Lamblin, Razvan Pascanu, Guillaume Desjardins,
Joseph Turian, David Warde-Farley, and Yoshua Ben-
gio. 2010. Theano: a CPU and GPU math expres-
sion compiler. In Proceedings of the Python for Scien-
tific Computing Conference, SciPy ’10, Austin, Texas,
USA.

Mauro Cettolo, Christian Girardi, and Marcello Federico.
2012. WIT3: Web inventory of transcribed and trans-
lated talks. In Proceedings of the 16th Conference
of the European Association for Machine Translation,
EAMT ’12, pages 261–268, Trento, Italy.

George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence

statistics. In Proceedings of the Second Interna-
tional Conference on Human Language Technology
Research, HLT ’02, pages 138–145, San Diego, Cal-
ifornia, USA.

Cicero dos Santos, Luciano Barbosa, Dasha Bogdanova,
and Bianca Zadrozny. 2015. Learning hybrid rep-
resentations to retrieve semantically equivalent ques-
tions. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 2: Short Papers), ACL-
IJCNLP ’15, pages 694–699, Beijing, China.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning
Research, 12:2121–2159.

Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics - Volume 1, ACL
’03, pages 16–23, Sapporo, Japan.

Minwei Feng, Bing Xiang, Michael R Glass, Lidan
Wang, and Bowen Zhou. 2015. Applying deep
learning to answer selection: A study and an open
task. In Proceedings of the 2015 IEEE Automatic
Speech Recognition and Understanding Workshop,
ASRU ’15, Scottsdale, Arizona, USA.

Francisco Guzmán, Shafiq Joty, Lluı́s Màrquez, and
Preslav Nakov. 2015. Pairwise neural machine trans-
lation evaluation. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguis-
tics and the 7th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers),
ACL-IJCNLP ’15, pages 805–814, Beijing, China.

Francisco Guzmán, Lluı́s Màrquez, and Preslav Nakov.
2016. Machine translation evaluation meets commu-
nity question answering. In Proceedings of the 54th
Annual Meeting of the Association for Computational
Linguistics, ACL ’16, Berlin, Germany.

Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM Inter-
national Conference on Information and Knowledge
Management, CIKM ’05, pages 84–90, Bremen, Ger-
many.

Shafiq Joty, Alberto Barrón-Cedeño, Giovanni
Da San Martino, Simone Filice, Lluı́s Màrquez,
Alessandro Moschitti, and Preslav Nakov. 2015.
Global thread-level inference for comment clas-
sification in community question answering. In
Proceedings of the 2015 Conference on Empir-
ical Methods in Natural Language Processing,
EMNLP ’15, pages 573–578, Lisbon, Portugal.

893



Shafiq Joty, Lluı́s Màrquez, and Preslav Nakov. 2016.
Joint learning with global inference for comment clas-
sification in community question answering. In Pro-
ceedings of the 2016 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, NAACL-
HLT ’16, San Diego, California, USA.

Alon Lavie and Michael Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2–3):105–115.

Tao Lei, Hrishikesh Joshi, Regina Barzilay, Tommi S.
Jaakkola, Kateryna Tymoshenko, Alessandro Mos-
chitti, and Lluı́s Màrquez. 2016. Denoising bodies
to titles: Retrieving similar questions with recurrent
convolutional models. In Proceedings of the 15th An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, NAACL-HLT ’16, San
Diego, California, USA.

Shuguang Li and Suresh Manandhar. 2011. Improv-
ing question recommendation by exploiting informa-
tion need. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies - Volume 1, HLT ’11,
pages 1425–1434, Portland, Oregon, USA.

Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics for
paraphrase identification. In Proceedings of the 2012
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, NAACL-HLT ’12, pages 182–
190, Montréal, Canada.

Todor Mihaylov and Preslav Nakov. 2016a. Hunting for
troll comments in news community forums. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL ’16, Berlin,
Germany.

Todor Mihaylov and Preslav Nakov. 2016b. SemanticZ
at SemEval-2016 Task 3: Ranking relevant answers in
community question answering using semantic simi-
larity based on fine-tuned word embeddings. In Pro-
ceedings of the 10th International Workshop on Se-
mantic Evaluation, SemEval ’16, San Diego, Califor-
nia, USA.

Todor Mihaylov, Georgi Georgiev, and Preslav Nakov.
2015a. Finding opinion manipulation trolls in news
community forums. In Proceedings of the Nine-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ’15, pages 310–314, Beijing,
China.

Todor Mihaylov, Ivan Koychev, Georgi Georgiev, and
Preslav Nakov. 2015b. Exposing paid opinion manip-
ulation trolls. In Proceedings of the International Con-
ference Recent Advances in Natural Language Pro-

cessing, RANLP ’15, pages 443–450, Hissar, Bul-
garia.

Tsvetomila Mihaylova, Pepa Gencheva, Martin Boy-
anov, Ivana Yovcheva, Todor Mihaylov, Momchil
Hardalov, Yasen Kiprov, Daniel Balchev, Ivan Koy-
chev, Preslav Nakov, Ivelina Nikolova, and Galia An-
gelova. 2016. SUper Team at SemEval-2016 Task 3:
Building a feature-rich system for community question
answering. In Proceedings of the 10th International
Workshop on Semantic Evaluation, SemEval ’16, San
Diego, California, USA.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space word
representations. In Proceedings of the 2013 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies, NAACL-HLT ’13, pages 746–751, At-
lanta, Georgia, USA.

Preslav Nakov, Lluı́s Màrquez, Walid Magdy, Alessan-
dro Moschitti, Jim Glass, and Bilal Randeree. 2015.
Semeval-2015 task 3: Answer selection in commu-
nity question answering. In Proceedings of the 9th
International Workshop on Semantic Evaluation, Se-
mEval ’15, pages 269–281, Denver, Colorado, USA.

Preslav Nakov, Lluı́s Màrquez, Alessandro Moschitti,
Walid Magdy, Hamdy Mubarak, Abed Alhakim Frei-
hat, Jim Glass, and Bilal Randeree. 2016. SemEval-
2016 task 3: Community question answering. In Pro-
ceedings of the 10th International Workshop on Se-
mantic Evaluation, SemEval ’16, San Diego, Califor-
nia, USA.

Massimo Nicosia, Simone Filice, Alberto Barrón-
Cedeño, Iman Saleh, Hamdy Mubarak, Wei Gao,
Preslav Nakov, Giovanni Da San Martino, Alessandro
Moschitti, Kareem Darwish, Lluı́s Màrquez, Shafiq
Joty, and Walid Magdy. 2015. QCRI: Answer selec-
tion for community question answering - experiments
for Arabic and English. In Proceedings of the 9th
International Workshop on Semantic Evaluation, Se-
mEval ’15, pages 203–209, Denver, Colorado, USA.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meting of the Association for Computational
Linguistics, ACL ’02, pages 311–318, Philadelphia,
Pennsylvania, USA.

Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statisti-
cal machine translation for query expansion in answer
retrieval. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
ACL ’07, pages 464–471, Prague, Czech Republic.

Aliaksei Severyn and Alessandro Moschitti. 2015.
Learning to rank short text pairs with convolutional

894



deep neural networks. In Proceedings of the 38th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR ’15,
pages 373–382, Santiago, Chile.

Yikang Shen, Wenge Rong, Nan Jiang, Baolin Peng, Jie
Tang, and Zhang Xiong. 2015. Word embedding
based correlation model for question/answer match-
ing. arXiv preprint arXiv:1511.04646.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proceedings of the 7th Biennial Conference of
the Association for Machine Translation in the Amer-
icas, AMTA ’06, pages 223–231, Cambridge, Mas-
sachusetts, USA.

Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with compositional
vector grammars. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), ACL ’13, pages 455–
465, Sofia, Bulgaria.

Radu Soricut and Eric Brill. 2006. Automatic question
answering using the web: Beyond the factoid. Inf.
Retr., 9(2):191–206, March.

Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Comput. Lin-
guist., 37(2):351–383, June.

Ming Tan, Bing Xiang, and Bowen Zhou. 2015. Lstm-
based deep learning models for non-factoid answer se-
lection. arXiv preprint arXiv:1511.04108.

Quan Hung Tran, Vu Tran, Tu Vu, Minh Nguyen,
and Son Bao Pham. 2015. JAIST: Combining
multiple features for answer selection in community
question answering. In Proceedings of the 9th In-
ternational Workshop on Semantic Evaluation, Se-
mEval ’15, pages 215–219, Denver, Colorado, USA.

Di Wang and Eric Nyberg. 2015. A long short-term
memory model for answer sentence selection in ques-
tion answering. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguis-
tics and the 7th International Joint Conference on Nat-
ural Language Processing (Volume 2: Short Papers),
ACL-IJCNLP ’15, pages 707–712, Beijing, China.

Guangyou Zhou, Tingting He, Jun Zhao, and Po Hu.
2015. Learning continuous word embedding with
metadata for question retrieval in community question
answering. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural
Language Processing (Volume 2: Short Papers), ACL-
IJCNLP ’15, pages 250–259, Beijing, China.

895


