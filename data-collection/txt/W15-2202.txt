



















































Combining Active Learning and Partial Annotation for Domain Adaptation of a Japanese Dependency Parser


Proceedings of the 14th International Conference on Parsing Technologies, pages 11–19,
Bilbao, Spain; July 22–24, 2015. c©2015 Association for Computational Linguistics

Combining Active Learning and Partial Annotation
for Domain Adaptation of a Japanese Dependency Parser

Daniel Flannery∗
Vitei Inc.

Kankoboko-cho 79 Shimogyo-ku,
Kyoto, Japan

danielflannery@gmail.com

Shinsuke Mori
ACCMS, Kyoto University

Yoshida Honmachi, Sakyo-ku,
Kyoto, Japan

forest@i.kyoto-u.ac.jp

Abstract

The machine learning-based approaches
that dominate natural language processing
research require massive amounts of la-
beled training data. Active learning has
the potential to substantially reduce the
human effort needed to prepare this data
by allowing annotators to focus on only
the most informative training examples.
This paper shows that active learning can
be used for domain adaptation of depen-
dency parsers, not just in single-domain
settings. We also show that entropy-based
query selection strategies can be combined
with partial annotation to annotate infor-
mative examples in the new domain with-
out annotating full sentences. Simulations
are common in work on active learning,
but we measured the actual time needed
for manual annotation of data to better
frame the results obtained in our simula-
tions. We evaluate query strategies based
on both full and partial annotation in sev-
eral domains, and find that they reduce the
amount of in-domain training data needed
for domain adaptation by up to 75% com-
pared to random selection. We found
that partial annotation delivers better in-
domain performance for the same amount
of human effort than full annotation.

1 Introduction

Active learning is a promising approach for do-
main adaptation because it offers a way to reduce
the amount of data needed to train classifiers, min-
imizing the amount of difficult in-domain annota-
tion. This type of annotation requires annotators to
have both domain knowledge plus familiarity with

*This work was done when the first author was at Kyoto
University.

annotation standards. There has been much recent
work on active learning for a variety of natural lan-
guage processing tasks (Olsson, 2009), but most of
it is concerned only with the single-domain case.
Additionally, work on active learning commonly
reports results for simulations only because of the
high cost of annotation work.

We use active learning to perform domain adap-
tation for a Japanese dependency parsing task, and
measure the actual time required for manual anno-
tation of training data to better frame the results of
our experiments. This kind of evaluation is crucial
for assessing the effectiveness of active learning in
practice.

Previous work on active learning for structured
prediction tasks like parsing (Hwa, 2004) often
assumes that the training data must be fully an-
notated. But recent work on dependency parsing
(Spreyer et al., 2010; Flannery et al., 2011) has
shown that models trained from partially anno-
tated data (where only part of the tree structure is
annotated) can achieve competitive performance.
However, deciding which portion of the tree struc-
ture to annotate remains a difficult problem.

2 Related Work

Most previous work on active learning for parsing
(Hwa, 2004; Sassano and Kurohashi, 2010) stud-
ies the single-domain case, where the initial la-
beled data set and the pool of unlabeled data share
the same domain. An important difference from
previous work is that we focus on domain adapta-
tion, so we assume that the initial labeled data and
annotation pool come from different domains.

Previous work on active learning for parsing
(Tang et al., 2002; Hwa, 2004) has focused on se-
lecting sentences to be fully annotated. Sassano
and Kurohashi (2010) showed that smaller units
like phrases (bunsetsu) could also be used in an ac-
tive learning scenario for a Japanese dependency
parser. Their work included results for partially

11



i 1 2 3 4 5 6 7 8 9

an
no

ta
tio

n wi 政府 は 投資 に つなが る と 歓迎 し
Eng. Gov. subj. investment to leads ending that welcomes do
ti noun part. noun part. verb infl. part. noun verb

di full 2 8 4 5 6 7 8 9 NULL
di part. 8

fe
at

ur
es

F1 6
F2 は 歓迎
F3 part. noun
F4 NULL, NULL,政府 つなが,る,と
F5 投資,に,つなが し , NULL, NULL
F6 NULL, NULL, noun verb, infl., part.
F7 noun, part., verb verb, NULL, NULL

The second word, the case markerは (subj.), has two grammatically possible heads: the verbs つなが
(leads) and歓迎 (welcomes). In the partial annotation framework, only this word needs to be annotated.

Table 1: An example of full annotation (di full) and partial annotation (di part.) for a sentence. Features
for the dependency between the case markerは (subj.) and the verb歓迎 (welcomes) are also shown.

annotated sentences, but did not use entropy-based
query strategies (Tang et al., 2002; Hwa, 2004) de-
signed for selecting whole sentences because of
the difficulty of applying them. We use an even
smaller unit, words, and show how entropy-based
measures can be successfully applied to their se-
lection.

Mirroshandel and Nasr (2011) also investigated
selection of units smaller than sentences for a
graph-based parser in the single-domain setting.
Their query strategy used an entropy-based mea-
sure calculated from n-best lists, which are com-
putationally expensive and require modification of
the parser’s edge scoring function to produce. In
contrast, our query strategy is a simpler one that
does not require n-best lists.

All of the work discussed here reports results for
simulations only. This is common practice in ac-
tive learning research because large-scale annota-
tion is prohibitively expensive. Some recent work
on active learning has started to include more re-
alistic measures of the actual costs of annotation
(Settles et al., 2008). In this paper, we measure the
time needed to manually annotate sentences with
dependencies to better understand the costs of ac-
tive learning for dependency parsing.

3 MST Parsing

Currently, the two major types of data-driven de-
pendency parsers are shift-reduce parsers (Nivre
et al., 2006) and graph-based parsers (McDon-
ald et al., 2005). Shift-reduce parsers perform
parsing deterministically (so their time complex-
ity can be as fast as linear in the size of the input).
Graph-based dependency parsers treat parsing as

the search for a directed maximum spanning tree
(MST). We adopt the latter type in this paper be-
cause its accuracy is slightly higher especially for
long sentences (McDonald and Nivre, 2011).

3.1 Partial Annotation
Our goal is to reduce the total cost of preparing
data for domain adaptation. We do this by combin-
ing partial annotation with active learning. Partial
annotation refers to an annotation method where
only some dependencies in a sentence are anno-
tated with their heads. The standard method in
which all words must be annotated with heads is
called full annotation. Table 1 shows an example
of both types of annotation for a sentence.

Full sentences are the default unit of annotation
in full annotation, even though the parser is trained
from and operates on smaller units such as words
or chunks. The motivation for partial annotation
is to match the unit of annotation with the small-
est unit that the parser uses for training. In the
extreme case this is as small as a single depen-
dency between two words. This fine-grained anno-
tation unit is a natural fit for active learning, where
we seek to find training examples with the great-
est training value. However, fine-grained units are
cognitively more difficult for a human annotator
because less context is available. Thus, we must
balance the granularity of annotations against the
difficulty of processing them.

3.2 Pointwise MST Parsing
To enable the use of partial annotation in active
learning, we use a pointwise MST parser (Flan-
nery et al., 2011) that predicts each word’s head
independently. It uses only simple features based

12



on surface forms and part-of-speech (POS) tags
of words, and first-order features between pairs of
head and dependent words. Higher-order features
that refer to chains of two or more dependencies,
like the ones used in the second-order MST intro-
duced by McDonald and Pereira (2006), are not
used. These restrictions make it easier to train on
partially annotated sentences without sacrificing
accuracy. Flannery et al. (2011) reported that both
this parser and McDonald and Pereira (2006)’s
second-order MST parser achieved just under 97%
accuracy on a Japanese dependency parsing task.
The assumption that written Japanese is head-final
and that dependencies only go from left to right
may be one reason why there is less of a perfor-
mance gap between these two approaches than in
other languages. They also reported that the train-
ing time of their parser is fifteen times faster than
the second-order MST parser, making it easier to
use with active learning.

The following features, both individually and
as combination features, are used in the pointwise
parser that we adopt.

F1: The distance j − i between a dependent word
wi and its candidate head wj .

F2: The surface forms wi and wj .

F3: The parts-of-speech of wi and wj .

F4: The surface forms of up to three words to the
left of wi and wj .

F5: The surface forms of up to three words to the
right of wi and wj .

F6: The parts-of-speech of the words selected for
F4.

F7: The parts-of-speech of the words selected for
F5.

Table 1 shows the values of these features for
a partially annotated example sentence where one
word, the case markerは (subj.), has been anno-
tated with its head, the verb歓迎 (welcomes). Par-
tial annotation allows annotators to ignore trivial
dependencies that are assumed to have little train-
ing value.

4 Partial Annotation as a Query Strategy

In this section we give some background on active
learning and outline the query strategies that we
use to identify informative training examples.

oracle
(human

annotator)

pool of
unlabeled

data

machine
learning
model

labeled
training

data

train model

make query

Figure 1: The pool-based active learning cycle.

4.1 Pool-Based Active Learning
We use the pool-based approach to active learn-
ing (Lewis and Gale, 1994), because it is a natural
fit for domain adaptation. In this framework, we
have both initial training data DL (corresponding
to labeled source domain corpora) and a large pool
of unlabeled data DU (corresponding to unlabeled
target domain text) from which to choose training
examples. While labeling domain-specific text is
difficult, it is usually relatively easy to acquire (for
example, from the web).

In each iteration the entire pool is evaluated se-
quentially and its members are ranked by their es-
timated training value as determined by some cri-
terion, called the query strategy. The top instances
are typically selected greedily. The basic flow
of pool-based active learning is Figure 1 and de-
scribed below.

1. Use a base learner B to train a classifier C
from the labeled training set DL.

2. Apply C to the unlabeled data set DU and
select I , the n most informative training ex-
amples.

3. Make a query to the oracle for the correct la-
bels of training instances in I .

4. Move training instances in I from DU to DL.

5. Train a new classifier C ′ by applying B to
DL.

6. Repeat steps 2 to 5 until some stopping con-
dition is fulfilled.

The stopping condition for terminating active
learning depends on the application. It may be
convenient to stop after a classifier C ′ with a given
level of accuracy is reached or a fixed amount

13



of data has been labeled. In a realistic domain
adaptation scenario we are usually concerned with
achieving a reasonable level of in-domain perfor-
mance while keeping down annotation costs, so
these kinds of simple termination criteria are suf-
ficient.

4.2 Tree Entropy

Hwa (2004) proposed an active learning query
strategy called tree entropy for selecting sentences
to be fully annotated. Choosing a parse tree v for
a sentence from the set of possible parse trees V is
treated as assigning a value to the random variable
V . The entropy of V ,

H(V ) = −
∑
v∈V

p(v) log2(p(v)), (1)

is equivalent to the expected number of bits needed
to encode the distribution of possible parse trees.
Here, p(v) is the probability of assigning a single
parse tree V = v using a given parsing model.
Distributions close to uniform have higher en-
tropy, corresponding to higher uncertainty of the
model. Longer sentences will have more parse
trees in V and thus a larger value of H(V ). To
compare sentences of varying lengths we normal-
ize H(V ) by the log of the number of parse trees
in V .

Hn(V ) =
H(V )

log2(|V|)
(2)

4.3 1-Stage Selection

To use tree entropy as a strategy for partial anno-
tation, we propose to change the unit of selection
to words as follows. Consider a word wi in an in-
put sentence w = 〈w1, w2, . . . , wn〉, tagged with
POS tags t = 〈t1, t2, . . . , tn〉 by a tagger. We
will model the distribution of its possible heads, or
head entropy. Let wj be a single head word for wi,
where j > i and wj 6= wi1. Then we can redefine
v as a choice of position j and V as the set of legal
values for j. Thus p(v) becomes the probability
of choosing the word at position j as the head of
the one at position i. The parser we use (Flannery
et al., 2011) calculates p(v) = p(j|i) as follows.
The feature vector φ = 〈φ1, φ2, . . . , φm〉 consists
of real values calculated from features on pairs

1We assume that Japanese is a head-final language, and
that each head wj is located to the right of its dependent wi
in the sentence.

(i, j) along with their contexts w and t, with cor-
responding weights given by the parameter vector
θ = 〈θ1, θ2, . . . , θm〉.

p(j|w, t, i,θ) = exp (θ · φ(i, j))∑
j′∈J exp (θ · φ(i, j′))

(3)

The simplest way to combine this query strategy
with partial annotation is to calculate the head en-
tropy for each word appearing in a sentence in the
annotation pool, and then choose individual words
with the highest head entropy for annotation. We
call this query strategy 1-stage.

4.4 2-Stage Selection
We expect 1-stage to perform well at identifying
words with high training value. However, in re-
ality it is difficult to annotate heads for individ-
ual words without considering the overall sentence
structure, so annotators must consider other de-
pendencies. 1-stage does not realistically model
annotation costs.

To address this problem, we propose a novel
strategy called 2-stage which more accurately re-
flects the annotation process. It balances the
ability to select fine-grained units for annotation
against the difficultly of annotating them.

Words to annotate with heads are chosen in two
steps. First, the entropy of each sentence in the
pool is calculated by summing the head entropy of
its words, and sentences are ranked from highest
to lowest summed head entropy. Next, the sen-
tence with the highest head entropy is chosen and
the words it contains are ranked in decreasing or-
der by their head entropy. A fixed proportion r
of the highest-entropy words are then annotated.
This value balances annotation granularity against
annotation difficulty. A value of r = 1.0 is the
standard full annotation case where all words are
annotated with heads, which we refer to as 2-stage
full. A value of r = 0.33 means that the top 33%
of the highest-entropy words in the sentence will
be annotated, so we call this strategy 2-stage par-
tial2. In Section 5, we report results for these two
values, though several were tried.

5 Evaluation

To evaluate the query strategies, we measured the
reduction in target domain annotations needed to

2We chose this value because it had good results in pre-
liminary experiments where we tried values in the range from
0.1 to 1.0.

14



ID source sentences words avg. length dependencies
EHJ-train Dictionary examples 11,700 147,964 12.6 136,264

po
ol

NKN-train Newspaper articles 9,023 263,425 29.2 254,402
JNL-train Journal abstracts 322 12,263 38.1 11,941
NPT-train NTCIR patents 450 18,378 40.8 17,928

te
st

NKN-test Newspaper articles 1,002 29,037 29.0 28,035
JNL-test Journal abstracts 32 1,116 34.9 1,084
NPT-test NTCIR patents 50 2,275 45.5 2,225

Table 2: Sizes of corpora.

reach a certain level of in-domain accuracy. For
the 2-stage strategy, we also measured how many
dependencies a real annotator could annotate in a
given time using partial and full annotation. Mea-
suring the actual annotation time is important be-
cause our goal of active learning is to reduce the
amount of human effort needed to prepare labeled
training data for domain adaptation.

We used a corpus of example Japanese sen-
tences from a dictionary as source domain train-
ing data (Mori et al., 2014). This data was used as
to train the initial model in each experiment. We
also collected Japanese text from three target do-
mains: newspapers3, journal article abstracts, and
patents (Goto et al., 2011). For each domain, there
is a large annotation pool of potential training ex-
amples and a smaller test set. See Table 2 for the
details. Domain adaptation is needed in each case,
because sentence length and vocabulary differs for
each. Words in each sentence were manually seg-
mented and assigned POS automatically with the
tagger KyTea. This step can be done automatically
because KyTea’s F-measure score for word seg-
mentation and POS tagging is about 98% (Neubig
et al., 2011). Words were then manually annotated
with their heads.

5.1 Number of Annotations

We first investigate how much strategies reduce
the number of in-domain dependencies needed for
domain adaptation. Because real annotation is
costly and not strictly necessary to measure this
reduction, we simulate active learning by selecting
the gold standard dependency labels from the an-
notation pool. In practice, we are also concerned
with the time needed for a human to annotate
dependencies, which we examine in Section 5.3.
Thus, good performance in this first experiment is

3The newspaper is similar to the Wall Street Journal and
focuses on economics.

0.86

0.87

0.88

0.89

0.90

0.91

0.92

 0  5  10  15  20  25  30

Iterations (x100 Annotations)

Target Domain Dependency Accuracy

1-stage
2-stage, partial

2-stage, full
random

length

Figure 2: Newspaper (NKN) domain learning
curves.

a necessary but not sufficient condition for an ef-
fective strategy. Because we assume that Japanese
is a head-final language and heads always occur
to the right of their dependents, for all strategies
the last word in each sentence skipped. For 1-
stage and 2-stage, we also skipped the second-to-
last word in each sentence.

In addition to the 1-stage and 2-stage meth-
ods, we also tested two simple baselines. The
strategy random simply selects words randomly
from the pool. The length strategy simply
chooses words with the longest possible depen-
dency length4. This strategy reflects our intuition
that long-distance dependencies are more difficult
and thus more informative.

We use the dictionary example sentences (see
Table 2) as the initial training set and performed
thirty iterations of active learning. In each itera-
tion, we select a batch of one hundred target do-
main dependency annotations, retrain the model,
and then measure its in-domain accuracy.

4This is the same as selecting dependencies with the
largest number of potential heads because we do not refer to
the gold dependencies until after words have been selected.

15



Figure 2 shows the results for the newspaper
domain. The accuracy of the random strategy in-
creases slowly and peaks at just over 90.5%. For
the first ten iterations the length strategy delivers
an improvement over random, but performs essen-
tially the same after that. This is probably be-
cause newspaper sentences are on average longer
than dictionary examples (see Table 2), so at first
words with the potential for longer dependencies
are slightly more informative. However, this strat-
egy is focused only on the training data and does
not reflect the continuous updates of the model,
and it soon begins to falter.

The 2-stage partial strategy dominates all other
methods, though 1-stage reaches the same level
after thirty-five iterations. Its peak accuracy is
slightly higher than 91%, and it outperforms the
best accuracy achieved by random after just sev-
enteen iterations. In contrast, 2-stage full performs
consistently worse than the partial annotation ver-
sion, with behavior similar to length. While the 1-
stage strategy always outperforms the random one,
it lags behind 2-stage partial.

5.2 Annotation Pool Size

From Table 2, we can see that the size of the an-
notation pool for the newspaper domain is ten to
twenty times as large as the ones for the other do-
mains. The total number of dependencies selected
is 3k, which is only 1.2% of the newspaper pool.
Because the 2-stage strategy chooses some depen-
dencies with lower entropy over competing ones
with higher entropy from other sentences in the
pool, we expect its accuracy to suffer when a much
larger fraction of the pool is selected.

To investigate this effect, we created a smaller
pool from NKN-train that is closer in size to the
ones from the other domains. We used the first
12,165 dependencies for this smaller pool. The re-
sults are shown in Figure 3. It can be seen that
2-stage partial’s lead over the 1-stage strategy has
been eliminated. After seventeen rounds of an-
notation the 1-stage strategy begins to outperform
the 2-stage strategy. The 2-stage partial strategy
still dominates the 2-stage full strategy. This con-
firms our intuition that the relative performance of
strategies is influenced by the size of the annota-
tion pool. In general we expect the number of in-
formative dependencies to increase as the pool size
increases. Comparing these results with the results
for the newspaper domain in Figure 2, we see that

0.86

0.87

0.88

0.89

0.90

0.91

0.92

 0  5  10  15  20  25  30

Iterations (x100 Annotations)

Target Domain Dependency Accuracy

1-stage
2-stage, partial

2-stage, full

Figure 3: Newspaper (NKN) domain accuracy
with a small annotation pool.

the 1-stage strategy is robust to changes in the pool
size, but the 2-stage partial can outperform it for a
very large pool.

5.3 Time Required for Annotation

Simulation experiments are still common when
using active learning because the cost of annota-
tion is very high. However, recently there is in-
creased interest in measuring the true costs of an-
notation work when doing active learning (Settles
et al., 2008). For a more realistic evaluation of
active learning for parsing, we also measured an-
notation time for the 2-stage strategy. We trained
a model on EHJ-train plus NKN-train and used
this model and the 2-stage strategy to select de-
pendencies to be annotated by a human annota-
tor. The pool is 747 blog sentences5 from the Bal-
anced Corpus of Contemporary Written Japanese
(Maekawa, 2008). We selected 2k dependencies
in a single iteration so the annotator did not need
to wait while the model was retrained after each
batch of annotations. While real annotation times
are not constant, this simplification is justified be-
cause we expect the annotation strategy (partial or
full) to have a larger effect on the overall annota-
tion speed than the dependencies that are selected.

A single annotator performed annotations for
one hour each using the 2-stage strategy with both
partial annotation and full annotation, alternating
strategies every fifteen minutes. Sentences with
more than forty words were not presented to the
annotator. Table 3 shows the total number of de-
pendencies annotated after each time period. After

5This data was taken from the Yahoo! Blog (OY) subcor-
pus.

16



0.86

0.87

0.88

0.89

0.90

0.91

0.92

 0  0.5  1  1.5  2  2.5  3

Estimated Annotation Time (Hours)

Target Domain Dependency Accuracy

2-stage, partial
2-stage, full

Figure 4: Estimated annotation time for the news-
paper (NKN) domain.

method 0.25 [h] 0.5 [h] 0.75 [h] 1.0 [h]
partial 226 458 710 1056
full 141 402 756 1018

Table 3: Annotation times for 2-stage methods.

the first fifteen minutes, the annotator completed
226 annotations compared with 141 for full anno-
tation, an increase of about 60%. However, as time
progresses the difference becomes smaller, and af-
ter one hour the number of annotations was almost
identical for both strategies.

From Table 3, we can see that the annotation
speed reaches a maximum of about 350 anno-
tations per fifteen minutes in the full annotation
case, or 1.4k dependencies per hour. We expected
more annotations to be completed when full anno-
tation was used, because sentences have many triv-
ial dependencies. However, the annotator reported
that it was frustrating to check the annotation stan-
dard and how it handled subtle linguistic phenom-
ena. Most of this work can be skipped when using
partial annotation because the annotator was al-
lowed to delete the estimated heads, so the annota-
tion speeds ended up being almost identical. This
result shows the importance of accurately model-
ing the annotation costs in active learning.

For both methods, the average speed is around
1k dependencies per hour. We used these speeds
to estimate the rate of annotation for the experi-
ments from Section 5.1. While this is not entirely
realistic because speeds are likely to vary across
domains, it is sufficient for comparing the relative
performance of strategies in the same domain. The
results are shown in Figure 4. We can see that ac-

0.88

0.89

0.90

0.91

0.92

0.93

 0  5  10  15  20  25  30

Iterations (x100 Annotations)

Target Domain Dep. Accuracy

1-stage
2-stage, partial

2-stage, full
random

length

Figure 5: Journal (JNL) domain learning curves.

0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.90
0.91
0.92
0.93

 0  5  10  15  20  25  30

Iterations (x100 Annotations)

Target Domain Dep. Accuracy

1-stage
2-stage, partial

2-stage, full
random

length

Figure 6: Patent (NPT) domain learning curves.

curacy improves faster for partial than it does for
full, and the difference becomes pronounced after
about half an hour of annotation work. In sum-
mary, partial annotation is more efficient and thus
delivers a greater return on investment than full an-
notation for the proposed query strategy.

5.4 Results for Additional Domains

We also tested the proposed method in two addi-
tional domains. See Table 2 for the details of these
corpora. Figure 5 and Figure 6 show results for the
journal and patent domains, respectively. In these
domains, 2-stage partial failed to outperform the
1-stage strategy. However, it still performed better
than the 2-stage full strategy. As discussed in Sec-
tion 5.2, the performance of the proposed method
suffers when a large portion of the dependencies
in the pool are selected. Here, the 3k dependen-
cies that are selected are a much larger fraction of
the pool – specifically, 16.7% for the patent do-
main and 25.1% for the journal domain. As in the

17



0.88

0.89

0.90

0.91

0.92

0.93

 0  0.5  1  1.5  2  2.5  3

Estimated Annotation Time (Hours)

Target Domain Dependency Accuracy

2-stage, partial
2-stage, full

Figure 7: Estimated annotation time for the jour-
nal (JNL) domain.

domain random full partial
NKN 3,000 – 1,300
JNL 3,000 1,800 900
NPT 2,700 – 1,500

Table 4: Reduction in in-domain data.

newspaper domain, in the patent domain the per-
formance of 2-stage with full annotation is better
than random for the first few iterations but soon
becomes similar. This is not true in the journal do-
main, where this strategy consistently beats ran-
dom. The length strategy edges out random for a
few iterations in both domains, but ultimately their
performance is similar.

Table 4 shows the number of annotations
needed for the highest accuracy by the random
baseline in the second column, while the next two
show the number of annotations needed for the
full and partial versions of 2-stage to outperform
it. Thus, smaller numbers are better. Compared
to the random strategy, 2-stage full had mixed re-
sults. In the journal abstract domain (JNL), it out-
performed the random baseline while using only
60% of the amount of labeled data. However, it
failed to outperform random selection in the other
two domains. In contrast, 2-stage partial consis-
tently outperforms random with only 45% to 55%
of the labeled data. In terms of target domain data
that must be prepared, it is clear that 2-stage par-
tial offers large savings compared to random. It
also does so more consistently and with less data
than 2-stage full.

We also plotted the results for these domains
in terms of estimated annotation time as we did

0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.90
0.91
0.92

 0  0.5  1  1.5  2  2.5  3

Estimated Annotation Time (Hours)

Target Domain Dependency Accuracy

2-stage, partial
2-stage, full

Figure 8: Estimated annotation time for the patent
(NPT) domain.

in Section 5.3. Figure 7 shows the results for the
journal domain and Figure 8 shows the results for
the patent domain. As before, 2-stage full is more
efficient than 2-stage partial. In these domains,
partial dominates full after about one hour of an-
notation work. The gap between them is largest
for the patent domain and smallest for the journal
domain.

6 Conclusions

We combined partial annotation with active learn-
ing to adapt a Japanese dependency parser to new
domains, and showed that active learning is not
limited to single-domain settings. We showed that
an entropy-based query strategy can successfully
identify units smaller than sentences, and that par-
tial annotation can be successfully applied to ac-
tive learning of structured prediction tasks like
parsing. This strategy reduced the amount of in-
domain training data needed for domain adapta-
tion by up to 75%. We also investigated how the
overall size of the annotation pool affects the per-
formance of the query strategy, and found that the
proposed method performs best for large annota-
tion pools.

To more accurately frame our results, we mea-
sured the actual annotation time required by a hu-
man annotator to prepare labeled data using differ-
ent strategies. Using these results to estimate an-
notation times for earlier experiments, we showed
that for the proposed method partial annotation is
more efficient in terms of in-domain performance
obtained per unit of annotation time than full an-
notation.

18



Acknowledgments

This work was supported by JSPS Grants-in-Aid
for Scientific Research Grant Numbers 26280084
and 26540190, NTT agreement dated 06/19/2015,
and Basic Research on Corpus Annotation project
of The National Institute for Japanese Language
and Linguistics. We are also grateful to the an-
notators for their contribution to the design of the
annotation guidelines and their efforts in following
them.

References
Daniel Flannery, Yusuke Miayo, Graham Neubig, and

Shinsuke Mori. 2011. Training dependency parsers
from partially annotated corpora. In Proceedings of
the Fifth International Joint Conference on Natural
Language Processing, Chiang Mai, Thailand.

Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 work-
shop. In Proceedings of NTCIR-9 Workshop Meet-
ing, pages 559–578.

R. Hwa. 2004. Sample selection for statistical parsing.
Computational Linguistics, 30(3).

D. Lewis and W. Gale. 1994. A sequential algorithm
for training text classifiers. In Proceedings of the
17th annual ACM SIGIR conference on research and
development in information retrieval.

Kikuo Maekawa. 2008. Balanced corpus of contem-
porary written Japanese. In Proceedings of the 6th
Workshop on Asian Language Resources.

Ryan McDonald and Joakim Nivre. 2011. Analyzing
and integrating dependency parsers. Computational
Linguistics, 37(4):197–230.

Ryan McDonald and Fernando Pereira. 2006. Online
Learning of Approximate Dependency Parsing Al-
gorithms. In Proceedings of the Eleventh European
Chapter of the Association for Computational Lin-
guistics, volume 6.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajič. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523–530.

Seyed Abolghasem Mirroshandel and Alexis Nasr.
2011. Active learning for dependency parsing us-
ing partially annotated sentences. In Proceedings of
the 12th International Conference on Parsing Tech-
nologies, Dublin, Ireland.

Shinsuke Mori, Hideki Ogura, and Tetsuro Sasada.
2014. A Japanese word dependency corpus. In
Proceedings of the Nineth International Conference
on Language Resources and Evaluation, pages 753–
758.

Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
Japanese morphological analysis. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics.

Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of the Fifth In-
ternational Conference on Language Resources and
Evaluation.

F. Olsson. 2009. A literature survey of active machine
learning in the context of natural language process-
ing. Technical Report T2009:06, Swedish Institute
of Computer Science.

Manabu Sassano and Sadao Kurohashi. 2010. Using
smaller constituents rather than sentences in active
learning for Japanese dependency parsing. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics.

B. Settles, M. Craven, and L. Friedland. 2008. Active
learning with real annotation costs. In Proceedings
of the NIPS Workshop on Cost-Sensitive Learning.

Kathrin Spreyer, Lilja Øvrelid, and Jonas Kuhn. 2010.
Training parsers on partial trees: a cross-language
comparison. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation.

Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002.
Active learning for statistical natural language pars-
ing. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics.

19


