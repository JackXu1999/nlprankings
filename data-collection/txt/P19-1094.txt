



















































From Surrogacy to Adoption; From Bitcoin to Cryptocurrency: Debate Topic Expansion


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 977–990
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

977

From Surrogacy to Adoption; From Bitcoin to Cryptocurrency:
Debate Topic Expansion

Roy Bar-Haim∗, Dalia Krieger∗, Orith Toledo-Ronen∗, Lilach Edelstein, Yonatan Bilu,
Alon Halfon, Yoav Katz, Amir Menczel, Ranit Aharonov and Noam Slonim

IBM Research

Abstract
When debating a controversial topic, it is of-
ten desirable to expand the boundaries of dis-
cussion. For example, we may consider the
pros and cons of possible alternatives to the de-
bate topic, make generalizations, or give spe-
cific examples. We introduce the task of De-
bate Topic Expansion - finding such related
topics for a given debate topic, along with a
novel annotated dataset for the task. We fo-
cus on relations between Wikipedia concepts,
and show that they differ from well-studied
lexical-semantic relations such as hypernyms,
hyponyms and antonyms. We present algo-
rithms for finding both consistent and con-
trastive expansions and demonstrate their ef-
fectiveness empirically. We suggest that de-
bate topic expansion may have various use
cases in argumentation mining.

1 Introduction

Recent years saw substantial advancement of De-
bating Technologies – computational technologies
developed directly to enhance, support, and en-
gage with human debating (Gurevych et al., 2016).
A recent milestone in this field is IBM R©’s Project
Debater R©1, the first demonstration of a live com-
petitive debate between an AI system and a human
debate champion.

When debating a controversial topic, it is often
desirable to expand the boundaries of the discus-
sion, and bring up arguments about related topics.

For example, when discussing the pros and cons
of the presidential system, it is natural to contrast it
with those of the parliamentary system. When de-
bating alternative medicine, we may discuss spe-
cific examples, such as homeopathy and naturopa-
thy. Conversely, when discussing bitcoins, we can
speak more broadly on cryptocurrency.

∗First three authors equally contributed to this work.
1https://www.research.ibm.com/artificial-

intelligence/project-debater/

Consider the use of debating technologies for
decision support, where the pros and cons of a
given proposal are extracted from a large cor-
pus, summarized and presented to the user. Cur-
rent methods for topic-related, corpus-wide argu-
ment mining only specify the given debate topic
in their search queries (Levy et al., 2017, 2018;
Wachsmuth et al., 2017; Stab et al., 2018a,b).
As a result, much of the relevant argumentative
content is left out of their reach. Alternatively,
context-independent argument mining can exhaus-
tively extract argumentative content from a cor-
pus (Lippi and Torroni, 2015), but it cannot tell
which arguments are actually relevant for the topic
in question.

In this work we take a step towards closing this
gap, by introducing the task of Debate Topic Ex-
pansion – finding related topics that can enrich our
arguments and strengthen our case when debat-
ing a given topic. Following previous work (Levy
et al., 2017, 2018), we focus on topics that are
Wikipedia concepts (article titles in Wikipedia).

Two types of expansions are studied: con-
sistent and contrastive (Bar-Haim et al., 2017).
Arguing in favor or against a consistent ex-
pansion may support the same stance towards
the original topic, whereas for contrastive ex-
pansions the stance is reversed. For ex-
ample, Bitcoin⇒Cryptocurrency and Alterna-
tive medicine⇒Homeopathy are consistent expan-
sions, while Presidential system⇒Parliamentary
system is a contrastive expansion, since we may
support the presidential system by criticizing the
parliamentary system. While these relations
may seem reminiscent of hypernyms/hyponyms,
antonyms and co-hyponyms, we show that they
differ from these well-studied relations.

We propose a three-step method for debate topic
expansion. First, expansion candidates are ex-
tracted from a large corpus using a set of prede-



978

fined patterns. Each expansion type makes use of
a different type of corpus and patterns. In the sec-
ond step, we apply a set of filters to the extraction
results. Candidates that pass these filters are man-
ually annotated as good/bad expansions, resulting
in the first dataset for this task.2

The labeled dataset is utilized in the final
step, where we employ supervised classification
to identify good expansions amongst the candi-
dates. We explore two approaches: (i) traditional
feature-based classification, for which we intro-
duce a novel set of features; (ii) a deep neu-
ral network, which is trained by distant supervi-
sion. Experiments with hundreds of unseen topics
show promising results, and the best performance
is achieved by combining both classification ap-
proaches.

2 Task Description

Let DC (debate concept) be a Wikipedia concept
representing a debate topic. Our goal is to find
Wikipedia concepts that represent consistent and
contrastive expansions of DC, as defined in the
previous section. Table 1 lists several positive and
negative examples of candidates extracted for each
expansion type, denoted ⇒ and ;, respectively.
The examples are taken from our labeled dataset,
to be described in the next sections.

Consistent expansions in our dataset include
both broader concepts (examples 1,8,10 in Ta-
ble 1) and more specific concepts (examples
3,5,7). While some of these expansions are strict
hypernyms (1,8) or hyponyms (5,7), other are not
(3,10). Moreover, broader/narrower concepts do
not necessarily make relevant expansions. For ex-
ample, while Vegetarianism is a type of Diet (2),
arguing about diets in general does not seem rele-
vant for a debate about vegetarianism, in particular
since such a debate typically contrasts vegetarian-
ism with other types of diet.

Contrastive expansions involve diverse seman-
tic relations and subtle distinctions. For exam-
ple, opposites may be relevant expansions in some
cases, e.g. (19), but irrelevant in others (15). Con-
trastive expansions are often co-hyponyms of the
DC. For instance, Democracy and Dictatorship
(11) are both forms of government. However,
co-hyponyms are not always appropriate as con-

2The dataset is available at https://www.
research.ibm.com/haifa/dept/vst/
debating_data.shtml

trastive expansions. When debating about Boxing
(e.g., whether it should be banned), we would not
contrast it with Wrestling (17), despite both being
combat sports, as most arguments for and against
boxing equally apply to wrestling.

The above examples illustrate some of the chal-
lenges in this task. The criterion for a good ex-
pansion – its usefulness in a debate – requires
some knowledge and understanding of the possi-
ble contexts in which the given DC may be de-
bated. Moreover, such judgments are, to some ex-
tent, inherently subjective.

3 Acquisition of Expansion Candidates

3.1 Candidate Extraction

The first step in expanding a given DC is extract-
ing expansion concepts for each expansion type.
An expansion concept (EC) is a Wikipedia con-
cept that co-occurs with the DC in some prede-
fined pattern that is matched in a corpus. We use a
wikification tool that matches different variations
of mentioning the same concept in the text. Below
we describe the patterns and corpora used for each
expansion type. For both types we require at least
two pattern matches for each expansion concept.

Consistent expansions. Our list of patterns for
extracting consistent expansions includes some of
the well-known Hearst patterns for extracting hy-
ponyms (Hearst, 1992), as well as some additional
patterns. Some examples are ‘X such as Y ’, ‘X is
a Y ’, and ‘X and other Y ’, whereX matchesDC
and Y matchesEC or vice versa.3 Despite the dif-
ferences between hypernyms/hyponyms and con-
sistent expansions, these patterns provide a rea-
sonable starting point for our algorithm. The pat-
terns are matched in a corpus CN of news articles,
comprising about 10 billion sentences. The sen-
tences undergo wikification and indexing, which
allows efficient pattern search for a given concept.

Contrastive expansions. As previously ob-
served by Bar-Haim et al. (2017), queries to web
search engines often contain contrastive expres-
sions, e.g. “why is renewable energy better than
fossil fuels”, and are typically succinct and easy to
parse.4 We used a corpus CQ of 1.2 billion queries

3See appendices B.1 and B.2 for a complete list of consis-
tent and contrastive patterns.

4It is unlikely, however, to find a concept and its general-
ization in the same query, hence we did not use this corpus
for extracting consistent expansions.

https://www.research.ibm.com/haifa/dept/vst/debating_data.shtml
https://www.research.ibm.com/haifa/dept/vst/debating_data.shtml
https://www.research.ibm.com/haifa/dept/vst/debating_data.shtml


979

Consistent Expansions Contrastive Expansions
Debate Expansion Debate Expansion
Concept Concept Concept Concept

1 Wind power ⇒ Renewable energy 11 Democracy ⇒ Dictatorship
2 Vegetarianism ; Diet (nutrition) 12 Democracy ; Socialism
3 Entrepreneurship ⇒ Startup company 13 Flat tax ⇒ Progressive tax
4 Abortion ; Fetus 14 Flat tax ; Value-added tax
5 Fast food ⇒ McDonald’s 15 Global warming ; Global cooling
6 Fast food ; Restaurant 16 Carbon tax ⇒ Emission trading
7 Gender inequality ⇒ Gender pay gap 17 Boxing ; Wrestling
8 Gender inequality ⇒ Social inequality 18 Polyamory ⇒ Monogamy
9 Organic food ; Local food 19 Private university ⇒ Public university

10 Casino ⇒ Gambling 20 Recycling ⇒ Landfill

Table 1: Positive (⇒) and negative (;) examples of consistent and contrastive expansions.

(450 million distinct queries) from the Blekko
search engine. Sample patterns include ‘X * vs *
Y ’, ‘X * better than * Y ’, and ‘difference between
* X * and * Y ’, where ‘*’ matches any number of
non-concept tokens.

3.2 Candidate Filtering
Candidate extraction is followed by a candidate
filtering step, in which we apply a set of filters to
each extracted pair (DC,EC). The filters for each
expansion type are described below.

Filters for Consistent Expansions
Directionality. We aim to determine whether a
consistent expansion EC is a generalization or a
specialization of the DC based on the number of
times EC it is matched in a each role in any of the
patterns. If the direction is not clearly determined,
i.e. the majority role is matched in less than 80%
of the cases, the expansion is discarded.

Named Entity. Only the more specific concept
amongst DC and EC (according to the deter-
mined direction) may be a named entity5.

Frequency Ratio. Incompatible frequencies of
DC and EC may indicate a bad expansion. Ac-
cordingly, this filter restricts the ratio between the
frequencies of DC and EC in CN . We require
min(Freq(DC)Freq(EC) ,

Freq(EC)
Freq(DC)) ≥ 0.2.

Distributional Similarity. Consistent expan-
sions are expected to occur in contexts similar to
the DC. This is captured by measuring the distri-
butional similarity sD between DC and EC. We
derived concept-level word2vec vectors (Mikolov
et al., 2013a) from CN , where each wikified men-
tion of a concept C was considered an occurrence

5A Wikipedia concept is considered a named entity if its
page type is defined as person, organization or location.

of C. sD(DC,EC) is then defined as the cosine
similarity between the representations of DC and
EC, and we require sD(DC,EC) ≥ 0.5. This
filter may also remove EC that is too broad or too
narrow with respect to DC.

Substring. We require that EC is not a sub-
string of DC and vice versa. This filter discards
expansions such as Private university;University
and Marriage;Gay Marriage.

Additional filters. We also filter concepts con-
taining the phrases ‘Anti-’, ‘List of’ and ‘Lists of’,
and pairs (DC,EC) that co-occur in the same sen-
tence in CN less than 10 times.

Filters for Contrastive Expansions
Named Entity. Neither DC nor EC may be
named entities.

Substring. Same as for consistent expansions.

Semantic relatedness. We found that unlike
consistent relations, contrastive relations better
correlate with semantic relatedness than with dis-
tributional similarity. We use WORT (Ein Dor
et al., 2018), a semantic relatedness tool for
Wikipedia concepts, as our relatedness measure.
We denote this relation sR, and require that sR ≥
0.4.

4 The Debate Topic Expansion Dataset

Based on our candidate acquisition method,
we created the DTE (Debate Topic Expansion)
dataset, comprising about 3,000 annotated pairs of
debate concepts and their expansion candidates.
The dataset contains positive and negative exam-
ples of both consistent and contrastive expansions.
The construction of the dataset is described below.



980

We manually collected a diverse set of 632 de-
bate concepts from a variety of sources, includ-
ing the idebate website6. For each debate con-
cept, we performed candidate extraction and fil-
tering for consistent and contrastive expansions,
as described in the previous section. Each of the
resulting (DC,EC) pairs was assessed by five an-
notators, and was labeled as either positive (good
expansion) or negative (bad expansion), based on
the majority labeling.

One intriguing subtlety we noticed early on is
that in the case of contrastive expansions, whether
or not EC is a good expansion somewhat depends
on our stance towards theDC. If we argue against
the DC (Con stance), we may choose any plau-
sible alternative as our EC, following a line of
argument such as “EC is a better alternative to
DC”. However, when we argue in favor of DC
(Pro stance), the typical argument changes to “if
we don’t choose DC then we are left with EC”,
which requires EC to be the “default” alterna-
tive to DC. For example, when arguing against
atheism, one may argue that Christians are hap-
pier than atheists; however, when taking a pro-
atheism stance, it is better to argue against reli-
gion in general than specifically against Christian-
ity. The annotators were therefore asked to assess
contrastive expansions for both positive and nega-
tive stances. However, developing a classifier that
is able to make such fine distinctions falls out of
the scope of the current work. Instead, we take the
union of good expansions for each stance as our
positive instances, while keeping per-stance anno-
tations in the dataset for future research.

Table 2 provides some statistics on the result-
ing dataset. Our candidate acquisition method was
found to be applicable to a significant portion of
the topics: one or more good consistent expan-
sions were found for 43% of the topics, and good
contrastive expansions were found for 19% of the
topics. Precision, however, is low, even after ap-
plying our filters: 49% for consistent expansions,
and 19% for contrastive expansions. This moti-
vates an additional supervised classification step,
to be presented in the next section. These statistics
suggest that identifying contrastive expansions is
considerably more challenging than finding con-
sistent expansions.

Fleiss’ κ is 0.45 for consistent expansions and
0.43 for the unified contrastive expansions, which

6https://idebate.org/debatabase

Consistent Contrastive
Expansions Expansions

Debate topics
Total 632 632
With expansions 360 (57%) 286 (45%)
With good
expansions 269 (43%) 120 (19%)

Annotated expansions
Total 1,741 1,326
Good expansions 845 (49%) 251 (19%)

Inter-annotator agreement
Fleiss’ κ 0.45 0.43

Table 2: Statistics on the DTE (Debate Topic Expan-
sion) dataset

corresponds to “moderate agreement” (Landis and
Koch, 1997). This level of agreement reflects the
complexity and inherent subjectivity of the task, as
discussed in Section 2, and is comparable to pre-
vious results for annotation tasks in argumentation
mining. For example, Aharoni et al. (2014) report
κ of 0.39-0.4 for claim and evidence annotation in
Wikipedia articles.

5 Supervised Candidate Classification

We experimented with two complementary super-
vised classification methods: feature-based clas-
sification, which integrates diverse types of ev-
idence from various sources, and a distantly-
supervised neural network, which learns to dis-
criminate between positive and negative pairs
based on the contexts in which they co-occur.

5.1 Feature-Based Classification
We train a logistic regression classifier for each ex-
pansion type. The classifiers make use of novel
sets of features designed for this task. Most fea-
tures are shared by both classifiers, and a few ad-
ditional features were developed specifically for
each task. Below we give an overview of the fea-
tures extracted for a given (DC,EC) pair. A more
detailed and complete description is found in Ap-
pendix A.

Similarity & relatedness. The similarity and
relatedness measures sD, sR, defined in Sec-
tion 3.2.

Wikipedia. The following features take advan-
tage of DC and EC both being Wikipedia titles,
and make use of information found in their respec-
tive pages: (i) Number of Wikipedia categories
shared by DC and EC; (ii) Count of occurrences
of DC in EC categories or EC in DC categories

https://idebate.org/debatabase


981

up to two category levels; (iii) count of shared
Wikipedia outlinks of DC and EC.

WordNet. Whether DC is a hypernym, hy-
ponym, synonym or co-hyponym of EC in Word-
Net (Miller, 1995) - four binary features.

Sentiment. Consistent expansions are expected
to have the same sentiment polarity as the DC,
whereas opposite polarities may indicate con-
trastive expansions (e.g., Democracy vs. Dictator-
ship). Similar to Iyyer et al. (2015), we train a lin-
ear SVM classifier on the sentiment lexicon of Hu
and Liu (2004), using the word2vec word embed-
dings computed over the CN corpus as the features
and the word polarities as the labels. Word polar-
ity can then be determined by the sign of the clas-
sifier’s output score, and the sentiment strength by
its magnitude. We take the product of the classi-
fier’s scores for DC and EC as a single sentiment
feature.

Corpus statistics. Simple corpus-based features
are derived from the number of co-occurrences of
DC and EC in the same sentence in CN or in the
same query in CQ. These features are normalized
to [0, 1] by setting for each feature an upper thresh-
old k on the count. Counts in the range of [0, k] are
linearly transformed to [0, 1], and counts above k
are set to 1. We also consider other corpus-based
measures, such as pointwise mutual information
(PMI) between DC and EC. For the consistent
expansions classifier we also use as a feature the
frequency ratio measure, defined in Section 3.2.

Other corpus-based features are based on pat-
tern matching. For instance, we define a set of
contrastive patterns, e.g, ‘Xvs Y ’ and ‘X instead
of Y ’7, and derive features such as the (normal-
ized) count of (DC,EC) matches for these pat-
terns, and the PMI of DC and EC in the subset of
sentences/queries matching the patterns.

Overall, the feature count for the consistent ex-
pansions classifer is 15, and 22 for the contrastive
expansions classifier.

5.2 Distantly Supervised Neural Network
The other classification approach we experi-
mented with is based on distant supervision (Mintz
et al., 2009). As before, we train two separate
classifiers for consistent and for contrastive expan-
sions, using their respective training sets. For each

7This set of patterns partially overlaps with the contrastive
patterns described in Section 3.1.

pair (DC,EC) from the training set, we retrieve
from the CN index up to 10,000 sentences that con-
tain mentions of both DC and EC. The retrieved
sentences are all labeled with the pair’s label - pos-
itive or negative. These labels are noisy, since not
every co-occurrence of DC and EC in a sentence
is indicative of the relation between them. Our
hope, however, is that the large number of train-
ing sentences collected this way would compen-
sate for the noisy labels. The mentions of DC
andEC in each sentence are replaced with generic
symbols, DC and EC, to facilitate generalization
over specific instances. We found that for consis-
tent expansions, it is better to keep only the text
between these two symbols, while for contrastive
expansions, using the whole sentence works bet-
ter. We balance the dataset to have an equal num-
ber of positive and negative training instances for
each type.

The sentences collected for the whole training
set are then used to train a neural network. Es-
sentially, the network aims to determine whether
a given sentence is a positive or a negative evi-
dence for the existence of the target relation (con-
sistent or contrastive expansion) between DC and
EC. When applying the classifier to a new pair, we
collect up to 500 sentences for that pair, and aver-
age the classifier’s predictions for each sentence.

Neural network description. Our network is a
bi-directional LSTM (Graves and Schmidhuber,
2005) with an additional attention layer (Yang
et al., 2016). The models are all trained with
a dropout of 0.85, using a single dropout across
all timesteps as proposed by Gal and Ghahramani
(2016). The cell size in the LSTM layers is 128,
and the attention layer is of size 100. We use the
Adam method as an optimizer (Kingma and Ba,
2015) with a learning rate of 0.001. Words are
represented using the 300 dimensional GloVe em-
beddings learned on 840B Common Crawl tokens
and are left untouched during training (Pennington
et al., 2014).

6 Experiments

6.1 Experimental Setup

We assess the performance of our method on the
following practical task: given a debate concept
DC, find one good expansion concept EC for
each expansion type. Recall that our dataset in-
cludes annotations for all the expansion candidates



982

0.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95

1

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8

P
re
c
is
io
n
@
1

Recall@1

LR+DNN LR DNN SIM

(a) Consistent expansions

0.3

0.35

0.4

0.45

0.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95

1

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

P
re
c
is
io
n
@
1

Recall@1

LR+DNN LR DNN FREQ-Q

(b) Contrastive expansions

Figure 1: Comparison of candidate scoring methods

found for each DC by the candidate acquisition
algorithm. Here we compare different methods
for choosing one good expansion from these can-
didates. For each expansion type, we assume a
scoring function f(X,Y ) over a pair of concepts,
which predicts the likelihood of the target relation
holding between X and Y . We further assume a
threshold α representing the minimum score for a
good expansion. Given a debate concept DC, we
choose its highest scoring expansion, if its score
exceeds the threshold. If no expansion was found,
or all the expansion scores are below the threshold,
we make no prediction. By modifying the thresh-
old, we can explore the tradeoff between the num-
ber of predictions we make and their quality.

The following scoring functions were assessed:

Unsupervised baselines. (i) SIM: the distribu-
tional similarity measure sD ; (ii) REL: the se-
mantic relatedness measure sR; (iii) FREQ-N co-
occurrence frequency in CN ; (iv) FREQ-Q: co-
occurrence frequency in CQ.

Supervised Classification methods. (i) LR: the
output score of the logistic regression classifier
(Section 5.1); (ii) DNN: the output score of the
distantly-supervised neural network (Section 5.2);
(iii) LR+DNN: a simple combination of the LR
and DNN classifiers, defined as the sum of their
outputs.

Data and training. The 632 debate concepts in
the dataset were split into a test set (230 con-
cepts), a train set (295 concepts) and a develop-
ment set (107 concepts). The train and develop-
ment set for the DNN classifiers contain in total,
for consistent expansions 736,305 sentences per

class (positive/negative), and for contrastive ex-
pansions 252,298 sentences per class. The DNN
classifiers were trained on the train portion of this
dataset for 5 epochs, each resulting in a different
model, and the best preforming model on the de-
velopment set was chosen.

Due to the small number of instances in the de-
velopment set, we did not use it to tune the LR
classifier, but rather used both the train and the de-
velopment sets to train the classifier. Together they
contain 983/626 consistent/contrastive expansion
candidates, respectively.

Performance measures. Let N be the total
number of debate concepts in the test set, let C
be the number of correct predictions, let P be
the number of predictions made, and let R be
the number of debate concepts in the test set for
which good expansions exist. We define the fol-
lowing measures: (i) Precision@1= CP ; (ii) Re-
call@1= CR ; and (iii) Coverage=

P
N .

6.2 Results

Figure 1 compares the above candidate scoring
methods for both consistent (a) and contrastive
(b) expansions. For each configuration, the Pre-
cision@1 vs Recall@1 graph is obtained by modi-
fying the threshold α. Only the best-performing
baseline for each expansion type is shown, for
readability. Both the LR and the LR+DNN con-
figurations outperform the strongest baseline by a
large margin. This result illustrates the importance
of supervised learning for this task.

For consistent expansions, LR+DNN is clearly
the best-preforming configuration. For con-
trastive expansions, it outperforms the LR classi-



983

Recall@1
Consistent Contrastive

Precision@1 Expansions Expansions
0.9 0.248 0.164
0.8 0.651 0.262
0.7 0.706 0.525
0.6 0.780 0.574
0.5 0.798 0.721

Table 3: LR+DNN – Recall@1 for selected Preci-
sion@1 values.

fier in high-precision/low recall areas (Recall@1<
0.53). For high-recall/low-precision areas, the LR
classifier performs better.

As one may expect, the performance for con-
sistent expansions is better than the performance
for contrastive expansions, as the latter seems a
more challenging task. Interestingly, for consis-
tent expansions, SIM is the strongest baseline,
whereas for contrastive expansions the best base-
line is FREQ-Q. The performance of the DNN for
consistent expansions is comparable to the best
baseline, but for contrastive expansions it is much
higher. Again, this may be attributed to difference
in the difficulty of the two tasks, which requires,
for contrastive expansions, more powerful meth-
ods.

We now take a closer look at the results for the
LR+DNN configuration. To illustrate its perfor-
mance, Table 3 includes sample data points for this
configuration, for each expansion type.8

So far we used the Recall@1 measure to com-
pare the coverage of different scoring methods
with respect to the given set of candidates. Thus,
the coverage of the candidate acquisition step was
not taken into account in this assessment. In order
to assess the end-to-end performance of our sys-
tem, we next consider the tradeoff between Preci-
sion@1 and Coverage, as the latter measures the
fraction of debate concepts for which we make a
prediction out of all debate concepts in the test set.

The LR+DNN results for both expansion types
are shown in Figure 2, and sample values are
shown in Table 4. For example, by setting the
threshold appropriately, we can find consistent ex-
pansions for 38.3% of the debate concepts with (at
least) 80% precision. Precision and coverage for
contrastive expansions are lower. For example,
when requiring precision of 70%, we can make

8For each given Precision@1 value p, we look for the
maximal Recall@1 value such that its corresponding Preci-
sion@1 is at least p.

0.3

0.35

0.4

0.45

0.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95

1

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7

P
re
c
is
io
n
@
1

Coverage

Consistent Contrastive

Figure 2: LR+DNN Precision@1 vs. Coverage

Coverage
Consistent Contrastive

Precision@1 Expansions Expansions
0.9 0.130 0.048
0.8 0.383 0.087
0.7 0.478 0.196
0.6 0.613 0.252
0.5 0.665 0.383

Table 4: LR+DNN – Coverage for selected Preci-
sion@1 values.

predictions for nearly 20% of the topics.

7 Related Work

There is a vast body of research work on
identifying semantic relations between a pair
of terms. Most studied relations include hy-
ponyms/hypernyms, synonyms, antonyms, and
meronyms. The main approaches applied to this
task are summarized below.

Pattern-based methods. A fundamental type of
evidence for detecting such relations is based on
co-occurrence of the two terms in some text, typi-
cally in the same sentence. Pattern-based methods
define lexico-syntactic contexts containing slots to
be filled by instances of the target relation. Pat-
terns can be defined over surface forms, or over
syntactic representations such as paths in a depen-
dency parse. Hearst (1992) introduced a pattern-
based method for hyponym extraction, using a
small set of manually-constructed textual patterns
(for example “NP1 such as NP2”). Similar meth-
ods were used by Berland and Charniak (1999)
for extracting meronyms, and by Lin et al. (2003)
for identifying non-synonyms among semantically
similar words.



984

Snow et al. (2005) developed a method to auto-
matically learn new path-based patterns and used
these patterns as features for hypernym classifi-
cation; they later expanded this method for tax-
onomy construction (Snow et al., 2006). Schulte
im Walde and Köper (2013) used automatically
acquired word patterns to distinguish between
antonyms, synonyms and hypernyms in German.

Distant supervision. Mintz et al. (2009) intro-
duced the concept of distant supervision for re-
lation extraction. The idea is to use an external
knowledge base as a source of supervision instead
of using labeled text (Riedel et al., 2010). The
distant supervision paradigm assumes that any
sentence that contains an entity pair of a known
relation is likely to express the relation in the
text. Since this assumption leads to noisy data
and features, researchers have developed multi-
instance approaches to deal with invalid sentences
and wrong labels (Zeng et al., 2015; Riedel et al.,
2010; Surdeanu et al., 2012). Another solution for
the noisy data problem is using sentence-level at-
tention model (Lin et al., 2016).

Distributional methods. Distributional meth-
ods aim to determine the relation between two
terms by independently modeling the contexts in
which each term occurs. Lin (1998) and later
Weeds and Weir (2003) developed distributional
similarity measures and showed that they can be
used to predict hypernymy relations over WordNet
terms.

Translation in a word embedding space may
capture various syntactic and semantic relations
between the words. This was demonstrated by
Mikolov et al. (2013b) and Pennington et al.
(2014) on the task of solving word analogies.
Word embeddings were used for various rela-
tion extraction tasks, by taking their difference
(Roller et al., 2014) or their concatenation (Ba-
roni et al., 2012). Nguyen et al. (2016) used
a distributional-based method for distinguishing
antonyms and synonyms. Roller et al. (2018) com-
pared the performance of Hearst patterns with dis-
tributional methods for hypernymy prediction and
showed that co-occurrence measures of pairs ex-
tracted by Hearst patterns outperforms the distri-
butional methods.

Neural approaches. Following recent advances
in deep learning, many neural network architec-
tures for relation classification have been pro-

posed. Vu et al. (2016) combine recurrent and
convolutional neural networks for relation classi-
fication. Shwartz et al. (2016) combine depen-
dency path embeddings and distributional infor-
mation for hypernym detection, and Nguyen et al.
(2017) present a pattern-based neural network for
distinguishing antonyms and synonyms.

Taxonomy induction from Wikipedia. Apart
from relation extraction, taxonomy induction over
Wikipedia concepts and categories is another line
of research that is related to the current work. Ex-
amples are WikiTaxonomy (Ponzetto and Strube,
2007), YAGO (Hoffart et al., 2013), and the
Wikipedia Bitaxonomy project (Flati et al., 2014).
As described by Gupta et al. (2016), these works
utilize information about Wikipedia concepts, the
category network and the link structure.

The current work makes the following contribu-
tions with respect to previous relation extraction
work. First and foremost, it introduces and stud-
ies a new relation extraction task - finding consis-
tent and contrastive expansions for a given debate
topic. To address this challenging task, we pro-
pose a hybrid architecture that combines diverse
knowledge sources and techniques. Another con-
tribution of this work is a novel set of patterns, fil-
ters and features designed specifically for this task.

Stance classification. Consistent and con-
trastive relations were previously discussed in
the stance classification literature. Somasun-
daran et al. (2009) refer to these relations as
same/alternative, and use them in conjunction
with discourse relations to improve the prediction
of opinion polarity. However, they do not attempt
to identify these relations, but rather take them
from a labeled dataset. Bar-Haim et al. (2017), as
part of their work on claim stance classification,
developed a classifier that aims to distinguish con-
sistent from contrastive relations defined between
the sentiment targets of a claim and the debate
proposition. By contrast, our work addresses both
candidate acquisition and classification, and most
candidates are neither consistent nor contrastive
expansions.

8 Conclusion

This work introduced a new task, debate topic ex-
pansion, along with a corresponding benchmark
dataset, which we plan to make publicly available.
We presented a working solution for this challeng-



985

ing task that achieved promising empirical results.
The best results are obtained by combining diverse
methods and techniques: pattern-based extraction,
a novel set of filters and classification features, and
a distantly-supervised neural network.

Debate topic expansion may be highly valuable
for argumentation mining. For instance, topic-
related argument mining has many potential use
cases, such as helping individuals and organiza-
tions make better decisions, enhancing civic dis-
course by identifying arguments raised in the me-
dia, and promoting critical thinking among stu-
dents. Debate topic expansion can enhance the
coverage of existing argument mining methods by
matching relevant arguments that do not mention
the given topic explicitly. In addition, distinguish-
ing consistent and contrastive expansions may im-
prove argument stance classification. We plan to
pursue these research directions in future work.

References
Ehud Aharoni, Anatoly Polnarov, Tamar Lavee, Daniel

Hershcovich, Ran Levy, Ruty Rinott, Dan Gutfre-
und, and Noam Slonim. 2014. A benchmark dataset
for automatic detection of claims and evidence in the
context of controversial topics. In Proceedings of
the First Workshop on Argumentation Mining, pages
64–68, Baltimore, Maryland. Association for Com-
putational Linguistics.

Roy Bar-Haim, Indrajit Bhattacharya, Francesco Din-
uzzo, Amrita Saha, and Noam Slonim. 2017. Stance
classification of context-dependent claims. In Pro-
ceedings of the 15th Conference of the European
Chapter of the Association for Computational Lin-
guistics: Volume 1, Long Papers, pages 251–261.
Association for Computational Linguistics.

Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-chieh Shan. 2012. Entailment above the
word level in distributional semantics. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguis-
tics, pages 23–32, Avignon, France. Association for
Computational Linguistics.

Matthew Berland and Eugene Charniak. 1999. Find-
ing parts in very large corpora. In Proceedings
of the 37th Annual Meeting of the Association for
Computational Linguistics, pages 57–64, College
Park, Maryland, USA. Association for Computa-
tional Linguistics.

Liat Ein Dor, Alon Halfon, Yoav Kantor, Ran Levy,
Yosi Mass, Ruty Rinott, Eyal Shnarch, and Noam
Slonim. 2018. Semantic relatedness of wikipedia
concepts - benchmark data and a working solu-
tion. In Proceedings of the Eleventh International

Conference on Language Resources and Evaluation
(LREC-2018). European Language Resource Asso-
ciation.

Tiziano Flati, Daniele Vannella, Tommaso Pasini, and
Roberto Navigli. 2014. Two is bigger (and better)
than one: the wikipedia bitaxonomy project. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 945–955, Baltimore, Maryland. As-
sociation for Computational Linguistics.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Proceedings of the 30th Interna-
tional Conference on Neural Information Processing
Systems, NIPS’16, pages 1027–1035, USA. Curran
Associates Inc.

Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional lstm
and other neural network architectures. Neural Net-
works, 18(5-6):602–610.

Amit Gupta, Francesco Piccinno, Mikhail
Kozhevnikov, Marius Pasca, and Daniele Pighin.
2016. Revisiting taxonomy induction over
wikipedia. In Proceedings of COLING 2016, the
26th International Conference on Computational
Linguistics: Technical Papers, pages 2300–2309.
The COLING 2016 Organizing Committee.

Iryna Gurevych, Eduard H. Hovy, Noam Slonim,
and Benno Stein. 2016. Debating Technologies
(Dagstuhl Seminar 15512). Dagstuhl Reports,
5(12):18–46.

Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th Conference on Computational Linguistics
- Volume 2, COLING ’92, pages 539–545, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. Yago2: A
spatially and temporally enhanced knowledge base
from wikipedia. Artif. Intell., 194:28–61.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’04, pages
168–177, New York, NY, USA. ACM.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daumé III. 2015. Deep unordered compo-
sition rivals syntactic methods for text classification.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
1681–1691, Beijing, China. Association for Com-
putational Linguistics.

http://www.aclweb.org/anthology/W14-2109
http://www.aclweb.org/anthology/W14-2109
http://www.aclweb.org/anthology/W14-2109
http://aclweb.org/anthology/E17-1024
http://aclweb.org/anthology/E17-1024
http://www.aclweb.org/anthology/E12-1004
http://www.aclweb.org/anthology/E12-1004
https://doi.org/10.3115/1034678.1034697
https://doi.org/10.3115/1034678.1034697
http://aclweb.org/anthology/L18-1408
http://aclweb.org/anthology/L18-1408
http://aclweb.org/anthology/L18-1408
http://www.aclweb.org/anthology/P14-1089
http://www.aclweb.org/anthology/P14-1089
http://dl.acm.org/citation.cfm?id=3157096.3157211
http://dl.acm.org/citation.cfm?id=3157096.3157211
http://dl.acm.org/citation.cfm?id=3157096.3157211
http://dblp.uni-trier.de/db/journals/nn/nn18.html#GravesS05
http://dblp.uni-trier.de/db/journals/nn/nn18.html#GravesS05
http://dblp.uni-trier.de/db/journals/nn/nn18.html#GravesS05
http://aclweb.org/anthology/C16-1217
http://aclweb.org/anthology/C16-1217
https://doi.org/10.4230/DagRep.5.12.18
https://doi.org/10.4230/DagRep.5.12.18
https://doi.org/10.3115/992133.992154
https://doi.org/10.3115/992133.992154
http://dblp.uni-trier.de/db/journals/ai/ai194.html#HoffartSBW13
http://dblp.uni-trier.de/db/journals/ai/ai194.html#HoffartSBW13
http://dblp.uni-trier.de/db/journals/ai/ai194.html#HoffartSBW13
https://doi.org/10.1145/1014052.1014073
https://doi.org/10.1145/1014052.1014073
http://www.aclweb.org/anthology/P15-1162
http://www.aclweb.org/anthology/P15-1162


986

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceed-
ings of the 3rd International Conference on Learn-
ing Representations, San Diego, 2015.

J. R. Landis and G. G. Koch. 1997. The measurements
of observer agreement for categorical data. Biomet-
rics, 33:159–174.

Ran Levy, Ben Bogin, Shai Gretz, Ranit Aharonov,
and Noam Slonim. 2018. Towards an argumentative
content search engine using weak supervision. In
Proceedings of the 27th International Conference on
Computational Linguistics, pages 2066–2081. Asso-
ciation for Computational Linguistics.

Ran Levy, Shai Gretz, Benjamin Sznajder, Shay Hum-
mel, Ranit Aharonov, and Noam Slonim. 2017. Un-
supervised corpus–wide claim detection. In Pro-
ceedings of the 4th Workshop on Argument Mining,
pages 79–84. Association for Computational Lin-
guistics.

Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of the Fifteenth Inter-
national Conference on Machine Learning, ICML
’98, pages 296–304, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.

Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among distribu-
tionally similar words. In Proceedings of the 18th
International Joint Conference on Artificial Intelli-
gence, IJCAI’03, pages 1492–1493, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2124–2133, Berlin, Germany. Associa-
tion for Computational Linguistics.

Marco Lippi and Paolo Torroni. 2015. Context-
independent claim detection for argument mining.
In Proceedings of the 24th International Conference
on Artificial Intelligence, IJCAI’15, pages 185–191.
AAAI Press.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013a. Distributed repre-
sentations of words and phrases and their composi-
tionality. In Proceedings of the 26th International
Conference on Neural Information Processing Sys-
tems, NIPS’13, pages 3111–3119, USA. Curran As-
sociates Inc.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746–751, Atlanta,
Georgia. Association for Computational Linguistics.

George A. Miller. 1995. Wordnet: A lexical database
for english. Commun. ACM, 38(11):39–41.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 1003–1011, Suntec, Singapore. Association
for Computational Linguistics.

Kim Anh Nguyen, Sabine Schulte im Walde, and
Ngoc Thang Vu. 2016. Integrating distributional
lexical contrast into word embeddings for antonym-
synonym distinction. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
454–459, Berlin, Germany. Association for Compu-
tational Linguistics.

Kim Anh Nguyen, Sabine Schulte im Walde, and
Ngoc Thang Vu. 2017. Distinguishing antonyms
and synonyms in a pattern-based neural network. In
Proceedings of the 15th Conference of the European
Chapter of the Association for Computational Lin-
guistics: Volume 1, Long Papers, pages 76–85, Va-
lencia, Spain. Association for Computational Lin-
guistics.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543, Doha,
Qatar. Association for Computational Linguistics.

Simone Paolo Ponzetto and Michael Strube. 2007. De-
riving a large scale taxonomy from wikipedia. In
Proceedings of the 22Nd National Conference on
Artificial Intelligence - Volume 2, AAAI’07, pages
1440–1445. AAAI Press.

Sebastian Riedel, Limin Yao, and Andrew McCal-
lum. 2010. Modeling relations and their men-
tions without labeled text. In Proceedings of the
2010 European Conference on Machine Learning
and Knowledge Discovery in Databases: Part III,
ECML PKDD’10, pages 148–163, Berlin, Heidel-
berg. Springer-Verlag.

Stephen Roller, Katrin Erk, and Gemma Boleda. 2014.
Inclusive yet selective: Supervised distributional hy-
pernymy detection. In Proceedings of COLING
2014, the 25th International Conference on Compu-
tational Linguistics: Technical Papers, pages 1025–
1036, Dublin, Ireland. Dublin City University and
Association for Computational Linguistics.

Stephen Roller, Douwe Kiela, and Maximilian Nickel.
2018. Hearst patterns revisited: Automatic hyper-
nym detection from large text corpora. In Proceed-
ings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 358–363. Association for Computa-
tional Linguistics.

http://aclweb.org/anthology/C18-1176
http://aclweb.org/anthology/C18-1176
https://doi.org/10.18653/v1/W17-5110
https://doi.org/10.18653/v1/W17-5110
http://dl.acm.org/citation.cfm?id=645527.657297
http://dl.acm.org/citation.cfm?id=645527.657297
http://dl.acm.org/citation.cfm?id=1630659.1630908
http://dl.acm.org/citation.cfm?id=1630659.1630908
http://www.aclweb.org/anthology/P16-1200
http://www.aclweb.org/anthology/P16-1200
http://dl.acm.org/citation.cfm?id=2832249.2832275
http://dl.acm.org/citation.cfm?id=2832249.2832275
http://dl.acm.org/citation.cfm?id=2999792.2999959
http://dl.acm.org/citation.cfm?id=2999792.2999959
http://dl.acm.org/citation.cfm?id=2999792.2999959
http://www.aclweb.org/anthology/N13-1090
http://www.aclweb.org/anthology/N13-1090
https://doi.org/10.1145/219717.219748
https://doi.org/10.1145/219717.219748
http://www.aclweb.org/anthology/P/P09/P09-1113
http://www.aclweb.org/anthology/P/P09/P09-1113
http://anthology.aclweb.org/P16-2074
http://anthology.aclweb.org/P16-2074
http://anthology.aclweb.org/P16-2074
http://www.aclweb.org/anthology/E17-1008
http://www.aclweb.org/anthology/E17-1008
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
http://dl.acm.org/citation.cfm?id=1619797.1619876
http://dl.acm.org/citation.cfm?id=1619797.1619876
http://dl.acm.org/citation.cfm?id=1889788.1889799
http://dl.acm.org/citation.cfm?id=1889788.1889799
http://www.aclweb.org/anthology/C14-1097
http://www.aclweb.org/anthology/C14-1097
http://aclweb.org/anthology/P18-2057
http://aclweb.org/anthology/P18-2057


987

Sabine Schulte im Walde and Maximilian Köper. 2013.
Pattern-based distinction of paradigmatic relations
for german nouns, verbs, adjectives. In Language
Processing and Knowledge in the Web - 25th Inter-
national Conference, GSCL 2013, Darmstadt, Ger-
many, September 25-27, 2013. Proceedings, pages
184–198.

Vered Shwartz, Yoav Goldberg, and Ido Dagan. 2016.
Improving hypernymy detection with an integrated
path-based and distributional method. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2389–2398, Berlin, Germany. Associa-
tion for Computational Linguistics.

Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and
Léon Bottou, editors, Advances in Neural Informa-
tion Processing Systems 17, pages 1297–1304. MIT
Press, Cambridge, MA.

Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 801–808, Sydney, Aus-
tralia. Association for Computational Linguistics.

Swapna Somasundaran, Galileo Namata, Janyce
Wiebe, and Lise Getoor. 2009. Supervised and
unsupervised methods in employing discourse rela-
tions for improving opinion polarity classification.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
170–179, Singapore. Association for Computational
Linguistics.

Christian Stab, Johannes Daxenberger, Chris Stahlhut,
Tristan Miller, Benjamin Schiller, Christopher
Tauchmann, Steffen Eger, and Iryna Gurevych.
2018a. Argumentext: Searching for arguments in
heterogeneous sources. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Demon-
strations, pages 21–25. Association for Computa-
tional Linguistics.

Christian Stab, Tristan Miller, Benjamin Schiller,
Pranav Rai, and Iryna Gurevych. 2018b. Cross-
topic argument mining from heterogeneous sources.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
3664–3674. Association for Computational Linguis-
tics.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 455–
465, Jeju Island, Korea. Association for Computa-
tional Linguistics.

Ngoc Thang Vu, Heike Adel, Pankaj Gupta, and Hin-
rich Schütze. 2016. Combining recurrent and convo-
lutional neural networks for relation classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 534–539. Association for Computational Lin-
guistics.

Henning Wachsmuth, Martin Potthast, Khalid
Al Khatib, Yamen Ajjour, Jana Puschmann, Jiani
Qu, Jonas Dorsch, Viorel Morari, Janek Bevendorff,
and Benno Stein. 2017. Building an argument
search engine for the web. In Proceedings of the
4th Workshop on Argument Mining, pages 49–59.
Association for Computational Linguistics.

Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
the 2003 Conference on Empirical Methods in Nat-
ural Language Processing, pages 81–88.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, pages 1480–1489. Association for Computa-
tional Linguistics.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
piecewise convolutional neural networks. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1753–
1762. Association for Computational Linguistics.

Appendices

A Features Used by the Logistic
Regression Classifiers

Section A.1 lists all features used by either the
consistent or contrastive classifiers (or both). ♠
marks features used by the consistent classifier;
♣ marks features used by the contrastive classi-
fier. Some of the features use auxiliary definitions;
those are listed in Section A.2. Some of the fea-
tures are normalized as follows: let n be a normal-
ization factor and f be the feature value. Then

fn =

{
f
n f ≤ n
1 otherwise

In such cases, we mark the feature with Nn.

A.1 Features

1. CQ TOT COUNT: count of DC, EC co-
occurrences in CQ queries. ♣N5000

https://doi.org/10.1007/978-3-642-40722-2_19
https://doi.org/10.1007/978-3-642-40722-2_19
http://www.aclweb.org/anthology/P16-1226
http://www.aclweb.org/anthology/P16-1226
http://www.stanford.edu/~jurafsky/paper887.pdf
http://www.stanford.edu/~jurafsky/paper887.pdf
https://doi.org/10.3115/1220175.1220276
https://doi.org/10.3115/1220175.1220276
http://www.aclweb.org/anthology/D/D09/D09-1018
http://www.aclweb.org/anthology/D/D09/D09-1018
http://www.aclweb.org/anthology/D/D09/D09-1018
https://doi.org/10.18653/v1/N18-5005
https://doi.org/10.18653/v1/N18-5005
http://aclweb.org/anthology/D18-1402
http://aclweb.org/anthology/D18-1402
http://www.aclweb.org/anthology/D12-1042
http://www.aclweb.org/anthology/D12-1042
https://doi.org/10.18653/v1/N16-1065
https://doi.org/10.18653/v1/N16-1065
https://doi.org/10.18653/v1/W17-5106
https://doi.org/10.18653/v1/W17-5106
http://www.aclweb.org/anthology/W03-1011.pdf
http://www.aclweb.org/anthology/W03-1011.pdf
https://doi.org/10.18653/v1/N16-1174
https://doi.org/10.18653/v1/N16-1174
https://doi.org/10.18653/v1/D15-1203
https://doi.org/10.18653/v1/D15-1203


988

2. CQ VS COUNT: count of DC, EC co-
occurrences in CQ queries matching a VS ex-
traction pattern (see Section B.2). ♣N5000

3. CQ PMI: pointwise mutual information
(PMI) of DC, EC over all CQ queries. ♣
p(DC) = CQ DC COUNT/CQ SIZE
p(EC) = CQ EC COUNT/CQ SIZE
p(DCEC) = CQ TOT COUNT/CQ SIZE

pmi(DC, EC) =


log p(DCEC)p(DC)·p(EC) p(DC) > 0

p(EC) > 0

0 otherwise
(1)

4. CQ VS PMI: PMI of DC, EC over CQ
queries matching some VS extraction pattern.
Same definition as Equation (1), but with the
following quantities:
p(DC) = CQ DC VS COUNT/CQ VS SIZE
p(EC) = CQ EC VS COUNT/CQ VS SIZE
p(DCEC) = CQ VS COUNT/CQ VS SIZE
♠♣

5. SIMILARITY R: semantic relatedness sR
between DC and EC (defined in Sec-
tion 3.2). ♠♣

6. SIMILARITY D: distributional similarity
sD between DC and EC (defined in Sec-
tion 3.2). ♠♣

7. EC DC SENTIMENT: the product of DC
concept sentiment and EC concept senti-
ment. ♠♣

8. CATEGORIES SHARED COUNT: count
of Wikipedia categories shared by DC and
EC. ♠

9. CATEGORIES CONTAINED COUNT:
count of occurrences of DC in EC category
names and EC in DC category names, up to
two category levels; occurrences are counted
after stemming both the category and the
concept. ♠♣

10. OUTLINKS COUNT: count of shared
Wikipedia outlinks of DC and EC. ♠♣N30

11. DC HYPERNYM EC: a binary feature in-
dicating if DC is a hypernym of EC in
WordNet. ♠♣

12. DC HYPONYM EC: a binary feature indi-
cating if DC is a hyponym of EC in Word-
Net. ♠♣

13. DC SYNONYM EC: a binary feature indi-
cating if DC is a synonym of EC in Word-
Net. ♠♣

14. DC COHYPONYM EC: a binary feature
indicating if DC is a co-hyponym of EC in
WordNet. ♠♣

15. CN TOT ALL: count of DC, EC co-
occurrence in CN sentences, all surface
forms, with distance of at most 10 tokens.
♠♣N5000

16. CN VS ALL: count of DC, EC co-
occurrence in CN sentences matching a VS
classification pattern (see Section B.3.1), all
surface forms. ♠♣N100

17. CN VS EXACT MATCH: count of DC,
EC co-occurrence in CN sentences match-
ing a VS classification pattern, exact surface
forms. ♣N100

18. CN DEBATE ALL: count of DC, EC co-
occurrence in CN sentences matching a
DEBATE classification pattern (see Sec-
tion B.3.2), all surface forms. ♣N100

19. CN DEBATE EXACT MATCH: count of
DC, EC co-occurrence in CN sentences
matching a DEBATE classification pattern,
exact surface forms. ♣N10

20. CN AND ALL: count of DC, EC co-
occurrence in CN sentences matching
an AND classification pattern (see Sec-
tion B.3.3), all surface forms. ♣N100

21. CN PATTERN PROB ALL: probability of
VS classification pattern, all surface forms.
♠♣

p(DC, EC) =


log CN VS ALLCN TOT ALL CN VS ALL > 0

CN TOT ALL > 0

−10 otherwise

22. CN PATTERN PROB EXACT MATCH:
Probability of VS classification pattern, exact
match. ♣



989

23. CN VS AND RELATION: A feature based
on the ratio between the frequencies of the
VS classification pattern and the AND classi-
fication pattern. ♣

p(DC, EC) =

{
log CN VS ALL+1CN TOT ALL+1 CN VS ALL > 0

−10 otherwise

24. CN FREQ RATIO: The frequency ratio
measure, defined in Section 3.2. ♠

A.2 Auxiliary Definitions
1. CQ SIZE: total count of CQ queries.

2. CQ VS SIZE: total count of CQ queries
matching a VS pattern.

3. CQ DC COUNT: count of DC occurrences
in CQ queries.

4. CQ EC COUNT: count of EC occurrences
in CQ queries.

5. CQ DC VS COUNT: count of DC occur-
rences in CQ queries matching a VS pattern.

6. CQ EC VS COUNT: count of EC occur-
rences in CQ queries matching a VS pattern.

7. CN TOT DC: count of DC occurrences in
CN sentences, all surface forms.

8. CN TOT EC: count of EC occurrences in
CN sentences, all surface forms.

B Patterns

In the patterns listed in this section, (X ,Y ) stand
for either (DC,EC) or (EC,DC), ‘*’ matches
any number of non-concept tokens, and [] indi-
cates optional characters.

B.1 Patterns Used for Consistent Candidate
Extraction

1. X is a Y

2. X is an Y

3. X is a kind of Y

4. X is a form of Y

5. X is an example of Y

6. X is a special case of Y

7. X or other Y

8. X or other types of Y

9. X or other kinds of Y

10. X or another type of Y

11. X and other Y

12. X and other types of Y

13. X and other kinds of Y

14. Y such as X

15. Y including X

16. Y e.g. X

B.2 VS Patterns Used for Contrastive
Candidate Extraction

1. X * vs[.] * Y

2. X * versus * Y

3. X * preferable to * Y

4. X * instead of * Y

5. X * in contrast to * Y

6. X * better than * Y

7. X * healthier than * Y

8. X * safer than * Y

9. X * cleaner than * Y

10. difference[s] between * X * and * Y

B.3 Patterns Used for Candidate
Classification

B.3.1 VS Patterns
1. X v[.] Y

2. X vs[.] Y

3. X versus Y

4. X instead of Y

5. X in contrast to Y

6. X [is|are] preferable to Y

7. X [is|are] better than Y

8. X [is|are] healthier than Y

9. X [is|are] safer than Y

10. X [is|are] cleaner than Y



990

B.3.2 DEBATE Patterns
1. X 〈VS〉 Y (debate|controversy); 〈VS〉 stands

for any of the connectors listed in Sec-
tion B.3.1.

2. X (and|or) Y (debate|controversy)

B.3.3 AND Patterns
1. both X (and|or) Y

2. including X (and|or) Y

3. such as X (and|or) Y

4. for example X (and|or) Y

5. for instance X (and|or) Y


