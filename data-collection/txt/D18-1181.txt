











































Auto-Encoding Dictionary Definitions into Consistent Word Embeddings


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1522–1532
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1522

Auto-Encoding Dictionary Definitions into Consistent Word Embeddings

Tom Bosc
Mila, Université de Montréal
tom.bosc@umontreal.ca

Pascal Vincent
Mila, Université de Montréal, CIFAR,

Facebook AI Research
pascal.vincent@umontreal.ca

Abstract

Monolingual dictionaries are widespread and
semantically rich resources. This paper
presents a simple model that learns to com-
pute word embeddings by processing dictio-
nary definitions and trying to reconstruct them.
It exploits the inherent recursivity of dictio-
naries by encouraging consistency between the
representations it uses as inputs and the repre-
sentations it produces as outputs. The result-
ing embeddings are shown to capture seman-
tic similarity better than regular distributional
methods and other dictionary-based methods.
In addition, the method shows strong perfor-
mance when trained exclusively on dictionary
data and generalizes in one shot.

1 Introduction

Dense, low-dimensional, real-valued vector repre-
sentations of words known as word embeddings
have proven very useful for NLP tasks (Turian
et al., 2010). They can be learned as a by-product
of solving a particular task (Collobert et al., 2011).
Alternatively, one can pretrain generic embed-
dings based on co-occurrence counts or using an
unsupervised criterion such as predicting nearby
words (Bengio et al., 2003; Mikolov et al., 2013).
These methods implicitly rely on the distributional
hypothesis (Harris, 1954; Sahlgren, 2008), which
states that words that occur in similar contexts tend
to have similar meanings.

It is common to study the relationships captured
by word representations in terms of either simi-
larity or relatedness (Hill et al., 2016). “Coffee”
is related to “cup” as coffee is a beverage often
drunk in a cup, but “coffee” is not similar to “cup”
in that coffee is a beverage and cup is a container.
Methods relying on the distributional hypothesis
often capture relatedness very well, reaching hu-
man performance, but fare worse in capturing sim-

ilarity and especially in distinguishing it from re-
latedness (Hill et al., 2016).

It is useful to specialize word embeddings to fo-
cus on either relation in order to improve perfor-
mance on specific downstream tasks. For instance,
Kiela et al. (2015) report that improvements on
relatedness benchmarks also yield improvements
on document classification. In the other direction,
embeddings learned by neural machine translation
models capture similarity better than distributional
unsupervised objectives (Hill et al., 2014).

There is a wealth of methods that postprocess
embeddings to improve or specialize them, such
as retrofitting (Faruqui et al., 2014). On similarity
benchmarks, they are able to reach correlation co-
efficients close to inter-annotator agreements. But
these methods rely on additional resources such
as paraphrase databases (Wieting et al., 2016) or
graphs of lexical relations such as synonymy, hy-
pernymy, and their converse (Mrkšić et al., 2017).

Rather than relying on such curated lexical re-
sources that are not readily available for the ma-
jority of languages, we propose a method capa-
ble of improving embeddings by leveraging the
more common resource of monolingual dictionar-
ies.1 Lexical databases such as WordNet (Fell-
baum, 1998) are often built from dictionary defi-
nitions, as was proposed earlier by Amsler (1980).
We propose to bypass the process of explicitly
building a lexical database – during which infor-
mation is structured but information is also lost –
and instead directly use its detailed source: dictio-
nary definitions. The goal is to obtain better rep-
resentations for more languages with less effort.

The ability to process new definitions is also de-
sirable for future natural language understanding
systems. In a dialogue, a human might want to ex-
plain a new term by explaining it in his own words,

1See Appendix A for a list of online monolingual dictio-
naries.



1523

and the chatbot should understand it. Similarly,
question-answering systems should also be able to
grasp definitions of technical terms that often oc-
cur in scientific writing.

We expect the embedding of a word to rep-
resent its meaning compactly. For interpretabil-
ity purposes, it would be desirable to be able to
generate a definition from that embedding, as a
way to verify what information it captured. Case
in point: to analyse word embeddings, Noraset
et al. (2017) used RNNs to produce definitions
from pretrained embeddings, manually annotated
the errors in the generated definitions, and found
out that more than half of the wrong definitions
fit either the antonyms of the defined words, their
hypernyms, or related but different words. This
points in the same direction as the results of in-
trinsic evaluations of word embeddings: lexical
relationships such as lexical entailment, similar-
ity and relatedness are conflated in these embed-
dings. It also suggests a new criterion for evalu-
ating word representations, or even learning them:
they should contain the necessary information to
reproduce their definition (to some degree). In this
work, we propose a simple model that exploits this
criterion. The model consists of a definition au-
toencoder: an LSTM processes the definition of a
word to yield its corresponding word embedding.
Given this embedding, the decoder attempts to re-
construct the bag-of-words representation of the
definition. Importantly, to address and leverage
the recursive nature of dictionaries – the fact that
words that are used inside a definition have their
own associated definition – we train this model
with a consistency penalty that ensures proxim-
ity of the embeddings produced by the LSTM and
those that are used by the LSTM.

Our approach is self-contained: it yields good
representations when trained on nothing but dic-
tionary data. Alternatively, it can also leverage
existing word embeddings and is then especially
apt at specializing them for the similarity relation.
Finally, it is also extremely data-efficient, as it per-
mits to create representations of new words in one
shot from a short definition.

2 Model

2.1 Setting and motivation

We suppose that we have access to a dictionary
that maps words to one or several definitions. Def-
initions themselves are sequences of words. Our

hidden states
of LSTM

definition

input 
embeddings

output 
embeddings

definition 
embedding

min  reconstruction 
error

min consistency 
penalty

simple
language 

model

provide evidence forprove:

Figure 1: Overview of the CPAE model.

training criterion is built on the following princi-
ple: we want the model to be able to recover the
definition from which the representation was built.
This objective should produce similar embeddings
for words which have similar definitions. Our hy-
pothesis is that this will help capture semantic sim-
ilarity, as opposed to relatedness. Reusing the pre-
vious example, “coffee” and “cup” should get dif-
ferent representations in virtue of having very dif-
ferent definitions, while “coffee” and “tea” should
get similar representations as they are both defined
as beverages and plants.

We chose to compute a single embedding per
word in order to avoid having to disambiguate
word senses. Indeed, word sense disambiguation
remains a challenging open problem with mixed
success on downstream task applications (Navigli,
2009). Also, recent papers have shown that a sin-
gle word vector can capture polysemy and that
having several vectors per word is not strictly nec-
essary (Li and Jurafsky, 2015) (Yaghoobzadeh and
Schütze, 2016). Thus, when a word has several
definitions, we concatenate them to produce a sin-
gle embedding.

2.2 Autoencoder model

Let VD be the set of all words that are used in
definitions and VK the set of all words that are
defined. We let w ∈ VK be a word and Dw =
(Dw,1, . . . , Dw,T ) be its definition, where Dw,t is
the index of a word in vocabulary VD. We en-
code such a definitionDw by processing it with an



1524

LSTM (Hochreiter and Schmidhuber, 1997).
The LSTM is parameterized by Ω and a matrix

E of size |VD| ×m, whose ith row Ei contains an
m-dimensional input embedding for the ith word
of VD. These input embeddings can either be
learned by the model or be fixed to a pretrained
embedding. The last hidden state computed by
this LSTM is then transformed linearly to yield an
m-dimensional definition embedding h. Thus the
encoder whose parameters are θ = {E,Ω,W, b}
computes this embedding h as

h = fθ(Dw) = W LSTME,Ω(Dw) + b.

The subsequent decoder can be seen as a condi-
tional language model trained by maximum likeli-
hood to regenerate definition Dw given definition
embedding h = fθ(Dw). We use a simple con-
ditional unigram model with a linear parametriza-
tion θ′ = {E′, b′} where E′ is a |VD| ×m matrix
and b′ is a bias vector. 2

We maximize the log-probability of definition
Dw under that model:

log pθ′(Dw|h) =
∑
t

log pθ′(Dw,t|h)

=
∑
t

log
e

〈
E′Dw,t

,h
〉

+b′Dw,t∑
k e
〈E′k,h〉+b′k

(1)

where 〈, 〉 denotes an ordinary dot product. We call
E′ the output embedding matrix. The basic au-
toencoder training objective to minimize over the
dictionary can then be formulated as

Jr(θ
′, θ) = −

∑
w∈VK

log pθ′(Dw|fθ(Dw)).

2.3 Consistency penalty
We introduced 3 different embeddings: a) defini-
tion embeddings h, produced by the definition en-
coder, are the embeddings we are ultimately in-
terested in computing; b) input embeddings E are
used by the encoder as inputs; c) output embed-
dings E′ are compared to definition embeddings
to yield a probability distribution over the words
in the definition. We propose a soft weight-tying

2We have tried using a LSTM decoder but it didn’t yield
good representations. It might overfit because the set of dic-
tionary definitions is small. Also, using teacher forcing, we
condition on ground-truth words, making it easier to predict
the next words. More work is needed to address these issues.

scheme that brings the input embeddings closer
to the definition embeddings. We call this term
a consistency penalty because its goal is to to en-
sure that the embeddings used by the encoder (in-
put embeddings) and the embeddings produced by
the encoder (definition embeddings) are consistent
with each other. It is implemented as

Jp(θ) =
∑

w∈VD∩VK
d(Ew, fθ(Dw))

where d is a distance. In our experiments, we
choose d to be the Euclidian distance. The penalty
is only applied to some words because VD 6= VK.
Indeed, some words are defined but are not used in
definitions and some words are used in definitions
but not defined. In particular, inflected words are
not defined. To balance the two terms, we intro-
duce two hyperparameters λ, α ≥ 0 and the com-
plete objective is

J(θ′, θ) = αJr(θ
′, θ) + λJp(θ).

We call the model CPAE, for Consistency Pe-
nalized AutoEncoder when α > 0 and λ > 0 (see
Figure 1).3

The consistency penalty is a cheap proxy for
dealing with the circularity found in dictionary
definitions. We want the embeddings of the words
in definitions to be compositionally built from
their definition as well. The recursive process of
fetching definitions of words in definitions does
not terminate, because all words are defined using
other words. To counter that, our model uses input
embeddings that are brought closer to definition
embeddings and vice versa in an asynchronous
manner.

Moreover, if λ is chosen large enough, then
Ew ≈ fθ(Dw) after optimisation. This means that
the definition embedding for w is close enough
to the corresponding input embedding to be used
by the encoder for producing other definition em-
beddings for other words. In that case, the model
could enrich its vocabulary by computing embed-
dings for new words and consistently reusing them
as inputs for defining other words.

Finally, the consistency penalty can be used to
leverage pretrained embeddings and bootstrap the
learning process. For that purpose, the encoder’s
input embeddingsE can be fixed to pretrained em-
beddings. These provide targets to the encoder but

3Our implementation is available at
https://github.com/tombosc/cpae

https://github.com/tombosc/cpae


1525

also helps the encoder to produce better definition
embeddings in virtue of using input embeddings
that already contain meaningful information.

To summarize, the consistency penalty has sev-
eral motivations. Firstly, it deals with the fact that
the recursive process of building representation of
words out of definitions does not terminate. Sec-
ondly, it is a way to enrich the vocabulary with
new words dynamically. Finally, it is a way to in-
tegrate prior knowledge in the form of pretrained
embeddings.

In order to study the two terms of the objec-
tive in isolation, we introduce two special cases.
When λ = 0 and α > 0, the model reduces to AE
for Autoencoder. When α = 0 and λ > 0, we
retrieve Hill’s model, as presented by Hill et al.
(2015).4 Hill’s model is simply a recurrent en-
coder that uses pretrained embeddings as targets
so it only makes sense in the case we use fixed
pretrained embeddings.

3 Related work

3.1 Extracting lexical knowledge from
dictionaries

There is a long history of attempts to extract and
structure the knowledge contained in dictionaries.
Amsler (1980) studies the possibility of automat-
ically building taxonomies out of dictionaries, re-
lying on the syntactic and lexical regularities that
definitions display. One relation is particularly
straightforward to identify: it is the is a relation
that translates to hypernymy. Dictionary defini-
tions often contain a genus which is the hyper-
nym of the defined word, as well as a differentia
which differentiates the hypernym from the de-
fined word. For example, the word “hostage” is
defined as “a prisoner who is held by one party
to insure that another party will meet specified
terms”, where “prisoner” is the genus and the rest
is the differentia.

To extract such relations, early works by
Chodorow et al. (1985) and Calzolari (1984) use
string matching heuristics. Binot and Jensen
(1987) operate at the syntactic parse level to detect

4It is not exactly their model as we use Euclidian dis-
tance instead of the cosine distance or the ranking loss. They
also explore several variants where the input embeddings are
learned, which we didn’t find to produce any improvement.
We haven’t experimented with the ranking loss, but the cosine
distance does not seem to improve over Euclidian. Finally,
they also use a simple encoder that averages word vectors,
which we found to be inferior.

these relations. Whether based on the string rep-
resentation or the parse tree of a definition, these
rule-based systems have helped to create large lex-
ical databases. We aim to reduce the manual labor
involved in designing the rules and directly obtain-
ing representations from raw definitions.

3.2 Improving word embeddings using
lexical resources

Postprocessing methods for word embeddings use
lexical resources to improve already trained word
embeddings irrespective of how they were ob-
tained. When it is used with fixed pretrained em-
beddings, our method can be seen as a postpro-
cessing method.

Postprocessing methods typically have two
terms for trading off conservation of distributional
information that is brought by the original vec-
tors with the new information from lexical re-
sources. There are two main ways to preserve dis-
tributional information: Attract-Repel (Vulić and
Mrkšić, 2017), retrofitting (Mrkšić et al., 2017)
and our method control the distance between the
original vector and the postprocessed vector so
that the new vector does not drift too far away from
the original vector. Counter-Fitting (Mrkšić et al.,
2016) and dict2vec (Tissier et al., 2017) ensure
that the neighbourhood of a vector in the original
space is roughly the same as the neighbourhood in
the new space.

Finally, methods differ by the nature of the
lexical resources they use. To our knowledge,
dict2vec is the only technique that uses dictio-
naries. Other postprocessing methods use vari-
ous data from WordNet: sets of synonyms and
sometimes antonyms, hypernyms, and hyponyms.
For instance, Lexical Entailment Attract-Repel
(LEAR) uses all of these (Vulić and Mrkšić, 2017).
Other methods rely on paraphrase databases (Wi-
eting et al., 2016).

3.3 Dictionaries and word embeddings
We now turn to the most relevant works that in-
volve dictionaries and word embeddings.

Dict2vec (Tissier et al., 2017) combines the
word2vec skip-gram objective (predicting all the
words that appear in the context of a target word)
with a cost for predicting related words. These re-
lated words either form strong pairs or weak pairs
with the target word. Strong pairs have a greater
influence in the cost. They are pairs of words that
are in the neighbourhood of the target word in the



1526

original embedding, as well as pairs of words for
which the definitions make reference to each other.
Weak pairs are pairs of words where only one
word appears in the definition of the other. Un-
like dict2vec, our method can be used as either a
standalone or a postprocessing method (when used
with pretrained embeddings). It also focuses on
handling and leveraging the recursivity found in
dictionary definitions with the consistency penalty
whereas dict2vec ignores this aspect of the struc-
ture of dictionaries.

Besides dict2vec, Hill et al. (2015) train neu-
ral language models to predict a pretrained word
embedding given a definition. Their goal was
to learn a general-purpose sentence encoder use-
ful for downstream tasks. Noraset et al. (2017)
propose the task of generating definitions based
on word embeddings for interpretability purposes.
Our model unifies these two approaches into an
autoencoder. However, we have a different goal:
that of creating or improving word representa-
tions. Their methods assume that pretrained em-
beddings are available to provide either targets or
inputs, whereas our model is unsupervised, and
the use of pretrained embeddings is optional.

Bahdanau et al. (2017) present a related model
that produces embeddings from definitions such
that it improves performance on a downstream
task. By contrast our approach is used either
stand-alone or as as a postprocessing step, to pro-
duce general-purpose embeddings at a lesser com-
putational cost. The core novelty is the way we
leverage the recursive structure of dictionaries.

Finally, Herbelot and Baroni (2017) also aim at
learning representations for word embeddings in
a few shots. The method consists of fine-tuning
word2vec hyperparameters and can learn in one or
several passes, but it is not specifically designed to
handle dictionary definitions.

4 Experiments

4.1 Setup

We experiment on English to benefit from the
many evaluation benchmarks available. The dic-
tionary we use is that of WordNet (Fellbaum,
1998). WordNet contains graphs of linguistic re-
lations such as synonymy, antonymy, hyponymy,
etc. but also definitions. We emphasize that our
method trains exclusively on the definitions and is
thus applicable to any electronic dictionary.

However, in order to evaluate the quality of em-

beddings on unseen definitions, WordNet relations
comes in handy: we use the sets of synonyms to
split the dictionary into a train set and a test set,
as explained in Section 7. Moreover, WordNet has
a wide coverage and high quality, so we do not
need to aggregate several dictionaries as done by
Tissier et al. (2017). Finally, WordNet is explic-
itly made available for research purposes, there-
fore we avoid technical and legal difficulties asso-
ciated with crawling proprietary online dictionar-
ies.

We do not include part of speech tags that go
with definitions. WordNet does not contain func-
tion words but contains homonyms of function
words. We filter these out.

4.2 Similarity and relatedness benchmarks

Evaluating the learned representations is a com-
plex issue (Faruqui et al., 2016). Indeed, different
evaluation methods yield different rankings of em-
beddings: there is no single embedding that out-
performs others on all tasks (Schnabel et al., 2015)
and thus no single best evaluation method.

We focus on intrinsic evaluation methods. In
particular, we study how different models trade
off similarity and relatedness. We use benchmarks
which consist of pairs of words scored according
to some criteria. They vary in terms of annotation
guidelines, number of annotators, selection of the
words, etc. To evaluate our embeddings, we score
each pair by computing the cosine similarity be-
tween the corresponding word vectors. Then the
predicted scores and the ground truth are ranked
and the correlation between the ranks is measured
by Spearman’s ρ. We leave aside analogy predic-
tion benchmarks as they suffer from many prob-
lems (Linzen, 2016; Rogers et al., 2017).

We adopt one of the methods proposed by
Faruqui et al. (2016) and use separate datasets for
model selection. We choose the development set
to be the development set of SimVerb3500 (Gerz
et al., 2016) and MEN (Bruni et al., 2014), the only
benchmarks with a standard train/test split.

We justified our emphasis on the similarity re-
lation in Section 1: capturing this relation remains
a challenge, and we hypothesize that dictionary
data should improve representations in that re-
spect. The model selection procedure reflects that
we want embeddings specialized in similarity. To
do that, we set the validation loss as a weighted
mean which weights SimVerb twice as MEN.



1527

4.3 Baselines

The objective function presented in section 2 gives
us 3 different models: CPAE, AE, and Hill’s
model. The objective of CPAE comprises the sum
of the objective of Hill’s model and of AE. We
compare the CPAE model to both of these to eval-
uate the individual contribution of the two terms to
the performance. In addition, when we use exter-
nal corpora to pretrain embeddings, we compare
these models to dict2vec and retrofitting. The hy-
perparameter search is described in Appendix C.

The test benchmarks for the similarity relation
includes SimLex999 (Hill et al., 2016) and more
particularly SimLex333, a challenging subset of
SimLex999 which contains only highly related
pairs but in which similarity scores vary a lot.
For relatedness, we use MEN (Bruni et al., 2014),
RG (Rubenstein and Goodenough, 1965), WS353
(Finkelstein et al., 2001), SCWS (Huang et al.,
2012), and MTurk (Radinsky et al., 2011; Halawi
et al., 2012). The evaluation is carried out by a
modified version of the Word Embeddings Bench-
marks project.5 Conveniently, all these bench-
marks contain mostly lemmas, so we do not suffer
too much from the problem of missing words.6

5 Results in the dictionary-only setting

In the first evaluation round, we train models only
using a single monolingual dictionary. This allows
us to check our hypothesis that dictionaries con-
tain information for capturing the similarity rela-
tion between words.

Our baselines are regular distributional mod-
els: GloVe (Pennington et al., 2014) and word2vec
(Mikolov et al., 2013). They are trained on the
concatenation of defined words with their defini-
tions.

Such a formatting introduces spurious co-
occurrences that do not otherwise appear in free
text. But these baselines are not designed for
dictionaries and cannot deal with their particular
structure.

We compare these models to the autoencoder
model without (AE) and with (CPAE) the consis-
tency penalty. In this setting, we cannot use Hill’s

5Original project available at
https://github.com/kudkudak/
word-embeddings-benchmarks, modified version
distributed with our code.

6Missing words are not removed from the dataset, but they
are assigned a null vector.

model as it requires pretrained embeddings as tar-
gets. We also trained an additional CPAE model
with pretrained word2vec embeddings trained on
the concatenated definitions. The results are pre-
sented in Table 1.

GloVe is outperformed by word2vec by a large
margin so we ignore this model in later experi-
ments. Word2vec captures more relatedness than
CPAE (+10.7 on MEN-t, +13.5 on MT, +13.2 on
WS353) but less similarity than CPAE. The differ-
ence in the nature of the relations captured is ex-
emplified by the scores on SimLex333. This sub-
set of SimLex999 focuses on pairs of words that
are very related but that can be either similar or
dissimilar. On this subset, CPAE fares better than
word2vec (+13.1).

The consistency penalty improves performance
on every dataset. This penalty provides targets
to the encoder, but these targets are themselves
learned and change during the learning process.
The exact dynamics of the system are unknown. It
can be seen as a regularizer because it puts strong
weight-sharing constraints on both types of em-
beddings. It also resembles bootstrapping in re-
inforcement learning, which consists of building
estimates of values functions on top of over esti-
mates (Sutton and Barto, 1998).

The last model is the CPAE model that uses
the word2vec embeddings pretrained on the dic-
tionary data. This combination not only equals
other models on some benchmarks but outper-
forms them, sometimes by a large margin (+6.3
on SimLex999 and +7.5 on SimVerb3500 com-
pared to CPAE, +6.1 on SCWS, +5.4 on MT
compared to word2vec). Thus, the two kinds of
algorithms are complementary through the differ-
ent relationships that they capture best. The pre-
training helps in two different ways, by providing
quality input embeddings and targets to the en-
coder. The pretrained word2vec targets are already
remarkably good. That is why the chosen con-
sistency penalty coefficient selected is very high
(λ = 64). The model can pay a small cost and
deviate from the targets in order to encode infor-
mation about the definitions.

To sum up, dictionary data contains a lot of
data relevant to modeling the similarity relation-
ship. Autoencoder based models learn different
relationships than regular distributional methods.
The consistency penalty is a very helpful prior and
regularizer for dictionary data, as it always helps,

https://github.com/kudkudak/word-embeddings-benchmarks
https://github.com/kudkudak/word-embeddings-benchmarks


1528

Development Similarity Relatedness
SV-d MENd SL999 SL333 SV-t RG SCWS MENt MT 353

GloVe 12.0 54.8 19.8 -9.1 7.8 57.5 46.8 57.0 49.4 44.4
word2vec 35.2 62.3 34.5 16.0 36.4 65.7 54.5 59.9 56.1 61.9
AE 34.9 42.7 35.6 26.8 32.5 64.8 50.2 42.2 38.6 41.4
CPAE (λ = 8) 42.8 48.5 39.5 29.1 34.8 67.1 54.3 49.2 42.6 48.7
CPAE-P (λ = 64) 44.1 65.1 45.8 30.9 42.3 72.0 60.4 63.8 61.5 61.3

Table 1: Positive effect of the consistency penalty and word2vec pretraining. Spearman’s correlation coefficient
ρ × 100 on benchmarks. Without pretraining, autoencoders (AE and CPAE) improve on similarity benchmarks
while capturing less relatedness than distributional methods. The consistency penalty (CPAE) helps even without
pretrained targets. Our method, combined with pretrained embeddings on the same dictionary data (CPAE-P),
significantly improves on every benchmark.

regardless of what relationship we focus on. Fi-
nally, our model can drastically improve embed-
dings that were trained on the same data but with
a different algorithm.

6 Improving pretrained embeddings

We have seen that CPAE with pretraining is very
efficient. But does this result generalizes to other
kind of pretraining data? To answer this ques-
tion, we experiment using embeddings pretrained
on the first 50 million tokens of a Wikipedia dump,
as well as the entire Wikipedia dump. We com-
pare our method to existing postprocessing meth-
ods such as dict2vec and retrofitting, which also
aims at improving embeddings with external lexi-
cal resources.

Retrofitting, which operates on graphs, is not
tailored for dictionary data, which consists in pairs
of words along with their definitions. We build a
graph where nodes are words and edges between
nodes correspond to the presence of one of the
words into the definition of another. Obviously,
we lose word order in the process.

The results for the small corpus are presented
in Table 2. By comparing Table 2 with Table
1, we see that word2vec does worse on similar-
ity than when trained on dictionary data, but bet-
ter on relatedness. Both dict2vec and retrofitting
improve with regards to word2vec on similarity
benchmarks and seem roughly on par. However,
dict2vec fails to improve on relatedness bench-
marks, whereas retrofitting sometimes improves
(as in RG, MEN, and MT), sometimes equals
(SCWS) and does worse (353).

We do an ablation study by comparing Hill’s
model and AE with CPAE. Recall that Hill’s
model lacks the reconstruction cost while AE
lacks the consistency penalty. Firstly, CPAE al-

ways improves over AE. Thus, we confirm the re-
sults of the previous section on the importance of
the consistency penalty. In that setting, it is more
obvious why this penalty helps, as it now pro-
vides pretrained targets to the encoder. Secondly,
CPAE improves over Hill on all similarity bench-
marks by a large margin (+12.2 on SL999, +13.7
on SL333, +16.1 on SV3500). It is sometimes
slightly worse on relatedness benchmarks (−3.3
on MEN-t, −5.6 on MT), other times better or
equal. We conclude that both terms of the CPAE
objective matter.

We see identical trends when using the full
Wikipedia dump. As expected, CPAE can still
improve over even higher quality embeddings by
roughly the same margins. The results are pre-
sented in Appendix D.

Remarkably, the best model among all our ex-
periments is CPAE in Table 1 and uses only the
dictionary data. This supports our hypothesis
that dictionaries contain similarity-specific infor-
mation.

7 Generalisation on unseen definitions

A model that uses definitions to produce word
representations is appealing because it could be
extremely data-efficient. Unlike regular distribu-
tional methods which iteratively refine their repre-
sentation as occurrences accumulate, such a model
could output a representation in one shot. We now
evaluate CPAE in a setting where some definitions
are not seen during training.

The dictionary is split into train, validation (for
early stopping) and test splits. The algorithm for
splitting the dictionary puts words in batches. It
ensures two things: firstly, that words which share
at least one definition are in the same batch, and
secondly, that each word in a batch is associated



1529

Development Similarity Relatedness
SV-d MEN-d SL999 SL333 SV-t RG SCWS MEN-t MT 353

word2vec 21.7 71.1 33.2 6.9 21.2 68.5 65.8 71.5 61.1 65.3
retrofitting 28.5 71.6 36.9 14.1 26.1 78.9 65.7 74.4 62.8 60.7
dict2vec 26.3 63.5 36.2 15.5 22.0 69.2 63.8 63.9 54.9 60.8
Hill 26.9 63.3 27.7 12.9 21.7 72.9 58.4 64.1 54.0 52.3
AE 33.5 47.0 33.1 20.4 32.5 66.0 52.0 46.4 40.2 43.7
CPAE (λ = 4) 39.5 60.8 39.9 26.6 37.8 69.7 59.2 60.8 48.4 55.6

Table 2: Improving pretrained embeddings computed on a small corpus. Spearman’s correlation coefficient
ρ × 100 on benchmarks. All methods use pretrained embeddings. All methods (except maybe Hill) manage
to improve the embeddings. Retrofitting outperforms dict2vec and efficiently specializes for relatedness. CPAE
outperforms AE and allows to trade off relatedness for similarity.

Similarity Relatedness
999 333 SV-t 353 MEN SCWS MT

All 36.5 27.7 36.9 43.7 48.8 53.2 41.5
Train 40.0 28.5 36.7 46.9 53.4 57.1 42.9
Test 27.5 25.4 38.5 42.5 44.1 40.3 39.1

Table 3: One pass generalisation. Spearman’s correlation coefficient ρ×100 on benchmarks. The model is CPAE
(without pretrained embeddings). All: all pairs in the benchmarks. Train: pairs for which both words are in the
training or validation set. Test: pairs which contain at least one word in the test set. Correlation is lower for test
pairs but remains strong (ρ > 0.3): the model has good generalisation abilities.

with all its definitions. We can then group batches
to build the training and the test sets such that the
test set does not contain synonyms of words from
the other sets. We sort the batches by the num-
ber of distinct definitions they contain. We use
the largest batch returned by the algorithm as the
training set: it contains mostly frequent and pol-
ysemous words. The validation and the test sets,
on the contrary, contain many multiword expres-
sions, proper nouns, and rarer words. More details
are given in Appendix B.1.

We train CPAE only on the train split of the
dictionary, with randomly initialized input embed-
dings. Table 3 presents the same correlation co-
efficients as in the previous tables but also dis-
tinguishes between two subsets of the pairs: the
pairs for which all the definitions were seen during
training (train) and the pairs for which at least one
word was defined in the test set (test). Unfortu-
nately, there are not enough pairs of words which
both appear in the test set to be able to compute
significant correlations. On small-sized bench-
marks, correlation coefficients are sometimes not
significant so we do not report them (when p-value
> 0.01).

The scores of CPAE on the test pairs are quite
correlated with the ground truth: except on Sim-
Lex999 and SCWS, there is no drop in correlation

coefficients between the two sets. The scores of
Hill’s model follow similar trends, but are lower
on every benchmark so we do not report them.
This shows that recurrent encoders are able to
generalize and produce coherent embeddings as a
function of other embeddings in one pass.

8 Conclusion and future work

We have focused on capturing the similarity rela-
tion. It is a challenging task which we have pro-
posed to solve using dictionaries, as definitions
seem to encode the relevant kind of information.

We have presented an alternative for learning
word embeddings that uses dictionary definitions.
As a definition autoencoder, our approach is self-
contained, but it can alternatively be used to im-
prove pretrained embeddings, and includes Hill’s
model (Hill et al., 2015) as a special case.

In addition, our model leverages the inherent re-
cursivity of dictionaries via a consistency penalty,
which yields significant improvements over the
vanilla autoencoder.

Our method outperforms dict2vec and
retrofitting on similarity benchmarks by a
quite large margin. Unlike dict2vec, our method
can be used as a postprocessing method which
does not require going through the original



1530

pretraining corpus, it has fewer hyperparameters,
and it generalises to new words.

We see several directions for future work.
Firstly, more work is needed to evaluate the rep-

resentations on other languages and tasks.
Secondly, solving downstream tasks requires

representations for the inflected words as well. We
have set aside this issue by focusing on bench-
marks involving lemmas. To address it in future
work, we might want to split word representa-
tions into a lexical and a morphological part. With
such a split representation, we could postprocess
only the lexical component, and all the words,
whether inflected or not, would benefit from this.
This seems desirable for postprocessing methods
in general and would make them more suitable for
synthetic languages.

Thirdly, dictionary defines every sense of
words, so we could produce one embedding per
sense (Chen et al., 2014) (Iacobacci et al., 2015).
This requires potentially complicated modifica-
tions to our model as we would need to disam-
biguate senses inside each definition. However,
some class of words might benefit a lot from such
representations, for example words that can be
used as different parts of speech.

Lastly, a more speculative direction could be
to study iterative constructions of the set of em-
beddings. As our algorithm can generalize in one
shot, we could start the training with a small set of
words and their definitions and iteratively broaden
the vocabulary and refine the representations with-
out retraining the model. This could be useful in
discovering a set of semantic primes from which
one can define all the other words (Wierzbicka,
1996).

Acknowledgements

We thank NSERC and Facebook for financial sup-
port, Calcul Canada for computational resources,
Dzmitry Bahdanau and Stanisław Jastrzębski for
contributing to the code on which the implemen-
tation is based, the developers of Theano (Theano
Development Team, 2016), Blocks and Fuel (van
Merriënboer et al., 2015) as well as Laura Ball for
proofreading.



1531

References

Robert Alfred Amsler. 1980. The structure of the
merriam-webster pocket dictionary.

Dzmitry Bahdanau, Tom Bosc, Stanislaw Jastrzebski,
Edward Grefenstette, Pascal Vincent, and Yoshua
Bengio. 2017. Learning to compute word embed-
dings on the fly. CoRR, abs/1706.00286.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of machine learning research,
3(Feb):1137–1155.

Jean-Louis Binot and Karen Jensen. 1987. A semantic
expert using an online standard dictionary. In Pro-
ceedings of the 10th international joint conference
on Artificial intelligence-Volume 2, pages 709–714.
Morgan Kaufmann Publishers Inc.

Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. J. Artif. Intell.
Res.(JAIR), 49(2014):1–47.

Nicoletta Calzolari. 1984. Detecting patterns in a lex-
ical data base. In Proceedings of the 10th Interna-
tional Conference on Computational Linguistics and
22nd annual meeting on Association for Computa-
tional Linguistics, pages 170–173. Association for
Computational Linguistics.

Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.
A unified model for word sense representation and
disambiguation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1025–1035.

Martin S Chodorow, Roy J Byrd, and George E Hei-
dorn. 1985. Extracting semantic hierarchies from a
large on-line dictionary. In Proceedings of the 23rd
annual meeting on Association for Computational
Linguistics, pages 299–304. Association for Com-
putational Linguistics.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12(Aug):2493–2537.

Manaal Faruqui, Jesse Dodge, Sujay K. Jauhar, Chris
Dyer, Eduard Hovy, and Noah A. Smith. 2014.
Retrofitting Word Vectors to Semantic Lexicons.
arXiv:1411.4166 [cs]. ArXiv: 1411.4166.

Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi,
and Chris Dyer. 2016. Problems with evaluation of
word embeddings using word similarity tasks. arXiv
preprint arXiv:1605.02276.

Christiane Fellbaum. 1998. WordNet. Wiley Online
Library.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web, pages 406–
414. ACM.

Daniela Gerz, Ivan Vulić, Felix Hill, Roi Reichart, and
Anna Korhonen. 2016. Simverb-3500: A large-
scale evaluation set of verb similarity. arXiv preprint
arXiv:1608.00869.

Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and
Yehuda Koren. 2012. Large-scale learning of word
relatedness with constraints. In Proceedings of
the 18th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1406–
1414. ACM.

Zellig S Harris. 1954. Distributional structure. Word,
10(2-3):146–162.

Aurélie Herbelot and Marco Baroni. 2017. High-risk
learning: acquiring new word vectors from tiny data.
arXiv preprint arXiv:1707.06556.

Felix Hill, Kyunghyun Cho, Sébastien Jean, Coline
Devin, and Yoshua Bengio. 2014. Embedding word
similarity with neural machine translation. CoRR,
abs/1412.6448.

Felix Hill, Kyunghyun Cho, Anna Korhonen, and
Yoshua Bengio. 2015. Learning to understand
phrases by embedding the dictionary. arXiv preprint
arXiv:1504.00548.

Felix Hill, Roi Reichart, and Anna Korhonen. 2016.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Computational Linguis-
tics.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873–882. Asso-
ciation for Computational Linguistics.

Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2015. Sensembed: Learning sense
embeddings for word and relational similarity. In
Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), vol-
ume 1, pages 95–105.

Douwe Kiela, Felix Hill, and Stephen Clark. 2015.
Specializing word embeddings for similarity or re-
latedness. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2044–2048.



1532

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Jiwei Li and Dan Jurafsky. 2015. Do multi-sense em-
beddings improve natural language understanding?
EMNLP 2015.

Tal Linzen. 2016. Issues in evaluating seman-
tic spaces using word analogies. arXiv preprint
arXiv:1606.07736.

Bart van Merriënboer, Dzmitry Bahdanau, Vincent
Dumoulin, Dmitriy Serdyuk, David Warde-Farley,
Jan Chorowski, and Yoshua Bengio. 2015. Blocks
and fuel: Frameworks for deep learning. CoRR,
abs/1506.00619.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Nikola Mrkšić, Ivan Vulić, Diarmuid Ó Séaghdha, Ira
Leviant, Roi Reichart, Milica Gašić, Anna Korho-
nen, and Steve Young. 2017. Semantic special-
isation of distributional word vector spaces using
monolingual and cross-lingual constraints. arXiv
preprint arXiv:1706.00374.

Nikola Mrkšić, Diarmuid Ó Séaghdha, Blaise Thom-
son, Milica Gašić, Lina M. Rojas-Barahona, Pei-
Hao Su, David Vandyke, Tsung-Hsien Wen, and
Steve Young. 2016. Counter-fitting word vectors to
linguistic constraints. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 142–148, San Diego,
California. Association for Computational Linguis-
tics.

Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys (CSUR), 41(2):10.

Roberto Navigli and Simone Paolo Ponzetto. 2012.
Babelnet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217–
250.

Thanapon Noraset, Chen Liang, Larry Birnbaum, and
Doug Downey. 2017. Definition Modeling: Learn-
ing to Define Word Embeddings in Natural Lan-
guage. In AAAI, pages 3259–3266.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Olivier Picard, Alexandre Blondin-Massé, Stevan Har-
nad, Odile Marcotte, Guillaume Chicoisne, and Yas-
sine Gargouri. 2009. Hierarchies in dictionary defi-
nition space. arXiv preprint arXiv:0911.5703.

Kira Radinsky, Eugene Agichtein, Evgeniy
Gabrilovich, and Shaul Markovitch. 2011. A
word at a time: computing word relatedness using
temporal semantic analysis. In Proceedings of the
20th international conference on World wide web,
pages 337–346. ACM.

Anna Rogers, Aleksandr Drozd, and Bofang Li. 2017.
The (Too Many) Problems of Analogical Reasoning
with Word Vectors. In Proceedings of the 6th Joint
Conference on Lexical and Computational Seman-
tics (* SEM 2017), pages 135–148.

Herbert Rubenstein and John B Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627–633.

Magnus Sahlgren. 2008. The distributional hypothesis.
Italian journal of linguistics, 20(1):33–54.

Tobias Schnabel, Igor Labutov, David M Mimno, and
Thorsten Joachims. 2015. Evaluation methods for
unsupervised word embeddings.

Richard S Sutton and Andrew G Barto. 1998. Intro-
duction to reinforcement learning.

Theano Development Team. 2016. Theano: A Python
framework for fast computation of mathematical ex-
pressions. arXiv e-prints, abs/1605.02688.

Julien Tissier, Christophe Gravier, and Amaury
Habrard. 2017. Dict2vec: Learning Word Embed-
dings using Lexical Dictionaries. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2017), pages 254–263.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th annual meeting of the association for compu-
tational linguistics, pages 384–394. Association for
Computational Linguistics.

Ivan Vulić and Nikola Mrkšić. 2017. Specialising
word vectors for lexical entailment. arXiv preprint
arXiv:1710.06371.

Anna Wierzbicka. 1996. Semantics: Primes and uni-
versals: Primes and universals. Oxford University
Press, UK.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016. Charagram: Embedding words and
sentences via character n-grams. arXiv preprint
arXiv:1607.02789.

Yadollah Yaghoobzadeh and Hinrich Schütze. 2016.
Intrinsic subspace evaluation of word embedding
representations. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 236–
246, Berlin, Germany. Association for Computa-
tional Linguistics.


