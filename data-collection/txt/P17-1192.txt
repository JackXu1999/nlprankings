



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2099–2109
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1192

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2099–2109
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1192

Identifying 1950s American Jazz Musicians:
Fine-Grained IsA Extraction via Modifier Composition

Ellie Pavlick∗

University of Pennsylvania
3330 Walnut Street

Philadelphia, Pennsylvania 19104
epavlick@seas.upenn.edu

Marius Paşca
Google Inc.

1600 Amphitheatre Parkway
Mountain View, California 94043

mars@google.com

Abstract

We present a method for populating
fine-grained classes (e.g., “1950s Amer-
ican jazz musicians”) with instances
(e.g., Charles Mingus). While state-
of-the-art methods tend to treat class
labels as single lexical units, the pro-
posed method considers each of the in-
dividual modifiers in the class label rel-
ative to the head. An evaluation on the
task of reconstructing Wikipedia cate-
gory pages demonstrates a >10 point
increase in AUC, over a strong baseline
relying on widely-used Hearst patterns.

1 Introduction

The majority of approaches (Snow et al., 2006;
Shwartz et al., 2016) for extracting IsA rela-
tions from text rely on lexical patterns as the
primary signal of whether an instance belongs
to a class. For example, observing a pattern
like “X such as Y” is a strong indication that
Y (e.g., “Charles Mingus”) is an instance of
class X (e.g., “musician”) (Hearst, 1992).

Methods based on these “Hearst patterns”
assume that class labels can be treated as
atomic lexicalized units. This assumption has
several significant weakness. First, in order to
recognize an instance of a class, these pattern-
based methods require that the entire class la-
bel be observed verbatim in text. The require-
ment is reasonable for class labels containing
a single word, but in practice, there are many
possible fine-grained classes: not only “mu-
sicians” but also “1950s American jazz mu-
sicians”. The probability that a given label
will appear in its entirety within one of the
expected patterns is very low, even in large

∗Contributed during an internship at Google.

1950s American jazz musicians
. . . seminal musicians such as Charles Mingus
and George Russell. . .
. . .A virtuoso bassist and composer, Mingus ir-
revocably changed the face of jazz. . .
. . .Mingus truly was a product of America in
all its historic complexities. . .
. . .Mingus dominated the scene back in the
1950s and 1960s. . .

Figure 1: We extract instances of fine-grained
classes by considering each of the modifiers in
the class label individually. This allows us to
extract instances even when the full class label
never appears in text.

amounts of text. Second, when class labels
are treated as though they cannot be decom-
posed, every class label must be modeled inde-
pendently, even those containing overlapping
words (“American jazz musician”, “French
jazz musician”). As a result, the number of
meaning representations to be learned is ex-
ponential in the length of the class label, and
quickly becomes intractable. Thus, composi-
tional models of taxonomic relations are nec-
essary for better language understanding.

We introduce a compositional approach for
reasoning about fine-grained class labels. Our
approach is based on the notion from formal
semantics, in which modifiers (“1950s”) corre-
spond to properties that differentiate instances
of a subclass (“1950s musicians”) from in-
stances of the superclass (“musicians”) (Heim
and Kratzer, 1998). Our method consists of
two stages: interpreting each modifier rela-
tive to the head (“musicians active during
1950s”), and using the interpretations to iden-
tify instances of the class from text (Figure
1). Our main contributions are: 1) a compo-
sitional method for IsA extraction, which in-

2099

https://doi.org/10.18653/v1/P17-1192
https://doi.org/10.18653/v1/P17-1192


volves a novel application of noun-phrase para-
phrasing methods to the task of semantic tax-
onomy induction and 2) the operationalization
of a formal semantics framework to address
two aspects of semantics that are often kept
separate in NLP: assigning intrinsic “mean-
ing” to a phrase, and reasoning about that
phrase in a truth-theoretic context.

2 Related Work

Noun Phrase Interpretation. Compound
noun phrases (“jazz musician”) communicate
implicit semantic relations between modifiers
and the head. Many efforts to provide se-
mantic interpretations of such phrases rely on
matching the compound to pre-defined pat-
terns or semantic ontologies (Fares et al., 2015;
Ó Séaghdha and Copestake, 2007; Tratz and
Hovy, 2010; Surtani and Paul, 2015; Choi
et al., 2015). Recently, interpretations may
take the form of arbitrary natural language
predicates (Hendrickx et al., 2013). Most
approaches are supervised, comparing un-
seen noun compounds to the most similar
phrase seen in training (Wijaya and Gianfor-
toni, 2011; Nulty and Costello, 2013; Van de
Cruys et al., 2013). Other unsupervised ap-
proaches apply information extraction tech-
niques to paraphrase noun compounds (Kim
and Nakov, 2011; Xavier and Strube de Lima,
2014; Paşca, 2015). They focus exclusively on
providing good paraphrases for an input noun
compound. To our knowledge, ours is the first
attempt to use these interpretations for the
downstream task of IsA relation extraction.

IsA Relation Extraction. Most efforts to
acquire taxonomic relations from text build on
the seminal work of Hearst (1992), which ob-
serves that certain textual patterns–e.g., “X
and other Y”–are high-precision indicators of
whether X is a member of class Y. Recent
work focuses on learning such patterns au-
tomatically from corpora (Snow et al., 2006;
Shwartz et al., 2016). These IsA extraction
techniques provide a key step for the more gen-
eral task of knowledge base population. The
“universal schema” approach (Riedel et al.,
2013; Kirschnick et al., 2016; Verga et al.,
2017), which infers relations using matrix fac-
torization, often includes Hearst patterns as
input features. Graphical (Bansal et al., 2014)

and joint inference models (Movshovitz-Attias
and Cohen, 2015) typically require Hearst pat-
terns to define an inventory of possible classes.
A separate line of work avoids Hearst pat-
terns by instead exploiting semi-structured
data from HTML markup (Wang and Cohen,
2009; Dalvi et al., 2012; Pasupat and Liang,
2014). These approaches all share the limita-
tion that, in practice, in order for a class to
be populated with instances, the entire class
label has to have been observed verbatim in
text. This requirement limits the ability to
handle arbitrarily fine-grained classes. Our
work addresses this limitation by modeling
fine-grained class labels compositionally. Thus
the proposed method can combine evidence
from multiple sentences, and can perform IsA
extraction without requiring any example in-
stances of a given class.1

Taxonomy Construction. Previous work
on the construction of a taxonomy of IsA rela-
tions (Flati et al., 2014; de Melo and Weikum,
2010; Kozareva and Hovy, 2010; Ponzetto and
Strube, 2007; Ponzetto and Navigli, 2009) con-
siders that task to be different than extracting
a flat set of IsA relations from text in prac-
tice. Challenges specific to taxonomy con-
struction include overall concept positioning
and how to discover whether concepts are un-
related, subordinated or parallel to each other
(Kozareva and Hovy, 2010); the need to re-
fine and enrich the taxonomy (Flati et al.,
2014); the difficulty in adding relevant IsA
relations towards the top of the taxonomy
(Ponzetto and Navigli, 2009); eliminating cy-
cles and inconsistencies (Ponzetto and Navigli,
2009; Kozareva and Hovy, 2010). For prac-
tical purposes, these challenges are irrelevant
when extracting flat IsA relations. Whereas
Flati et al. (2014); Bizer et al. (2009); de
Melo and Weikum (2010); Nastase and Strube
(2013); Ponzetto and Strube (2007); Ponzetto
and Navigli (2009); Hoffart et al. (2013) rely
on data within human-curated resources, our
work operates over unstructured text. Re-
sources constructed in Bizer et al. (2009); Nas-
tase and Strube (2013); Hoffart et al. (2013)
contain not just a taxonomy of IsA relations,

1Pasupat and Liang (2014) also focuses on zero-shot
IsA extraction, but exploits HTML document struc-
ture, rather than reasoning compositionally.

2100



but also relation types other than IsA.

3 Modifiers as Functions

Formalization. In formal semantics, mod-
ification is modeled as function application.
Specifically, let MH be a class label consisting
of a head H, which we assume to be a com-
mon noun, preceded by a modifier M . We use
J·K to represent the “interpretation function”
that maps a linguistic expression to its deno-
tation in the world. The interpretation of a
common noun is the set of entities2 in the uni-
verse U . They are denoted by the noun (Heim
and Kratzer, 1998):

JHK = {e ∈ U | e is a H} (1)

The interpretation of a modifier M is a func-
tion that maps between sets of entities. That
is, modifiers select a subset3 of the input set:

JMK(H) = {e ∈ H | e satisfies M} (2)

This formalization leaves open how one de-
cides whether or not “e satisfiesM”. This non-
trivial, as the meaning of a modifier can vary
depending on the class it is modifying: if e is
a “good student”, e is not necessarily a “good
person”, making it difficult to model whether
“e satisfies good” in general. We therefore re-
frame the above equation, so that the decision
of whether “e satisfies M” is made by calling
a binary function φM , parameterized by the
class H within which e is being considered:

JMK(H) = {e ∈ H | φM (H, e)} (3)

Conceptually, φM captures the core “mean-
ing” of the modifier M , which is the set of
properties that differentiate members of the
output class MH from members of the more
general input class H. This formal semantics
framework has two important consequences.
First, the modifier has an intrinsic “mean-
ing”. The properties entailed by the modi-
fier are independent of the particular state of
the world. This makes it possible to make in-
ferences about “1950s musician” even if no

2We use “entities” and “instances” interchange-
ably;“entities” is standard terminology in linguistics.

3As does virtually all previous work in information
extraction, we assume that modifiers are subsective, ac-
knowledging the limitations (Kamp and Partee, 1995).

1950s musician have been observed. Second,
the modifier is a function that can be applied
in a truth-theoretic setting. That is, apply-
ing “1950s” to the set of “musicians” returns
exactly the set of “1950s musicians”.

Computational Approaches. While the
notion of modifiers as functions has been in-
corporated into computational models previ-
ously, prior work focuses on either assigning an
intrinsic meaning to M or on operationalizing
M in a truth-theoretic sense, but not on do-
ing both simultaneously. For example, Young
et al. (2014) focuses exclusively on the subset
selection aspect of modification. That is, given
a set of instances H and a modifier M , their
method could return the subset MH. How-
ever, their method does not model the mean-
ing of the modifier itself, so that, e.g., if there
were no red cars in their model of the world,
the phrase “red cars” would have no mean-
ing. In contrast, Baroni and Zamparelli (2010)
models the meaning of modifiers explicitly as
functions that map between vector-space rep-
resentations of nouns. However, their model
focuses on similarity between class labels–e.g.,
to say that “important routes” is similar to
“major roads”–and it is not obvious how the
method could be operationalized in order to
identify instances of those classes. A contri-
bution of our work is to model the semantics
of M intrinsically, but in a way that permits
application in the model theoretic setting. We
learn an explicit model of the “meaning” of a
modifier M relative to a head H, represented
as a distribution over properties that differen-
tiate the members of the class MH from those
of the class H. We then use this representa-
tion to identify the subset of instances of H,
which constitute the subclass MH.

4 Learning Modifier Interpretations

4.1 Setup

For each modifier M , we would like to learn
the function φM from Eq. 3. Doing so makes
it possible, given H and an instance e ∈ H,
to decide whether e has the properties re-
quired to be an instance of MH. In gen-
eral, there is no systematic way to determine
the implied relation between M and H, as
modifiers can arguably express any semantic
relation, given the right context (Weiskopf,

2101



2007). We therefore model the semantic re-
lation between M and H as a distribution
over properties that could potentially define
the subclass MH ⊆ H. We will refer to
this distribution as a “property profile” for
M relative to H. We make the assumption
that relations between M and H that are dis-
cussed more often are more likely to capture
the important properties of the subclass MH.
This assumption is not perfect (Section 4.4)
but has given good results for paraphrasing
noun phrases (Nakov and Hearst, 2013; Paşca,
2015). Our method for learning property pro-
files is based on the unsupervised method pro-
posed by Paşca (2015), which uses query logs
as a source of common sense knowledge, and
rewrites noun compounds by matching MH
(“American musicians”) to queries of the form
“H(.∗)M” (“musicians from America”).

4.2 Inputs

We assume two inputs: 1) an IsA repository,
O, containing 〈e, C〉 tuples where C is a cat-
egory and e is an instance of C, and 2) a
fact repository, D, containing 〈s, p, o, w〉 tuples
where s and o are noun phrases, p is a pred-
icate, and w is a confidence that p expresses
a true relation between s and o. Both O and
D are extracted from a sample of around 1
billion Web documents in English. The sup-
plementary material gives additional details.

We instantiate O with an IsA repository
constructed by applying Hearst patterns to
the Web documents. Instances are rep-
resented as automatically-disambiguated en-
tity mentions4 which, when possible, are re-
solved to Wikipedia pages. Classes are rep-
resented as (non-disambiguated) natural lan-
guage strings. We instantiate D with a large
repository of facts extracted using in-house im-
plementations of ReVerb (Fader et al., 2011)
and OLLIE (Mausam et al., 2012). The
predicates are extracted as natural language
strings. Subjects and objects may be either
disambiguated entity references or natural lan-
guage strings. Every tuple is included in
both the forward and the reverse direction.
E.g. 〈jazz, perform at, venue〉 also appears as
〈venue,←perform at, jazz〉, where ← is a spe-

4“Entity mentions” may be individuals, like
“Barack Obama”, but may also be concepts like “jazz”.

cial character signifying inverted predicates.
These inverted predicates simplify the follow-
ing definitions. In total, O contains 1.1M tu-
ples and D contains 30M tuples.

4.3 Building Property Profiles

Properties. Let I be a function that takes
as input a noun phrase MH and returns a
property profile for M relative to H. We de-
fine a “property” to be a tuple of a subject,
predicate and object in which the subject posi-
tion5 is a wildcard, e.g. 〈∗, born in,America〉.
Any instance that fills the wildcard slot then
“has” the property. We expand adjectival
modifiers to encompass nominalized forms us-
ing a nominalization dictionary extracted from
WordNet (Miller, 1995). If MH is “Ameri-
can musician” and we require a tuple to have
the form 〈H, p,M,w〉, we will include tuples in
which the third element is either “American”
or “America”.

Relating M to H Directly. We first build
property profiles by taking the predicate and
object from any tuple in D in which the sub-
ject is the head and the object is the modifier:

I1(MH) = {〈〈p,M〉, w〉 | 〈H, p,M,w〉 ∈ D}
(4)

Relating M to an Instance of H. We also
consider an extension in which, rather than
requiring the subject to be the class label H,
we require the subject to be an instance of H.

I2(MH) = {〈〈p,M〉, w〉 | 〈e,H〉 ∈ O
∧〈e, p,M,w〉 ∈ D} (5)

Modifier Expansion. In practice, when
building property profiles, we do not require
that the object of the fact tuple match the
modifier exactly, as suggested in Eq. 4 and 5.
Instead, we follow Paşca (2015) and take ad-
vantage of facts involving distributionally sim-
ilar modifiers. Specifically, rather than look-
ing only at tuples in D in which the object
matches M , we consider all tuples, but dis-
count the weight proportionally to the simi-
larity between M and the object of the tuple.

5Inverse predicates capture properties in which the
wildcard is conceptually the object of the relation, but
occupies the subject slot in the tuple. For example,
〈venue,←perform at, jazz〉 captures that a “jazz venue”
is a “venue” e such that “jazz performed at e”.

2102



Good Property Profiles Bad Property Profiles
rice dish French violinist Led Zeppelin song still life painter child actor risk manager
* serve with rice * live in France Led Zeppelin write * * known for still life * have child * take risk
* include rice * born in France Led Zeppelin play * * paint still life * expect child * be at risk
* consist of rice * speak French Led Zeppelin have * still life be by * * play child * be aware of risk

Table 1: Example property profiles learned by observing predicates that relate instances of class
H to modifier M (I2). Results are similar when using the class label H directly (I1). We spell
out inverted predicates (Section 4.2) so wildcards (*) may appear as subjects or objects.

Thus, I1 is computed as below:

I1(MH) = {〈〈p,M〉, w × sim(M,N)〉
| 〈H, p,N,w〉 ∈ D} (6)

where sim(M,N) is the cosine similarity be-
tween M and N . I2 is computed analogously.
We compute sim using a vector space built
from Web documents following Lin and Wu
(2009); Pantel et al. (2009). We retain the 100
most similar phrases for each of∼10M phrases,
and consider all other similarities to be 0.

4.4 Analysis of Property Profiles

Table 1 provides examples of good and bad
property profiles for several MHs. In general,
frequent relations between M and H capture
relevant properties of MH, but it is not always
the case. To illustrate, the most frequently dis-
cussed relation between “child” and “actor”
is that actors have children, but this property
is not indicative of the meaning of “child ac-
tor”. Qualitatively, the top-ranked interpreta-
tions learned by using the head noun directly
(I1, Eq. 4) are very similar to those learned
using instances of the head (I2, Eq. 5). How-
ever, I2 returns many more properties (10 on
average per MH) than I1 (just over 1 on av-
erage). Anecdotally, we see that I2 captures
more specific relations than does I1. For exam-
ple, for “jazz musicians”, both methods return
“* write jazz” and “* compose jazz”, but I2
additionally returns properties like “* be ma-
jor creative influence in jazz”. We compare
I1 and I2 quantitatively in Section 6. Impor-
tantly, we do see that both I1 and I2 are capa-
ble of learning head-specific property profiles
for a modifier. Table 2 provides examples.

5 Class-Instance Identification

Instance finding. After finding properties
that relate a modifier to a head, we turn to
the task of identifying instances of fine-grained

Class Label Property Profile

American company * based in America
American composer * born in America
American novel * written in America

jazz album * features jazz

jazz composer * writes jazz

jazz venue jazz performed at *

Table 2: Head-specific property profiles
learned by relating instances of H to the mod-
ifier M (I2). Results are similar using I1.

classes. That is, for a given modifier M , we
want to instantiate the function φM from Eq.
3. In practice, rather than being a binary func-
tion that decides whether or not e is in class
MH, our instantiation, φ̂M , will return a real-
valued score expressing the confidence that e
is a member of MH. For notational conve-
nience, let D(〈s, p, o〉) = w, if 〈s, p, o, w〉 ∈ D
and 0 otherwise. We define φ̂M as follows:

φ̂M (H, e) =
∑

〈〈p,o〉,ω〉∈I(MH)
ω×D(〈e, p, o〉) (7)

Applying M to H, then, is as in Eq. 3 ex-
cept that instead of a discrete set, it returns a
scored list of candidate instances:

JMK(H) = {〈e, φ̂M (H, e)〉 | 〈e,H〉 ∈ O} (8)

Ultimately, we need to identify instances of ar-
bitrary class labels, which may contain mul-
tiple modifiers. Given a class label C =
M1 . . .MkH that contains a head H preceded
by modifiers M1 . . .Mk, we generate a list of
candidate instances by finding all instances of
H that have some property to support every
modifier:

k⋂

i=1

{〈e, s(e)〉 | 〈e, w〉 ∈ JMiK(H) ∧w > 0} (9)

2103



where s(e) is the mean6 of the scores assigned
by each separate φ̂Mi . From here on, we use
Mods to refer to our method that generates
lists of instances for a class using Eq. 8 and
9. When φ̂M (Eq. 7) is implemented using
I1, we use the name ModsH (for “heads”).
When it is implemented using I2, we use the
name ModsI (for “instances”).

Weakly Supervised Reranking. Eq. 8
uses a naive ranking in which the weight for
e ∈MH is the product of how often e has been
observed with some property and the weight
of that property for the class MH. Thus, in-
stances of H with overall higher counts in D
receive high weights for every MH. We there-
fore train a simple logistic regression model to
predict the likelihood that e belongs to MH.
We use a small set of features7, including the
raw weight as computed in Eq. 7. For train-
ing, we sample 〈e, C〉 pairs from our IsA repos-
itory O as positive examples and random pairs
that were not extracted by any Hearst pattern
as negative examples. We frame the task as a
binary prediction of whether e ∈ C, and use
the model’s confidence as the value of φ̂M in
place of the function in Eq. 7.

6 Evaluation

6.1 Experimental Setup

Evaluation Sets. We evaluate our models
on their ability to return correct instances for
arbitrary class labels. As a source of evalu-
ation data, we use Wikipedia category pages
(e.g., http://en.wikipedia.org/wiki/Category:
Pakistani film actresses). These are pages in
which the title is the name of the category
(“pakistani film actresses”) and the body is
a manually curated list of links to other pages
that fall under the category. We measure the
precision and recall of each method for discov-
ering the instances listed on these pages given
the page title (henceforth “class label”).

We collect the titles of all Wikipedia cate-
gory pages, removing those in which the last
word is capitalized or which contain fewer than
three words. These heuristics are intended to
retain compositional titles in which the head
is a single common noun. We also remove

6Also tried minimum, but mean gave better results.
7Feature templates in supplementary material.

Evaluation Set: Examples of Class Labels
UniformSet: 2008 california wildfires · australian
army chaplains · australian boy bands · canadian
military nurses · canberra urban places · cellular
automaton rules · chinese rice dishes · coldplay
concert tours · daniel libeskind designs · economic
stimulus programs · german film critics · invasive
amphibian species · latin political phrases · log
flume rides · malayalam short stories · pakistani
film actresses · puerto rican sculptors · string the-
ory books
WeightedSet: ancient greek physicists · art deco
sculptors · audio engineering schools · ballet train-
ing methods · bally pinball machines · british
rhythmic gymnasts · calgary flames owners · cana-
dian rock climbers · canon l-series lenses · emi clas-
sics artists · free password managers · georgetown
university publications · grapefruit league venues ·
liz claiborne subsidiaries · miss usa 2000 delegates
· new zealand illustrators · russian art critics

Table 3: Examples of class labels from evalu-
ation sets.

any titles that contain links to sub-categories.
This is to favor fine-grained classes (“pak-
istani film actresses”) over coarse-grained ones
(“film actresses”). We perform heuristic mod-
ifier chunking in order to group together mul-
tiword modifiers (e.g., “puerto rican”); for de-
tails, see supplementary material. From the
resulting list of class labels, we draw two sam-
ples of 100 labels each, enforcing that no H
appear as the head of more than three class
labels per sample. The first sample is cho-
sen uniformly at random (denoted Uniform-
Set). The second (WeightedSet) is weighted
so that the probability of drawing M1 . . .MkH
is proportional to the total number of class la-
bels in which H appears as the head. These
different evaluation sets8 are intended to eval-
uate performance on the head versus the tail
of class label distribution, since information
retrieval methods often perform differently on
different parts of the distribution. On average,
there are 17 instances per category in Uniform-
Set and 19 in WeightedSet. Table 3 gives ex-
amples of class labels.

Baselines. We implement two baselines us-
ing our IsA repository (O as defined in Section
4.1). Our simplest baseline ignores modifiers
altogether, and simply assumes that any in-
stance of H is an instance of MH, regardless
of M . In this case the confidence value for

8Available at http://www.seas.upenn.edu/∼nlp/
resources/finegrained-class-eval.gz

2104



〈e,MH〉 is equivalent to that for 〈e,H〉. We
refer to this baseline simply as Baseline. Our
second, stronger baseline uses the IsA reposi-
tory directly to identify instances of the fine-
grained class C = M1 . . .MkH. That is, we
consider e to be an instance of the class if
〈e, C〉 ∈ O, meaning the entire class label ap-
peared in a source sentence matching some
Hearst pattern. We refer to this baseline as
Hearst. The weight used to rank the candi-
date instances is the confidence value assigned
by the Hearst pattern extraction (Section 4.2).

Compositional Models. As a baseline
compositional model, we augment the Hearst
baseline via set intersection. Specifically, for
a class C = M1 . . .MkH, if each of the MiH
appears in O independently, we take the in-
stances of C to be the intersection of the in-
stances of each of the MiH. We assign the
weight of an instance e to be the sum of
the weights associated with each independent
modifier. We refer to this method as Hearst∩.
It is roughly equivalent to (Paşca, 2014). We
contrast it with our proposed model, which
recognizes instances of a fine-grained class by
1) assigning a meaning to each modifier in
the form of a property profile and 2) checking
whether a candidate instance exhibits these
properties. We refer to the versions of our
method as ModsH and ModsI , as described
in Section 5. When relevant, we use “raw”
to refer to the version in which instances are
ranked using raw weights and “RR” to refer to
the version in which instances are ranked us-
ing logistic regression (Section 5). We also try
using the proposed methods to extend rather
than replace the Hearst baseline. We com-
bine predictions by merging the ranked lists
produced by each system: i.e. the score of
an instance is the inverse of the sum of its
ranks in each of the input lists. If an in-
stance does not appear at all in an input list,
its rank in that list is set to a large constant
value. We refer to these combination systems
as Hearst+ModsH and Hearst+ModsI .

6.2 Results

Precision and Coverage. We first com-
pare the methods in terms of their cover-
age, the number of class labels for which the
method is able to find some instance, and their

precision, to what extent the method is able to
correctly rank true instances of the class above
non-instances. We report total coverage, the
number of labels for which the method returns
any instance, and correct coverage, the num-
ber of labels for which the method returns a
correct instance. For precision, we compute
the average precision (AP) for each class la-
bel. AP ranges from 0 to 1, where 1 indicates
that all positive instances were ranked above
all negative instances. We report mean av-
erage precision (MAP), which is the mean of
the APs across all the class labels. MAP is
only computed over class labels for which the
method returns something, meaning methods
are not punished for returning empty lists.

Table 4 gives examples of instances returned
for several class labels and Table 5 shows the
precision and coverage for each of the meth-
ods. Figure 2 illustrates how the single mean
AP score (as reported in Table 5) can misrep-
resent the relative precision of different meth-
ods. In combination, Table 5 and Figure 2
demonstrate that the proposed methods ex-
tract instances about as well as the baseline,
whenever the baseline can extract anything at
all; i.e. the proposed method does not cause a
precision drop on classes covered by the base-
line. In addition, there are many classes for
which the baseline is not able to extract any
instances, but the proposed method is. None
of the methods can extract some of the gold
instances, such as “Dictator perpetuo” and
“Furor Teutonicus” of the gold class “latin po-
litical phrases”.

Table 5 also reveals that the reranking
model (RR) consistently increases MAP for
the proposed methods. Therefore, going for-
ward, we only report results using the rerank-
ing model (i.e. ModsH and ModsI will refer to
ModsH RR and ModsI RR, respectively).

Manual Re-Annotation. It possible that
true instances of a class are missing from our
Wikipedia reference set, and thus that our
precision scores underestimate the actual pre-
cision of the systems. We therefore man-
ually verify the top 10 predictions of each
of the systems for a random sample of 25
class labels. We choose class labels for which
Hearst was able to return at least one in-
stance, in order to ensure reliable precision

2105



Flemish still life painters: Clara Peeters · Willem Kalf · Jan Davidsz de Heem · Pieter
Claesz · Peter Paul Rubens · Frans Snyders · Jan Brueghel the Elder · Hans Memling · Pieter
Bruegel the Elder · Caravaggio · Abraham Brueghel
Pakistani cricket captains: Salman Butt · Shahid Afridi · Javed Miandad · Azhar Ali ·
Greg Chappell · Younis Khan · Wasim Akram · Imran Khan · Mohammad Hafeez · Rameez
Raja · Abdul Hafeez Kardar · Waqar Younis · Sarfraz Ahmed
Thai buddhist temples: Wat Buddhapadipa · Wat Chayamangkalaram · Wat Mongkol-
ratanaram · Angkor Wat · Preah Vihear Temple · Wat Phra Kaew · Wat Rong Khun · Wat
Mahathat Yuwaratrangsarit · Vat Phou · Tiger Temple · Sanctuary of Truth · Wat Chalong
· Swayambhunath · Mahabodhi Temple · Tiger Cave Temple · Harmandir Sahib

Table 4: Instances extracted for several fine-grained classes from Wikipedia. Lists shown are
from ModsI . Instances in italics were also returned by Hearst∩. Strikethrough denotes incorrect.

UniformSet WeightedSet
Coverage MAP Coverage MAP

Baseline 95 / 70 0.01 98 / 74 0.01
Hearst 9 / 9 0.63 8 / 8 0.80
Hearst∩ 13 / 12 0.62 9 / 9 0.80
ModsH raw 56 / 32 0.23 50 / 30 0.16
ModsH RR 56 / 32 0.29 50 / 30 0.25
ModsI raw 62 / 36 0.18 59 / 38 0.20
ModsI RR 62 / 36 0.24 59 / 38 0.23

Table 5: Coverage and precision for populat-
ing Wikipedia category pages with instances.
“Coverage” is the number of class labels (out
of 100) for which at least one instance was
returned, followed by the number for which
at least one correct instance was returned.
“MAP” is mean average precision. MAP does
not punish methods for returning empty lists,
thus favoring the baseline (see Figure 2).

Figure 2: Distribution of AP over 100 class
labels in WeightedSet. The proposed method
(red) and the baseline method (blue) achieve
high AP for the same number of classes, but
ModsI additionally finds instances for classes
for which the baseline returns nothing.

estimates. For each of these labels, we man-
ually check the top 10 instances proposed by

each method to determine whether each be-
longs to the class. Table 6 shows the precision
scores for each method computed against the
original Wikipedia list of instances and against
our manually-augmented list of gold instances.
The overall ordering of the systems does not
change, but the precision scores increase no-
tably after re-annotation. We continue to eval-
uate against the Wikipedia lists, but acknowl-
edge that reported precision is likely an under-
estimate of true precision.

Wikipedia Gold

Hearst 0.56 0.79
Hearst∩ 0.53 0.78
ModsH 0.23 0.39
ModsI 0.24 0.42
Hearst+ModsH 0.43 0.63
Hearst+ModsI 0.43 0.63

Table 6: P@10 before vs. after re-annotation;
Wikipedia underestimates true precision.

UniformSet WeightedSet
AUC Recall AUC Recall

Baseline 0.55 0.23 0.53 0.28
Hearst 0.56 0.03 0.52 0.02
Hearst∩ 0.57 0.04 0.53 0.02
ModsH 0.68 0.08 0.60 0.06
ModsI 0.71 0.09 0.65 0.09
Hearst∩+ModsH 0.70 0.09 0.61 0.08
Hearst∩+ModsI 0.73 0.10 0.66 0.10

Table 7: Recall of instances on Wikipedia cat-
egory pages, measured against the full set of
instances from all pages in sample. AUC cap-
tures tradeoff between true and false positives.

2106



(a) Uniform random sample (UniformSet). (b) Weighted random sample (WeightedSet).

Figure 3: ROC curves for selected methods (Hearst in blue, proposed in red). Given a ranked
list of instances, ROC curves plot true positives vs. false positives retained by setting various
cutoffs. The curve becomes linear once all remaining instances have the same score (e.g., 0), as
this makes it impossible to add true positives without also including all remaining false positives.

Precision-Recall Analysis. We next look
at the precision-recall tradeoff in terms of
the area under the curve (AUC) when each
method attempts to rank the complete list of
candidate instances. We take the union of all
of the instances proposed by all of the methods
(including the Baseline method which, given
a class label M0 . . .MkH, proposes every in-
stance of the head H as a candidate). Then,
for each method, we rank this full set of candi-
dates such that any instance returned by the
method is given the score the method assigns,
and every other instance is scored as 0. Table 7
reports the AUC and recall. Figure 3 plots the
full ROC curves. The requirement by Hearst
that class labels appear in full in a single sen-
tence results in very low recall, which trans-
lates into very low AUC when considering the
full set of candidate instances. In comparison,
the proposed compositional methods make use
of a larger set of sentences, and provide non-
zero scores for many more candidates, result-
ing in a >10 point increase in AUC on both
UniformSet and WeightedSet (Table 7).

7 Conclusion

We have presented an approach to IsA extrac-
tion that takes advantage of the composition-
ality of natural language. Existing approaches
often treat class labels as atomic units that
must be observed in full in order to be pop-

ulated with instances. As a result, current
methods are not able to handle the infinite
number of classes describable in natural lan-
guage, most of which never appear in text.
Our method reasons about each modifier in
the label individually, in terms of the proper-
ties that it implies about the instances. This
approach allows us to harness information that
is spread across multiple sentences, signifi-
cantly increasing the number of fine-grained
classes that we are able to populate.

Acknowledgments

The paper incorporates suggestions on an ear-
lier version from Susanne Riehemann. Ryan
Doherty offered support in refining and access-
ing the fact repository used in the evaluation.

References

M. Bansal, D. Burkett, G. de Melo, and D. Klein.
2014. Structured learning for taxonomy induc-
tion with belief propagation. In Proceedings
of the 52nd Annual Meeting of the Association
for Computational Linguistics (ACL-14). Balti-
more, Maryland, pages 1041–1051.

M. Baroni and R. Zamparelli. 2010. Nouns are
vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space.
In Proceedings of the 2010 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP-10). Cambridge, Massachusetts, pages
1183–1193.

2107



C. Bizer, J. Lehmann, G. Kobilarov, S. Auer,
C. Becker, R. Cyganiak, and S. Hellmann. 2009.
DBpedia - a crystallization point for the Web of
data. Journal of Web Semantics 7(3):154–165.

E. Choi, T. Kwiatkowski, and L. Zettlemoyer.
2015. Scalable semantic parsing with partial
ontologies. In Proceedings of the 53rd An-
nual Meeting of the Association for Compu-
tational Linguistics (ACL-15). Beijing, China,
pages 1311–1320.

B. Dalvi, W. Cohen, and J. Callan. 2012. Web-
sets: Extracting sets of entities from the Web
using unsupervised information extraction. In
Proceedings of the 5th ACM Conference on Web
Search and Data Mining (WSDM-12). Seattle,
Washington, pages 243–252.

G. de Melo and G. Weikum. 2010. MENTA: In-
ducing multilingual taxonomies from Wikipedia.
In Proceedings of the 19th International Con-
ference on Information and Knowledge Man-
agement (CIKM-10). Toronto, Canada, pages
1099–1108.

A. Fader, S. Soderland, and O. Etzioni. 2011. Iden-
tifying relations for open information extraction.
In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Process-
ing (EMNLP-11). Edinburgh, Scotland, pages
1535–1545.

M. Fares, S. Oepen, and E. Velldal. 2015. Identify-
ing compounds: On the role of syntax. In Inter-
national Workshop on Treebanks and Linguis-
tic Theories (TLT-14). Warsaw, Poland, pages
273–283.

T. Flati, D. Vannella, T. Pasini, and R. Navigli.
2014. Two is bigger (and better) than one: the
Wikipedia Bitaxonomy project. In Proceedings
of the 52nd Annual Meeting of the Association
for Computational Linguistics (ACL-14). Balti-
more, Maryland, pages 945–955.

M. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings
of the 14th Conference on Computational Lin-
guistics (COLING-92). pages 539–545.

I. Heim and A. Kratzer. 1998. Semantics in Gen-
erative Grammar , volume 13. Blackwell Oxford.

I. Hendrickx, Z. Kozareva, P. Nakov, D. Ó
Séaghdha, S. Szpakowicz, and T. Veale. 2013.
SemEval-2013 task 4: Free paraphrases of noun
compounds. In Proceedings of Proceedings of the
7th International Workshop on Semantic Eval-
uation (SemEval-13). pages 138–143.

J. Hoffart, F. Suchanek, K. Berberich, and
G. Weikum. 2013. YAGO2: a spatially
and temporally enhanced knowledge base from
Wikipedia. Artificial Intelligence Journal. Spe-
cial Issue on Artificial Intelligence, Wikipedia
and Semi-Structured Resources 194:28–61.

H. Kamp and B. Partee. 1995. Prototype theory
and compositionality. Cognition 57(2):129–191.

N. Kim and P. Nakov. 2011. Large-scale noun com-
pound interpretation using bootstrapping and
the Web as a corpus. In Proceedings of the
2011 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-11). Edin-
burgh, Scotland, pages 648–658.

J. Kirschnick, H. Hemsen, and V. Markl. 2016.
Jedi: Joint entity and relation detection using
type inference. In Proceedings of the 54th An-
nual Meeting of the Association for Computa-
tional Linguistics (ACL-16) - System Demon-
strations. Berlin, Germany, pages 61–66.

Z. Kozareva and E. Hovy. 2010. A semi-supervised
method to learn and construct taxonomies us-
ing the web. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing (EMNLP-10). Cambridge,
Massachusetts, pages 1110–1118.

D. Lin and X. Wu. 2009. Phrase clustering for dis-
criminative learning. In Proceedings of the 47th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL-IJCNLP-09). Singa-
pore, pages 1030–1038.

Mausam, M. Schmitz, S. Soderland, R. Bart, and
O. Etzioni. 2012. Open language learning for
information extraction. In Proceedings of the
2012 Joint Conference on Empirical Methods
in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-
CoNLL-12). Jeju Island, Korea, pages 523–534.

G. Miller. 1995. WordNet: a lexical database.
Communications of the ACM 38(11):39–41.

D. Movshovitz-Attias and W. Cohen. 2015. Kb-
lda: Jointly learning a knowledge base of hi-
erarchy, relations, and facts. In Proceedings of
the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language
Processing (ACL-IJCNLP-15). Beijing, China,
pages 1449–1459.

P. Nakov and M. Hearst. 2013. Semantic in-
terpretation of noun compounds using verbal
and other paraphrases. ACM Transactions on
Speech and Language Processing 10(3):1–51.

V. Nastase and M. Strube. 2013. Transforming
Wikipedia into a large scale multilingual concept
network. Artificial Intelligence 194:62–85.

P. Nulty and F. Costello. 2013. General and
specific paraphrases of semantic relations be-
tween nouns. Natural Language Engineering
19(03):357–384.

2108



D. Ó Séaghdha and A. Copestake. 2007. Co-
occurrence contexts for noun compound inter-
pretation. In Proceedings of the Workshop on a
Broader Perspective on Multiword Expressions.
Prague, Czech Republic, pages 57–64.

M. Paşca. 2014. Acquisition of open-domain
classes via intersective semantics. In Proceed-
ings of the 23rd World Wide Web Conference
(WWW-14). Seoul, Korea, pages 551–562.

M. Paşca. 2015. Interpreting compound noun
phrases using web search queries. In Proceed-
ings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Compu-
tational Linguistics: Human Language Tech-
nologies (NAACL-HLT-15). Denver, Colorado,
pages 335–344.

P. Pantel, E. Crestan, A. Borkovsky, A. Popescu,
and V. Vyas. 2009. Web-scale distributional
similarity and entity set expansion. In Proceed-
ings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-
09). Singapore, pages 938–947.

P. Pasupat and P. Liang. 2014. Zero-shot en-
tity extraction from Web pages. In Proceedings
of the 52nd Annual Meeting of the Association
for Computational Linguistics (ACL-14). Balti-
more, Maryland, pages 391–401.

S. Ponzetto and R. Navigli. 2009. Large-scale tax-
onomy mapping for restructuring and integrat-
ing Wikipedia. In Proceedings of the 21st Inter-
national Joint Conference on Artificial Intelli-
gence (IJCAI-09). Pasadena, California, pages
2083–2088.

S. Ponzetto and M. Strube. 2007. Deriving a large
scale taxonomy from Wikipedia. In Proceed-
ings of the 22nd National Conference on Artifi-
cial Intelligence (AAAI-07). Vancouver, British
Columbia, pages 1440–1447.

S. Riedel, L. Yao, A. McCallum, and B. Mar-
lin. 2013. Relation extraction with matrix fac-
torization and universal schemas. In Proceed-
ings of the 2013 Conference of the North Amer-
ican Association for Computational Linguistics
(NAACL-HLT-13). Atlanta, Georgia, pages 74–
84.

V. Shwartz, Y. Goldberg, and I. Dagan. 2016. Im-
proving hypernymy detection with an integrated
path-based and distributional method. In Pro-
ceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-
16). Berlin, Germany, pages 2389–2398.

R. Snow, D. Jurafsky, and A. Ng. 2006. Se-
mantic taxonomy induction from heterogenous
evidence. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics
and 44th Annual Meeting of the Association for

Computational Linguistics (COLING-ACL-06).
Sydney, Australia, pages 801–808.

N. Surtani and S. Paul. 2015. A vsm-based statis-
tical model for the semantic relation interpreta-
tion of noun-modifier pairs. Proceedings of the
International Conference on Recent Advances
in Natural Language Processing (RANLP-15)
pages 636–645.

S. Tratz and E. Hovy. 2010. A taxonomy, dataset,
and classifier for automatic noun compound in-
terpretation. In Proceedings of the 48th Annual
Meeting of the Association for Computational
Linguistics (ACL-10). Uppsala, Sweden, pages
678–687.

T. Van de Cruys, S. Afantenos, and P. Muller.
2013. MELODI: A supervised distributional
approach for free paraphrasing of noun com-
pounds. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval-
13). Atlanta, Georgia, pages 144–147.

P. Verga, A. Neelakantan, and A. McCallum. 2017.
Generalizing to unseen entities and entity pairs
with row-less universal schema. In Proceedings
of the 15th Conference of the European Chapter
of the Association for Computational Linguistics
(EACL-17). Valencia, Spain, pages 613–622.

R. Wang and W. Cohen. 2009. Automatic set in-
stance extraction using the Web. In Proceedings
of the 47th Annual Meeting of the Association
for Computational Linguistics (ACL-IJCNLP-
09). Singapore, pages 441–449.

D. Weiskopf. 2007. Compound nominals, context,
and compositionality. Synthese 156(1):161–204.

D. Wijaya and P. Gianfortoni. 2011. Nut case:
What does it mean?: Understanding seman-
tic relationship between nouns in noun com-
pounds through paraphrasing and ranking the
paraphrases. In Proceedings of the 1st Interna-
tional Workshop on Search and Mining Entity-
Relationship Data (SMER-11). Glasgow, United
Kingdom, pages 9–14.

C. Xavier and V. Strube de Lima. 2014. Boosting
open information extraction with noun-based re-
lations. In Proceedings of the 9th International
Conference on Language Resources and Evalua-
tion (LREC-14). Reykjavik, Iceland, pages 96–
100.

P. Young, A. Lai, M. Hodosh, and J. Hockenmaier.
2014. From image descriptions to visual deno-
tations: New similarity metrics for semantic in-
ference over event descriptions. Transactions of
the Association for Computational Linguistics
(TACL) 2:67–78.

2109


	Identifying 1950s American Jazz Musicians: Fine-Grained IsA Extraction via Modifier Composition

