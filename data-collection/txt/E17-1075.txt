



















































Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 797–807,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Fine-Grained Entity Type Classification by Jointly Learning
Representations and Label Embeddings

Abhishek, Ashish Anand and Amit Awekar
Department of Computer Science and Engineering

Indian Institute of Technology Guwahati
Assam, India - 781039

{abhishek.abhishek, anand.ashish, awekar}@iitg.ernet.in

Abstract

Fine-grained entity type classification
(FETC) is the task of classifying an entity
mention to a broad set of types. Distant su-
pervision paradigm is extensively used to
generate training data for this task. How-
ever, generated training data assigns same
set of labels to every mention of an en-
tity without considering its local context.
Existing FETC systems have two major
drawbacks: assuming training data to be
noise free and use of hand crafted fea-
tures. Our work overcomes both draw-
backs. We propose a neural network
model that jointly learns entity mentions
and their context representation to elim-
inate use of hand crafted features. Our
model treats training data as noisy and
uses non-parametric variant of hinge loss
function. Experiments show that the pro-
posed model outperforms previous state-
of-the-art methods on two publicly avail-
able datasets, namely FIGER(GOLD) and
BBN with an average relative improvement
of 2.69% in micro-F1 score. Knowledge
learnt by our model on one dataset can
be transferred to other datasets while us-
ing same model or other FETC systems.
These approaches of transferring knowl-
edge further improve the performance of
respective models.

1 Introduction

Entity type classification is the task for assigning
types or labels such as organization, location to
entity mentions in a document. This classifica-
tion is useful for many natural language process-
ing (NLP) tasks such as relation extraction (Mintz
et al., 2009), machine translation (Koehn et al.,

2007), question answering (Lin et al., 2012) and
knowledge base construction (Dong et al., 2014).

There has been considerable amount of work
on Named Entity Recognition (NER) (Collins and
Singer, 1999; Tjong Kim Sang and De Meul-
der, 2003; Ratinov and Roth, 2009; Manning et
al., 2014), which classifies entity mentions into
a small set of mutually exclusive types, such as
Person, Location, Organization and Misc. How-
ever, these types are not enough for some NLP
applications such as relation extraction, knowl-
edge base construction (KBC) and question an-
swering. In relation extraction and KBC, know-
ing fine-grained types for entities can significantly
increase the performance of the relation extrac-
tor (Ling and Weld, 2012; Koch et al., 2014;
Mitchell et al., 2015) since this helps in filtering
out candidate relation types that do not follow the
type constrain. Fine-grained entity types provide
additional information while matching questions
to its potential answers and significantly improves
performance (Dong et al., 2015). For example, Li
and Roth (2002) rank questions based on their ex-
pected answer types (will the answer be food, ve-
hicle or disease).

Typically, FETC systems use over hundred la-
bels, arranged in a hierarchical structure. An im-
portant aspect of FETC is that based on local con-
text, two different mentions of same entity can
have different labels. We illustrate this through an
example in Figure 1. All three sentences S1, S2,
and S3 mention same entity Barack Obama. How-
ever, looking at the context, we can infer that S1
mentions Obama as a person/author, S2 mentions
Obama only as a person, and S3 mentions Obama
as a person/politician.

Available training data for FETC has noisy la-
bels. Creating manually annotated training data
for FETC is time consuming, expensive, and er-
ror prone. Note that, a human annotator will

797



Figure 1: Noise introduced via distant supervision
process. S1-S3 indicates sentences where only a
subset of labels for entity mention (bold typeface)
are relevant given context, highlighted in T1-T3.

have to assign a subset of correct labels from a
set of around hundred labels for each entity men-
tion in the corpus. Existing FETC systems use
distant supervision paradigm (Craven and Kum-
lien, 1999) to automatically generate training data.
Distant supervision maps each entity in the cor-
pus to knowledge bases such as Freebase (Bol-
lacker et al., 2008), DBpedia (Auer et al., 2007),
YAGO (Suchanek et al., 2007). This method as-
signs same set of labels to all mentions of an
entity across the corpus. For example, Barack
Obama is a person, politician, lawyer, and author.
If a knowledge base has these four matching la-
bels for Barack Obama, then distant supervision
assigns all of them to every mention of Barack
Obama. Training data generated with distant su-
pervision will fail to distinguish between mentions
of Barack Obama in sentences S1, S2, and S3.

Existing FETC systems have one or both of fol-
lowing drawbacks:

1. Assuming training data to be noise free (Ling
and Weld, 2012; Yosef et al., 2012; Yogatama
et al., 2015; Shimaoka et al., 2016)

2. Use of hand crafted features (Ling and Weld,
2012; Yosef et al., 2012; Yogatama et al.,
2015; Ren et al., 2016)

We have observed that for real world datasets,
more than twenty five percent of training data has
noisy labels. First drawback propagates this noise
in training data to the FETC model. To extract
hand crafted features various NLP tools are used.
Since errors inevitably exist in such tools, the sec-
ond drawback propagates errors of these tools to
FETC model.

We propose a neural network based model to
overcome the two drawbacks of existing FETC
systems. First, we separate training data into clean
and noisy partitions using the same method as in
AFET system (Ren et al., 2016). For these parti-

tions, we use simple yet effective non-parametric
variant of hinge loss function while training. To
avoid use of hand crafted features, we learn repre-
sentations for given entity mention and its context.

Additionally, we investigate effectiveness of us-
ing transfer learning (Pratt, 1993) for FETC task
both at feature and model level. We show that
feature level transfer learning can be used to im-
prove performance of other FETC system such as
AFET, by up to 4.5% in micro-F1 score. Simi-
larly, model level transfer learning can be used to
improve performance of the same model using dif-
ferent dataset by up to 3.8% in micro-F1 score.

Our contributions can be summarized as fol-
lows:

1. We propose a simple neural network model
that learns representations for entity mention
and its context, and incorporate noisy label
information using a variant of non-parametric
hinge loss function. Experimental results on
two publicly available datasets demonstrate
the effectiveness of proposed model, with an
average relative improvement of 2.69% in
micro-F1 score.

2. We investigate the use of feature level and
model level transfer-learning strategies in the
domain of the FETC task. The proposed
transfer learning strategies further improve
the state-of-the-art on BBN dataset by 3.8%
in micro-F1 score.

2 Related Work

Ling et al. (2012) proposed the first system for
FETC task, which used 112 overlapping labels.
They used linear classifier perceptron for multi-
label classification. Yosef et al. (2012) used multi-
ple binary SVM classifiers in a hierarchy, to clas-
sify an entity mention to a set of 505 types. While
the initial work assumed that all labels present in a
training dataset for an entity mention are correct,
Gillick et al. (2014) introduced context dependent
FETC and proposed a set of heuristics for pruning
labels that might not be relevant given the entity
mention’s local context. Yogatama et al. (2015)
proposed an embedding based model where user-
defined features and labels were embedded into a
low dimensional feature space to facilitate infor-
mation sharing among labels.

Shimaoka et al. (2016) proposed an attentive
neural network model that used LSTMs to en-
code entity mention’s context and used an atten-

798



(a) α models label-label cor-
relation. Higher the α, lower
is the margin between non-
correlation labels.

(b) During inference, labels
above this threshold are pre-
dicted as positive.

Figure 2: Effect of change of parameters on
AFET’s performance.

tion mechanism to allow the model to focus on rel-
evant expressions in the entity mention’s context.
However, the model assumed that all labels ob-
tained via distant supervision are correct. In con-
trast, our model does not assume that all labels are
correct. To learn entity representation, we propose
a scheme which is simpler yet more effective.

Most recently, Ren et al. (2016) have proposed
AFET, an FETC system. AFET separates the
loss function for clean and noisy entity mentions.
AFET uses label-label correlation information ob-
tained by given data in its parametric loss function
(model parameter α). During inference, AFET
uses a threshold to separate positive types from
negative types (similarity threshold parameter d).
However, AFET’s loss function is sensitive to
change in parameters, which are data dependent.
Figure 2 shows the effect of parameter α and
d, on AFET performance evaluated on different
datasets. In contrast, our model uses a simple yet
effective variant of hinge loss function. This func-
tion does not need to tune the similarity threshold.

Transfer learning is well applied to many NLP
applications, such as cross-domain document clas-
sification (Shi et al., 2010), multi-lingual word
clustering (Täckström et al., 2012) and sentiment
classification (Mou et al., 2016). Initialization of
word vectors with pre-trained word vectors in neu-
ral network models can be considered as one of the
best example of transfer learning in NLP. Wang
et al. (2015) provide a broad overview of transfer
learning techniques used for language processing.

3 The Proposed Model

3.1 Problem description

Our task is to automatically classify type informa-
tion of entity mentions present in natural language

Figure 3: The system overview.

sentences. Figure 3 shows a general overview of
our proposed approach.
Input: The input to the model is a training and
testing corpus consisting of a set of sentences on
which entity mentions have been identified. In
training corpus, every entity mention will have
corresponding labels according to a given hierar-
chy. Formally, a training corpus Dtrain consists
of a set of sentences, S = {si}Ni=1. Each sen-
tence si will have one or more entity mentions de-
noted by mij,k, where j and k denotes indices of
start and end tokens, respectively. SetM consists
of all the entity mentions mij,k. For every entity
mention mij,k, there will be a corresponding label
vector lij,k ∈ {0, 1}K , which is a binary vector,
where lij,kt = 1 if t

th type is true otherwise it will
be zero. K denotes the total number of labels in a
given hierarchy Ψ. Testing corpus Dtest will only
contain sentences and entity mentions.
Output: For entity mentions in testing corpus
Dtest, predict their corresponding labels.

3.2 Training set partition

Similar to AFET, we partition the mention setM
of training corpus Dtrain into two parts, a setMc,
consisting only of clean entity mentions and a set
Mn, consisting only of noisy entity mentions. An
entity mention mij,k is said to be clean if its labels
lij,k belong to only a single path (not necessary to
be leaf) in the hierarchy Ψ, that is its labels are
not ambiguous; otherwise, it is noisy. For exam-
ple, as per hierarchy given in figure 1, an entity
mention with labels person, artist and politician
will be considered as noisy, whereas entity men-
tion with labels person, artist and actor will be
considered as clean.

799



Figure 4: The architecture of the proposed model.

3.3 Feature representations

Mention representation: This representation
captures information about entity mention’s mor-
phology and orthography. We decompose an en-
tity mention into character sequence, and use a
vanilla LSTM encoder (Hochreiter and Schmidhu-
ber, 1997) to encode character sequences to a fixed
dimensional vector. Formally, for entity mention
mij,k, we decompose it into a sequence of charac-
ter tokens cij,k1 , c

i
j,k2

, . . . ,cij,k|mi
j,k
|
, where |mij,k|

denotes the total number of characters present in
the entity mention. For entity mention containing
multiple tokens, we join these tokens with a space
in between tokens. Every character will have cor-
responding vector representation in a lookup table
for characters. The character sequence is then fed
one by one to a LSTM encoder, and the final out-
put is used as a feature representation for entity
mention mij,k. We denote this process by a func-
tion Fm : M → RDm , where Dm is the num-
ber of dimensions for mention representation. The
whole process is illustrated in figure 4 (Mention
representation).
Context representation: This representation cap-
tures information about the context surrounding
the entity mention. Context representation is fur-
ther divided into two parts, left and right context
representation. The left context consists of a se-
quence of tokens within a sentence from the start
of a sentence till the last token of entity men-
tion. The right context consists of a sequence

of tokens from the start of entity mention till the
end of a sentence. We use bi-directional LSTM
encoders (Graves et al., 2013) to encode token
level sequences of both context to a fixed dimen-
sional vector. Formally, for an entity mentionmij,k
present in a sentence si, decompose si into a se-
quence of tokens si1, s

i
2, . . . , s

i
k for the left context,

and sij , s
i
j+1, . . . , s

i
|si| for the right context, where

|si| denotes the number of tokens in the sentence.
Every token will have a corresponding vector rep-
resentation in a lookup tables for token. The token
sequence is then fed one by one to a bi-directional
LSTM encoder, and the final output will be used as
feature representation. We denote this whole pro-
cess by function Flc : (M,S) → RDlc for com-
puting left context and Frc : (M,S) → RDrc for
computing right context. Dlc andDrc are the num-
ber of dimensions for the left context and the right
context representation, respectively. The whole
process is illustrated in figure 4 (Left and right
context representation).

The context representation described above is
slightly different from what was proposed in (Shi-
maoka et al., 2016), here we include entity men-
tion tokens within both left and right context, to
explicitly encode context relative to an entity men-
tion.

In the end, we concatenate entity mention and
its context representation into a single Df dimen-
sional vector, where Df = Dm + Dlc + Drc.
This complete process is denoted by a function

800



F : (M,S)→ RDf given by:

F (mij,k, s
i) = Fm(mij,k)⊕ Flc(mij,k, si)⊕ Frc(mij,k, si)

(1)
where ⊕ denotes vector concatenation. For
brevity, we will now omit the use of subscript j, k
from mij,k and l

i
j,k, and will use f

i to denote fea-
ture representation for entity mention and its con-
text obtained via equation 1.

3.4 Feature and label embeddings
Similar to Yogatama et al. (2015) and Ren et
al. (2016), we embed feature representations and
labels in a same dimensional space such that an
object is embedded closer to the objects that share
similar types than the objects that do not. For-
mally, we are trying to learn linear mapping func-
tions φM : RDf → RDe and φL : RDK → RDe ,
where De is the size of embedding space. These
mappings are given by:

φM(f i) = f i
T
U ; φL(lit) = l

iT

t V (2)

where, U ∈ RDf×De and V ∈ RDK×De are pro-
jection matrices for features representations and
type labels respectively and lit is one-hot vector
representation for label t.
We assign a score to each label type t and feature
vector as a dot product of their embeddings. For-
mally, we denote a score as:

s(f i, lit) = φM(f
i) · φL(lit) (3)

3.5 Optimization
We use two different loss functions to model clean
and noisy entity mentions. For clean entity men-
tions, we use a hinge loss function. The intuition
is simple: maintain a margin, centered at zero,
between positive and negative type scores. The
scores are computed by similarity between an en-
tity mention and label types (eq. 3). Hinge loss
function has two advantages. First, it intuitively
seprates positive and negative labels during infer-
ence. Second, it is independent of data dependent
parameter. Formally, for a given entity mention
mi and its label li we compute the associated loss
as given by:

Lc(mi, li) =
∑
t∈γ

max(0, 1− s(mi, lit))

+
∑
t∈γ̄

max(0, 1 + s(mi, lit)) (4)

where γ and γ̄ are set of indices that have positive
and negative labels respectively.

For noisy entity mentions, we propose a variant
of a hinge loss where, like Lc, score for all neg-
ative labels should go below −1. However, for
positive labels, as we don’t know which labels are
relevant to entity mention’s local context, we pro-
pose that the maximum score from the set of given
positive labels should be greater than one. This
maintains a margin between all negative types and
the most relevant positive type. Formally, noisy
label loss, Ln is defined as:

Ln(mi, li) =
∑
t∈γ̄

max(0, 1 + s(mi, lit))

+ max(0, 1− s(mi, lit∗));
t∗ = arg max

t∈γ s(m
i, lit) (5)

Again, using this loss function makes it intuitive
to set a threshold of zero during inference.

These loss functions are different from the loss
functions used in (Yogatama et al., 2015; Ren et
al., 2016) in a way that, we make strict absolute
criteria to distinguish between positive and neg-
ative labels. Whereas in (Yogatama et al., 2015;
Ren et al., 2016) positive labels should have a
higher score than negative labels. As their scoring
is relative, the final result varies on the threshold
used to separate positive and negative labels.

To train the partitioned dataset together, we for-
mulate the joint objective problem as:

min
θ
O =

∑
m∈Mc

Lc(m, l) +
∑

m∈Mn
Ln(m, l) (6)

where θ is the collection of all model parame-
ters that needs to be learned. To jointly optimize
the objective O, we use Adam (Kingma and Ba,
2014), a stochastic gradient-based optimization al-
gorithm.

3.6 Inference
For every entity mention in setM from Dtest, we
perform a top-down search in the given type hi-
erarchy Ψ, and estimate the correct type path Ψ∗.
Starting from the tree root, we recursively com-
pute the best type among node’s children by com-
puting its score with obtained feature representa-
tions. We select the node that has maximum score
among other nodes. We continue this process till
a leaf node is encountered or the score associated

801



with a node falls below an absolute threshold zero.
The thresold is fixed across all datasets used.

3.7 Transfer learning

We want to investigate, whether the feature repre-
sentations learnt for an entity mention are useful.
We study what contribution these feature repre-
sentations make to an existing feature engineering
based method such as AFET. We learn the pro-
posed model on one training dataset, namely Wiki
dataset, which has the highest number of entity
mentions among other datasets and use this model
to generate representations that is F (mij,k, s

i) for
another training and testing data. These rep-
resentations, which are Df dimensional vectors,
are used as feature for an existing state-of-the-art
model, AFET, in place of the hand-crafted features
that were originally used. AFET model is then
trained using these feature representations. We
call this as feature level transfer learning. On the
other hand, we also evaluate model level transfer
learning, where we initialize weights of LSTM en-
coders for a new dataset with the weights learnt
from the model trained on another dataset, namely
Wiki dataset.

4 Experiments

4.1 Datasets used

We evaluate the proposed model on three publicly
available datasets, provided in a pre-processed to-
kenized format by Ren et al. (2016). Statistics of
the datasets used in this work are shown in Table 1.
The details of the datasets are as follows:
Wiki/FIGER(GOLD): The training data consists
of Wikipedia sentences and was automatically
generated in distant supervision paradigm, by
mapping hyperlinks in Wikipedia articles to Free-
base. The test data, mainly consisting of sentences
from news reports, was manually annotated as de-
scribed in (Ling and Weld, 2012).
OntoNotes: OntoNotes dataset consists of sen-
tences from newswire documents present in
OntoNotes text corpus (Weischedel et al., 2013).
DBpedia spotlight (Daiber et al., 2013) was used
to automatically link entity mention in sentences
to Freebase. For this corpus, manually annotated
test data was shared by Gillick et al. (2014).
BBN: BBN dataset consists of sentences from
Wall Street Journal articles and is completely man-
ually annotated (Weischedel and Brunstein, 2005).
Please refer to (Ren et al., 2016) for more details

Datasets Wiki/FIGER(GOLD) OntoNotes BBN
# types 128 89 47
# training mentions 2690286 220398 86078
# testing mentions 563 9603 13187
% clean training mentions 64.58 72.61 75.92
% clean testing mentions 88.28 94.00 100
% pronominal testing mentions1 0.00 6.78 0.00
Max hierarchy depth 2 3 2

Table 1: Statistics of the datasets used in this work.

of the datasets.

4.2 Evaluation settings
4.2.1 Baselines
We compared the proposed model with
state-of-the-art entity classification meth-
ods2: (1) FIGER (Ling and Weld, 2012);
(2) HYENA (Yosef et al., 2012); (3) AFET-
NoCo (Ren et al., 2016): AFET without data
based label-label correlation modeled in loss
function; (4) AFET-CoH (Ren et al., 2016):
AFET with hierarchy based label-label correlation
modeled in loss function; (5) AFET (Ren et al.,
2016); (6) Attentive (Shimaoka et al., 2016): An
attentive neural network based model.

We compare these baselines with variants of
our proposed model: (1) our: complete model;
(2) our-AllC assuming all mentions are clean; (3)
our-NoM without mention representation.

4.2.2 Experimental setup
We use Accuracy or Strict-F1 score, Macro-
averaged F1 score, and Micro-averaged F1 score
as metrics for evaluation. Existing methods for
FETC use same measures (Ling and Weld, 2012;
Yogatama et al., 2015; Shimaoka et al., 2016; Ren
et al., 2016). We removed entity mentions that do
not have any label in training as well as test set.
We also remove entity mentions that have spuri-
ous indices (i.e entity mention length of 0).3 For
all the three datasets, we randomly sampled 10%
of the test set, and use it as a development set, on
which we tune model parameters. The remaining
90% is used for final evaluation. For all our exper-
iments, we train each model using same hyperpa-
rameters five times and report their performance in
terms of micro-F1 score on the development set as

1We considered an entity mention as pronominal, if all of
its tokens have POS tag as pronoun.

2Whenever possible, the baselines result are reported
from (Ren et al., 2016), otherwise we re-implemented base-
line methods based on description available in corresponding
papers.

3The code to replicate the work is available at https:
//github.com/abhipec/fnet

802



Figure 5: These box-plots show the performance of different baselines on validation set. The red line,
boxes and whiskers indicate the median, quartiles and range.

shown in Figure 5. On Wiki dataset, we observed
a large variance in performance as compared to
other two datasets. This might be because of the
fact that Wiki dataset has a very small develop-
ment set. From each of these five runs, we pick
the best performing model based on the develop-
ment set and report its result on the test set.
Hyperparameter setting: All the neural network
based models in this paper used 300 dimensional
pre-trained word embeddings distributed by Pen-
nington et al. (2014). The hidden-layer size of
word level bi-directional LSTM was 100, and that
of character level LSTM was 200. Vectors for
character embeddings were randomly initialized
and were of size 200. We use dropout with the
probability of 0.5 on the output of LSTM en-
coders. The embedding dimension used was 500.
We use Adam (Kingma and Ba, 2014) as optimiza-
tion method with learning rate of 0.0005-0.001
and mini-batch size in the range of 800 to 1500.
The proposed model and some of the baselines
were implemented using TensorFlow4 framework.

4.3 Transfer learning

In feature level transfer learning, we use the
best performing proposed model trained on Wiki
dataset to generate representations that is Df di-
mensional vector for every entity mention present
in the train, development, and test set of the BBN
and the OntoNotes dataset. Figure 4 illustrates an
example for the encoding process. Then we use
these representations as a feature vector in place
of the user-defined features and train the AFET

4http://tensorflow.org/

model. Its hyper-parameters were tuned on the de-
velopment set. These results are shown in table 2
as feature level transfer-learning.

In model level transfer learning, we use the
learnt weights of LSTM encoders from the best
performing proposed model trained on Wiki
dataset and initialize the LSTM encoders of the
same model with these weights while training on
BBN and OntoNotes datasets. These results are
shown in table 2 as model level transfer learn-
ing.

4.4 Performance comparison and analysis
Table 2 shows the results of the proposed method,
its variants and the baseline methods.
Comparison with other feature learning meth-
ods: The proposed model and its variants (our-
AllC, our-NoM) perform better than the existing
feature learning method by Shimaoka et al. (2016)
(Attentive), consistently on all datasets. This
indicates benefits of the proposed representation
scheme and joint learning of representation and la-
bel embedding.
Comparison with feature engineering meth-
ods: The proposed model performs better than
the existing feature engineered methods (FIGER,
HYENA, AFET-NoCo, AFET-CoH) consis-
tently across all datasets on Micro-F1 and Macro-
F1 evaluation metrics. These methods do not
model label-label correlation based on data. In
comparison with AFET, the proposed model out-
performs AFET on Wiki and BBN dataset in terms
of Micro-F1 evaluation metric. This indicates ben-
efits of feature learning as well as data driven
label-label correlation. We do a type-wise perfor-

803



Typing methods
Wiki/FIGER(GOLD) OntoNotes BBN

Acc. Ma-F1 Mi-F1 Acc. Ma-F1 Mi-F1 Acc. Ma-F1 Mi-F1
FIGER* (Ling and Weld, 2012) 0.474 0.692 0.655 0.369 0.578 0.516 0.467 0.672 0.612
HYENA* (Yosef et al., 2012) 0.288 0.528 0.506 0.249 0.497 0.446 0.523 0.576 0.587
AFET-NoCo* (Ren et al., 2016) 0.526 0.693 0.654 0.486 0.652 0.594 0.655 0.711 0.716
AFET-CoH* (Ren et al., 2016) 0.433 0.583 0.551 0.521 0.680 0.609 0.657 0.703 0.712
AFET* (Ren et al., 2016) 0.533 0.693 0.664 0.551 0.711 0.647 0.670 0.727 0.735
AFET†‡ (Ren et al., 2016) 0.509 0.689 0.653 0.553 0.712 0.646 0.683 0.744 0.747
Attentive† (Shimaoka et al., 2016) 0.581 0.780 0.744 0.473 0.655 0.586 0.484 0.732 0.724
our-AllC† 0.662 0.805 0.770 0.514 0.672 0.626 0.655 0.736 0.752
our-NoM† 0.646 0.808 0.768 0.521 0.683 0.626 0.615 0.742 0.755
our† 0.658 0.812 0.774 0.522 0.685 0.633 0.604 0.741 0.757
model level transfer-learning† - - - 0.531 0.684 0.637 0.645 0.784 0.795
feature level transfer-learning† - - - 0.471 0.689 0.635 0.733 0.791 0.792

Table 2: Performance analysis of entity classification methods on the three datasets.

mance comparison on OntoNotes dataset in sub-
section 4.5.
Comparison with variants of our model: The
proposed model performs better on all dataset as
compared to our-AllC in terms of micro-F1 score.
However, we find the performance difference on
Wiki and OntoNotes dataset is not statistically sig-
nificant. We investigated it further and found that
across all three datasets, there exist only few entity
types for which more than 85% of entity mentions
are noisy. These types consist of approximately 3-
4% of test set, and our model fails on these types
(zero micro-F1 score). However, our-AllC per-
forms relatively well on these types. Examples of
such types are: /building, /person/political figure,
/GPE/STATE PROVINCE. This indicates two lim-
itations of the proposed model. First, the separat-
ing of clean and noisy mentions based on the hier-
archy has its own inherent limitation of assuming
labels within a path are correct. Second, our model
learns better if more clean examples are available
at the cost of not learning very noisy types. We
will try to address these limitations in our future
work. Compared with our-NoM, the proposed
model performs slightly better across all datasets
in terms of micro-F1 score.
Feature level transfer learning analysis: We
observed 4.5% performance increase in micro-
F1 score of AFET on BBN dataset, after replac-
ing hand-crafted features with feature representa-
tions generated by the proposed model. This in-
dicates usefulness of the learnt feature representa-
tions. However, if we repeat the same process with
OntoNotes dataset, there is only a subtle change in
performance. This is majorly because of the data
distribution of OntoNotes dataset is different from

that of Wiki dataset. This issue is discussed in the
next subsection.
Model level transfer learning analysis: In model
level transfer learning, sharing knowledge from
similar dataset (Wiki to BBN) increases the perfor-
mance by 3.8% in terms of micro-F1 score. How-
ever, sharing knowledge from Wiki to OntoNotes
dataset slightly increases the performance by 0.4%
in terms of micro-F1 score.

4.5 Case analysis: OntoNotes dataset

We observed three things; (i) all models perform
relatively poor on OntoNotes dataset compared
to their performance on other two datasets; (ii)
the proposed model outperforms other models in-
cluding AFET on the other two datasets, but gave
worse performance on OntoNotes dataset; (iii) the
two variants of transfer learning significantly im-
prove performance of the proposed model on the
BBN dataset but resulted in only a subtle perfor-
mance change on OntoNotes dataset.

Statistics of the dataset (Table 1) indicates that
presence of pronominal or other kinds of men-
tions are relatively higher in OntoNotes (6.78%
in test set) than the other two datasets (0% in
test set). Examples of such mentions are 100
people, It, the director, etc. Table 3 shows 20
randomly sampled entity mentions from test set
of OntoNotes datasets. Some of these mentions
are very generic and likely to be dependent on

*These results are from (Ren et al., 2016) that also uses
10% of the test set as development set and the remaining for
evaluation.

‡We used the publicly available code distributed by Ren
et al. (2016).

†All of these results are on exact same train, development
and test set.

804



previous sentences. As all the methods use fea-
tures solely based on the current sentence, they
fail to transfer cross-sentence boundary knowl-
edge. Removing pronominal mentions from test
set increases the performance of all feature learn-
ing methods by around 3%.

his thousands of angry people
A reporter export competitiveness
Freddie Mac Messrs. Malson and Seelenfreund
the numbers Hollywood and New York
his explanation April
volatility This institution
their hands the 1987 crash
it January 4th
Macau investment enterprises
France any means

Table 3: 20 randomly sampled entity mentions
present in the test set of OntoNotes dataset.

Next we analyse where the proposed model is
failing as compared to AFET. For this, we look
at type-wise performance for the top-10 most fre-
quent types in the OntoNotes test dataset. Results
are shown in Table 4. Compared to AFET, the
proposed model performs better in all types ex-
cept other in the top-10 frequent types. The other
type, which is dominant in test set (42.6% of en-
tity mentions are of type other) and is a collection
of multiple broad subtypes such as product, event,
art, living thing, food. Performance of AFET sig-
nificantly drops (AFET-NoCo) when data-driven
label-label correlation is ignored, which indicates
that modeling data-driven correlation helps. How-
ever, as shown in Figure 2a, the use of label-label
correlation depends on appropriate values of pa-
rameters which vary from one dataset to another.

Label type
Support our AFET

Prec. Rec. F-1 Prec. Rec. F-1
/other 42.6% 0.838 0.809 0.823 0.774 0.962 0.858
/organization 11.0% 0.588 0.490 0.534 0.903 0.273 0.419
/person 9.9% 0.559 0.467 0.508 0.669 0.352 0.461
/organization/company 7.8% 0.932 0.166 0.282 1.0 0.127 0.225
/location 7.5% 0.687 0.796 0.737 0.787 0.609 0.687
/organization/government 2.1% 0 0 0 0 0 0
/location/country 2.0% 0.783 0.614 0.688 0.838 0.498 0.625
/other/legal 1.8% 0 0 0 0 0 0
/location/city 1.8% 0.919 0.610 0.733 0.816 0.637 0.715
/person/political figure 1.6% 0 0 0 0 0 0

Table 4: Performance analysis of the proposed
model and AFET on top 10 (in terms of type fre-
quency) types present in OntoNotes dataset.

5 Conclusion and Future Work

In this paper, we propose a neural network based
model for the task of fine-grained entity classi-
fication. The proposed model learns represen-
tations for entity mention, its context and in-
corporate label noise information in a variant of
non-parametric hinge loss function. Experiments
show that the proposed model outperforms exist-
ing state-of-the-art models on two publicly avail-
able datasets without explicitly tuning data depen-
dent parameters.

Our analysis indicates the following observa-
tions. First, OntoNotes dataset has a different dis-
tribution of entity mentions compared with other
two datasets. Second, if data distribution is simi-
lar, then transfer learning is very helpful. Third,
incorporating data-driven label-label correlation
helps in the case of labels of mixed types. Fourth,
there is an inherent limitation in assuming all la-
bels to be clean if they belong to the same path of
the hierarchy. Fifth, the proposed model fails to
learn label types that are very noisy.

Future work could analyse the effect of label
noise reduction techniques on the proposed model,
revisiting the definition of clean and noisy labels
and modeling label-label correlation in a princi-
pled way that is not dependent on dataset specific
parameters.

Acknowledgments

We thank the anonymous reviewers for their in-
valuable and insightful comments. Abhishek is
supported by MHRD fellowship, Government of
India. We acknowledge the use of computing re-
sources made available from the Board of Re-
search in Nuclear Science (BRNS), Dept. of
Atomic Energy (DAE), Govt. of India spon-
sered project (No.2013/13/8-BRNS/10026) by Dr.
Aryabartta Sahu at Department of Computer Sci-
ence and Engineering, IIT Guwahati.

References
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens

Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open data.
In Proceedings of the 6th International The Seman-
tic Web and 2nd Asian Conference on Asian Seman-
tic Web Conference, ISWC’07/ASWC’07, pages
722–735, Berlin, Heidelberg. Springer-Verlag.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A

805



collaboratively created graph database for structur-
ing human knowledge. In Proceedings of the 2008
ACM SIGMOD International Conference on Man-
agement of Data, SIGMOD ’08, pages 1247–1250,
New York, NY, USA. ACM.

Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In
In Proceedings of the Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora, pages 100–110.

Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Proceedings of the Sev-
enth International Conference on Intelligent Systems
for Molecular Biology, pages 77–86. AAAI Press.

Joachim Daiber, Max Jakob, Chris Hokamp, and
Pablo N. Mendes. 2013. Improving efficiency and
accuracy in multilingual entity extraction. In Pro-
ceedings of the 9th International Conference on Se-
mantic Systems, I-SEMANTICS ’13, pages 121–
124, New York, NY, USA. ACM.

Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko
Horn, Ni Lao, Kevin Murphy, Thomas Strohmann,
Shaohua Sun, and Wei Zhang. 2014. Knowl-
edge vault: A web-scale approach to probabilistic
knowledge fusion. In Proceedings of the 20th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ’14, pages 601–
610, New York, NY, USA. ACM.

Li Dong, Furu Wei, Hong Sun, Ming Zhou, and Ke Xu.
2015. A hybrid neural model for type classification
of entity mentions. In Proceedings of the 24th In-
ternational Conference on Artificial Intelligence, IJ-
CAI’15, pages 1243–1249. AAAI Press.

Dan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse
Kirchner, and David Huynh. 2014. Context-
dependent fine-grained entity type tagging. arXiv
preprint arXiv:1412.1820.

Alex Graves, Abdel-rahman Mohamed, and Geoffrey
Hinton. 2013. Speech recognition with deep recur-
rent neural networks. In 2013 IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing, pages 6645–6649. IEEE.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780, November.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Mitchell Koch, John Gilmer, Stephen Soderland, and
Daniel S. Weld. 2014. Type-aware distantly su-
pervised relation extraction with linked arguments.
In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1891–1901, Doha, Qatar, October.
Association for Computational Linguistics.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180, Prague, Czech Republic,
June. Association for Computational Linguistics.

Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proceedings of the 19th International Con-
ference on Computational Linguistics - Volume 1,
COLING ’02, pages 1–7, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Thomas Lin, Mausam, and Oren Etzioni. 2012. No
noun phrase left behind: Detecting and typing un-
linkable entities. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 893–903, Jeju Island, Korea,
July. Association for Computational Linguistics.

Xiao Ling and Daniel S. Weld. 2012. Fine-grained
entity recognition. In Proceedings of the Twenty-
Sixth AAAI Conference on Artificial Intelligence,
AAAI’12, pages 94–100. AAAI Press.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The stanford corenlp natural language pro-
cessing toolkit. In Proceedings of 52nd Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 55–60, Bal-
timore, Maryland, June. Association for Computa-
tional Linguistics.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003–1011, Suntec, Singapore, August. Association
for Computational Linguistics.

T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar,
J. Betteridge, A. Carlson, B. Dalvi, M. Gardner,
B. Kisiel, J. Krishnamurthy, N. Lao, K. Mazaitis,
T. Mohamed, N. Nakashole, E. Platanios, A. Rit-
ter, M. Samadi, B. Settles, R. Wang, D. Wijaya,
A. Gupta, X. Chen, A. Saparov, M. Greaves, and
J. Welling. 2015. Never-ending learning. In Pro-
ceedings of the Twenty-Ninth AAAI Conference on
Artificial Intelligence, AAAI’15, pages 2302–2310.
AAAI Press.

Lili Mou, Ran Jia, Yan Xu, Ge Li, Lu Zhang, and Zhi
Jin. 2016. Distilling word embeddings: An encod-
ing approach. In Proceedings of the 25th ACM Inter-
national on Conference on Information and Knowl-
edge Management, CIKM ’16, pages 1977–1980,
New York, NY, USA. ACM.

806



Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543, Doha,
Qatar, October. Association for Computational Lin-
guistics.

Lorien Y. Pratt. 1993. Discriminability-based trans-
fer between neural networks. In Advances in Neural
Information Processing Systems 5, pages 204–211,
San Francisco, CA, USA. Morgan Kaufmann Pub-
lishers Inc.

Lev Ratinov and Dan Roth. 2009. Design chal-
lenges and misconceptions in named entity recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 147–155, Boulder, Colorado,
June. Association for Computational Linguistics.

Xiang Ren, Wenqi He, Meng Qu, Lifu Huang, Heng
Ji, and Jiawei Han. 2016. Afet: Automatic fine-
grained entity typing by hierarchical partial-label
embedding. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1369–1378, Austin, Texas, Novem-
ber. Association for Computational Linguistics.

Lei Shi, Rada Mihalcea, and Mingjun Tian. 2010.
Cross language text classification by model trans-
lation and semi-supervised learning. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1057–1067,
Cambridge, MA, October. Association for Compu-
tational Linguistics.

Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and
Sebastian Riedel. 2016. An attentive neural ar-
chitecture for fine-grained entity type classification.
In Proceedings of the 5th Workshop on Automated
Knowledge Base Construction, pages 69–74, San
Diego, CA, June. Association for Computational
Linguistics.

Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowl-
edge. In Proceedings of the 16th International Con-
ference on World Wide Web, WWW ’07, pages 697–
706, New York, NY, USA. ACM.

Oscar Täckström, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 477–
487, Montréal, Canada, June. Association for Com-
putational Linguistics.

Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In

Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of the Seventh Conference on Natural Lan-
guage Learning at HLT-NAACL 2003, pages 142–
147.

Dong Wang and Thomas Fang Zheng. 2015. Trans-
fer learning for speech and language processing.
In 2015 Asia-Pacific Signal and Information Pro-
cessing Association Annual Summit and Conference
(APSIPA), pages 1225–1237. IEEE.

Ralph Weischedel and Ada Brunstein. 2005.
BBN Pronoun Coreference and Entity Type Cor-
pus LDC2005T33. Linguistic Data Consortium,
Philadelphia, 112.

Ralph Weischedel, Martha Palmer, Mitchell Marcus,
Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, et al. 2013. Ontonotes release 5.0
ldc2013t19. Linguistic Data Consortium, Philadel-
phia, PA.

Dani Yogatama, Daniel Gillick, and Nevena Lazic.
2015. Embedding methods for fine grained entity
type classification. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 2:
Short Papers), pages 291–296, Beijing, China, July.
Association for Computational Linguistics.

Mohamed Amir Yosef, Sandro Bauer, Johannes Hof-
fart, Marc Spaniol, and Gerhard Weikum. 2012.
HYENA: Hierarchical type classification for entity
names. In Proceedings of COLING 2012: Posters,
pages 1361–1370, Mumbai, India, December. The
COLING 2012 Organizing Committee.

807


