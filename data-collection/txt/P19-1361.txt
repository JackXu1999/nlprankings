



















































Incremental Learning from Scratch for Task-Oriented Dialogue Systems


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3710–3720
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3710

Incremental Learning from Scratch for Task-Oriented Dialogue Systems

Weikang Wang1,2, Jiajun Zhang1,2, Qian Li3,
Mei-Yuh Hwang3, Chengqing Zong1,2,4 and Zhifei Li3

1 National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China
2 University of Chinese Academy of Sciences, Beijing, China

3 Mobvoi, Beijing, China
4 CAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China

{weikang.wang, jjzhang, cqzong}@nlpr.ia.ac.cn
{qli, mhwang, zfli}@mobvoi.com

Abstract

Clarifying user needs is essential for exist-
ing task-oriented dialogue systems. How-
ever, in real-world applications, developers
can never guarantee that all possible user de-
mands are taken into account in the design
phase. Consequently, existing systems will
break down when encountering unconsidered
user needs. To address this problem, we pro-
pose a novel incremental learning framework
to design task-oriented dialogue systems, or
for short Incremental Dialogue System (IDS),
without pre-defining the exhaustive list of user
needs. Specifically, we introduce an uncer-
tainty estimation module to evaluate the con-
fidence of giving correct responses. If there is
high confidence, IDS will provide responses to
users. Otherwise, humans will be involved in
the dialogue process, and IDS can learn from
human intervention through an online learning
module. To evaluate our method, we propose
a new dataset which simulates unanticipated
user needs in the deployment stage. Experi-
ments show that IDS is robust to unconsidered
user actions, and can update itself online by
smartly selecting only the most effective train-
ing data, and hence attains better performance
with less annotation cost.1

1 Introduction

Data-driven task-oriented dialogue systems have
been a focal point in both academic and industry
research recently. Generally, the first step of build-
ing a dialogue system is to clarify what users are
allowed to do. Then developers can collect data
to train dialogue models to support the defined ca-
pabilities. Such systems work well if all possi-
ble combinations of user inputs and conditions are
considered in the training stage (Paek and Pierac-
cini, 2008; Wang et al., 2018). However, as shown

1https://github.com/Leechikara/
Incremental-Dialogue-System

What should I do to update the operating system? 

Our products support Android and iOS. Which one do you prefer?

Hi, I can help you find the most suitable product.

Figure 1: An example of task-oriented dialogue sys-
tem. The system is designed to guide users to find a
suitable product. Thus, when encountering unconsid-
ered user needs such as ”how to update the operating
system”, the system will give unreasonable responses.

in Fig. 1, if users have unanticipated needs, the
system will give unreasonable responses.

This phenomenon is mainly caused by a biased
understanding of real users. In fact, before system
deployment, we do not know what the customers
will request of the system. In general, this prob-
lem can be alleviated by more detailed user stud-
ies. But we can never guarantee that all user needs
are considered in the system design. Besides, the
user inputs are often diverse due to the complexity
of natural language. Thus, it is impossible to col-
lect enough training samples to cover all variants.
Consequently, the system trained with biased data
will not respond to user queries correctly in some
cases. And these errors can only be discovered af-
ter the incident.

Since the real user behaviors are elusive, it is
obviously a better option to make no assumptions
about user needs than defining them in advance.
To that end, we propose the novel Incremental
Dialogue System (IDS). Different from the exist-
ing training-deployment convention, IDS does not
make any assumptions about the user needs and
how they express intentions. In this paradigm, all
reasonable queries related to the current task are
legal, and the system can learn to deal with user
queries online.

Specifically, after the user sends a query to our
system, we use an uncertainty estimation module

https://github.com/Leechikara/Incremental-Dialogue-System
https://github.com/Leechikara/Incremental-Dialogue-System


3711

to evaluate the confidence that the dialogue model
can respond correctly. If there is high confidence,
IDS will give its response to the user. Otherwise,
human will intervene and provide a reasonable an-
swer. When humans are involved, they can select
a response from the current response candidates
or give a new response to the user. If a new an-
swer is provided, we add it to the system response
candidates. Then, the generated context-response
pair from humans will be fed into the dialogue
model to update the parameters by an online learn-
ing module. Through continuous interactions with
users after deployment, the system will become
more and more knowledgeable, and human inter-
vention will become less and less needed.

To evaluate our method, we build a new dataset
consisting of five sub-datasets (named SubD1,
SubD2, SubD3, SubD4 and SubD5) within the
context of customer services. Following the ex-
isting work (Bordes et al., 2016), our dataset is
generated by complicated and elaborated rules.
SubD1 supports the most limited dialogue scenar-
ios. Then each later sub-dataset covers more sce-
narios than its previous one. To simulate the unan-
ticipated user needs, we train the dialogue mod-
els on simpler datasets and test them on the harder
ones. Extensive experiments show that IDS is ro-
bust to the unconsidered user actions and can learn
dialogue knowledge online from scratch. Besides,
compared with existing methods, our approach
significantly reduces annotation cost.

In summary, our main contributions are three-
fold: (1) To the best of our knowledge, this is the
first work to study the incremental learning frame-
work for task-oriented dialogue systems. In this
paradigm, developers do not need to define user
needs in advance and avoid collecting biased train-
ing data laboriously. (2) To achieve this goal, we
introduce IDS which is robust to new user actions
and can extend itself online to accommodate new
user needs. (3) We propose a new benchmark
dataset to study the inconsistency of training and
testing in task-oriented dialogue systems.

2 Background and Problem Definition

Existing work on data-driven task-oriented dia-
logue systems includes generation based meth-
ods (Wen et al., 2016; Eric and Manning, 2017)
and retrieval based methods (Bordes et al., 2016;
Williams et al., 2017; Li et al., 2017). In this paper,
we focus on the retrieval based methods, because

they always return fluent responses.
In a typical retrieval based system, a user gives

an utterance xt to the system at the t-th turn. Let
(xt,1, ..., xt,N ) denote the tokens of xt. Then, the
system chooses an answer yt = (yt,1, ..., yt,M )
from the candidate response set R based on the
conditional distribution p(yt|Ct), where Ct =
(x1, y1, ..., xt−1, yt−1, xt) is the dialogue context
consisting of all user utterances and responses up
to the current turn.

By convention, the dialogue system is designed
to handle predefined user needs. And the users are
expected to interact with the system based on a
limited number of dialogue actions. However, pre-
defining all user demands is impractical and unex-
pected queries may be given to the system after the
system is deployed. In this work, we mainly focus
on handling this problem.

3 Incremental Dialogue System

As shown in Fig. 2, IDS consists of three main
components: dialogue embedding module, uncer-
tainty estimation module and online learning mod-
ule.

Dialogue 
Embedding

Data Pool

Uncertainty 
Estimation

MachineUser

Human

Utterance

Low 
Confidence

High 
Confidence

Response

Response

Context-Response 
Pair

Online Learning

tE C( )

Figure 2: An overview of the proposed IDS.

In the context of customer services, when the
user sends an utterance to the system, the dialogue
embedding module will encode the current con-
text into a vector. Then, the uncertainty estima-
tion module will evaluate the confidence of giving
a correct response. If there is high confidence, IDS
will give its response to the user. Otherwise, the
hired customer service staffs will be involved in
the dialogue process and provide a reasonable an-
swer, which gives us a new ground truth context-
response pair. Based on the newly added context-
response pairs, the system will be updated via the
online learning module.

3.1 Dialogue Embedding
Given dialogue context Ct in the t-th turn, we first
embed each utterance in Ct using a Gated Recur-
rent Unit (GRU) (Chung et al., 2014) based bidi-
rectional recurrent neural networks (bi-RNNs).



3712

The bi-RNNs transform each utterance2 x =
(w1, w2, ..., wN ) in Ct into hidden representation
H = (h1, h2, ..., hN ) as follows:

−→
h n = GRU(

−→
h n−1, φ

emb(wn))
←−
h n = GRU(

←−
h n+1, φ

emb(wn))

hn =
−→
h n ⊕

←−
h n

(1)

where φemb(wn) is the embedding of word wn.
To better encode a sentence, we use the self-

attention layer (Lin et al., 2017) to capture in-
formation from critical words. For each element
hn in bi-RNNs outputs, we compute a scalar self-
attention score as follows:

an = MLP(hn)
pn = softmax(an)

(2)

The final utterance representation E(x) is the
weighted sum of bi-RNNs outputs:

E(x) =
∑
n

pnhn (3)

After getting the encoding of each sentence in
Ct, we input these sentence embeddings to another
GRU-based RNNs to obtain the context embed-
ding E(Ct) as follows:

E(Ct) = GRU(E(x1), E(y1), ..., E(yt−1), E(xt)) (4)

3.2 Uncertainty Estimation

In the existing work (Williams et al., 2017; Bordes
et al., 2016; Li et al., 2017), after getting the con-
text representation, the dialogue system will give
a response yt to the user based on p(yt|Ct). How-
ever, the dialogue system may give unreasonable
responses if unexpected queries happen. Thus,
we introduce the uncertainty estimation module to
avoid such risks.

To estimate the uncertainty, we decompose the
response selection process as follows:

p(yt|Ct) =
∫
p(yt|z, Ct)p(z|Ct)dz (5)

As shown in Fig. 3(a), from the viewpoint of
probabilistic graphical models (Koller and Fried-
man, 2009), the latent variable z can be seen as
an explanation of the dialogue process. In an ab-
stract sense, given Ct, there is an infinite number
of paths z fromCt to yt. And p(yt|Ct) is an expec-
tation of p(yt|z, Ct) over all possible paths. If the

2We use x to represent each user utterance and y for each
response for simplicity. All utterances use the same encoder.

(a) (b)

tC ty

z

tC tr

z

Figure 3: Graphical models of (a) response selection,
and (b) online learning. The gray and white nodes rep-
resent the observed and latent variables respectively.

system has not seen enough instances similar to
Ct before, the encoding of Ct will be located in an
unexplored area of the dialogue embedding space.
Thus, the entropy of prior p(z|Ct) will be large.
If we sample latent variable z based on p(z|Ct)
multiple times and calculate p(yt|z, Ct), we can
find p(yt|z, Ct) has a large variance under differ-
ent sampled latent variables z.

Based on such intuitive analysis, we design the
uncertainty measurement for IDS. Specifically, we
assume that the latent variable z obeys a multi-
variate diagonal Gaussian distribution. Following
the reparametrization trick (Kingma and Welling,
2014), we sample � ∼ N (0, I) and reparameterize
z = µ+ σ · �. The mean and variance of the prior
p(z|Ct) can be calculated as:[

µ
log(σ2)

]
= MLP(E(Ct)) (6)

After sampling a latent variable z from the prior
p(z|Ct), we calculate the response probability for
each element in the current candidate response set
R. In IDS, R will be extended dynamically. Thus,
we address the response selecting process with the
ranking approach. For each response candidate,
we calculate the scoring as follows:

ρ(yt|z, Ct) = (E(Ct)⊕ z)TWE(yt)
p(yt|z, Ct) = softmax(ρ(yt|z, Ct))

(7)

where E(yt) is the encoding of yt ∈ R, and W is
the weight matrices.

To estimate the variance of p(yt|z, Ct) under
different sampled latent variables, we repeat the
above process K times. Assume that the probabil-
ity distribution over the candidate response set in
the k-th repetition is Pk and the average response
probability distribution ofK sampling is Pavg. We
use the Jensen-Shannon divergence (JSD) to mea-
sure the distance between Pk and Pavg as follows:

JSD(Pk||Pavg) =
1

2
(KL(Pk||Pavg) +KL(Pavg||Pk)) (8)



3713

(a)

0.7

0.1

0.1

...

0.1

0.65

0.2

...

0.1

0.15

0.6

...

...

(b)

0.1

0.12

0.11

...

0.12

0.15

0.13

...

0.1

0.11

0.12

...

...
tC tC

1z

2z

Kz

1z

2z

Kz

Figure 4: A toy example to show the uncertainty esti-
mation criterions. (a) means a large variance in the re-
sponse probability under different sampled latent vari-
ables. (b) means close weights to all response candi-
dates in the early stage of online learning.

where KL(P ||Q) is the Kullback-Leibler di-
vergence between two probability distributions.
Then, we get the average JSD as follows:

JSDavg =
1

K

K∑
k=1

JSD(Pk||Pavg) (9)

Because the average JSD can be used to measure
the degree of divergence of {P1, P2, ..., PK}, as
shown in Fig. 4(a), the system will refuse to re-
spond if JSDavg is higher than a threshold τ1.

However, the dialogue model tends to give close
weights to all response candidates in the early
stage of training, as shown in Fig. 4(b). It results in
a small average JSD but the system should refuse
to respond. Thus, we add an additional criterion
for the uncertainty measurement. Specifically, if
the maximum probability in Pavg is lower than a
threshold τ2, the system will refuse to respond.

3.3 Online Learning

If the confidence is high enough, IDS will give the
response with the maximum score in Pavg to the
user. Otherwise, the hired customer service staffs
will be asked to select an appropriate response
from the top T response candidates of Pavg or pro-
pose a new response if there is no appropriate can-
didate. If a new response is proposed, it will be
added to R. We denote the human response as rt.
Then, we can observe a new context-response pair
dt = (Ct, rt) and add it to the training data pool.

The optimization objective is to maximize the
likelihood of the newly added data dt. However,
as shown in Eq. 5, calculating the likelihood re-
quires an intractable marginalization over the la-
tent variable z. Fortunately, we can obtain its
lower bound (Hoffman et al., 2013; Miao et al.,

2016; Sohn et al., 2015) as follows:

L = Eq(z|dt) [log p(rt|z, Ct)]− KL(q(z|dt)||p(z|Ct))

≤ log
∫
p(rt|z, Ct)p(z|Ct)dz

= log p(rt|Ct)
(10)

where L is called evidence lower bound (ELBO)
and q(z|dt) is called inference network. The learn-
ing process of the inference network is shown in
Fig. 3(b).

Similar to the prior network p(z|Ct), the infer-
ence network q(z|dt) approximates the mean and
variance of the posterior p(z|dt) as follows:[

µ′

log(σ′2)

]
= MLP(E(Ct)⊕ E(rt)) (11)

whereE(Ct) andE(rt) denote the representations
of dialogue context and human response in cur-
rent turn, respectively. We use the reparametriza-
tion trick to sample z from the inference network
and maximize the ELBO by gradient ascent on a
Monte Carlo approximation of the expectation.

It is worth noting that tricks such as mixing dt
with the instances in the data pool and updating
IDS for a small number of epochs (Shen et al.,
2017) can be easily adopted to increase the uti-
lization of labeled data. But, in our experiments,
we find there is still a great improvement without
these tricks. To reduce computation load, we up-
date IDS with each dt only once in a stream-based
fashion and leave these tricks in our future work.

4 Construction of Experimental Data

To simulate the new unconsidered user needs, one
possible method is to delete some question types
in the training set of existing datasets (e.g., bAbI
tasks (Bordes et al., 2016)) and test these ques-
tions in the testing phase. However, the dialogue
context plays an important role in the response se-
lection. Simply deleting some turns of a dialogue
will result in a different system response. For ex-
ample, in bAbI Task5, deleting those turns on up-
dating api calls will result in a different recom-
mended restaurant. Thus, we do not modify exist-
ing datasets but construct a new benchmark dataset
to study the inconsistency of training and testing in
task-oriented dialogue systems.

We build this dataset based on the following
two principles. First of all, we ensure all inter-
actions are reasonable. To achieve that, we follow
the construction process of existing work (Bordes



3714

et al., 2016) and generate the dataset by compli-
cated and elaborated rules. Second, the dataset
should contain several subsets and the dialogue
scenarios covered in each subset are incremental.
To simulate the new unconsidered user needs, we
train the dialogue system on a smaller subset and
test it on a more complicated one.

Specifically, our dataset contains five different
subsets within the context of customer services.
From SubD1 to SubD5, the user needs become
richer in each subset, as described below.
SubD1 includes basic scenarios of the customer
services in which users can achieve two primary
goals. First, users can look up a product or query
some attributes of interested products. For exam-
ple, they can ask “Is $entity 5$3 still on sales?” to
ask the discount information of $entity 5$. Sec-
ond, after finding the desired product, users can
consult the system about the purchase process and
delivery information.
SubD2 contains all scenarios in SubD1. Besides,
users can confirm if a product meets some ad-
ditional conditions. For example, they can ask
“Does $entity 9$ support Android?” to verify the
operating system requirement.
SubD3 contains all scenarios in SubD2. In addi-
tion, users can compare two different items. For
example, they can ask “Is $entity 5$ cheaper than
$entity 9$?” to compare the prices of $entity 5$
and $entity 9$.
SubD4 contains all scenarios in SubD3. And there
are more user needs related to the after-sale ser-
vice. For example, users can consult on how to
deal with network failure and system breakdown.
SubD5 contains all scenarios in SubD4. Further
more, users can give emotional utterances. For ex-
ample, if users think our product is very cheap,
they may say “Oh, it’s cheap and high-quality. I
like it!”. The dialogue system is expected to re-
ply emotionally, such as “Thank you for your ap-
proval.”. If the user utterance contains both emo-
tional and task-oriented factors, the system should
consider both. For example, if users say “I cannot
stand the old operating system, what should I do
to update it?”, the dialogue system should respond
“I’m so sorry to give you trouble, please refer to
this: $api call update system$.”.

It is worth noting that it often requires multi-
ple turns of interaction to complete a task. For

3We use special tokens to anonymize all private informa-
tion in our corpus.

example, a user wants to compare the prices of
$entity 5$ and $entity 9$, but not explicitly gives
the two items in a single turn. To complete the
missing information, the system should ask which
two products the user wants to compare. Besides,
the context plays an important role in the dia-
logue. For example, if users keep asking the same
product many times consecutively, they can use
the subject ellipsis to query this item in the cur-
rent turn and the system will not ask users which
product they are talking about. In addition, tak-
ing into account the diversity of natural language,
we design multiple templates to express the same
intention. The paraphrase of queries makes our
dataset more diverse. For each sub-dataset, there
are 20,000 dialogues for training and 5,000 dia-
logues for testing. A dialogue example in SubD5
and detailed data statistics are provided in the Ap-
pendices A.

5 Experimental Setup

5.1 Data Preprocessing

It is possible for the dialogue model to retrieve re-
sponses directly without any preprocessing. How-
ever, the fact that nearly all utterances contain en-
tity information would lead to a slow model con-
vergence. Thus, we replace all entities with the
orders in which they appear in dialogues to nor-
malize utterances. For example, if the $entity 9$
is the second distinct entity which appears in a di-
alogue, we rename it with $entity order 2$ in the
current episode. After the preprocessing, the num-
ber of normalized response candidates on both the
training and test sets in each sub-dataset is shown
in Table 1.

SubD1 SubD2 SubD3 SubD4 SubD5

# of RSP 41 41 66 72 137

Table 1: The number of normalized response candi-
dates in each sub-dataset after entity replacement, both
training and test data included.

5.2 Baselines

We compare IDS with several baselines:

• IR: the basic tf-idf match model used in (Bor-
des et al., 2016; Dodge et al., 2015).

• Supervised Embedding Model (SEM): the
supervised word embedding model used
in (Bordes et al., 2016; Dodge et al., 2015).



3715

• Dual LSTM (DLSTM): the retrieval-based
dialogue model used in (Lowe et al., 2015).

• Memory Networks (MemN2N): the scoring
model which is used in QA (Sukhbaatar et al.,
2015) and dialogue systems (Bordes et al.,
2016; Dodge et al., 2015).

• IDS−: IDS without updating model parame-
ters during testing. That is, IDS− is trained
only with human intervention data on the
training set and then we freeze parameters.

5.3 Measurements
Following the work of Williams et al. (2017) and
Bordes et al. (2016), we report the average turn ac-
curacy. The turn is correct if the dialogue model
can select the correct response, and incorrect if
not. Because IDS requires human intervention to
reduce risks whenever there is low confidence, we
calculate the average turn accuracy only if IDS
chooses to respond without human intervention.
That is, compared with baselines, IDS computes
the turn accuracy only on a subset of test sets. To
be fair, we also report the rate at which IDS refuses
to respond on the test set. The less the rejection
rate is, the better the model performs.

5.4 Implementation Details
Our word embeddings are randomly initialized.
The dimensions of word embeddings and GRU
hidden units are both 32. The size of the latent
variable z is 20. In uncertainty estimation, the
repetition time K is 50. In all experiments, the
average JSD threshold τ1 and the response proba-
bility threshold τ2 are both set to 0.34. In online
learning, the number of Monte Carlo sampling is
50. In all experiments, we use the ADAM opti-
mizer (Kingma and Ba, 2014) and the learning rate
is 0.001. We train all models in mini-batches of
size 32.

6 Experimental Results

6.1 Robustness to Unconsidered User Actions
To simulate unexpected user behaviors after de-
ployment, we use the hardest test set, SubD5, as
the common test set, but train all models on a sim-
ple dataset (SubD1-SubD4) individually. The av-
erage turn accuracy is shown in Table 2.

4The smaller τ1 or larger τ2 will result in a higher average
turn accuracy but a larger human intervention frequency. In
our preliminary experiments, we find that setting both τ1 and
τ2 to 0.3 is a good trade-off.

Training DataSet
Model SubD1 SubD2 SubD3 SubD4

IR 34.7% 35.2% 44.0% 55.1%
SEM 35.1% 35.4% 43.4% 52.7%
DLSTM 48.2% 52.0% 61.7% 74.0%
MemN2N 50.5% 50.4% 64.0% 77.4%

IDS− 78.6% 77.3% 83.2% 92.7%
IDS 98.1% 96.7% 99.0% 99.7%

Table 2: The average turn accuracy of different mod-
els. Models are trained on SubD1-SubD4 respectively,
but all tested on SubD5. Note that, unlike the existing
methods, IDS− and IDS give responses only if there is
high degree of confidence.

Training DataSet
Model SubD1 SubD2 SubD3 SubD4

IDS− 42.0% 35.5% 30.4% 32.0%
IDS 79.4% 79.0% 66.6% 62.8%

Table 3: The rejection rate on the test set of SubD5.

When trained on SubD1 to SubD4 and tested on
SubD5, as shown in Table 2, the existing methods
are prone to poor performance because these mod-
els are not aware of which instances they can han-
dle. However, equipped with the uncertainty es-
timation module, IDS− can refuse to respond the
uncertain instances and hence achieves better per-
formance. For example, when trained on SubD1
and tested on SubD5, IDS− achieves 78.6% turn
accuracy while baselines achieve only 50.5% turn
accuracy at most. Moreover, if updating the model
with human intervention data during testing, IDS
attains nearly perfect accuracy in all settings.

Due to the uncertainty estimation module, IDS−

and IDS will refuse to respond if there is low con-
fidence. The rejection rates of them are shown in
Table 3. The rejection rate will drop if the train-
ing set is similar to the test set. Unfortunately, the
rejection rate of IDS is much higher than that of
IDS−. We guess the reason is the catastrophic for-
getting (French, 1999; Kirkpatrick et al., 2017).
When IDS learns to handle new user needs in
SubD5, the knowledge learnt in the training phase
will be somewhat lost. Thus, IDS needs more hu-
man intervention to re-learn the forgotten knowl-
edge. However, forgetting will not occur if IDS
is deployed from scratch and accumulates knowl-
edge online because weights of IDS are optimized
alternatively on all possible user needs.

6.2 Deploying without Initialization

Compared with existing methods, IDS can accu-
mulate knowledge online from scratch. The un-



3716

Model SubD1 SubD2 SubD3 SubD4 SubD5

IR 66.3% 66.5% 70.8% 74.1% 75.7%
SEM 67.6% 68.4% 64.1% 60.8% 65.8%
DLSTM 99.9% 99.9% 98.8% 97.7% 96.7%
MemN2N 93.4% 94.5% 89.8% 85.3% 80.8%

IDS− 100% 100% 100% 99.8% 99.9%

Table 4: The average turn accuracy of different systems
on SubDi test set. Note each baseline is trained on the
entire SubDi training data, but IDS− is trained only on
the low-confidence subset of SubDi training set. The
parameters of all system are frozen during testing.

SubD1 SubD2 SubD3 SubD4 SubD5

24.1% 27.4% 38.4% 56.5% 61.6%

Table 5: The rejection rate of IDS− on SubDi training
set.

SubD1 SubD2 SubD3 SubD4 SubD5

0.3% 0.7% 3.2% 13.8% 24.1%

Table 6: The rejection rate of IDS− on SubDi test set.

certainty estimation module will guide us to label
only valuable data. This is similar to active learn-
ing (Balcan et al., 2009; Dasgupta et al., 2005).

To prove that, we train baselines on each of
the SubDi training data with one epoch of back
propagation5 and test these models on each of the
SubDi test set. In contrast, for each SubDi training
set, IDS− is trained from random initialization.
Whenever IDS− refuses to respond, the current
context-response pair in the training set will be
used to update the model until all training data in
SubDi are finished. Hence IDS− is trained on the
subset of SubDi where the response confidence is
below the threshold. After the training is finished,
we freeze the model parameters and test IDS− on
the test set of SubDi.

Table 4 shows the average turn accuracy of dif-
ferent models. Table 5 shows the rejection rate of
IDS− on each SubDi training set. We see that,
compared with all baselines, IDS− achieves better
performance with much less training data. This
shows the uncertainty estimation module can se-
lect the most valuable data to label online.

Table 6 shows the rejection rate of IDS− on
each SubDi test data. We can see that the rejection
rate is negligible on SubD1, SubD2 and SubD3. It
means IDS− can converge to a low rejection rate
after deployment. For SubD4 and SubD5, there

5In the online learning process of IDS−, each labeled data
in the data pool is used only once. For the sake of fairness,
we train baselines with only one epoch in this section.

0 500 1000 1500 2000 2500 3000 3500
Iterations

0

5

10

15

20

25

30

Hu
m

an
 In

te
rv

en
tio

ns

SubD1
SubD2
SubD3
SubD4
SubD5

Figure 5: The intervention frequency curves after de-
ploying IDS− without any initialization.

are still some instances IDS− can not handle. It is
due to the fact that SubD4 and SubD5 are much
more complicated than others. In the next section,
we further show that as online learning continues,
the rejection rate will continue to drop as well.

6.3 Frequency of Human Intervention
The main difference between our approach and
others is that we introduce humans in the system
loop. Therefore, we are interested in the question
of how frequently humans intervene over time.

The human intervention frequency curves of de-
ploying IDS− without any initialization (i.e., the
online learning stage of IDS− in Section 6.2) are
shown in Fig. 5. As shown, the frequency of
human intervention in a batch will decrease with
time. In the early stage of deployment, IDS−

has a large degree of uncertainty because there
are only a few context-response pairs in the data
pool. Through continuous interactions with users,
the labeled data covered in the data pool will be-
come more and more abundant. Thus, humans are
not required to intervene frequently.

Besides, human intervention curves of differ-
ent datasets have different convergence rates. The
curve of SubD1 has the fastest convergence rate.
As the dataset covers more and more user needs,
the convergence rate becomes slower. However,
there is still a trend to converge for SubD4 and
SubD5 as long as we continue the online learning.
This phenomenon is in line with the intuition that a
more complicated dialogue system requires more
training data than a simple one.

6.4 Visual Analysis of Context Embedding
To better understand the behavior of our approach,
we train IDS− on the SubD5 training set until
2,000 batches online updates are finished, and then



3717

0.0 0.2 0.4 0.6 0.8 1.0

0.0

0.2

0.4

0.6

0.8

1.0 High Confidence
Low Confidence

0.0 0.2 0.4 0.6 0.8 1.0

0.0

0.2

0.4

0.6

0.8

1.0 High Confidence
Low Confidence

0.0 0.2 0.4 0.6 0.8 1.0

0.0

0.2

0.4

0.6

0.8

1.0

High Confidence
Low Confidence

0.0 0.2 0.4 0.6 0.8 1.0

0.0

0.2

0.4

0.6

0.8

1.0 High Confidence
Low Confidence

Figure 6: t-SNE visualization on the context represen-
tations of four different system responses. Red dots
are contexts responded by IDS− with high confidence,
while blue dots are contexts with low confidence.

freeze the model parameters and test it on the
SubD5 test set. As Table 1 shows, there are 137
unique normalized responses. Among these re-
sponses, we pick four of them and draw their con-
text embedding vectors. Each vector is reduced
to a 2-dimensional vector via t-SNE (Maaten and
Hinton, 2008) for visualization, one sub-graph per
response in Fig. 6. In each figure, the red dots are
contexts responded by IDS− with high confidence,
while the blue dots are contexts responded by hu-
man where there is low confidence.

These graphs show a clear separation of sure vs.
unsure contexts. Some blue dots are far away from
the red. Humans should pay attention to these con-
texts to avoid risks. Besides, there are only a small
number of cases when the two classes are mingled.
We guess these cases are located in the confidence
boundary. In addition, there are multiple clusters
in each class. It is due to the fact the same system
response can appear in different dialogue scenes.
For example, “the system requesting user’s phone
number” appears in scenes of both exchange and
return goods. Although these contexts have the
same response, their representations should be dif-
ferent if they belong to different dialogue scenes.

7 Related Work

Task-oriented dialogue systems have attracted nu-
merous research efforts. Data-driven methods,
such as reinforcement learning (Williams et al.,
2017; Zhao and Eskenazi, 2016; Li et al., 2017)
and supervised learning (Wen et al., 2016; Eric
and Manning, 2017; Bordes et al., 2016), have
been applied to optimize dialogue systems auto-
matically. These advances in task-oriented dia-

logue systems have resulted in impressive gains in
performance. However, prior work has mainly fo-
cused on building task-oriented dialogue systems
in a closed environment. Due to the biased as-
sumptions of real users, such systems will break
down when encountering unconsidered situations.

Several approaches have been adopted to ad-
dress this problem. Gašic et al. (2014) explic-
itly defined kernel functions between belief states
from different domains to extend the domain of
dialogue systems. But it is difficult to define an
appropriate kernel function when the ontology has
changed drastically. Shah et al. (2016) proposed to
integrate turn-level and task-level reward signals
to learn how to handle new user intents. Lipton
et al. (2018) proposed to use BBQ-Networks to ex-
tend the domain. However, Shah et al. (2016) and
Lipton et al. (2018) have reserved a few bits in the
dialogue state for the domain extension. To relax
this assumption, Wang et al. (2018) proposed the
teacher-student framework to maintain dialogue
systems. In their work, the dialogue system can
only be extended offline after finding errors and
it requires hand-crafted rules to handle new user
actions. In contrast, we can extend the system on-
line in an incremental6 way with the help of hired
customer service staffs.

Our proposed method is inspired by the cumula-
tive learning (Fei et al., 2016), which is a form of
lifelong machine learning (Chen and Liu, 2016).
This learning paradigm aims to build a system that
learns cumulatively. The major challenges of the
cumulative learning are finding unseen classes in
the test set and updating itself efficiently to ac-
commodate new concepts (Fei et al., 2016). To
find new concepts, the heuristic uncertainty es-
timation methods (Tong and Koller, 2001; Cu-
lotta and McCallum, 2005) in active learning (Bal-
can et al., 2009; Dasgupta et al., 2005) can be
adopted. When learning new concepts, the cumu-
lative learning system should avoid retraining the
whole system and catastrophic forgetting (French,
1999; Kirkpatrick et al., 2017). But the catas-
trophic forgetting does not happen if the dialogue
system is trained with all possible user needs al-
ternatively from scratch.

The uncertainty estimation and online learn-

6The term “incremental” refers to systems able to operate
on a word by word basis in the previous work (Eshghi et al.,
2017; Schlangen and Skantze, 2009). In our work, it refers to
the system which can adapt to new dialogue scenarios after
deployment.



3718

ing methods in our work are inspired by varia-
tional inference approach (Rezende et al., 2014;
Kingma and Welling, 2014). In the existing work,
this approach was used to generate diverse ma-
chine responses in both open domain dialogue
systems (Zhao et al., 2017; Serban et al., 2016)
and task-oriented dialogue systems (Wen et al.,
2017). In contrast, our work makes use of the
Bayesian nature of variational inference to es-
timate the uncertainty and learn from humans.
Specifically, we sample variables from the prior
network as the random perturbation to estimate the
model uncertainty following the idea of Query-
By-Committee (Seung et al., 1992) and optimize
model parameters by maximizing the ELBO.

8 Conclusion

This paper presents a novel incremental learning
framework to design dialogue systems, which we
call IDS. In this paradigm, users are not expected
to follow any definition, and IDS has potential to
handle new situations. To simulate new user ac-
tions after deployment, we propose a new dataset
consisting of five different subsets. Experiments
show that IDS is robust to new user actions. Im-
portantly, with humans in the loop, IDS requires
no data for initialization and can update itself on-
line by selecting the most valuable data. As the
usage grows, IDS will cumulate more and more
knowledge over time.

9 Acknowledgments

The research work described in this paper has
been supported by the National Key Research and
Development Program of China under Grant No.
2017YFB1002103 and the Natural Science Foun-
dation of China under Grant No. U1836221.

References
Maria-Florina Balcan, Alina Beygelzimer, and John

Langford. 2009. Agnostic active learning. Journal
of Computer and System Sciences, 75(1):78–89.

Antoine Bordes, Y-Lan Boureau, and Jason Weston.
2016. Learning end-to-end goal-oriented dialog.
arXiv preprint arXiv:1605.07683.

Zhiyuan Chen and Bing Liu. 2016. Lifelong machine
learning. Synthesis Lectures on Artificial Intelli-
gence and Machine Learning, 10(3):1–145.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of

gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555.

Aron Culotta and Andrew McCallum. 2005. Reduc-
ing labeling effort for structured prediction tasks. In
AAAI, volume 5, pages 746–751.

Sanjoy Dasgupta, Adam Tauman Kalai, and Claire
Monteleoni. 2005. Analysis of perceptron-based
active learning. In International Conference on
Computational Learning Theory, pages 249–263.
Springer.

Jesse Dodge, Andreea Gane, Xiang Zhang, Antoine
Bordes, Sumit Chopra, Alexander Miller, Arthur
Szlam, and Jason Weston. 2015. Evaluating prereq-
uisite qualities for learning end-to-end dialog sys-
tems. arXiv preprint arXiv:1511.06931.

Mihail Eric and Christopher D Manning. 2017. Key-
value retrieval networks for task-oriented dialogue.
arXiv preprint arXiv:1705.05414.

Arash Eshghi, Igor Shalyminov, and Oliver Lemon.
2017. Bootstrapping incremental dialogue systems
from minimal data: the generalisation power of di-
alogue grammars. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2220–2230.

Geli Fei, Shuai Wang, and Bing Liu. 2016. Learn-
ing cumulatively to become more knowledgeable.
In Proceedings of the 22nd ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining, pages 1565–1574. ACM.

Robert M French. 1999. Catastrophic forgetting in
connectionist networks. Trends in cognitive sci-
ences, 3(4):128–135.

Milica Gašic, Dongho Kim, Pirros Tsiakoulis, Cather-
ine Breslin, Matthew Henderson, Martin Szummer,
Blaise Thomson, and Steve Young. 2014. Incre-
mental on-line adaptation of pomdp-based dialogue
managers to extended domains. In Proceedings on
InterSpeech.

Matthew D Hoffman, David M Blei, Chong Wang,
and John Paisley. 2013. Stochastic variational infer-
ence. The Journal of Machine Learning Research,
14(1):1303–1347.

D. P. Kingma and M. Welling. 2014. Auto-encoding
variational bayes. In Conference Proceedings: Pa-
pers Accepted To the International Conference on
Learning Representations.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,
Joel Veness, Guillaume Desjardins, Andrei A Rusu,
Kieran Milan, John Quan, Tiago Ramalho, Ag-
nieszka Grabska-Barwinska, et al. 2017. Overcom-
ing catastrophic forgetting in neural networks. Pro-
ceedings of the national academy of sciences, page
201611835.



3719

Daphne Koller and Nir Friedman. 2009. Probabilistic
graphical models: principles and techniques. MIT
press.

Xiujun Li, Yun-Nung Chen, Lihong Li, Jianfeng Gao,
and Asli Celikyilmaz. 2017. End-to-end task-
completion neural dialogue systems. In Proceedings
of the Eighth International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers),
pages 733–743.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. arXiv preprint arXiv:1703.03130.

Zachary Lipton, Xiujun Li, Jianfeng Gao, Lihong Li,
Faisal Ahmed, and Li Deng. 2018. Bbq-networks:
Efficient exploration in deep reinforcement learn-
ing for task-oriented dialogue systems. In Thirty-
Second AAAI Conference on Artificial Intelligence.

Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle
Pineau. 2015. The ubuntu dialogue corpus: A large
dataset for research in unstructured multi-turn dia-
logue systems. arXiv preprint arXiv:1506.08909.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of machine
learning research, 9(Nov):2579–2605.

Yishu Miao, Lei Yu, and Phil Blunsom. 2016. Neu-
ral variational inference for text processing. In In-
ternational Conference on Machine Learning, pages
1727–1736.

Tim Paek and Roberto Pieraccini. 2008. Automating
spoken dialogue management design using machine
learning: An industry perspective. Speech commu-
nication, 50(8):716–729.

Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Wierstra. 2014. Stochastic backpropagation and
approximate inference in deep generative models.
pages 1278–1286.

David Schlangen and Gabriel Skantze. 2009. A gen-
eral, abstract model of incremental dialogue pro-
cessing. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 710–718. Association
for Computational Linguistics.

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron Courville, and
Yoshua Bengio. 2016. A hierarchical latent variable
encoder-decoder model for generating dialogues.

H Sebastian Seung, Manfred Opper, and Haim Som-
polinsky. 1992. Query by committee. In Proceed-
ings of the fifth annual workshop on Computational
learning theory, pages 287–294. ACM.

Pararth Shah, Dilek Hakkani-Tür, and Larry Heck.
2016. Interactive reinforcement learning for task-
oriented dialogue management. In NIPS 2016 Deep
Learning for Action and Interaction Workshop.

Yanyao Shen, Hyokun Yun, Zachary C Lipton, Yakov
Kronrod, and Animashree Anandkumar. 2017.
Deep active learning for named entity recognition.
arXiv preprint arXiv:1707.05928.

Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015.
Learning structured output representation using
deep conditional generative models. In Advances
in Neural Information Processing Systems, pages
3483–3491.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Advances
in neural information processing systems, pages
2440–2448.

Simon Tong and Daphne Koller. 2001. Support vec-
tor machine active learning with applications to
text classification. Journal of machine learning re-
search, 2(Nov):45–66.

Weikang Wang, Jiajun Zhang, Han Zhang, Mei-Yuh
Hwang, Chengqing Zong, and Zhifei Li. 2018. A
teacher-student framework for maintainable dialog
manager. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 3803–3812.

Tsung-Hsien Wen, Yishu Miao, Phil Blunsom, and
Steve Young. 2017. Latent intention dialogue mod-
els. In Proceedings of the 34th International Confer-
ence on Machine Learning-Volume 70, pages 3732–
3741. JMLR. org.

Tsung-Hsien Wen, David Vandyke, Nikola Mrksic,
Milica Gasic, Lina M Rojas-Barahona, Pei-Hao Su,
Stefan Ultes, and Steve Young. 2016. A network-
based end-to-end trainable task-oriented dialogue
system. arXiv preprint arXiv:1604.04562.

Jason D Williams, Kavosh Asadi, and Geoffrey
Zweig. 2017. Hybrid code networks: practical
and efficient end-to-end dialog control with super-
vised and reinforcement learning. arXiv preprint
arXiv:1702.03274.

Tiancheng Zhao and Maxine Eskenazi. 2016. To-
wards end-to-end learning for dialog state tracking
and management using deep reinforcement learning.
arXiv preprint arXiv:1606.02560.

Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi.
2017. Learning discourse-level diversity for neural
dialog models using conditional variational autoen-
coders. arXiv preprint arXiv:1703.10960.

A Appendices

A.1 Dialogue Example
Our corpus is in Chinese. We translate an example
dialogue into English and show it in Table 7.

A.2 Data Statistics
Data statistics are show in Table 8. Dialogue sce-
narios in each sub-dataset are shown in 9.



3720

User: The operating system breaks down. What should I do?

System: You can try to update the system.

User: The system always has problems. I’m very disappointed.

System: I’m so sorry to give you trouble, we will do our best to improve the operating system.

User: I don’t know how to update the system.

System: Please refer to this: $api call update system$.

User: It looks so troublesome. I want to send the product back.

System: Please tell me your order number.

User: My order number is $orderNumber$.

System: Please send the item to this address: $address$.

Table 7: A sample dialogue in SubD5. The dialogue consists of two dialogue scenarios: “consult system error”
and “return goods”. And in the second turn, the user expressed negative emotion. Besides, it is worth noting that
different user needs involve different business logics. For example, if the user wants to return goods, the system
need know the order number and ask the user to send the goods to the $address$.

SubD1 SubD2 SubD3 SubD4 SubD5

# of Scenarios per Dialogue 5.2 5.2 4.6 3.7 3.9

# of Utterances per Dialogue 11.7 11.7 10.6 9.8 12.4

# of Tokens per Utterance 3.8 4.0 4.1 4.3 5.1

# of Paraphrases per Query 8.9 7.0 6.5 6.9 6.9

Vocab Size after Preprocessing 194 253 303 430 620

# of Products 50

Training Dialogues 20000

Validation Dialogues 5000

Test Dialogues 5000

Table 8: Data statistics of each sub-dataset.

SubD1 query product information, query payment methods, query express information

SubD2 scenarios of SubD1, verify product information

SubD3 scenarios of SubD2, compare two products

SubD4
scenarios of SubD3, ask for an invoice, consult system error, consult nfc error,
consult network error, return goods, exchange goods, query logistics

SubD5 scenarios of SubD4, express positive emotion, express negative emotion

Table 9: The dialogue scenarios covered in each sub-dataset.


