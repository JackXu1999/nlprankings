



















































Adversarial Training for Weakly Supervised Event Detection


Proceedings of NAACL-HLT 2019, pages 998–1008
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

998

Adversarial Training for Weakly Supervised Event Detection

Xiaozhi Wang1∗ , Xu Han1∗, Zhiyuan Liu1† , Maosong Sun1, Peng Li2
1Department of Computer Science and Technology, Tsinghua University, Beijing, China

Institute for Artificial Intelligence, Tsinghua University, Beijing, China
State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China

2Pattern Recognition Center, WeChat, Tencent Inc, China
{xz-wang16, hanxu17}@mails.tsinghua.edu.cn

{liuzy, sms}@tsinghua.edu.cn
patrickpli@tencent.com

Abstract

Modern weakly supervised methods for event
detection (ED) avoid time-consuming human
annotation and achieve promising results by
learning from auto-labeled data. However,
these methods typically rely on sophisticated
pre-defined rules as well as existing instances
in knowledge bases for automatic annotation
and thus suffer from low coverage, topic bias,
and data noise. To address these issues, we
build a large event-related candidate set with
good coverage and then apply an adversar-
ial training mechanism to iteratively identify
those informative instances from the candi-
date set and filter out those noisy ones. The
experiments on two real-world datasets show
that our candidate selection and adversarial
training can cooperate together to obtain more
diverse and accurate training data for ED,
and significantly outperform the state-of-the-
art methods in various weakly supervised sce-
narios. The datasets and source code can
be obtained from https://github.com/
thunlp/Adv-ED.

1 Introduction

Event detection (ED) aims at detecting event trig-
gers, which are often words or phrases evoking
events in instances, and then identifying their spe-
cific event types. For example, we can extract
the trigger “married” of the event “Marry”
from the text “Mark Twain and Olivia Langdon
married in 1870”. Detecting and identifying
events is an important subtask of event extraction
and also beneficial for various downstream NLP
applications, such as question answering (Yang
et al., 2003), information retrieval (Basile et al.,
2014), and reading comprehension (Cheng and
Erk, 2018). Hence, many efforts have been de-
voted to detecting event triggers and types.

∗ indicates equal contribution
† Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn)

Most prior methods for ED are based on feature
engineering, such as the token-level features (Ahn,
2006; Ji and Grishman, 2008) and the structured
features (Li et al., 2013; Araki and Mitamura,
2015). As the rapid development of neural net-
works, various neural models have been proposed
to directly embed textual semantic information
into a low-dimensional space and then detect event
triggers based on those feature vectors (Chen et al.,
2015; Nguyen and Grishman, 2015; Nguyen et al.,
2016; Ghaeini et al., 2016; Feng et al., 2016).
These methods follow a supervised learning ap-
proach to train models on human-annotated data,
and their requirement of human-annotated data is
a bottleneck in practice. Considering weak su-
pervision is widely adopted to take full advan-
tages of large-scale raw data, especially some spe-
cific work for information extraction (Mintz et al.,
2009; Riedel et al., 2010; Zeng et al., 2015; Cao
et al., 2018), weak supervision has been explored
to automatically label training data for ED (Chen
et al., 2017; Zeng et al., 2018; Yang et al., 2018;
Araki and Mitamura, 2018). Compared with those
supervised ED methods, the weakly supervised
methods can be generalized to real-world ED ap-
plications efficiently without intensive labor.

Although promising results have been achieved
by these weakly supervised methods, there are
still some severe problems for these weakly super-
vised ED models: (1) Weakly supervised meth-
ods naturally suffer from the inevitable noise in
data. (2) Current weakly supervised ED mod-
els adopt sophisticated pre-defined rules and in-
complete knowledge bases to automatically obtain
data, which results in the auto-labeled data with
low coverage and topic bias.

In order to construct a large-scale dataset with
better coverage and reduce topic bias, we avoid
adopting sophisticated pre-defined rules and heavy
toolkits for semantic component analysis. Instead,

https://github.com/thunlp/Adv-ED
https://github.com/thunlp/Adv-ED


999

D

Bob [contacted] Lily by post. E

... ... ...

Reliable Set

Unreliable Set

G

Bob [wrote] to Lily. E

Bob [wrote] a novel. E

...

...

... ... ... ...

EncoderE GeneratorG DiscriminatorD

Figure 1: The overall architecture of adversarial training method for ED. The event type is Contact.

we propose a simple trigger-based latent instance
discovery strategy, by applying an assumption that
if a given word 1 serves as the trigger in a known
event instance, all instances mentioning this word
may also express an event. As compared with the
sophisticated rules, this strategy is less restrictive
in the correlation among words, triggers and event
types. Hence, our strategy can obtain a candidate
set covering more topics and instances without any
manual design.

We further propose an adversarial training
mechanism like Goodfellow et al. (2014); Radford
et al. (2016), which can not only distill those infor-
mative instances from the candidate set but also
improve the performance of ED model on a noisy
scenario such as distant supervision. As shown in
Figure 1, we split the dataset into a reliable set and
an unreliable set respectively, and design a dis-
criminator and a generator. The discriminator is
applied to judge whether a given instance is in-
formative and annotated correctly, and the gener-
ator is used to select the most confusing instances
from raw data to fool the discriminator. The dis-
criminator is trained with the reliable data as pos-
itive instances and the data selected by the gener-
ator as negative ones. Meanwhile, the generator
is trained to select data to fool the discriminator.
During the training process, the generator can pro-
vide large amounts of latent noisy data to enhance
the discriminator, and the discriminator can influ-
ence the generator to select those more informa-
tive data. Since noisy data makes no effect on op-
timizing both the generator and the discriminator,

1We treat phrases as words in this paper.

when the generator and the discriminator reach a
balance, the discriminator can boost resistance to
noise and better categorize events, and the gener-
ator can effectively select informative instances to
the discriminator.

We conduct experiments on both semi-
supervised and distantly supervised scenarios.
The experimental results demonstrate that our
trigger-based latent instance discovery strategy
and adversarial training method can cooperate to
obtain more diverse and accurate training data
as well as reduce the side effect of the noise
problem, and thus significantly outperform the
state-of-the-art ED models.

2 Related Work

ED has attracted wide attention recently. Tradi-
tional feature-based methods (Ahn, 2006; Ji and
Grishman, 2008; Gupta and Ji, 2009; Riedel et al.,
2010; Hong et al., 2011; McClosky et al., 2011;
Huang and Riloff, 2012a,b; Araki and Mitamura,
2015; Li et al., 2013; Yang and Mitchell, 2016;
Liu et al., 2016b) rely on manually designed fea-
tures to detect the event triggers and event types.
With the development of neural networks, various
neural methods have also been proposed (Chen
et al., 2015; Nguyen and Grishman, 2015; Nguyen
et al., 2016; Duan et al., 2017; Nguyen et al., 2016;
Ghaeini et al., 2016; Lin et al., 2018).

Furthermore, some efforts have been made to
improve the performance of ED systems with ex-
ternal knowledge (Liu et al., 2016a, 2017), con-
textual information (Liu et al., 2018b), document-
level information (Duan et al., 2017; Zhao et al.,



1000

2018) and multimodal integration (Zhang et al.,
2017). Some advanced architectures have also
been applied, such as attention mechanism (Liu
et al., 2017, 2018a), graph convolutional net-
works (Nguyen and Grishman, 2018) and gener-
ative adversarial networks (Hong et al., 2018).

All the supervised methods above rely on
human-annotated data, and the data is often re-
stricted to a small scale due to the expensive
human annotation. Hence, unsupervised meth-
ods (Huang et al., 2016; Yuan et al., 2018) and var-
ious weakly supervised methods on ED are pro-
posed. Muis et al. (2018) adopt distant supervision
to create training instances for low-resource lan-
guage. Araki and Mitamura (2018) adopt Word-
Net (Miller et al., 1990) and rule-based meth-
ods to generate open-domain data without event-
type labels. Chen et al. (2017) and Zeng et al.
(2018) use distant supervision to generate large-
scale data from existing structured event knowl-
edge in knowledge bases. Liao and Grishman
(2010a), Huang and Riloff (2012a) and Ferguson
et al. (2018) conduct semi-supervised ED with
bootstrapping. Nevertheless, due to the low cov-
erage of existing knowledge bases as well as lack
of advanced denoising mechanism, those weakly
supervised methods still suffer from the problem
of low coverage and noisy data.

Inspired by Szegedy et al. (2013) and Good-
fellow et al. (2014), adversarial training has been
explored for several NLP applications recently to
resist noise, such as text classification (Miyato
et al., 2016) and text generation (Xie et al., 2017;
Chen et al., 2018). Adversarial training has also
been adopted for information extraction (Wu et al.,
2017; Hong et al., 2018; Qin et al., 2018; Wang
et al., 2018; Han et al., 2018). These adversarial
information extraction methods either generate ad-
versarial instances by adding simple noise pertur-
bation to embeddings (Wu et al., 2017; Hong et al.,
2018), or mainly adopt models to denoise data and
neglect to discover more training instances from
raw data (Qin et al., 2018; Han et al., 2018). Com-
pared with these methods, our adversarial method
samples adversarial examples from the real-world
data rather than generating pseudo noisy pertur-
bations. Furthermore, our method not only de-
noises auto-labeled data but also labels unlabeled
instances to extend datasets for higher coverage.
Hence, our method can effectively alleviate low
coverage, topic bias, and noise problem in ED.

3 Methodology

In this section, we introduce the overall framework
of our proposed models for weakly supervised ED.

3.1 Framework

As shown in Figure 1, the overall framework con-
sists of three modules, including instance encoder,
adversarial training strategy, and their adaption for
various weakly supervised ED scenarios.

The instance encoder is applied to encode the
instances into its corresponding embeddings to
provide semantic features for the other mod-
ules of our models. Given an instance x =
{w1, . . . , t, . . . , wn} consisting of n words and its
candidate trigger t, we adopt several effective neu-
ral models to represent the semantic features of the
instance x with the embedding x. Details of the
instance encoder are shown in Section 3.2.

After representing instances into their embed-
dings by the instance encoder, an adversarial train-
ing strategy is applied, which aims at highlight-
ing those informative instances and filtering out
those noisy instances from a large-scale unreliable
dataset U under the guidance of another reliable
dataset R. The adversarial training strategy is the
core module of our framework, and we will intro-
duce its details in Section 3.3.

Each instance x in U andR will be labeled with
a trigger word t and an event type e ∈ E . If an
instance does not have a trigger and cannot ex-
press any definite events, it will be labeled with
a special event NA, which indicates that the event
of this instance is not available. Before applying
adversarial training, U andR are automatically la-
beled. The details of splitting and automatically
labeling U and R for various weakly supervised
ED scenarios, as well as utilizing adversarial train-
ing strategy to extend datasets, will be introduced
in Section 3.4.

3.2 Instance Encoder

In this paper, we select CNN (Chen et al., 2015)
and BERT (Devlin et al., 2018) as representative
encoders to encode the given instances.

CNN After representing all words in the in-
stance x into their input embeddings, including
both word embeddings and position embeddings
which encode the relative position to candidate
triggers, CNN slides a convolution kernel over the
input embeddings to get hidden embeddings as



1001

follows,

{h1, . . . ,hn} = CNN
(
w1, . . . , t, . . . , wn

)
. (1)

BERT Similar to CNN, after summing word
piece (Wu et al., 2016), segment and position em-
beddings of all words in the instance x as input
embeddings, BERT adopt a multi-layer bidirec-
tional transformer encoder (Vaswani et al., 2017)
to get hidden embeddings as follows,

{h1, . . . ,hn} = BERT
(
w1, . . . , t, . . . , wn

)
. (2)

Because the candidate trigger t splits the in-
stance x into two parts, we follow Chen et al.
(2015) to adopt a dynamic multi-pooling opera-
tion over the hidden embeddings to achieve the in-
stance embedding x,

[←−x ]j = max{[h1]j , . . . , [hi]j},
[−→x ]j = max{[hi+1]j , . . . , [hn]j},

x = [←−x ;−→x ],
(3)

where [·]j is the j-th value of a vector and i is the
position of the trigger t. As CNN and BERT adopt
a dynamic multi-pooling operation, we name them
“DMCNN” and “DMBERT” in this paper.

3.3 Adversarial Training

As shown in Figure 1, the overall framework of
our adversarial strategy consists of a discriminator
and a generator. The discriminator is adopted to
detect event triggers and identify event types for
each instance in datasets. When given a noisy in-
stance, the discriminator is also expected to resist
noise and explicitly point out that there are no trig-
gers and events. The generator is used to select
instances from the unreliable dataset U to confuse
the discriminator as much as possible.

Each instance x ∈ R is assumed to explic-
itly express its labeled trigger t and event type
e. In contrast, each instance x ∈ U is assumed
to be untrustworthy during the adversarial train-
ing, i.e., there is a certain probability that it is la-
beled incorrectly. Hence, we design the discrimi-
nator to judge whether a given instance can expose
its labeled event type, which aims at maximizing
the conditional probability P (e|x, t), x ∈ R and
1 − P (e|x, t), x ∈ U . The generator is trained
to select the most confusing instances from U to
fool the discriminator, i.e., selecting the instances
by P (e|x, t), x ∈ U . The training process is an

adversarial min-max game as follows,

φD = max
(
Ex∼PR

[
log
(
P (e|x, t)

)]
+Ex∼PU

[
log
(
1− P (e|x, t)

)])
,

φG = maxEx∼PU
[
log
(
P (e|x, t)

)]
,

(4)

where PR is the reliable data distribution, and the
generator samples adversarial examples from the
unreliable data according to the probability distri-
bution PU . Although φD and φG are conflicting,
noisy data in U has the side effect for both φD and
φG. Hence, when the generator and the discrim-
inator reaching a balance after sufficient training,
the generator tends to select those informative in-
stances with a higher probability compared with
those noisy ones, and the discriminator boosts re-
sistance to noise and can better categorize events.

Discriminator
Given an instance x and its labeled trigger t and
event type e, the discriminator is responsible for
judging whether the given instance exposes its la-
beled trigger and event type. After representing
the instance x with its embedding x, we imple-
ment the discriminator as follows,

D(e|x, t) = e · x,

P (e|x, t) =
exp

(
D(e|x, t)

)∑
ê∈E exp

(
D(ê|x, t)

) , (5)
where e is the embedding of the event type e ∈ E .

An optimized discriminator will assign high
scores to those instances inR, and meanwhile dis-
trust those instances and their labels in U . Hence,
in practice, we formalize the loss function to opti-
mize the discriminator as follows,

LD = −
∑
x∈R

1

|R|
log
(
P (e|x, t)

)
−
∑
x∈U

PU (x) log
(
1− P (e|x, t)

)
.

(6)

When optimizing the discriminator, we regard the
component of the encoder andD(e|x, t) as param-
eters for updating. This loss function LD is corre-
sponding to φD in Eq. (4).

Generator
The generator aims at selecting the most confus-
ing instances from U to cheat the discriminator.
We design the generator to optimize the probabil-
ity distribution PU to select instances. The gener-
ator computes confusing scores for all instances in



1002

U to evaluate their perplexity and further computes
the confusing probability PU as follows,

f(x) = W · x+ b,

PU (x) =
exp

(
f(x)

)∑
x̂∈U exp

(
f(x̂)

) . (7)
where x is the embedding of the instance x com-
puted by the encoder. W and b are parameters for
a separating hyperplane.

We regard that the higher scores computed by
the discriminator the instances have, the more con-
fusing the instances are, because they are more
likely to fool the discriminator to make a wrong
decision. We expect that an optimized generator
pays more attention to those most confusing in-
stances. Hence, given an instance x ∈ U and its
unreliable-labeled trigger t and event type e, we
formalize the loss function to optimize the gener-
ator as follows,

LG = −
∑
x∈U

PU (x) log
(
P (e|x, t)

)
, (8)

where P (e|x, t) is computed by the discrimina-
tor. When optimizing the generator, we regard the
component to compute PU (x) as parameters for
updating. This loss function LG is corresponding
to φG in Eq. (4).

There may be some instances in U labeled NA
and these instances are always wrongly predicted
into some other events. Thus we specifically use
the average scores over all feasible events to re-
place their P (e|x, t) in Eq. (8) as follows,

P (NA|x, t) = 1
|E| − 1

∑
e∈E,e 6=NA

P (e|x, t), (9)

where E indicates the set of event types.

Training and Implementation Details

Because there may be large amounts of instances
in R and U , directly computing LD and LG
is time-consuming, and frequently traversing the
whole dataset of R and U also accordingly be-
comes difficult. For improving training efficiency,
we sample subsets ofR and U to approximate the
essential probability distribution, and formalize a

new loss function for optimization,

L̃D = −
∑
x∈R̃

1

|R̃|
log
(
P (e|x, t)

)
−
∑
x∈Ũ

PŨ (x) log
(
1− P (e|x, t)

)
,

L̃G = −
∑
x∈Ũ

PŨ (x) log
(
P (e|x, t)

)
,

(10)

where R̃ and Ũ are the subsets sampled from R
and U , and PŨ is the approximation to Eq. (7),

PŨ (x) =
exp

(
f(x)α

)∑
x̂∈Ũ exp

(
f(x̂)α

) . (11)
α is a hyperparameter that controls the sharpness
of the probability distribution to avoid the weights
concentrating on some specific instances. Finally,
the overall optimization function is,

L = L̃D + λL̃G, (12)

where λ is a harmonic factor. In practice, L̃D
and L̃G in adversarial training are optimized al-
ternately, and λ is also integrated into the learning
rate of L̃G to avoid adjusting λ additionally.

3.4 Adaption for Weakly Supervised
Scenarios

In this section, we introduce the adaption of ad-
versarial training strategy for various weakly su-
pervised ED scenarios (semi-supervised scenarios
and distantly supervised scenarios), as well as the
method to automatically label and split the reliable
set and unreliable set used for adversarial training.

Trigger-based Latent Instance Discovery
To utilize unlabeled data, we propose a simple
trigger-based latent instance discovery strategy,
which can automatically label trigger words and
event types for raw data. The trigger-based strat-
egy is based on a heuristic assumption that if a
given word serves as the trigger in a known in-
stance, all other instances mentioning this word in
raw data are latent instances and may also express
an event. For example, the word “married”
serves as the trigger in the instance “Mark Twain
and Olivia Langdon married in 1870” to expose
the event “Marry”, and then all instances in un-
labeled data containing the word “married” will
be picked up and added into a latent instance can-
didate set. As compared with the sophisticated



1003

rules used in existing weakly supervised ED mod-
els, our trigger-based latent instance discovery is
simple, without the need of considering the corre-
lation among words, triggers, and event types. Be-
cause our strategy is less restrictive, it is effective
and efficient to obtain a large-scale candidate set
without any special manual design. Meanwhile,
the candidate set can cover much more instances
and topics than the existing strategies.

Semi-supervised Scenarios
When adapting our adversarial training strategy
for semi-supervised scenarios, we first use the
small-scale labeled data to pretrain the encoder
and discriminator to let them gain the ability to
detect event triggers and identify event types to a
certain extent. Then, we construct a large-scale
latent candidate set based on our instance discov-
ery strategy with the trigger words in the labeled
data as heuristic seeds. We use the pretrained
encoder and discriminator to automatically label
triggers and event types for all instances in the can-
didate set to build noisy large-scale data. With the
small-scale labeled data as the reliable set R and
the large-scale auto-labeled data as the unreliable
set U , we can optimize the encoder, discrimina-
tor, and generator together to carry out adversarial
training. During the adversarial training, when the
discriminator and generator reach a balance after
certain training epochs, all instances from the un-
reliable set U recommended by the generator and
regarded as being labeled correctly by the discrim-
inator will be adjusted from U to R. Conducting
adversarial training iteratively can identify infor-
mative instances and filter out noisy instances in
U , and accomplish utilizing large-scale unlabeled
data to enrich small-scale labeled data.

Distantly Supervised Scenarios
The adaption for distantly supervised scenarios is
similar to the adaption for semi-supervised sce-
narios. We first use the whole auto-labeled data
to pretrain the encoder and discriminator. Then,
the encoder and discriminator are used to com-
pute confident scores for all instances in the auto-
labeled set. By setting a particular threshold, we
can split the whole auto-labeled set into two parts.
The instances with scores higher than the thresh-
old will be added into the reliable set R, and the
other instances with lower scores will be added
into the unreliable set U . After the whole auto-
labeled set being split into R and U , we can con-

duct adversarial training to reduce the side effect
of those noise in U and enhance the discriminator
for better identifying events. Intuitively, the reli-
able set R isolated from the auto-labeled set can
be used as seeds to utilize more raw data in a sim-
ilar way applied in semi-supervised scenarios.

4 Experiments

We evaluate our models on both semi-supervised
and distantly supervised scenarios. Before intro-
ducing the detailed experimental settings and re-
sults, we list the hyperparameters first.

4.1 Hyperparameter Settings

For DMCNN, following the settings of previous
work, we use the pre-trained word embeddings
learned by Skip-Gram (Mikolov et al., 2013) as the
initial word embeddings. We implement DMCNN
by ourselves and follow the same hyperparameters
used in Chen et al. (2015) for fair comparisons.
For DMBERT, we follow the same hyperparam-
eters used for BERTBASE in Devlin et al. (2018)
and apply the pre-trained model2 to initialize the
parameters. We list the essential hyperparameters
of the discriminator and the generator for adver-
sarial training in Table 1.

Dropout Probability p 5× 10−1
Learning Rate αgc for the generators (DMCNN) 5× 10−3
Learning Rate αdc for the discriminators (DMCNN) 2× 10−2
Learning Rate αgb for the generators (DMBERT) 2× 10−5
Learning Rate αdb for the discriminators (DMBERT) 1× 10−4

Table 1: Hyperparameter settings.

4.2 Distantly Supervised Scenarios

Dataset and Evaluation
For distantly supervised scenarios, we utilize the
distantly supervised dataset developed by Chen
et al. (2017) with FreeBase (Bollacker et al.,
2008). The dataset contains 142, 611 labeled in-
stances and 21 event types. Following previous
work (Mintz et al., 2009; Chen et al., 2017), we
evaluate our adversarial training mechanism by
held-out evaluation. We report the precision-recall
curves of recall under 0.7 since we mainly focus
on the performance of those top-ranked results. To
give a complete view of the overall performance,
we also report the area under the curve (AUC).

2https://github.com/google-research/
bert

https://github.com/google-research/bert
https://github.com/google-research/bert


1004

0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Recall

0.60

0.65

0.70

0.75

0.80

0.85

0.90

0.95

1.00

Pr
ec

isi
on

DMCNN+ADV
DMCNN+NA
DMCNN+MIL
DMCNN

Figure 2: The aggregated precision-recall curves of
DMCNN models.

0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Recall

0.60

0.65

0.70

0.75

0.80

0.85

0.90

0.95

1.00

Pr
ec

isi
on

DMBERT+ADV
DMBERT+NA
DMBERT+MIL
DMBERT

Figure 3: The aggregated precision-recall curves of
DMBERT models.

To evaluate the effectiveness of our proposed
models, we compare our adversarial training mod-
els (DMCNN+ADV, DMBERT+ADV) with var-
ious neural baselines, including: (1) DMCNN and
DMBERT proposed in Chen et al. (2015) and
this paper respectively, which are the basic models
without any adaption to the noisy distant supervi-
sion. (2) +MIL models, which improve the ba-
sic models with multi-instance learning proposed
in Chen et al. (2017) to alleviate the noise prob-
lem. (3) +NA models, which simply treat the in-
stances in the unreliable set as negative instances
with the label NA. This method could be regarded
as a simplified version of our adversarial training
to conduct ablation study. In this experiment, we
separate the reliable and unreliable set by the con-
fidence of the basic models following Section 3.4.

Overall Evaluation Results

The precision-recall curves of DMCNN models
and DMBERT models are shown in Figure 2 and
Figure 3, and the results of AUC are shown in Ta-
ble 2. From the results, we can observe that: (1)

Method AUCMicro Macro

DMCNN 67.6 38.7
DMCNN+MIL 75.7 43.3
DMCNN+NA 70.6 25.8
DMCNN+ADV 85.5 50.7

DMBERT 70.6 42.2
DMBERT+MIL 79.4 47.3
DMBERT+NA 74.0 38.6
DMBERT+ADV 91.5 67.6

Table 2: The AUC results (%) of various models.

BERT-based models significantly outperform the
CNN-based models, which is due to the ability to
capture contextual information as well as large-
scale pre-training of BERT. And benefiting from
the effective pre-trained parameters, the BERT-
based models all have high precision when the re-
call is under 0.3. (2) The +NA models achieve
similar performance with +MIL and even outper-
form them in low-recall range, but +NA models
have the worst macro AUC. It indicates that the
separation of reliable and unreliable set is effec-
tive but also have severe side effects, and our ad-
versarial training method works well to overcome
the side effect. (3) Our adversarial training method
significantly outperforms all the baselines in every
metric. This demonstrates the strong ability of our
method to alleviate the noise problem on distantly
supervised scenarios.

4.3 Semi-supervised Scenarios

Dataset and Evaluation

For semi-supervised scenarios, we conduct exper-
iments on a widely-used benchmark dataset ACE-
2005 (Walker et al., 2006) containing 599 docu-
ments annotated with 8 types and 33 subtypes of
events. Following the previous work (Liao and Gr-
ishman, 2010b; Li et al., 2013; Chen et al., 2015),
we use the same test set containing 40 newswire
documents, development set with 30 randomly se-
lected documents and training set with the remain-
ing 529 documents.

As described in Section 3.4, using existing trig-
gers in ACE-2005 training set as heuristic seeds
and our trigger-based latent instance discovery
strategy, we construct a large-scale candidate set
from the New York Times corpus (Sandhaus,
2008) and use our adversarial training strategy to
filter out the noisy instances to build a new ACE-
style dataset. We extend the ACE-2005 training
set with the new dataset, and then test the models
trained on the extended training set on the orig-



1005

inal test set. Our models trained on the orig-
inal training set are named DMCNN and DM-
BERT, and our bootstrapped models trained on
the extended dataset are named DMCNN+Boot
and DMBERT+Boot.

We compare our bootstrapped models with var-
ious state-of-the-art methods on the ACE-2005
dataset, including: (1) The feature-based mod-
els. We select Li’s joint (Li et al., 2013) as
the representative, which achieves the best per-
formance among feature-based models. (2) The
vanilla neural network models, including the DM-
CNN (Chen et al., 2015) and JRNN (Nguyen
et al., 2016). (3) The neural network mod-
els with external information, including ANN-
FN (Liu et al., 2016a) leveraging the information
of FrameNet (Baker et al., 1998), DLRNN (Duan
et al., 2017) using document-level information,
GMLATT (Liu et al., 2018a) utilizing multi-
lingual attentions, and the bootstrapped model
DMCNN+Chen’s DS (Chen et al., 2017) trained
with additional data distantly supervised by Free-
Base (Bollacker et al., 2008). (4) The neural
network models with advanced architecture, in-
cluding: Bi-LSTM+GAN (Hong et al., 2018)
utilizing GAN to conduct self-regulation, GCN-
ED (Nguyen and Grishman, 2018) utilizing graph
convolutional network to model dependency trees.

Overall Evaluation Results

The results are shown in Table 3. From the re-
sults, we have the following observations: (1)
As compared with the basic DMCNN and DM-
BERT, the bootstrapped models achieve signifi-
cant improvement (+1.7% and +0.5%). Further-
more, our DMCNN+Boot model achieves simi-
lar performance with the ANN-FN and DLRNN
which design complex architectures to utilize the
additional information. These results indicate that
our methods can construct high-quality dataset
without sophisticated rules and large-scale knowl-
edge bases, and can effectively collect diverse in-
stances which will benefit training models. (2)
DMBERT and DMBERT+Boot achieve the best
performance among all the models. This is bene-
fiting from the effective architecture and the large-
scale pre-training information of BERT, as well
as the dynamic multi-pooling mechanism for ED.
Our methods augment the training data to further
enhance BERT, which achieve better performance
and demonstrate the effectiveness of our models.

Method
Trigger Identification

+Classification

P R F1

Li’s Joint (Li et al., 2013) 73.7 62.3 67.5
DMCNN (Chen et al., 2015) 75.6 63.6 69.1
JRNN (Nguyen et al., 2016) 66.0 73.0 69.3
ANN-FN (Liu et al., 2016a) 77.6 65.2 70.7
DLRNN (Duan et al., 2017) 77.2 64.9 70.5
GMLATT (Liu et al., 2018a) 78.9 66.9 72.4
DMCNN+Chen’s DS (Chen et al., 2017) 75.7 66.0 70.5
Bi-LSTM+GAN (Hong et al., 2018) 71.3 74.7 73.0
GCN-ED (Nguyen and Grishman, 2018) 77.9 68.8 73.1

DMBERT 77.6 71.8 74.6
DMCNN+Boot 77.7 65.1 70.8
DMBERT+Boot 77.9 72.5 75.1

Table 3: The overall performance (%) of different mod-
els on ACE-2005.

Method Average Precision Fleiss’s Kappa

Chen et al. (2017) 88.9 -
Zeng et al. (2018) 91.0 -
Our First Iteration 91.7 61.3
Our Second Iteration 87.5 52.0

Table 4: The human evaluation results (%) of auto-
labeled data in different iterations.

Manual Evaluation
To perform a fine-grained evaluation for the qual-
ity of the dataset constructed with our trigger-
based instance discovery strategy and adversarial
training strategy, we manually evaluate the preci-
sion of the constructed dataset. To be specific, we
randomly select 150 instances from the newly con-
structed dataset and recruit four well-trained anno-
tators to annotate the instances independently. We
ask the annotators to label an instance as correct
if and only if the trigger and event-type are both
correct. We use the Fleiss’ kappa (Fleiss, 1971) to
measure the annotation consistency among these
annotators. The results of the data distilled in
different iterations during adversarial training are
shown in Table 4. From the results, we can ob-
serve that the precision of the dataset constructed
with our models is comparable to existing dis-
tant supervision methods (Chen et al., 2017; Zeng
et al., 2018) using sophisticated human-designed
rules and knowledge bases, and even outperforms
them in the first iteration. It indicates that our
models can distill informative instances with high
precision.

Case Study
To further show the effectiveness of our models
to improve the coverage of the dataset, we give
an example in Table 5. The instance in the “In
ACE-2005” row is a typical instance of the Sue



1006

Event-Type: Justice Subtype: Sue

In ACE-2005 Dell sued for ”bait and switch” and false promises.

Discovered
1. The lawyers for the four former state officials who
have been sued told the jurors . . .
2. But litigation held up the project until . . . .

Table 5: The examples with highlighting triggers.

events, and the two instances in the “Discovered”
row are sampled from the dataset constructed with
our methods. In the “Discovered” row, the first in-
stance is with an existing trigger in ACE-2005 but
different in syntax, and the second instance is with
a newly discovered trigger which is not contained
in ACE-2005. In our extended dataset, there are
1.2% of the triggers are newly discovered. This
demonstrates that our methods can not only find
new instances from the unlabeled data which is
similar to those instances in the labeled data, but
also discover new triggers and extend the coverage
of datasets substantially.

5 Conclusion and Future Work

In this paper, we take advantages of adversar-
ial training and propose an effective method for
weakly supervised ED. To be specific, our method
is able to denoise and enhance distantly supervised
ED models, as well as automatically construct
more diverse and accurate training data for semi-
supervised ED models. The experiments on two
real-world datasets show that our method achieves
the state-of-the-art results on the settings of both
distant supervision and semi-supervision. In the
future, we plan to explore the following directions:
(1) We will extend our method to further extract
event arguments and perform event extraction. (2)
We will develop a large-scale and clean dataset for
ED based on our method, which will benefit fur-
ther research in this field.

Acknowledgments

This work is supported by the National Key Re-
search and Development Program of China (No.
2018YFB1004503), the National Natural Science
Foundation of China (NSFC No. 61572273) and
China Association for Science and Technology
(2016QNRC001). Wang is also supported by the
Tsinghua University Initiative Scientific Research
Program.

References
David Ahn. 2006. The stages of event extraction. In

Proceedings of ACL Workshop on Annotating and
Reasoning about Time and Events, pages 1–8.

Jun Araki and Teruko Mitamura. 2015. Joint event
trigger identification and event coreference resolu-
tion with structured perceptron. In Proceedings of
EMNLP, pages 2074–2080.

Jun Araki and Teruko Mitamura. 2018. Open-domain
event detection using distant supervision. In Pro-
ceedings of COLING, pages 878–891.

Collin F Baker, Charles J Fillmore, and John B Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of COLING, pages 86–90.

P Basile, A Caputo, G Semeraro, and L Siciliani. 2014.
Extending an information retrieval system through
time event extraction. In Proceedings of DART,
pages 36–47.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of SIGMOD, pages
1247–1250.

Yixin Cao, Lei Hou, Juanzi Li, Zhiyuan Liu,
Chengjiang Li, Xu Chen, and Tiansi Dong. 2018.
Joint representation learning of cross-lingual words
and entities via attentive distant supervision. In Pro-
ceedings of EMNLP, pages 227–237.

Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi,
and Cho-Jui Hsieh. 2018. Attacking visual language
grounding with adversarial examples: A case study
on neural image captioning. In Proceedings of ACL,
pages 2587–2597.

Yubo Chen, Shulin Liu, Xiang Zhang, Kang Liu, and
Jun Zhao. 2017. Automatically labeled data genera-
tion for large scale event extraction. In Proceedings
of ACL, pages 409–419.

Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng,
and Jun Zhao. 2015. Event extraction via dynamic
multi-pooling convolutional neural networks. In
Proceedings of ACL-IJCNLP, pages 167–176.

Pengxiang Cheng and Katrin Erk. 2018. Implicit argu-
ment prediction with event knowledge. In Proceed-
ings of ACL, pages 831–840.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. arXiv preprint arXiv:1810.04805.

Shaoyang Duan, Ruifang He, and Wenli Zhao. 2017.
Exploiting document level information to improve
event detection via recurrent neural networks. In
Proceedings of IJCNLP, pages 352–361.

http://aclweb.org/anthology/W06-0901
https://doi.org/10.18653/v1/D15-1247
https://doi.org/10.18653/v1/D15-1247
https://doi.org/10.18653/v1/D15-1247
http://aclweb.org/anthology/C18-1075
http://aclweb.org/anthology/C18-1075
http://aclweb.org/anthology/C98-1013
http://ceur-ws.org/Vol-1314/paper-04.pdf
http://ceur-ws.org/Vol-1314/paper-04.pdf
https://doi.org/10.1145/1376616.1376746
https://doi.org/10.1145/1376616.1376746
https://doi.org/10.1145/1376616.1376746
http://aclweb.org/anthology/D18-1021
http://aclweb.org/anthology/D18-1021
http://www.aclweb.org/anthology/P18-1241
http://www.aclweb.org/anthology/P18-1241
http://www.aclweb.org/anthology/P18-1241
http://aclweb.org/anthology/P17-1038
http://aclweb.org/anthology/P17-1038
https://doi.org/10.3115/v1/P15-1017
https://doi.org/10.3115/v1/P15-1017
https://doi.org/10.18653/v1/N18-1076
https://doi.org/10.18653/v1/N18-1076
https://arxiv.org/pdf/1810.04805
https://arxiv.org/pdf/1810.04805
https://arxiv.org/pdf/1810.04805
http://www.aclweb.org/anthology/I17-1036
http://www.aclweb.org/anthology/I17-1036


1007

Xiaocheng Feng, Lifu Huang, Duyu Tang, Bing
Qin, Heng Ji, and Ting Liu. 2016. A language-
independent neural network for event detection. In
Proceedings of ACL, pages 66–71.

James Ferguson, Colin Lockard, Daniel Weld, and
Hannaneh Hajishirzi. 2018. Semi-supervised event
extraction with paraphrase clusters. In Proceedings
of NAACL, pages 359–364.

Joseph L Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological bulletin,
76(5):378.

Reza Ghaeini, Xiaoli Fern, Liang Huang, and Prasad
Tadepalli. 2016. Event nugget detection with
forward-backward recurrent neural networks. In
Proceedings of ACL, pages 369–373.

Ian J Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2014. Explaining and harnessing adversar-
ial examples. In Proceedings of ICLR, pages 2574–
2582.

Prashant Gupta and Heng Ji. 2009. Predicting un-
known time arguments based on cross-event propa-
gation. In Proceedings of ACL-IJCNLP, pages 369–
372.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2018. De-
noising distant supervision for relation extraction via
instance-level adversarial training. arXiv preprint
arXiv:1805.10959.

Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao,
Guodong Zhou, and Qiaoming Zhu. 2011. Using
cross-entity inference to improve event extraction.
In Proceedings of ACL-HLT, pages 1127–1136.

Yu Hong, Wenxuan Zhou, Guodong Zhou, Qiaoming
Zhu, et al. 2018. Self-regulation: Employing a gen-
erative adversarial network to improve event detec-
tion. In Proceedings of ACL, pages 515–526.

Lifu Huang, Taylor Cassidy, Xiaocheng Feng, Heng Ji,
Clare R. Voss, Jiawei Han, and Avirup Sil. 2016.
Liberal event extraction and event schema induction.
In Proceedings of ACL, pages 258–268.

Ruihong Huang and Ellen Riloff. 2012a. Bootstrapped
training of event extraction classifiers. In Proceed-
ings of EACL, pages 286–295.

Ruihong Huang and Ellen Riloff. 2012b. Modeling
textual cohesion for event extraction. In Proceed-
ings of AAAI, pages 1664–1670.

Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of ACL, pages 254–262.

Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of ACL, pages 73–82.

Shasha Liao and Ralph Grishman. 2010a. Filtered
ranking for bootstrapping in event extraction. In
Proceedings of COLING, pages 680–688.

Shasha Liao and Ralph Grishman. 2010b. Using doc-
ument level cross-event inference to improve event
extraction. In Proceedings of ACL, pages 789–797.

Hongyu Lin, Yaojie Lu, Xianpei Han, and Le Sun.
2018. Nugget proposal networks for Chinese event
detection. In Proceedings of ACL, pages 1565–
1574.

Jian Liu, Yubo Chen, Kang Liu, and Jun Zhao. 2018a.
Event detection via gated multilingual attention
mechanism. In Proceedings of AAAI, pages 4865–
4872.

Shaobo Liu, Rui Cheng, Xiaoming Yu, and Xueqi
Cheng. 2018b. Exploiting contextual information
via dynamic memory network for event detection.
In Proceedings of EMNLP, pages 1030–1035.

Shulin Liu, Yubo Chen, Shizhu He, Kang Liu, and Jun
Zhao. 2016a. Leveraging Framenet to improve auto-
matic event detection. In Proceedings of ACL, vol-
ume 1, pages 2134–2143.

Shulin Liu, Yubo Chen, Kang Liu, and Jun Zhao. 2017.
Exploiting argument information to improve event
detection via supervised attention mechanisms. In
Proceedings of ACL, pages 1789–1798.

Shulin Liu, Kang Liu, Shizhu He, and Jun Zhao. 2016b.
A probabilistic soft logic based approach to exploit-
ing latent and global information in event classifica-
tion. In Proceedings of AAAI, pages 2993–2999.

David McClosky, Mihai Surdeanu, and Christopher D
Manning. 2011. Event extraction as dependency
parsing. In Proceedings of ACL, pages 1626–1635.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proceedings of ICLR.

George A Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J Miller.
1990. Introduction to WordNet: An on-line lexi-
cal database. International journal of lexicography,
3(4):235–244.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of ACL-
IJCNLP, pages 1003–1011.

Takeru Miyato, Shin-ichi Maeda, Masanori Koyama,
Ken Nakae, and Shin Ishii. 2016. Distributional
smoothing with virtual adversarial training. Pro-
ceedings of ICLR.

Aldrian Obaja Muis, Naoki Otani, Nidhi Vyas,
Ruochen Xu, Yiming Yang, Teruko Mitamura, and
Eduard Hovy. 2018. Low-resource cross-lingual
event type detection via distant supervision with
minimal effort. In Proceedings of COLING, pages
70–82.

https://doi.org/10.18653/v1/P16-2011
https://doi.org/10.18653/v1/P16-2011
https://doi.org/10.18653/v1/N18-2058
https://doi.org/10.18653/v1/N18-2058
http://psycnet.apa.org/record/1972-05083-001
http://psycnet.apa.org/record/1972-05083-001
https://doi.org/10.18653/v1/P16-2060
https://doi.org/10.18653/v1/P16-2060
https://arxiv.org/pdf/1412.6572.pdf
https://arxiv.org/pdf/1412.6572.pdf
http://www.aclweb.org/anthology/P/P09/P09-2093
http://www.aclweb.org/anthology/P/P09/P09-2093
http://www.aclweb.org/anthology/P/P09/P09-2093
https://arxiv.org/pdf/1805.10959.pdf
https://arxiv.org/pdf/1805.10959.pdf
https://arxiv.org/pdf/1805.10959.pdf
http://aclweb.org/anthology/P11-1113
http://aclweb.org/anthology/P11-1113
http://aclweb.org/anthology/P18-1048
http://aclweb.org/anthology/P18-1048
http://aclweb.org/anthology/P18-1048
https://doi.org/10.18653/v1/P16-1025
http://aclweb.org/anthology/E12-1029
http://aclweb.org/anthology/E12-1029
https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5113/5316
https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5113/5316
http://aclweb.org/anthology/P08-1030
http://aclweb.org/anthology/P08-1030
http://aclweb.org/anthology/P13-1008
http://aclweb.org/anthology/P13-1008
http://aclweb.org/anthology/P13-1008
http://aclweb.org/anthology/C10-1077
http://aclweb.org/anthology/C10-1077
http://aclweb.org/anthology/P10-1081
http://aclweb.org/anthology/P10-1081
http://aclweb.org/anthology/P10-1081
http://aclweb.org/anthology/P18-1145
http://aclweb.org/anthology/P18-1145
https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16371
https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16371
http://aclweb.org/anthology/D18-1127
http://aclweb.org/anthology/D18-1127
http://aclweb.org/anthology/P16-1201
http://aclweb.org/anthology/P16-1201
https://doi.org/10.18653/v1/P17-1164
https://doi.org/10.18653/v1/P17-1164
https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11990
https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11990
https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11990
http://aclweb.org/anthology/P11-1163
http://aclweb.org/anthology/P11-1163
https://arxiv.org/pdf/1301.3781.pdf
https://arxiv.org/pdf/1301.3781.pdf
http://wordnetcode.princeton.edu/5papers.pdf
http://wordnetcode.princeton.edu/5papers.pdf
http://aclweb.org/anthology/P09-1113
http://aclweb.org/anthology/P09-1113
https://arxiv.org/pdf/1507.00677.pdf
https://arxiv.org/pdf/1507.00677.pdf
http://aclweb.org/anthology/C18-1007
http://aclweb.org/anthology/C18-1007
http://aclweb.org/anthology/C18-1007


1008

Thien Nguyen and Ralph Grishman. 2018. Graph con-
volutional networks with argument-aware pooling
for event detection. In Proceedings of AAAI, pages
5900–5907.

Thien Huu Nguyen, Kyunghyun Cho, and Ralph Gr-
ishman. 2016. Joint event extraction via recurrent
neural networks. In Proceedings of NAACL, pages
300–309.

Thien Huu Nguyen and Ralph Grishman. 2015. Event
detection and domain adaptation with convolutional
neural networks. In Proceedings of ACL-IJCNLP,
pages 365–371.

Pengda Qin, Weiran Xu, and William Yang Wang.
2018. DSGAN: Generative adversarial training for
distant supervision relation extraction. In Proceed-
ings of ACL, pages 496–505.

Alec Radford, Luke Metz, and Soumith Chintala.
2016. Unsupervised representation learning with
deep convolutional generative adversarial networks.
In Proceedings of ICLR.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of ECML-PKDD,
pages 148–163.

Evan Sandhaus. 2008. The New York Times annotated
corpus. Linguistic Data Consortium, Philadelphia,
6(12):e26752.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and
Rob Fergus. 2013. Intriguing properties of neural
networks. arXiv preprint arXiv:1312.6199.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proceedings of NIPS, pages 5998–
6008.

Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. ACE 2005 multilin-
gual training corpus. Linguistic Data Consortium,
Philadelphia, 57.

Xiaozhi Wang, Xu Han, Yankai Lin, Zhiyuan Liu, and
Maosong Sun. 2018. Adversarial multi-lingual neu-
ral relation extraction. In Proceedings of COLING,
pages 1156–1166.

Yi Wu, David Bamman, and Stuart Russell. 2017. Ad-
versarial training for relation extraction. In Proceed-
ings of EMNLP, pages 1778–1783.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144.

Ziang Xie, Sida I Wang, Jiwei Li, Daniel Lévy, Aiming
Nie, Dan Jurafsky, and Andrew Y Ng. 2017. Data
noising as smoothing in neural network language
models. Proceedings of ICLR.

Bishan Yang and Tom Mitchell. 2016. Joint extraction
of events and entities within a document context. In
Proceedings of NAACL, pages 289–299.

Hang Yang, Yubo Chen, Kang Liu, Yang Xiao, and Jun
Zhao. 2018. DCFEE: a document-level Chinese fi-
nancial event extraction system based on automati-
cally labeled training data. In Proceedings of ACL,
pages 50–55.

Hui Yang, Tat-Seng Chua, Shuguang Wang, and Chun-
Keat Koh. 2003. Structured use of external knowl-
edge for event-based open domain question answer-
ing. In Proceedings of SIGIR, pages 33–40.

Quan Yuan, Xiang Ren, Wenqi He, Chao Zhang, Xinhe
Geng, Lifu Huang, Heng Ji, Chin-Yew Lin, and Ji-
awei Han. 2018. Open-schema event profiling for
massive news corpora. In Proceedings of CIKM,
pages 587–596.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
piecewise convolutional neural networks. In Pro-
ceedings of EMNLP, pages 1753–1762.

Ying Zeng, Yansong Feng, Rong Ma, and Zheng Wang.
2018. Scale up event extraction learning via auto-
matic training data generation. In Proceedings of
AAAI, pages 878–891.

Tongtao Zhang, Spencer Whitehead, Hanwang Zhang,
Hongzhi Li, Joseph Ellis, Lifu Huang, Wei Liu,
Heng Ji, and Shih-Fu Chang. 2017. Improving event
extraction via multimodal integration. In Proceed-
ings of MM, pages 270–278.

Yue Zhao, Xiaolong Jin, Yuanzhuo Wang, and Xueqi
Cheng. 2018. Document embedding enhanced event
detection with hierarchical and supervised attention.
In Proceedings of ACL, pages 414–419.

https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16329
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16329
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16329
https://doi.org/10.18653/v1/N16-1034
https://doi.org/10.18653/v1/N16-1034
https://doi.org/10.3115/v1/P15-2060
https://doi.org/10.3115/v1/P15-2060
https://doi.org/10.3115/v1/P15-2060
http://aclweb.org/anthology/P18-1046
http://aclweb.org/anthology/P18-1046
https://arxiv.org/pdf/1511.06434.pdf
https://arxiv.org/pdf/1511.06434.pdf
https://doi.org/10.1007/978-3-642-15939-8_10
https://doi.org/10.1007/978-3-642-15939-8_10
https://catalog.ldc.upenn.edu/LDC2008T19
https://catalog.ldc.upenn.edu/LDC2008T19
https://arxiv.org/pdf/1312.6199.pdf
https://arxiv.org/pdf/1312.6199.pdf
https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
https://catalog.ldc.upenn.edu/LDC2006T06
https://catalog.ldc.upenn.edu/LDC2006T06
http://aclweb.org/anthology/C18-1099
http://aclweb.org/anthology/C18-1099
http://www.aclweb.org/anthology/D17-1187
http://www.aclweb.org/anthology/D17-1187
https://arxiv.org/pdf/1609.08144.pdf
https://arxiv.org/pdf/1609.08144.pdf
https://arxiv.org/pdf/1609.08144.pdf
https://arxiv.org/pdf/1703.02573.pdf
https://arxiv.org/pdf/1703.02573.pdf
https://arxiv.org/pdf/1703.02573.pdf
http://aclweb.org/anthology/N16-1033"
http://aclweb.org/anthology/N16-1033"
http://aclweb.org/anthology/P18-4009
http://aclweb.org/anthology/P18-4009
http://aclweb.org/anthology/P18-4009
https://doi.org/10.1145/860435.860444
https://doi.org/10.1145/860435.860444
https://doi.org/10.1145/860435.860444
https://doi.org/10.1145/3269206.3271674
https://doi.org/10.1145/3269206.3271674
https://doi.org/10.18653/v1/D15-1203
https://doi.org/10.18653/v1/D15-1203
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16119
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16119
https://doi.org/10.1145/3123266.3123294
https://doi.org/10.1145/3123266.3123294
http://aclweb.org/anthology/P18-2066
http://aclweb.org/anthology/P18-2066

