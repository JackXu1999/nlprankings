



















































Extrapolation in NLP


Proceedings of the Workshop on Generalization in the Age of Deep Learning, pages 28–33
New Orleans, Louisiana, June 5, 2018. c©2018 Association for Computational Linguistics

Extrapolation in NLP

Jeff Mitchell, Pasquale Minervini, Pontus Stenetorp and Sebastian Riedel
University College London

Department of Computer Science
{j.mitchell, p.minervini, p.stenetorp, s.riedel}@cs.ucl.ac.uk

Abstract

We argue that extrapolation to examples out-
side the training space will often be easier for
models that capture global structures, rather
than just maximise their local fit to the training
data. We show that this is true for two popular
models: the Decomposable Attention Model
and word2vec.

1 Introduction

In a controversial essay, Marcus (2018a) draws the
distinction between two types of generalisation:
interpolation and extrapolation; with the former
being predictions made between the training data
points, and the latter being generalisation outside
this space. He goes on to claim that deep learning
is only effective at interpolation, but that human
like learning and behaviour requires extrapolation.

On Twitter, Thomas Diettrich rebutted this
claim with the response that no methods extrapo-
late; that what appears to be extrapolation from X
to Y is interpolation in a representation that makes
X and Y look the same. 1

It is certainly true that extrapolation is hard, but
there appear to be clear real-world examples. For
example, in 1705, using Newton’s then new in-
verse square law of gravity, Halley predicted the
return of a comet 75 years in the future. This
prediction was not only possible for a new ce-
lestial object for which only a limited amount of
data was available, but was also effective on an or-
bital period twice as long as any of those known
to Newton. Pre-Newtonian models required a set
of parameters (deferents, epicycles, equants, etc.)
for each body and so would struggle to generalise
from known objects to new ones. Newton’s theory
of gravity, in contrast, not only described celes-

1https://twitter.com/tdietterich/
status/948811920001282049

Figure 1: Generalising to unseen data: dotted line =
training manifold; black arrows = interpolation; grey
arrows = extrapolation. Both directions are represented
globally in the training data, but local interpolation is
only effective in one of them at each point.

tial orbits but also predicted the motion of bodies
thrown or dropped on Earth.

In fact, most scientists would regard this sort
of extrapolation to new phenomena as a vital test
of any theory’s legitimacy. Thus, the question of
what is required for extrapolation is reasonably
important for the development of NLP and deep
learning.

Marcus (2018a) proposes an experiment, con-
sisting of learning the identity function for binary
numbers, where the training set contains only the
even integers but at test time the model is required
to generalise to even numbers. A standard multi-
layer perceptron (MLP) applied to this data fails
to learn anything about the least significant bit in
input and output, as it is constant throughout the
training set, and therefore fails to generalise to the
test set. Many readers of the article ridiculed the
task and questioned its relevance. Here, we will
argue that it is surprisingly easy to solve Marcus’
even-odd task and that the problem it illustrates is
actually endemic throughout machine learning.

Marcus (2018a) links his experiment to the sys-
tematic ways in which the meaning and use of a
word in one context is related to its meaning and

28



use in another (Fodor and Pylyshyn, 1988; Lake
and Baroni, 2017). These regularities allow us to
extrapolate from sometimes even a single use of a
word to understand all of its other uses.

In fact, we can often use a symbol effectively
with no prior data. For example, a language
user that has never have encountered the sym-
bol Socrates before may nonetheless be able to
leverage their syntactic, semantic and inferential
skills to conclude that Socrates is mortal contra-
dicts Socrates is not mortal.

Marcus’ experiment essentially requires extrap-
olating what has been learned about one set of
symbols to a new symbol in a systematic way.
However, this transfer is not facilitated by the
techniques usually associated with improving gen-
eralisation, such as L2-regularisation (Tikhonov,
1963), drop-out (Srivastava et al., 2014) or prefer-
ring flatter optima (Hochreiter and Schmidhuber,
1995).

In the next section, we present four ways to
solve this problem and discuss the role of global
symmetry in effective extrapolation to the unseen
digit. Following that we present practical exam-
ples of global structure in the representation of
sentences and words. Global, in these examples,
means a model form that introduces dependencies
between distant regions of the input space.

2 Four Ways to Learn the Identity
Function

The problem is described concretely by Marcus
(1998), with inputs and outputs both consisting of
five units representing the binary digits of the in-
tegers zero to thirty one. The training data con-
sists of the binary digits of the even numbers
(0, 2, 4, 8, . . . , 30) and the test set consists of the
odd numbers (1, 3, 5, 7, . . . , 31). The task is to
learn the identity function from the training data
in a way that generalises to the test set.

The first model (SLP) we consider is a simple
linear single layer perceptron from input to output.

In the second model (FLIP), we employ a change
of representation. Although the inputs and outputs
are given and fixed in terms of the binary digits 1
and 0, we will treat these as symbols and exploit
the freedom to encode these into numeric values
in the most effective way for the task. Specifically,
we will represent the digit 1 with the number 0 and
the digit 0 with the number 1. Again, the network
will be a linear single layer perceptron without bi-

Model Train Test

SLP 8.12e-06 0.99
FLIP 6.79e-05 1.04e-05
ORTHO 1.27e-04 4.09e-05
CONV 1.71e-04 3.20e-05
PROJ 5.15e-06 8.07e-06

Table 1: Mean Squared Error on the Train (even num-
bers) and Test (odd numbers) Sets.

ases.
Returning to the original common-sense repre-

sentation, 1 → 1 and 0 → 0, the third model
(ORTHO) attempts to improve generalisation by
imposing a global condition on the matrix of
weights in the linear weights. In particular, we re-
quire that the matrix is orthogonal, and apply the
absolute value function at the output to ensure the
outputs are not negative.

For the fourth model (CONV), we use a linear
Convolutional Neural Network (ConvNet, Lecun
et al., 1998) with a filter of width five. In other
words, the network weights define a single linear
function that is shifted across the inputs for each
output position.

Finally, in our fifth model (PROJ) we employ an-
other change of representation, this time a dimen-
sionality reduction technique. Specifically, we
project the 5-dimensional binary digits d onto an
n dimensional vector r and carry out the learning
using an n-to-n layer in this smaller space.

r = Ad (1)

where the entries of the matrix A are Aij =
eβ(j−i). In each case, our loss and test evaluation
is based on squared error between target and pre-
dicted outputs.

Training. Each model is implemented in Ten-
sorFlow (Abadi et al., 2015) and optimised for
1,000 epochs. In Eq. (1), we find that values of
β = ln(2) and n = 1 work well in practice.

Results. As can be seen in Table 1, SLP fails to
learn a function that generalises to the test set.
In contrast, all the other models (FLIP, ORTHO,
CONV, PROJ) generalise almost perfectly to the
test set. Thus, we are left with four potential ap-
proaches to learning the identity function. Is low-
est test set error the most appropriate means of
choosing between them?

29



Discussion. This decision probably isn’t as mo-
mentous as the choice discussed by Galileo in his
Dialogue Concerning the Two Chief World Sys-
tems, where he presented the arguments for and
against the heliocentric and geocentric models of
planetary motion. These pre-Newtonian models
could, in principle, attain as much predictive accu-
racy as desired, given enough data, by simply in-
corporating more epicycloids for each planet. On
the other hand, they could not extrapolate beyond
the bodies in that training data. Here, we will try to
extract something useful from our results by con-
sidering how each model might generalise to other
data and problems.

Although FLIP has the second lowest test set er-
ror, it is at best a cheap hack2 which works only
in the limited circumstance of this particular prob-
lem. If there were more than a single fixed digit in
the training data, this trick would not work.

ORTHO suffers from the same problem, though
it does embody the principle that everything in the
input should end up in the output which seems to
be part of this task.

CONV on the other hand will generalise to any
size of input and output, and will even generalise
to multiplication by powers of 2, rather than just
learning the identity function.

PROJ, with the values β = ln(2) and n = 1,
boils down to converting the binary digits into the
equivalent single real value and learning the iden-
tity function via linear regression. This approach
will extrapolate to values of any magnitude3 and
generalise to learning any linear function, rather
than just the identity. As such, it is probably
the only practically sensible solution, although it
cheats by avoiding the central difficulty in the orig-
inal problem.

At its most general, this central difficulty is the
problem of extrapolating in a direction that is per-
pendicular to the training manifold. The even
number inputs lay on a 4 dimensional subspace,
while the odd numbers were displaced in a direc-
tion at right angles to that subspace. In this general
form, the problem of how to respond to variation
in the test set that is perpendicular to the training
manifold lacks a well-defined unique solution, and

2Nonetheless, such tricks are hardly unknown in machine
learning research.

3Generalisation to values outside the training set would
not be so successful had we used an MLP rather than a uni-
form linear function. Fitting to the training set using sigmoids
will not yield a function that continues to approximate the
identity very far beyond its range in the training set.

this helps to explain why many people dismissed
the task entirely.

However, this problem is in fact pervasive in
most of machine learning. Training instances will
typically lie on a low dimensional manifold and
effective generalisation to new data sources will
commonly require handling variation that is or-
thogonal to that manifold in an appropriate man-
ner, e.g. Fig. 1. If prediction is based on local
interpolation using a highly non-linear function,
then no amount of smoothing of the fit will help.

Convolution is able to extrapolate from even
to odd numbers because it exploits the key struc-
ture of the ordering of digits that a human would
use. A human, given this task, would recognise
the correspondence between input and output po-
sitions and then apply the same copying operation
at each digit, which is essentially what convolu-
tion learns to do. It implicitly assumes that there
is a global translational symmetry4 across input
positions, and this reduces the number of param-
eters and allows generalisation from one digit to
another.

Returning to the linguistic question that inspired
the task, we can think of systematicity in terms of
symmetries that preserve the meaning of a word
or sentence (Kiddon and Domingos, 2015). Ide-
ally, our NLP models should embody or learn the
symmetries that allow the same meaning to be ex-
pressed within multiple grammatical structures.

Unfortunately, syntax is complex and prohibits
a short and clear investigation here. On the other
hand, relations between sentences (e.g. contradic-
tion) sometimes have much simpler symmetries.
In the next section, we examine how global sym-
metries can be exploited in an inference task.

3 Global Symmetries in Natural
Language Inference

The Stanford Natural Language Inference (SNLI,
Bowman et al., 2015) dataset attempts to provide
training and evaluation data for the task of cat-
egorising the logical relationship between a pair
of sentences. Systems must identify whether each
hypothesis stands in a relation of entailment, con-
tradiction or neutral to its corresponding premise.
A number of neural net architectures have been

4Coincidentally, the rejection of the Earth centred model
in favour of planetary motions orbiting the Sun played an im-
portant role in the recognition that the laws of physics also
have a global translational symmetry, i.e. that no point in
space is privileged or special.

30



proposed that effectively learn to make test set pre-
dictions based purely on patterns learned from the
training data, without additional knowledge of the
real world or of the logical structure of the task.

Here, we evaluate the Decomposable Attention
Model (DAM, Parikh et al., 2016) in terms of its
ability to extrapolate to novel instances, consisting
of contradictions from the original test set which
have been reversed. For a human that understands
the task, such generalisation is obvious: knowing
that A contradicts B is equivalent to knowing that
B contradicts A. However, it is not at all clear that
a model will learn this symmetry from the SNLI
data, without it being imposed on the model in
some way. Consequently we also evaluate a modi-
fication, S-DAM, where this constraint is enforced
by design.

Models. Both models build representations, vp
and vh, of premise and hypothesis in attend and
compare steps. The original DAM model then
combines these representations by concatenating
them and then transforming and aggregating the
result to produce a final representation uph, form-
ing the input to a 3-way softmax:

uph = t(vp;vh),

p(i) = s(uph ·Wi), with i ∈ {c, e, n}.
(2)

In S-DAM, we break the prediction into two de-
cisions: contradiction vs. non-contradiction fol-
lowed by entailment vs. neutral. The first deci-
sion is symmetrised by concatenating the vectors
in both orders and then summing the output of
the same transformation applied to both concate-
nations:

ũph = t(vp;vh) + t(vh;vp),

p(j) = s(ũph · W̃j), with j ∈ {c,¬c}.
(3)

Predictions for entailment and neutral are then
made conditioned on ¬c:

ūph = t(vp;vh),

p(k|¬c) = s(ūph · W̄k), with k ∈ {e, n}.
(4)

Results. Table 2 gives the accuracies for both
models on the whole SNLI test set, the subset of
contradictions, and the same set of contradictions
reversed. In the last row, the DAM model suffers a
significant fall in performance when the contradic-
tions are reversed. In comparison, the S-DAM’s
performance is almost identical on both sets.

Instances DAM S-DAM

Whole Test Set 86.71% 85.95%
Contradictions 85.94% 85.69%
Reversed Contradictions 78.13% 85.20%

Table 2: Accuracy on all instances, contradictions and
reversed contradictions from the SNLI test set.

Thus, the S-DAM model extrapolates more ef-
fectively because its architecture exploits a global
symmetry of the relation between sentences in
the task. In the following section, we investi-
gate a global symmetry within the representation
of words.

4 Global Structure in Word Embeddings

Word embeddings, such as GloVe (Pennington
et al., 2014) and word2vec (Mikolov et al., 2013),
have been enormously effective as input repre-
sentations for downstream tasks such as question
answering or natural language inference. One
well known application is the king = queen −
woman+man example, which represents an im-
pressive extrapolation from word co-occurrence
statistics to linguistic analogies (Levy and Gold-
berg, 2014). To some extent, we can see this pre-
diction as exploiting a global structure in which
the differences between analogical pairs, such as
man − woman, king − queen and father −
mother, are approximately equal.

Here, we consider how this global structure in
the learned embeddings is related to a linearity in
the training objective. In particular, linear func-
tions have the property that f(a + b) = f(a) +
f(b), imposing a systematic relation between the
predictions we make for a, b and a+ b. In fact, we
could think of this as a form of translational sym-
metry where adding a to the input has the same
effect on the output throughout the space.

We hypothesise that breaking this linearity, and
allowing a more local fit to the training data will
undermine the global structure that the analogy
predictions exploit.

Models. These embedding models typically rely
on a simple dot product comparison of target and
context vectors as the basis for predicting some
measure of co-occurrence s:

s = f

(∑

i

targeti · contexti
)
. (5)

31



D Linear Non-Linear

100 50.38% 42.96%
200 53.18% 40.66%
400 50.77% 32.43%

Table 3: Accuracy on the analogy task.

We replace this simple linear function of the con-
text vectors, with a set of non-linear broken-stick
functions gi( · ).

s = f

(∑

i

gi (contexti)

)
,

gi (x) =

{
mix if nix+ ci < 0,
(mi + ni)x+ ci otherwise.

We modify the CBOW algorithm in the publicly
available word2vec code to incorporate this non-
linearity and train on the commonly used text8 cor-
pus of 17M words from Wikipedia. As this modi-
fication doubles the number of parameters used for
each word, we test models of dimensions 100, 200
and 400.

Results. Table 3 reports the performance on
the standard analogy task distributed with the
word2vec code. The non-linear modification of
CBOW is substantially less successful than the
original linear version on this task. This is true
on all the sizes of models we evaluated, indicating
that this decrease is not simply a result of over-
parameterisation.

Thus, destroying the global linearity in the em-
bedding model undermines extrapolation to the
analogy task.

5 Conclusions

Language is a very complex phenomenon, and
many of its quirks and idioms need to be treated as
local phenomena. However, we have also shown
here examples in the representation of words and
sentences where global structure supports extrap-
olation outside the training data.

One tool for thinking about this dichotomy is
the equivalent kernel (Silverman, 1984), which
measures the extent to which a given prediction
is influenced by nearby training examples. Typi-
cally, models with highly local equivalent kernels
- e.g. splines, sigmoids and random forests - are
preferred over non-local models - e.g. polynomi-

als - in the context of general curve fitting (Hastie
et al., 2001).

However, these latter functions are also typi-
cally those used to express fundamental scientific
laws - e.g. E = mc2, F = Gm1m2

r2
- which fre-

quently support extrapolation outside the original
data from which they were derived. Local models,
by their very nature, are less suited to making pre-
dictions outside the training manifold, as the influ-
ence of those training instances attenuates quickly.

We suggest that NLP will benefit from incorpo-
rating more global structure into its models. Exist-
ing background knowledge is one possible source
for such additional structure (Marcus, 2018b; Min-
ervini et al., 2017). But it will also be necessary to
uncover novel global relations, following the ex-
ample of the other natural sciences.

We have used the development of the scientific
understanding of planetary motion as a repeated
example of the possibility of uncovering global
structures that support extrapolation, throughout
our discussion. Kepler and Newton found laws
that went beyond simply maximising the fit to the
known set of planetary bodies to describe regular-
ities that held for every body, terrestrial and heav-
enly.

In our SNLI example, we showed that simply
maximising the fit on the development and test
sets does not yield a model that extrapolates to re-
versed contradictions. In the case of word2vec, we
showed that performance on the analogy task was
related to the linearity in the objective function.

More generally, we want to draw attention to the
need for models in NLP that make meaningful pre-
dictions outside the space of the training data, and
to argue that such extrapolation requires distinct
modelling techniques from interpolation within
the training space. Specifically, whereas the latter
can often effectively rely on local smoothing be-
tween training instances, the former may require
models that exploit global structures of the lan-
guage phenomena.

Acknowledgments

The authors are immensely grateful to Ivan
Sanchez Carmona for many fruitful disagree-
ments. This work has been supported by the Eu-
ropean Union H2020 project SUMMA (grant No.
688139), and by an Allen Distinguished Investiga-
tor Award.

32



References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene

Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dandelion Mané, Rajat Monga, Sherry
Moore, Derek Murray, Chris Olah, Mike Schus-
ter, Jonathon Shlens, Benoit Steiner, Ilya Sutskever,
Kunal Talwar, Paul Tucker, Vincent Vanhoucke,
Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals,
Pete Warden, Martin Wattenberg, Martin Wicke,
Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow:
Large-scale machine learning on heterogeneous sys-
tems. Software available from tensorflow.org.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infer-
ence. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing
(EMNLP). Association for Computational Linguis-
tics.

Jerry A. Fodor and Zenon W. Pylyshyn. 1988. Connec-
tionism and cognitive architecture. Cognition, 28(1-
2):3–71.

Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2001. The Elements of Statistical Learning.
Springer Series in Statistics. Springer New York
Inc., New York, NY, USA.

Sepp Hochreiter and Jrgen Schmidhuber. 1995. Sim-
plifying neural nets by discovering flat minima. In
Advances in Neural Information Processing Systems
7, pages 529–536. MIT Press.

Chloé Kiddon and Pedro Rauel Cândido Domin-
gos. 2015. Symmetry-based semantic pars-
ing. Https://homes.cs.washington.edu/ pe-
drod/papers/sp14.pdf.

B. M. Lake and M. Baroni. 2017. Generalization with-
out systematicity: On the compositional skills of
sequence-to-sequence recurrent networks. ArXiv e-
prints.

Yann Lecun, Lon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. In Proceedings of the IEEE,
pages 2278–2324.

Omer Levy and Yoav Goldberg. 2014. Linguistic regu-
larities in sparse and explicit word representations.
In Proceedings of the Eighteenth Conference on
Computational Natural Language Learning, CoNLL
2014, pages 171–180.

G. Marcus. 2018a. Deep Learning: A Critical Ap-
praisal. ArXiv e-prints.

G. Marcus. 2018b. Innateness, AlphaZero, and Artifi-
cial Intelligence. ArXiv e-prints.

Gary F. Marcus. 1998. Rethinking eliminative connec-
tionism. Cognitive Psychology, 37:243–282.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proceedings of the 26th International Con-
ference on Neural Information Processing Systems -
Volume 2, NIPS’13, pages 3111–3119, USA. Curran
Associates Inc.

Pasquale Minervini, Thomas Demeester, Tim
Rocktäschel, and Sebastian Riedel. 2017. Ad-
versarial sets for regularising neural link predictors.
In Proceedings of the Thirty-Third Conference on
Uncertainty in Artificial Intelligence, UAI 2017,
Sydney, Australia, August 11-15, 2017. AUAI Press.

Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016, pages
2249–2255.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In In EMNLP.

B. W. Silverman. 1984. Spline smoothing: The
equivalent variable kernel method. Ann. Statist.,
12(3):898–916.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15:1929–1958.

A. N. Tikhonov. 1963. Solution of incorrectly formu-
lated problems and the regularization method. So-
viet Math. Dokl., 4:1035–1038.

33


