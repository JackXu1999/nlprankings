Proceedings of Knowledge Resources for the Socio-Economic Sciences and Humanities associated with RANLP-17, pages 19–25,

19

Varna, Bulgaria, 7 Sep 2017.

https://doi.org/10.26615/978-954-452-040-3_003

GRaSP: Grounded Representation and Source Perspective

Antske Fokkens♣, Piek Vossen♣, Marco Rospocher♦, Rinke Hoekstra♥♣ and Willem R. van Hage♠

♣ CLTL and Computer Science, Vrije Universiteit, Amsterdam, The Netherlands

♦ Fondazione Bruno Kessler, Trento, Italy
♥ Elsevier BV, Amsterdam, The Netherlands

♠ Netherlands eScience Center, Amsterdam, The Netherlands

{antske.fokkens,piek.vossen}@vu.nl, rospocher@fbk.eu
r.hoekstra@elsevier.com, w.vanhage@esciencecenter.nl

Abstract

When people or organizations provide in-
formation, they make choices regarding
what they include and how they repre-
sent it. These two aspects combined (the
content and the stance) represent a per-
spective.
Investigating perspectives can
provide useful insights into the reliabil-
ity of information, changes in viewpoints
over time, shared beliefs among social or
political groups and contrasts with other
groups, etc. This paper introduces GRaSP,
a generic framework for modeling per-
spectives and their sources.

1

Introduction

Structured data and knowledge resources typically
provide what is seen as factual information. They
contain deﬁnitions of concepts, ontologies, infor-
mation about origins, dates, locations, etc. Meth-
ods have been developed to automatically extract
such information from text (Hearst, 1992; Buite-
laar et al., 2004; Wu and Weld, 2010, among oth-
ers). However, knowledge consists of much more
than ontological classiﬁcations and basic veriﬁ-
able properties of objects and people. It involves
information about various entities, events and con-
cepts, connecting this information and judging its
validity. For social science and humanities, these
aspects of knowledge are particularly interesting,
i.e. how information is connected, how people
judge validity, how knowledge changes, what un-
certainty and sentiment that accompanies it.

When people or organizations provide informa-
tion, they make choices regarding what they in-
clude and how they present information. These
two aspects together (the content and stance pro-
vided by the source) represent a perspective, an
element of interest for many disciplines. Commu-

nication scientists and social psychologists study
(e.g.) how common opinions or existing stereo-
types are displayed in the media. Political scien-
tists can investigate how various sources present
hot topics. Historians may look into how perspec-
tives on historic events change over time. Outside
of academia, perspectives can be of interest to in-
formation professionals, decision makers, adver-
tisers, journalists and any citizen interested in crit-
ical thinking and ﬁnding balanced information.

Natural language processing (NLP) can offer
support in identifying the topic of text, classify-
ing stances, identifying sentiment and opinions,
determining factuality values of events, etc. To
our knowledge, these technologies are generally
investigated in isolation and have, up to date,
not been connected in order to obtain a more
full-ﬂedged representation of perspectives.
In
this paper, we take the ﬁrst step towards such
a representation by introducing a framework that
formally represents perspectives:
the Grounded
Representation and Source Perspective framework
(GRaSP). GRaSP is a unique and generic ﬂexible
framework that combines the formal representa-
tion of the content and of the source perspective
in one single model. It is compatible with exist-
ing models, but can also model subtleties that can
be expressed in natural language but remain chal-
lenging for RDF representations.

The rest of this paper is structured as follows.
We provide background on GRaSP in Section 2.
We then introduce the framework itself in Sec-
tion 3. We describe an automatically generated
dataset represented in GRaSP in Section 4. After
discussing related work, we conclude.

2 Background

The origins of GRaSP lie in the projects News-
Reader (Vossen et al., 2016) and BiographyNet

20

(Fokkens et al., 2014). NewsReader aimed at ex-
tracting what happened to whom, when and where
from large amounts of (ﬁnancial) news, creating
structured data to support decision making.
In
BiographyNet, we aimed to extract information
about individuals in biographical dictionaries for
historians. We investigated in connections be-
tween people and how the same person or event
was depicted in different biographical dictionar-
ies. An essential step for addressing these chal-
lenges is to indicate which documents talk about
the same entity or event. In addition, the prove-
nance of information is essential in both projects,
i.e. end-users need insight into the source of spe-
ciﬁc information. NewsReader and BiographyNet
also shared the vision of comparing differences in
information from various sources.

More recent projects dive deeper in perspec-
tives. Understanding Language By Machines in-
vestigates the relations between events, uncer-
tainty, sentiment and opinons and how this infor-
mation results into storylines and world views. In
Reading between the lines, we look at more subtle
cues of perspectives addressing questions such as
“which background information given when talk-
ing about people from different ethnic groups?”
or “when do we chose to generalize (e.g. by
calling someone a thief rather than a suspect of
having stolen something)?”. QuPiD2 addresses
(among others) what evidence is discussed and
how sources build their argumentation around it.
With GRaSP, we aim to design a framework
that can support the research questions central to
these projects following six requirements. First,
we want to represent various perspectives on the
same entity, proposition or topic next to each other.
Second, it should represent the source of each per-
spective, so that users can e.g. select all perspec-
tives of a speciﬁc source; group sources according
to shared or conﬂicting views on a given content;
ﬁnd all sources that have a perspective on the same
content or share a perspective; and, ﬁnd available
background information about the source. Third,
we want to provide the means to semantically
compare the (propositional) content across state-
ments and represent whether sources mention the
same, similar or related content (e.g. more or less
speciﬁc), or a different framing of content (e.g.
murdered, which is intentional, or killed which
may be accidental). Fourth, it should be possi-
ble to represent a wide range of perspective-related

phenomena, including: sentiment, emotion, judg-
ment, negation, certainty, speculation, reporting,
framing and salience. Fifth, we want to make al-
ternative interpretations of the same statement ex-
plicit, since statements might be (deliberately) am-
biguous, not well formulated or difﬁcult to process
with Natural Language Processing (NLP) technol-
ogy. Finally, users should be able to gain insight
in the full provenance of any information provided
by GRaSP. Next to the source, it should provide
information about how this perspective was ana-
lyzed (e.g. expert analysis of a text, crowd annota-
tions, text mining).

The ﬁrst three requirements allow users to place
various perspectives next to each other allowing
them to compare, among others, which sources
agree or disagree on what, which sources change
their mind, which sources speculate and whether
their predictions were accurate. In addition, they
would allow identifying all content and stances
given on a speciﬁc topic by a source and, for ex-
ample, display this on a timeline. Researchers can
thus investigate what information is important to
sources who hold a speciﬁc opinion. The fourth
and ﬁfth requirement ensure that the model is ﬂex-
ible enough to support various needs of end-users
as well as to accommodate the variation of infor-
mation provided by different systems or datasets.
Tools used to gather and interpret information
can introduce biases end-users should be aware
of (Lin, 2012; Rieder and R¨ohle, 2012). Provid-
ing clear provenance of information (including in-
volved processes) is a necessary component for
creating such awareness (sixth requirement).

There are several ontologies that can be used
to model perspective-related information. We will
outline the most inﬂuential ones and explain which
part of the requirements they fulﬁll in Section 5.

3 The GRaSP Framework

Perspectives are expressed by statements (which
can be spoken or written language, images, sig-
nals, etc.) from a speciﬁc source. A perspective
can be conveyed in many ways, some more ex-
plicit than others. Explicit opinions or highly sub-
jective terms are easily identiﬁed, but perspectives
can be expressed more subtly. The selection and
implicit framing of information plays a role (e.g.
does an article report on someone’s ethnicity, do
they report an expert’s political preference when
citing them on a societal matter) as well as choices

21

in how information is presented (e.g. using neutral
or marked words, certainty, conﬁrming or deny-
ing something). We therefore see a perspective
as the combination of the content of one or more
statements (which information is included) and the
stance sources take on this content.

GRaSP makes the link between the content and
stance of a statement as well as to their source
explicit. The framework achieves this through a
triple layered representation consisting of a men-
tion layer, an instance layer and an attribution
layer. The mention layer is the central layer of
the model. Mentions are physical objects, such as
a (piece of) text, (part of) an image or a sound,
that signal information and can be embedded in
a larger physical object. Mentions can be com-
bined and form a statement that displays a per-
spective on some propositional content by some
source. Propositions are abstract meaning repre-
sentations that make reference to events and par-
ticipating entities. Both events and entities are rep-
resented as instances in some (presumed) world in
the instance layer. Finally, the stance expressed
by the statement is represented in the attribution
layer. This layer models attitudinal information
such as beliefs, judgments, certainty and sentiment
of the source towards the propositional content.
This section introduces these layers and illustrate
how they are used to model perspectives.1

3.1 Grounding
An essential part of representing perspectives is
making explicit what the perspective is about, i.e.
representing the described (real-world) situation.
This is captured by the two top layers of our frame-
work; the instance layer and the mention layer.
These two layers, as well as their connecting rela-
tion are based on the architecture proposed in the
Grounded Annotation Framework (Fokkens et al.,
2013, GAF), which is incorporated in GRaSP.
Consider the following examples:

1. During 2000-2014, measles vaccination pre-

vent an estimated 17.1 million deaths

2. The search result contained 108 deaths over
resulting from four different

this period,
measles vaccines

3. There have been no measles death reported in

the U.S. since 2003

1The ontology and examples

https://github.com/cltl/GRaSP

can be

found at:

These sentences above make statements about
whether measles or vaccinations cause death. Fig-
ure 1 illustrates how this is represented in the top
two layers of GRaSP.

Figure 1: Instance and mention layers

The content of statements is represented in the
instance layer. This layer can represent informa-
tion about events, their participants, their locations
or their time, but also information about (generic)
concepts or ideas. Typically, propositions are ex-
pressed in terms of the Simple Event Model (SEM,
(van Hage et al., 2011)), but information in this
layer can be represented using other vocabularies
as well. SEM is a generic RDF vocabulary for
event-participant relations that allows for reason-
ing over the propositional content of statements.
Event-event relations can be represented as well
in the instance layer:
the example in Figure 1
includes a causal relation between measles and
death, and one between vaccination and death.

The second layer represents mentions. Men-
tions are (pieces of) resources that denote enti-
ties or propositions from the instance layer: they
can be expressions in text, spoken words, num-
bers or signals on a display, images, videos, etc.
The mention layer allows us to trace all resources
where a speciﬁc event, a person or idea is men-
tioned. It also records each speciﬁc way in which
an instance of interest is presented in a resource.
Following Semantic Web practice, GRaSP identi-
ﬁes mentions by IRIs (Internationalized Resource
Identiﬁers). This allows us to link them to addi-
tional information, including their surface string
(the literal text) and lemma and their exact posi-
tion within a text or image. This feature is partic-
ular relevant for scholars working with automati-
cally analyzed text, since it allows them to easily
identify where speciﬁc information is mentioned
in the original source and hence verify it.

measles

sem:causes

vaccination

sem:causes

death

grasp:denotedIn

grasp:denotedIn

grasp:denotedIn

During 2000-2014, measles 
vaccination prevent an 
estimated 17.1 million 
deaths

The search result contained 108 
deaths over this period, 
resulting from four diﬀerent 
measles vaccines 

There have been no measles death 
reported in the U.S. since 2003 

I

N
S
T
A
N
C
E
S

M
E
N
T
O
N
S

I

22

Entities, events and statements in the instance
layer can be linked to expressions in the men-
tion layer by the relations grasp:denotedBy
pointing out the exact words or linguistic struc-
ture that expresses the event or statement, or
by grasp:denotedIn which indicates that the
statement is made somewhere in a sentence, para-
graph or document. In our example the causal re-
lation between ‘vaccination’ and ‘death’ is linked
to Sentences 1.
and 2. The relation between
‘measles’ and ‘death’ is linked to Sentence 3.
Through these links, researchers interested in how
various sources talk about the risks of measles
or vaccinations can ﬁnd snippets of text that talk
about these issues. However, the source and the
stance taken are not made explicit yet. The next
subsection introduces the attribution layer, which
allows us to add this information.

3.2 Source Perspective

Grounding (modeled by the link between in-
stances and mentions) establishes what a speciﬁc
message is about. Two components need to be
added to complete a framework that can capture
perspectives from various sources. First, men-
tions should be linked to the source that expresses
the perspective. Second, we want to represent
the stance the source takes on the content of the
message. The stance typically includes informa-
tion on factuality (e.g. does the source conﬁrm or
deny, is it certain or hesitant, is it talking about the
future?), judgment, sentiment and emotion (e.g.
does the source consider the content ethical, is the
source scared by the content?).

The third layer, the attribution layer, adds these
components to GRaSP. Figure 2 adds the attribu-
tion layer to our example. Each of the mentions is
linked to an attribution node. These nodes are in
turn linked to the source that published them and,
if applicable, the source reported in the text. We
use the PROV-DM (Moreau et al., 2013) to model
the source of publication and a speciﬁed variant
of the wasAttributedTo relation introduced
by GRaSP for quoted sources. Attribution nodes
also receive values that make the stance taken by
the source explicit.
In this case, the statements
expressing opposing views to vaccines leading to
deaths or measles leading to death are connected
to a factuality value indicating that the source de-
nied this relation without expressing doubt.

Through the addition of this layer, end-users can

Figure 2: Instance, mention and attribution layers

explore opposing views on the same topic or state-
ment. This allows users to compare sources re-
ported on both viewpoints, identifying what other
opinions these sources express and investigating
the overall argumentation (the statement that there
were no measles death can both been used by peo-
ple opposing to vaccination, because “measles are
not deadly”, or by people supporting it, stating that
“measles deaths are avoided thanks to vaccina-
tion”). Additional information about sources can
be gathered leading to investigations on, among
others, how reports on speciﬁc topics evaluate
over time, the difference in certainty expressed by
politicians or scientists or how different countries
report on the same event.

The examples we have shown here are simpli-
ﬁed for the purpose of illustration. Sentence 1.
does not express the opposing view from vac-
cinations causes deaths by saying they do not,
but exactly states vaccinations actively prevent
death. This relation of prevention can also be
modeled in GRaSP. Under this representation, we
would model the statement that vaccination pre-
vents death and that this forms an opposing view to
them causing death. We can even model that both
these statements can be considered to be true by
the same source (e.g. believing that vaccinations
avoid many deaths and sometimes cause them).
The details of how to represent more complex rela-
tions and how viewpoints connect are beyond the
scope of this paper.

measles

sem:causes

vaccination

sem:causes

death

grasp:denotedIn

grasp:denotedIn

grasp:denotedIn

During 2000-2014, measles 
vaccination prevent an 
estimated 17.1 million 
deaths

The search result contained 108 
deaths over this period, 
resulting from four diﬀerent 
measles vaccines 

grasp:hasAttribution

grasp:wasAttributedTo

i

y
t
i
t
n
e
_
d
e
ﬁ
c
e
p
s
r
e
d
n
u

There have been no measles death 
reported in the U.S. since 2003 

grasp:hasAttribution

grasp:hasAttribution

grasp:wasAttributedTo

i

d
b
p
e
d
a
:
A
n
n
e
_
S
c
h
u
c
h
a
t

grasp:wasAttributedTo

grasp:wasAttributedTo

prov:wasAttributedTo

rdf:value

t
n

i
.
i

.

h
w
w
w
w

rdf:value

rdf:value

FACT:CT_NEG

FACT:CT_POS

t
h
e
 
s
e
a
r
c
h
 
r
e
s
u
l
t

I

N
S
T
A
N
C
E
S

M
E
N
T
O
N
S

I

I

A
T
T
R
B
U
T
O
N

I

B

r
i
a
n
_
S
h

i
l

h
a
v
y
_
H
I
_
n
e
w
s
e
d
i
t
o
r

23

4 GRaSP illustrated

One of the main challenges involved in represent-
ing perspectives in GRaSP is the question of how
to obtain this information accurately. In principle,
GRaSP can be used in combination with close-
reading manual methods, where researchers use it
to meticulously record the information they base
their conclusions on.
It becomes more interest-
ing when we can represent massive amounts of
data and help researchers ﬁnd information auto-
matically. Sentiment analysis, factuality classiﬁ-
cation, opinion mining, event extraction and ar-
gumentation mining are challenging tasks. Auto-
matically creating highly accurate representations
of perspectives in GRaSP is a challenge for the
future. Nevertheless, current methods can pro-
vide output that we believe to be useful for re-
searchers interested in perspectives.
In this sec-
tion, we illustrate what information can currently
be generated by NLP tools through a dataset that
the GRaSP framework for representation made
available through an interface providing an open
source visualization (van der Zwaan et al., 2016;
van Meersbergen et al., 2017).

The GRaSP dataset consists of WikiNews texts2
by the Open Source pipeline of NewsReader
(Vossen et al., 2016). The pipeline includes soft-
ware for identifying events, relations between
events, factuality of events and opinions. The in-
terpretation program turning the linguistic repre-
sentations of the NLP tools into RDF representa-
tion in GRaSP speciﬁcally targets Source Intro-
ducing Predicates (e.g. say, believe), identifying
who said what according to the text. All content
not in the scope of these predicates is attributed to
the author of the text.

The interactive visualization showing perspec-
tives on immigration and external EU borders in
WikiNews.3 is available on github and can be ex-
plored for better understanding of the following
passage.4 Figure 3 provides a partial screenshot.
On the left hand side, the sources are provided.
There are two lists of sources, the bottom list pro-
vides the authors or publishers of news articles.
The top list provides sources quoted in the article.
The events mentioned by the sources are displayed
in the central image, with actual text on the right.
Statistics on sentiment and factuality are provided

2https://en.wikinews.org/wiki/Main Page
3http://wikinews.org/
4http://nlesc.github.io/UncertaintyVisualization/

by the diagrams at the bottom of the visualization.
The visualization is interactive: sources and events
can be selected leading to updates of perspective
information and text.

5 Related Work

GRaSP offers ways to connect statements (in texts,
video, images, etc.)
to their source, the entities
and events they mention and the stance they dis-
play. Arguably, this connection can be seen as a
form of annotation. The Web Annotation Data
Model (OA)5 of the W3C represents annotations
as the related combination of a body (the annota-
tion) and a target (the annotated source). The rela-
tion is directed, the body says something about the
target, but not vice versa. Directionality of OA,
and the annotation view in general, is not com-
patible with the goals of GRaSP. A traditional an-
notation would just say that the link between an
instance and its mention is a form of semantic en-
richment of the text containing mention. The real
question is: does the semantic representation of an
instance determine how mentions should be un-
derstood, or do the combined mentions of an in-
stance collectively determine its semantics? This
nuance is of central importance when e.g. study-
ing concept drift across historical sources, and it is
the reason that GRaSP commits to the neutral de-
notation relation between instances and mentions.
Secondly, the OA speciﬁcation forces annotation
targets to be dereferencable, which is problematic
for sources that are not owned by the agent pro-
ducing the annotations. License and other con-
straints may prohibit republication, and on a tech-
nical level dereferenceability cannot be guaran-
teed for sources hosted at an external location.

Marl6 provides a model to represent subjective
opinions in text. Marl is used by the Onyx ontol-
ogy7 for representing emotions expressed in text.
It has also been combined with lexical information
on sentiment from Lemon (Buitelaar et al., 2013).8
GRaSP shares this ﬂexibility of being compati-
ble with various models that express aspects of
perspectives. Unlike GRaSP, Marl is restricted
to text. It furthermore confounds the layers that
GRaSP carefully separates: the opinion (attribu-
tion in GRaSP) is a central node, that refers to

5http://www.w3.org/TR/annotation-model/
6http://gsi.dit.upm.es/ontologies/marl/
7http://www.gsi.dit.upm.es/ontologies/onyx/
8http://lemon-model.net

24

Figure 3: Screenshot of visualized perspective information

an object/feature (instance in GRaSP) and the lit-
eral text that reﬂects the opinion (mentions). This
has two consequences. First, Marl only relates the
opinion to the source (text or url) in which it was
found without making the opinion holder explicit.
GRaSP links mentions to their provenance and at-
tributions of stances to the source that expressed
the opinion. Marl thus does not seem to provide
the means to collect all perspectives from a spe-
ciﬁc source. Second, GRaSP’s separation of these
layers makes it more ﬂexible in dealing with alter-
native interpretations of mentions, both at the at-
tribution and instance layer. Finally, GRaSP is not
limited to explicitly subjective opinions, but can
connect all stances taken by a source (including
factual statements).

GRaSP can be combined with various existing
models. We use PROV (Moreau et al., 2013) to
model the provenance of mentions and interpreta-
tions made on them (i.e. to model the NLP pro-
cess following Ockeloen et al. (2013)). The NLP
Interchange Format (NIF, Hellmann et al. 2013)
is an RDF/OWL vocabulary for representing NLP
annotations in a common way, to foster interop-
erability between NLP tools, language resources
and annotations. The core of NIF consists of a
vocabulary and a URI design that permit describ-
ing strings and substrings, to which arbitrary an-
notations can be attached using vocabularies exter-
nal to NIF. NIF itself does not speciﬁcally address
the representation of source or attribution infor-
mation, but can be combined with GRaSP. GRaSP
bases the format of IRIs of mentions on NIF and
uses it to represent some mention layer attributes
(e.g. char offset in the text). Finally, GRaSP uses

the grounding relations provided by GAF, as men-
tioned above. GRaSP’s main contribution com-
pared to GAF is that GRaSP adds an attribution
layer tying sources and their stances to mentions.

6 Conclusion and Discussion

This paper introduces GRaSP, a formal framework
to represent perspectives on content. The GRaSP
framework was designed out of need from various
NLP projects that deal with automatically iden-
tifying perspectives. We explained how GRaSP
provides the structure to study perspectives from
various view points (starting with a topic, source,
sentiment, or stance). We provide a dataset ac-
tively using GRaSP that allows users to study the
perspective various sources express on events in
WikiNews.

The way perspectives are expressed in natural
language is highly complex. Space limitations
prevented us to illustrate how phenomena such as
scope, alternative interpretations and framing can
be represented in GRaSP. The wide range of pos-
sibilities for applying this and how researchers can
deal with (lack of) accuracy of NLP tools also re-
quires more space than available in a short paper.
We plan to address these issues in future work.

Acknowledgement

The work presented in this paper was funded
by the European Union through the FP7 project
NewsReader and by the Netherlands Organization
for Scientiﬁc Research (NWO) via the Spinoza
grant, awarded to Piek Vossen and via VENI grant
275-89-029 awarded to Antske Fokkens.

25

Willem Robert van Hage, V´eronique Malais´e, Roxane
Segers, Laura Hollink, and Guus Schreiber. 2011.
Design and use of the Simple Event Model (SEM).
Journal of Web Semantics 9(2):128–136.

Maarten van Meersbergen, Piek Vossen,

Janneke
van der Zwaan, Antske Fokkens, Willem van Hage,
Inger Leemans, and Isa Maks. 2017. Storyteller: Vi-
sual analytics of perspectives on rich text interpreta-
tions. In Proceedings of Natural Language Process-
ing meets Journalism. Copenhagen, Denmark.

Piek Vossen, Rodrigo Agerri, Itziar Aldabe, Agata Cy-
bulska, Marieke van Erp, Antske Fokkens, Egoitz
Laparra, Anne-Lyse Minard, Alessio Palmero Apro-
sio, German Rigau, Marco Rospocher, and Roxane
Segers. 2016. Newsreader: Using knowledge re-
sources in a cross-lingual reading machine to gener-
ate more knowledge from massive streams of news.
Knowledge-Based Systems 110:60–85.

Fei Wu and Daniel S Weld. 2010. Open information
extraction using wikipedia.
In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics, pages 118–127.

References
Paul Buitelaar, Mihael Arcan, Carlos A Iglesias, J Fer-
nando S´anchez-Rada, and Carlo Strapparava. 2013.
Linguistic linked data for sentiment analysis. In 2nd
Workshop on Linked Data in Linguistics. page 1.

Paul Buitelaar, Daniel Olejnik, and Michael Sintek.
2004. A prot´eg´e plug-in for ontology extraction
from text based on linguistic analysis. In European
Semantic Web Symposium. Springer, pages 31–44.

Antske Fokkens, Serge Ter Braake, Niels Ockeloen,
Piek Vossen, Susan Legˆene, and Guus Schreiber.
2014. Biographynet: Methodological issues when
nlp supports historical research.
In LREC. pages
3728–3735.

Antske Fokkens, Marieke van Erp, Piek Vossen, Sara
Tonelli, Willem Robert van Hage, Luciano Seraﬁni,
Rachele Sprugnoli, and Jesper Hoeksema. 2013.
GAF: A grounded annotation framework for events.
In The 1st Workshop on Events. Atlanta, USA.

Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics-
Volume 2. Association for Computational Linguis-
tics, pages 539–545.

Sebastian Hellmann, Jens Lehmann, Sren Auer, and
Martin Brmmer. 2013.
Integrating NLP using
Linked Data. In Proc. of ISWC. pages 98–113. See
also http://persistence.uni-leipzig.
org/nlp2rdf/.

Yu-wei Lin. 2012. Transdisciplinarity and digital hu-
manities: Lessons learned from developing text-
mining tools for textual analysis. In Understanding
digital humanities, Springer, pages 295–314.

Luc Moreau, Paolo Missier, Khalid Belhajjame, Reza
B’Far, James Cheney, Sam Coppens, Stephen Cress-
well, Yolanda Gil, Paul Groth, Graham Klyne, et al.
2013. Prov-dm: The prov data model. Retrieved
July 30:2013.

Niels Ockeloen, Antske Fokkens, Serge Ter Braake,
Piek Vossen, Victor De Boer, Guus Schreiber, and
Susan Legˆene. 2013. Biographynet: Managing
provenance at multiple levels and from different per-
spectives.
In LiSc-Volume 1116. CEUR-WS. org,
pages 59–71.

Bernhard Rieder and Theo R¨ohle. 2012. Digital meth-
ods: Five challenges. In Understanding digital hu-
manities, Springer, pages 67–84.

Janneke van der Zwaan, Maarten van Meersbergen,
Antske Fokkens, Serge ter Braake, Inger Leemans,
Erika Kuijpers, Piek Vossen, and Isa Maks. 2016.
Storyteller: Visualizing perspectives in digital hu-
manities projects.
In Computational History and
Data-Driven Humanities: Second IFIP WG 12.7 In-
ternational Workshop, CHDDH 2016, Dublin, Ire-
land, May 25, 2016, Revised Selected Papers 2.
Springer, pages 78–90.

