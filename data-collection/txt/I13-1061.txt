










































Learning Efficient Information Extraction on Heterogeneous Texts


International Joint Conference on Natural Language Processing, pages 534–542,
Nagoya, Japan, 14-18 October 2013.

Learning Efficient Information Extraction on Heterogeneous Texts

Henning Wachsmuth
University of Paderborn, s-lab

Paderborn, Germany
hwachsmuth@s-lab.upb.de

Benno Stein
Bauhaus-Universität Weimar

Weimar, Germany
benno.stein@uni-weimar.de

Gregor Engels
University of Paderborn, s-lab

Paderborn, Germany
engels@upb.de

Abstract
From an efficiency viewpoint, information
extraction means to filter the relevant por-
tions of natural language texts as fast as
possible. Given an extraction task, differ-
ent pipelines of algorithms can be devised
that provide the same precision and recall
but that vary in their run-time due to dif-
ferent pipeline schedules. While recent re-
search investigated how to determine the
run-time optimal schedule for a collection
or a stream of texts, this paper goes one
step beyond: we analyze the run-times of
efficient schedules as a function of the het-
erogeneity of the texts and we show how
this heterogeneity is characterized from a
data perspective. For extraction tasks on
heterogeneous big data, we present a self-
supervised online adaptation approach that
learns to predict the optimal schedule de-
pending on the input text. Our evaluation
suggests that the approach will significant-
ly improve efficiency on collections and
streams of texts of high heterogeneity.

1 Introduction

Information extraction analyzes natural language
text in order to find relevant information about en-
tities and the events they participate in. An extrac-
tion task often requires to fill event templates with
considerable numbers of slots. Such a task implies
several analysis steps, e.g. certain types of entity
and relation extraction, and it is therefore typically
tackled with a pipeline Π = 〈A, π〉, where A is a
set of extraction algorithms and π a schedule that
prescribes the order of algorithm application. The
information sought for is anchored in text units of
a certain size, e.g. in a sentence or paragraph.

In times of big data, the run-time efficiency
of information extraction receives much attention
in research and industry (Chiticariu et al., 2010).

Among others, a growing need for business intelli-
gence can be regarded as the driving force behind.
This trend is equally observed by consulting com-
panies who see evolving markets for predictive an-
alytics (Harper, 2011), by global software players
who exploit big data for decision making (White,
2011), and by researchers who seek to annotate ta-
bles at web scale (Limaye et al., 2010).

Generally, a pipeline Π = 〈A, π〉 can be sped
up by parallelization (Agichtein, 2005), if given
enough resources, or by using faster but less ef-
fective algorithms (Al-Rfou’ and Skiena, 2012).
In addition, information extraction can always be
approached as a filtering task as discussed in detail
in (Wachsmuth et al., 2013b): By filling a template
slot, each algorithm in A implicitly classifies cer-
tain units of an input text as relevant. Only these
units need to be filtered for the next algorithm in π.
As a result, a smart schedule π will often signif-
icantly improve the overall extraction efficiency.
If the input requirements of all algorithms in A
are met within π, the effectiveness of Π (in terms
of both precision and recall) will be maintained,
since the output of Π exactly lies in the filtered text
units (Wachsmuth and Stein, 2012).1

When given a big data filtering task, the desig-
ner of a pipeline faces two challenges: (1) How to
determine the most efficient schedule for a set of
extraction algorithms and a collection or a stream
of texts? (2) How to maintain efficiency under het-
erogeneous text characteristics? With regard to the
former challenge we resort to existing research (cf.
Section 2). The latter becomes an issue where in-
put texts are not fully known or come from differ-
ent sources as in the web. Moreover, streams of
texts can undergo substantial changes in the distri-

1The simplest filtering task is to extract a relation between
two entity types, such as <ORG> was founded in <TIME>.
E.g., the sentence “Google was established by two Stanford
students.” needs not to be filtered for relation extraction, as
it contains no time entity. The schedule of the two implied
entity recognition steps will affect the extraction efficiency.

534



bution of relevant information. We argue that such
kinds of uncertainty and lack of a-priori knowl-
edge cannot be tackled offline, but they require to
learn and to adapt to the characteristics of input
texts to avoid a noticeable efficiency loss.

1.1 Contributions and Outline
In this paper, we analyze to what extent the hetero-
geneity of natural language texts in the distribution
of relevant information affects the efficiency of an
information extraction pipeline. For a high hetero-
geneity, we propose an adaptation of the pipeline’s
schedule, which we address with online learning.
Our learning algorithm maps basic linguistic char-
acteristics of a text to run-times of pipelines and
chooses the pipeline with the lowest predicted run-
time. The algorithm learns self-supervised and it
is language-independent. To measure the impact
of heterogeneity, we evaluate the algorithm on pre-
cisely constructed text corpora of different hetero-
geneity. Our contributions are three-fold:

1. We develop a self-supervised online adapta-
tion algorithm that learns the efficiency of in-
formation extraction pipelines (Section 3).

2. We quantify the heterogeneity of natural lan-
guage texts with regard to the distribution of
relevant information (Section 4).

3. We evaluate the need for online adaptation in
efficient information extraction as a function
of the heterogeneity of input texts (Section 5).

2 Related Work
One line of research on extraction efficiency refers
to declarative information extraction (Shen et al.,
2007). In particular, Krishnamurthy et al. (2009)
created SYSTEMT to address the needs of enter-
prise extraction applications. SYSTEMT involves
optimization strategies such as the ordering and in-
tegration of analysis steps (Reiss et al., 2008), but
it is restricted to rule-based extraction.

In (Wachsmuth et al., 2011a), we introduced a
method that optimizes the schedule of an arbitrary
set of extraction algorithms. This method captures
much optimization potential and it can be auto-
mated using techniques from artificial intelligence
(Wachsmuth et al., 2013a). Unlike the approach in
this paper, however, the method does not handle
variances in the characteristics of input texts.

Our approach applies to all extraction tasks with
dependencies between the relevant types of infor-
mation. We target at template filling, which con-

sists in relating a number of entities to events of
predefined types (Cunningham, 2006). Recent re-
search, e.g. (Jean-Louis et al., 2011), and major
evaluation tracks, e.g. (Kim et al., 2011), show the
ongoing importance of template filling.

We consider extraction pipelines that perform
filtering, which have a long tradition (Cowie and
Lehnert, 1996). Sarawagi (2008) sees the efficient
filtering of relevant portions of input texts as a
main challenge. In the pipelines we focus on, each
algorithm takes on one analysis (Grishman, 1997).
Other approaches such as joint information extrac-
tion (Choi et al., 2006) can be effective, but they
are not suitable when efficiency is important.

van Noord (2009) trades parsing efficiency for
parsing effectiveness by learning a heuristic filter-
ing of useful parses. In contrast, we develop a self-
supervised online learning algorithm to achieve ef-
ficient extraction without reducing effectiveness.
While our approach works with every predefined
relation and event type, arbitrary binary relations
are found in self-supervised open information ex-
traction (Fader et al., 2011). Self-supervised lear-
ning aims to fully overcome manual text labeling,
mostly for learning language like McClosky et al.
(2010). To our knowledge, we are the first to apply
it for predicting extraction efficiency.

3 Learning Efficient Extraction
The run-time efficiency of an information extrac-
tion pipeline depends on the distribution of rele-
vant information in its input texts (Wachsmuth and
Stein, 2012). For situations where this distribution
varies, we now present an approach that chooses
a pipeline schedule depending on the text at hand.
To maintain precision and recall, we consider only
schedules that fulfill the input requirements of all
algorithms employed in a pipeline.

3.1 Splitting the Pipeline
Most extraction tasks require some analyses (e.g.
tokenization) to be performed on the whole input
texts, as they are needed for most or all subsequent
analyses. We exploit this notion in that we use the
results of the first algorithms in a pipeline to pre-
dict the best schedule of the remaining algorithms.
To this end, we split a pipeline into a fixed first part
and a variable second part. We call the first part the
prefix pipeline, denoted as Πpre, and each second
part Π1, . . . ,Πk a main pipeline.

In general, k has an upper bound ofm! wherem
is the number of algorithms in a main pipeline.

535



 

 

6.1. Predict run-time
of main pipeline ∏1 

6.k. Predict run-time
of main pipeline ∏k 

... 7. Execute ∏* withlowest prediction 
8. Get run-time

of main pipeline ∏*
9. Update model of

main pipeline ∏*

4.k. Get run-time 
of main pipeline ∏k

3.k. Execute
main pipeline ∏k

 5.k. Update model
of main pipeline ∏k

... ... ...

3.1. Execute
main pipeline ∏1

4.1. Get run-time
of main pipeline ∏1

 5.1. Update model
of main pipeline ∏1  

2. Compute
feature values 

1. Execute prefix
pipeline ∏pre  

[training
 phase]

[update
 phase]

Figure 1: Illustration of the online adaptation algorithm with a prefix pipeline Πpre and k main pipelines
Π1, . . . ,Πk for one input text, which either goes through the training phase or through the update phase.

Due to the algorithms’ input constraints, how-
ever, k is normally much lower in practice. Also,
there might be ways to restrict the set of candidate
schedules to a reasonable selection, which is itself
a non-trivial problem that is beyond the scope of
this paper. In the following, we simply assume
that k ≤ m! main pipelines are given.

3.2 Self-Supervised Learning of Run-times

For a collection or a stream of texts D, our goal is
to determine the most efficient main pipeline for
each text in D. We approach this goal as an on-
line regression problem by learning to predict the
run-time per text unit t(Πi) of each main pipeline
Πi ∈ {Π1, . . . ,Πk}. Based on the results of Πpre,
we represent each text D ∈D as a feature vector
(x1, ..., xp) in order to create a regression model
for each t(Πi). Concretely, we map the feature
values x(D)1 , ..., x

(D)
p forD to a predicted run-time

t̃(Πi). Then, D is processed by the main pipeline
Π∗ with the lowest prediction.

In this manner, learning can be approached self-
supervised, as all training data is generated auto-
matically: the feature values and the observed run-
time t(Π∗) of Π∗ on D serve as a new training in-
stance, and the prediction error is given by the dif-
ference between t̃(Π∗) and t(Π∗). Still, an explicit
training set that is processed by all main pipelines
helps to create initial regression models.

3.3 The Online Adaptation Algorithm

Let a prefix pipeline Πpre, a set of main pipelines
Π1, . . . ,Πk, and a collection or a stream of texts D
be given. Then D is split into two parts DT and
DU to serve the following two phases of the online
adaptation algorithm:

1. Training. On each textD∈DT, execute Πpre
and each Πi ∈ {Π1, . . . ,Πk}. Update the re-
gression model of each Πi wrt. the results of
Πpre and the run-time t(Πi) of Πi on D.

2. Update. On each textD ∈ DU, execute Πpre
and predict t̃(Πi) for all Πi ∈ {Π1, . . . ,Πk}.
Execute the Π∗with the lowest prediction and
update its regression model wrt. the results of
Πpre and the run-time t(Π∗) of Π∗ on D.

Figure 1 illustrates the online adaptation algorithm
on one single text. An intuitive extension is to it-
eratively schedule each extraction algorithm sepa-
rately. This would allow us to use detailed knowl-
edge in later predictions. Since the first predic-
tions are most decisive, however, we do not con-
sider the iterative scenario here for simplicity.

3.4 Baselines and the Gold Standard
One way to evaluate our approach is to compare it
to each pipeline (Πpre,Πi), Πi∈{Π1, . . . ,Πk}. In
practice, relying on such a fixed pipeline involves
the danger of choosing a slow one. Hence, we also
consider two baseline approaches below:

1. Random baseline. For each text D ∈ DU,
choose a main pipeline (pseudo-) randomly.

2. Optimal baseline. For each text D ∈ DU,
choose the main pipeline that has achieved
the best overall run-time on DT.

The optimal baseline serves as a strong competitor
on homogeneous collections and streams of texts,
where it will often find the run-time optimal fixed
pipeline. In contrast, the random baseline appears
rather weak, but it will never fail completely.

Besides, we compute the gold standard below,
i.e., an oracle that knows the fastest main pipeline
for each text beforehand. The gold standard de-
fines the upper ceiling for a set of main pipelines,
thereby quantifying the general optimization po-
tential. In that, it helps to evaluate whether online
adaptation is suitable for the input texts at hand.

4 The Heterogeneity of Texts
The introduced algorithm is domain-independent
and language-independent. It targets at situations

536



where input texts are heterogeneous in content or
style, as is typical for the results of an exploratory
web search. From an extraction perspective, the
heterogeneity of a collection or a stream of texts D
can be regarded as the extent to which the texts
in D vary in the distribution of instances of the rel-
evant information types C1, . . . , Cm, i.e., the en-
tity, relation, and event types to be extracted.

As motivated in Section 1, text units need to
be analyzed only if they may contain instances of
all relevant information types. Hence, a pipeline’s
efficiency depends on the density ρi(D) of each
type Ci in the input texts in D. Here, the density
corresponds to the fraction of text units in D that
contain an instance of Ci. The density ρi(D) of
Ci in a single text D ∈D can be defined accord-
ingly. Now, differences in the run-time per text
unit of a pipeline mainly result from varying den-
sities ρi(D). In this regard, the heterogeneity of D
can be quantified by measuring the variance of all
densities in the texts in D. The outlined consider-
ations give rise to the following measure:
Averaged Deviation Let C = {C1, . . . , Cm} be
the set of relevant information types for an extrac-
tion task, and let σi(D) be the standard deviation
of the density ρi(D) of Ci ∈ C in a collection or
a stream of texts D. Then, the averaged deviation
of C in D is

σ(C|D) = 1
m
·

m∑
i=1

σi(D).

We compute exact values σ(C|D) in Section 5
to measure the impact of heterogeneity. In general,
the averaged deviation can also be estimated on a
sample of texts. For illustration, Table 1 lists the
deviations for the three most common named en-
tity types in the German part of the CoNLL’03 cor-
pus (Tjong Kim Sang and De Meulder, 2003), in
the Revenue corpus (Wachsmuth et al., 2010), in a
sample of the German Wikipedia (the first 10,000
articles according to internal page ID), and in the
LFA-11 smartphone corpus, which is a web crawl
of blog posts (Wachsmuth and Bujna, 2011). Here,
we recognized entities using Stanford NER (Finkel
et al., 2005; Faruqui and Padó, 2010).

Different from other sampling-based efficiency
estimations, cf. (Wang et al., 2011), the averaged
deviation does not measure the typical characteris-
tics of input texts, but it quantifies how much these
characteristics vary. By that, it helps pipeline de-
signers to decide whether an online adaptation of
pipeline schedules is needed to ensure efficient ex-

Information type Ci σi(Dco) σi(Drv) σi(Dwk) σi(Dbp)
Person entities 18.4% 11.1% 15.9% 16.6%
Organization entities 18.1% 16.0% 14.1% 23.4%
Location entities 16.6% 10.9% 16.0% 15.3%

Averaged deviation 17.7% 12.7% 15.3% 18.4%

Table 1: The standard deviation σi of the density of
three entity types in the CoNLL’03 corpus Dco,
the Revenue corpus Drv, a sample of 10,000 Wiki-
pedia articles Dwk, and a crawl of blog posts Dbp.
The bottom line shows their averaged deviations.

traction. However, its current form leaves unclear
how to compare deviations across tasks. In future
work, a solution will be to normalize the averaged
deviation—either with respect to a reference cor-
pus or with respect to the given task, e.g. to a situ-
ation where all schedules perform equally well.

4.1 Text Corpora of Different Heterogeneity

For a careful evaluation of online adapation, we
need input texts that refer to different levels of het-
erogeneity while being appropriate for analyzing a
single and sufficiently complex filtering task at the
same time. Most corpora for extraction tasks are
too small to create reasonable subsets of different
heterogeneity like Dco and Drv. As an alternative,
a web crawl such as Dbp typically yields high het-
erogeneity, but it tends to include a large fraction
of task-irrelevant texts. This conceals which effi-
ciency differences are due to scheduling and, thus,
is not suitable for controlled experiments.

To address this difficulty, we also use precisely
constructed text corpora below, which consist of
both original texts from existing corpora and artifi-
cially modified versions of these texts. Concretely,
we modified a text by randomly duplicating one of
its sentences, ensuring that each text in a corpus
comprises a unique set of sentences while being
grammatically valid. Thereby, we limit the online
adaptation algorithm to a certain degree in learn-
ing linguistic features from the texts, but we gain
that we can measure the benefit of online adapta-
tion as a function of the averaged deviation.

5 Evaluation

We now present controlled experiments with the
online adaptation algorithm on text corpora of dif-
ferent heterogeneity. The goal is to show the cir-
cumstances under which online adaptation will be
needed for efficiency and, conversely, when a run-
time optimal fixed pipeline appears sufficient.

537



5.1 Experimental Set-up
We consider the filtering task to extract financial
forecast statements with resolvable time informa-
tion and a monetary value for an organization. An
example for such a statement is “Apple’s annual
revenues could hit $400 billion by 2015”. Accord-
ingly, we have a set C of five relevant information
types: time entities, money entities, organization
entities, financial statements, and forecast events.

Algorithms Table 2 outlines the eight algorithms
that we used in all experiments. We employed the
UIMA tokenizer2 to generate tokens and sentences,
and the TreeTagger for part-of-speech tagging and
chunking (Schmid, 1995). As in (Wachsmuth et
al., 2011a), we relied on regexes for money and
time recognition, while we applied support vector
machines SD and FD for event detection. Organi-
zation names were extracted with the CRF-based
Stanford NER (cf. Section 4). Finally, we imple-
mented a rule-based time normalizer TN.

While, in general, our approach works for each
kind of extraction algorithm, all algorithms in Ta-
ble 2 perform only in-sentence extraction.

Pipelines As stated in Section 3, we split all pipe-
lines into two parts. The prefix pipeline consists
of the two algorithms used for preprocessing only:
Πpre = (UT, TT). For the reasons given below, we
evaluated the following k = 3 main pipelines:

Π1 = (TR, FD, MR, SD, TN, OR)
Π2 = (TR, MR, FD, TN, OR, SD)
Π3 = (MR, TR, FD, OR, SD, TN)

Only 108 of the 6! = 720 possible main pipelines
fulfill all dependencies listed in Table 2. Based on
the method from (Wachsmuth et al., 2011a), we
found that main pipelines starting with TR, MR,
and FD (which are significantly faster than OR, SD,
and TN) dominate others. We selected Π1, Π2, and
Π3, as they target at very different distributions of
relevant information. While k = 3 appears small,
it allows for a concise evaluation and suffices to
discuss the impact of online adaptation. Still, we
evaluate all 108 main pipelines in Section 5.4.

Features For generality, we restricted our view
to simple features that neither require a preced-
ing run over the training set nor exploit knowledge
about the employed algorithms: (1) Lexical statis-
tics, namely, the average and maximum number of
characters in a token and of tokens in a sentence as

2UIMA tokenizer, http://uima.apache.org/sandbox.html.

Algorithm A t(A) depends on
UT UIMA tokenizer 0.06 ms –
TT TreeTagger 0.59 ms UT

TR Time recognition 0.36 ms UT
MR Money recognition 0.64 ms UT
OR Organization recognition 2.52 ms UT, TT
SD Financial statement detection 3.95 ms UT, TR, MR
FD Forecast event detection 0.29 ms UT, TT, TR
TN Time normalization 0.95 ms UT, TR

Table 2: Each evaluated algorithm A with its esti-
mated average run-time per sentence t(A) and the
algorithm A depends on.

well as the length of the text, (2) the average run-
times per sentence of each algorithm in Πpre, and
(3) the frequencies of all part-of-speech tags.

In the feature evaluation in Section 5.4, we also
have two further types that capture general charac-
teristics of entities: (4) The frequency of each uni-
gram and bigram of all chunk tags and (5) the fre-
quencies of regex matches of a regex for arbitrary
numbers and of a regex for upper-case words.3

Learning Algorithm For run-time prediction, we
applied Stochastic Gradient Descent (SGD) from
Weka 3.7.5 (Hall et al., 2009). After some prelim-
inary tests, we set the learning rate of SGD to 0.01
for all experiments. Accordingly, we always used
10−5 for regularization and we always let SGD it-
erate 10 epochs over the training texts.

Datasets We constructed four partly artificial cor-
pora D0, . . . ,D3 as motivated in Section 4.1. In
case of D0, we randomly mixed 1500 texts of the
German CoNLL’03 corpus and the Revenue cor-
pus (cf. Section 4).4 For D1, we took the 300 texts
from D0 with the highest differences in the den-
sity of relevant information. We created modified
versions of these texts in order to obtain a corpus
size of 1500. D2 and D3 were built analogously
for the 200 and 100 highest-difference texts, re-
spectively. Table 3 lists all averaged deviations for
the text unit “sentence”. Where not stated other-
wise, we used the first 500 texts of each corpus for
training and the remaining 1000 for testing.

Efficiency The efficiency of all pipelines on each
text was measured as the run-time in milliseconds
per sentence, averaged over 10 runs. All run-times
and their standard deviations were saved. For de-
terminism, we loaded these run-times during eval-

3We experimented with further regexes, but their impact
was low. Therefore, we do not report on them in this paper.

4Notice that the evaluated set of features does not target
at characteristics that are specific to the German language.

538



Information type Ci σi(D0) σi(D1) σi(D2) σi(D3)
Time entities 19.1% 22.5% 24.6% 25.9%
Money entities 19.8% 19.1% 20.4% 22.3%
Organization entities 19.3% 21.6% 22.4% 25.0%
Financial statements 7.1% 7.8% 8.9% 10.6%
Forecast events 3.8% 5.9% 6.7% 8.5%

Averaged deviation 13.8% 15.4% 16.6% 18.5%

Table 3: The standard deviation σi(D) of the den-
sity of each relevant information types Ci∈C for
each corpus D∈{D0, . . . ,D3} as well as the av-
eraged deviation σ(C|D) of C in each D.

uation instead of executing pipelines.5 In case of
the online adaptation algorithm, we also computed
the mean run-time prediction error and the clas-
sification error, i.e., the fraction of texts the best
main pipeline was not found for.

Effectiveness We omit to evaluate effectiveness
here for lack of relevance: Our experiment setting,
which is similar to (Wachsmuth et al., 2011a),
yields no trade-off between efficiency and effec-
tiveness, since we only consider schedules that ful-
fill all dependencies in Table 2. Thus, all pipelines
always achieve the same precision and recall.

System and Software All experiments were con-
ducted on a 2 GHz Intel Core 2 Duo MacBook with
4 GB memory. The Java source code and the pre-
computed run-time files used in the evaluation can
be accessed at http://www.arguana.com.

5.2 The Impact of Heterogeneity
We ran the online adaptation algorithm and the
baselines from Section 3 on the test set of each of
the corpora D0, . . . ,D3. Both the algorithm and
the optimal baseline were trained on the respective
training sets. Figure 2 illustrates the run-times of
the main pipelines for each approach as a function
of the averaged deviation and compares them to
the gold standard. The shown confidence intervals
result from the run-times’ standard deviations σ,
which ranged between 0.029ms and 0.043ms.

On the least heterogeneous corpus D0 with an
averaged deviation of 13.8%, the online adaptation
algorithm achieved an average run-time of 0.98ms
per sentence, which is faster than the random base-
line but slower than the optimal baseline at a low
confidence level. For σ(C|D1) = 15.4%, the on-
line adaptation algorithm succeeded with 0.87ms
per sentence as opposed to 0.9ms of the optimal
baseline. This gap gets significantly larger un-

5Section 5.4 shows the effects of errors of measurement.

0.55

0.7

0.85

1

13.8% 15.4% 16.6% 18.5%ru
n-

tim
e 

in
 m

s 
pe

r s
en

te
nc

e

averaged deviation (heterogeneity)

optimal
baseline

gold standard

1.06

1.000.95

1.02
0.98

0.73

0.85

0.62

online 
adaptation

random
baseline

Figure 2: The average run-times of the main pipe-
lines of both baseline approaches, the online adap-
tation algorithm, and the gold standard as a func-
tion of the averaged deviation. The background
areas denote the 95% confidence intervals (±2σ).

der higher averaged deviation. At 18.5%, both
baselines are clearly outperformed, taking 37%
and 40% more time on average, respectively.6

One reason for the weak result of online adap-
tation on D0 lies in the low optimization potential
for that corpus: the main pipeline of the optimal
baseline took only 12% more time on average than
the gold standard (0.95ms vs. 0.85ms), which im-
plies very small differences in the main pipelines’
run-times. This does not only render online adap-
tation hard but also unnecessary. Conversely, Fig-
ure 3 shows an optimization potential of over 50%
for D3. A reasonable hypothesis is therefore that
online adaptation succeeds only on collections and
streams of texts of high heterogeneity as indicated
by large differences in the pipelines’ run-times.

5.3 Run-time and Error Analysis
Figure 3 details the run-times of the three fixed
pipelines, the online adaptation algorithm, and the
gold standard on D0 and D3. The small black in-
dicators denote the standard deviations.

In total, the fixed pipelines are 16% to 25% slo-
wer than the online adaptation algorithm on D3.
The algorithm’s run-time mainly breaks down into
0.51ms of the prefix pipeline and 0.73ms of the
main pipelines, while the time for feature com-
putations (0.03ms) and regression (0.01ms) is al-
most negligible. A similar situation is observed for
D0. Here, online adaptation is worse only than
(Πpre,Π1), which did best on 598 of the 1000 test
texts. (Πpre,Π2) and (Πpre,Π3) had the lowest
run-time on 229 and 216 texts, respectively.7

6On the training set of D3, the optimal baseline did not
find the fastest main pipeline. This might be coincidence, but
is, of course, more likely under higher heterogeneity.

7The numbers of texts sum up to more than 1000 because
on some texts different pipelines performed equally well.

539



run-time in ms 
per sentence0 0.4 0.8 1.2 1.600.40.81.21.6

(∏pre ,∏2 )0.65
1.02 1.520.51

1.061.71
(∏pre ,∏3 ) 1.08 1.590.65 0.511.091.73

(∏pre ,∏1 )0.650.951.6
0.96 1.470.51

gold
standard 0.62 1.130.65 0.510.851.49

online
adaptation

0.73 1.27
prefix pipeline main pipeline features regression

0.65 0.510.981.67
prefix pipelinemain pipelinefeaturesregression

D0 D3

Figure 3: Average run-times of the fixed pipelines and online adaptation on the test sets of D0 and D3.

The online adaptation algorithm did not succeed
on D0, since its mean run-time prediction error
of 0.45ms was almost half as high as the average
run-times to be predicted, which is too inaccurate
under the small differences in the pipelines’ run-
times. As a result, for only 39% of the input texts,
the best pipeline was chosen (i.e., a classification
error of 0.61). However, the impact on D3 does
not emanate from a low mean prediction error (that
was in fact 0.24ms higher), but the classification
error was reduced to 0.45. Consequently, the main
reason lies in larger differences in the pipelines’
run-times, which supports our hypothesis.

An insightful linguistic phenomenon is that the
prefix pipeline Πpre took significantly more time
per sentence on D0 than on D3. Since the run-
times of both algorithms in Πpre scale linearly
with the number of input tokens, the average sen-
tence length of D0 must exceed that of D3. The
reason is that shorter sentences tend to contain less
relevant information. Hence, many sentences can
be discarded after a few analysis steps, which in-
creases the need for input-dependent scheduling.

5.4 Parameter Analysis

To give further evidence for the hypothesis from
Section 5.2 and to test the applicability of online
adaptation, we evaluated some major parameters:

Impact of Features For each of the five feature
types in isolation, we trained a regression model
on D0 and analyzed its impact. The lexical statis-
tics achieved the lowest classification error (0.41),
followed by the run-times (0.53). In terms of run-
time prediction, the regex matches (0.46ms) and
the part-of-speech tags (0.48ms) performed best,
whereas the chunk tags failed both in classification
(0.57) and prediction (0.58ms).8 We used feature
types 1–3 in all other experiments, since comput-

8The low correlation of the prediction and classification
error seems counterintuitive, but it indicates the limitations of
these measures: E.g, a small prediction error can still be prob-
lematic if run-times differ only slightly, while a high classifi-
cation error may have few negative effects in this case. If both
errors are small, however, this normally implies success.

0.25

0.35

0.45

0.55

0.65

1000 5000 10,000 15,000

pr
ed

ic
tio

n 
er

ro
r i

n 
m

s 

input text

0.56

0.48

0.37
0.40

mean (0.43)

Figure 4: The mean run-time prediction error for
the pipelines chosen by the online adaptation algo-
rithm on 15,000 modified versions of the texts in
D0 for training size 1. The values of the two inter-
polated learning curves denote the mean of 1000
and 100 consecutive predictions, respectively.

ing them took only 0.05ms per sentence on aver-
age. In contrast, the regex matches needed 0.16ms
alone, which exceeds the difference between the
optimal baseline and the gold standard on D0 (cf.
Section 5.2) and, thus, renders the regex matches
useless in the given setting.

The regex matches emphasize the obvious need
for a scheduling mechanism that avoids spending
more time than can be saved later on. At the same
time, such a mechanism should capture character-
istics of a text that reliably model its complexity,
which the evaluated features did not fully achieve.

Impact of Training Size We evaluated the on-
line adaptation algorithm on D0 for nine training
sizes between 1 and 5000. As the training set of
D0 is limited, we created modified versions of its
texts where needed (cf. Section 4.1). Online adap-
tation always did better than the random baseline
but not than the optimal baseline except for train-
ing size 1. In case of 1000 or more training texts,
the algorithm mimicked the optimal baseline, i.e.,
it chose Π1 for about 90% of the texts.

Online Learning For training size 1, we ran the
online adaptation algorithm on 15,000 modified
versions of the texts in D0. Figure 4 shows two
levels of detail of the algorithm’s learning curve in
its update phase. As the bold curve conveys, the
mean run-time prediction error decreases on the

540



0 0.4 0.8 1.2 1.6
run-time in ms per sentence

0.65 0.98a) loaded

0.51 0.73a) loaded

0.68 1.02b) measured
prefix pipeline main pipeline features regression

0.53 0.77b) measured

D0

D3

Figure 5: The average run-times and standard de-
viations of the online adaptation algorithm on D0
and D3 when run-times are a) loaded from a pre-
computed file or b) measured during execution.

first 5000 texts to an area around 0.4ms where it
stays most of the time afterwards, though the light
curve discovers many outliers. Still, online learn-
ing apparently seems to work well.
Overall Optimization Potential To measure the
overall optimization potential of scheduling, we
made an experiment with all k = 108 correct
main pipelines on D0 (cf. Section 5.1).9 The re-
sulting average run-times of the fixed main pipe-
lines span from 0.79ms per sentence in case of
Πbest = (TR, FD, MR, OR, TN, SD) to 3.27ms of
Πworst = (OR, TR, TN, MR, SD, FD). This shows
that the efficiency loss of choosing a wrong sched-
ule can be very high. The online adaptation algo-
rithm achieved 0.86ms with a mean run-time pre-
diction error of 0.37ms. Altogether, 21 of the 108
schedules were used on the 1000 test texts, and the
best schedule was chosen for 30% of the texts.
Real Execution As mentioned, we loaded all run-
times from a precomputed file, which is not pos-
sible in case of real execution. In Figure 5, we
compare the results of loading run-times to the
run-times measured during the execution of the
online adaptation algorithm. The measured val-
ues mainly differ in terms of larger standard devi-
ations, i.e., 0.16ms on D0 and 0.12ms on D3. This
seems to have a fairly negative effect on the main
pipelines’ run-times. However, the measured pre-
fix pipeline run-times also exceed the saved ones,
which suggests that the effect is due to a higher
system load only. In any case, the measured run-
time on D3 indicates that the online adaptation al-
gorithm applies for practical applications.

Conclusion
In this paper, we analyze the efficiency of informa-
tion extraction pipelines as a function of the het-

9Notice that the training time increases linear to k, so a
high k implies a high training overhead. For space reasons,
we omit to report on training time here at all. However, train-
ing time will always be amortized in large-scale scenarios.

erogeneity of their input texts. In particular, we
quantify heterogeneity with regard to the distribu-
tion of relevant information and we provide a self-
supervised online adaptation algorithm that learns
which pipeline schedule to choose for what input
text in order to optimize efficiency while maintain-
ing precision and recall. On this basis, we investi-
gate the need for pipelines that adapt to their input
within time-critical extraction tasks.

Our experiments suggest that the benefit of on-
line adaptation is significant on heterogeneous col-
lections and streams of texts: The online adapta-
tion algorithm achieves gains of about 30% over
the most efficient fixed schedule, which we see as
important in times of big data. Conversely, when
relevant information is uniformly distributed, find-
ing an efficient fixed schedule appears sufficient,
as approached in (Wachsmuth et al., 2013a).

A setting still to be evaluated refers to streams
of input texts whose characteristics change slowly
over time. Also, other extraction tasks may yield
more insights. In order to decide how to approach
a task at hand, a better understanding of the pro-
cessing complexity of collections and streams of
texts is required, to which our research contributes
substantial building blocks.

In general, our self-supervised learning concept
can be transferred to each natural language proces-
sing task that meets two basic conditions: (1) The
task can be approached in different manners where
each approach performs best for certain situations
or inputs. (2) The performance of each approach
can be measured or it is clear by definition.

Acknowledgments

This work was partly funded by the German Fed-
eral Ministry of Education and Research (BMBF)
under contract number 01IS11016A.

References
Eugene Agichtein. 2005. Scaling Information Extrac-

tion to Large Document Collections. Bulletin of the
IEEE Computer Society TCDE, 28:3–10.

Rami Al-Rfou’ and Steven Skiena 2012. SpeedRead:
A Fast Named Entity Recognition Pipeline. In Proc.
of the 24th COLING, pages 51–66.

Laura Chiticariu, Yunyao Li, Sriram Raghavan, and
Frederick R. Reiss. 2010. Enterprise Information
Extraction: Recent Developments and Open Chal-
lenges. In Proc. of the 16th COMAD, pages 1257–
1258.

541



Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
Extraction of Entities and Relations for Opinion
Recognition. In Proc. of the 2006 EMNLP, pages
431–439.

Hamish Cunningham. 2006. Information Extraction,
Automatic. Encyclopedia of Language & Linguis-
tics, 4:665–677.

Jim Cowie and Wendy Lehnert. 1996. Information Ex-
traction. Communications of the ACM, 39(1):80–91.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying Relations for Open Information
Extraction. In Proc. of the 2011 EMNLP, pages
1535–1545.

Jenny R. Finkel, Trond Grenager, and Christopher D.
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proc. of the 43nd Annual Meeting of
the ACL, pages 363–370.

Manaal Faruqui and Sebastian Padó. 2010. Training
and Evaluating a German Named Entity Recognizer
with Semantic Generalization. In Proc. of KON-
VENS 2010, pages 129–133.

Ralph Grishman. 1997. Information Extraction: Tech-
niques and Challenges, In International Summer
School on Information Extraction: A Multidisci-
plinary Approach to an Emerging Information Tech-
nology, pages 10–27.

Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11(1):10–18.

Fern Harper. 2011. Predictive Analytics: The Hurwitz
Victory Index Report, Hurwitz & Associates.

Ludovic Jean-Louis, Romaric Besançon, and Olivier
Ferret. Text Segmentation and Graph-based Method
for Template Filling in Information Extraction. In
Proc. of the 5th IJCNLP, pages 723–731, 2011.

Jin-D. Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun’ichi Tsujii. 2011. Overview of
BioNLP Shared Task 2011. In Proc. of the BioNLP
2011 Workshop Companion Volume for Shared Task,
pages 1–6.

Rajasekar Krishnamurthy, Yunyao Li, Sriram Ragha-
van, Frederick R. Reiss, Shivakumar Vaithyanathan,
and Huaiyu Zhu. 2009. SystemT: A System for
Declarative Information Extraction. In SIGMOD
Records, 37(4):7–13.

Girija Limaye, Sunita Sarawagi, Soumen Chakrabarti.
2010. Annotating and Searching Web Tables using
Entities, Types and Relationships. In Proc. of the
VLDB Endowment, 3(1):1338–1347.

Christopher D. Manning and Hinrich Schütze. 1999.
Foundations of Statistical Natural Language Pro-
cessing, MIT Press, Cambridge, MA, USA.

David McClosky, Eugene Charniak, and Mark John-
son. 2010. Automatic Domain Adaptation for Pars-
ing. In Proc. of the HLT/NAACL, pages 28–36.

Gertjan van Noord. 2009. Learning Efficient Parsing.
In Proc. of the 12th EACL, pages 817–825.

Frederick R. Reiss, Sriram Raghavan, Rajasekar
Krishnamurthy, Huaiyu Zhu, and Shivakumar
Vaithyanathan. 2008. An Algebraic Approach to
Rule-Based Information Extraction. In Proc. of the
2008 IEEE 24th ICDE, pages 933–942.

Sunita Sarawagi. 2008. Information Extraction. Foun-
dations and Trends in Databases, 1(3):261–377.

Helmut Schmid. 1995. Improvements in Part-of-
Speech Tagging with an Application to German. In
Proc. of the ACL SIGDAT-Workshop, pages 47–50.

Warren Shen, AnHai Doan, Jeffrey F. Naughton, and
Raghu Ramakrishnan. 2007. Declarative Informa-
tion Extraction using Datalog with Embedded Ex-
traction Predicates. In Proc. of the 33rd VLDB,
pages 1033–1044.

Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 Shared
Task: Language-independent Named Entity Recog-
nition In Proc. of the 7th CoNLL, pages 142–147.

Henning Wachsmuth, Peter Prettenhofer, and Benno
Stein. 2010. Efficient Statement Identification for
Automatic Market Forecasting. In Proc. of the 23rd
COLING, pages 1128–1136.

Henning Wachsmuth, Benno Stein, and Gregor Engels.
2011. Constructing Efficient Information Extraction
Pipelines. In Proc. of the 20th ACM CIKM, pages
2237–2240.

Henning Wachsmuth and Kathrin Bujna. 2011. Back
to the Roots of Genres: Text Classification by Lan-
guage Function. In Proc. of the 5th IJCNLP, pages
632–640.

Henning Wachsmuth and Benno Stein. 2012. Optimal
Scheduling of Information Extraction Algorithms.
In Proc. of the 24th COLING: Posters, pages 1281–
1290.

Henning Wachsmuth, Mirko Rose, and Gregor Engels.
2013. Automatic Pipeline Construction for Real-
Time Annotation. In Proc. of the 14th CICLing,
pages 38–49.

Henning Wachsmuth, Benno Stein, and Gregor Engels.
2013. Information Extraction as a Filtering Task. To
appear in Proc. of the 22th ACM CIKM.

Daisy Z. Wang, Long Wei, Yunyao Li, Frederick R.
Reiss, and Shivakumar Vaithyanathan. 2011. Selec-
tivity Estimation for Extraction Operators over Text
Data. In Proc. of the 2011 IEEE 27th ICDE, pages
685–696.

Colin White. 2011. Using Big Data for Smarter Deci-
sion Making, BI Research.

542


