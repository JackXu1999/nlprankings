



















































Reinforced Product Metadata Selection for Helpfulness Assessment of Customer Reviews


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 1675–1683,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

1675

Reinforced Product Metadata Selection for Helpfulness Assessment of
Customer Reviews

Miao Fan, Chao Feng, Mingming Sun, Ping Li
Cognitive Computing Lab

Baidu Research
No.10 Xibeiwang East Road, Beijing, 10085, China

10900 NE 8th St, Bellevue, WA 98004, USA
{fanmiao, v fegchao, sunmingming01, liping11}@baidu.com

Abstract

To automatically assess the helpfulness of
a customer review online, conventional ap-
proaches generally acquire various linguistic
and neural embedding features solely from the
textual content of the review itself as the ev-
idence. We, however, find out that a help-
ful review is largely concerned with the meta-
data (such as the name, the brand, the cate-
gory, etc.) of its target product. It leaves us
with a challenge of how to choose the correct
key-value product metadata to help appraise
the helpfulness of free-text reviews more pre-
cisely. To address this problem, we propose
a novel framework composed of two mutual-
benefit modules. Given a product, a selector
(agent) learns from both the keys in the prod-
uct metadata and one of its reviews to take
an action that selects the correct value, and a
successive predictor (network) makes the free-
text review attend to this value to obtain better
neural representations for helpfulness assess-
ment. The predictor is directly optimized by
SGD with the loss of helpfulness prediction,
and the selector could be updated via policy
gradient rewarded with the performance of the
predictor. We use two real-world datasets from
Amazon.com and Yelp.com, respectively, to
compare the performance of our framework
with other mainstream methods under two ap-
plication scenarios: helpfulness identification
and regression of customer reviews. Exten-
sive results demonstrate that our framework
can achieve state-of-the-art performance with
substantial improvements.

1 Introduction

The massive number of reviews left by many ex-
perienced consumers on online products are our
priceless treasure in e-commerce. We believe that
online customer reviews can provide more sub-
jective and informative opinions from various per-
spectives on the products besides the objective de-

Figure 1: A screenshot of the top review on a product
sold in Amazon.com: SodaStream Fruit Drops Variety
Pack, 1.67 Pound. Online users can vote for the help-
fulness of each customer review by clicking the “Help-
ful” button underneath.

scriptions given by their merchants. Hence, we
prefer browsing the reviews online for the sake
of finding our desirable products. However, it is
quite time-consuming for potential buyers to sift
through millions of online reviews with uneven
qualities to make purchase decisions.

To discover and surface helpful reviews for cus-
tomers, some well-known websites (e.g., Ama-
zon.com) have launched a module illustrated by
Figure 1 which asks for feedback on the help-
fulness of online reviews. And a recent study1

reported that this featured module can increase
the revenue of Amazon with an estimated 27 bil-
lion U.S. dollars annually. Although this crowd-
sourcing module is quite useful to find a fraction
of helpful reviews, statistics (Fan et al., 2018) in-
dicate that roughly 60% online reviews still do
not receive any vote of helpfulness or unhelpful-

1articles.uie.com/magicbehindamazon/

articles.uie.com/magicbehindamazon/


1676

ness. This phenomenon on unknown helpfulness
is even more prominent in low-traffic items includ-
ing those less popular and new arrival products.

We believe it is a promising study that estab-
lishes an automatic helpfulness assessment sys-
tem for online reviews, which can be as useful
as a product recommendation engine (Park et al.,
2012) in e-commerce. Moreover, review helpful-
ness assessment (Diaz and Ng, 2018) and senti-
ment analysis (Liu and Zhang, 2012) are two dif-
ferent but related lines of work to study on the
user-generated content (UGC). Sentiment analysis
mainly focuses on identifying the opinion orien-
tation of the reviewer himself/herself on the target
product. A reviewer can express several emotional
words in his/her comment. However, this com-
ment may contain less information on the target
product and may not be helpful to other potential
consumers. Therefore, review helpfulness assess-
ment concerns more about whether a comment is
useful/helpful to other potential consumers.

So far as we know, a series of work on re-
view helpfulness prediction has been proposed
from two perspectives: 1) some work lever-
aged domain-specific knowledge to extract a wide
range of hand-crafted features (including struc-
tural (Mudambi and Schuff, 2010; Xiong and Lit-
man, 2014), lexical (Kim et al., 2006; Xiong and
Litman, 2011), syntactic (Kim et al., 2006), emo-
tional (Martin and Pu, 2014), semantic (Yang
et al., 2015) and even argument features (Liu
et al., 2017)) from the raw text of reviews as
the evidence fed to off-the-shelf learning tools
for helpfulness prediction; and 2) recent stud-
ies (Chen et al., 2018a,b; Fan et al., 2018) took
advantages of deep neural nets by modifying the
convolutional neural network (Kim, 2014) to ac-
quire the low-dimensional representations of help-
ful reviews without the aid of feature engineering.
Overall, these mainstream methods extract various
linguistic and neural features solely from the raw
text of a review as the evidence for helpfulness as-
sessment.

We, on the other hand, consider that identifying
the helpfulness of a review should be fully aware
of the metadata (such as the name, the brand, the
place of origin, the category, the description, etc.)
of the target product besides the textual content
of the review itself. To illustrate our idea, Fig-
ure 2 shows an example of two customer reviews
in Amazon.com with diverse helpfulness scores on

Figure 2: Two customer reviews with diverse helpful-
ness scores on the same product in Amazon.com.

the same product online. We can easily figure out
that the helpful review (104 of 114 people found
it helpful) may concern the key of “description”
in the metadata of the product, while the unhelp-
ful review (0 of 17 people found it helpful) talks
nothing about the product but expresses deep re-
morse. Without direct supervision from human be-
ings, it is difficult for machines to infer the correct
key/aspect in the product metadata that helpful re-
views really concern.

It leaves us with a problem of how to teach
machines learning to choose the correct key-value
product metadata to help assess the helpfulness of
free-text reviews more precisely. To address the is-
sue, this paper introduces a novel framework com-
posed of two mutual-benefit modules: a product
metadata selector (agent) and a review helpfulness
predictor (network). This work is proposed to be
deployed in a real-world system. Therefore, we
suggest to decouple the two modules (i.e., the se-
lector and the predictor). Leveraging the attention
mechanism is an alternative approach on an end-
to-end framework. However, the decoupled mod-
ules are preferred in an industrial system.

Given a product, the selector explores the con-
nection between the keys in the product metadata
and one of its reviews to take an action that se-
lects the correct value, and the successive predic-
tor exploits this value attended by the free-text re-
view to acquire better neural representations for
helpfulness prediction. The predictor is directly



1677

Predictor

Keys Values
Title Sodastream Sodamix Variety Pack …

Brand SodaStream

Categories Grocery, Gourmet Food

Description SodaStream soda mixes let you add new flavors
to your repertoire for parities, dinners and other
gatherings. This variety pack includes…

Product Metadata

Customer Review

Selector (Agent)
value

Loss

Policy Gradient Stochastic Gradient Descent

Reward

I did not need this but I read a review about it that
highly recommended getting it. Waste of my money.

Helpfulness: [0, 17]

Predicted score

Figure 3: Our reinforced framework for review help-
fulness prediction: R2HP.

optimized by the stochastic gradient descent algo-
rithm (Ruder, 2016) with the loss of prediction,
and the selector can be updated via the policy gra-
dient method (Sutton et al., 2000) rewarded with
the performance of the predictor.

We use two real-world datasets from Ama-
zon.com and Yelp.com, to compare the perfor-
mance with other mainstream approaches on two
application tasks: helpfulness identification and
regression of customer reviews. The experimental
results reveal that our framework can reach state-
of-the-art performance on both tasks with substan-
tial improvements. In addition, our framework can
help acquire the embeddings of the keys in product
metadata, and visualization results illustrate that
they can capture various aspects on customer re-
views.

2 Framework

The intuition of our framework (named reinforced
review helpfulness prediction, abbr. as R2HP) is to
predict the helpfulness of a customer review which
is fully aware of the correct product metadata se-
lected by a reinforced selector (agent). This work
is proposed to be deployed in a real-world sys-
tem which is responsible for recommending help-
ful reviews to millions of customers. Therefore,
we suggest decoupling the two modules (selector
and predictor). Leveraging the attention mecha-
nism (Vaswani et al., 2017) is an alternative ap-

proach to establishing an end-to-end framework.
However, the decoupled modules are preferred by
the industry.

As shown by Figure 3, the selector (agent)
learns from both the keys in the product metadata
and one of its reviews to take an action that se-
lects the correct value, and a successive predictor
(network) makes the free-text review attend to this
value to obtain better neural representations for
helpfulness prediction. The predictor is directly
optimized by SGD with the loss of prediction, and
the selector could be updated via policy gradient
rewarded with the performance of the predictor.

2.1 Reinforced Product Metadata Selection
Given a customer review and the key-value formed
product metadata, our reinforced neural selector π
takes them as input, to output a policy p which is
the probability distribution over the keys. Suppose
that the product metadata contains k keys each of
which is represented by an l-dimensional vector.
Then we can achieve the embeddings of the keys:
K ∈ Rl×k.

Assume that there are n words/tokens in the
customer review c. We align each word/token
with the embedding dictionary acquired by the
word embedding approaches such as Word2Vec
(Mikolov et al., 2013), Glove (Pennington et al.,
2014) or Elmo (Peters et al., 2018) to initialize the
distributed representations of the customer review
C ∈ Rl×n. To achieve the local contextual embed-
dings of the customer review c, we use a Bi-LSTM
network (Schuster and Paliwal, 1997) which takes
the word embeddings of the customer review C as
input:

Hc = Bi-LSTM(C). (1)

Hc ∈ R2l×n stands for the contextual embeddings
where each word can obtain two hidden units with
the length of 2l encoding both the backward and
the forward contextual information of the cus-
tomer review locally.

We use B ∈ Rk×n to obtain the bilinear rela-
tionship between the embeddings of the keys K
and the local contextual embeddings of the cus-
tomer review Hc:

B = ReLU(KTWHc), (2)

where W ∈ Rl×2l is the weight matrix for the
Rectifier Linear Unit (ReLU). The i-th row of B
contains the aspect/topic feature aligned by the lo-
cal contextual embeddings of the customer review.



1678

We apply the reduce max strategy to each row of B
to keep the most significant feature for each meta-
data key, and use the softmax function to gain the
policy p ∈ Rk:

p = Softmax(Reduce max(B, axis = 1)). (3)

Then our reinforced selector (agent) can select a
metadata value v ∼ π(v|K,C) in terms of p.

2.2 Product-aware Review Helpfulness
Prediction

In this part, we elaborate our neural predictor for
review helpfulness prediction. It is devised based
on the motivation that the helpfulness of an on-
line review should be fully aware of the selected
metadata of its target product besides the textual
content of the customer review itself. The pre-
dictor is composed of two components: 1) the lo-
cal contextual embeddings of a review and 2) the
product-aware distributed representations of the
review. We have explained how to obtain the local
contextual embeddings of a review Hc ∈ R2l×n
in the previous subsection. Here we will describe
how to achieve the product-aware distributed rep-
resentations of the review, denoted by Hc.

We use m to stand for the number of to-
kens/words in the selected metadata value v pro-
vided by our reinforced selector. Similarly, we can
refine the word embeddings of the value V of the
product metadata via another Bi-LSTM network:

Hv = Bi-LSTM(V), (4)

and achieve the contextual embeddings of the se-
lected product metadata Hv ∈ R2l×m. To make
the contextual embeddings of the customer review
fully aware of the product metadata, we design a
word-level matching mechanism as follows,

Q = ReLU(W′Hv + b′ ⊗ e)THc, (5)

where W′ ∈ R2l×2l is the weight matrix and
b′ ∈ R2l is the bias vector. The outer product ⊗
copys the bias vector b′ form times (i.e., e ∈ Rm)
to generate a 2l ×m matrix. Then Q ∈ Rm×n is
the sparse matrix that holds the word-level match-
ing information between the value v of the product
metadata and the customer review c.

If we further apply the softmax function to each
column of Q, we will obtain G ∈ Rm×n, the i-
th column of which represents the normalized at-
tention weights over all the words in the metadata

value v for the i-th word in the customer review c:

G = Softmax(Q). (6)
Then we can use the attention matrix G ∈ Rm×n
and the contextual embeddings of the product
metadata Hv ∈ R2l×m to re-form the product-
aware review representation Hc ∈ R2l×n:

Hc = HvG. (7)

Driven by original motivation, we need to join
the local contextual embeddings of the review
(Hc) and the product-aware distributed represen-
tations of the review (Hc) together for predicting
its helpfulness with the feature matrix H ∈ R2l×n:

H = Hc + Hc. (8)
H can also benefit from the idea of ResNet (He
et al., 2016) that efficiently acquires the residual
between Hc and Hc, and provides a highway to
update Hc if the residual is tiny.

Generally speaking, we define a loss function2

L(sg|Hc) which takes Hc as the feature to predict
a helpfulness score sp judged by the ground-truth
score sg. Given the value v is selected by π, our
objective is to minimize the expectation:

J(Θ) = Ev∼π(v|K,C)[L(sg|v,Hc)], (9)
where Θ are parameters to be learned. The gradi-
ent of J(Θ) with respect to Θ is:
∇ΘJ(Θ) (10)

= ∇Θ
∑
v

π(v|K,C)[L(sg|v,Hc)]

=
∑
v

(L(s
g|v,Hc)∇Θπ(v|K,C) + π(v|K,C)∇ΘL(sg|v,Hc))

= Ev∼π(v|K,C)[L(s
g|v,Hc)∇Θ log π(v|K,C) +∇ΘL(sg|v,Hc)],

where ∇ΘL(sg|v,Hc) refers to training the pre-
dictor by SGD and we use the REINFORCE al-
gorithm (Williams, 1992) to update the selector
with the gradient of log π(v|K,C) and the reward
L(sg|v,Hc) ∈ (0.0, 1.0].

3 Experiments

3.1 Real-world Datasets
We look up two well-formatted JSON resources
online which contain plenty of product metadata
(including titles, brands, categories, and descrip-
tions) and numerous customer reviews. One is
the data collection3 of Amazon.com crawled by

2We usually use the mean square error (MSE) as the
loss function for helpfulness score regression and the cross-
entropy as the loss function for helpfulness identification.

3The data collection is publicly available at http://
jmcauley.ucsd.edu/data/amazon/links.html

http://jmcauley.ucsd.edu/data/amazon/links.html
http://jmcauley.ucsd.edu/data/amazon/links.html


1679

Category (Amazon) Training Set Test Set
# (P.) # (R. ≥ 1v.) # (R. ≥ 0.75h.r.) # (P.) # (R. ≥ 1v.) # (R. ≥ 0.75h.r.)

Clothing, Shoes & Jewelry 8,789 52,402 37,036 964 6,630 4,668
Electronics 16,456 215,512 136,798 1,858 21,706 13,845
Grocery & Gourmet Food 30,617 231,131 151,828 3,392 25,984 16,982
Health & Personal Care 19,817 216,839 135,501 2,198 23,517 14,565
Home & Kitchen 17,999 202,549 147,324 1,978 24,163 17,896
Movies & TV 7,518 316,235 143,790 847 35,880 15,973
Pet Supplies 18,189 167,775 123,287 2,031 21,170 15,650
Tools & Home Improvement 30,105 217,397 150,483 3,454 24,971 17,426

TOTAL 149,490 1,619,840 1,026,047 16,722 184,021 117,005

Table 1: The statistics of the Amazon dataset for helpfulness prediction of online reviews. # (P.): the num-
ber of products; # (R. ≥ 1v.): the number of the reviews, each receiving at least 1 vote regardless of helpful-
ness/unhelpfulness; # (R. ≥ 0.75h.r.): the number of the reviews, each regarded as helpful by at least 75% votes.

Category (Yelp) Training Set Test Set
# (P.) # (R. ≥ 1v.) # (R. ≥ 0.75h.r.) # (P.) # (R. ≥ 1v.) # (R. ≥ 0.75h.r.)

Beauty & Spas 8,552 101,383 56,711 949 10,007 5,659
Health & Medical 10,079 90,045 57,967 1,132 10,434 6,758
Home Services 7,675 71,247 47,369 876 7,011 4,575
Restaurants 3,432 100,613 41,139 383 11,668 4,779
Shopping 11,706 104,555 49,833 1,295 12,892 5,969

TOTAL 41,444 467,843 253,019 4,635 52,012 27,740

Table 2: The statistics of the Yelp dataset for helpfulness prediction of online reviews. # (P.): the number of prod-
ucts; # (R.≥ 1v.): the number of the reviews, each receiving at least 1 vote regardless of helpfulness/unhelpfulness;
# (R. ≥ 0.75h.r.): the number of the reviews, each regarded as helpful by at least 75% votes.

(He and McAuley, 2016) up to July 2014. The
other one is the dump file4 directly provided by
Yelp.com for academic purposes. We use the
product ids (i.e., “asin” in Amazon and “busi-
ness id” in Yelp) as the foreign keys to align the
metadata of products with customer reviews. 80%
products with online reviews are randomly picked
as the training set, leaving the rest as the test set.
In this way, two real-world datasets, i.e., Amazon
and Yelp, are built, and the statistics of them are
shown by Table 1 and Table 2, respectively.

In this study, we regard the reviews which re-
ceive at least 1 vote for helpfulness/unhelpfulness,
i.e., the column # (R.) ≥ 1v. in Table 1 and Ta-
ble 2, as the experimental samples. In Amazon, the
crowd-sourcing module for voting helpful reviews
provides an “X of Y ” helpfulness score, where
“Y ” stands for the total number of users who par-
ticipate in voting, and “X” denotes the number of
users who think the review is helpful. Yelp of-
fers more options: useful: X , cool: Y , and funny:
Z, to the users who are willing to give feedback.

4The dump file can be downloaded from https://
www.yelp.com/dataset

Regardless of the difference, we generally con-
sider the reviews which receive at least 0.75 ratio
of helpfulness/usefulness (helpfulness/usefulness
score≥ 0.75) as positive samples, leaving the oth-
ers as the negative samples for classification.

3.2 Comparison Methods

We compare our framework (R2HP) with a wide
range of prior arts. Specifically, we re-implement
the methods of learning from deep neural net-
works and with hand-crafted features. The up-
to-date neural approaches involve the embedding-
gated CNN (EG-CNN) (Chen et al., 2018a,b) and
the multi-task neural learning (MTNL) architec-
ture (Fan et al., 2018) for review helpfulness pre-
diction. The hand-crafted features include the
structural features (STR) (Mudambi and Schuff,
2010; Xiong and Litman, 2014), the lexical fea-
tures (LEX) (Xiong and Litman, 2011), the emo-
tional features (GALC) (Martin and Pu, 2014) and
the semantic features (INQUIRER) (Yang et al.,
2015). We also add two more experiments on
integrating all the hand-crafted features via the
Support Vector Machines (SVM) and the Random

https://www.yelp.com/dataset
https://www.yelp.com/dataset


1680

Forest (R.F.) model for review helpfulness assess-
ment.

3.3 Application Scenarios

Previous studies mostly reported their perfor-
mance on either the application scenario of review
helpfulness identification or regression. There-
fore, we conduct extensive experiments compar-
ing the performance of our framework with all the
other approaches under both scenarios.

Identification of helpful reviews: As both the
training and test sets are imbalanced, we adopt
the Area under Receiver Operating Characteris-
tic (AUROC) as the metric to evaluate the per-
formance of all the approaches on helpful review
identification. In line with Table 3 and Table 4,
MTNL (Fan et al., 2018) achieves the best perfor-
mance up-to-date on this classification task among
the baseline approaches as it achieves the best
performance on 12 of 14 categories in Amazon
and Yelp datasets. R2HP surpasses MTNL on
both datasets and obtains state-of-the-art (micro-
averaged) results of 67.5% AUROC (Amazon)
and 75.1% AUROC (Yelp) with absolute improve-
ments of 4.9% AUROC and 4.7% AUROC, re-
spectively.

Regression of helpfulness score: In this task, all
the approaches are required to predict the fraction
of helpful votes that each review receives. We
use the data in the column # (R. ≥ 1v.) in Ta-
ble 1 and Table 2 as the training and test sets.
The Squared Correlation Coefficient (R2-score) is
adopted as the metric to evaluate the performance
of all the approaches on helpfulness score regres-
sion. Table 5 and Table 6 show that MTNL (Fan
et al., 2018) achieves state-of-the-art performance
on this regression task among the baseline ap-
proaches. Our framework outperforms MTNL on
both datasets and obtains state-of-the-art (micro-
averaged) results of 62.3% R2-score (Amazon)
and 74.0% R2-score (Yelp) with absolute improve-
ments of 5.4% R2-score and 5.8% R2-score, re-
spectively.

4 Discussions

4.1 Ablation Study on Metadata Selector

In this part, we conduct the ablation study on dif-
ferent metadata selectors: i.e., random selector,
heuristic selector, and our reinforced selector. The
random selector requires no prior knowledge but
randomly picks one pair of (key, value) from the

0.597 0.613

0.332 0.351

0.632

0.714

0.523

0.6640.675

0.751

0.623

0.74

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

Amazon (Micro-
averaged AUROC)

Yelp (Micro-averaged 
AUROC)

Amazon (Micro-
averaged R2 Score)

Yelp (Micro-averaged 
R2 Score)

Random Selector Heuristic Selector Reinforced Selector

Figure 4: Performances of review helpfulness identi-
fication and regression supported by the random, the
heuristic, and our reinforced selector on the Amazon
and Yelp datasets.

product metadata with uniform distribution. The
heuristic selector keeps choosing the pair of (key,
value) in which the value contains the longest
text5. Our reinforced selector learns from the re-
ward given by the helpfulness predictor and makes
a wise decision on the metadata selection. The val-
ues of product metadata selected by the three se-
lectors are fed into the same predictor.

Figure 4 shows the performance of both identi-
fication and regression of review helpfulness sup-
ported by the three different selectors, and the re-
sults demonstrate that our reinforced selector sur-
passes the other policies (the random selector and
the heuristic selector) for metadata selection.

4.2 Case Study on Key Embeddings
Our framework also helps to acquire the dis-
tributed representations of the keys in product
metadata. We believe these key embeddings can
capture various aspects/topics on customer re-
views and lead to the correct value for review
helpfulness prediction. With the help of t-SNE
(Maaten and Hinton, 2008), we can map the em-
beddings of the metadata keys from the Amazon
and Yelp datasets into 2D vectors and illustrate
them in Figure 5.

It shows that the key embeddings in Amazon lo-
cate at the bottom-right and the key embeddings
in Yelp are generally at upper-left. The keys which
express similar meanings within the same dataset
are close to each other, such as the keys “city”,
“state” and “address” in Yelp, and the keys “title”,
“description” and “brand” in Amazon. Even across

5In most cases, the “name” or the “description” of a prod-
uct is selected by the heuristic selector.



1681

Category (Amazon) Area under Receiver Operating Characteristic (AUROC)
STR LEX GALC INQUIRER FUSION (SVM) FUSION (R.F.) EG-CNN MTNL R2HP

Clothing, Shoes & Jewelry 0.548 0.536 0.562 0.601∗ 0.579 0.550 0.583 0.589 0.623 (+0.022)
Electronics 0.583 0.549 0.588 0.616∗ 0.577 0.580 0.611 0.613 0.661 (+0.045)
Grocery & Gourmet Food 0.536 0.526 0.553 0.602 0.532 0.546 0.611 0.626∗ 0.657 (+0.031)
Health & Personal Care 0.558 0.523 0.559 0.610 0.591 0.562 0.613 0.620∗ 0.683 (+0.063)
Home & Kitchen 0.568 0.537 0.565 0.597 0.569 0.573 0.603 0.610∗ 0.646 (+0.036)
Movies & TV 0.603 0.558 0.621 0.634 0.603 0.607 0.648 0.652∗ 0.713 (+0.061)
Pet Supplies 0.560 0.542 0.585 0.603 0.548 0.558 0.580 0.629∗ 0.692 (+0.063)
Tools & Home Improvement 0.584 0.558 0.580 0.592 0.575 0.586 0.617 0.624∗ 0.672 (+0.048)

MACRO AVERAGE 0.568 0.541 0.577 0.607 0.572 0.570 0.608 0.620∗ 0.668 (+0.048)
MICRO AVERAGE (Primary) 0.571 0.543 0.580 0.609 0.573 0.574 0.614 0.625∗ 0.675 (+0.049)

Table 3: Results of performance (AUROC) of up-to-date methods on identifying helpful reviews evaluated by
the test sets of Amazon. (italic fonts∗: the best performance among the baseline approaches; bold fonts: the
state-of-the-art performance of all the approaches)

Category (Yelp) Area under Receiver Operating Characteristic (AUROC)
STR LEX GALC INQUIRER FUSION (SVM) FUSION (R.F.) EG-CNN MTNL R2HP

Beauty & Spas 0.573 0.610 0.627 0.674 0.651 0.662 0.693 0.702∗ 0.742 (+0.040)
Health & Medical 0.585 0.617 0.638 0.676 0.632 0.631 0.680 0.697∗ 0.725 (+0.028)
Home Services 0.582 0.596 0.625 0.684 0.635 0.638 0.663 0.704∗ 0.762 (+0.058)
Restaurants 0.595 0.616 0.652 0.682 0.669 0.654 0.681 0.695∗ 0.738 (+0.043)
Shopping 0.582 0.618 0.660 0.699 0.672 0.685 0.682 0.719∗ 0.784 (+0.065)

MACRO AVERAGE 0.583 0.611 0.640 0.683 0.652 0.654 0.680 0.703∗ 0.750 (+0.047)
MICRO AVERAGE (Primary) 0.584 0.613 0.643 0.684 0.654 0.656 0.681 0.704∗ 0.751 (+0.047)

Table 4: Results of performance (AUROC) on identifying helpful reviews evaluated by the test sets of Yelp.

Category (Amazon) Squared Correlation Coefficient (R
2-score)

STR LEX GALC INQUIRER FUSION (SVM) FUSION (R.F.) EG-CNN MTNL R2HP

Clothing, Shoes & Jewelry 0.265 0.337 0.398 0.588 0.490 0.506 0.558 0.613∗ 0.678 (+0.065)
Electronics 0.254 0.303 0.356 0.550 0.496 0.497 0.534 0.593∗ 0.636 (+0.043)
Grocery & Gourmet Food 0.242 0.319 0.408 0.450 0.426 0.425 0.469 0.506∗ 0.570 (+0.064)
Health & Personal Care 0.237 0.242 0.376 0.453 0.420 0.445 0.489 0.519∗ 0.583 (+0.064)
Home & Kitchen 0.236 0.298 0.330 0.464 0.439 0.461 0.502 0.588∗ 0.644 (+0.056)
Movies & TV 0.253 0.328 0.412 0.487 0.460 0.452 0.523 0.594∗ 0.636 (+0.042)
Pet Supplies 0.237 0.337 0.385 0.420 0.401 0.439 0.501 0.573∗ 0.657 (+0.084)
Tools & Home Improvement 0.234 0.301 0.387 0.437 0.447 0.502 0.532 0.591∗ 0.624 (+0.033)

MACRO AVERAGE 0.245 0.308 0.382 0.481 0.447 0.466 0.514 0.572∗ 0.629 (+0.057)
MICRO AVERAGE (Primary) 0.243 0.307 0.382 0.471 0.444 0.461 0.510 0.569∗ 0.623 (+0.054)

Table 5: Results of performance (R2-score) on helpfulness voting regression evaluated by the test sets of Amazon.

Category (Yelp) Squared Correlation Coefficient (R
2-score)

STR LEX GALC INQUIRER FUSION (SVM) FUSION (R.F.) EG-CNN MTNL R2HP

Beauty & Spas 0.383 0.484 0.511 0.537 0.549 0.518 0.650 0.662∗ 0.727 (+0.065)
Health & Medical 0.353 0.454 0.533 0.557 0.598 0.579 0.672 0.686∗ 0.735 (+0.049)
Home Services 0.364 0.452 0.516 0.554 0.592 0.601 0.670 0.695∗ 0.741 (+0.046)
Restaurants 0.396 0.438 0.483 0.501 0.556 0.597 0.610 0.623∗ 0.682 (+0.059)
Shopping 0.406 0.447 0.537 0.623 0.634 0.709 0.737 0.742∗ 0.805 (+0.063)

MACRO AVERAGE 0.380 0.455 0.516 0.554 0.586 0.601 0.668 0.682∗ 0.738 (+0.056)
MICRO AVERAGE (Primary) 0.383 0.454 0.516 0.557 0.587 0.606 0.670 0.682∗ 0.740 (+0.058)

Table 6: Results of performance (R2-score) on helpfulness voting regression evaluated by the test sets of Yelp.
(italic fonts∗: the best performance among the baseline approaches; bold fonts: the state-of-the-art performance
of all the approaches)



1682

Figure 5: The 2D visualization of the embeddings of
the metadata keys in Amazon and Yelp.

different datasets, the key embeddings draw near
to each other if they are close in meaning, e.g., “ti-
tle” (Amazon) and “name” (Yelp), or “asin” (Ama-
zon) and “business id” (Yelp).

5 Conclusion and Future Work

Driven by the intuition that the helpfulness of an
online review should be fully aware of the meta-
data of its target product besides the textual con-
tent of the review itself, we take on the challenge
of selecting the correct key-value product meta-
data to help predict the helpfulness of free-text re-
views more precisely. To address the problem, we
propose a novel framework in this paper, which is
composed of two interdependent modules. Given
a product, an agent (selector) learns from both the
keys in the product metadata and one of its reviews
to take an action that selects the correct value, and
a successive network (predictor) makes the free-
text review attend to this value to produce bet-
ter neural representations for helpfulness predic-
tion. We use two real-world datasets from Ama-
zon and Yelp, respectively, to compare the perfor-
mance of our framework with other mainstream
methods on two tasks: helpfulness identification
and regression of online reviews. Extensive re-
sults show that our framework can achieve state-
of-the-art performance with substantial improve-
ments. Further discussions demonstrate that it can
not only provide better policies on selecting the
correct value of product metadata but also acquire
the embeddings of the keys in the product meta-
data.

We also believe the study on review helpfulness
assessment could be as important as the topic of
product recommendation, and several open prob-

lems are deserved to be explored in the future:
User-specific and explainable recommendation

of helpful reviews: As different users may concern
about various aspects of the products online, help-
ful review recommendation needs to be more user-
specific and self-explainable.

Enhancing the prediction of helpful reviews
with unlabeled data: As a small proportion of re-
views could be heuristically regarded as helpful or
unhelpful, it thus becomes a promising study to
automatically predict the helpfulness of online re-
views based on the small amount of labeled data
and a vast amount of unlabeled data.

Cross-domain helpfulness prediction of online
reviews (Chen et al., 2018b): Given that it costs
a lot on manually annotating a sufficient number
of helpful reviews in a new domain, we should
explore effective approaches on transferring use-
ful knowledge from limited labeled samples in an-
other domain.

References
Cen Chen, Minghui Qiu, Yinfei Yang, Jun Zhou, Jun

Huang, Xiaolong Li, and Forrest Sheng Bao. 2018a.
Review helpfulness prediction with embedding-
gated CNN. CoRR, abs/1808.09896.

Cen Chen, Yinfei Yang, Jun Zhou, Xiaolong Li, and
Forrest Sheng Bao. 2018b. Cross-domain review
helpfulness prediction based on convolutional neu-
ral networks with auxiliary domain discriminators.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL’18), pages 602–607. Association for Com-
putational Linguistics.

Gerardo Ocampo Diaz and Vincent Ng. 2018. Model-
ing and prediction of online product review helpful-
ness: a survey. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (ACL’18), pages 698–708.

Miao Fan, Yue Feng, Mingming Sun, Ping Li, Haifeng
Wang, and Jianmin Wang. 2018. Multi-task neu-
ral learning architecture for end-to-end identifica-
tion of helpful reviews. In Proceedings of the
2018 IEEE/ACM International Conference on Ad-
vances in Social Networks Analysis and Mining
(ASONAM’18), pages 343–350.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
Jian Sun. 2016. Deep residual learning for image
recognition. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition
(CVPR’16), pages 770–778.

Ruining He and Julian McAuley. 2016. Ups and
downs: Modeling the visual evolution of fashion



1683

trends with one-class collaborative filtering. In Pro-
ceedings of the 25th International Conference on
World Wide Web (WWW’16), pages 507–517. In-
ternational World Wide Web Conferences Steering
Committee.

Soo-Min Kim, Patrick Pantel, Tim Chklovski, and
Marco Pennacchiotti. 2006. Automatically assess-
ing review helpfulness. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP’06), pages 423–430. As-
sociation for Computational Linguistics.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP’14), pages 1746–1751.
Association for Computational Linguistics.

Bing Liu and Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. In Mining text data,
pages 415–463. Springer.

Haijing Liu, Yang Gao, Pin Lv, Mengxue Li, Shiqiang
Geng, Minglan Li, and Hao Wang. 2017. Using
argument-based features to predict and analyse re-
view helpfulness. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP’17), pages 1369–1374. Asso-
ciation for Computational Linguistics.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research (JMLR), 9(Nov):2579–2605.

Lionel Martin and Pearl Pu. 2014. Prediction of help-
ful reviews using emotions extraction. In Proceed-
ings of the Twenty-Eighth AAAI Conference on Ar-
tificial Intelligence (AAAI’14), pages 1551–1557.
AAAI Press.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems (NIPS’13), pages 3111–3119.

Susan M. Mudambi and David Schuff. 2010. What
makes a helpful online review? a study of cus-
tomer reviews on amazon.com. MIS Quarterly,
34(1):185–200.

Deuk Hee Park, Hyea Kyeong Kim, Il Young Choi, and
Jae Kyeong Kim. 2012. A literature review and clas-
sification of recommender systems research. Expert
Systems with Applications, 39(11):10059–10072.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP’14), pages 1532–1543.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL’18), pages 2227–2237. Asso-
ciation for Computational Linguistics.

Sebastian Ruder. 2016. An overview of gradient
descent optimization algorithms. arXiv preprint
arXiv:1609.04747.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673–2681.

Richard S Sutton, David A McAllester, Satinder P
Singh, and Yishay Mansour. 2000. Policy gradi-
ent methods for reinforcement learning with func-
tion approximation. In Advances in Neural Infor-
mation Processing Systems (NIPS’00), pages 1057–
1063.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems (NIPS’17), pages 5998–6008.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Wenting Xiong and Diane Litman. 2011. Automat-
ically predicting peer-review helpfulness. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL’11), pages 502–507. As-
sociation for Computational Linguistics.

Wenting Xiong and Diane J Litman. 2014. Empirical
analysis of exploiting review helpfulness for extrac-
tive summarization of online reviews. In Proceed-
ings of the 25th International Conference on Com-
putational Linguistics (COLING’14), pages 1985–
1995. Association for Computational Linguistics.

Yinfei Yang, Yaowei Yan, Minghui Qiu, and Forrest
Bao. 2015. Semantic analysis and helpfulness pre-
diction of text for online product reviews. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (ACL’15), pages 38–44. Association for
Computational Linguistics.


