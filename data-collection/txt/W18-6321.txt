



















































Simple Fusion: Return of the Language Model


Proceedings of the Third Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 204–211
Belgium, Brussels, October 31 - Novermber 1, 2018. c©2018 Association for Computational Linguistics

https://doi.org/10.18653/v1/W18-64021

Simple Fusion: Return of the Language Model

Felix Stahlberg†∗ and James Cross‡ and Veselin Stoyanov‡
†Department of Engineering, University of Cambridge, UK

‡Applied Machine Learning, Facebook, Menlo Park, CA, USA
fs439@cam.ac.uk, jcross@fb.com, vesko.st@gmail.com

Abstract

Neural Machine Translation (NMT) typically
leverages monolingual data in training through
backtranslation. We investigate an alterna-
tive simple method to use monolingual data
for NMT training: We combine the scores of
a pre-trained and fixed language model (LM)
with the scores of a translation model (TM)
while the TM is trained from scratch. To
achieve that, we train the translation model to
predict the residual probability of the train-
ing data added to the prediction of the LM.
This enables the TM to focus its capacity on
modeling the source sentence since it can rely
on the LM for fluency. We show that our
method outperforms previous approaches to
integrate LMs into NMT while the architec-
ture is simpler as it does not require gating
networks to balance TM and LM. We observe
gains of between +0.24 and +2.36 BLEU on
all four test sets (English-Turkish, Turkish-
English, Estonian-English, Xhosa-English) on
top of ensembles without LM. We compare
our method with alternative ways to uti-
lize monolingual data such as backtranslation,
shallow fusion, and cold fusion.

1 Introduction

Machine translation (MT) relies on parallel train-
ing data, which is difficult to acquire. In contrast,
monolingual data is abundant for most languages
and domains. Traditional statistical machine trans-
lation (SMT) effectively leverages monolingual
data using language models (LMs) (Brants et al.,
2007). The combination of LM and TM in SMT
can be traced back to the noisy-channel model
which applies the Bayes rule to decompose a

0This work was done when the first author was on an in-
ternship at Facebook.

translation system (Brown et al., 1993):

ŷ =argmax
y

P (y|x)

= argmax
y

PTM (x|y)PLM (y)
(1)

where x = (x1, . . . , xm) is the source sentence,
y = (y1, . . . , yn) is the target sentence, and
PTM (·) and PLM (·) are translation model and lan-
guage model probabilities.

In contrast, NMT (Sutskever et al., 2014; Bah-
danau et al., 2014) uses a discriminative model and
learns the distribution P (y|x) directly end-to-end.
Therefore, the vanilla training regimen for NMT is
not amenable to integrating an LM or monoglin-
gual data in a straightforward manner.

An early attempt to use LMs for NMT, also
known as shallow fusion, combines LM and
NMT scores at inference time in a log-linear
model (Gulcehre et al., 2015, 2017). In contrast,
we integrate the LM scores during NMT train-
ing. Our training procedure first trains an LM on
a large monolingual corpus. We then hold the LM
fixed and train the NMT system to optimize the
combined score of LM and NMT on the parallel
training set. This allows the NMT model to fo-
cus on modeling the source sentence, while the
LM handles the generation based on the target-
side history. Sriram et al. (2017) explored a simi-
lar idea for speech recognition using a gating net-
work for controlling the relative contribution of
the LM. We show that our simpler architecture
without an explicit control mechanism is effective
for machine translation. We observe gains of up to
more than 2 BLEU points from adding the LM to
TM training. We also show that our method can
be combined with backtranslation (Sennrich et al.,
2016a), yielding further gains over systems with-
out LM.

204

https://doi.org/10.18653/v1/W18-64021


2 Related Work

2.1 Inference-time Combination
Shallow fusion (Gulcehre et al., 2015) integrates
an LM by changing the decoding objective to:

ŷ = argmax
y

logPTM(y|x) + λ logPLM(y). (2)

PLM(·) is produced by an LSTM-based RNN-
LM (Mikolov et al., 2010) which has been
trained on monolingual target language data.
PTM(·) can be a typical encoder-decoder Seq2Seq
model (Sutskever et al., 2014; Bahdanau et al.,
2014; Luong et al., 2015a). λ is a hyper-parameter
which is tuned on the development set.

2.2 Cold Fusion
Shallow fusion combines a fixed TM with a fixed
LM at inference time. Sriram et al. (2017) pro-
posed to keep the LM fixed, but train a sequence
to sequence (Seq2Seq) NMT model from scratch
which includes the LM as a fixed part of the net-
work. They argue that this approach allows the
Seq2Seq network to use its model capacity for the
conditioning on the source sequence since the lan-
guage modeling aspect is already covered by the
LM. Their cold fusion architecture includes a gat-
ing network which learns to regulate the contribu-
tions of the LM at each time step. They demon-
strated superior performance of cold fusion on a
speech recognition task.

2.3 Other Approaches
Gulcehre et al. (2015, 2017) suggest to combine a
pre-trained RNN-LM with a pre-trained NMT sys-
tem using a controller network that dynamically
adjusts the weights between RNN-LM and NMT
at each time step (deep fusion). Both deep fusion
and n-best reranking with count-based LMs have
been used in WMT evaluation systems (Jean et al.,
2015; Wang et al., 2017). An important limitation
of these approaches is that LM and TM are trained
independently.

A second line of research augments the parallel
training data with additional synthetic data from a
monolingual corpus in the target language. The
source sentences can be generated with a sepa-
rate translation system (Schwenk, 2008; Sennrich
et al., 2016a) (backtranslation), or simply copied
over from the target side (Currey et al., 2017).
Since data augmentation methods rely on some

balance between real and synthetic data (Sennrich
et al., 2016a; Currey et al., 2017; Poncelas et al.,
2018), they can often only use a small fraction of
the available monolingual data. A third class of
approaches change the NMT training loss func-
tion to incorporate monolingual data. For exam-
ple, Cheng et al. (2016); Tu et al. (2017) pro-
posed to add autoencoder terms to the training
objective which capture how well a sentence can
be reconstructed from its translated representation.
However, training with respect to the new loss is
often computationally intensive and requires ap-
proximations. Alternatively, multi-task learning
has been used to incorporate source-side (Zhang
and Zong, 2016) and target-side (Domhan and
Hieber, 2017) monolingual data. Another way
of utilizing monolingual data in both source and
target language is to warm start Seq2Seq train-
ing from pre-trained encoder and decoder net-
works (Ramachandran et al., 2017; Skorokhodov
et al., 2018). We note that pre-training can be used
in combination with our approach.

An extreme form of leveraging monolingual
training data is unsupervised NMT (Lample et al.,
2017; Artetxe et al., 2017) which removes the need
for parallel training data entirely. In this work, we
assume to have access to some amount of parallel
training data, but aim to improve the translation
quality even further by using a language model.

3 Translation Model Training under
Language Model Predictions

In spirit of the cold fusion technique of Sriram
et al. (2017) we also keep the LM fixed when train-
ing the translation network. However, we greatly
simplify the architecture by removing the need for
a gating network. We follow the usual left-to-right
factorization in NMT:

P (y|x) =
n∏

t=1

P (yt|yt−11 ,x). (3)

Let STM(yt|yt−11 ,x) be the output of the TM
projection layer without softmax, i.e., what we
would normally call the logits. We investigate
two different ways to parameterize P (yt|yt−11 ,x)
using STM(yt|yt−11 ,x) and a fixed and pre-
trained language model PLM(·): POSTNORM and
PRENORM.

POSTNORM This variant is directly inspired by
shallow fusion (Eq. 2) as we turn STM(yt|yt−11 ,x)

205



into a probability distribution using a softmax
layer, and sum its log-probabilities with the log-
probabilities of the LM, i.e. multiply their proba-
bilities:

P (yt|yt−11 ,x) =softmax(STM(yt|yt−11 ,x))
· PLM(yt|yt−11 ).

(4)

PRENORM Another option is to apply normal-
ization after combining the raw STM(yt|yt−11 ,x)
scores with the LM log-probability:

P (yt|yt−11 ,x) =softmax
(
STM(yt|yt−11 ,x)

+ logPLM(yt|yt−11 )
)
.

(5)

3.1 Theoretical Discussion of POSTNORM
and PRENORM

Note that P (yt|yt−11 ,x) might not represent a
valid probability distribution under the POST-
NORM criterion since, as component-wise prod-
uct of two distributions, it is not guaranteed to
sum to 1. A way to fix this issue would be to
combine TM and LM probabilities in the proba-
bility space rather than in the log space. However,
we have found that probability space combination
does not work as well as POSTNORM in our exper-
iments. We can describe STM(yt|yt−11 ,x) under
POSTNORM informally as the residual probability
added to the prediction of the LM.

It is interesting to investigate what signal is
actually propagated into STM(yt|yt−11 ,x) when
training with the PRENORM strategy. We can
rewrite P (yt|yt−11 ,x) as:

P (yt|yt−11 ,x) =
P (yt, y

t−1
1 |x)

P (yt−11 |x)

=
P (yt,x|yt−11 )
P (x|yt−11 )

=
P (x|yt, yt−11 )
P (x|yt−11 )

P (yt|yt−11 ).

(6)

Alternatively, we can decompose P (yt|yt−11 ,x) as

Language pair # Sentences
Turkish-English (WMT) 207.7K
Estonian-English (WMT) 2,178.0K
Xhosa-English (INTERNAL) 739.2K

Table 1: Parallel training data.

Language # Sentences LM Perplexity
dev test

English (WMT) 26.9M 91.16 87.77
Turkish (WMT) 3.0M 59.19 70.46
English (INTERNAL) 20.0M 105.28 108.19

Table 2: Monolingual training data.

follows using Eq. 5:

P (yt|yt−11 ,x) =softmax
(
STM(yt|yt−11 ,x)

+ logPLM(yt|yt−11 )
)

∝ exp
(
STM(yt|yt−11 ,x)

+ logPLM(yt|yt−11 )
)

=exp(STM(yt|yt−11 ,x))
· PLM(yt|yt−11 ).

(7)

Combining Eq. 6 and Eq. 7 leads to:

exp(STM(yt|yt−11 ,x)) ∝
P (x|yt1)
P (x|yt−11 )

(8)

This means that STM(yt|yt−11 ,x) under
PRENORM is trained to predict how much
more likely the source sentence becomes when a
particular target token yt is revealed.

4 Experimental Setup

We evaluate our method on a variety of pub-
licly available and proprietary data sets. For
our Turkish-English (tr-en), English-Turkish (en-
tr), and Estonian-English (et-en) experiments we
use all available parallel data from the WMT18
evaluation campaign to train the translation mod-
els. Our language models are trained on News
Crawl 2017. We use news-test2017 as develop-
ment (“dev”) set and news-test2018 as test set.

Additionally, we collected our own proprietary
corpus of public posts on Facebook. We refer to
it as ‘INTERNAL’ data set. This corpus consists
of monolingual English in-domain sentences and
parallel data in Xhosa-English. Training set sizes
are summarized in Tables 1 and 2.

Our preprocessing consists of lower-casing, to-
kenization, and subword-segmentation using joint

206



Architecture Hyperparameters
Source vocab size (BPE) 16,000
Target vocab size (BPE) 16,000
Embedding size (all) 256
Encoder LSTM units 512
Encoder layers 2
Decoder LSTM units 512
Decoder layers 2
Attention type dot product

Training Settings
Optimization Vanilla SGD
Learning rate 0.5
Batch size 32
Label smoothing � 0.1
Checkpoint averaging Last 10

Table 3: Summary of NMT settings for all models.

byte pair encoding (Sennrich et al., 2016b) with
16K merge operations. On Turkish, we addition-
ally remove diacritics from the text.

On WMT we use lower-cased Sacre-
BLEU1 (Post, 2018) to be comparable with
the literature.2 On our internal data we report
tokenized BLEU scores.

Our Seq2Seq models are encoder-decoder ar-
chitectures (Sutskever et al., 2014; Bahdanau
et al., 2014) with dot-product attention (Luong
et al., 2015b) trained with our PyTorch Trans-
late library.3 Both decoder and encoder consist
of two 512-dimensional LSTM layers and 256-
dimensional embeddings. The first encoder layer
is bidirectional, the second one runs from right to
left. Our training and architecture hyperparame-
ters are summarized in Tab. 3. Our LSTM-based
LMs have the same size and architecture as the de-
coder networks, but do not use attention and do not
condition on the source sentence. We run beam
search with beam size of 6 in all our experiments.

For each setup we train five models using SGD
(batch size of 32 sentences) with learning rate
decay and label smoothing, and either select the
best one (single system) or ensemble the four best
models based on dev set BLEU score.

5 Results

Tab. 4 compares our methods PRENORM and
POSTNORM on the tested language pairs. Shal-
low fusion (Sec. 2.1) often leads to minor im-
provements over the baseline for both single sys-
tems and ensembles. We also reimplemented the

1SacreBLEU signature for tr-en test-2017:
BLEU+c.lc+l.tr-en+#.1+s.exp+t.wmt17+tok.13a+v.1.2.10

2For translation into Turkish we evaluate after diacritics
removal.

3https://github.com/pytorch/translate

English-Turkish (WMT)
Method Single 4-Ensemble

dev test dev test
Baseline (no LM) 12.23 11.56 14.17 13.35
Shallow fusion 12.45 11.61 14.43 13.51
Cold fusion 12.39 11.54 14.20 13.23
This work: PRENORM 12.82 11.93 14.78 13.41
This work: POSTNORM 13.30 12.27 14.77 13.61

Turkish-English (WMT)
Method Single 4-Ensemble

dev test dev test
Baseline (no LM) 16.14 16.60 18.01 18.67
Shallow fusion 16.11 16.70 18.01 18.67
Cold fusion 16.25 16.21 17.99 18.40
This work: PRENORM 15.88 16.39 17.95 18.40
This work: POSTNORM 16.59 17.03 18.38 19.17

Estonian-English (WMT)
Method Single 4-Ensemble

dev test dev test
Baseline (no LM) 16.02 16.57 16.83 17.91
Shallow fusion 16.02 16.57 16.83 17.91
Cold fusion 15.40 15.99 16.48 17.79
This work: PRENORM 16.80 17.44 17.78 19.01
This work: POSTNORM 16.43 17.10 17.62 18.63

Xhosa-English (INTERNAL)
Method Single 4-Ensemble

dev test dev test
Baseline (no LM) 10.39 11.49 13.87 15.43
Shallow fusion 10.69 11.65 14.06 15.54
Cold fusion 10.72 11.29 13.66 15.13
This work: PRENORM 11.06 12.13 14.50 16.07
This work: POSTNORM 12.34 13.27 15.45 17.79

Table 4: Comparison of our PRENORM and POST-
NORM combination strategies with shallow fu-
sion (Gulcehre et al., 2015) and cold fusion (Sriram
et al., 2017) under an RNN-LM.

cold fusion technique (Sec. 2.2) for comparison.
For our machine translation experiments we re-
port mixed results with cold fusion, with per-
formance ranging between 0.33 BLEU gain on
Xhosa-English and slight BLEU degradation in
most of our Turkish-English experiments.

Both of our methods, PRENORM and POST-
NORM yield significant improvements in BLEU
across the board. We report more consistent gains
with POSTNORM than with PRENORM. All our
POSTNORM systems outperform both shallow fu-
sion and cold fusion on all language pairs, yielding
test set gains of up to +2.36 BLEU (Xhosa-English
ensembles).

6 Discussion and Analysis

Backtranslation A very popular technique to
use monolingual data for NMT is backtransla-
tion (Sennrich et al., 2016a). Backtranslation

207



Figure 1: Performance using backtranslation on
English-Turkish. Synthetic sentences are mixed at a
ratio of 1:n where n is plotted on the x-axis.

Figure 2: Convergence of NMT training with and with-
out LM on English-Turkish.

uses a reverse NMT system to translate mono-
lingual target language sentences into the source
language, and adds the newly generated sentence
pairs to the training data. The amount of monolin-
gual data which can be used for backtranslation is
usually limited by the size of the parallel corpus
as the translation quality suffers when the mix-
ing ratio between synthetic and real source sen-
tences is too large (Poncelas et al., 2018). This
is a severe limitation particularly for low-resource
MT. Fig. 1 shows that both our baseline system
without LM and our POSTNORM system benefit
greatly from backtranslation up to a mixing ratio
of 1:8, but degrade slightly if this ratio is exceeded.
POSTNORM is significantly better than the base-
line even when using it in combination with back-
translation.

Training convergence We have found that
training converges faster under the POSTNORM
loss. Fig. 2 plots the training curves of our sys-

English-Turkish (WMT, single system)
Method Dev set Test set

FFN RNN FFN RNN
Baseline (no LM) 12.23 11.56
Shallow fusion 12.25 12.45 11.53 11.61
Cold fusion 12.33 12.39 11.51 11.54
This work: PRENORM 12.76 12.82 11.82 11.93
This work: POSTNORM 12.65 13.30 11.79 12.27

Table 5: Comparison between using a recurrent LM
(RNN) and an n-gram based feedforward LM (FFN)
on English-Turkish.

English-Turkish (WMT), POSTNORM strategy
LM type Single 4-Ensemble

FFN RNN dev test dev test
12.23 11.56 14.17 13.35

X 12.65 11.79 14.36 13.48
X 13.30 12.27 14.77 13.61

X X 12.86 12.02 14.72 13.70

Table 6: Combining an RNN-LM and a feedforward
LM with the translation model using the POSTNORM
strategy.

tems. The baseline (orange curve) reaches its max-
imum of 19.39 BLEU after 28 training epochs.
POSTNORM surpasses this BLEU score already
after 12 epochs.

Language model type So far we have used re-
current neural network language models (Mikolov
et al., 2010, RNN-LM) with LSTM cells in all
our experiments. We can also parameterize an
n-gram language model with a feedforward neu-
ral network (Bengio et al., 2003, FFN-LM). In
order to compare both language model types we
trained a 4-gram feedforward LM with two 512-
dimensional hidden layers and 256-dimensional
embeddings on Turkish monolingual data. Tab. 5
shows that the PRENORM strategy works particu-
larly well for the n-gram LM. However, using an
RNN-LM with the POSTNORM strategy still gives
the best overall performance. Using both RNN
and n-gram LM at the same time does not improve
translation quality any further (Tab. 6).

Impact on the TM distribution With the POST-
NORM strategy, the TM still produces a distribu-
tion over the target vocabulary as the scores are

Method Perplexity Average entropy
Baseline (no LM) 23.46 3.19
RNN-LM 59.19 4.66
TM under POSTNORM 113.69 1.82

Table 7: Perplexity and average entropies of the dis-
tributions generated by our systems on the English-
Turkish dev set.

208



Method BLEU Precisions BP
1-gram 2-gram 3-gram 4-gram

Baseline (no LM) 17.91 53.0 23.7 12.3 6.6 0.996
This work: PRENORM 19.01 54.0 24.9 13.4 7.4 1.000
Relative improvement +6.14% +1.89% +5.06% +8.94% +12.12% –

Table 8: BLEU n-gram precisions for Estonian-English.

Source Eestis ja Hispaanias peeti kinni neli Kemerovo grupeeringu liiget
Reference Four members of the Kemerovo group arrested in Estonia and Spain
Baseline (no LM) In Estonia and Spain, four kemerovo groups were held
This work (PRENORM) Four Kemerovo group members were held in Estonia and Spain
Source Ta tleb, et elab aastaid hiljem endiselt hirmus.
Reference He says that years later, he still lives in fear.
Baseline (no LM) He says that, for years, he still lives in fear.
This work (PRENORM) He says that many years later he still lives in fear.
Source “Ma kardan,” tleb ta.
Reference “I’m afraid,” he says.
Baseline (no LM) “I fear,” says he.
This work (PRENORM) “I am afraid,” he says.

Table 9: Translation samples from the Estonian-English test set.

normalized before the combination with the LM.
This raises a natural question: How different are
the distributions generated by a TM trained un-
der POSTNORM loss from the distributions of the
baseline system without LM? Tab. 7 gives some
insight to that question. As expected, the RNN-
LM has higher perplexity than the baseline as it is
a weaker model of translation. The RNN-LM also
has a higher average entropy which indicates that
the LM distributions are smoother than those from
the baseline translation model. The TM trained
under POSTNORM loss has a much higher per-
plexity which suggests that it strongly relies on the
LM predictions and performs poorly when it is not
combined with it. However, the average entropy is
much lower (1.82) than both other models, i.e. it
produces much sharper distributions.

Language models improve fluency A tradi-
tional interpretation of the role of an LM in MT
is that it is (also) responsible for the fluency of
translations (Koehn, 2009). Thus, we would ex-
pect more fluent translations from our method than
from a system without LM. Tab. 8 breaks down
the BLEU score of the baseline and the PRENORM
ensembles on Estonian-English into n-gram preci-
sions. Most of the BLEU gains can be attributed to
the increase in precision of higher order n-grams,
indicating improvements in fluency. Tab. 9 shows
some examples where our PRENORM system pro-
duces a more fluent translation than the baseline.

Training set size We artificially reduced the size
of the English-Turkish training set even further

Figure 3: English-Turkish BLEU over training set size.

to investigate how well our method performs in
low-resource settings (Fig. 3). Our POSTNORM
strategy outperforms the baseline regardless of the
number of training sentences, but the gains are
smaller on very small training sets.

7 Conclusion

We have presented a simple yet very effective
method to use language models in NMT which in-
corporates the LM already into NMT training. We
reported significant and consistent gains from us-
ing our method in four language directions over
two alternative ways to integrate LMs into NMT
(shallow fusion and cold fusion) and showed that
our approach works well even in combination with
backtranslation and on top of ensembles. Our
method leads to faster training convergence and
more fluent translations than a baseline system
without LM.

209



References
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and

Kyunghyun Cho. 2017. Unsupervised neural ma-
chine translation. arXiv preprint arXiv:1710.11041.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of machine learning research,
3(Feb):1137–1155.

Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 858–867, Prague, Czech Republic.
Association for Computational Linguistics.

Peter E. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2).

Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Semi-
supervised learning for neural machine translation.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1965–1974. Association for
Computational Linguistics.

Anna Currey, Antonio Valerio Miceli Barone, and Ken-
neth Heafield. 2017. Copied monolingual data im-
proves low-resource neural machine translation. In
Proceedings of the Second Conference on Machine
Translation, pages 148–156. Association for Com-
putational Linguistics.

Tobias Domhan and Felix Hieber. 2017. Using target-
side monolingual data for neural machine translation
through multi-task learning. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing, pages 1500–1505. Associa-
tion for Computational Linguistics.

Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun
Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2015. On us-
ing monolingual corpora in neural machine transla-
tion. arXiv preprint arXiv:1503.03535.

Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun
Cho, and Yoshua Bengio. 2017. On integrating
a language model into neural machine translation.
Computer Speech & Language, 45:137 – 148.

Sébastien Jean, Orhan Firat, Kyunghyun Cho, Roland
Memisevic, and Yoshua Bengio. 2015. Montreal

neural machine translation systems for WMT’15. In
Proceedings of the Tenth Workshop on Statistical
Machine Translation, pages 134–140. Association
for Computational Linguistics.

Philipp Koehn. 2009. Statistical machine translation.
Cambridge University Press.

Guillaume Lample, Ludovic Denoyer, and
Marc’Aurelio Ranzato. 2017. Unsupervised
machine translation using monolingual corpora
only. arXiv preprint arXiv:1711.00043.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015a. Effective approaches to attention-
based neural machine translation. In Proceedings of
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1412–1421. Asso-
ciation for Computational Linguistics.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015b. Effective approaches to attention-
based neural machine translation. In Proceedings of
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1412–1421. Asso-
ciation for Computational Linguistics.

Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan
Černockỳ, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In
Eleventh Annual Conference of the International
Speech Communication Association.

Alberto Poncelas, Dimitar Shterionov, Andy Way,
Gideon Maillette de Buy Wenniger, and Pey-
man Passban. 2018. Investigating backtransla-
tion in neural machine translation. arXiv preprint
arXiv:1804.06189.

Matt Post. 2018. A call for clarity in reporting BLEU
scores. arXiv preprint arXiv:1804.08771.

Prajit Ramachandran, Peter Liu, and Quoc Le. 2017.
Unsupervised pretraining for sequence to sequence
learning. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 383–391. Association for Computational
Linguistics.

Holger Schwenk. 2008. Investigations on large-scale
lightly-supervised training for statistical machine
translation. In In IWSLT, pages 182–189.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving neural machine translation mod-
els with monolingual data. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
86–96. Association for Computational Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715–
1725. Association for Computational Linguistics.

210



Ivan Skorokhodov, Anton Rykachevskiy, Dmitry
Emelyanenko, Sergey Slotin, and Anton Ponkratov.
2018. Semi-supervised neural machine translation
with language models. In Proceedings of the AMTA
2018 Workshop on Technologies for MT of Low Re-
source Languages (LoResMT 2018), pages 37–44.
Association for Machine Translation in the Ameri-
cas.

Anuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and
Adam Coates. 2017. Cold fusion: Training seq2seq
models together with language models. arXiv
preprint arXiv:1708.06426.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of the 27th International
Conference on Neural Information Processing Sys-
tems - Volume 2, NIPS’14, pages 3104–3112, Cam-
bridge, MA, USA. MIT Press.

Zhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu,
and Hang Li. 2017. Neural machine translation with
reconstruction.

Yuguang Wang, Shanbo Cheng, Liyang Jiang, Jiajun
Yang, Wei Chen, Muze Li, Lin Shi, Yanfeng Wang,
and Hongtao Yang. 2017. Sogou neural machine
translation systems for WMT17. In Proceedings
of the Second Conference on Machine Translation,
pages 410–415. Association for Computational Lin-
guistics.

Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ing source-side monolingual data in neural machine
translation. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1535–1545. Association for Compu-
tational Linguistics.

211


