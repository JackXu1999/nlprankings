Estimating User Interest from Open-Domain Dialogue

Michimasa Inaba

Kenichi Takahashi

Hiroshima City University

3-4-1 Ozukahigashi, Asaminami-ku, Hiroshima, Japan
finaba, takahashig@hiroshima-cu.ac.jp

Abstract

Dialogue personalization is an important
issue in the ﬁeld of open-domain chat-
oriented dialogue systems.
If these sys-
tems could consider their users’ interests,
user engagement and satisfaction would
be greatly improved. This paper proposes
a neural network-based method for esti-
mating users’ interests from their utter-
ances in chat dialogues to personalize di-
alogue systems’ responses. We introduce
a method for effectively extracting topics
and user interests from utterances and also
propose a pre-training approach that in-
creases learning efﬁciency. Our experi-
mental results indicate that the proposed
model can estimate user’s interest more
accurately than baseline approaches.

1

Introduction

Chat is a very important part of human com-
munication.
In fact, it has been reported that
it makes up about 62% of all conversations
(Koiso et al., 2016). Since chat is also impor-
tant for human-to-machine communication, stud-
ies of dialogue systems that aim to enable open-
domain chat have received much attention in re-
cent years (Ritter et al., 2011; Higashinaka et al.,
2014; Sordoni et al., 2015; Vinyals and Le, 2015;
Zhao et al., 2017). In these studies, dialogue per-
sonalization is an important issue: if such systems
could consider users’ experiences and interests
when engaging them in a conversation, it would
greatly improve user satisfaction. To this end, Hi-
rano et al. extracted predicate-argument structures
(Hirano et al., 2015), Zhang and Chai focused on
conversational entailment (Zhang and Chai, 2009,
2010) and Bang et al. extracted entity relation-
ships (Bang et al., 2015). These studies aimed to

employ users’ utterance histories to generate per-
sonalized responses.

In contrast, this study aims to estimate the user’s
interest in particular topics (e.g., music, fashion,
or health) to personalize the dialogue system’s re-
sponses based on these interests. This would allow
it to focus on topics the user is interested in and
avoid topics they dislike, enhancing user engage-
ment and satisfaction.

This paper therefore proposes a neural network-
based method for estimating users’ interests using
their utterances in chat dialogues. Our method es-
timates their levels of interest not only in topics
that appear in the dialogues, but also in other top-
ics that have not appeared. Even if a user enjoys
talking about the current topic, they will get bored
if the system talks about it endlessly. By gauging
the user’s potential interest in topics that have not
directly appeared in the dialogue, the system can
expand the discussion to other topics before the
user gets bored.

In this study, we use data from human-to-
human dialogues because the current performance
of chat-oriented dialogue systems is not sufﬁcient
for them to talk with humans naturally. We also
use textual dialogue data to avoid speech recog-
nition issues.
In addition, to estimate the target
user’s interests independently of the dialogue sys-
tem’s utterances, we only consider their own utter-
ances and ignore those of their dialogue partner.

This paper brings three main contributions, as
follows.
1. We propose a topic-speciﬁc sen-
tence attention approach that enables topics and
user interests to be efﬁciently extracted from ut-
terances. 2. We develop a method for pre-training
our model’s utterance encoder, so it learns what
topics are related to each target user’s utterance.
3. We show experimentally that the proposed sen-
tence attention and pre-training methods can pro-
vide high performance when used together.

32

Melbourne, Australia, 12-14 July 2018. c(cid:13)2018 Association for Computational Linguistics

Proceedings of the SIGDIAL 2018 Conference, pages 32–40,

2 Related Work

Many studies related to estimating user interest
from text data have targeted social network
services (SNS), especially Twitter users.
For
proposed a method of
example, Chen et al.
modeling interest using the
frequencies of
words in tweets by the target user and followers
(Chen et al., 2010). Some methods have also been
proposed that consider superordinate concepts
acquired from knowledge bases. For example,
Abel et al. modeled Twitter users using the
appearance frequencies of certain named entities
(e.g., people, events, or music groups), acquired
using OpenCalais 1 (Abel et al., 2011).
In addi-
tion, some methods have used categories from
Wikipedia
2010;
Kapanipathi et al., 2014; Zarrinkalam et al., 2015)
or DBPedia (Kapanipathi et al., 2011). Several
methods have also been proposed that use topic
models, such as latent dirichlet allocation (LDA)
(Weng et al., 2010; Bhattacharya et al., 2014;
Han and Lee, 2016). However, it is difﬁcult to
apply such methods directly to dialogue because
they assume that users are posting about subjects
they are interested in.
This is a reasonable
assumption for SNS data, but in conversations,
people do not always limit themselves to topics
they are interested in. For instance, people will
play along and discuss subjects the other persons
are interested in, even if they are not interested in
them, as well.

(Michelson and Macskassy,

Other studies have attempted to estimate users’
levels of interest (LOI) from dialogues. Schuller
et al. tackled the task of estimating listeners’ in-
terest in a product from dialogues between them
and someone introducing a particular product,
proposing a support vector machine (SVM)-based
method incorporating acoustic and linguistic fea-
tures (Schuller et al., 2006)．In 2010, LOI estima-
tion was selected as a sub-challenge of the INTER-
SPEECH Paralinguistic Challenge (Schuller et al.,
2006), but there the focus was on single-topic
(product) interest estimation from spoken dia-
logue, not open-domain estimation.
In addition,
that task considered business dialogues, not chats.

3 Model Architecture

The task considered in this paper is as follows.
Given an utterance set Us = (u1; u2; :::; un) ut-

1http://www.opencalais.com/

Figure 1: Overview of the proposed interest esti-
mation model.

tered by a speaker s during dialogues with other
speakers, we estimate their degrees of interest
Ys = (y1; y2; :::; ym) in topics in a given topic
set T = (t1; t2; :::; tm). Here, the ti correspond
to concrete topics, such as movies or travel while
yi indicates the speaker’s level of interest in ti, on
the three-point scale used for the LOI estimation
task described in the previous section. Using this
scale, the yi can take the values 0 (disinterest, in-
difference and neutrality), 1 (light interest), or 2
(strong interest).

To accurately gauge the speaker’s interest from
their utterances, we believe it is important to ex-
tract the following two types of information efﬁ-
ciently.

(cid:15) The topic of each utterance
(cid:15) How interested the speaker is in the topic

Our proposed interest estimation model extracts
this information efﬁciently and uses a pre-training
method to improve learning. Figure 1 presents
an overview of our neural network model, which
ﬁrst encodes the word sequence, applies word at-
tention and topic-speciﬁc sentence attention, and
ﬁnally estimates the degrees of interest Ds =
(dt1; dt2; :::; dtm).
The proposed pre-training
method is used for the word sequence encoder.
The model is described in detail below.

33

3.1 Word Sequence Encoder
The word sequence encoder converts utterances
into ﬁxed-length vectors using a recurrent neural
network (RNN). First, the words in each utterance
are converted into word vectors using Word2vec
(Mikolov et al., 2013), giving word vector se-
quences x = (x1; x2; :::; xl). The RNN encoder
uses a hidden bidirectional-GRU (BiGRU) layer,
which consists of a forward GRU that reads from
x1 to xl in order and a backward GRU that reads
(cid:0)!
from xl to x1 in reverse order. The forward GRU
hi as follows.
computes the forward hidden states
(cid:0)(cid:0)!
hi(cid:0)1)

(cid:0)(cid:0)(cid:0)!
GRU (xi;

(cid:0)!
hi =

(1)

 (cid:0)
The backward GRU calculates the backward
hidden states
hi in a similar way. By combining
the outputs of both GRUs, we obtain the objective
hidden state hi:

(cid:0)!
hi :

 (cid:0)
hi ]

hi = [

(2)

where [:] represents vector concatenation.

3.2 Topic Classiﬁcation Pre-Training
Estimating the user’s level of interest in each topic
requires ﬁrst assessing the topic of each utterance.
Since this is not given explicitly, the model must
infer this information from the utterance set and
degrees of interest in each topic, so the learning
difﬁculty is high. In this study, based on the idea
of pre-training (Erhan et al., 2010), we introduce
a new pre-training method for the sentence topic
classiﬁcation task to the word sequence encoder.
The important point to note about this task is that
the topic classes involved are identical to those in
the topic set Ys. This helps to reduce the difﬁculty
of learning to estimate the relationships between
utterances and topics and allows the model to fo-
cus on interest estimation during the main training
phase.

During pre-training, the classiﬁcation probabil-
ity p for each topic is calculated as follows, based
on the output hl of the BiGRU after inputting the
last word vector xl. (Word attention, as described
in the next section, is not used in pre-training.)

p = sof tmax(Wchl + bc)

(3)

where Wc and bc are parameters for topic classiﬁ-
cation. The cross-entropy is used as the loss func-
tion during pre-training.

3.3 Word Attention
Based on an idea from Yang et al., we also in-
cluded word attention in our model (Yang et al.,
2016). Word attention is based on the idea that all
words do not contribute equally to the desired re-
sult and involves using an attention mechanism to
weight each word differently. The resulting utter-
ance vector z is obtained as follows.

vhi = tanh(W!hi + b!)

(cid:11)hi =

∑
∑

hiv!)

exp(vT
i exp(vT

hiv!)

z =

(cid:11)hihi

i

(4)

(5)

(6)

where W! and b! are parameters. Unlike the orig-
inal attention mechanism used in neural transla-
tion (Bahdanau et al., 2015) and neural dialogue
(Shang et al., 2015) models, the word attention
mechanism uses a common parameter, called con-
text vector v! to calculate weight (cid:11)i for each hid-
den state. v! is a high-level representation for
calculating word importance and, like the model’s
other parameters, is randomly initialized and then
optimized.

3.4 Topic Speciﬁc Sentence Attention
Our model uses a word sequence encoder with
word attention to convert the utterance set Us =
(u1; u2; :::; un) into the utterance vector set Zs =
(z1; z2; :::; zn). It then extracts information for es-
timating the level of interest in each topic from Zs,
but, as with word attention, not all utterances con-
tribute equally. Yang et al. proposed a sentence
attention mechanism that takes the same approach
as for word attention, but, since it uses only one
parameter to calculate sentence importance (simi-
lar to the context vector v! for word attention), it
is not capable of topic-speciﬁc estimation. This is
because the important utterances in a given utter-
ance set differ from topic to topic. For example, “I
jog every morning” is probably useful for estimat-
ing interest in topics, such as sports or health, but
not in, say, computers or vehicles.

In this study, we therefore propose a new topic-
speciﬁc sentence attention approach. The topic
vector vti represents the importance of each sen-
tence for topic ti, and the associated content vector
cti is calculated as follows.

vj = tanh(Wrzj + br)

(7)

34

(cid:11)jti =

∑
∑

j vti)

exp(vT
j exp(vT

j vti)

cti =

(cid:11)jtizj

j

(8)

(9)

Here, Wr and br are shared, topic-independent pa-
rameters. The topic vector vti is randomly initial-
ized and then optimized during training.

3.5 Interest Estimation
We then use the content vector cjti to compute the
degree of interest dti in topic ti as follows.

dti = tanh(Wticti + bti) + 1

(10)

Here, the parameters Wti and bti estimate the over-
all degree of interest in the topics ti, and it is dif-
ferent for each topic after optimization. Finally,
one is added, so that dti uses the same 0 to 2 range
as the correct values yi.

During training, we use the mean squared error
(MSE) between the correct answer yi and dti as
the loss function:

(yi (cid:0) dti)2

(11)

n∑

i

L =

1
n

4 Experiments

We conducted a series of experiments to evaluate
the proposed method’s performance. For these,
we created a dataset based on logs of one-to-one
text chats between human subjects and the results
of questionnaires answered by each subject. We
also tested several baseline methods for compari-
son purposes.

4.1 Datasets
We asked each subject to ﬁrst ﬁll out a question-
naire about their interests and then engage in text
chats in Japanese with partners they had not previ-
ously been acquainted with. We recruited 163 sub-
jects via the CrowdWorks2 crowd-sourcing site.
The subjects were asked to rate their levels of in-
terest in the 24 topic categories shown in Table
1 using a three-point scale discussed in Section
3. These topics were selected based on the cate-
gories used by Yahoo! Chiebukuro3, a Japanese
question-and-answer site, focusing on topics that
are likely to appear in one-to-one dialogues be-
tween strangers.

2https://crowdworks.jp/
3https://chiebukuro.yahoo.co.jp/

Table 1: Topic Categories

Celebrities

Movies
Reading
Computers

Travel
Anime / Manga
Music
Home Appliances
Games
Sports / Exercise
Beauty
Health
Outdoor Activities
Housing Housekeeping Marriage / Love
Animals
Cooking / Meal
Vehicles

Politics / Economy

Fashion
School

Family
History

Table 2: Example dialogue (translated by authors)

A 対話を開始します。よろしくお願いしま

す。
Let’s start a conversation. Nice to meet
you.

B はい、よろしくお願いします。

Hi, nice to meet you.

A 何かご趣味はありますか？

What are your hobbies?

B 最近はペット中心の生活になっているの

でペットが趣味になりますね。
Currently,
lifestyle. So, raising pets is my hobby.

I am living a pet-centered

A 何を飼ってらっしゃるのですか？

Which pets do you have?

B 猫を飼っています。３匹いるのでにぎや

かですよ。
I have three cats and they are lively.

A 3 匹ですか、いいですね！雑種ですか？
Three cats. That sounds great! Are they
mixed breed?

B はい、全部雑種です。手がかからなくて
楽ですね。何か動物は飼っていますか？
Yes, they are all mixed breed cats. They
are low-maintenance and easy to keep. Do
you have any animals?

Each dialogue lasted for one hour and was con-
ducted via Skype instant messaging. We only in-
structed the subjects to “Please try to ﬁnd things
you and your partner are both interested in and
then try to broaden your conversation about these
subjects.” We gave the subjects no speciﬁc instruc-
tions as to the intended content or topics of their
conversations. Table 2 shows an example dialogue
between subject A and B.

All the utterances in the chat data were then
classiﬁed by subject. Each data point consisted of
all the data about one subject, namely their chat ut-

35

Table 3: Data Statistics

Number of users (data points)
Number of dialogues
Number of utterances
Avg. number of strong interest topics
Avg. number of light interest topics
Avg. number of neutral topics

163
408
49029
11.48
7.30
5.21

terances and questionnaire responses (correspond
to Us and Ys deﬁned in Section 3). The data was
evaluated using 10-fold cross-validation and their
statistics are shown in Table 3.

4.2 Settings
Word2Vec (Mikolov et al., 2013) was trained us-
ing 100 GB of Twitter data with 200 embedding
cells, a minimum word frequency of 10, and a
skip-gram window size of 5.

The word sequence encoder was a single-layer
BiGRU RNN with 200 input cells and 400 out-
put cells. The word and sentence attention layers
had 400 input and output cells while the estimation
layer had 400 input cells and 1 output cell. The
model was trained using Adam (Kingma and Ba,
2015).

During pre-training, we used questions and an-
swers from the Yahoo! Chiebukuro Data (2nd edi-
tion))4) for each topic. All topics were equally
covered, and a total of 770k sentences were used
for training while 2400 sentences (100 for each
topic) were used for testing. After pre-training,
the topic classiﬁcation accuracy for the test data
was 0.755.

4.3 Evaluation
When using the proposed method as part of a dia-
logue system, it is effective to select the best topic
from those available for the system to generate an
appropriate response. Therefore, in this experi-
ment, each topic was ranked based on the esti-
mated degree of interest dti, and the methods were
evaluated based on whether the topics the user was
interested in should have been ranked higher or the
other topics ranked lower. The rankings were eval-
uated using the normalized discounted cumulative
gain (N DCG), a widely used metric in the ﬁeld of
information retrieval. This gives values between 0
and 1, with higher values indicating more accurate

4http://www.nii.ac.jp/dsc/idr/yahoo/chiebkr2/Y chiebuku

ro.html

ranking predictions and is calculated as follows.

N DCG@k =

DCGk = reli +

i=2

DCGk
IDCGk

K∑

reli
log2i

(12)

(13)

Here, k is the number of top-ranked objects used
for the N DCG calculation, and reli is the graded
relevance of the result at position i, which was
given by the degree of interest Ys in this experi-
ment. The ideal DCG (IDCG) is the DCG if
the ranking list had been correctly ordered by rel-
evance.

In addition, to evaluate the accuracy of the es-
timated degrees of interest in each topic, we also
calculated the MSEs between the results of each
method and the correct answers.

4.4 Baseline Methods
To evaluate the proposed model, we also con-
ducted experiments using the following three
modiﬁed models.

Without Pre-Training 　

To evaluate the effectiveness of topic clas-
siﬁcation pre-training, we tested our model
without this step. Instead, the word sequence
encoder was randomly initialized and then
trained. This model was otherwise identical
to the proposed method.

Without Sentence Attention 　

To evaluate the effectiveness of topic-speciﬁc
sentence attention, we tried instead using
max-pooling to obtain the content vector.
Again, this model was identical to the pro-
posed method.

Without Pre-Training or Sentence Attention

This model combined the two modiﬁcations
mentioned above: it did not use topic classi-
ﬁcation pre-training and used max-pooling to
obtain the content vectors, but was otherwise
identical to the proposed method.

We also compared our model’s performance to

those of the following two baseline methods.

Topic Frequency 　

The ﬁrst baseline was based on a method,
proposed by Abel et al., that identiﬁes the
named entities (such as people, events, or

36

4.5 Results

Figure 2 shows the N DCG results for the top-
ics ranked in the top k. These indicate that the
proposed method performed better than the other
methods for all values of k. Comparing the per-
formances of the methods that used pre-training
(“Proposed” and “Without Sentence Attention”)
with those of the ones that did not (“Without
Pre-Training” and “Without Pre-Training or Sen-
tence Attention”) indicates that the proposed pre-
training step was effective. On the other hand,
a method that used sentence attention (“Without
Pre-Training”) showed nearly the same results as
one that did not (“Without Pre-Training or Sen-
tence Attention”), although the latter did achieve
higher NDCGs for k (cid:21) 5. This indicates that
using sentence attention alone does not improve
performance. However, the proposed method per-
formed better than the method without sentence at-
tention, conﬁrming that sentence attention is use-
ful, but only if it is used in conjunction with pre-
training.

Turning now to the SVR-based methods, we
observe that using only unigram features worked
better than using both unigrams and bigrams, al-
though both methods were still inferior to the
neural network-based methods, including the pro-
posed method.

When k = 1,

the topic frequency baseline
achieved higher N DCGs than the SVR-based
methods, because it correctly noted that users were
strongly interested in the topics they spoke about
most frequently. However, these results were still
inferior to those of the neural network-based meth-
ods. Furthermore, it presented the worst N DCG
results among all the methods for k (cid:21) 4, due
to speakers sometimes talking about subjects they
were not interested in, as discussed in Section 2.

Table 4 shows the MSEs between the degree of
interest results for each method and the correct
answers (excluding Topic Frequency, which can-
not output the degree of interest). The proposed
method gave signiﬁcantly smaller MSE value, in-
dicating that its estimates were the most accurate.
In addition, the “Without Pre-Training” method
showed the lowest performance of all the neural
network-based methods, also indicating that the
proposed sentence attention approach is not effec-
tive without also using pre-training.

Figure 2: NDCG@k results for all methods, for k
between 1 and 24.

Table 4: Mean Squared Error

Proposed
Without Pre-Training
Without Sentence Attention
Without Pre-Training or
Sentence Attention
SVR (unigram)
SVR (unigram + bigram)

0.533
0.580
0.561
0.568

0.597
0.611

music groups) associated with words in the
user’s tweets using OpenCalais and models
the user’s interests using a named entity fre-
quency vector (Abel et al., 2011). However,
as we used Japanese dialogues, we could not
use OpenCalais, so we instead used the topic
classiﬁer described in Section 3.2. Since this
classiﬁer is trained for classiﬁcation for sen-
tences and not for words, we employed sen-
tence level topic frequency. The topic fre-
quency was used to gauge the user’s interest,
and the topics were ranked in frequency or-
der.

SVR 　

The second baseline method used support
vector regression (SVR) to estimate the de-
gree of interest. We conducted experiments
using only unigrams, and using both uni-
grams and bigrams. We used the RBF ker-
nel function. The SVR models were trained
for each topic individually and then used to
estimate the degrees of interest.

37

Figure 3: Visualization of Attention (Proposed)

Figure 4: Visualization of Attention (Without Pre-Training)

4.6 Discussion

The experimental results discussed in the previous
section indicate that it is important to use the pro-
posed pre-training and sentence attention steps to-
gether. To analyze the sentence attention mecha-
nism further, we visualized the sentence weights
(cid:11)jti given by equation (8) for selected topics and
utterances. Figures 3 and 4 show the sentence

weights with and without pre-training, respec-
tively. Here, darker cells indicate higher (cid:11)jti val-
ues.

Figure 3 shows that the sentence weights for the
topics corresponding to the actual meaning of the
sentence are high.
(1), (2) and (3) are easy-to-
understand examples. The topics related to each
utterance take the highest weights.
In addition,

38

utterance (4) includes sports-related words, such
as “baseball” and “rule”, but the weight of the
“Sports / Exercise” topic is not high because the
utterance did not indicate such an interest on the
part of the speaker. Thus, the sentence weights
do not simply reﬂect the topics of the words, but
also the user’s level of interest in the topic.
In-
terestingly, although the utterance (6) refers to the
smartphone game “Pokemon GO”, the weight of
the “Game” topic is not very high, but those of
the “Sports/Exercise” and “Health” topics are both
high. Pokemon GO is interesting to people who
do not usually play games, and this appears to be
reﬂected in the results. On the other hand, utter-
ance (7) shows high weights for several topics that
intuitively appear to be unrelated to the utterance
itself.

The sentence weights shown in Figure 4 often
do not correspond to the topics or meanings of
the utterances. For example, utterance (5) is not
important for interest estimation and its weights
in Figure 3 are small. However, in Figure 4, all
weights are relatively high. Similarly, utterances
(7) and (8) show high weights for unrelated top-
ics.

The above results conﬁrm that the pre-training
step is important for learning the topic-speciﬁc
sentence attention correctly. Without pre-training,
the model must learn the relationships between
utterances and topics by starting from a clean
slate, and the difﬁculty of this task makes harder
to determine the appropriate results. The ex-
perimental results in the previous section show
that pre-training makes this task easier and im-
proves performance. With proper training, topic-
speciﬁc sentence attention then enabled the pro-
posed method to achieve the best performance.

5 Conclusion

In this paper, we have presented a neural network-
based method for estimating users’ levels of in-
terest in a pre-determined list of topics based on
their utterances in chat dialogues. The proposed
method ﬁrst encodes utterances by using BiGRU
and considering word attention, a set of utterance
vectors was obtained. It then uses these to gener-
ate content vectors corresponding to each topic via
topic-speciﬁc sentence attention. Finally, it uses
the content vectors to estimate the user’s degree
of interest in each topic. The utterance encoder
is pre-trained to classify sentences by topic before

the whole model is trained. Our experimental re-
sults showed that the proposed method can esti-
mate degrees of interest in topics more accurately
than baseline methods. In addition, we found that
it was most effecting to use topic-speciﬁc sentence
attention and topic classiﬁcation pre-training in
combination.

In future work, we plan to apply the proposed
method to a dialogue system and conduct dialogue
experiments with human users. Even if we can es-
timate which topics a user is interested in, gener-
ating and selecting concrete utterances remains a
challenging problem. For example, users who are
interested in sports are not equally interested in all
of them: someone may be interested in football
but not in golf, for instance. We therefore plan to
develop an appropriate way of incorporating the
proposed method into such a system.

Acknowledgements

The Yahoo! Chiebukuro Data (2nd edition) pro-
vided to National Institute of Informatics by Ya-
hoo Japan Corporation was used in this study.

This study received a grant of JSPS Grants-in-

aid for Scientiﬁc Research 16H05880.

References
Fabian Abel, Qi Gao, Geert-Jan Houben, and Ke Tao.
2011. Analyzing user modeling on twitter for per-
sonalized news recommendations. User Modeling,
Adaption and Personalization pages 1–12.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. Proc. ICLR .

Jeesoo Bang, Hyungjong Noh, Yonghee Kim, and
Gary Geunbae Lee. 2015. Example-based chat-
oriented dialogue system with personalized long-
term memory. In IEEE International Conference on
Big Data and Smart Computing (BigComp). pages
238–243.

Parantapa Bhattacharya, Muhammad Bilal Zafar, Niloy
Ganguly, Saptarshi Ghosh, and Krishna P Gummadi.
2014.
Inferring user interests in the twitter social
network. In Proceedings of the 8th ACM Conference
on Recommender systems. ACM, pages 357–360.

Jilin Chen, Rowan Nairn, Les Nelson, Michael Bern-
stein, and Ed Chi. 2010. Short and tweet: experi-
ments on recommending content from information
streams. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems. ACM,
pages 1185–1194.

39

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural Responding Machine for Short Text Conver-
sation. Proceedings of the 53th Annual Meeting of
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing pages 1577–1586.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. In Proceedings
of NAACL-HLT.

Oriol Vinyals and Quoc Le. 2015. A neural conver-
sational model. In Proceedings of the ICML Deep
Learning Workshop. pages 1–7.

Jianshu Weng, Ee-Peng Lim, Jing Jiang, and Qi He.
2010. Twitterrank: ﬁnding topic-sensitive inﬂuen-
tial twitterers. In Proceedings of the third ACM in-
ternational conference on Web search and data min-
ing. ACM, pages 261–270.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alexander J Smola, and Eduard H Hovy. 2016. Hi-
erarchical attention networks for document classiﬁ-
cation. In HLT-NAACL. pages 1480–1489.

and Weichang Du.

Fattane Zarrinkalam, Hossein Fani, Ebrahim Bagheri,
2015.
Mohsen Kahani,
interest detection from
Semantics-enabled user
In Web Intelligence and Intelligent Agent
twitter.
Technology (WI-IAT), 2015 IEEE/WIC/ACM Inter-
national Conference on. IEEE, volume 1, pages
469–476.

Chen Zhang and Joyce Y Chai. 2009. What do we
know about conversation participants: Experiments
In Proceedings of the
on conversation entailment.
SIGDIAL 2009 Conference. pages 206–215.

Chen Zhang and Joyce Y Chai. 2010. Towards con-
versation entailment: An empirical investigation.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing. pages
756–766.

Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi.
2017. Learning discourse-level diversity for neural
dialog models using conditional variational autoen-
In Proceedings of the 55th Annual Meet-
coders.
ing of the Association for Computational Linguistics
(Volume 1: Long Papers). Association for Computa-
tional Linguistics, pages 654–664.

Dumitru Erhan, Yoshua Bengio, Aaron Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? Journal of Machine Learning
Research 11(Feb):625–660.

Jonghyun Han and Hyunju Lee. 2016. Characterizing
the interests of social media users: Reﬁnement of a
topic model for incorporating heterogeneous media.
Information Sciences 358:112–128.

Ryuichiro Higashinaka, Kenji Imamura, Toyomi Me-
guro, Chiaki Miyazaki, Nozomi Kobayashi, Hiroaki
Sugiyama, Toru Hirano, Toshiro Makino, and Yoshi-
hiro Matsuo. 2014. Towards an open-domain con-
versational system fully based on natural language
processing. In COLING. pages 928–939.

Toru Hirano, Nozomi Kobayashi, Ryuichiro Hi-
gashinaka, Toshiro Makino, and Yoshihiro Matsuo.
2015. User information extraction for personal-
ized dialogue systems. The 19th Workshop on the
Semantics and Pragmatics of Dialogue (SemDial)
pages 67–75.

Pavan Kapanipathi, Prateek Jain, Chitra Venkatara-
mani, and Amit Sheth. 2014. User interests iden-
tiﬁcation on twitter using a hierarchical knowl-
edge base. In European Semantic Web Conference.
Springer, pages 99–113.

Pavan Kapanipathi, Fabrizio Orlandi, Amit P Sheth,
and Alexandre Passant. 2011. Personalized ﬁltering
of the twitter stream pages 6–13.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. Proceedings of
the 3rd International Conference on Learning Rep-
resentations .

Hanae Koiso, Tomoyuki Tsuchiya, Ryoko Watanabe,
Daisuke Yokomori, Masao Aizawa, and Yasuharu
Den. 2016. Survey of conversational behavior: To-
wards the design of a balanced corpus of everyday
japanese conversation. In LREC 2016.

Matthew Michelson and Sofus A Macskassy. 2010.
Discovering users’ topics of interest on twitter: a
In Proceedings of the fourth workshop
ﬁrst look.
on Analytics for noisy unstructured text data. ACM,
pages 73–80.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems. pages 3111–3119.

Alan Ritter, Colin Cherry, and William B Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the conference on empirical methods
in natural language processing. pages 583–593.

Bj¨orn W Schuller, Niels K¨ohler, Ronald M¨uller, and
Gerhard Rigoll. 2006. Recognition of interest in hu-
man conversational speech. In INTERSPEECH.

40

