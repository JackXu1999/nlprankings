



















































Modelling the Interpretation of Discourse Connectives by Bayesian Pragmatics


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 531–536,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Modeling the Interpretation of Discourse Connectives
by Bayesian Pragmatics

Frances Yung
Nara Institute of Science and Technology

8916-5 Takayama, Ikoma,
Nara, 630-0101, Japan

pikyufrances-y@is.naist.jp

Kevin Duh
John Hopkins University
810 Wyman Park Drive,

Baltimore, MD 21211-2840, USA
kevinduh@cs.jhu.edu

Taku Komura
University of Edinburgh

10 Crichton Street,
Edinburgh, EH8 9AB, United Kingdom

tkomura@inf.ed.ac.uk

Yuji Matsumoto
Nara Institute of Science and Technology

8916-5 Takayama, Ikoma,
Nara, 630-0101, Japan
matsu@is.naist.jp

Abstract

We propose a framework to model hu-
man comprehension of discourse connec-
tives. Following the Bayesian pragmatic
paradigm, we advocate that discourse con-
nectives are interpreted based on a sim-
ulation of the production process by the
speaker, who, in turn, considers the ease of
interpretation for the listener when choos-
ing connectives. Evaluation against the
sense annotation of the Penn Discourse
Treebank confirms the superiority of the
model over literal comprehension. A fur-
ther experiment demonstrates that the pro-
posed model also improves automatic dis-
course parsing.

1 Introduction

A growing body of evidence shows that human
interpretation and production of natural language
are inter-related (Clark, 1996; Pickering and Gar-
rod, 2007; Zeevat, 2011; Zeevat, 2015). In par-
ticular, evidence shows that during interpretation,
listeners simulate how the utterance is produced;
and during language production, speakers simu-
late how the utterance will be perceived. One
explanation is that the human brain reasons by
Bayesian inference (Doya, 2007; Kilner et al.,
2007), which is, at the same time, a popular for-
mulation used in language technology.

In this work, we model how humans interpret
the sense of a discourse relation based on the
Bayesian pragmatic framework. Discourse rela-
tions are relations between units of texts that make
a document coherent. These relations are either

marked by discourse connectives (DCs), such as
‘but’, ‘as a result’, or implied implicitly, as in the
following examples:

1. He came late. In fact, he came at noon.

2. It is late. I will go to bed.

The explicit DC ‘in fact’ in Example (1) marks a
Specification relation. On the other hand, a Result
relation can be inferred between the two sentences
in Example (2) although there are not any explicit
markers. We say the two sentences (called argu-
ments) are connected by an implicit DC.

Discourse relations have a mixture of semantic
and pragmatic properties (Van Dijk, 1980; Lewis,
2006). For example, the sense of a discourse rela-
tion is encoded in the semantics of a DC (Exam-
ple (1)), yet the interpretation of polysemic DCs
(such as ‘since’, ‘as’) and implicit DCs relies on
the pragmatic context (Example (2)).

This work seeks to find out if Bayesian prag-
matic approaches are applicable to human com-
prehension of discourse relations. Our contribu-
tion includes: (i) an adaptation of the Bayesian
Rational Speech Acts model to DC interpretation
using a discourse-annotated corpus, the Penn Dis-
course Treebank; (ii) integration of the proposed
model with a state-of-the-art automatic discourse
parser to improve discourse sense classification.

2 Related work

There is increasing literature arguing that the hu-
man motor control and sensory systems make es-
timations based on a Bayesian perspective (Doya,
2007; Oaksford and Chater, 2009). For example, it
is proposed that the brain’s mirror neuron system

531



recognizes a perceptual input by Bayesian infer-
ence (Kilner et al., 2007). Similarly, behavioural,
physiological and neurocognitive evidences sup-
port that the human brain reasons about the uncer-
tainty in natural languages comprehension by em-
ulating the language production processes (Galan-
tucci et al., 2006; Pickering and Garrod, 2013).

Analogous to this principle of Bayesian lan-
guage perception, a series of studies have devel-
oped the Grice’s Maxims (Grice, 1975) based on
game-theoretic approaches (Jäger, 2012; Frank
and Goodman, 2012; Goodman and Stuhlmüller,
2013; Goodman and Lassiter, 2014; Benz et al.,
2016). These proposals argue that the speaker and
the listener cooperate in a conversation by recur-
sively inferring the reasoning of each other in a
Bayesian manner. The proposed framework suc-
cessfully explains existing psycholinguistic theo-
ries and predict experimental results at various lin-
guistic levels, such as the perception of scalar im-
plicatures (e.g. ‘some’ meaning ‘not all’ in prag-
matic usage) and the production of referring ex-
pressions (Lassiter and Goodman, 2013; Bergen
et al., 2014; Kao et al., 2014; Potts et al., 2015;
Lassiter and Goodman, 2015). Recent efforts also
acquire and evaluate the models using corpus data
(Orita et al., 2015; Monroe and Potts, 2015).

Production and interpretation of discourse re-
lations is also a kind of cooperative communica-
tion between speakers and listeners (or authors and
readers). We hypothesize that the game-theoretic
account of Bayesian pragmatics also applies to
human comprehension of the meaning of a DC,
which can be ambiguous or even dropped.

3 Method

This section explains how we model the interpre-
tation of discourse relations by Bayesian pragmat-
ics. The model is based on the formal framework
known as Rational Speech Acts model (Frank and
Goodman, 2012; Lassiter and Goodman, 2015).
Section 3.1 explains the key elements of the RSA
model, and Section 3.2 illustrates how it is adapted
for discourse interpretation.

3.1 The Rational Speech Acts model

The Rational Speech Acts (RSA) model describes
the speaker and listener as rational agents who
cooperate towards efficient communication. It is
composed of a speaker model and a listener model.

In the speaker model, the utility function U de-

fines the effectiveness for the speaker to use utter-
ance d to express the meaning s in context C.

U(d; s, C) = lnPL(s|d,C)− cost(u) (1)

PL(s|d,C) is the probability that the listener
can interpret speaker’s intended meaning s. The
speaker selects an utterance which, s/he thinks,
is informative to the listener. The utility of d is
thus defined by its informativeness towards the in-
tended interpretation, which is quantified by nega-
tive surprisal (lnPL(s|d,C)), according to Infor-
mation Theory (Shannon, 1948). The utility is
modified by production cost (cost(d)), which is
related to articulation and retrieval difficulties, etc.
PS(d|s, C) is the probability for the speaker to

use utterance d for meaning s. It is proportional to
the soft-max of the utility of d.

PS(d|s, C) ∝ exp(α · U(d; s, C)) (2)
where α, the decision noise parameter, is set to 1.

On the other hand, the probability for the lis-
tener to infer meaning s from utterance d is de-
fined by Bayes’ rule.

PL(s|d,C) ∝ PS(d|s, C)PL(s) (3)
The listener infers the speaker’s intended mean-

ing by considering how likely, s/he thinks, the
speaker uses that utterance (PS(d|s, C)). The in-
ference is also related to the salience of the mean-
ing (PL(s)), a private preference of the listener.

To summarize, the speaker and listener emulate
the language processing of each other. However,
instead of unlimitted iterations (i.e. the speaker
thinks the listener thinks the speaker thinks..), the
inference is grounded on literal interpretation of
the utterance. Figure 1 illustrates the direction of
pragmatic inference between the speaker and lis-
tener in their minds.

Figure 1: Pragmatic listeners/speakers reason for 1
or more levels, but not the literal listener/speaker.

Our experiment compares the predictions of the
literal listener (L0), the pragmatic listener who

532



reasons for one level (L1), and the pragmatic lis-
tener who reasons for two levels (L2). Previ-
ous works demonstrate that one level of reason-
ing is robust in modeling human’s interpretation of
scalar implicatures (Lassiter and Goodman, 2013;
Goodman and Stuhlmüller, 2013).

3.2 Applying the RSA model on discourse
relation interpretation

We use the listener model of RSA to model how
listeners interpret the sense a DC. Given the DC
d and context C in a text, the listener’s inter-
preted relation sense si is the sense that maximizes
PL(s|d,C). si is specifically defined as

si = arg max
s∈S

PL(s|d,C) (4)

where S is the set of defined relation senses.
The literal listener, L0, interprets a DC directly

by its most likely sense in the context. The proba-
bility is estimated by counting the co-occurrences
in corpus data, the Penn Discourse Treebank, in
which explicit and implicit DCs are labelled with
discourse relation senses.

PL0(s|d,C) =
count(s, d, C)
count(d,C)

(5)

More details about the annotation of PDTB will be
explained in Section 4.1.

As shown in Figure 1, the pragmatic speaker S1
estimates the utility of a DC by emulating the com-
prehension of the literal listener L0 (Eq. 1, 2). The
probability for the pragmatic speaker Sn to use DC
d to express meaning s is estimated as:

PSn(d|s, C)

=
exp(lnPLn−1(s|d,C)− cost(d))∑

d′∈D
exp(lnPLn−1(s|d′, C)− cost(d′))

(6)

where n ≥ 1. D is the set of annotated DCs, in-
cluding ‘null’, which stands for an implicit DC.

The cost function in Equation 6, cost(d), mea-
sures the production effort of the DC. As DCs are
mostly short words, we simply define the cost of
producing any explicit DC by a constant positive
value, which is tuned manually in the experiments.
On the other hand, the production cost for an im-
plicit DC is 0, since no word is produced .

In turn, the pragmatic listener L1 emulates the
DC production of the pragmatic speaker S1 (Eq.

3). The probability for the pragmatic listener Ln
to assign meaning s to DC d is estimated as:

PLn(s|d,C) =
PSn(d|s, C)PL(s)∑

s′∈S
PSn(d|s′, C)PL(s′) (7)

where n ≥ 1 and S is the set of defined sense. The
salience of a relation sense in Equation 7, PL(s), is
defined by the frequency of the sense in the corpus.

PL(s) =
count(s)∑

s′∈S
count(s′)

(8)

Lastly, we propose to define the context vari-
able C by the the immediately previous discourse
relation to resemble incremental processing. We
hypothesize that certain patterns of relation tran-
sitions are more expected and predictable. Dis-
course context in terms of relation sense, relation
form (explicit DC or not), and the sense-form pair
are compared in the experiments.

4 Experiment

This section describes experiments that evaluate
the model against discourse-annotated corpus. We
seek to answer the following questions: (1) Can
the proposed model explain the sense interpreta-
tion (annotation) of the DCs in the corpus? (2)
Is the DC interpretation refined by the context in
terms of previous discourse structure? (3) Does
the proposed model help automatic discourse pars-
ing? We first briefly introduce the corpus resource
we use, the Penn Discourse Treebank.

4.1 Penn Discourse Treebank
The Penn Discourse Treebank (PDTB) (Prasad
et al., 2008) is the largest available discourse-
annotated resource in English. The raw text are
collected from news articles of the Wall Street
Journals. On the PDTB, all explicit DCs are an-
notated with discourse senses, while implicit dis-
course senses are annotated between two adja-
cent sentences. Other forms of discourse relations,
such as ‘entity relations’, are also labeled. In total,
there are 5 form labels and 42 distinct sense labels,
some of which only occur very sparsely.

We thus use a simplified version of the annota-
tion, which has 2 form labels (Explicit and Non-
explicit DC) and 15 sense labels (first column of
Table 3), following the mapping convention of
the CONLL shallow discourse parsing shared task
(Xue et al., 2015). Sections 2-22 are used as the

533



training set and the rest of the corpus, Sections 0,
1, 23 and 24, are combined as the test set. Sizes of
the data sets are summarized in Table 1.

Train Test Total
Explicit 15,402 3,057 18,459
Non-Exp 18,569 3,318 21,887
Total 33,971 6,375 40,346

Table 1: Sample count per data set

4.2 Does RSA explain DC interpretation?
The RSA model argues that a rational listener does
not just stick to the literal meaning of an utter-
ance. S/he should reason about how likely the
speaker will use that utterance, in the current con-
text, based on the informativeness and production
effort of the utterance. If the RSA model explains
DC interpretation as well, discourse sense predic-
tions made by the pragmatic listeners should out-
perform predictions by the literal listener.

In this experiment, we compare the DC inter-
pretation by the literal listener L0, and pragmatic
listeners L1 and L2. Given a DC d and the dis-
course context C for each test instance, the rela-
tion sense is deduced by maximizing the proba-
bility estimate PL(s|d,C). PL0(s|d,C) is simply
based on co-occurrences in the training data (Eq.
5). PL1(s|d,C) and PL2(s|d,C) are calculated by
Eq. 6 and 7, in which the salience of each sense is
also extracted from the training data (Eq. 8).

context C Explicit Non-Explicit
L0 constant (BL) .8767 .2616

prev. form .8754 .2616
prev. sense .8727 .2507
form-sense .8684 .2692

L1 constant .8853* .2616
prev. form .8830 .2616
prev. sense .8671 .2698*
form-sense .8621 .2671

L2 constant .8853* .2616
prev. form .8830 .2616
prev. sense .8671 .2616
form-sense .8621 .2616

Table 2: Accuracy of prediction by L0, L1 and
L2. Improvements above the baseline are bolded.
* means significant at p < 0.02 by McNemar Test.

Table 2 shows the accuracy of discourse sense
prediction by listeners L0, L1 and L2, when pro-
vided with various discourse contexts. Predictions

by L1, when they are differ from the predictions
by L0 under ‘constant’ context, are more accurate
than expected by chance. This provides support
that the RSA framework models DC interpreta-
tion. Overall, predictions of non-implicit senses
hardly differ among different models, since an im-
plicit DC is much less informative than an explicit
DC. Moreover, previous relation senses or forms
do not improve the accuracy, suggesting that a
more generalized formulation of contextual infor-
mation is required to refine discourse understand-
ing. It is also observed that predictions by L2 are
mostly the same as L1. This implies that the lis-
tener is unlikely to emulate speaker’s production
iteratively at deeper levels.

4.3 Insights on automatic discourse parsing
Next, we investigate if the proposed method helps
automatic discourse sense classification. A full
discourse parser typically consists of a pipeline of
classifiers: explicit and implicit DCs are first clas-
sified and then processed separately by 2 classi-
fiers (Xue et al., 2015). On the contrary, the prag-
matic listener of the RSA model considers if the
speaker would prefer a particular DC, explicit or
implicit, when expressing the intended sense.

In this experiment, we integrate the output of
an automatic discourse parser with the probabil-
ity prediction by the pragmatic listener L1. We
employ the winning parser of the CONLL shared
task (Wang and Lan, 2015). The parser is also
trained on Sections 2-22 of PDTB, and thus does
not overlap with our test set. The sense classi-
fication of the parser is based on a pool of lex-
icosyntactic features drawn from gold standard
arguments, DCs and automatic parsed trees pro-
duced by CoreNLP (Manning et al., 2014).

For each test sample, the parser outputs a prob-
ability estimate for each sense. We use these esti-
mates to replace the salience measure (PL(s)) (in
Eq. 8) and deduce P ′L1(s|d,C), where C is the
previous relation form.

P ′L1(s|d,C) =
PS1(d|s, C)Pparser(s)∑

s′∈S
PS1(d|s′, C)Pparser(s′) (9)

Table 3 compares the performance of the origi-
nal parser output and the prediction based on P ′L1 .

1This does not match with Table 1 as samples labeled with
2 senses are double counted. Multi-sense training samples
are splitted into multiple samples, each labelled with one of
the senses. In testing, a prediction is considered correct if it
matches with one of the multiple senses.

534



discourse parser P ′L1 test
relation sense tags output output counts
Conjunction .7022 .7079 1479
Contrast .7382 .7152 1152
Entity .5174 .5249 862
Reason .4844 .5105 661
Restatement .2773 .2871 567
Result .4019 .4150 405
Instantiation .4346 .4357 282
Synchrony .6553 .7007 264
Condition .9087 .9302 238
Succession .7022 .7210 204
Precedence .7523 .7762 200
Concession .3048 .4382 146
Chosen alternative .5000 .5200 36
Alternative .8421 .8929 28
Exception 1.00 1.00 1
Accuracy / Total .5833 .5916 65251

Table 3: F1 scores of original parser output vs
parser output modified with P ′L1 . Higher scores
are bolded. The improvement in accuracy is sig-
nificant at p < 0.05 by McNemar Test.

Significant improvement in classification accuracy
is achieved and the F1 scores for most senses are
improved. This confirms the applicational poten-
tial of our model on automatic discourse parsing.

5 Conclusion

We propose a new framework to model the inter-
pretation of discourse relations based on Bayesian
pragmatics. Experimental results support the ap-
plicability of the model on human DC comprehen-
sion and automatic discourse parsing. As future
work, we plan to deduce a more general abstrac-
tion of the context governing DC interpretation. A
larger picture is to design a full, incremental dis-
course parsing algorithm that is motivated by the
psycholinguistic reality of human discourse pro-
cessing.

References

Anton Benz, Gerhard Jäger, Robert Van Rooij, and
Robert Van Rooij. 2016. Game theory and prag-
matics. Springer.

Leon Bergen, Roger Levy, and Noah D. Goodman.
2014. Pragmatic reasoning through semantic infer-
ence.

Herbert H Clark. 1996. Using language. Cambridge
university press.

Kenji Doya. 2007. Bayesian brain: Probabilistic ap-
proaches to neural coding. MIT press.

Michael C. Frank and Noah D. Goodman. 2012. Pre-
dicting pragmatic reasoning in lanugage games. Sci-
ence, 336(6084):998.

Bruno Galantucci, Carol A Fowler, and Michael T
Turvey. 2006. The motor theory of speech per-
ception reviewed. Psychonomic bulletin & review,
13(3):361–377.

Noah D Goodman and Daniel Lassiter. 2014. Prob-
abilistic semantics and pragmatics: Uncertainty in
language and thought. Handbook of Contemporary
Semantic Theory. Wiley-Blackwell.

Noah D Goodman and Andreas Stuhlmüller. 2013.
Knowledge and implicature: modeling language un-
derstanding as social cognition. Topics in cognitive
science, 5(1):173–184.

HP Grice. 1975. Logic and conversation in p. cole
and j. morgan (eds.) syntax and semantics volume 3:
Speech acts.

Gerhard Jäger. 2012. Game theory in semantics
and pragmatics. In Claudia Maienborn, Klaus von
Heusinger, and Paul Portner, editors, Semantics:
An International Handbook of Natural Language
Meaning, volume 3, pages 2487–2425. Mouton de
Gruyter.

Justine T Kao, Jean Y Wu, Leon Bergen, and Noah D
Goodman. 2014. Nonliteral understanding of num-
ber words. Proceedings of the National Academy of
Sciences, 111(33):12002–12007.

James M Kilner, Karl J Friston, and Chris D Frith.
2007. Predictive coding: an account of the mir-
ror neuron system. Cognitive processing, 8(3):159–
166.

Daniel Lassiter and Noah D Goodman. 2013. Context,
scale structure, and statistics in the interpretation of
positive-form adjectives. In Semantics and Linguis-
tic Theory, volume 23, pages 587–610.

Daniel Lassiter and Noah D Goodman. 2015. Adjecti-
val vagueness in a bayesian model of interpretation.
Synthese, pages 1–36.

Diana Lewis. 2006. Discourse markers in english: a
discourse-pragmatic view. Approaches to discourse
particles, pages 43–59.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkey, Steven J. Bethard, and David Mc-
Closky. 2014. The standord corenlp natural lan-
guage processing toolkit. Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 55–60.

535



Will Monroe and Christopher Potts. 2015. Learning
in the rational speech acts model. arXiv preprint
arXiv:1510.06807.

Mike Oaksford and Nick Chater. 2009. Prcis of
bayesian rationality: The probabilistic approach to
human reasoning. Behavioral and Brain Sciences,
32:69–84, 2.

Naho Orita, Eliana Vornov, Naomi H. Feldman, and
Hal Daumé III. 2015. Why discourse affects speak-
ers’ choice of refering expressions. Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.

Martin J Pickering and Simon Garrod. 2007. Do
people use language production to make predictions
during comprehension? Trends in cognitive sci-
ences, 11(3):105–110.

Martin J Pickering and Simon Garrod. 2013.
An integrated theory of language production and
comprehension. Behavioral and Brain Sciences,
36(04):329–347.

Christopher Potts, Daniel Lassiter, Roger Levy, and
Michael C. Frank. 2015. Embedded implicatures
as pragmatic inferences under compositional lexical
uncertainty. Manuscript.

Rashmi Prasad, Nikhit Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0.
Proceedings of the Language Resource and Evalua-
tion Conference.

C.E. Shannon. 1948. A mathematical theory of com-
munication. The Bell System Technical Journal,
27(379-423; 623-656).

Teun A Van Dijk. 1980. The semantics and pragmat-
ics of functional coherence in discourse. Speech act
theory: Ten years later, pages 49–66.

Jianxiang Wang and Man Lan. 2015. A refined end-to-
end discourse parser. CoNLL 2015, page 17.

Nianwen Xue, Hwee Tou Ng, Sameer Pradhan, Rashmi
PrasadO Christopher Bryant, and Attapol T Ruther-
ford. 2015. The conll-2015 shared task on shallow
discourse parsing. CoNLL 2015, page 1.

Henk Zeevat. 2011. Bayesian interpretation and opti-
mality theory. Bidirectional Optimality Theory. Pal-
grave Macmillan, Amsterdam, pages 191–220.

Henk Zeevat. 2015. Perspectives on bayesian natu-
ral language semantics and pragmatics. In Bayesian
Natural Language Semantics and Pragmatics, pages
1–24. Springer.

536


