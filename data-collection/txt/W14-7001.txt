





































Overview of the 1st Workshop on Asian Translation
Toshiaki Nakazawa
Japan Science and

Technology Agency

nakazawa@pa.jst.jp

Hideya Mino
National Institute of

Information and
Communications Technology

hideya.mino@nict.go.jp

Isao Goto
NHK

goto.i-es@nhk.or.jp

Sadao Kurohashi
Kyoto University

kuro@i.kyoto-u.ac.jp

Eiichiro Sumita
National Institute of

Information and
Communications Technology

eiichiro.sumita@nict.go.jp

Abstract
This paper presents the results of the
1st workshop on Asian translation
(WMT2014) shared tasks, which included
J↔E translation subtasks and J↔C trans-
lation subtasks. As the first year of WAT,
12 institutions participated to the shared
tasks. More than 300 translation results
have been submitted to the automatic eval-
uation server, and selected submissions
were manually evaluated.

1 Introduction

The Workshop on Asian Translation (WAT) is a
new open evaluation campaign focusing on Asian
languages. We would like to invite a broad range
of participants and conduct various forms of ma-
chine translation experiments and evaluation. Col-
lecting and sharing our knowledge will allow us to
understand the essence of machine translation and
the problems to be solved. We are working toward
the practical use of machine translation among all
Asian countries.

For the 1st WAT, we chose scientific papers as
the targeted domain, and selected the languages
Japanese, Chinese and English.

What makes WAT unique:

• Open innovation platform
The test data is fixed and open, so you can re-
peat evaluations on the same data and confirm
changes in translation accuracy over time.
WAT has no deadline for the automatic trans-
lation quality evaluation (continuous evalua-
tion), so you can submit translation results at
any time.

• Domain and language pairs
WAT is the world’s first workshop that uses

LangPair Train Dev DevTest Test
ASPEC-JE 3,008,500 1,790 1,784 1,812
ASPEC-JC 672,315 2,090 2,148 2,107

Table 1: Statistics of ASPEC.

scientific papers as a domain and Japanese-
Chinese as a language pair. In the future, we
will add more Asian languages, such as Ko-
rean, Vietnamese, Indonesian, Thai, Myan-
mar and so on.

• Evaluation method
Evaluation will be done by both automatic
and human evaluation. For human evalua-
tion, WAT will use crowdsourcing, which is
low cost and allows multiple evaluations.

2 Dataset

WAT uses Asian Scientific Paper Excerpt Corpus
(ASPEC)1 as the dataset. ASPEC is constructed
by the Japan Science and Technology Agency
(JST) in collaboration with the National Institute
of Information and Communications Technology
(NICT). It consists of a Japanese-English scientific
paper abstract corpus (ASPEC-JE), which is used
for J↔E subtasks, and a Japanese-Chinese scien-
tific paper excerpt corpus (ASPEC-JC), which is
used for J↔C subtasks. The statistics of each cor-
pus are described in Table1.

2.1 ASPEC-JE

The training data of ASPEC-JE was constructed
by the NICT from approximately 2 million
Japanese-English scientific paper abstracts owned
by the JST. Because the paper abstracts are kind

1http://lotus.kuee.kyoto-u.ac.jp/ASPEC/

1
Proceedings of the 1st Workshop on Asian Translation (WAT2014), pages 1‒19,

Tokyo, Japan, 4th October 2014.
2014 Copyright is held by the author(s).



of comparable corpora, the sentence correspon-
dences are automatically found using the method
of (Utiyama and Isahara, 2007). Each sentence
pair is accompanied with the similarity score and
the field symbol. The similarity scores are cal-
culated by the method of (Utiyama and Isahara,
2007). The field symbols are single letters A-
Z and show the scientific field of each docu-
ment2. The correspondance between the symbols
and field names, along with the frequency and oc-
curance ratios for the training data, are given in the
README of ASPEC-JE.

The development, development-test and test
data were extracted from parallel sentences from
Japanese-English paper abstracts owned by JST
that are not contained in the training data. Each
data set contains 400 documents. Furthermore, the
data has been selected to contain the same relative
field coverage across each data set. The document
alignment was conducted automatically and only
documents with a 1-to-1 alignment are included.
It is therefore possible to restore the original doc-
uments. The format is the same as for the training
data except that there is no similarity score.

2.2 ASPEC-JC

ASPEC-JC is a parallel corpus consisting of
Japanese scientific papers from the literature
database and electronic journal site J-STAGE of
JST that have been translated to Chinese after re-
ceiving permission from the necessary academic
associations. The parts selected were abstracts and
paragraph units from the body text, as these con-
tain the highest overall vocabulary coverage.

The development, development-test and test
data are extracted at random from documents con-
taining single paragraphs across the entire cor-
pus. Each set contains 400 paragraphs (docu-
ments). Therefore there are no documents sharing
the same data across the training, development,
development-test and test sets.

3 Baseline Systems

Human evaluations were conducted as pairwise
comparisons between the translation results for a
specific baseline system and translation results for
each participant’s system. That is, the specific
baseline system was the standard of human eval-
uation. A phrase-based statistical machine trans-
lation (SMT) system was adopted as the specific

2http://opac.jst.go.jp/bunrui/index.html

baseline system at WAT 2014.
In addition to the results for the baseline phrase-

based SMT system, we produced results for the
baseline systems that consisted of a hierarchical
phrase-based SMT system, a string-to-tree syntax-
based SMT system, a tree-to-string syntax-based
SMT system, five commercial rule-based machine
translation (RBMT) systems, and two online trans-
lation systems. The SMT baseline systems con-
sisted of publicly available software, and the pro-
cedures for building the systems and translating
using the systems were published on the WAT
2014 web page3. We used Moses (Koehn et al.,
2007; Hoang et al., 2009) as the implementation
of the baseline SMT systems. The Berkeley parser
(Petrov et al., 2006) was used to obtain syntactic
annotations. The baseline systems are shown in
Table 2.

The commercial RBMT systems and the online
translation systems were operated by the organiz-
ers. We note that these RBMT companies and on-
line translation companies did not submit them-
selves. Since our objective is not to compare com-
mercial RBMT systems or online translation sys-
tems from companies that did not themselves par-
ticipate, the system description of these systems
are anonymized in this paper.

We describe the detail of the baseline SMT sys-
tems.

3.1 Data for Training
We used the following data for the training of the
SMT baseline systems.

• Training data for the language model: All of
the target language sentences in the parallel
corpus.

• Training data for the translation model: Sen-
tences that were 40 words or less in length.
(For Japanese–English training data, we only
used train-1.txt, which consisted of 1 million
parallel sentence pairs with high similarity
scores.)

• Development data for tuning: All of the de-
velopment data.

3.2 Common Settings for Baseline SMT
We used the following tools for tokenization.

• Juman version 7.04 for Japanese segmenta-
tion.

3http://lotus.kuee.kyoto-u.ac.jp/WAT/
4http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN

2



System ID System Type JE EJ JC CJ
SMT Phrase Moses’ Phrase-based SMT SMT ✓ ✓ ✓ ✓
SMT Hiero Moses’ Hierarchical Phrase-based SMT SMT ✓ ✓ ✓ ✓
SMT S2T Moses’ String-to-Tree Syntax-based SMT and Berkeley parser SMT ✓ ✓
SMT T2S Moses’ Tree-to-String Syntax-based SMT and Berkeley parser SMT ✓ ✓
RBMT X The Honyaku V15 (Commercial system) RBMT ✓ ✓
RBMT X ATLAS V14 (Commercial system) RBMT ✓ ✓
RBMT X PAT-Transer 2009 (Commercial system) RBMT ✓ ✓
RBMT X J-Beijing 7 (Commercial system) RBMT ✓ ✓
RBMT X Hohrai 2011 (Commercial system) RBMT ✓ ✓
Online X Google translate (July, 2014) (SMT) ✓ ✓ ✓ ✓
Online X Bing translator (July, 2014) (SMT) ✓ ✓ ✓ ✓

Table 2: Baseline Systems

• Stanford Word Segmenter version 2014-01-
045 (Chinese Penn Treebank (CTB) model)
for Chinese segmentation.

• The Moses toolkit for English tokenization.

To obtain word alignments, GIZA++ and grow-
diag-final-and heuristics were used. We used 5-
gram language models with modified Kneser-Ney
smoothing, which were built using a tool in the
Moses toolkit (Heafield et al., 2013).

3.3 Phrase-based SMT

We used the following Moses’ configuration for
the phrase-based SMT system.

• distortion-limit = 20
• msd-bidirectional-fe lexicalized reordering
• Phrase score option: GoodTuring

The default values were used for the other system
parameters.

3.4 Hierarchical Phrase-based SMT

We used the following Moses’ configuration for
the hierarchical phrase-based SMT system.

• max-chart-span = 1000
• Phrase score option: GoodTuring

The default values were used for the other system
parameters.

3.5 String-to-Tree Syntax-based SMT

We used Berkeley parser to obtain target language
syntax. We used the following Moses’ configura-
tion for the string-to-tree syntax-based SMT sys-
tem.

• max-chart-span = 1000
• Phrase score option: GoodTuring
5http://nlp.stanford.edu/software/segmenter.shtml

• Phrase extraction options: MaxSpan = 1000,
MinHoleSource = 1, and NonTermConsec-
Source.

The default values were used for the other system
parameters.

3.6 Tree-to-String Syntax-based SMT
We used Berkeley parser to obtain source language
syntax. We used the following Moses’ configu-
ration for the baseline tree-to-string syntax-based
SMT system.

• max-chart-span = 1000
• Phrase score option: GoodTuring
• Phrase extraction options: MaxSpan = 1000,

MinHoleSource = 1, MinWords = 0,
NonTermConsecSource, and AllowOnlyU-
nalignedWords.

The default values were used for the other system
parameters.

4 Automatic Evaluation

4.1 Procedure of Calculating Automatic
Evaluation Score

We calculated automatic evaluation scores of the
translation results applying two popular metrics:
BLEU (Papineni et al., 2002) and RIBES (Isozaki
et al., 2010). BLEU scores were calculated with
multi-bleu.perl distributed with the Moses toolkit
(Koehn et al., 2007). RIBES scores were calcu-
lated with RIBES.py version 1.02.4 6. All scores
of each task were calculated using one reference.
Before the calculation of the automatic evaluation
scores, the translation results have been tokenized
with word segmentation tools on each language.

6http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html

3



Fi
gu

re
1:

T
he

su
bm

is
si

on
w

eb
pa

ge
fo

rp
ar

tic
ip

an
ts

4



For Japanese segmentation we use three differ-
ent tools, which are Juman version 7.0 (Kurohashi
et al., 1994), KyTea 0.4.6 (Neubig et al., 2011)
with Full SVM model 7 and MeCab 0.996 (Kudo,
2005) with IPA dictionary 2.7.0 8. For Chinese
segmentation we use two different tools, which are
KyTea 0.4.6 with Full SVM Model in MSR model
and Stanford Word Segmenter version 2014-06-16
with Chinese Penn Treebank (CTB) and Peking
University (PKU) model 9 (Tseng, 2005). For En-
glish segmentation we use tokenizer.perl 10 in the
Moses toolkit.

The detailed procedures for the automatic eval-
uation are shown at WAT2014 evaluation web
page 11.

4.2 Automatic Evaluation System

The participants submit the translation results
via an automatic evaluation system deployed at
WAT2014 web page, which give them automatic
evaluation scores of the results they upload. Fig-
ure 1 shows the submitting interface for partici-
pants. The system requires the participants to pro-
vide the following information when they upload
the translation results:

• Subtask (J ↔ E, J ↔ C)

• Method (SMT, RBMT, SMT and RBMT,
EBMT, Other)

• Existence of the use of other resources in ad-
dition to ASPEC

• Permission of publishing the automatic eval-
uation scores on WAT2014 web page

The server of the system keeps all submitted in-
formation including translation results or scores
and participants can confirm the only information
they uploaded. The information of translation re-
sults which the participant permits to publish is
disclosed on the web page. In addition to sub-
mitting the translation results for automatic eval-
uation, participants submit the results for human
evaluation with the same web interface. This au-
tomatic evaluation system will be available even

7http://www.phontron.com/kytea/model.html
8http://code.google.com/p/mecab/downloads/detail?

name=mecab-ipadic-2.7.0-20070801.tar.gz
9http://nlp.stanford.edu/software/segmenter.shtml

10https://github.com/moses-smt/mosesdecoder/tree/
RELEASE-2.1.1/scripts/tokenizer/tokenizer.perl

11http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/index.html

after WAT2014. Everybody can use the system by
registering on the registration web page 12.

5 Human Evaluation

5.1 Using Crowdsourcing

As all the MT researchers know, the human eval-
uation costs a lot of time and money. One of the
solutions to reduce them is using crowdsourcing.
Other machine translation evaluation campaigns
such as IWSLT (2011, 2012) and WMT (2012,
2013) also used crowdsourcing for the human
evaluation. Recently, there are so many crowd-
sourcing services in the world: Amazon Mechan-
ical Turk13, CrowdFlower14, Yahoo Crowdsourc-
ing15, Lancers16 and so on. Among these ser-
vices, we used Lancers for the human evaluation
of WAT2014.

There are two reasons of choosing Lancers. One
is that we can set the category of the crowdsourc-
ing task (’Translation’ for this case). We can reach
the appropriate workers by setting the appropriate
categories. The other reason is that we can ask the
task to the identity-verified workers. This func-
tion guarantee the quality of the workers. These
two advantages can keep the evaluation quality at
higher level.

5.2 Human Evaluation Method

For the human evaluation, we randomly chose
documents from the Test set of ASPEC data, in
total 400 sentence pairs for JE and JC. We ex-
cluded the documents which contains a sentence
with longer than 100 Japanese characters. Each
submission is compared with the baseline transla-
tion (Phrase-based SMT, described in Section 3)
and given HUMAN score.

5.2.1 Pairwise Evaluation of Sentences
We conducted pairwise evaluation of each test sen-
tence of the 400 sentences. The input sentence
and two translations (the baseline and a submis-
sion) are shown to the workers, and the workers
are asked to judge which translation is better than
the other, or they are of the same quality. The or-
der of the two translations are at random. Figure 2
shows the illustration of the evaluation.

12http://lotus.kuee.kyoto-u.ac.jp/WAT/registration/index.html
13https://www.mturk.com
14http://www.crowdflower.com
15http://crowdsourcing.yahoo.co.jp (Japanese service)
16http://www.lancers.jp (Japanese service)

5



Figure 2: The illustration of the crowdsourcing evaluation. The workers are asked to judge which trans-
lation is better, or the same.

Worker 1 A A A A A A Tie Tie Tie B
Worker 2 A A A Tie Tie B Tie Tie B B
Worker 3 A Tie B Tie B B Tie B B B
Decision A A A A Tie B Tie B B B

Table 3: The combinations of human judgements and the final decision of each sentence pairs from
system A and B.

5.2.2 Voting
The crowdsourcing workers are not specialists,
thus the quality of the judgements are not nec-
essarily precise. To guarantee the quality of the
evaluation, each sentence is evaluated by 3 differ-
ent workers and the final decision is made by the
voting of the judgements. Table 3 shows all the
combinations of the worker judgements and the fi-
nal decision.

5.2.3 HUMAN Score Calculation
Suppose W to be the number of wins compared to
the baseline, L to be the number of losses and T to
be the number of ties, the HUMAN score, which is
the official human evaluation score of WAT2014,
can be calculated by the following formula:

HUMAN = 100× W − L
W + L+ T

From the definition, the HUMAN score ranges be-
tween -100 and 100.

5.2.4 Confidence Interval Estimation
As there are several ways to estimate the confi-
dence interval, we chose the bootstrap resampling
(Koehn, 2004) to estimated 95% confidence inter-
val. The procedure is as follows:

1. randomly select 300 sentences from the 400
human evaluation sentences, and calculate
the HUMAN score on the selected sentences

2. iterate the previous step 1000 times and get
1000 HUMAN scores

3. sort the 1000 scores and estimate the 95%
confidence interval by discarding top and bot-
tom 25 scores

5.3 Cost of Evaluation

One big benefit of using crowdsourcing is that we
can reduce the cost of evaluations. In WAT2014,
one judgement costs 5 JPY. The evaluation of a
submission requires 3 (judgements) × 400 (sen-
tence pairs) = 1,200 judgements and it costs 5 ×
1,200 = 6,000 JPY. The time for the evaluation
differs depending on the translation direction. On
the average, one evaluation finished in a couple of
days.

6 Participants List

Table 4 shows the list of participants to WAT2014.
There are not only the Japanese organizations, but
some organizations came from outside Japan. 12
teams submitted one or more translation results to

6



the automatic evaluation server, and 11 teams sub-
mitted one or more translation results to the human
evaluation.

7 Evaluation Results

In this section, the evaluation results of WAT2014
are reported from several perspectives. Parts of the
results of both automatic and human evaluations
are also accessible at WAT2014 website17.

7.1 Official Automatic Evaluation Results

Figure 3 shows the official automatic evaluation
results of the representative submissions and base-
line systems. The automatic evaluation results of
all the submissions are shown in Section Appendix
A.

7.2 Official Human Evaluation Results

HUMAN scores
Figure 4 shows the official human evaluation re-
sults. The error bars in the figures show the 95%
confidence interval (see Section 5.2.4). Note that
overlapping the error bars between two submis-
sions does not necessarily mean that there is no
significant difference. If an error bar crosses the
x-axis (HUMAN score = 0), it means that there is
no significant difference between the submission
and the baseline (SMT Phrase).

From the results, the followings can be ob-
served:

• The best SMT system achieved better quality
than RBMT system.

• The translation quality of the widely used
systems was Phrase-based SMT < Hierarchi-
cal PBSMT < Syntax-based SMT (S2T and
T2S).

• Forest-to-String Syntax-based SMT system
(Neubig, 2014) achieved the best quality for
all the translation directions.

Statistical Significance Testing between
Submissions
Tables 5, 6, 7 and 8 show the results of statistical
significance testings of JE, EJ, JC and CJ trans-
lations respectively where all the pairs of submis-
sions are tested. ≫, ≫ and > mean that the sys-
tem in the row is better than the system in the col-
umn by p < 0.01, 0.05, 0.1 respectively. The test-

17http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/index.html

ings are also done by the bootstrap resampling as
follows:

1. randomly select 300 sentences from the 400
human evaluation sentences, and calculate
the HUMAN scores on the selected sentences
for both systems

2. iterate the previous step 1000 times and count
the number of wins (W ), losses (L) and ties
(T )

3. calculate p = LW+L

Inter-annotator Agreement
To assess the reliability of agreement between the
crowdsourcing workers, we calculated the Fleiss’
kappa (Fleiss and others, 1971) values. The re-
sults are shown in Table 9. We can see that the
Kappa values are larger for X → J translations
than J → X translations. This may be because
we used Japanese crowdsourcing service for the
evaluation and the majority of the crowdsourcing
workers are Japanese. The MT evaluation of their
mother tongue is much easier than the others in
general.

Case Study: Direct Comparison and Relative
Comparison
Looking at evaluation results of WEBLIO-EJ1
1 and 2 submissions (see Table 12), the auto-
matic and human evaluations are inconsistent:
the WEBLIO-EJ1 2 is consistently better than
WEBLIO-EJ1 1 in the automatic evaluation, how-
ever it is much worse in the human evaluation. Ac-
cording to the descriptions of the two submissions,
the difference of the two is whether it uses the for-
est input or not. It is natural that using the for-
est input improves the translation quality, thus we
conducted the human evaluation of WEBLIO-EJ1
2 compared to WEBLIO-EJ1 1, which means we
used WEBLIO-EJ1 1 as the baseline for the hu-
man evaluation.

The HUMAN score was 2.50 ± 4.17 which
means there is no significant difference between
the two, and this result is far from the results of
the official results. Actually, taking the confidence
intervals into consideration, this conclusion can be
derived under some probability.

The Fleiss’ kappa value was 0.528 and it is
much higher than the other E→J human evalua-
tions. This may be because the outputs of the two
systems are quite similar and it is very easy for the

7



non-removal removal
JE BLEU 0.46489 0.95098
JE RIBES 0.78255 0.83691
EJ BLEU 0.41524 0.84418
EJ RIBES 0.75105 0.85730
JC BLEU 0.49240 0.07937
JC RIBES 0.38695 0.10198
CJ BLEU 0.78713 0.82592
CJ RIBES 0.70081 0.83209

Table 10: The changes of correlations (R2) before
and after removing RBMT and online systems.

workers to judge which translation is better. If two
translations have both better and worse parts than
the other, the workers would evaluate differently
from person to person.

7.3 Correlation between Automatic and
Human Evaluations

Figure 5 shows the correlations between automatic
evaluation measures (BLEU/RIBES) and the HU-
MAN score. It is well known that the automatic
and human evaluations do not have good correla-
tions for RBMT and online systems. Removing
these systems from the graph changes the correla-
tion values (R2) like in Table 10. The correlation
becomes much better after removing the RBMT
and online systems for all the translation directions
other than J→C.

8 Submitted Data

The number of published automatic evaluaition
results of 12 teams exceeded 100 by the day of
WAT2014 workshop and 37 translation results for
human evaluation was submitted by 11 teams. We
will organize the all submitted data for human
evaluation and make it public.

9 Conclusion and Future Perspective

This paper summarized the WAT2014 machine
translation evaluation campaing. We had 12 par-
ticipants worldwide, and collected a large number
of submissions which are useful to improve the
current machine translation systems by analyzing
the submissions and finding the issues.

For the next WAT workshop, we are planning to
conduct context-aware MT evaluations. The test
data of WAT is prepared using the paragraph as a
unit, while almost all other evaluation campaigns
use the sentence as a unit. Therefore, it is suitable
to investigate the importance of the context for the
translation.

Also, we are very happy to include other lan-
guages if there are available resources.

Appendix A Submissions

Tables 11, 12, 13 and 14 summarize all the sub-
missions listed in the automatic evaluation server
at the point of WAT2014 workshop (4th, October,
2014). The OTHER RESOURCES column shows
the use of other resources such as parallel corpora,
monolingual corpora and parallel dictionaries in
addition to ASPEC.

8



Team ID Organization JE EJ JC CJ
NAIST (Neubig, 2014) Nara Institute of Science and Technology ✓ ✓ ✓ ✓
EIWA (Ehara, 2014) Yamanashi Eiwa College ✓ ✓
Kyoto-U (Richardson et al., 2014) Kyoto University ✓ ✓ ✓ ✓
WEBLIO-EJ1 (Zhu, 2014) Weblio, Inc. ✓
TMU (Ohwada et al., 2014) Tokyo Metropolitan University ✓
BJTUNLP (Cai et al., 2014) Beijing Jiaotong University ✓
NII (Hoshino et al., 2014) National Institute of Informatics ✓
SAS MT (Wang et al., 2014) SAS Research and Development Co., Ltd ✓ ✓
Sense (Tan and Bond, 2014) Saarland University & Nanyang Technological University ✓ ✓ ✓ ✓
NICT (Ding et al., 2014) National Institute of Information and Communication Technology ✓
TOSHIBA (Sonoh et al., 2014) Toshiba Corporation ✓ ✓
WASUIPS (Yang and Lepage, 2014) Waseda University ✓* ✓*

Table 4: The list of participants which submitted translation results to WAT2014 and their participations
to each subtasks. (*Only submitted to automatic evaluations.)

N
A

IS
T

2

K
yo

to
-U

1

SM
T

S2
T

TO
SH

IB
A

1

R
B

M
T

D

E
IW

A

K
yo

to
-U

2

TO
SH

IB
A

2

O
nl

in
e

D

SM
T

H
ie

ro

Se
ns

e

N
II

1

N
II

2

T
M

U
1

T
M

U
2

NAIST 1 - ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
NAIST 2 ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
Kyoto-U 1 - - - - - > ≫ ≫ ≫ ≫ ≫ ≫ ≫
SMT S2T - - - > > ≫ ≫ ≫ ≫ ≫ ≫ ≫
TOSHIBA 1 - - - - ≫ ≫ ≫ ≫ ≫ ≫ ≫
RBMT D - - - ≫ ≫ ≫ ≫ ≫ ≫ ≫
EIWA - - ≫ ≫ ≫ ≫ ≫ ≫ ≫
Kyoto-U 2 - ≫ ≫ ≫ ≫ ≫ ≫ ≫
TOSHIBA 2 ≫ ≫ ≫ ≫ ≫ ≫ ≫
Online D ≫ ≫ ≫ ≫ ≫ ≫
SMT Hiero ≫ ≫ ≫ ≫ ≫
Sense ≫ ≫ ≫ ≫
NII 1 ≫ ≫ ≫
NII 2 - -
TMU 1 -

Table 5: Statistical significance testing of JE results.

N
A

IS
T

2

W
E

B
L

IO
-E

J1
1

O
nl

in
e

A

K
yo

to
-U

1

W
E

B
L

IO
-E

J1
2

SM
T

T
2S

K
yo

to
-U

2

SM
T

H
ie

ro

SA
S

M
T

Se
ns

e

R
B

M
T

B

NAIST 1 ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
NAIST 2 ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
WEBLIO-EJ1 1 - ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
Online A > ≫ ≫ ≫ ≫ ≫ ≫ ≫
Kyoto-U 1 - > ≫ ≫ ≫ ≫ ≫
WEBLIO-EJ1 2 - - > ≫ ≫ ≫
SMT T2S - - ≫ ≫ ≫
Kyoto-U 2 - ≫ ≫ ≫
SMT Hiero > ≫ ≫
SAS MT ≫ ≫
Sense -

Table 6: Statistical significance testing of EJ results.

9



Figure 3: The official automatic evaluation results.

10



!"#$"%%

&'#$"%%

($#$"%%($#""%%
(&#($%%(&#""%%((#$"%%

()#($%%("#($%%

)&#'$%%

'#'$%%

)#($%%
"#""%%

*$#'$%%

*)!#($%%
*)'#""%%*)'#($%%

*&"#""%%

*("#""%%

*)"#""%%

"#""%%

)"#""%%

("#""%%

&"#""%%

!"#""%%

$"#""%%

+
,-
./
%)
%

+
,-
./
%(
%

.0
/%
.(
/%

12
34
3*
5
%)
%

/6
.7
-8
,%
)%

98
0
/%
:
%

;-
<
,%

12
34
3*
5
%(
%

/6
.7
-8
,%
(%

6
=>
?=
@%
:
%

.0
/%
7
?@
A3
%

.@
=B
@%

.0
/%
CD
AE
B@
%

+
--%
)%

+
--%
(%

/0
5
%)
%

/0
5
%(
%

!"#$%&'(#)*+,-!

!"#$!%%

!&#!'%%

()#$!%%($#!'%%

)*#''%%
)"#''%%

)(#$!%%))#+!%%
)&#!'%%

$+#!'%%

)#+!%%
'#+!%% '#''%%

,&'#''%%

'#''%%

&'#''%%

$'#''%%

)'#''%%

('#''%%

!'#''%%

"'#''%%

+'#''%%

-
./
01
%&
%

-
./
01
%$
%

2
34
5/
6
,3
7%&
%&
%

6
89
:8
;%
.%

<=
>?
>,
@
%&
%

2
34
5/
6
,3
7&
%$
%

0A
1%
1$
0%

<=
>?
>,
@
%$
%

0A
1%
B
:;
C>
%

0.
0D
A
1%

0;
8E
;%

F4
A
1%
4%

0A
1%
GH
CI
E;
%

!"#$%&'(#)*+,-!

!"#"$%%

!&#''%%

!'#''%%

(#$'%%

)#"$%%
!#*$%% '#"$%% '#''%%

+'#"$%%

+)#"$%%
+$#*$%%

+,#"$%%

+!&#$'%%

+*'#''%%

+)'#''%%

+*'#''%%

+!'#''%%

'#''%%

!'#''%%

*'#''%%

)'#''%%

-
./
01
%!
%

02
1%
0*
1%

03
45
3%

-
/6
1%

02
1%
7
83
9:
%

-
./
01
%*
%

1;
07
/<
.%
!%

02
1%
=>
9?
53
%

@A
:B
:+
C
%!
%

<D
1C
-
E=
%

1;
07
/<
.%
*%

@A
:B
:+
C
%*
%

;
4F
84
3%
G
%

H<
2
1%
<%

!"#$%&'(#)*+,-! !"#$!%%

&'#""%%

((#!"%%

)*#""%% )!#""%%

$#!"%% *#""%% +#$!%%

"#""%%

,)#""%%

,()#$!%%

,&$#$!%%

,!"#""%%

,+"#""%%

,&"#""%%

,("#""%%

,)"#""%%

"#""%%

)"#""%%

("#""%%

&"#""%%

+"#""%%

!"#""%%

*"#""%%

-
./
01
%)
%

-
./
01
%(
%

0.
02
3
1%

03
1%
1(
0%

4/
5
.%

67
89
8,
:
%)
%

67
89
8,
:
%(
%

03
1%
;
<=
>8
%

03
1%
?@
>A
B=
%

0=
CB
=%

D
CE
<C
=%
.%

FG
3
1%
.%

!"#$%&'(#)*+,-!

Figure 4: The official human evaluation results.

SM
T

S2
T

Se
ns

e

N
IC

T

SM
T

H
ie

ro

N
A

IS
T

2

TO
SH

IB
A

1

K
yo

to
-U

1

B
JT

U
N

L
P

TO
SH

IB
A

2

K
yo

to
-U

2

O
nl

in
e

D

R
B

M
T

B

NAIST 1 - ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
SMT S2T - ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
Sense - ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
NICT - > ≫ ≫ ≫ ≫ ≫ ≫ ≫
SMT Hiero - - - ≫ ≫ ≫ ≫ ≫
NAIST 2 - - > ≫ ≫ ≫ ≫
TOSHIBA 1 - > ≫ ≫ ≫ ≫
Kyoto-U 1 - > ≫ ≫ ≫
BJTUNLP - > ≫ ≫
TOSHIBA 2 - ≫ ≫
Kyoto-U 2 ≫ ≫
Online D >

Table 7: Statistical significance testing of JC results.

11



N
A

IS
T

2

SA
S

M
T

SM
T

T
2S

E
IW

A

K
yo

to
-U

1

K
yo

to
-U

2

SM
T

H
ie

ro

Se
ns

e

O
nl

in
e

A

R
B

M
T

A

NAIST 1 ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
NAIST 2 ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
SAS MT ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫
SMT T2S - ≫ ≫ ≫ ≫ ≫ ≫
EIWA ≫ ≫ ≫ ≫ ≫ ≫
Kyoto-U 1 - - ≫ ≫ ≫
Kyoto-U 2 - ≫ ≫ ≫
SMT Hiero ≫ ≫ ≫
Sense ≫ ≫
Online A ≫

Table 8: Statistical significance testing of CJ results.

JE
System ID Kappa
NAIST 1 0.162
NAIST 2 0.047
SMT S2T 0.099
Kyoto-U 1 0.070
TOSHIBA 1 0.098
RBMT D 0.075
EIWA 0.083
Kyoto-U 2 0.139
TOSHIBA 2 0.078
Online D 0.055
SMT Hiero 0.119
Sense 0.245
NII 1 0.119
NII 2 0.086
TMU 1 0.091
TMU 2 0.136
ave. 0.106

EJ
System ID Kappa
NAIST 1 0.280
NAIST 2 0.250
WEBLIO-EJ1 1 0.238
Online A 0.219
Kyoto-U 1 0.216
WEBLIO-EJ1 2 0.240
SMT T2S 0.240
Kyoto-U 2 0.229
SMT Hiero 0.277
SAS MT 0.248
Sense 0.395
RBMT B 0.217
ave. 0.254

JC
System ID Kappa
NAIST 1 0.077
SMT S2T 0.069
Sense 0.087
NICT 0.066
SMT Hiero 0.202
NAIST 2 0.093
TOSHIBA 1 0.089
Kyoto-U 1 0.091
BJTUNLP 0.198
TOSHIBA 2 0.066
Kyoto-U 2 0.163
Online D 0.035
RBMT B 0.083
ave. 0.101

CJ
System ID Kappa
NAIST 1 0.168
NAIST 2 0.203
SAS MT 0.167
SMT T2S 0.236
EIWA 0.175
Kyoto-U 1 0.199
Kyoto-U 2 0.180
SMT Hiero 0.274
Sense 0.228
Online A 0.239
RBMT A 0.130
ave. 0.200

Table 9: The Fleiss’ kappa values of human evaluation results.

12



!"#$%&'&

!"#$%&(&

$)%&$(%&*+,-,./&'&
%0$1#2"&'&32)%&4& 5#6"& *+,-,./&(&

%0$1#2"&(&

07897:&4&

$)%&19:;,&

$:7<:&

!##&'&

!##&(&
%)/&'&%)/&(&

3=&>&?@ABACD&

.(?&

.'?&

?&

'?&

(?&

E?&

A?&

'F& 'B& 'G& 'C& 'D& (?& ('& ((& (E& (A& (F&

!
"
#
$
%
!

&'("!

)(*+!"#$%,&'("-*
!"#$%&'&

!"#$%&(&

$)%&$(%& *+,-,./&'&
%0$1#2"&'&32)%&4& 5#6"&*+,-,./&(&

%0$1#2"&(&

07897:&4&

$)%&19:;,&

$:7<:&

!##&'&

!##&(&
%)/&'&%)/&(&

3=&>&?@AB(CC&

.(?&

.'?&

?&

'?&

(?&

D?&

E?&

F'& FD& FC& FA& FG& A'& AD&

!
"
#
$
%
!

&'("!

)(*+!"#$%,-.&(/0*

!"#$%&'&

!"#$%&(&

)*+,#-.*/'&'&-01203&"&

45676.8&'&
)*+,#-.*/'&(&

$9%&%($&45676.8&(&
$9%&:23;6&

$"$<9%&

$30=3&

>+9%&+&

>?&@&ABC'D(C&

A&

'A&

(A&

EA&

CA&

DA&

FA&

'A& 'D& (A& (D& EA& ED& CA&

!
"
#
$
%
!

&'("!

()*+!"#$%,&'("-!
!"#$%&'&

!"#$%&(&

)*+,#-.*/'&'&-01203&"&

45676.8&'&
)*+,#-.*/'&(&

$9%&%($&45676.8&(&
$9%&:23;6&

$"$<9%&

$30=3&

>+9%&+&

>?&@&ABCD'AD&

A&

'A&

(A&

EA&

FA&

DA&

GA&

GG& GH& CA& C(& CF& CG& CH& HA& H(&

!
"
#
$
%
!

&'()*!

)+,-!"#$%.&'()*/!

!"#$%&'&

$(%&$)%&

$*+,*&

!#-%&

$(%&./*01&

!"#$%&)&%2$.#3"&'&
4516178&'&

39%8!:;&
%2$.#3"&)&

4516178&)&

2+</+*&=&

>3(%&3&

>?&@&ABCD)C&

7)E&

7)A&

7'E&

7'A&

7E&

A&

E&

'A&

'E&

)A&

E& 'A& 'E& )A& )E& FA& FE&

!
"
#
$
%
!

&'("!

)*+,!"#$%-&'(".! !"#$%&'&

$(%&$)%&

$*+,*&

!#-%&

$(%&./*01&

!"#$%&)&%2$.#3"&'&
4516178&'&

39%8!:;&
%2$.#3"&)&

4516178&)&

<3(%&3&

<=&>&?@ABCDE&

7)E&

7)?&

7'E&

7'?&

7E&

?&

E&

'?&

'E&

)?&

FG& FE& FC& FF& FB& FD& B?& B'& B)& BA& BG&

!
"
#
$
%
!

&'()*!

+,-.!"#$%/&'()*0!

!"#$%&'&

!"#$%&(&

$"$)*%&

$*%&%($&+#,"&

-./0/12&'&-./0/12&(&$*%&3456/&

$5785&

97:475&"&

;<*%&"&

;=&>&?@ABA'C&

1D?&

1C?&

1(?&

1'?&

?&

'?&

(?&

C?&

D?&

E?&

E& '?& 'E& (?& (E& C?& CE& D?& DE&

!
"
#
$
%
!

&'("!

)*+,!"#$%-&'(".! !"#$%&'&

!"#$%&(&

$"$)*%&

$*%&%($&+#,"&

-./0/12&'&-./0/12&(&$*%&3456/&

$5785&

97:475&"&

;<*%&"&

;=&>&?@A??B'&

1C?&

1D?&

1(?&

1'?&

?&

'?&

(?&

D?&

C?&

E?&

EF& GC& GF& AC& AF& BC&

!
"
#
$
%
!

&'()*!

+,-.!"#$%/&'()*0!

Figure 5: The correlations between BLEU/RIBES and HUMAN scores.
13



SY
ST

E
M

ID
ID

M
E

T
H

O
D

O
T

H
E

R
R

E
SO

U
R

C
E

S
B

L
E

U
R

IB
E

S
H

U
M

A
N

SY
ST

E
M

D
E

SC
R

IP
T

IO
N

SM
T

H
ie

ro
2

SM
T

N
O

18
.7

2
0.

65
10

66
+7

.7
5

H
ie

ra
rc

hi
ca

lP
hr

as
e-

ba
se

d
SM

T
SM

T
Ph

ra
se

6
SM

T
N

O
18

.4
5

0.
64

51
37

—
–

Ph
ra

se
-b

as
ed

SM
T

SM
T

S2
T

9
SM

T
N

O
20

.3
6

0.
67

82
53

+2
5.

50
St

ri
ng

-t
o-

Tr
ee

SM
T

O
nl

in
e

D
35

O
th

er
Y

E
S

15
.0

8
0.

64
35

88
+1

3.
75

O
nl

in
e

D
R

B
M

T
E

76
O

th
er

Y
E

S
14

.8
2

0.
66

38
51

—
–

R
B

M
T

E
R

B
M

T
F

79
O

th
er

Y
E

S
13

.8
6

0.
66

13
87

—
–

R
B

M
T

F
O

nl
in

e
C

87
O

th
er

Y
E

S
10

.6
4

0.
62

48
27

—
–

O
nl

in
e

C
R

B
M

T
D

96
O

th
er

Y
E

S
15

.2
9

0.
68

33
78

+2
3.

00
R

B
M

T
D

N
A

IS
T

1
46

SM
T

Y
E

S
23

.8
2

0.
72

25
99

+4
0.

50
Tr

av
at

ar
-b

as
ed

Fo
re

st
-t

o-
St

ri
ng

SM
T

Sy
st

em
w

ith
E

xt
ra

D
ic

tio
na

ri
es

N
A

IS
T

2
11

9
SM

T
N

O
23

.2
9

0.
72

35
41

+3
7.

50
Tr

av
at

ar
-b

as
ed

Fo
re

st
-t

o-
St

ri
ng

SM
T

Sy
st

em
N

A
IS

T
3

12
5

SM
T

N
O

23
.4

7
0.

72
36

70
—

–
Tr

av
at

ar
-b

as
ed

Fo
re

st
-t

o-
St

ri
ng

SM
T

Sy
st

em
(T

un
ed

B
L

E
U

+R
IB

E
S)

E
IW

A
11

6
SM

T
an

d
R

B
M

T
Y

E
S

19
.8

6
0.

70
66

86
+2

2.
50

C
om

bi
na

tio
n

of
R

B
M

T
an

d
SP

E
(s

ta
tis

tic
al

po
st

ed
iti

ng
)

K
yo

to
-U

3
13

6
E

B
M

T
N

O
20

.0
2

0.
68

98
29

—
–

O
ur

ba
se

lin
e

sy
st

em
us

in
g

3M
pa

ra
lle

ls
en

te
nc

es
K

yo
to

-U
2

25
6

E
B

M
T

N
O

20
.6

0
0.

70
11

54
+2

1.
25

O
ur

ne
w

ba
se

lin
e

sy
st

em
af

te
rs

ev
er

al
m

od
ifi

ca
tio

ns
K

yo
to

-U
1

26
2

E
B

M
T

N
O

21
.0

7
0.

69
89

53
+2

5.
00

O
ur

ne
w

ba
se

lin
e

sy
st

em
af

te
rs

ev
er

al
m

od
ifi

ca
tio

ns
+

20
-b

es
tp

ar
se

s,
K

N
7,

R
N

N
L

M
re

ra
nk

in
g

T
M

U
2

30
0

SM
T

N
O

15
.5

5
0.

64
46

98
-1

7.
25

O
ur

ba
se

lin
e

sy
st

em
w

ith
pr

eo
rd

er
in

g
m

et
ho

d
T

M
U

1
30

1
SM

T
N

O
15

.9
5

0.
64

88
79

-1
7.

20
O

ur
ba

se
lin

e
sy

st
em

w
ith

an
ot

he
rp

re
or

de
ri

ng
m

et
ho

d
T

M
U

3
30

7
SM

T
N

O
15

.4
0

0.
61

31
19

—
–

O
ur

ba
se

lin
e

sy
st

em
N

II
1

27
1

SM
T

N
O

17
.4

7
0.

63
08

25
-5

.7
5

O
ur

B
as

el
in

e
N

II
2

27
2

SM
T

N
O

17
.0

1
0.

61
08

33
-1

4.
25

O
ur

B
as

el
in

e
w

ith
Pr

eo
rd

er
in

g
Se

ns
e

1
16

4
SM

T
N

O
18

.8
2

0.
64

62
04

+1
.2

5
Pa

ra
ph

ra
se

m
ax

10
Se

ns
e

2
18

5
SM

T
N

O
18

.5
7

0.
64

03
93

—
–

B
as

el
in

e
SM

T
Se

ns
e

3
19

1
SM

T
N

O
18

.0
0

0.
64

13
77

—
–

C
on

te
xt

se
ns

iti
ve

SM
T

Se
ns

e
4

20
5

SM
T

N
O

18
.8

7
0.

64
61

33
—

–
SM

T
w

ith
le

xi
co

n
Se

ns
e

5
20

6
SM

T
N

O
18

.9
1

0.
63

73
75

—
–

SM
T

w
ith

le
xi

co
n

X
5

TO
SH

IB
A

2
24

0
R

B
M

T
Y

E
S

15
.6

9
0.

68
71

22
+2

0.
25

R
B

M
T

sy
st

em
TO

SH
IB

A
1

24
1

SM
T

an
d

R
B

M
T

Y
E

S
20

.6
1

0.
70

79
36

+2
3.

25
R

B
M

T
w

ith
SP

E
(S

ta
tis

tic
al

Po
st

E
di

tin
g)

sy
st

em

Ta
bl

e
11

:J
E

su
bm

is
si

on
s

14



SY
ST

E
M

ID
ID

M
E

T
H

O
D

O
T

H
E

R
B

L
E

U
R

IB
E

S
H

U
M

A
N

SY
ST

E
M

D
E

SC
R

IP
T

IO
N

R
E

SO
U

R
C

E
S

ju
m

an
ky

te
a

m
ec

ab
ju

m
an

ky
te

a
m

ec
ab

SM
T

Ph
ra

se
5

SM
T

N
O

27
.4

8
29

.8
0

28
.2

7
0.

68
37

35
0.

69
19

26
0.

69
53

90
—

–
Ph

ra
se

-b
as

ed
SM

T
SM

T
T

2S
12

SM
T

N
O

31
.0

5
33

.4
4

32
.1

0
0.

74
88

83
0.

75
80

31
0.

76
05

16
+3

4.
25

Tr
ee

-t
o-

St
ri

ng
SM

T
O

nl
in

e
A

34
O

th
er

Y
E

S
19

.6
6

21
.6

3
20

.1
7

0.
71

80
19

0.
72

34
86

0.
72

58
48

+4
2.

50
O

nl
in

e
A

R
B

M
T

B
66

O
th

er
Y

E
S

13
.1

8
14

.8
5

13
.4

8
0.

67
19

58
0.

68
07

48
0.

68
26

83
+0

.7
5

R
B

M
T

B
R

B
M

T
A

68
O

th
er

Y
E

S
12

.8
6

14
.4

3
13

.1
6

0.
67

01
67

0.
67

64
64

0.
67

89
34

—
–

R
B

M
T

A
O

nl
in

e
B

91
O

th
er

Y
E

S
17

.0
4

18
.6

7
17

.3
6

0.
68

77
97

0.
69

33
90

0.
69

81
26

—
–

O
nl

in
e

B
R

B
M

T
C

95
O

th
er

Y
E

S
12

.1
9

13
.3

2
12

.1
4

0.
66

83
72

0.
67

26
45

0.
67

60
18

—
–

R
B

M
T

C
SM

T
H

ie
ro

36
7

SM
T

N
O

30
.1

9
32

.5
6

30
.9

4
0.

73
47

05
0.

74
69

78
0.

74
77

22
+3

1.
50

H
ie

ra
rc

hi
ca

lP
hr

as
e-

ba
se

d
SM

T
N

A
IS

T
1

11
8

SM
T

N
O

35
.0

3
37

.1
6

35
.8

1
0.

79
60

79
0.

80
15

20
0.

80
65

81
+5

6.
25

Tr
av

at
ar

-b
as

ed
Fo

re
st

-t
o-

St
ri

ng
SM

T
Sy

st
em

N
A

IS
T

2
12

6
SM

T
N

O
34

.8
4

37
.1

5
35

.6
7

0.
80

17
42

0.
80

70
10

0.
81

10
81

+5
1.

50
Tr

av
at

ar
-b

as
ed

Fo
re

st
-t

o-
St

ri
ng

SM
T

Sy
st

em
(T

un
ed

B
L

E
U

+R
IB

E
S)

K
yo

to
-U

3
13

4
E

B
M

T
N

O
28

.9
3

31
.6

1
29

.5
9

0.
74

39
69

0.
75

57
44

0.
75

65
45

—
–

O
ur

ba
se

lin
e

sy
st

em
us

in
g

3M
pa

ra
lle

ls
en

te
nc

es
K

yo
to

-U
4

18
6

E
B

M
T

N
O

30
.2

5
32

.7
8

30
.8

4
0.

75
56

29
0.

76
52

51
0.

76
64

95
—

–
U

si
ng

n-
be

st
pa

rs
es

an
d

R
N

N
L

M
K

yo
to

-U
2

25
3

E
B

M
T

N
O

29
.7

6
32

.4
6

30
.4

6
0.

75
20

58
0.

76
40

49
0.

76
64

35
+3

3.
75

O
ur

ne
w

ba
se

lin
e

sy
st

em
af

te
rs

ev
er

al
m

od
ifi

ca
tio

ns
K

yo
to

-U
1

26
7

E
B

M
T

N
O

31
.0

9
33

.5
5

31
.7

3
0.

76
64

35
0.

77
09

08
0.

77
15

45
+3

8.
00

O
ur

ne
w

ba
se

lin
e

sy
st

em
af

te
r

se
ve

ra
lm

od
ifi

ca
tio

ns
+

20
-b

es
t

pa
rs

es
,K

N
7,

R
N

N
L

M
re

ra
nk

in
g

W
E

B
L

IO
-E

J1
1

13
2

SM
T

N
O

32
.5

3
34

.8
7

33
.2

6
0.

78
20

66
0.

78
69

02
0.

79
26

16
+4

3.
25

W
eb

lio
Pr

e-
re

or
de

ri
ng

SM
T

Sy
st

em
B

as
el

in
e

W
E

B
L

IO
-E

J1
2

20
2

SM
T

N
O

32
.6

9
35

.0
4

33
.4

0
0.

78
50

15
0.

79
00

66
0.

79
50

27
+3

6.
00

W
eb

lio
Pr

e-
re

or
de

ri
ng

SM
T

Sy
st

em
(w

ith
fo

re
st

in
pu

ts
)

SA
S

M
T

26
4

SM
T

N
O

30
.4

7
33

.0
0

31
.4

7
0.

75
94

15
0.

77
09

48
0.

77
16

05
+2

7.
50

Sy
nt

ac
tic

re
or

de
ri

ng
H

ie
ra

rc
hi

ca
lS

M
T

(u
si

ng
pa

rt
of

da
ta

)
Se

ns
e

2
16

3
SM

T
N

O
27

.8
8

30
.2

7
28

.7
2

0.
69

07
18

0.
69

93
34

0.
70

31
39

—
–

Pa
ra

ph
ra

se
m

ax
10

Se
ns

e
1

18
4

SM
T

N
O

27
.9

2
30

.1
8

28
.6

6
0.

69
04

64
0.

70
05

83
0.

70
30

49
+3

.7
5

B
as

el
in

e
SM

T
Se

ns
e

3
19

0
SM

T
N

O
26

.5
9

28
.4

6
27

.1
5

0.
68

44
67

0.
69

46
78

0.
69

72
57

—
–

C
on

te
xt

se
ns

iti
ve

SM
T

Se
ns

e
4

26
5

SM
T

N
O

27
.0

0
29

.1
5

27
.8

1
0.

68
11

94
0.

68
96

23
0.

69
35

60
—

–
SM

T
w

ith
20

x
le

xi
co

n
Se

ns
e

5
27

4
SM

T
N

O
27

.3
3

29
.5

4
28

.1
6

0.
67

96
66

0.
68

88
01

0.
69

10
11

—
–

SM
T

w
ith

le
xi

co
n

X
5

Ta
bl

e
12

:E
J

su
bm

is
si

on
s

15



SY
ST

E
M

ID
ID

M
E

T
H

O
D

O
T

H
E

R
B

L
E

U
R

IB
E

S
H

U
M

A
N

SY
ST

E
M

D
E

SC
R

IP
T

IO
N

R
E

SO
U

R
C

E
S

ky
te

a
st

an
fo

rd
(c

tb
)

st
an

fo
rd

(p
ku

)
ky

te
a

st
an

fo
rd

(c
tb

)
st

an
fo

rd
(p

ku
)

SM
T

H
ie

ro
3

SM
T

N
O

27
.7

1
27

.7
0

27
.3

5
0.

80
91

28
0.

80
95

61
0.

81
13

94
+3

.7
5

H
ie

ra
rc

hi
ca

lP
hr

as
e-

ba
se

d
SM

T
SM

T
Ph

ra
se

7
SM

T
N

O
27

.9
6

28
.0

1
27

.6
8

0.
78

89
61

0.
79

02
63

0.
79

09
37

—
–

Ph
ra

se
-b

as
ed

SM
T

SM
T

S2
T

10
SM

T
N

O
28

.6
5

28
.6

5
28

.3
5

0.
80

76
06

0.
80

94
57

0.
80

84
17

+1
4.

00
St

ri
ng

-t
o-

Tr
ee

SM
T

O
nl

in
e

D
37

O
th

er
Y

E
S

9.
37

8.
93

8.
84

0.
60

69
05

0.
60

63
28

0.
60

41
49

-1
4.

50
O

nl
in

e
D

O
nl

in
e

C
21

6
O

th
er

Y
E

S
7.

26
7.

01
6.

72
0.

61
28

08
0.

61
30

75
0.

61
15

63
—

–
O

nl
in

e
C

R
B

M
T

B
24

3
R

B
M

T
N

O
17

.8
6

17
.7

5
17

.4
9

0.
74

48
18

0.
74

58
85

0.
74

37
94

-2
0.

00
R

B
M

T
B

R
B

M
T

C
24

4
R

B
M

T
N

O
9.

62
9.

96
9.

59
0.

64
22

78
0.

64
87

58
0.

64
53

85
—

–
R

B
M

T
C

N
A

IS
T

1
12

2
SM

T
N

O
30

.5
3

30
.4

6
30

.2
5

0.
81

80
40

0.
81

94
06

0.
81

94
92

+1
7.

75
Tr

av
at

ar
-b

as
ed

Fo
re

st
-t

o-
St

ri
ng

SM
T

Sy
st

em
N

A
IS

T
2

12
3

SM
T

N
O

29
.8

3
29

.7
7

29
.5

4
0.

82
96

27
0.

83
08

39
0.

83
05

29
+1

.2
5

Tr
av

at
ar

-b
as

ed
Fo

re
st

-t
o-

St
ri

ng
SM

T
Sy

st
em

(T
un

ed
B

L
E

U
+R

IB
E

S)
K

yo
to

-U
3

18
E

B
M

T
N

O
26

.6
9

26
.4

8
26

.3
0

0.
79

64
02

0.
79

80
84

0.
79

83
83

—
–

O
ur

ba
se

lin
e

sy
st

em
K

yo
to

-U
1

25
7

E
B

M
T

N
O

27
.2

1
27

.0
2

26
.8

3
0.

79
12

70
0.

79
21

66
0.

79
07

43
-0

.7
5

O
ur

ne
w

ba
se

lin
e

sy
st

em
af

te
rs

ev
er

al
m

od
ifi

ca
tio

ns
K

yo
to

-U
2

25
9

E
B

M
T

N
O

27
.6

7
27

.4
4

27
.3

4
0.

78
83

21
0.

78
90

69
0.

78
82

06
-8

.7
5

O
ur

ne
w

ba
se

lin
e

sy
st

em
af

te
r

se
ve

ra
lm

od
ifi

ca
tio

ns
+

20
-b

es
t

pa
rs

es
,K

N
7,

R
N

N
L

M
re

ra
nk

in
g

B
JT

U
N

L
P

22
4

SM
T

N
O

24
.1

2
23

.7
6

23
.5

5
0.

79
48

34
0.

79
61

86
0.

79
30

54
-3

.7
5

SM
T

Se
ns

e
2

17
5

SM
T

N
O

27
.9

2
28

.0
3

27
.6

7
0.

79
38

76
0.

79
65

89
0.

79
73

32
—

–
SM

T
Se

ns
e

1
20

1
SM

T
N

O
23

.0
9

22
.9

4
23

.0
4

0.
77

94
95

0.
77

95
02

0.
78

02
62

+1
0.

00
C

ha
ra

ct
er

ba
se

d
SM

T
N

IC
T

26
0

SM
T

N
O

27
.9

8
28

.1
8

27
.8

4
0.

80
60

70
0.

80
86

84
0.

80
78

09
+6

.5
0

Pr
e-

re
or

de
ri

ng
fo

r
ph

ra
se

-b
as

ed
SM

T
(d

ep
en

de
nc

y
pa

rs
in

g
+

m
an

ua
lr

ul
es

)
TO

SH
IB

A
2

23
6

R
B

M
T

Y
E

S
19

.2
8

18
.9

3
18

.8
2

0.
76

44
91

0.
76

53
46

0.
76

39
31

-5
.2

5
R

B
M

T
sy

st
em

TO
SH

IB
A

1
23

8
SM

T
an

d
R

B
M

T
Y

E
S

27
.4

2
26

.8
2

26
.7

9
0.

80
44

44
0.

80
33

02
0.

80
39

80
+0

.7
5

R
B

M
T

w
ith

SP
E

(S
ta

tis
tic

al
Po

st
E

di
tin

g)
sy

st
em

W
A

SU
IP

S
1

37
1

SM
T

N
O

22
.7

1
22

.4
9

22
.3

9
0.

77
63

23
0.

77
76

15
0.

77
73

27
—

–
O

ur
ba

se
lin

e
sy

st
em

(s
eg

m
en

ta
tio

n
to

ol
s:

ur
he

en
an

d
m

ec
ab

,
m

os
es

:1
.0

).
W

A
SU

IP
S

2
37

3
SM

T
Y

E
S

24
.7

0
24

.2
5

24
.2

8
0.

79
00

30
0.

79
04

60
0.

79
08

98
—

–
O

ur
ba

se
lin

e
sy

st
em

+
ad

di
tio

na
l

qu
as

i-
pa

ra
lle

l
co

rp
us

(s
eg

-
m

en
ta

tio
n

to
ol

s:
ur

he
en

an
d

m
ec

ab
,m

os
es

:1
.0

).
W

A
SU

IP
S

3
37

6
SM

T
N

O
25

.4
4

25
.0

4
24

.9
8

0.
79

42
44

0.
79

39
45

0.
79

48
23

—
–

O
ur

ba
se

lin
e

sy
st

em
(s

eg
m

en
ta

tio
n

to
ol

s:
ur

he
en

an
d

m
ec

ab
,

m
os

es
:2

.1
.1

).
W

A
SU

IP
S

4
37

7
SM

T
Y

E
S

25
.6

0
25

.1
0

25
.0

7
0.

79
47

16
0.

79
57

86
0.

79
55

94
—

–
O

ur
ba

se
lin

e
sy

st
em

+
ad

di
tio

na
lq

ua
si

-p
ar

al
le

lc
or

pu
s

se
gm

en
-

ta
tio

n
to

ol
s:

ur
he

en
an

d
m

ec
ab

,m
os

es
:2

.1
.1

).
W

A
SU

IP
S

5
38

1
SM

T
N

O
22

.0
1

21
.8

1
21

.6
1

0.
76

74
18

0.
76

74
14

0.
76

60
92

—
–

O
ur

ba
se

lin
e

sy
st

em
(s

eg
m

en
ta

tio
n

to
ol

s:
ky

te
a,

m
os

es
:1

.0
).

W
A

SU
IP

S
6

38
2

SM
T

Y
E

S
22

.2
0

22
.0

2
21

.9
1

0.
77

19
52

0.
77

33
41

0.
77

21
07

—
–

O
ur

ba
se

lin
e

sy
st

em
+

ad
di

tio
na

l
qu

as
i-

pa
ra

lle
l

co
rp

us
(s

eg
-

m
en

ta
tio

n
to

ol
s:

ky
te

a,
m

os
es

:1
.0

).
W

A
SU

IP
S

7
38

5
SM

T
N

O
25

.4
5

25
.1

0
25

.0
1

0.
79

38
19

0.
79

33
08

0.
79

30
29

—
–

O
ur

ba
se

lin
e

sy
st

em
(s

eg
m

en
ta

tio
n

to
ol

s:
ky

te
a,

m
os

es
:2

.1
.1

).
W

A
SU

IP
S

8
38

6
SM

T
Y

E
S

25
.6

8
25

.0
1

25
.1

1
0.

79
57

21
0.

79
55

04
0.

79
51

29
—

–
O

ur
ba

se
lin

e
sy

st
em

+
ad

di
tio

na
l

qu
as

i-
pa

ra
lle

l
co

rp
us

(s
eg

-
m

en
ta

tio
n

to
ol

s:
ky

te
a,

m
os

es
:2

.1
.1

).
W

A
SU

IP
S

9
38

9
SM

T
N

O
25

.0
8

24
.8

1
24

.6
4

0.
79

04
98

0.
79

14
30

0.
79

01
42

—
–

O
ur

ba
se

lin
e

sy
st

em
(s

eg
m

en
ta

tio
n

to
ol

s:
st

an
fo

rd
-c

tb
an

d
ju

-
m

an
,m

os
es

:2
.1

.1
).

W
A

SU
IP

S
10

39
0

SM
T

Y
E

S
25

.6
3

25
.3

0
25

.1
8

0.
79

46
46

0.
79

53
07

0.
79

40
24

—
–

O
ur

ba
se

lin
e

sy
st

em
+

ad
di

tio
na

l
qu

as
i-

pa
ra

lle
l

co
rp

us
(s

eg
-

m
en

ta
tio

n
to

ol
s:

st
an

fo
rd

-c
tb

an
d

ju
m

an
,m

os
es

:2
.1

.1
).

Ta
bl

e
13

:J
C

su
bm

is
si

on
s

16



SY
ST

E
M

ID
ID

M
E

T
H

O
D

O
T

H
E

R
B

L
E

U
R

IB
E

S
H

U
M

A
N

SY
ST

E
M

D
E

SC
R

IP
T

IO
N

R
E

SO
U

R
C

E
S

ju
m

an
ky

te
a

m
ec

ab
ju

m
an

ky
te

a
m

ec
ab

SM
T

H
ie

ro
4

SM
T

N
O

35
.4

3
35

.9
1

35
.6

4
0.

81
04

06
0.

79
87

26
0.

80
76

65
+4

.7
5

H
ie

ra
rc

hi
ca

lP
hr

as
e-

ba
se

d
SM

T
SM

T
Ph

ra
se

8
SM

T
N

O
34

.6
5

35
.1

6
34

.7
7

0.
77

24
98

0.
76

63
84

0.
77

10
05

—
–

Ph
ra

se
-b

as
ed

SM
T

SM
T

T
2S

13
SM

T
N

O
36

.5
2

37
.0

7
36

.6
4

0.
82

52
92

0.
82

04
90

0.
82

50
25

+1
6.

00
Tr

ee
-t

o-
St

ri
ng

SM
T

O
nl

in
e

A
36

O
th

er
Y

E
S

11
.6

3
13

.2
1

11
.8

7
0.

59
59

25
0.

59
81

72
0.

59
85

73
-2

1.
75

O
nl

in
e

A
O

nl
in

e
B

21
5

O
th

er
Y

E
S

10
.4

8
11

.2
6

10
.4

7
0.

60
07

33
0.

59
60

06
0.

60
07

06
—

–
O

nl
in

e
B

R
B

M
T

A
23

9
R

B
M

T
N

O
9.

37
9.

87
9.

35
0.

66
62

77
0.

65
24

02
0.

66
17

30
-3

7.
75

R
B

M
T

A
R

B
M

T
D

24
2

R
B

M
T

N
O

8.
39

8.
70

8.
30

0.
64

11
89

0.
62

64
00

0.
63

33
19

—
–

R
B

M
T

D
N

A
IS

T
1

12
0

SM
T

N
O

40
.1

1
41

.2
9

40
.3

0
0.

84
24

77
0.

83
48

24
0.

84
22

35
+5

0.
75

Tr
av

at
ar

-b
as

ed
Fo

re
st

-t
o-

St
ri

ng
SM

T
Sy

st
em

N
A

IS
T

2
12

4
SM

T
N

O
40

.2
1

40
.8

2
40

.1
5

0.
84

54
86

0.
83

80
92

0.
84

56
25

+3
8.

00
Tr

av
at

ar
-b

as
ed

Fo
re

st
-t

o-
St

ri
ng

SM
T

Sy
st

em
(T

un
ed

B
L

E
U

+R
IB

E
S)

E
IW

A
2

13
7

R
B

M
T

Y
E

S
18

.6
9

18
.3

3
18

.3
2

0.
74

01
83

0.
72

02
81

0.
73

24
66

—
–

R
B

M
T

pl
us

us
er

di
ct

io
na

ry
E

IW
A

1
13

8
SM

T
an

d
R

B
M

T
Y

E
S

33
.5

3
33

.7
4

33
.8

7
0.

81
13

50
0.

80
05

06
0.

80
85

04
+1

5.
00

R
B

M
T

w
ith

us
er

di
ct

io
na

ry
pl

us
SP

E
(s

ta
tis

tic
al

po
st

ed
iti

ng
)

K
yo

to
-U

3
13

3
E

B
M

T
N

O
33

.2
6

35
.0

9
33

.6
2

0.
79

16
80

0.
78

71
05

0.
79

12
69

—
–

U
si

ng
n-

be
st

pa
rs

es
an

d
R

N
N

L
M

K
yo

to
-U

4
13

5
E

B
M

T
N

O
32

.6
8

33
.3

0
32

.4
5

0.
78

62
29

0.
78

30
16

0.
78

63
52

—
–

O
ur

ba
se

lin
e

sy
st

em
K

yo
to

-U
2

25
8

E
B

M
T

N
O

33
.5

7
34

.4
3

33
.4

5
0.

80
09

49
0.

79
53

90
0.

80
09

86
+6

.0
0

O
ur

ne
w

ba
se

lin
e

sy
st

em
af

te
rs

ev
er

al
m

od
ifi

ca
tio

ns
K

yo
to

-U
1

26
8

E
B

M
T

N
O

34
.7

5
35

.8
9

34
.8

3
0.

80
26

29
0.

79
86

31
0.

80
29

30
+7

.5
0

O
ur

ne
w

ba
se

lin
e

sy
st

em
af

te
r

se
ve

ra
lm

od
ifi

ca
tio

ns
+

20
-b

es
t

pa
rs

es
,K

N
7,

R
N

N
L

M
re

ra
nk

in
g

SA
S

M
T

2
23

2
SM

T
N

O
36

.5
8

36
.2

2
36

.1
0

0.
82

21
80

0.
80

75
35

0.
81

73
68

—
–

Sy
nt

ac
tic

re
or

de
ri

ng
ph

ra
se

-b
as

ed
SM

T
(S

A
S

to
ke

n
to

ol
)

SA
S

M
T

1
26

3
SM

T
N

O
37

.4
2

37
.6

5
37

.0
7

0.
83

41
70

0.
82

55
51

0.
83

30
48

+2
2.

50
Sy

nt
ac

tic
re

or
de

ri
ng

H
ie

ra
rc

hi
ca

lS
M

T
(u

si
ng

SA
S

to
ke

n
to

ol
)

Se
ns

e
2

17
4

SM
T

N
O

34
.5

6
35

.0
8

34
.6

4
0.

77
19

75
0.

76
64

70
0.

77
10

81
—

–
SM

T
Se

ns
e

1
20

0
SM

T
N

O
33

.6
6

33
.8

6
33

.4
6

0.
78

94
95

0.
77

43
38

0.
78

40
12

-1
.0

0
C

ha
ra

ct
er

ba
se

d
SM

T
W

A
SU

IP
S

1
36

9
SM

T
N

O
27

.6
6

28
.0

9
28

.2
0

0.
77

91
83

0.
76

29
49

0.
77

08
46

—
–

O
ur

ba
se

lin
e

sy
st

em
(s

eg
m

en
ta

tio
n

to
ol

s:
ur

he
en

an
d

m
ec

ab
,

m
os

es
:1

.0
).

W
A

SU
IP

S
2

37
0

SM
T

Y
E

S
30

.4
4

30
.9

2
30

.8
6

0.
78

98
24

0.
77

31
42

0.
78

14
75

—
–

O
ur

ba
se

lin
e

sy
st

em
+

ad
di

tio
na

l
qu

as
i-

pa
ra

lle
l

co
rp

us
(s

eg
-

m
en

ta
tio

n
to

ol
s:

ur
he

en
an

d
m

ec
ab

,m
os

es
:1

.0
).

W
A

SU
IP

S
3

37
4

SM
T

N
O

31
.8

7
32

.2
6

32
.2

6
0.

79
43

03
0.

77
78

76
0.

78
64

22
—

–
O

ur
ba

se
lin

e
sy

st
em

(s
eg

m
en

ta
tio

n
to

ol
s:

ur
he

en
an

d
m

ec
ab

,
m

os
es

:2
.1

.1
).

W
A

SU
IP

S
4

37
5

SM
T

Y
E

S
32

.1
9

32
.5

5
32

.5
4

0.
79

58
38

0.
78

00
27

0.
78

75
91

—
–

O
ur

ba
se

lin
e

sy
st

em
+

ad
di

tio
na

l
qu

as
i-

pa
ra

lle
l

co
rp

us
(s

eg
-

m
en

ta
tio

n
to

ol
s:

ur
he

en
an

d
m

ec
ab

,m
os

es
:2

.1
.1

).
W

A
SU

IP
S

5
37

9
SM

T
N

O
27

.3
7

28
.2

8
27

.4
3

0.
77

44
23

0.
75

37
49

0.
76

70
73

—
–

O
ur

ba
se

lin
e

sy
st

em
(s

eg
m

en
ta

tio
n

to
ol

s:
ky

te
a,

m
os

es
:1

.0
).

W
A

SU
IP

S
6

38
0

SM
T

Y
E

S
27

.8
6

28
.8

9
28

.0
0

0.
77

65
50

0.
75

67
21

0.
76

94
09

—
–

O
ur

ba
se

lin
e

sy
st

em
+

ad
di

tio
na

l
qu

as
i-

pa
ra

lle
l

co
rp

us
(s

eg
-

m
en

ta
tio

n
to

ol
s:

ky
te

a,
m

os
es

:1
.0

).
W

A
SU

IP
S

7
38

3
SM

T
N

O
32

.0
8

33
.0

9
32

.1
8

0.
79

32
30

0.
77

51
68

0.
78

76
65

—
–

O
ur

ba
se

lin
e

sy
st

em
(s

eg
m

en
ta

tio
n

to
ol

s:
ky

te
a,

m
os

es
:2

.1
.1

).
W

A
SU

IP
S

8
38

4
SM

T
Y

E
S

32
.4

3
33

.3
6

32
.4

8
0.

79
62

20
0.

77
80

75
0.

78
96

57
—

–
O

ur
ba

se
lin

e
sy

st
em

+
ad

di
tio

na
l

qu
as

i-
pa

ra
lle

l
co

rp
us

(s
eg

-
m

en
ta

tio
n

to
ol

s:
ky

te
a,

m
os

es
:2

.1
.1

).
W

A
SU

IP
S

9
38

7
SM

T
N

O
32

.5
2

32
.6

9
32

.4
7

0.
79

60
59

0.
78

04
02

0.
79

01
07

—
–

O
ur

ba
se

lin
e

sy
st

em
(s

eg
m

en
ta

tio
n

to
ol

s:
st

an
fo

rd
-c

tb
an

d
ju

-
m

an
,m

os
es

:2
.1

.1
).

W
A

SU
IP

S
10

38
8

SM
T

Y
E

S
32

.6
5

32
.8

1
32

.5
9

0.
79

67
77

0.
78

17
33

0.
79

12
19

—
–

O
ur

ba
se

lin
e

sy
st

em
+

ad
di

tio
na

l
qu

as
i-

pa
ra

lle
l

co
rp

us
(s

eg
-

m
en

ta
tio

n
to

ol
s:

st
an

fo
rd

-c
tb

an
d

ju
m

an
,m

os
es

:2
.1

.1
).

Ta
bl

e
14

:C
J

su
bm

is
si

on
s

17



References
Jingsheng Cai, Yujie Zhang, Hua Shan, and Jinan Xu.

2014. System Description: Dependency-based Pre-
ordering for Japanese-Chinese Machine Translation.
In Proceedings of the 1st Workshop on Asian Trans-
lation (WAT2014).

Chenchen Ding, Masao Utiyama, Eiichiro Sumita, and
Mikio Yamamoto. 2014. Word Order Does NOT
Differ Significantly Between Chinese and Japanese.
In Proceedings of the 1st Workshop on Asian Trans-
lation (WAT2014).

Terumasa Ehara. 2014. A machine translation system
combining rule-based machine translation and sta-
tistical post-editing. In Proceedings of the 1st Work-
shop on Asian Translation (WAT2014).

J.L. Fleiss et al. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378–382.

Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modified
kneser-ney language model estimation. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 690–696, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.

Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A unified framework for phrase-based, hierarchical,
and syntax-based statistical machine translation. In
Proceedings of the International Workshop on Spo-
ken Language Translation, pages 152–159.

Sho Hoshino, Hubert Soyer, Yusuke Miyao, and Akiko
Aizawa. 2014. Japanese to English Machine Trans-
lation using Preordering and Compositional Dis-
tributed Semantics. In Proceedings of the 1st Work-
shop on Asian Translation (WAT2014).

Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic
evaluation of translation quality for distant language
pairs. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’10, pages 944–952, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), demonstration session.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.

T. Kudo. 2005. Mecab : Yet another
part-of-speech and morphological analyzer.
http://mecab.sourceforge.net/.

Sadao Kurohashi, Toshihisa Nakamura, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Improve-
ments of Japanese morphological analyzer JUMAN.
In Proceedings of The International Workshop on
Sharable Natural Language, pages 22–28.

Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
japanese morphological analysis. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies: Short Papers - Volume 2, HLT ’11, pages 529–
533, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Graham Neubig. 2014. Forest-to-String SMT for
Asian Language Translation: NAIST at WAT 2014.
In Proceedings of the 1st Workshop on Asian Trans-
lation (WAT2014).

Kenichi Ohwada, Ryosuke Miyazaki, and Mamoru
Komachi. 2014. Predicate-Argument Structure-
based Preordering for Japanese-English Statistical
Machine Translation of Scientific Papers. In Pro-
ceedings of the 1st Workshop on Asian Translation
(WAT2014).

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311–
318.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433–440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.

John Richardson, Fabien Cromières, Toshiaki
Nakazawa, and Sadao Kurohashi. 2014. Ky-
otoEBMT System Description for the 1st Workshop
on Asian Translation. In Proceedings of the 1st
Workshop on Asian Translation (WAT2014).

Satoshi Sonoh, Satoshi Kinoshita, Hiroyuki Tanaka,
and Satoshi Kamatani. 2014. Toshiba MT System
Description for the WAT2014 Workshop. In Pro-
ceedings of the 1st Workshop on Asian Translation
(WAT2014).

Liling Tan and Francis Bond. 2014. Manipulating In-
put Data in Machine Translation. In Proceedings of
the 1st Workshop on Asian Translation (WAT2014).

Huihsin Tseng. 2005. A conditional random field word
segmenter. In In Fourth SIGHAN Workshop on Chi-
nese Language Processing.

18



Masao Utiyama and Hitoshi Isahara. 2007. A
japanese-english patent parallel corpus. In MT sum-
mit XI, pages 475–482.

Rui Wang, Xu Yang, and Yan Gao. 2014. The SAS
Statistical Machine Translation System for WAT
2014. In Proceedings of the 1st Workshop on Asian
Translation (WAT2014).

Wei Yang and Yves Lepage. 2014. Consistent
Improvement in Translation Quality of Chinese–
Japanese Technical Texts by Adding Additional
Quasi-parallel Training Data. In Proceedings of the
1st Workshop on Asian Translation (WAT2014).

Zhongyuan Zhu. 2014. Weblio Pre-reordering Statisti-
cal Machine Translation System. In Proceedings of
the 1st Workshop on Asian Translation (WAT2014).

19




