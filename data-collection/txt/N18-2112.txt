



















































Feudal Reinforcement Learning for Dialogue Management in Large Domains


Proceedings of NAACL-HLT 2018, pages 714–719
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Feudal Reinforcement Learning for Dialogue Management in Large
Domains

Iñigo Casanueva1∗, Paweł Budzianowski1, Pei-Hao Su2,
Stefan Ultes1, Lina Rojas-Barahona1, Bo-Hsiang Tseng1 and Milica Gašić1

1Department of Engineering, University of Cambridge, UK
2PolyAI Limited, London, UK

ic340@cam.ac.uk

Abstract

Reinforcement learning (RL) is a promising
approach to solve dialogue policy optimisa-
tion. Traditional RL algorithms, however, fail
to scale to large domains due to the curse
of dimensionality. We propose a novel Dia-
logue Management architecture, based on Feu-
dal RL, which decomposes the decision into
two steps; a first step where a master policy
selects a subset of primitive actions, and a sec-
ond step where a primitive action is chosen
from the selected subset. The structural in-
formation included in the domain ontology is
used to abstract the dialogue state space, tak-
ing the decisions at each step using different
parts of the abstracted state. This, combined
with an information sharing mechanism be-
tween slots, increases the scalability to large
domains. We show that an implementation
of this approach, based on Deep-Q Networks,
significantly outperforms previous state of the
art in several dialogue domains and environ-
ments, without the need of any additional re-
ward signal.

1 Introduction

Task-oriented Spoken Dialogue Systems (SDS),
in the form of personal assistants, have recently
gained much attention in both academia and in-
dustry. One of the most important modules of a
SDS is the Dialogue Manager (DM) (or policy),
the module in charge of deciding the next action in
each dialogue turn. Reinforcement Learning (RL)
(Sutton and Barto, 1999) has been studied for sev-
eral years as a promising approach to model dia-
logue management (Levin et al., 1998; Henderson
et al., 2008; Pietquin et al., 2011; Young et al.,
2013; Casanueva et al., 2015; Su et al., 2016).
However, as the dialogue state space increases, the
number of possible trajectories needed to be ex-

∗Currently at PolyAI, inigo@poly-ai.com

plored grows exponentially, making traditional RL
methods not scalable to large domains.

Hierarchical RL (HRL), in the form of tempo-
ral abstraction, has been proposed in order to miti-
gate this problem (Cuayáhuitl et al., 2010, 2016;
Budzianowski et al., 2017; Peng et al., 2017).
However, proposed HRL methods require that the
task is defined in a hierarchical structure, which is
usually handcrafted. In addition, they usually re-
quire additional rewards for each subtask. Space
abstraction, instead, has been successfully applied
to dialogue tasks such as Dialogue State Track-
ing (DST) (Henderson et al., 2014b), and pol-
icy transfer between domains (Gašić et al., 2013,
2015; Wang et al., 2015). For DST, a set of bi-
nary classifiers can be defined for each slot, with
shared parameters, learning a general way to track
slots. The policy transfer method presented in
(Wang et al., 2015), named Domain Independent
Parametrisation (DIP), transforms the belief state
into a slot-dependent fixed size representation us-
ing a handcrafted feature function. This idea could
also be applied to large domains, since it can be
used to learn a general way to act in any slot.

In slot-filling dialogues, a HRL method that re-
lies on space abstraction, such as Feudal RL (FRL)
(Dayan and Hinton, 1993), should allow RL scale
to domains with a large number of slots. FRL di-
vides a task spatially rather than temporally, de-
composing the decisions in several steps and using
different abstraction levels in each sub-decision.
This framework is especially useful in RL tasks
with large discrete action spaces, making it very
attractive for large domain dialogue management.

In this paper, we introduce a Feudal Dialogue
Policy which decomposes the decision in each turn
into two steps. In a first step, the policy decides if
it takes a slot independent or slot dependent ac-
tion. Then, the state of each slot sub-policy is ab-
stracted to account for features related to that slot,

714



and a primitive action is chosen from the previ-
ously selected subset. Our model does not require
any modification of the reward function and the
hierarchical architecture is fully specified by the
structured database representation of the system
(i.e. the ontology), requiring no additional design.

2 Background

Dialogue management can be cast as a continu-
ous MDP (Young et al., 2013) composed of a con-
tinuous multivariate belief state space B, a finite
set of actions A and a reward function R(bt, at).
At a given time t, the agent observes the belief
state bt ∈ B, executes an action at ∈ A and re-
ceives a reward rt ∈ R drawn from R(bt, at). The
action taken, a, is decided by the policy, defined
as the function π(b) = a. For any policy π and
b ∈ B, the Q-value function can be defined as the
expected (discounted) returnR, starting from state
b, taking action a, and then following policy π un-
til the end of the dialogue at time step T :

Qπ(b, a) = E{R|bt = b, at = a} (1)

where R =
∑T−1

τ=t γ
(τ−t)rτ and γ is a discount

factor, with 0 ≤ γ ≤ 1.
The objective of RL is to find an optimal pol-

icy π∗, i.e. a policy that maximizes the expected
return in each belief state. In Value-based algo-
rithms, the optimal policy can be found by greed-
ily taking the action which maximises Qπ(b, a).

In slot-filling SDSs the belief state space B is
defined by the ontology, a structured representa-
tion of a database of entities that the user can re-
trieve by talking to the system. Each entity has
a set of properties, refereed to as slots S, where
each of the slots can take a value from the set Vs.
The belief state b is then defined as the concate-
nation of the probability distribution of each slot,
plus a set of general features (e.g. the commu-
nication function used by the user, the database
search method...) (Henderson et al., 2014a). The
setA is defined as a set of summary actions, where
the actions can be either slot dependent (e.g. re-
quest(food), confirm(area)...) or slot independent1

(e.g. hello(), inform()...).
The belief space B is defined by the ontol-

ogy, therefore belief states of different domains
will have different shapes. In order to transfer

1We include the summary actions dependent on all the
slots, such as inform(), in this group.

𝜙"#(𝑏)

𝜋"#

Main	belief	state	(𝑏)

𝜋"(
𝜋")

𝜙"((𝑏)
𝜙")(𝑏)

𝜋*

𝑄,(-
* …𝑄,)-

* 𝑄
,(/
"( …𝑄,)/

"( 𝑄
,(/
"# …𝑄,)/

"# 𝑄
,(/
") …𝑄

,)/
")

𝑄,-0
1 𝑄,/0

1

…

…

…𝜙*(𝑏)

Argmax over	slot	
independent	primitives

Argmax over	slots	and	slot	
dependent	primitives

Argmax
over master	
actions

𝜙1(𝑏)

𝜋1𝑏*

𝑏1
𝑏"( 𝑏"#

𝑏")

𝜋2

Figure 1: Feudal dialogue architecture used in this
work. The sub-policies surrounded by the dashed line
have shared parameters. The simple lines show the data
flow and the double lines the sub-policy decisions.

knowledge between domains, Domain Indepen-
dent Parametrization (DIP) (Wang et al., 2015)
proposes to abstract the belief state b into a fixed
size representation. As each action is either slot in-
dependent or dependent on a slot s, a feature func-
tion φdip(b, s) can be defined, where s ∈ S∪si and
si stands for slot independent actions. Therefore,
in order to compute the policy, Q(b, a) can be ap-
proximated as Q(φdip(b, s), a), where s is the slot
associated to action a.

Wang et al. (2015) presents a handcrafted fea-
ture function φdip(b, s). It includes the slot inde-
pendent features of the belief state, a summarised
representation of the joint belief state, and a sum-
marised representation of the belief state of the
slot s. Section 4 gives a more detailed description
of the φdip(b, s) function used in this work.

3 Feudal dialogue management

FRL decomposes the policy decision π(b) = a in
each turn into several sub-decisions, using differ-
ent abstracted parts of the belief state in each sub-
decision. The objective of a task oriented SDS
is to fulfill the users goal, but as the goal is not
observable for the SDS, the SDS needs to gather
enough information to correctly fulfill it. There-
fore, in each turn, the DM can decompose its de-
cision in two steps: first, decide between taking
an action in order to gather information about the
user goal (information gathering actions) or tak-
ing an action to fulfill the user goal or a part of it
(information providing actions) and second, select
a (primitive) action to execute from the previously
selected subset. In a slot-filling dialogue, the set of

715



information gathering actions can be defined as the
set of slot dependent actions, while the set of in-
formation providing actions can be defined as the
remaining actions.

The architecture of the feudal policy proposed
by this work is represented schematically in Figure
1. The (primitive) actions are divided between two
subsets; slot independent actions Ai (e.g. hello(),
inform()); and slot dependent actions Ad (e.g. re-
quest(), confirm())2. In addition, a set of master
actionsAm = (ami , amd ) is defined, where ami cor-
responds to taking an action from Ai and amd to
taking an action fromAd. Then, a feature function
φs(b) = bs is defined for each slot s ∈ S , as well
as a slot independent feature function φi(b) = bi
and a master feature function φm(b) = bm. These
feature functions can be handcrafted (e.g. the DIP
feature function introduced in section 2) or any
function approximator can be used (e.g. neural
networks trained jointly with the policy).

Finally, a master policy πm(bm) = am, a slot
independent policy πi(bi) = ai and a set of slot
specific policies πs(bs) = ad, one for each s ∈ S ,
are defined, where am ∈ Am, ai ∈ Ai and
ad ∈ Ad. Contrary to other feudal policies, the
slot specific sub-policies have shared parameters,
in order to generalise between slots (following the
idea used by Henderson et al. (2014b) for DST).
The differences between the slots (size, value dis-
tribution...) are accounted by the feature function
φs(b). Therefore πm(bm) is defined as:

πm(bm) = argmax
am∈Am

Qm(bm, a
m) (2)

If πm(bm) = ami , the sub-policy run is πi:

πi(bi) = argmax
ai∈Ai

Qi(bi, a
i) (3)

Else, if πm(bm) = amd , πd is selected. This policy
runs each slot specific policy, πs, for all s ∈ S,
choosing the action-slot pair that maximises the Q
function over all the slot sub-policies.

πd(bs|∀s ∈ S) = argmax
ad∈Ad,s∈S

Qs(bs, a
d) (4)

Then, the summary action a is constructed by join-
ing ad and s (e.g. if ad=request() and s=food,
then the summary action will be request(food)). A
pseudo-code of the Feudal Dialogue Policy algo-
rithm is given in Appendix A.

2Note that the actions of this set are composed just by the
communication function of the slot dependent actions, thus
reducing the number of actions compared to A.

Domain Code # constraint slots # requests # values
Cambridge Restaurants CR 3 9 268
San Francisco Restaurants SFR 6 11 636
Laptops LAP 11 21 257

Env. 1 Env. 2 Env. 3 Env. 4 Env. 5 Env. 6
SER 0% 0% 15% 15% 15% 30%
Masks on off on off on on
User Std. Std. Std. Std. Unf. Std.

Table 1: Sumarised description of the domains and
environments used in the experiments. Refer to
(Casanueva et al., 2017) for a detailed description.

4 Experimental setup

The models used in the experiments have been im-
plemented using the PyDial toolkit (Ultes et al.,
2017)3 and evaluated on the PyDial benchmark-
ing environment (Casanueva et al., 2017). This
environment presents a set of tasks which span
different size domains, different Semantic Error
Rates (SER), and different configurations of ac-
tion masks and user model parameters (Standard
(Std.) or Unfriendly (Unf.)). Table 1 shows a sum-
marised description of the tasks. The models de-
veloped in this paper are compared to the state-of-
the-art RL algorithms and to the handcrafted pol-
icy presented in the benchmarks.

4.1 DIP-DQN baseline

An implementation of DIP based on Deep-Q
Networks (DQN) (Mnih et al., 2013) is imple-
mented as an additional baseline (Papangelis and
Stylianou, 2017). This policy, named DIP-DQN,
uses the same hyperparameters as the DQN im-
plementation released in the PyDial benchmarks.
A DIP feature function based in the description in
(Wang et al., 2015) is used, φdip(b, s) = ψ0(b) ⊕
ψj(b)⊕ ψd(b, s), where:
• ψ0(b) accounts for general features of the belief
state, such as the database search method.
• ψj(b) accounts for features of the joint belief
state, such as the entropy of the joint belief.
• ψd(b, s) accounts for features of the marginal
distribution of slot s, such as the entropy of s.
Appendix B shows a detailed description of the
DIP features used in this work.

4.2 Feudal DQN policy

A Feudal policy based on the architecture de-
scribed in sec. 3 is implemented, named FDQN.
Each sub-policy is constructed by a DQN policy

3The implementation of the models can be obtained in
www.pydial.org

716



Feudal-DQN DIP-DQN Bnch. Hdc.
Task Suc. Rew. Suc. Rew. Rew. Rew.

E
nv

.1
CR 89.3% 11.7 48.8% -2.8 13.5 14.0
SFR 71.1% 7.1 25.8% -7.4 11.7 12.4
LAP 65.5% 5.7 26.6% -8.8 10.5 11.7

E
nv

.2

CR 97.8% 13.1 85.5% 9.6 12.2 14.0
SFR 95.4% 12.4 85.7% 8.4 9.6 12.4
LAP 94.1% 12.0 89.5% 9.7 7.3 11.7

E
nv

.3

CR 92.6% 11.7 86.1% 8.9 11.9 11.0
SFR 90.0% 9.7 59.3% 0.2 8.6 9.0
LAP 89.6% 9.4 71.5% 3.1 6.7 8.7

E
nv

.4

CR 91.4% 11.2 82.6% 8.7 10.7 11.0
SFR 90.3% 10.2 86.1% 9.2 7.7 9.0
LAP 88.7% 9.8 74.8% 6.0 5.5 8.7

E
nv

.5

CR 96.3% 11.5 74.4% 2.9 10.5 9.3
SFR 88.9% 7.9 75.5% 3.2 4.5 6.0
LAP 78.8% 5.2 64.4% -0.4 4.1 5.3

E
nv

.6

CR 90.6% 10.4 83.4% 8.1 10.0 9.7
SFR 83.0% 7.1 71.9% 3.9 3.9 6.4
LAP 78.5% 6.0 66.5% 2.7 3.6 5.5

Table 2: Success rate and reward for Feudal-DQN and
DIP-DQN in the 18 benchmarking tasks, compared
with the reward of the best performing algorithm in
each task (Bnch.) and the handcrafted policy (Hdc.)
presented in (Casanueva et al., 2017).

(Su et al., 2017). These policies have the same
hyperparameters as the baseline DQN implemen-
tation, except for the two hidden layer sizes, which
are reduced to 130 and 50 respectively. As feature
functions, subsets of the DIP features are used:

φm(b) = φi(b) = ψ0(b)⊕ ψj(b)
φs(b) = ψ0(b)⊕ ψj(b)⊕ ψd(b, s)∀s ∈ S

The original set of summary actions of the bench-
marking environment, A, has a size of 5+ 3 ∗ |S|,
where |S| is the number of slots. This set is di-
vided in two subsets4: Ai of size 6 and Ad of size
4. Each sub-policy (including πm) is trained with
the same sparse reward signal used in the base-
lines, getting a reward of 20 if the dialogue is suc-
cessful or 0 otherwise, minus the dialogue length.

5 Results

The results in the 18 tasks of the benchmarking
environment after 4000 training dialogues are pre-
sented in Table 2. The same evaluation proce-
dure of the benchmarks is used, presenting the
mean over 10 different random seeds and testing
every seed for 500 dialogues. The FDQN policy
substantially outperforms every other other pol-
icy in all the environments except Env. 1. The

4An additional pass() action is added to each subset,
which is taken whenever the other sub-policy is executed.
This simplifies the training algorithm.

Figure 2: Learning curves for Feudal-DQN and DIP-
DQN in Env. 4, compared to the two best performing
algorithms in (Casanueva et al., 2017) (DQN and GP-
Sarsa). The shaded area depicts the mean ± the stan-
dard deviation over ten random seeds.

performance increase is more considerable in the
two largest domains (SFR and LAP), with gains
up to 5 points in accumulated reward in the most
challenging environments (e.g. Env. 4 LAP),
compared to the best benchmarked RL policies
(Bnch.). In addition, FDQN consistently outper-
forms the handcrafted policy (Hdc.) in environ-
ments 2 to 6, which traditional RL methods could
not achieve. In Env. 1, however, the results for
FDQN and DIP-DQN are rather low, specially for
DIP-DQN. Surprisingly, the results in Env. 2,
which only differs from Env. 1 in the absence of
action masks (thus, in principle, is a more com-
plex environment), outperform every other algo-
rithm. Analysing the dialogues individually, we
could observe that, in this environment, both poli-
cies are prone to “overfit” to an action 5. The per-
formance of FDQN and DIP-DQN in Env. 4 is also
better than in Env. 3, while the difference between
these environments also lies in the masks. This
suggests that an specific action mask design can
be helpful for some algorithms, but can harm the
performance of others. This is especially severe
in the DIP-DQN case, which shows good perfor-
mance in some challenging environments, but it is
more unstable and prone to overfit than FDQN.

However, the main purpose of action masks is
to reduce the number of dialogues needed to train
a policy. Observing the learning curves shown
in Figure 2, the FDQN model can learn a near-
optimal policy in large domains in about 1500 dia-
logues, even if no additional reward is used, mak-
ing the action masks unnecessary.

5The model overestimates the value of an incorrect action,
continuously repeating it until the user runs out of patience.

717



6 Conclusions and future work

We have presented a novel dialogue management
architecture, based on Feudal RL, which substan-
tially outperforms the previous state of the art in
several dialogue environments. By defining a set
of slot dependent policies with shared parameters,
the model is able to learn a general way to act in
slots, increasing its scalability to large domains.

Unlike other HRL methods applied to dialogue,
no additional reward signals are needed and the hi-
erarchical structure can be derived from a flat on-
tology, substantially reducing the design effort.

A promising approach would be to substitute
the handcrafted feature functions used in this work
by neural feature extractors trained jointly with
the policy. This would avoid the need to design
the feature functions and could be potentially ex-
tended to other modules of the SDS, making text-
to-action learning tractable. In addition, a sin-
gle model can be potentially used in different do-
mains (Papangelis and Stylianou, 2017), and dif-
ferent feudal architectures could make larger ac-
tion spaces tractable (e.g. adding a third sub-
policy to deal with actions dependent on 2 slots).

Acknowledgments

This research was funded by the EPSRC grant
EP/M018946/1 Open Domain Statistical Spoken
Dialogue Systems.

References
Paweł Budzianowski, Stefan Ultes, Pei-Hao Su, Nikola

Mrkšić, Tsung-Hsien Wen, Inigo Casanueva, Lina
M. Rojas Barahona, and Milica Gašić. 2017. Sub-
domain modelling for dialogue management with hi-
erarchical reinforcement learning. In Proc of SIG-
DIAL.

Iñigo Casanueva, Paweł Budzianowski, Pei-Hao Su,
Nikola Mrkšić, Tsung-Hsien Wen, Stefan Ultes,
Lina Rojas-Barahona, Steve Young, and Milica
Gašić. 2017. A benchmarking environment for re-
inforcement learning based task oriented dialogue
management. arXiv preprint arXiv:1711.11023 .

Inigo Casanueva, Thomas Hain, Heidi Christensen, Ri-
card Marxer, and Phil Green. 2015. Knowledge
transfer between speakers for personalised dialogue
management. In Proceedings of the 16th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue. pages 12–21.

Heriberto Cuayáhuitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2010. Evaluation of a hierar-

chical reinforcement learning spoken dialogue sys-
tem. Computer Speech & Language 24(2):395–429.

Heriberto Cuayáhuitl, Seunghak Yu, Ashley
Williamson, and Jacob Carse. 2016. Deep re-
inforcement learning for multi-domain dialogue
systems. NIPS Workkshop .

Peter Dayan and Geoffrey E Hinton. 1993. Feudal re-
inforcement learning. In Advances in neural infor-
mation processing systems. pages 271–278.

Milica Gašić, Catherine Breslin, Matthew Henderson,
Dongho Kim, Martin Szummer, Blaise Thomson,
Pirros Tsiakoulis, and Steve Young. 2013. Pomdp-
based dialogue manager adaptation to extended do-
mains. In Proceedings of the SIGDIAL Conference.

Milica Gašić, Nikola Mrkšić, Pei-hao Su, David
Vandyke, Tsung-Hsien Wen, and Steve Young.
2015. Policy committee for adaptation in multi-
domain spoken dialogue systems. In Automatic
Speech Recognition and Understanding (ASRU),
2015 IEEE Workshop on. IEEE, pages 806–812.

James Henderson, Oliver Lemon, and Kallirroi
Georgila. 2008. Hybrid reinforcement/supervised
learning of dialogue policies from fixed data sets.
Computational Linguistics 34(4):487–511.

M. Henderson, B. Thomson, and J. Williams. 2014a.
The Second Dialog State Tracking Challenge. In
Proc of SIGdial.

M. Henderson, B. Thomson, and S. J. Young. 2014b.
Word-based Dialog State Tracking with Recurrent
Neural Networks. In Proc of SIGdial.

Esther Levin, Roberto Pieraccini, and Wieland Eckert.
1998. Using markov decision process for learning
dialogue strategies. In Acoustics, Speech and Signal
Processing, 1998. Proceedings of the 1998 IEEE In-
ternational Conference on. IEEE, volume 1, pages
201–204.

Volodymyr Mnih, Koray Kavukcuoglu, David Sil-
ver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. 2013. Playing atari
with deep reinforcement learning. arXiv preprint
arXiv:1312.5602 .

Alexandros Papangelis and Yannis Stylianou. 2017.
Single-model multi-domain dialogue management
with deep learning. In International Workshop for
Spoken Dialogue Systems.

B. Peng, X. Li, L. Li, J. Gao, A. Celikyilmaz, S. Lee,
and K.-F. Wong. 2017. Composite Task-Completion
Dialogue System via Hierarchical Deep Reinforce-
ment Learning. ArXiv e-prints .

Olivier Pietquin, Matthieu Geist, Senthilkumar Chan-
dramohan, and Hervé Frezza-Buet. 2011. Sample-
efficient batch reinforcement learning for dialogue
management optimization. ACM Transactions on
Speech and Language Processing (TSLP) 7(3):7.

718



Pei-Hao Su, Pawel Budzianowski, Stefan Ultes, Mil-
ica Gasic, and Steve Young. 2017. Sample-efficient
actor-critic reinforcement learning with supervised
data for dialogue management. Proceedings of Sig-
Dial .

Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-
Barahona, Stefan Ultes, David Vandyke, Tsung-
Hsien Wen, and Steve Young. 2016. Continu-
ously learning neural dialogue management. arXiv
preprint arXiv:1606.02689 .

Richard S. Sutton and Andrew G. Barto. 1999. Rein-
forcement Learning: An Introduction. MIT Press.

Stefan Ultes, Lina M. Rojas-Barahona, Pei-Hao Su,
David Vandyke, Dongho Kim, Iñigo Casanueva,
Paweł Budzianowski, Nikola Mrkšić, Tsung-Hsien
Wen, Milica Gašić, and Steve J. Young. 2017. Py-
dial: A multi-domain statistical dialogue system
toolkit. In ACL Demo. Association of Computa-
tional Linguistics.

Zhuoran Wang, Tsung-Hsien Wen, Pei-Hao Su,
and Yannis Stylianou. 2015. Learning domain-
independent dialogue policies via ontology parame-
terisation. In SIGDIAL Conference. pages 412–416.

Steve Young, Milica Gašić, Blaise Thomson, and Ja-
son D Williams. 2013. Pomdp-based statistical spo-
ken dialog systems: A review. Proceedings of the
IEEE 101(5):1160–1179.

A Feudal Dialogue Policy algorithm

Algorithm 1 Feudal Dialogue Policy
1: for each dialogue turn do
2: observe b
3: bm = φm(b)
4: am = argmax

am∈Am
Qm(bm, a

m)

5: if am == ami then . drop to πi
6: bi = φi(b)
7: a = argmax

ai∈Ai
Qi(bi, a

i)

8: else am == amd then . drop to πd
9: bs = φs(b) ∀s ∈ S

10: slot, act = argmax
s∈S,ad∈Ad

Qs(bs, a
d)

11: a = join(slot, act)
12: end if
13: execute a
14: end for

B DIP features

This section gives a detailed description of the DIP
feature functions φdip(b, s) = ψ0(b) ⊕ ψj(b) ⊕
ψd(b, s) used in this work. The differences with
the features used in (Wang et al., 2015) and (Pa-
pangelis and Stylianou, 2017) are the following:

• No priority or importance features are used.

• No Potential contribution to DB search fea-
tures are used.

• The joint belief features ψj(b) are extended
to account for large-domain aspects.

Feature Feature Feature
function description size

ψ0(b) last user dialogue act (bin) * 7
DB search method (bin) * 6
# of requested slots (bin) 5
offer happened * 1
last action was Inform no venue * 1
normalised # of slots (1/# of slots) 1
normalised avg. slot length (1/avg. # of values) 1

ψj(b) prob. of the top 3 values of bj 3
prob. of *NONE* value of bj 1
entropy of bj 1
diff. between top and 2nd value probs. (bin) 5
# of slots with top value not *NONE* (bin) 5

ψd(b, s) prob. of the top 3 values of s 3
prob. of *NONE* value of s 1
diff. between top and 2nd value probs. (bin) 5
entropy of s 1
# of values of s with prob. > 0 (bin) 5
normalised slot length (1/# of values) 1
slot length (bin) 10
entropy of the distr. of values of s in the DB 1

total 64

Table 3: List of features composing the DIP features.
the tag (bin) denotes that a binary encoding is used for
this feature. Some of the joint features ψj(b) are ex-
tracted from the joint belief bj , computed as the Carte-
sian product of the beliefs of the individual slots. *
denotes that these features exist in the original belief
state b.

719


