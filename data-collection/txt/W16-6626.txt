



















































The WebNLG Challenge: Generating Text from DBPedia Data


Proceedings of The 9th International Natural Language Generation conference, pages 163–167,
Edinburgh, UK, September 5-8 2016. c©2016 Association for Computational Linguistics

The WebNLG Challenge: Generating Text from DBPedia Data

Emilie Colin1 Claire Gardent1 Yassine M’rabet2 Shashi Narayan3 Laura Perez-Beltrachini1
1 CNRS/LORIA and Université de Lorraine, Nancy, France

{emilie.colin,claire.gardent,laura.perez}@loria.fr
2 National Library of Medicine, Bethesda, USA

yassine.m’rabet@nih.gov
3 School of Informatics, University of Edinburgh, UK

snaraya2@inf.ed.ac.uk

1 Introduction

With the emergence of the linked data initiative and
the rapid development of RDF (Resource Descrip-
tion Format) datasets, several approaches have re-
cently been proposed for generating text from RDF
data (Sun and Mellish, 2006; Duma and Klein, 2013;
Bontcheva and Wilks, 2004; Cimiano et al., 2013;
Lebret et al., 2016). To support the evaluation and
comparison of such systems, we propose a shared
task on generating text from DBPedia data. The
training data will consist of Data/Text pairs where
the data is a set of triples extracted from DBPe-
dia and the text is a verbalisation of these triples.
In essence, the task consists in mapping data to
text. Specific subtasks include sentence segmenta-
tion (how to chunk the input data into sentences),
lexicalisation (of the DBPedia properties), aggrega-
tion (how to avoid repetitions) and surface realisa-
tion (how to build a syntactically correct and natural
sounding text).

2 Context and Motivation

DBPedia is a multilingual knowledge base that was
built from various kinds of structured information
contained in Wikipedia (Mendes et al., 2012). This
data is stored as RDF triples of the form (SUBJECT,
PROPERTY, OBJECT) where the subject is a URI (Uni-
form Resource Identifier), the property is a binary
relation and the object is either a URI or a literal
value such as a string, a date or a number. The En-
glish version of the DBpedia knowledge base cur-
rently encompasses 6.2M entities, 739 classes, 1,099
properties with reference values and 1,596 proper-

ties with typed literal values.1

There are several motivations for generating text
from DBPedia.

First, the RDF language in which DBPedia is en-
coded is widely used within the Linked Data frame-
work. Many large scale datasets are encoded in this
language (e.g., MusicBrainz2, FOAF3, LinkedGeo-
Data4) and official institutions5 increasingly publish
their data in this format. Being able to generate good
quality text from RDF data would permit e.g., mak-
ing this data more accessible to lay users, enriching
existing text with information drawn from knowl-
edge bases such as DBPedia or describing, compar-
ing and relating entities present in these knowledge
bases.

Second, RDF data, and in particular, DBPedia,
provide a framework that is both limited and arbi-
trarily extensible from a linguistic point of view. In
the simplest case, the goal would be to verbalise
a single triple. In that case, the task mainly con-
sists in finding an appropriate “lexicalisation” for
the property. The complexity of the generation task
can be closely monitored however by increasing the
number of input triples, using input with different
shapes6, working with different semantic domains
and/or enriching the RDF graphs with additional

1http://wiki.dbpedia.org/
dbpedia-dataset-version-2015-10

2https://musicbrainz.org/
3http://www.foaf-project.org/
4http://linkedgeodata.org/
5See http://museum-api.pbworks.com for exam-

ples.
6DBPedia data forms a graph. Different graph shapes induce

different verbalisation structures.

163



(e.g., discourse) information. We plan to produce
a dataset which varies along at least some of these
dimensions so as to provide a benchmark for gener-
ation that will test systems on input of various com-
plexity.

Third, there has been much work recently on ap-
plying deep learning (in particular, sequence to se-
quence) models to generation. The training data
used by these approaches however often have lim-
ited variability. For instance, (Wen et al., 2015)’s
data is restricted to restaurant descriptions and (Le-
bret et al., 2016)’s to WikiData frames. Typically the
number of attributes (property) considered by these
approaches is very low (between 15 and 40) and
the text to be produced have a stereotyped structure
(restaurant description, biographic abstracts). By
providing a more varied dataset, the WebNLG data-
text corpus will permit investigating how such deep
learning models perform on more varied and more
linguistically complex data.

3 Task Description

In essence, the task consists in mapping data to
text. Specific subtasks include sentence segmenta-
tion (how to chunk the input data into sentences),
lexicalisation (of the DBPedia properties), aggrega-
tion (how to avoid repetitions) and surface realisa-
tion (how to build a syntactically correct and natu-
ral sounding text). The following example illustrates
this.

(1) a. Data: (JOHN E BLAHA BIRTHDATE 1942 08 26)
(JOHN E BLAHA BIRTHPLACE SAN ANTONIO)
(JOHN E BLAHA OCCUPATION FIGHTER PILOT)

b. Text: John E Blaha, born in San Antonio on 1942-08-
26, worked as a fighter pilot

Given the input shown in (1a), generating (1b) in-
volves lexicalising the OCCUPATION property as the
phrase worked as, using PP coordination (born in San
Antonio on 1942-08-26) to avoid repeating the word born
(aggregation) and verbalising the 3 triples by a sin-
gle complex sentence including an apposition, a PP
coordination and a transitive verb construction (sen-
tence segmentation and surface realisation).

Relation to Previous Shared Tasks Other NLG
shared task evaluation challenges have been organ-
ised in the past. These have focused on different
generation subtasks overlapping with the task we

propose but our task differs from them in various
ways.
KBGen generation challenge. The recent KBGen
(Banik et al., 2013) task focused on sentence genera-
tion from Knowledge Bases (KB). In particular, the
task was organised around the AURA (Gunning et
al., 2010) KB on the biological domain which mod-
els n-ary relations. The input data selection process
targets the extraction of KB fragments which could
be verbalised as a single sentence. The content se-
lection approach was semi-automatic, starting with
the manual selection of a set of KB fragments. Then,
using patterns derived from those fragments, a new
set of candidate KB fragments was generated which
was finally manually revised. The verbalisation of
the sentence sized KB fragments was generated by
human subjects.

Although our task also concerns text generation
from KBs the definition of the task is different. Our
proposal aims at the generation of text beyond sen-
tences and thus involves an additional subtask that
is sentence segmentation. The tasks also differ on
the KBs used, we propose using DBPedia which fa-
cilitates changing the domain by focusing on dif-
ferent categories. Moreover, the set of relations on
both KBs pose different challenges for generation,
while the AURA KB contains n-ary relations DBPe-
dia contains relations names challenging for the lex-
icalisation subtask. A last difference with our task is
the content selection method. Our method is com-
pletely automatic and thus permits the inexpensive
generation of a large benchmark. Moreover, it can
be used to select content ranging from a single triple
to several triples and with different shapes.

The Surface Realisation Shared Task (SR’11). The
major goal of the SR’11 task (Belz et al., 2011)
was to provide a common ground for the compari-
son of surface realisers on the task of regenerating
sentences in a treebank. Two different tracks are
considered with different input representations. The
’shallow’ input provides a dependency tree of the
sentence to be generated and the ’deep’ input pro-
vides a graph representation where syntactic depen-
dencies have been replaced by semantic roles and
some function words have been removed.

The focus of the SR’11 task was on the linguis-
tic realisation subtask and the broad coverage of lin-

164



guistic phenomena. The task we propose here starts
from non-linguistic KB data and puts forward other
NLG subtasks.

Generating Referring Expressions (GRE). The GRE
shared tasks pioneered the proposed NLG chal-
lenges. The first shared task has only focused on
the selection of distinguishing attributes (Belz and
Gatt, 2007) while subsequent tasks have considered
the referring expression realisation subtask propos-
ing a complete referring expression generation task
(Gatt et al., 2008; Gatt et al., 2009). This tasks
aimed at the unique identification of the referent and
brevity of the referring expression. Slightly differ-
ent, the GREC challenges (Belz et al., 2008; Belz et
al., 2009; Belz et al., 2010) propose the generation
of referring expressions in a discourse context. The
GREC tasks use a corpus created from Wikipedia
abstracts on geographic entities and people and with
two referring expression annotation schemes, refer-
ence type and word strings. Rather than generating
from data input these tasks consist in labelling un-
derspecified referring expressions in a given text.

Our task concerns the generation of entity de-
scriptions and requires the production of referring
expressions, specially in the cases where multiple
sentences will be generated. However, it does not
foresee the selection of additional content (e.g. at-
tributes). In contrast, our proposal targets all gener-
ation subtasks involved in content realisation.

4 Data

As illustrated in Example 1 above, the training cor-
pus consists of (D,T ) pairs such that D is a set of
DBPedia triples and T is an English text (possibly
consisting of a single sentence). This corpus will
be constructed in two steps by first, extracting from
DBPedia content units that are both coherent and
diverse and second, associating these content units
with English text verbalising their content.

Data To extract content units from DBPedia, we
will use the content selection procedure sketched in
(Mohammed et al., 2016). This procedure consists
of two steps. First, bigram models of DBPedia prop-
erties specific to a given DBPedia category (e.g., As-
tronaut) are learned from the DBPedia graphs as-
sociated with entities of that category. Second, an

ILP program is used to extract from DBPedia, sub-
trees that maximise bigram probability. In effect,
the extracted DBPedia trees are coherent entity de-
scriptions in that the property bigram they contain
often cooccur together in the DBPedia graphs as-
sociated with entities of a given DBPedia category.
The method can be parameterised to produce con-
tent units for different DBPedia categories, differ-
ent DBPedia entities and various numbers of DBPe-
dia triples. It is fully automatic and permit produc-
ing DBPedia graphs that are both coherent, diverse
and that bear on different domains (e.g., Astronauts,
Universities, Musical work).

Text To associate the DBPedia trees extracted in
the first phase with text, we will combine automatic
techniques with crowdsourcing in two ways.

First, we will lexicalise DBPedia properties by
using the lexicalisations contained in the Lemon
English Lexicon for DBPedia7(Walter et al., 2013;
Walter et al., 2014a; Walter et al., 2014b) and
by manually filtering the lexicalisations produced
by the lexicalisation method described in (Perez-
Beltrachini and Gardent, 2016) and by the rela-
tion extraction and clustering method described in
(c.f. (Nakashole et al., 2012))8. We will then ask
crowdsourcers to verbalise sets of DBPEdia triples
in which properties have already been lexicalised
(e.g., CREW1UP will be lexicalised as commander of ).

Second, we will exploit the data-to-text alignment
method presented in (Mrabet et al., 2016) to semi-
automatically align Wikipedia text with sets of DB-
Pedia triples. The method consists in (i) automati-
cally annotating phrases with DBPedia entities, (ii)
associating sentences with DBPedia triples relating
entities annotating these sentences and (iii) using
crowdsourcing to align sentences with triples. In the
third step, annotators are asked to “align” triples and
sentences that is, to remove from the sentence all
material that is irrelevant to express the associated
triples and vice versa, to remove any triples that is
not expressed by the sentence.

Statistics, Schedule and Funding The WebNLG
shared task will be funded by the WebNLG ANR

7http://lemon-model.net/lexica/dbpedia_
en/

8https://d5gate.ag5.mpi-sb.mpg.de/
pattyweb/

165



Project9. We aim to produce a data-text corpus of
medium size (between 10K and 50K data-text pairs)
bearing on at least 5 different domains and consist-
ing of input data containing between 2 and 5 RDF
triples. Ideally, training data will be made available
early in 2017 and testing will be carried out in early
summer (May-June 2017).

5 Evaluation

Evaluation of the generated texts will be done both
with automatic evaluation metrics (BLEU, TER
or/and METEOR) and using human judgements ob-
tained through crowdsourcing. The human evalu-
ation will seek to assess such criteria as fluency,
grammaticality and appropriateness (does the text
correctly verbalise the input data?).

Acknowledgments

We thank the French National Research Agency for
funding the research presented in this paper in the
context of the WebNLG project10.

References

Eva Banik, Claire Gardent, and Eric Kow. 2013. The
kbgen challenge. In the 14th European Workshop on
Natural Language Generation (ENLG), pages 94–97.

Anja Belz and Albert Gatt. 2007. The attribute selection
for gre challenge: Overview and evaluation results.
Proceedings of UCNLG+ MT: Language Generation
and Machine Translation, pages 75–83.

Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.
2008. The grec challenge: Overview and evaluation
results.

Anja Belz, Eric Kow, and Jette Viethen. 2009. The grec
named entity generation challenge 2009: overview and
evaluation results. In Proceedings of the 2009 Work-
shop on Language Generation and Summarisation,
pages 88–98. Association for Computational Linguis-
tics.

Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.
2010. Generating referring expressions in context:
The grec task evaluation challenges. In Empirical
methods in natural language generation, pages 294–
327. Springer.

9http://talc1.loria.fr/webnlg/stories/
about.html

10http://talc1.loria.fr/webnlg/stories/
about.html

Anja Belz, Michael White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The first
surface realisation shared task: Overview and evalu-
ation results. In Proceedings of the 13th European
Workshop on Natural Language Generation, ENLG
’11, pages 217–226, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

Kalina Bontcheva and Yorick Wilks. 2004. Automatic
report generation from ontologies: the miakt approach.
In International Conference on Application of Natu-
ral Language to Information Systems, pages 324–335.
Springer.

Philipp Cimiano, Janna Lüker, David Nagel, and
Christina Unger. 2013. Exploiting ontology lexica
for generating natural language texts from rdf data. In
Proceedings of the 14th European Workshop on Natu-
ral Language Generation, pages 10–19.

Daniel Duma and Ewan Klein. 2013. Generating natural
language from linked data: Unsupervised template ex-
traction. Association for Computational Linguistics,
Potsdam, Germany, pages 83–94.

Albert Gatt, Anja Belz, and Eric Kow. 2008. The tuna
challenge 2008: Overview and evaluation results. In
Proceedings of the Fifth International Natural Lan-
guage Generation Conference, pages 198–206. Asso-
ciation for Computational Linguistics.

Albert Gatt, Anja Belz, and Eric Kow. 2009. The tuna-
reg challenge 2009: Overview and evaluation results.
In Proceedings of the 12th European Workshop on
Natural Language Generation, pages 174–182. Asso-
ciation for Computational Linguistics.

David Gunning, Vinay K. Chaudhri, Peter Clark, Ken
Barker, Shaw-Yi Chaw, Mark Greaves, Benjamin
Grosof, Alice Leung, David McDonald, Sunil Mishra,
John Pacheco, Bruce Porter, Aaron Spaulding, Dan
Tecuci, and Jing Tien. 2010. Project Halo Update –
Progress toward digital aristotle. AI Magazine, Fall.

Rémi Lebret, David Grangier, and Michael Auli. 2016.
Generating text from structured data with application
to the biography domain. CoRR, abs/1603.07771.

Pablo N Mendes, Max Jakob, and Christian Bizer. 2012.
Dbpedia: A multilingual cross-domain knowledge
base. In LREC, pages 1813–1817. Citeseer.

Rania Mohammed, Laura Perez-Beltrachini, and Claire
Gardent. 2016. Category-driven content selection. In
Proceedings of the nintth International Natural Lan-
guage Generation Conference, INLG 2016.

Yassine Mrabet, Pavlos Vougiouklis, Halil Kilicoglu,
Claire Gardent, Dina DemnerFushman, Jonathon
Hare, and Elena Simperl. 2016. Aligning texts and
knowledge bases with semantic sentence simplifica-
tion. In Proceedings of the 2nd International Work-
shop on Natural Language Generation and the Seman-
tic Web.

166



Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Discovering and exploring relations
on the web. Proceedings of the VLDB Endowment,
5(12):1982–1985.

Laura Perez-Beltrachini and Claire Gardent. 2016.
Learning embeddings to lexicalise rdf properties. In
Proceedings of the Fifth Joint Conference on Lexical
and Computational Semantics.

Xiantang Sun and Chris Mellish. 2006. Domain inde-
pendent sentence generation from rdf representations
for the semantic web. In Combined Workshop on
Language-Enabled Educational Technology and De-
velopment and Evaluation of Robust Spoken Dialogue
Systems, European Conference on AI, Riva del Garda,
Italy.

Sebastian Walter, Christina Unger, and Philipp Cimiano.
2013. A corpus-based approach for the induction of
ontology lexica. In Natural Language Processing and
Information Systems, pages 102–113. Springer.

Sebastian Walter, Christina Unger, and Philipp Cimiano.
2014a. Atolla framework for the automatic induction
of ontology lexica. Data & Knowledge Engineering,
94:148–162.

Sebastian Walter, Christina Unger, and Philipp Cimiano.
2014b. M-atoll: a framework for the lexicalization
of ontologies in multiple languages. In The Semantic
Web–ISWC 2014, pages 472–486. Springer.

Tsung-Hsien Wen, Milica Gasic, Nikola Mrkšić, Pei-Hao
Su, David Vandyke, and Steve Young. 2015. Seman-
tically conditioned lstm-based natural language gener-
ation for spoken dialogue systems. In Proceedings of
the 2015 Conference on Empirical Methods in Natural
Language Processing, pages 1711–1721, Lisbon, Por-
tugal, September. Association for Computational Lin-
guistics.

167


