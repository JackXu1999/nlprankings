Proceedings of the Biomedical NLP Workshop associated with RANLP 2017, pages 1–7,

Varna, Bulgaria, 8 September 2017.

https://doi.org/10.26615/978-954-452-044-1_001

1

Document retrieval and question answering in medical documents.

A large-scale corpus challenge.

Eric Curea

Research Institute for Artiﬁcial Intelligence

“MIHAI DRAGANESCU”,

Romanian Academy
eric@racai.ro

Abstract

Whenever employed on large datasets,
information retrieval works by isolating
a subset of documents from the larger
dataset and then proceeding with low-level
processing of the text. This is usually car-
ried out by means of adding index-terms
to each document in the collection.
In
this paper we deal with automatic docu-
ment classiﬁcation and index-term detec-
tion applied on large-scale medical cor-
pora.
In our methodology we employ a
linear classiﬁer and we test our results
on the BioASQ training corpora, which is
a collection of 12 million MeSH-indexed
medical abstracts. We cover both term-
indexing, result retrieval and result rank-
ing based on distributed word representa-
tions.

1

Introduction

Automatic key-wording is the process of enrich-
ing text documents with pre-speciﬁed classes (top-
ics or themes). The primary motivation is that
in information retrieval one can easily use these
keywords for automatically ﬁltering and obtaining
a subset of documents form a large-scale corpus,
documents that share common traits linked to their
domain, topic, title, publication source, authors,
etc. As such, automatic key-wording and docu-
ment indexing (based on these keywords) helps
people to ﬁnd information in huge resources.

Currently, most of the on-line information is
available in the form of unstructured documents
and this is unlikely to change in the foreseeable fu-
ture. Though, several initiatives to force users into
manually labeling their on-line publications using
specialized markup have been proposed (one good

example is Google Markup Language1), scientiﬁc
publications are unlikely to be subject to such an-
notations, mainly because they employ printable
formats such as Postscript and PDF (which, in for-
tunate situations, can be converted into plain text).
Thus, NLP task such as unsupervised document
clustering represents a key-task in information re-
trieval. Due to the increased availability of doc-
uments in digital form and the ensuing need to
access them in ﬂexible ways, content-based doc-
ument management tasks (collectively known as
information retrieval
IR) have gained a promi-
nent status in the research community in the past
decade. The task of Document classiﬁcation or
document categorization, the activity of labeling
natural language texts with thematic categories
from a predeﬁned set, is very important and still
evolving thanks to increased applicative interest
and to the availability of more powerful hardware.
To accomplish the task of document classiﬁ-
cation, an increasing number of computational
and statistical approaches have been developed
over the years, to mention a few: Suport Vector
Machines (SVMs) (Manevitz and Yousef, 2001;
Joachims, 1998), maximum entropy (Ratnaparkhi,
1998; El-Halees, 2015), word-distributional clus-
tering (Baker and McCallum, 1998), weighted K-
Nearest-Neighbor classiﬁcation (Han et al., 2001;
Larsen and Aone, 1999), linear classiﬁers (Lewis
et al., 1996), Naive Bayes methods (McCal-
lum and Nigam, 1998), artiﬁcial neural networks
(Zhang and Zhou, 2006; Collobert and Weston,
2008; Lai et al., 2015), decision trees (Lewis and
Ringuette, 1994).

Our work is focused on automatic labeling of
medical
text, using Medical Subject Headings
(MeSH)2 terms (Rogers, 1963) (see section 3)

1https://developers.google.com/search/docs/guides/intro-

structured-data - accessed 2017-05-18

2https://www.ncbi.nlm.nih.gov/pmc/articles/PMC35238/

2

and information retrieval for question answering
based on the analysis of article abstracts (see sec-
tion 4). The training, evaluation and test datasets
used in the validation of our procedure are part of
the BioASQ3 (Tsatsaronis et al., 2012) evaluation
campaign.

2 Corpus description
The train set corpus contained articles from the
free on-line repository PubMed 4

The training data is composed of a very large
number of documents collected from PubMed,
which have been semi-automatically annotated
with MeSH terms, with the help of human cura-
tors. Aside from the MeSH terms, each entry in
the dataset contained important meta-data such as:
the title of the paper, the journal where the paper
was published, publishing year and the paper’s ab-
stract.

The training set is JSON-encoded and contains

the following ﬁelds for each article:

1. pmid : An unique identiﬁer assigned to each
paper - used for internal evaluation purposes;

2. title : The original title of the article

3. abstractText : the abstract of the article,

4. year : the year the article was published,

5. journal : the journal the article was published,

and

6. meshMajor : a list with the major MeSH

headings of the article.

For clarity, we also provide an excerpt from the
training data, presenting the structure af each arti-
cle collected in the large-scale corpus:
{"articles": [{"journal":"journal..","
abstractText":"text..", "meshMajor
":["mesh1",...,"meshN"], "pmid":"
PMID", "title":"title..", "year":"
YYYY"},..., {..}]}

To offer a better view over the training data, we
must specify that the total number of articles is
12,834,585, published in over 9,000 journals, with
an average of 1,421.64 articles in each journal,
published from 1946 to 2016 with most articles
(over 600,000) selected from 2014, a distribution
- accessed 2017-05-18

3http://bioasq.org - last accessed 2017-05-09
4https://www.ncbi.nlm.nih.gov/pubmed/ - accessed 2017-

04-29

Table 1: Label distribution over training data
labeled documents distribution percentage
0.04%
0.03%
0.49%
0.80%
8.07%
36.20%
54.38%

>1,000,000
500,000 1,000,000
100,000 500,000
50,000 100,000
10,000 50,000
1,000 10,000
<1,000
total

10
7
137
223
2,240
10,053
15,103
27,773.00

of 12.66 average MeSHes per article, going from
the MeSH “humans” with an occurrence of over 8
millions to MeSHes like “tropaeolaceae” that only
occur once, yielding a MeSH coverage of 27,773
MeSHes composed either from a single word of a
construct like “magnetic resonance imaging”. All
this in a total of 20.5GB (plain/text) and 6.29GB
(compressed data). Table 2 provides generic in-
formation regarding frequent versus uncommon
MeSHes, while table 1 captures the “spread” of
the MeSHes throughout the training data.

As can easily be seen from table 1, 10 of the fre-
quent MeSHes like “humans”, “male”, “female”
or “animals”, are used to label more the 1M docu-
ments, only 7 fall within the 500K-1M range and
360 between 50K and 500K (we further refer to
them as category A). On the opposite side, 2K
MeSHes are found in 10K-50K documents, 10K
MeSHes in 1K-10K documents and more than
15K MeSHes have an occurrence of less than 1K
(category B). The high occurring MeSHes (cate-
gory A) represent less than 2% of the total number
of labels, which indicates that in most cases any
ML system will most likely not be able to model
the rest of 98% of the labels based on this cor-
pus. To clarify our previous statement, it is ex-
pected that most classiﬁers will have a small recall
for 98% of the labels, mainly because the objec-
tive of minimizing the “overall” accuracy is eas-
ily achieved by preferring not to emit any label
rather than incorrectly classifying documents with
bad labels and only for less than 2% of the total
number of labels the systems will have a chance
of a high recall.

3 Automatic MeSH labeling

Currently, there are 28,489 descriptors in MeSH
2017 that were used in the creation of the training
data. However, due to the unbalanced occurrence

3

female
animals
adult

ID MESH
1
humans
2 male
3
4
5
6 middle aged
aged
7
adolescent
8
9
rats
10 mice
11
child
time factors
12
13
aged+80 and over
14 molecular sequence data
15
16
17
18
19
20
21
22
23
24
25
26

treatment outcome
retrospective studies
child+preschool
young adult
risk factors
follow-up studies
cells+cultured
amino acid sequence
prospective studies
pregnancy
infant
base sequence

Table 2: MeSH distribution
count
ID MESH
kinetics
27
cell line
28
29
surveys and questionnaires
rna+messenger
30
dose-response relationship+drug
31
reproducibility of results
32
33
infant+newborn
34 mutation
35
36
37
38
39
40
41
42

united states
brain
rats+sprague-dawley
sensitivity and speciﬁcity
prognosis
in vitro techniques
age factors
liver
........
ephemerovirus
........
zigadenus
........
cytophagaceae infections
........
duboisia
........
childhood-onset ﬂuency disorder

8,103,280
5,351,269
5,169,536
3,932,184
3,119,705
2,782,688
1,936,405
1,219,944
1,116,126
1,045,215
826,020
793,584
636,261
590,276
571,489
547,781
510,539
494,101
450,495
447,572
428,059
395,146
394,813
392,281
387,000
385,031

count
366,997
331,436
316,552
314,638
313,386
285,023
283,249
278,419
272,593
269,598
265,472
264,091
259,335
258,033
254,441
248,866
........
5
........
4
........
3
........
2
........
1

of terms combined with the large scale of the cor-
pus, we ran our experiments on a smaller sub-set
of MeSH terms, composed of only 154 most fre-
quent items.

In the classiﬁcation process we took into ac-
count as much information as we can and have
access to, about each document in the large-scale
corpus. The title of a document usually holds key
information about the content of the document.
The journal in which it was published is likely
to carry weight in the label assigning process as
only speciﬁc types of documents can be published
in certain types of journals. The year in which
the document was published will tell the system if
the information retrieved from the document has
a chance of not being up to date or it might be
completely outdated and superseded by more re-
cent research, in which case the system should at
least try to see if newer publications might hold
better results or more important supplementary in-
formation. The abstract text is the place where
the system can spend most processing time and

apply as many tests, approximations and reﬁne-
ments, because this is the place where most arti-
cles condense the biggest amount of relevant in-
formation about the content of the document. Of
course ﬁnding possible relevant information in the
abstract text is only part of the equation. The
more important part is determining relevant rela-
tions between different relevant lexical tokens, the
location of the information segments, distance be-
tween the different relevant lexical tokens inside
the abstract, number of occurrences, similarity to
the information determined in the question (W2V,
cosine similarity(Steinbach et al., 2000)).

All the input features were treated in a bag-of-
words manner, from which we removed any fea-
ture (word) with an occurrence rate lower than
100. This threshold of 100 was selected after
testing different limits that yielded either too few
features left to test with or too low occurrence
rate for the feature to be relevant.
Initially, our
training data contained 7,466,119 unique features
and the pruning process reduced this number to

4

only 123,255. For the classiﬁcation task we em-
ployed an ensemble of linear classiﬁers. Each pos-
sible output MeSH was associated with a classi-
ﬁer, which was trained in a 1-vs-all style to predict
if the system should or should not assign that la-
bel, based on the input features. The output of the
linear model ranged from -1 (do not assign a la-
bel) to 1 (assign a label) and was computed using
Equation 1, with w computed using the delta-rule
(Equation 2):

nX

y =

wn · xn

1

∆wk = α · (t − y) · xk

(1)

(2)

where
y is the output of the classiﬁer
t is the desired output of the classiﬁer (-1 or 1)
xi is the ith input feature
wi is the weight of the i-th input feature
α is the learning-rate (set to 10−3)

When we trained our ensemble of classiﬁers we
divided our training data into 9/10 for training and
1/10 for development, while trying to preserve
as best as possible the initial distribution for
each of the labels in both sets. Training was
done iteratively (compute new value for w using
the training set and measure accuracy on the
development set) and the stopping condition was
not to have any improvements on the development
set for more than 20 iterations. At the end of the
training process we kept the w that achieved the
highest accuracy on the development set.

Table 3: Labeling results

MiP
System
0.0920
Sequencer
Default MTI
0.6148
Our System 0.7681
DeepMeSH4
0.6671
0.6495
MZ1
0.6898
DeepMeSH3
DeepMeSH2
0.6895
0.7025
DeepMeSH1
DeepMeSH5
0.7198

MiR
0.0964
0.6286
0.1472
0.6289
0.3985
0.6170
0.6432
0.6282
0.6122

Acc.
0.0494
0.4594
0.1381
0.4839
0.3299
0.4877
0.5059
0.5025
0.5024

Table 3 shows the accuracy (Acc), Micro Preci-
sion (MiP) and Micro Recall (MiR) of our system,
measured on one of the datasets. It also offers a

comparative view between our methodology and
the other systems present in the competition. We
must mention that the overall performance ﬁgures
are measured using all the available MeSHes, not
the pruned subset.

4 Result ranking

For this we take each lexical component of the key
set of data extracted from the corpus and we try
to ﬁnd if the classiﬁed documents from the cor-
pus approximate to possible synonyms of lexical
component. For each lexical component of the
key set of data extracted from the question, we
calculated a list of lexical elements that can be
considered similar in meaning using ”cosine sim-
ilarity” computed over distributed word represen-
tations (Mikolov et al., 2013). The vectors (100-
dimensional) were computed using the word2vec5
tool on a speciﬁc subset of Wikipedia combined
with additional raw text resources provided as part
of the BioASQ challenge. In order to compile the
subset from Wikipedia we followed a simple boot-
strapping procedure:

1. We downloaded the latest Wikipedia XML
Dump at that date from the ofﬁcial web-site,
on which we run a version of WikipediaEx-
tractor6, that was modiﬁed to preserve cate-
gories;

2. We seeded a list of categories, using the ﬁrst
level of categories on the Wikipedia site for
the “Biomedical” main category;

3. We iterated 3 times through the entire cor-
pus and we consolidated our category list, by
adding categories that were associated with
our initial category list, each time updating
our seeded list;

4. We kept all documents that had at least one

category from our ﬁnal category list.

Given a “question” our IR process is: (a) we
extract a list of keywords from the query, by re-
moving function words from using a predeﬁned
dictionary; (b) we use the keywords to retrieve the
top 1M documents from the initial corpus; (c) we
re-rank our results and obtain a list with the top-
10 most relevant documents. Document ranking

5https://github.com/dav/word2vec - accessed 2017-04-05
6https://github.com/bwbaugh/wikipedia-extractor
- ac-

cessed 2017-01-28

5

Table 4: Test-set results

System Name Mean precision Recall F-measure Map GMAP
0.0028
Top 100 Baseline
0.0024
Top 50 Baseline
fdu 5b
0.0084
0.1238
Our System
0.0005
MCTeamMM
0.0005
MCTeamMM10
Wishart-S1
0.0001

0.1333
0.1920
0.1791
0.2857
0.1249
0.0436
0.0350

0.2845
0.2591
0.2228
0.2222
0.1481
0.1481
0.0484

0.1606
0.1503
0.1300
0.1238
0.0892
0.0892
0.0237

0.2460
0.2470
0.1865
0.4000
0.2266
0.0326
0.0465

Figure 1: Distribution of publications each year

Figure 2: Distribution of words in articles

is performed using Equation 3, which is designed
to take into account keyword synonymic coverage,
but currently ignores synonymic frequencies in the
text (in our empirical experiments we found that
introducing this factor decrease the overall preci-
sion of the system - in our opinion, mainly because
word-embeddings are prone to capturing contex-
tual similarities, rather than actual synonymic be-
havior).

· kX

i=1

Sd =

1
k

maxm

j=1(cos(ti, dj))

(3)

where
Sd - is the relevance of document d
k - is the number of keywords in the query
m - is the number of words in the document
ti - is the word embedding for term i in the query
dj - is the word embedding for term j in the
document

Table 4 shows the precision, recall and F-score
of our system, measured on one of the datasets.
It also offers a comparative view between our
methodology and the other systems present in the
competition. We must mention that the overall
performance ﬁgures are measured using all the
available MeSHes, not the pruned subset.

5 Snippets
Usually not all the text in the retrieved abstract is
part of a good answer to a given question. So ﬁnd-
ing the most relevant, shortest part of the abstract
was nest step.

To approximate the shortest span of text in each
abstract of the documents, that represents the best
response to the question, we selected a list of all
the lexical tokens in the abstract text that corre-
spond or might have generated the relevant label.
At ﬁrst glance, the snippet would be starting from
the beginning of the ﬁrst sentence that contains a

6

token from the list and ﬁnishing at the end of the
last sentence that contains a token from the list.

Of course this list has a high probability of hav-
ing duplicates. These duplicates have no value for
detecting the shortest relevant text. So we calcu-
late from the current abstract, the shortest span of
text that still contains all of the lexical tokens but
we ignore any duplicates in the list.

To help explain the previous statement we will

use the following example:
"document": .... [token_1]....[token_1

]...[token_2].....[token_1]....[
token_3].....[token_4].....[token_5
]....[token_1] ...

It can easily be seen in the example that the ﬁrst
iteration of “token 1” holds no value for the pur-
pose of ﬁnding the shortest relevant span of text an
neither does the second iteration even though it is
position in closer proximity to another token from
the list. The list is not in any way ordered so the
placement of the second token: “token 2” in front
of the ﬁrst token “token 1” is irrelevant. The exis-
tence of a different token in front of the current to-
ken: “token 2” before “token 1” only means that
this iteration of “token 1” is a viable candidate for
the shortest relevant span of text. Finally the ﬁnal
iteration of “token 1” has no other tokens placed
after it so we considered this iteration to hold less
value for a snippet. No other token had a duplicate
in this example so in this case the shortest most
relevant span of text was:
"snippet": [token_2].....[token_1]....[
token_3].....[token_4].....[token_5]
It is worth noting that there were of course cases
when the system would present the snippet as be-
ing the same as the entirety of the abstract text.
6 Conclusions and future work
In this article we presented a “biomedical” ori-
ented system that automatically assigns MeSH la-
bels to documents in a large-scale corpus. Our ap-
prach is based on a linear classiﬁer, trained in a
1-vs-all style for each possible MESH.

The system then retrieves answers from said
corpus for questions relevant to the medical ﬁeld.
Each question yields a number of “n” best ranked
documents that relate to the question. We achieve
this by ﬁrst selecting the relevant lexical tokens
from the questions. Then we use Word2Vec for
100 length vectors in order to calculate the cosine
similarity to approximate “x” closest lexical con-
cepts for each of the tokens from the question.

Our system also provides a corresponding list of
“n” snippets from the best ranked documents, the
shortest span of text which contain the informa-
tion from the abstract most relevant for the current
question. This is done by discarding any sentence
from the abstract text that does not contain any to-
ken from a determined list or only contains low
relevance duplicates of tokens from said list.

Currently we do not deal with determining and
extracting lexical dependencies between words
and we only focus on relevant-document retrieval.
However, our future development plans include
extending our system to be able to answer yes/no,
factoid and item-list questions. Additionally we
plan to include multilingual data from various
sources and investigate cross-lingual techniques
for document retrieval and machine translation for
delivering the cross-lingual results in the user’s na-
tive language.

References
L Douglas Baker and Andrew Kachites McCallum.
1998. Distributional clustering of words for text
classiﬁcation. In Proceedings of the 21st annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 96–103.
ACM.

Ronan Collobert and Jason Weston. 2008. A uniﬁed
architecture for natural language processing: Deep
neural networks with multitask learning.
In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.

Alaa M El-Halees. 2015. Arabic text classiﬁcation
IUG Journal of Natural

using maximum entropy.
Studies, 15(1).

Eui-Hong Sam Han, George Karypis, and Vipin Ku-
mar. 2001. Text categorization using weight ad-
justed k-nearest neighbor classiﬁcation. In Paciﬁc-
asia conference on knowledge discovery and data
mining, pages 53–65. Springer.

Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many rel-
evant features. Machine learning: ECML-98, pages
137–142.

Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015.
Recurrent convolutional neural networks for text
classiﬁcation.
In AAAI, volume 333, pages 2267–
2273.

Bjornar Larsen and Chinatsu Aone. 1999. Fast and ef-
fective text mining using linear-time document clus-
tering. In Proceedings of the ﬁfth ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 16–22. ACM.

7

David D Lewis and Marc Ringuette. 1994. A compar-
ison of two learning algorithms for text categoriza-
tion. In Third annual symposium on document anal-
ysis and information retrieval, volume 33, pages 81–
93.

David D Lewis, Robert E Schapire, James P Callan,
and Ron Papka. 1996. Training algorithms for lin-
ear text classiﬁers. In Proceedings of the 19th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 298–306. ACM.

Larry M Manevitz and Malik Yousef. 2001. One-class
svms for document classiﬁcation. Journal of Ma-
chine Learning Research, 2(Dec):139–154.

A McCallum and K Nigam. 1998. A comparison
of event models for naive bayes text classiﬁcation;
1998. Disponıvel em:¡ citeseer. nj. nec. com/mccal-
lum98comparison. html.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity.
In Advances in neural information processing
systems, pages 3111–3119.

Adwait Ratnaparkhi. 1998. Maximum entropy mod-
els for natural language ambiguity resolution. Ph.D.
thesis, University of Pennsylvania.

FB Rogers. 1963. Medical subject headings. Bulletin

of the Medical Library Association, 51:114–116.

Michael Steinbach, George Karypis, Vipin Kumar,
et al. 2000. A comparison of document cluster-
ing techniques. In KDD workshop on text mining,
volume 400, pages 525–526. Boston.

George Tsatsaronis, Michael Schroeder, Georgios
Paliouras, Yannis Almirantis, Ion Androutsopoulos,
Eric Gaussier, Patrick Gallinari, Thierry Artieres,
Michael R Alvers, Matthias Zschunke, et al. 2012.
Bioasq: A challenge on large-scale biomedical se-
mantic indexing and question answering.
In AAAI
fall symposium: Information retrieval and knowl-
edge discovery in biomedical text.

Min-Ling Zhang and Zhi-Hua Zhou. 2006. Multilabel
neural networks with applications to functional ge-
nomics and text categorization. IEEE transactions
on Knowledge and Data Engineering, 18(10):1338–

1351.

