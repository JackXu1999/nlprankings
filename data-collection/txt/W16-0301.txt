



















































Detecting late-life depression in Alzheimer's disease through analysis of speech and language


Proceedings of the 3rd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 1–11,
San Diego, California, June 16, 2016. c©2016 Association for Computational Linguistics

Detecting late-life depression in Alzheimer’s disease through analysis of
speech and language

Kathleen C. Fraser1 and Frank Rudzicz2,1 and Graeme Hirst1
1Department of Computer Science, University of Toronto, Toronto, Canada

2Toronto Rehabilitation Institute-UHN, Toronto, Canada
{kfraser,frank,gh}@cs.toronto.edu

Abstract

Alzheimer’s disease (AD) and depression
share a number of symptoms, and commonly
occur together. Being able to differentiate be-
tween these two conditions is critical, as de-
pression is generally treatable. We use linguis-
tic analysis and machine learning to determine
whether automated screening algorithms for
AD are affected by depression, and to detect
when individuals diagnosed with AD are also
showing signs of depression. In the first case,
we find that our automated AD screening pro-
cedure does not show false positives for indi-
viduals who have depression but are otherwise
healthy. In the second case, we have moderate
success in detecting signs of depression in AD
(accuracy = 0.658), but we are not able to draw
a strong conclusion about the features that are
most informative to the classification.

1 Introduction

Depression and dementia are both medical condi-
tions that can have a strong negative impact on the
quality of life of the elderly, and they are often co-
morbid. However, depression is often treatable with
medication and therapy, whereas dementia usually
occurs as the result of an irreversible process of neu-
rodegeneration. It is therefore critical to be able to
distinguish between these two conditions.

However, distinguishing between depression and
dementia can be extremely difficult because of over-
lapping symptoms, including apathy, crying spells,
changes in weight and sleeping patterns, and prob-
lems with concentration and attention.

It is also important to detect when someone has
both AD and depression, as this serious situation can
lead to more rapid cognitive decline, earlier place-
ment in a nursing home, increased risk of depression
in the patient’s caregivers, and increased mortality
(Thorpe, 2009; Lee and Lyketsos, 2003).

Separate bodies of work have reported the util-
ity of spontaneous speech analysis in distinguish-
ing participants with depression from healthy con-
trols, and in distinguishing participants with demen-
tia from healthy controls. Here we consider whether
such analyses can be applied to the problem of de-
tecting depression in Alzheimer’s disease (AD). In
particular, we explore two questions: (1) In previous
work on detecting AD from speech (elicited through
a picture description task), are cognitively healthy
people with depression being misclassified as hav-
ing AD? (2) If we consider only participants with
AD, can we distinguish between those with depres-
sion and those without, using the same picture de-
scription task and analysis?

2 Background

There has been considerable work on detecting de-
pression from speech and on detecting dementia
from speech, but very little which combines the two.
We will first review the two tasks separately, and
then discuss some of the complexity that arises when
depression and AD co-occur.

2.1 Detecting depression from speech

Depression affects a number of cognitive and phys-
ical systems related to the production of speech, in-
cluding working memory, the phonological loop, ar-

1



ticulatory planning, and muscle tension and control
(Cummins et al., 2015). These changes can result
in word-finding difficulties, articulatory errors, de-
creased prosody, and lower verbal productivity.

Over the past decade or so, there has been grow-
ing interest in measuring properties of the speech
signal that correlate with the changes observed in de-
pression, and using these measured variables to train
machine learning classifiers to automatically detect
depression from speech.

Ozdas et al. (2004) found that mean jitter and the
slope of the glottal flow spectrum could distinguish
between 10 non-depressed controls, 10 participants
with clinical depression, and 10 high-risk suicidal
participants.

Moore et al. (2008) considered prosodic features
as well as vocal tract and glottal features. They per-
formed sex-dependent classification and found that
glottal features were more discriminative than vocal
tract features, but that the best results were achieved
using all three types of features.

Cohn et al. (2009) examined the utility of facial
movements and vocal prosody in discriminating par-
ticipants with moderate or severe depression from
those with no depression. They achieved 79% accu-
racy using only two prosodic features: variation in
fundamental frequency, and latency of response to
interviewer questions. They used a within-subjects
design, in which they predicted which participants
had responded to treatment in a clinical trial.

Low et al. (2011) analyzed speech from ado-
lescents engaged in normal conversation with their
parents (68 diagnosed with depression, 71 con-
trols). They grouped their acoustic features into
5 groups: spectral, cepstral, prosodic, glottal, and
those based on the Teager energy operator (TEO, a
nonlinear energy operator). They achieved higher
accuracies using sex-dependent models than sex-
independent models, and found that the best results
were achieved using the TEO-based features (up to
87% for males and 79% for females).

Cummins et al. (2011) distinguished 23 depressed
participants from 24 controls with a best accuracy
of 80% in a speaker-dependent configuration and
79% in a speaker-independent configuration. Spec-
tral features, particularly mel-frequency cepstral co-
efficients (MFCCs), were found to be useful.

Alghowinem et al. (2012) analyzed speech from

30 participants with depression and 30 healthy con-
trols. The speech was elicited through interview
questions about situations that had aroused signif-
icant emotions. Higher accuracy was achieved on
detecting depression in women than in men. Energy,
intensity, shimmer, and MFCC features were all in-
formative, and positive emotional speech was more
discriminatory than negative emotional speech.

Scherer et al. (2013) differentiated 18 depressed
participants from 18 controls with 75% accuracy,
using interviews captured with a simulated virtual
human. They found that glottal features such as
the normalized amplitude quotient (NAQ) and quasi-
open quotient (QOQ) differed significantly between
the groups.

Alghowinem et al. (2013) compared four clas-
sifiers and a number of different feature sets on
the task of detecting depression from spontaneous
speech. They found loudness and intensity features
to be the most discriminatory, and suggested pitch
and formant features may be more useful for longi-
tudinal comparisons within individuals.

While most of the literature concerning the detec-
tion of depression from speech has focused solely
on the speech signal, there is an associated body of
work on detecting depression from writing that fo-
cuses on linguistic cues. Rude et al. (2004) found
that college students with depression were signifi-
cantly more likely to use the first-person pronoun
I in personal essays than college students without
depression, and also used more words with neg-
ative emotional valence. Other work has found
differences in the frequency of different parts-of-
speech (POS) (De Choudhury et al., 2013) and in
the general topics chosen for discussion (Resnik et
al., 2015). Other work has accurately identified de-
pression (and differentiated PTSD and depression)
in Twitter social media texts with high accuracies
using n-gram language models (Coppersmith et al.,
2015). Similarly, Nguyen et al. (2014) showed
that specialized lexical norms and Linguistic Inquiry
and Word Count1 features significantly differentiate
clinical and control groups in blog post texts. Howes
et al. (2014) showed that lexical features (in style
and dialogue) could also be used to predict the sever-
ity of depression and anxiety during Cognitive Be-

1http://liwc.wpengine.com.

2



havioural Therapy treatment. It is not obvious that
these results generalize to the case where the topic
and structure of the narrative is constrained to a pic-
ture description.

2.2 Detecting Alzheimer’s disease from speech
A growing number of researchers have tackled the
problem of detecting dementia from speech and
language. Most of this work has focused on
Alzheimer’s disease (AD), which is the most com-
mon cause of dementia. Although the primary diag-
nostic symptom of AD is memory impairment, this
and other cognitive deficits often manifest in spon-
taneous language through word-finding difficulties,
a decrease in information content, and changes in
fluency, syntactic complexity, and prosody. Other
work, including that of Roark et al. (2007), focuses
on mild cognitive impairment, which is also broadly
applicable.

Thomas et al. (2005) classified spontaneous
speech samples from 95 AD patients and an unspeci-
fied number of controls by treating the problem as an
authorship attribution task, and employing a “com-
mon N-grams” approach. They were able to distin-
guish between patients with severe AD and controls
with a best accuracy of 94.5%, and between patients
with mild AD and controls with an 75.3% accuracy.

Habash and Guinn (2012) built classifiers to dis-
tinguish between AD and non-AD language samples
using 80 conversations between 31 AD patients and
57 cognitively normal conversation partners. They
found that features such as POS tags and measures
of lexical diversity were less useful than measuring
filled pauses, repetitions, and incomplete words, and
achieved a best accuracy of 79.5%.

Meilán et al. (2012) distinguished between 30 AD
patients and 36 healthy controls with temporal and
acoustic features alone, obtaining an accuracy of
84.8%. For each participant, their speech sample
consisted of two sentences read from a screen. The
discriminating features were percentage of voice
breaks, number of voice breaks, number of periods
of voice, shimmer, and noise-to-harmonics ratio.

Jarrold et al. (2014) used acoustic features, POS
features, and psychologically-motivated word lists
to distinguish between semi-structured interview re-
sponses from 9 AD participants and 9 controls with
an accuracy of 88%. They also confirmed their hy-

pothesis that AD patients would use more pronouns,
verbs, and adjectives and fewer nouns than controls.

Rentoumi et al. (2014) considered a slightly dif-
ferent problem: they used computational techniques
to differentiate between picture descriptions from
AD participants with and without additional vas-
cular pathology (n = 18 for each group). They
achieved an accuracy of 75% when they included
frequency unigrams and excluded binary unigrams,
syntactic complexity features, measures of vocabu-
lary richness, and information theoretic features.

Orimaye et al. (2014) obtained F-measure scores
up to 0.74 on transcripts from DementiaBank, com-
bining participants with different etiologies rather
than focusing on AD. In previous work, we also
studied data from DementiaBank (Fraser et al.,
2015). We computed acoustic and linguistic fea-
tures from the “Cookie Theft” picture descriptions
and distinguished 240 AD narratives from 233 con-
trol narratives with 81% accuracy using logistic re-
gression.

2.3 Relationship between dementia and
depression

The relationship between dementia and depression
is complicated, as the two conditions are not in-
dependent of each other and in fact frequently co-
occur. When someone is diagnosed with demen-
tia, feelings of depression are common. At the
same time, depression is a risk factor for devel-
oping Alzheimer’s disease (Korczyn and Halperin,
2009). The diagnosis of a third medical condition
(e.g., heart disease) can trigger depression and also
independently increase the risk of dementia. Sim-
ilarly, some risk factors for depression and demen-
tia are the same, such as alcohol use and cigarette
smoking (Thorpe, 2009). Furthermore, changes in
white matter connectivity have been linked to both
depression (Alexopoulos et al., 2008) and dementia
(Prins et al., 2004).

The prevalence of depression in AD has been es-
timated to be 30–50% (Lee and Lyketsos, 2003), al-
though these figures have been shown to vary widely
depending on the diagnostic method used (Müller-
Thomsen et al., 2005). In contrast, the prevalence
of depression in the general population older than
75 is estimated to be 7.2% (major depression) and
17.1% (depressive disorders) (Luppa et al., 2012).

3



The prevalence of Alzheimer’s disease is 11% for
people aged 65 and older, increasing to 33% for
people ages 85 and older (Alzheimer’s Association,
2015).

Symptoms which are common in both depression
and dementia include: poor concentration, impaired
attention (Korczyn and Halperin, 2009), apathy (Lee
and Lyketsos, 2003), changes to eating and sleeping
patterns, and reactive mood symptoms, e.g., tearful-
ness (Thorpe, 2009). However, both dementia and
depression are heterogeneous in presentation, which
can lead to many possible combinations of symp-
toms when they co-occur.

Studies examining spontaneous speech tasks to
discriminate between dementia and depression are
rare. Murray (2010) investigated whether clini-
cal depression could be distinguished from AD by
analyzing narrative speech. She found that there
were significant differences in the amount of infor-
mation that was conveyed in a picture description
task, with depressed participants communicating the
same amount of information as healthy controls, and
AD patients showing a reduction in information con-
tent. Other discourse measures relating to the quan-
tity of speech produced and the syntactic complexity
of the narrative did not differ between the groups. In
contrast to the current work, the study described in
Murray (2010) did not include participants with both
dementia and depression, involved a much smaller
data set (49 participants across 3 groups), and did
not seek to make predictions from the data.

3 Methods

3.1 Data
We use narrative speech data from the Pitt corpus
in the DementiaBank database2. These data were
collected between 1983 and 1988 as part of the
Alzheimer Research Program at the University of
Pittsburgh. Detailed information about the study
cohort is available from Becker et al. (1994), and
demographic information is presented for each ex-
periment below in Tables 1 and 3. Diagnoses were
made on the basis of a personal history and a neu-
ropsychological battery; a subset of these diagnoses
were confirmed post-mortem. The language sam-
ples were elicited using the “Cookie Theft” picture

2https://talkbank.org/DementiaBank/

description task from the Boston Diagnostic Apha-
sia Examination (BDAE) (Goodglass and Kaplan,
1983), in which participants are asked to describe
everything they see going on in a picture. We ex-
tract features from both the acoustic files (converted
from MP3 to 16-bit mono WAV format with a sam-
pling rate of 16 kHz) and the associated transcripts.
All examiner speech is excluded from the sample.

A subset of the participants also have Hamilton
Depression Rating Scale (HAM-D) scores (Hamil-
ton, 1960). The HAM-D is still one of the gold stan-
dards for depression rating (although it has also re-
ceived criticism; see Bagby et al. (2014) for an ex-
ample). It consists of 17 questions, for which the
patient’s responses are rated from 0–4 or 0–2 by the
examiner. A total score between 0–7 is considered
normal, 8–16 indicates mild depression, 17–23 indi-
cates moderate depression, and greater than 24 indi-
cates severe depression (Zimmerman et al., 2013).

3.2 Features

We extract a large number of textual features
(including part-of-speech tags, parse constituents,
psycholinguistic measures, and measures of com-
plexity, vocabulary richness, and informativeness),
and acoustic features (including fluency measures,
MFCCs, voice quality features, and measures of pe-
riodicity and symmetry). A complete list of features
is given in the Supplementary Material, and addi-
tional details are reported by Fraser et al. (2015).

3.3 Classification

We select a subset of the extracted features using a
correlation-based filter. Features are ranked by their
correlation with diagnosis and only the top N fea-
tures are selected, where we vary N from 5 to 400.
The selected features are fed to a machine learn-
ing classifier; in this study we compare logistic re-
gression (LR) with support vector machines (SVM)
(Hall et al., 2009). We use a cross-validation frame-
work and report the average accuracy across folds.
The data is partitioned across folds such that sam-
ples from a single speaker occur in either the training
set or test set, but never both. Error bars are com-
puted using the standard deviation of the accuracy
across folds. In some cases we also report sensitivity
and specificity, where sensitivity indicates the pro-
portion of people with AD (or depression) who were

4



AD Controls Sig.
n = 196 n = 128

Age 71.7 (8.7) 63.7 (7.6) **
Education 12.4 (2.9) 13.9 (2.4) **
Sex (M/F) 66/130 49/79

Table 1: Mean and standard deviation of demo-
graphic information for participants in Experiment
I. ** indicates p < 0.01.

correctly identified as such, and specificity indicates
the proportion of controls who were correctly iden-
tified as such.

4 Experiment I: Does depression affect
classification accuracy?

Our first experiment examines whether depression
is a confounding factor in our current diagnostic
pipeline. To answer this question, we consider the
subset of narratives for which associated HAM-D
scores are available. This leaves a set of 196 AD
narratives and 128 control narratives from 150 AD
participants and 80 control participants. Since par-
ticipants may have different scores on different vis-
its, we consider data per narrative, rather than per
speaker. Demographic information is given in Ta-
ble 1. The groups are not matched for age or educa-
tion, which is a limitation of the complete data set as
well; the AD participants tend to be both older and
less educated.

We then perform the classification procedure us-
ing the analysis pipeline described above, with 10-
fold cross-validation. The results for a logistic re-
gression and SVM classifier are shown in Figure 1.
This is a necessary first step to examine if depression
is a confounding factor.

Choosing the best result of 0.799 (SVM classi-
fier, 70 features), we then perform a more detailed
analysis. Accuracy, sensitivity, and specificity for
the full data set are reported in the first row of Ta-
ble 2. We first break down the data into two sep-
arate groups: those with a Hamilton score greater
than 7 (i.e., “depressed”) and those with a Hamil-
ton score less than or equal to 7 (“non-depressed”)
(Zimmerman et al., 2013). The accuracy, sensitivity,
and specificity for these sub-groups are also reported
in Table 2. Because there are far more AD partici-
pants with depression (n = 65) than controls with

Figure 1: Classification accuracy on the task of dis-
tinguishing AD from control narratives for varying
feature set sizes.

Data set Baseline Accuracy Sens. Spec.
All 0.605 0.799 0.826 0.758
Depressed 0.743 0.864 0.846 1.000
Non-depressed 0.552 0.780 0.816 0.739

Table 2: Accuracy, sensitivity, and specificity for
all participants, depressed participants, and non-
depressed participants.

depression (n = 9), we also report the accuracy of
a majority class classifier as a baseline with which
to compare the reported accuracies. Alternatives to
this approach, including synthetically balancing the
classes, e.g., with synthetic minority oversampling
(Chawla et al., 2002), is to be the subject of future
work.

A key result from this experiment is that although
there are only a few control participants who are
depressed, none of those are misclassified as AD
(specificity = 1.0 in this case).

Furthermore, if we partition the participants by
accuracy (those who were classified correctly ver-
sus incorrectly), we find no significant difference on
HAM-D scores (p > 0.05). This suggests that the
accuracy of the classifier is not affected by the pres-
ence or absence of depression.

5 Experiment II: Can we detect depression
in Alzheimer’s disease?

In our second experiment, we tackle the problem
of detecting depression when it is comorbid with

5



Depressed Non-dep. Sig.
n = 65 n = 65

Age 71.4 (8.6) 71.6 (8.6)
Education 11.7 (2.6) 12.9 (3.0) *
Sex (M/F) 21/44 19/46
MMSE 18.1 (5.5) 17.9 (5.4)

Table 3: Mean and standard deviation of demo-
graphic information for AD participants in Experi-
ment II. * indicates p < 0.05.

Figure 2: Classification accuracy on the task of dis-
tinguishing depressed from non-depressed AD nar-
ratives for varying feature set sizes.

Alzheimer’s disease. From the previous section, we
have 65 narratives from participants with both AD
and depression (HAM-D > 7). We select an addi-
tional 65 narratives from participants with AD but
no depression. These additional data are selected
randomly but such that participants are matched for
dementia severity, age, and sex. Demographic infor-
mation is given in Table 3.

5.1 Standard processing pipeline

We begin by using our standard processing pipeline
to assess whether it is capable of detecting depres-
sion. The classification accuracies are given in Fig-
ure 2. In this case, since the groups are the same
size, the baseline accuracy is 0.5. The best accu-
racy of 0.658 is achieved with the LR classifier using
60 features (sensitivity: 0.707, specificity: 0.610).
This represents a significant increase (paired t-test,
p < 0.05) of 15 percentage points over the random
baseline, but there is clearly room for improvement.

Rank Feature r Trend
1 Skewness MFCC 1 0.270 ↑
2 Info unit: boy −0.265 ↓
3 Mean ΔΔMFCC 8 0.229 ↑
4 VP → VB NP −0.223 ↓
5 Kurtosis MFCC 4 0.223 ↑
6 Kurtosis MFCC 3 0.217 ↑
7 Kurtosis ΔMFCC 2 0.213 ↑
8 Skewness ΔΔMFCC 2 0.211 ↑
9 Kurtosis MFCC 10 0.209 ↑
10 Determiners −0.206 ↓

Table 4: Highly ranked features for distinguishing
people with AD and depression from people with
only AD. The third column shows the correlation
with diagnosis, and the fourth column shows the di-
rection of the trend (increasing or decreasing) with
depression.

Data set Baseline Accuracy Sens. Spec.
All 0.500 0.658 0.707 0.610
Females 0.511 0.588 0.519 0.653
Males 0.525 0.650 0.580 0.717

Table 5: Accuracy, sensitivity, and specificity for all
participants, just females, and just males.

Table 4 shows the features which are most highly
correlated with diagnosis (over all data). Even for
the top-ranked features, the correlation is weak, and
the difference between groups is not significant af-
ter correcting for multiple comparisons. We there-
fore cannot conclusively draw conclusions about the
selected features, although we do note the apparent
importance of the MFCC features here.

5.2 Sex-dependent classification

Given that acoustic features naturally vary across the
sexes, and that previous work achieved better results
using sex-dependent classifiers, we also consider a
sex-dependent configuration. The drawback to this
approach is the reduction in data, particularly for
males. In these experiments we attempt to classify
21 males with depression+AD versus 19 males with
AD only, and 44 females with depression+AD ver-
sus 46 females with AD only. The results for these
experiments are shown in Figure 3, and the best ac-
curacies are given in Table 5.

The features which are most correlated with di-

6



(a) Females

(b) Males

Figure 3: Sex-dependent classification accuracy
on the task of distinguishing depressed from non-
depressed AD narratives for varying feature set
sizes.

agnosis for females are listed in Table 6a, and those
which are most correlated with diagnosis for males
are listed in Table 6b. Again, the selected fea-
tures tend to be either informational, grammatical,
or cepstral in nature, although none of the differ-
ences are significant after correcting for multiple
comparisons.

5.3 Additional features

To help our classifiers better distinguish between
people with and without depression, we implement
a number of additional features which have been re-
ported to be valuable in detecting depression. Many
of the acoustic features from the literature were al-
ready present in our feature set, but we now consider
a number of glottal features, including the mean
and standard deviations of the maximum voiced fre-

Rank Feature r Trend
1 Info unit: boy −0.323 ↓
2 Mean ΔMFCC 9 0.284 ↑
3 VP → VB NP −0.274 ↓
4 Kurtosis MFCC 3 0.266 ↑
5 Kurtosis Δ energy 0.261 ↑
6 Skewness MFCC 1 0.260 ↑
7 Kurtosis MFCC 4 0.256 ↑
8 NP → PRP$ NNS 0.251 ↑
9 Skewness ΔΔMFCC 2 0.249 ↑
10 NP → NP NP . 0.243 ↑

(a) Females
Rank Feature r Trend
1 Mean ΔΔMFCC 9 0.447 ↑
2 Skewness ΔΔMFCC 12 −0.406 ↓
3 VP → VB S 0.405 ↑
4 Mean ΔΔMFCC 2 −0.381 ↓
5 Info unit: stool 0.352 ↑
6 VP → VBG NP −0.351 ↓
7 Key word: chair 0.346 ↑
8 Mean ΔMFCC 11 −0.325 ↓
9 Key word: girl −0.318 ↓
10 Mean ΔΔMFCC 8 0.316 ↑

(b) Males

Table 6: Highly ranked features for distinguishing
individuals with AD and depression from individu-
als with only AD, in the sex-dependent case. (No
differences are significant after correcting for multi-
ple comparisons.)

quency, glottal closure instants, linear prediction
residuals, peak slope, glottal flow (and derivative),
normalized amplitude quotient (NAQ), quasi-open
quotient (QOQ), harmonic richness factor, parabolic
spectral parameter, and cepstral peak prominence.
These features are implemented in the COVAREP
toolkit (version 1.4.1) (Degottex et al., 2014).

We also include three additional psycholinguistic
variables relating to the affective qualities of words:
valence, arousal, and dominance. Valence describes
the degree of positive or negative emotion associated
with a word, arousal describes the intensity of the
emotion associated with a word, and dominance de-
scribes the degree of control associated with a word.
We use the crowd-sourced norms presented by War-
riner et al. (2013) for their broad coverage, and mea-

7



sure the mean and maximum value of each variable.
Finally, we count the frequency of occurrence of

first-person words (I, me, my, mine). In general, the
picture description task is completed in the third per-
son, but first-person words do occur.

However, including these new features actually
had a slightly negative effect on the sex-independent
classification, reducing the maximum accuracy from
0.658 to 0.650, as well as on the males-only case,
reducing maximum accuracy from 0.650 to 0.585.
This suggests that some of the new features are be-
ing selected in individual training folds, but not gen-
eralizing to the test folds. In contrast, the new fea-
tures did make a small, incremental improvement
in the females-only case, from 0.588 to 0.609 for
females. The new features that were most highly
ranked for females were the standard deviation of
the peak slope (rank 12, r = −0.237) and the stan-
dard deviation of NAQ (rank 35, r = −0.186), both
showing a weak negative correlation with diagnosis.
The most useful new feature for males was the mean
QOQ (rank 16, r = 0.273), with a weak positive cor-
relation with diagnosis.

6 Conclusion

In this paper, we considered two questions. The first
is related to previous work in the field showing that
speech analysis and machine learning can lead to
good, but not perfect, differentiation between par-
ticipants with AD and healthy controls. We won-
dered whether some control participants were being
misclassified as having AD when in fact they were
depressed. However, in our experiment we found
that none of the 9 depressed controls were misclas-
sified as having AD. This is a small sample, but it is
consistent with the findings of Murray (2010), who
found that although AD participants and controls
could be distinguished through analysis of their pic-
ture descriptions, there were no differences between
depressed participants and controls.

We then considered only participants with AD,
and tried to distinguish between those with comor-
bid depression and those without. Our best accu-
racy for this task was 0.658, which is considerably
lower than reported accuracies for detecting depres-
sion in the absence of AD, but reflects the difficulty
of the task given the wide overlap of symptoms in

the two conditions. In fact, previous work on detect-
ing depression from speech has focused overwhelm-
ingly on young and otherwise healthy participants,
and much work is needed on detecting depression in
other populations and with other comorbidities.

One limitation of this work is the type of speech
data available; previous work suggests that emo-
tional speech is more informative for detecting de-
pression. Another limitation is that we are assigning
our participants to the depressed and non-depressed
groups on the basis of a single test score, rather than
a confirmed clinical diagnosis. A related factor to
consider is the relatively mild depression that is ob-
served in this data set, which was developed for the
study of AD rather than depression – only 8 par-
ticipants met the criteria for “moderate” depression,
and none met the criteria for severe depression. Fur-
thermore, while the controls in Experiment 2 all had
scores below the threshold for mild depression, in
most cases the scores were still non-zero, and so the
classification task is not as clearly binary as we have
framed it here. Finally, limitations of the dataset
introduced issues of confounding variables (namely
age and education), and prohibited us from contrast-
ing speech from participants with only depression
versus those with only AD. We are currently under-
taking our own data collection to overcome the var-
ious challenges of this dataset.

Depression and Alzheimer’s disease both present
in different syndromes, and so it is probably unre-
alistic to clearly delineate between the many poten-
tial combinations of depression, AD, and other pos-
sible medical conditions through the analysis of a
single language task. On the other hand, previous
work suggests that this type of analysis can be very
fine-grained and sensitive to subtle cognitive impair-
ments. Ideally, future work will focus directly on the
task of distinguishing AD from depression, using
clinically validated data with a stronger emotional
component.

Acknowledgments

The authors would like to thank Dr. John Strauss for
his comments on an early draft of this paper. This
work was supported by the Natural Sciences and
Engineering Research Council of Canada (NSERC),
grant number RGPIN-2014-06020.

8



References

George S. Alexopoulos, Christopher F. Murphy, Faith M.
Gunning-Dixon, Vassilios Latoussakis, Dora Kanel-
lopoulos, Sibel Klimstra, Kelvin O. Lim, and
Matthew J. Hoptman. 2008. Microstructural white
matter abnormalities and remission of geriatric de-
pression. The American Journal of Psychiatry,
165(2):238–244.

Sharifa Alghowinem, Roland Goecke, Michael Wagner,
Julien Epps, Michael Breakspear, Gordon Parker, et al.
2012. From joyous to clinically depressed: Mood
detection using spontaneous speech. In Proceedings
of the Florida Artificial Intelligence Research Society
(FLAIRS) Conference, pages 141–146.

Sharifa Alghowinem, Roland Goecke, Michael Wagner,
Julien Epps, Tom Gedeon, Michael Breakspear, and
Gordon Parker. 2013. A comparative study of dif-
ferent classifiers for detecting depression from sponta-
neous speech. In Acoustics, Speech and Signal Pro-
cessing (ICASSP), 2013 IEEE International Confer-
ence on, pages 8022–8026.

Paavo Alku, Helmer Strik, and Erkki Vilkman. 1997.
Parabolic spectral parameter – a new method for quan-
tification of the glottal flow. Speech Communication,
22(1):67–79.

Alzheimer’s Association. 2015. 2015 Alzheimer’s dis-
ease facts and figures. Alzheimer’s & Dementia,
11(3):332.

R. Michael Bagby, Andrew G. Ryder, Deborah R.
Schuller, and Margarita B. Marshall. 2014. The
Hamilton Depression Rating Scale: has the gold stan-
dard become a lead weight? American Journal of Psy-
chiatry, 161(12):2163–2177.

James T. Becker, François Boiler, Oscar L. Lopez, Judith
Saxton, and Karen L. McGonigle. 1994. The natural
history of Alzheimer’s disease: description of study
cohort and accuracy of diagnosis. Archives of Neurol-
ogy, 51(6):585–594.

Sarah D. Breedin, Eleanor M. Saffran, and Myrna F.
Schwartz. 1998. Semantic factors in verb retrieval:
An effect of complexity. Brain and Language, 63:1–
31.

Étienne Brunet. 1978. Le vocabulaire de Jean Girau-
doux structure et évolution. Éditions Slatkine.

Marc Brysbaert and Boris New. 2009. Moving beyond
Kučera and Francis: A critical evaluation of current
word frequency norms and the introduction of a new
and improved word frequency measure for American
English. Behavior Research Methods, 41(4):977–990.

Romola S. Bucks, Sameer Singh, Joanne M. Cuer-
den, and Gordon K. Wilcock. 2000. Analysis of
spontaneous, conversational speech in dementia of

Alzheimer type: Evaluation of an objective tech-
nique for analysing lexical performance. Aphasiology,
14(1):71–91.

Jieun Chae and Ani Nenkova. 2009. Predicting the flu-
ency of text with shallow structural features: case stud-
ies of machine translation and human-written text. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 139–147.

Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall,
and W. Philip Kegelmeyer. 2002. SMOTE: Synthetic
Minority Over-sampling Technique. Journal of Artifi-
cial Intelligence Research, 16:321–357.

Jeffrey F. Cohn, Tomas Simon Kruez, Iain Matthews,
Ying Yang, Minh Hoai Nguyen, Margara Tejera
Padilla, Feng Zhou, and Fernando De La Torre. 2009.
Detecting depression from facial actions and vocal
prosody. In Affective Computing and Intelligent In-
teraction and Workshops, 2009. ACII 2009. 3rd Inter-
national Conference on, pages 1–7.

Glen Coppersmith, Mark Dredze, Craig Harman, Kristy
Hollingshead, and Margaret Mitchell. 2015. CLPsych
2015 shared task: Depression and PTSD on Twitter. In
Proceedings of the Shared Task for the NAACL Work-
shop on Computational Linguistics and Clinical Psy-
chology.

Michael A. Covington and Joe D. McFall. 2010. Cutting
the Gordian knot: The moving-average type–token ra-
tio (MATTR). Journal of Quantitative Linguistics,
17(2):94–100.

Bernard Croisile, Bernadette Ska, Marie-Josee Bra-
bant, Annick Duchene, Yves Lepage, Gilbert Aimard,
and Marc Trillet. 1996. Comparative study of
oral and written picture description in patients with
Alzheimer’s disease. Brain and Language, 53(1):1–
19.

Nicholas Cummins, Julien Epps, Michael Breakspear,
and Roland Goecke. 2011. An investigation of de-
pressed speech detection: Features and normalization.
In Proceedings of the 12th Annual Conference of the
International Speech Communication Association (IN-
TERSPEECH 2011), pages 2997–3000.

Nicholas Cummins, Stefan Scherer, Jarek Krajewski,
Sebastian Schnieder, Julien Epps, and Thomas F
Quatieri. 2015. A review of depression and suicide
risk assessment using speech analysis. Speech Com-
munication, 71:10–49.

Munmun De Choudhury, Scott Counts, and Eric Horvitz.
2013. Social media as a measurement tool of depres-
sion in populations. In Proceedings of the 5th Annual
ACM Web Science Conference, pages 47–56.

Gilles Degottex, John Kane, Thomas Drugman, Tuomo
Raitio, and Stefan Scherer. 2014. COVAREP – a col-

9



laborative voice analysis repository for speech tech-
nologies. In Acoustics, Speech and Signal Process-
ing (ICASSP), 2014 IEEE International Conference
on, pages 960–964.

Thomas Drugman and Yannis Stylianou. 2014. Maxi-
mum voiced frequency estimation: Exploiting ampli-
tude and phase spectra. Signal Processing Letters,
IEEE, 21(10):1230–1234.

Thomas Drugman, Baris Bozkurt, and Thierry Dutoit.
2012. A comparative study of glottal source esti-
mation techniques. Computer Speech & Language,
26(1):20–34.

Thomas Drugman. 2014. Maximum phase modeling for
sparse linear prediction of speech. Signal Processing
Letters, IEEE, 21(2):185–189.

Rubén Fraile and Juan Ignacio Godino-Llorente. 2014.
Cepstral peak prominence: A comprehensive analysis.
Biomedical Signal Processing and Control, 14:42–54.

Kathleen C. Fraser, Jed A. Meltzer, and Frank Rudzicz.
2015. Linguistic features identify Alzheimer’s disease
in narrative speech. Journal of Alzheimer’s Disease,
49(2):407–422.

Ken J. Gilhooly and Robert H. Logie. 1980. Age-
of-acquisition, imagery, concreteness, familiarity, and
ambiguity measures for 1,944 words. Behavior Re-
search Methods, 12:395–427.

Harold Goodglass and Edith Kaplan. 1983. Boston di-
agnostic aphasia examination booklet. Lea & Febiger
Philadelphia, PA.

Anthony Habash and Curry Guinn. 2012. Language
analysis of speakers with dementia of the Alzheimer’s
type. In Association for the Advancement of Artificial
Intelligence (AAAI) Fall Symposium, pages 8–13.

Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10–18.

Max Hamilton. 1960. A rating scale for depression.
Journal of Neurology, Neurosurgery, and Psychiatry,
23(1):56–62.

Antony Honoré. 1979. Some simple measures of rich-
ness of vocabulary. Association for Literary and Lin-
guistic Computing Bulletin, 7(2):172–177.

Christine Howes, Matthew Purver, and Rose McCabe.
2014. Linguistic indicators of severity and progress
in online text-based therapy for depression. In In Pro-
ceedings of the ACL Workshop on Computational Lin-
guistics and Clinical Psychology.

William Jarrold, Bart Peintner, David Wilkins, Dimitra
Vergryi, Colleen Richey, Maria Luisa Gorno-Tempini,
and Jennifer Ogar. 2014. Aided diagnosis of dementia
type through computer-based analysis of spontaneous

speech. In Proceedings of the ACL Workshop on Com-
putational Linguistics and Clinical Psychology, pages
27–36.

John Kane and Christer Gobl. 2011. Identifying regions
of non-modal phonation using features of the wavelet
transform. In 12th Annual Conference of the Interna-
tional Speech Communication Association 2011 (IN-
TERSPEECH 2011), pages 177–180.

Amos D. Korczyn and Ilan Halperin. 2009. Depression
and dementia. Journal of the Neurological Sciences,
283(1):139–142.

Hochang B. Lee and Constantine G. Lyketsos. 2003. De-
pression in Alzheimer’s disease: heterogeneity and re-
lated issues. Biological Psychiatry, 54(3):353–362.

Lu-Shih Alex Low, Namunu C. Maddage, Margaret
Lech, Lisa B. Sheeber, and Nicholas B. Allen. 2011.
Detection of clinical depression in adolescents’ speech
during family interactions. Biomedical Engineering,
IEEE Transactions on, 58(3):574–586.

Xiaofei Lu. 2010. Automatic analysis of syntactic
complexity in second language writing. International
Journal of Corpus Linguistics, 15(4):474–496.

Melanie Luppa, Claudia Sikorski, Tobias Luck,
L. Ehreke, Alexander Konnopka, Birgitt Wiese,
Siegfried Weyerer, H-H. König, and Steffi G. Riedel-
Heller. 2012. Age-and gender-specific prevalence
of depression in latest-life–systematic review and
meta-analysis. Journal of Affective Disorders,
136(3):212–221.

Juan J.G. Meilán, Francisco Martı́nez-Sánchez, Juan
Carro, José A. Sánchez, and Enrique Pérez. 2012.
Acoustic markers associated with impairment in lan-
guage processing in Alzheimer’s disease. The Spanish
Journal of Psychology, 15(02):487–494.

Elliot Moore, Mark Clements, John W. Peifer, and Lydia
Weisser. 2008. Critical analysis of the impact of glot-
tal features in the classification of clinical depression
in speech. Biomedical Engineering, IEEE Transac-
tions on, 55(1):96–107.

Tomas Müller-Thomsen, Sönke Arlt, Ulrike Mann, Rein-
hard Maß, and Stefanie Ganzer. 2005. Detecting de-
pression in Alzheimer’s disease: evaluation of four dif-
ferent scales. Archives of Clinical Neuropsychology,
20(2):271–276.

Laura L. Murray. 2010. Distinguishing clinical depres-
sion from early Alzheimer’s disease in elderly peo-
ple: Can narrative analysis help? Aphasiology, 24(6-
8):928–939.

Thin Nguyen, Dinh Phung, Bo Dao, Svetha Venkatesh,
and Michael Berk. 2014. Affective and content analy-
sis of online depression communities. IEEE Transac-
tions on Affective Computing, 5(3):217–226.

10



Sylvester Olubolu Orimaye, Jojo Sze-Meng Wong, and
Karen Jennifer Golden. 2014. Learning predictive lin-
guistic features for Alzheimer’s disease and related de-
mentias using verbal utterances. In Proceedings of the
1st Workshop on Computational Linguistics and Clin-
ical Psychology (CLPsych), pages 78–87.

Asli Ozdas, Richard G. Shiavi, Stephen E. Silverman,
Marilyn K. Silverman, and D. Mitchell Wilkes. 2004.
Investigation of vocal jitter and glottal flow spectrum
as possible cues for depression and near-term suicidal
risk. Biomedical Engineering, IEEE Transactions on,
51(9):1530–1540.

Niels D. Prins, Ewoud J. van Dijk, Tom den Heijer,
Sarah E. Vermeer, Peter J. Koudstaal, Matthijs Oud-
kerk, Albert Hofman, and Monique M.B. Breteler.
2004. Cerebral white matter lesions and the risk of
dementia. Archives of Neurology, 61(10):1531–1534.

Vassiliki Rentoumi, Ladan Raoufian, Samrah Ahmed,
Celeste A. de Jager, and Peter Garrard. 2014. Fea-
tures and machine learning classification of connected
speech samples from patients with autopsy proven
Alzheimer’s disease with and without additional vas-
cular pathology. Journal of Alzheimer’s Disease,
42:S3–S17.

Philip Resnik, William Armstrong, Leonardo Claudino,
and Thang Nguyen. 2015. The University of Mary-
land CLPsych 2015 shared task system. In Proceed-
ings of the 2nd Workshop on Computational Linguis-
tics and Clinical Psychology: From Linguistic Signal
to Clinical Reality, pages 54–60, Denver, Colorado,
June 5.

Brian Roark, John-Paul Hosom, Margaret Mitchell, and
Jeffrey A. Kaye. 2007. Automatically derived spoken
language markers for detecting mild cognitive impair-
ment. In Proceedings of the 2nd International Confer-
ence on Technology and Aging (ICTA), Toronto, ON,
June.

Stephanie Rude, Eva-Maria Gortner, and James Pen-
nebaker. 2004. Language use of depressed and
depression-vulnerable college students. Cognition &
Emotion, 18(8):1121–1133.

Stefan Scherer, Giota Stratou, Jonathan Gratch, and
Louis-Philippe Morency. 2013. Investigating voice
quality as a speaker-independent indicator of depres-
sion and PTSD. In Proceedings of Interspeech, pages
847–851.

Hans Stadthagen-Gonzalez and Colin J. Davis. 2006.
The Bristol norms for age of acquisition, imageabil-
ity, and familiarity. Behavior Research Methods,
38(4):598–605.

Calvin Thomas, Vlado Keselj, Nick Cercone, Kenneth
Rockwood, and Elissa Asp. 2005. Automatic detec-
tion and rating of dementia of Alzheimer type through

lexical analysis of spontaneous speech. In Proceed-
ings of the IEEE International Conference on Mecha-
tronics and Automation, pages 1569–1574.

Lilian Thorpe. 2009. Depression vs. dementia: How
do we assess? The Canadian Review of Alzheimer’s
Disease and Other Dementias, pages 17–21.

Amy Beth Warriner, Victor Kuperman, and Marc Brys-
baert. 2013. Norms of valence, arousal, and domi-
nance for 13,915 English lemmas. Behavior Research
Methods, 45(4):1191–1207.

Victor Yngve. 1960. A model and hypothesis for lan-
guage structure. Proceedings of the American Physi-
cal Society, 104:444–466.

Mark Zimmerman, Jennifer H. Martinez, Diane Young,
Iwona Chelminski, and Kristy Dalrymple. 2013.
Severity classification on the Hamilton depression rat-
ing scale. Journal of Affective Disorders, 150(2):384–
388.

11


