



















































Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4915–4920
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

4915

Unsupervised Neural Word Segmentation for Chinese
via Segmental Language Modeling

Zhiqing Sun
Peking University

1500012783@pku.edu.cn

Zhi-Hong Deng
Peking University

zhdeng@pku.edu.cn

Abstract

Previous traditional approaches to unsuper-
vised Chinese word segmentation (CWS) can
be roughly classified into discriminative and
generative models. The former uses the care-
fully designed goodness measures for candi-
date segmentation, while the latter focuses on
finding the optimal segmentation of the high-
est generative probability. However, while
there exists a trivial way to extend the dis-
criminative models into neural version by us-
ing neural language models, those of genera-
tive ones are non-trivial. In this paper, we pro-
pose the segmental language models (SLMs)
for CWS. Our approach explicitly focuses on
the segmental nature of Chinese, as well as
preserves several properties of language mod-
els. In SLMs, a context encoder encodes the
previous context and a segment decoder gen-
erates each segment incrementally. As far as
we know, we are the first to propose a neu-
ral model for unsupervised CWS and achieve
competitive performance to the state-of-the-
art statistical models on four different datasets
from SIGHAN 2005 bakeoff.

1 Introduction

Unlike English and many other languages, Chi-
nese sentences have no explicit word boundaries.
Therefore, Chinese Word Segmentation (CWS) is
a crucial step for many Chinese Natural Language
Processing (NLP) tasks such as syntactic pars-
ing, information retrieval and word representation
learning (Grave et al., 2018).

Recently, neural approaches for supervised
CWS are attracting huge interest. A great quan-
tities of neural models, e.g., tensor neural network
(Pei et al., 2014), recursive neural network (Chen
et al., 2015a), long-short-term-memory (RNN-
LSTM) (Chen et al., 2015b) and convolutional
neural network (CNN) (Wang and Xu, 2017), have

𝒚𝟏 〈𝒆𝒐𝒔〉

𝒚𝟏

𝒚𝟐 𝒚𝟑

𝒚𝟐

〈𝒆𝒐𝒔〉

𝒚3

𝒚𝟒 〈𝒆𝒐𝒔〉

𝒚𝟒

𝒚𝟏 𝒚𝟐 𝒚𝟑 𝒚𝟒𝒚𝟎

Context Encoder

Segment Decoder

Figure 1: A Segmental Language Model (SLM) works
on y = y1y2y3y4 with the candidate segmentation
y1, y2:3 and y4, where y0 is an additional start sym-
bol which is kept same for all sentences.

been proposed and given competitive results to the
best statistical models (Sun, 2010). However, the
neural approaches for unsupervised CWS have not
been investigated.

Previous unsupervised approaches to CWS can
be roughly classified into discriminative and gen-
erative models. The former uses carefully de-
signed goodness measures for candidate segmen-
tation, while the latter focuses on designing sta-
tistical models for Chinese and finds the optimal
segmentation of the highest generative probability.

Popular goodness measures for discriminative
models include Mutual Information (MI) (Chang
and Lin, 2003), normalized Variation of Branch-
ing Entropy (nVBE) (Magistry and Sagot, 2012)
and Minimum Description Length (MDL) (Mag-
istry and Sagot, 2013). There is a trivial way to
extend these statistical discriminative approaches,
because we can simply replace the n-gram lan-
guage models in these approaches by neural lan-
guage models (Bengio et al., 2003). There may



4916

exists other more sophisticated neural discrimina-
tive approaches, but it is not the focus of this paper.

For generative approaches, typical statistical
models includes Hidden Markov Model (HMM)
(Chen et al., 2014), Hierarchical Dirichlet Pro-
cess (HDP) (Goldwater et al., 2009) and Nested
Pitman-Yor Process (NPY) (Mochihashi et al.,
2009). However, none of them can be easily ex-
tended into a neural model. Therefore, neural gen-
erative models for word segmentation are remain-
ing to be investigated.

In this paper, we proposed the Segmental Lan-
guage Models (SLMs), a neural generative model
that explicitly focuses on the segmental nature of
Chinese: SLMs can directly generate segmented
sentences and give the corresponding generative
probability. We evaluate our methods on four dif-
ferent benchmark datasets from SIGHAN 2005
bakeoff (Emerson, 2005), namely PKU, MSR, AS
and CityU. To our knowledge, we are the first to
propose a neural model for unsupervised Chinese
word segmentation and achieve competitive per-
formance to the state-of-the-art statistical models
on four different datasets.1

2 Segmental Language Models

In this section, we present our segmental language
models (SLMs). Notice that in Chinese NLP, char-
acters are the atom elements. Thus in the context
of CWS, we use “character” instead of “word” for
language modeling.

2.1 Language Models

The goal of language modeling is to learn the joint
probability function of sequences of characters in
a language. However, This is intrinsically diffi-
cult because of the curse of dimensionality. Tradi-
tional approaches obtain generalization based on
n-grams, while neural approaches introduce a dis-
tributed representation for characters to fight the
curse of dimensionality.

A neural Language Model (LM) can give the
conditional probability of the next character given
the previous ones, and is usually implemented by
a Recurrent Neural Network (RNN):

ht = f(yt−1,ht−1) (1)

p(yt|y1:t−1) = g(ht,yt) (2)
1Our implementation can be found at https://

github.com/Edward-Sun/SLM

where yt is the distributed representation for the
tth character and ht represents the information of
the previous characters.

2.2 Segmental Language Models

Similar to neural language modeling, the goal of
segmental language modeling is to learn the joint
probability function of the segmented sequences
of characters. Thus, for each segment, we have:

p̂(y
(i)
t |y

(i)
1:t−1,y

(1:i−1)) = g(h
(i)
t ,y

(i)
t ) (3)

where y(i)t is the distributed representation for
the tth character in the ith segment and y(1:i−1)

is the previous segments. And the concatenation
of all segments y(i)1:Ti is exactly the whole sentence
y1:T , where Ti is the length of the ith segment y(i),
T is the length of the sentence y.

Moreover, we introduce a context encoder RNN
to process the character sequence y(1:i−1) in order
to make y(i)t conditional on y

(1:i−1). Specifically,
we initialize h(i)0 with the context encoder’s output
of y(1:i−1).

Notice that although we have an encoder and
the segment decoder g, SLM is not an encoder-
decoder model. Because the content that the de-
coder generates is not the same as what the en-
coder provides.

Figure 1 illustrates how SLMs work with a can-
didate segmentation.

2.3 Properties of SLMs

However, in unsupervised scheme, the given sen-
tences are not segmented. Therefore, the probabil-
ity for SLMs to generate a given sentence is the
joint probability of all possible segmentation:

p(y1:T ) =
∑

T1,T2,...

∏
i

p̂(y
(i)
1:Ti

)

=
∑

T1,T2,...

∏
i

Ti+1∏
t=1

p̂(y
(i)
t |y

(i)
0:t−1) (4)

where y(i)Ti+1 = 〈eos〉 is the end of segment
symbol at the end of each segment, and y(i)0 is the
context representation of y(1:i−1).

Moreover, for sentence generation, SLMs are
able to generate arbitrary sentences by generating

https://github.com/Edward-Sun/SLM
https://github.com/Edward-Sun/SLM


4917

segments one by one and stopping when gener-
ating end of sentence symbol 〈EOS〉. In addi-
tion, the time complexity is linear to the length of
the generated sentence, as we can keep the hid-
den state of the context encoder RNN and update
it when generating new words.

Last but not least, it is easy to verify that SLMs
preserve the probabilistic property of language
models: ∑

i

P (si) = 1 (5)

where si enumerates all possible sentences.
In summary, the segmental language models

can perfectly substitute vanilla language models.

2.4 Training and Decoding
Similar to language model, the training is achieved
by maximizing the training corpus log-likelihood:

L = − log p(y1:T ) (6)

Luckily, we can compute the loss objective
function in linear time complexity using dynamic
programming, given the initial condition that
p(y1:0) = 1:

p(y1:n) =
K∑
k=1

p(y1:n−k)p̂(yn−k+1:n) (7)

where p(·) is the joint probability of all possible
segmentation, p̂(·) is the probability of one seg-
ment andK is the maximal length of the segments.

We can also find the segmentation with maxi-
mal probability (namely, decoding) in linear time
using dynamic programming in the similarly way
with p̄(y1:0) = 1:

p̄(y1:n) =
K

max
k=1

p̄(y1:n−k)p̂(yn−k+1:n) (8)

δ(y1:n) = arg
K

max
k=1

p̄(y1:n−k)p̂(yn−k+1:n) (9)

where p̄ is the probability of the best segmenta-
tion and δ is used to trace back the decoding.

3 Experiments

3.1 Experimental Settings and Detail
We evaluate our models on SIGHAN 2005 bake-
off (Emerson, 2005) datasets and replace all the
punctuation marks with 〈punc〉, English charac-
ters with 〈eng〉 and Arabic numbers with 〈num〉

F1 score PKU MSR AS CityU
HDP 68.7 69.9 - -

HDP + HMM 75.3 76.3 - -
ESA 77.8 80.1 78.5 76.0

NPY-3 - 80.7 - 81.7
NPY-2 - 80.2 - 82.4
nVBE 80.0 81.3 76.6 76.7
Joint 81.1 81.7 - -

SLM-2 80.2 78.5 79.4 78.2
SLM-3 79.8 79.4 80.3 80.5
SLM-4 79.2 79.0 79.8 79.7

Table 1: Main results on SIGHAN 2005 bakeoff
datasets with previous state-of-the-art models (Chen
et al., 2014; Wang et al., 2011; Mochihashi et al., 2009;
Magistry and Sagot, 2012)

for all text and only consider segment the text be-
tween punctuations. Following Chen et al. (2014)
, we use both training data and test data for train-
ing and only test data are used for evaluation. In
order to make a fair comparison with the previous
works, we do not consider using other larger raw
corpus.

We apply word2vec (Mikolov et al., 2013) on
Chinese Gigaword corpus (LDC2011T13) to get
pretrained embedding of characters.

A 2-layer LSTM (Hochreiter and Schmidhuber,
1997) is used as the segment decoder and a 1-layer
LSTM is used as the context encoder.

We use stochastic gradient decent with a mini-
batch size of 256 and a learning rate of 16.0 to op-
timize the model parameters in the first 400 steps,
then we use Adam (Kingma and Ba, 2014) with
a learning rate of 0.005 to further optimize the
models. Model parameters are initialized by nor-
mal distributions as Glorot and Bengio (2010) sug-
gested. We use a gradient clip = 0.1 and apply a
dropout with dropout rate = 0.1 to the character
embedding and RNNs to prevent over-fit.

The standard word precision, recall and F1 mea-
sures (Emerson, 2005) are used to evaluate seg-
mentation performance.

3.2 Results and Analysis

Our final results are shown in Table 1, which
lists the results of several previous state-of-the-
art methods2, where we mark the best results in

2Magistry and Sagot (2012) evaluated their nVBE on the
training data, and the joint model of Chen et al. (2014) com-
bine HDP+HMM and is initialized with nVBE, so in princi-
ple these results can not be compared directly.



4918

F1 score PKU MSR AS CityU
SLM-4 79.2 79.0 79.8 79.7
SLM-4* 81.9 83.0 81.0 81.4
SLM-4† 87.5 84.3 84.2 86.0
SLM-4†* 87.3 84.8 83.9 85.8

Table 2: Results of SLM-4 incorporating ad hoc guide-
lines, where † represents using additional 1024 seg-
mented setences for training data and * represents using
a rule-based post-processing

boldface. We test the proposed SLMs with differ-
ent maximal segment length K = 2, 3, 4 and use
“SLM-K” to denote the corresponding model. We
do not tryK > 4 because there are rare words that
consist more than 4 characters.

As can be seen, it is hard to predict what choice
of K will give the best performance. This is be-
cause the exact definition of what a word remains
hard to reach and different datasets follow differ-
ent guidelines. Zhao and Kit (2008) use cross-
training of a supervised segmentation system in
order to have an estimation of the consistency be-
tween different segmentation guidelines and the
average consistency is found to be as low as 85
(f-score). Therefore, this can be regarded as a top
line for unsupervised CWS.

Table 1 shows that SLMs outperform previous
best discriminative and generative models on PKU
and AS datasets. This might be due to that the
segmentation guideline of our models are closer
to these two datasets.

Moreover, in the experiments, we observe that
Chinese particles often attach other words, for ex-
ample, “的” following adjectives and “了” follow-
ing verbs. It is hard for our generative models to
split them apart. Therefore, we propose a rule-
based post-processing module to deal with this
problem, where we explicitly split the attached
particles from other words.3 The post-processing
is applied on the results of “SLM-4”. In addi-
tion, we also evaluate “SLM-4” using the first
1024 sentences of the segmented training datasets
(about 5.4% of PKU, 1.2% of MSR, 0.1% of AS
and 1.9% of CityU) for training, in order to teach
“SLM-4” the corresponding ad hoc segmentation
guidelines. Table 2 shows the results.

We can find from the table that only 1024
guideline sentences can improve the performance
of “SLM-4” significantly. While rule-based

3The rules we use are listed in the appendix at https:
//github.com/Edward-Sun/SLM.

Error SLM-2 SLM-3 SLM-4
Insertion 7866 4803 3519
Deletion 3855 7518 8851

Table 3: Statistics of insertion errors and deletion errors
that SLM-K produces on PKU dataset

post-processing is very effective, “SLM-4†” can
outperform “SLM-4*” on all the four datasets.
Moreover, performance drops when applying the
rule-based post-processing to “SLM-4†” on three
datasets. These indicate that SLMs can learn the
empirical rules for word segmentation given only
a small amount of training data. And these guide-
line data can improve the performance of SLMs
naturally, superior to using explicit rules.

3.3 The Effect of the Maximal Segment
Length

The maximal segment length K represents the
prior knowledge we have for Chinese word seg-
mentation. For example K = 3 represents that
there are only unigrams, bigrams and trigrams in
the text. While there do exist words that con-
tain more than four characters, most of the Chi-
nese words are unigram or bigram. Therefore, K
denotes a trade-off between the accuracy of short
words and long words.

Specifically, we investigate two major segmen-
tation problems that might affect the accuracy of
word segmentation performance, namely, inser-
tion errors and deletion errors. An insertion error
insert a segment in a word, which split a correct
word. And an deletion error delete the segment
between two words, which results in a composi-
tion error (Li and Yuan, 1998). Table 3 shows the
statistics of different errors on PKU of our model
with differentK. We can observe that insertion er-
ror rate decrease with the increase of K, while the
deletion error rate increase with the increase ofK.

We also provide some examples in Table 4,
which are taken from the results of our models. It
clearly illustrates that different K could result in
different errors. For example, there is an insertion
error on “反过来” by SML-2, and a deletion error
on “促进” and “了” by SLM-4.

4 Related Work

Generative Models for CWS Goldwater et al.
(2009) are the first to proposed a generative
model for unsupervised word segmentation. They

https://github.com/Edward-Sun/SLM
https://github.com/Edward-Sun/SLM


4919

Model Example
SLM-2 而这些制度的完善反过来又促进了检察人员执法水平的进一步提高
SLM-3 而这些制度的完善反过来又促进了检察人员执法水平的进一步提高
SLM-4 而这些制度的完善反过来又促进了检察人员执法水平的进一步提高
Gold 而这些制度的完善反过来又促进了检察人员执法水平的进一步提高

Table 4: Examples of segmentation with different maximal segment length K

built a nonparametric Bayesian bigram language
model based on HDP (Teh et al., 2005). Mochi-
hashi et al. (2009) proposed a Bayesian hier-
archical language model using Pitman-Yor (PY)
process, which can generate sentences hierarchi-
cally. Chen et al. (2014) proposed a Bayesian
HMM model for unsupervised CWS inspired by
the character-based scheme in supervised CWS
task, where the hidden state of charaters are set to
{Single,Begin,End,Middle} to represents their
corresponding positions in the words. The seg-
mental language model is not a neural extension
of the above statistical models, as we model the
segments directly.

Segmental Sequence Models Sequence model-
ing via segmentations has been well investigated
by Wang et al. (2017), where they proposed the
Sleep-AWake Network (SWAN) for speech recog-
nition. SWAN is similar to SLM. However, SLMs
do not have sleep-awake states. And SLMs pre-
dict the following segment given the previous con-
text while SWAN tries to recover the information
in the encoded state. Therefore, the key differ-
ence is that SLMs are unsupervised language mod-
els while SWANs are supervised seq2seq models.
Thereafter, Huang et al. (2017) successfully apply
SWAN in their phrase-based machine translation.
Another related work in machine translation is the
online segment to segment neural transduction (Yu
et al., 2016), where the model is able to capture un-
bounded dependencies in both the input and output
sequences. Kong (2017) also proposed a Segmen-
tal Recurrent Neural Network (SRNN) with CTC
to solve segmental labeling problems.

5 Conclusion

In this paper, we proposed a neural generative
model for fully unsupervised Chinese word seg-
mentation (CWS). To the best of knowledge, this
is the first neural model for CWS. Our segmen-
tal language model is an intuitive generalization
of vanilla neural language models that directly
modeling the segmental nature of Chinese. Ex-

perimental results show that our models achieve
competitive performance to the previous state-of-
the-art statistical models on four datasets from
SIGHAN 2005. We also show the improvement of
incorporating ad hoc guidelines into our segmen-
tal language models. Our future work may include
the following two directions.

• In this work, we only consider the sequential
segmental language modeling. In the future,
we are interested in build a hierarchical neu-
ral language model like the Pitman-Yor pro-
cess.

• Like vanilla language models, the segmental
language models can also provide useful in-
formation for semi-supervised learning tasks.
It would also be interesting to explore our
models in the semi-supervised schemes.

Acknowledgements

This work is supported by the National Train-
ing Program of Innovation for Undergraduates
(URTP2017PKU001). We would also like to
thank the anonymous reviewers for their helpful
comments.

References
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and

Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of machine learning research,
3(Feb):1137–1155.

Jason S Chang and Tracy Lin. 2003. Unsupervised
word segmentation without dictionary. ROCLING
2003 Poster Papers, pages 355–359.

Miaohong Chen, Baobao Chang, and Wenzhe Pei.
2014. A joint model for unsupervised chinese word
segmentation. In Proceedings of the 2014 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 854–863, Doha, Qatar.
Association for Computational Linguistics.

Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing
Huang. 2015a. Gated recursive neural network for
chinese word segmentation. In Proceedings of the



4920

53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1744–1753, Beijing,
China. Association for Computational Linguistics.

Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu,
and Xuanjing Huang. 2015b. Long short-term mem-
ory neural networks for chinese word segmentation.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1197–1206, Lisbon, Portugal. Association for Com-
putational Linguistics.

Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the fourth SIGHAN workshop on Chinese language
Processing, volume 133, pages 123–133.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neu-
ral networks. In Proceedings of the Thirteenth In-
ternational Conference on Artificial Intelligence and
Statistics, pages 249–256.

Sharon Goldwater, Thomas L Griffiths, and Mark John-
son. 2009. A bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21–54.

Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-
mand Joulin, and Tomas Mikolov. 2018. Learning
word vectors for 157 languages. In Proceedings
of the International Conference on Language Re-
sources and Evaluation (LREC 2018).

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Po-Sen Huang, Chong Wang, Sitao Huang, Dengyong
Zhou, and Li Deng. 2017. Computer science ¿ com-
putation and language towards neural phrase-based
machine translation. arxiv.org/abs/1706.05565.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Lingpeng Kong. 2017. Neural Representation Learn-
ing in Linguistic Structured Prediction. Ph.D. thesis,
Google Research.

Haizhou Li and Baosheng Yuan. 1998. Chinese word
segmentation. In Proceedings of the 12th Pacific
Asia Conference on Language, Information and
Computation, pages 212–217.

Pierre Magistry and Benoı̂t Sagot. 2012. Unsupervized
word segmentation: the case for mandarin chinese.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Short
Papers-Volume 2, pages 383–387. Association for
Computational Linguistics.

Pierre Magistry and Benoı̂t Sagot. 2013. Can mdl
improve unsupervised chinese word segmentation?
In Sixth International Joint Conference on Natural
Language Processing: Sighan workshop, page 2.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested pitman-yor language modeling.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1-Volume 1, pages 100–108.
Association for Computational Linguistics.

Wenzhe Pei, Tao Ge, and Baobao Chang. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 293–303, Bal-
timore, Maryland. Association for Computational
Linguistics.

Weiwei Sun. 2010. Word-based and character-based
word segmentation models: Comparison and combi-
nation. In Coling 2010: Posters, pages 1211–1219,
Beijing, China. Coling 2010 Organizing Committee.

Yee W Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2005. Sharing clusters among re-
lated groups: Hierarchical dirichlet processes. In
Advances in neural information processing systems,
pages 1385–1392.

Chong Wang, Yining Wang, Po-Sen Huang, Abdel-
rahman Mohamed, Dengyong Zhou, and Li Deng.
2017. Sequence modeling via segmentations. arXiv
preprint arXiv:1702.07463.

Chunqi Wang and Bo Xu. 2017. Convolutional Neural
Network with Word Embeddings for Chinese Word
Segmentation. In Proceedings of the 8th Interna-
tional Joint Conference on Natural Language Pro-
cessing.

Hanshi Wang, Jian Zhu, Shiping Tang, and Xiaozhong
Fan. 2011. A new unsupervised approach to
word segmentation. Computational Linguistics,
37(3):421–454.

Lei Yu, Jan Buys, and Phil Blunsom. 2016. Online seg-
ment to segment neural transduction. arXiv preprint
arXiv:1609.08194.

Hai Zhao and Chunyu Kit. 2008. An empirical com-
parison of goodness measures for unsupervised chi-
nese word segmentation with a unified framework.
In Proceedings of the Third International Joint Con-
ference on Natural Language Processing: Volume-I.


