



















































Acquiring Predicate Paraphrases from News Tweets


Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 155–160,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics

Acquiring Predicate Paraphrases from News Tweets

Vered Shwartz Gabriel Stanovsky
Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel

{vered1986, gabriel.satanovsky}@gmail.com, dagan@cs.biu.ac.il

Ido Dagan

Abstract

We present a simple method for ever-
growing extraction of predicate para-
phrases from news headlines in Twitter.
Analysis of the output of ten weeks of col-
lection shows that the accuracy of para-
phrases with different support levels is es-
timated between 60-86%. We also demon-
strate that our resource is to a large extent
complementary to existing resources, pro-
viding many novel paraphrases. Our re-
source is publicly available, continuously
expanding based on daily news.

1 Introduction

Recognizing that various textual descriptions
across multiple texts refer to the same event or
action can benefit NLP applications such as rec-
ognizing textual entailment (Dagan et al., 2013)
and question answering. For example, to answer
“when did the US Supreme Court approve same-
sex marriage?” given the text “In June 2015, the
Supreme Court ruled for same-sex marriage”, ap-
prove and ruled for should be identified as describ-
ing the same action.

To that end, much effort has been devoted to
identifying predicate paraphrases, some of which
resulted in releasing resources of predicate entail-
ment or paraphrases. Two main approaches were
proposed for that matter; the first leverages the
similarity in argument distribution across a large
corpus between two predicates (e.g. [a]0 buy [a]1
/ [a]0 acquire [a]1) (Lin and Pantel, 2001; Berant
et al., 2010). The second approach exploits bilin-
gual parallel corpora, extracting as paraphrases
pairs of texts that were translated identically to for-
eign languages (Ganitkevitch et al., 2013).

While these methods have produced exhaustive
resources which are broadly used by applications,

[a]0 introduce [a]1 [a]0 welcome [a]1
[a]0 appoint [a]1 [a]0 to become [a]1
[a]0 die at [a]1 [a]0 pass away at [a]1

[a]0 hit [a]1 [a]0 sink to [a]1
[a]0 be investigate [a]1 [a]0 be probe [a]1

[a]0 eliminate [a]1 [a]0 slash [a]1
[a]0 announce [a]1 [a]0 unveil [a]1
[a]0 quit after [a]1 [a]0 resign after [a]1

[a]0 announce as [a]1 [a]0 to become [a]1
[a]0 threaten [a]1 [a]0 warn [a]1

[a]0 die at [a]1 [a]0 live until [a]1
[a]0 double down on [a]1 [a]0 stand by [a]1

[a]0 kill [a]1 [a]0 shoot [a]1
[a]0 approve [a]1 [a]0 pass [a]1

[a]0 would be cut under [a]1 [a]1 slash [a]0
seize [a]0 at [a]1 to grab [a]0 at [a]1

Table 1: A sample from the top-ranked predicate
paraphrases.

their accuracy is limited. Specifically, the first ap-
proach may extract antonyms, that also have sim-
ilar argument distribution (e.g. [a]0 raise to [a]1
/ [a]0 fall to [a]1) while the second may conflate
multiple senses of the foreign phrase.

A third approach was proposed to harvest para-
phrases from multiple mentions of the same event
in news articles.1 This approach assumes that
various redundant reports make different lexical
choices to describe the same event. Although
there has been some work following this approach
(e.g. Shinyama et al., 2002; Shinyama and Sekine,
2006; Roth and Frank, 2012; Zhang and Weld,
2013), it was less exhaustively investigated and
did not result in creating paraphrase resources.

In this paper we present a novel unsupervised
method for ever-growing extraction of lexically-
divergent predicate paraphrase pairs from news
tweets. We apply our methodology to create a
resource of predicate paraphrases, exemplified in
Table 1.

Analysis of the resource obtained after ten

1This corresponds to instances of event coreference
(Bagga and Baldwin, 1999).

155



weeks of acquisition shows that the set of para-
phrases reaches accuracy of 60-86% at differ-
ent levels of support. Comparison to existing
resources shows that, even as our resource is
still smaller in orders of magnitude from exist-
ing resources, it complements them with non-
consecutive predicates (e.g. take [a]0 from [a]1)
and paraphrases which are highly context specific.

The resource and the source code are avail-
able at http://github.com/vered1986/
Chirps.2 As of the end of May 2017, it con-
tains 456,221 predicate pairs in 1,239,463 differ-
ent contexts. Our resource is ever-growing and
is expected to contain around 2 million predicate
paraphrases within a year. Until it reaches a large
enough size, we will release a daily update, and at
a later stage, we plan to release a periodic update.

2 Background

2.1 Existing Paraphrase Resources
A prominent approach to acquire predicate para-
phrases is to compare the distribution of their argu-
ments across a corpus, as an extension to the dis-
tributional hypothesis (Harris, 1954). DIRT (Lin
and Pantel, 2001) is a resource of 10 million para-
phrases, in which the similarity between predicate
pairs is estimated by the geometric mean of the
similarities of their argument slots. Berant (2012)
constructed an entailment graph of distributionally
similar predicates by enforcing transitivity con-
straints and applying global optimization, releas-
ing 52 million directional entailment rules (e.g.
[a]0 shoot [a]1 → [a]0 kill [a]1).

A second notable source for extracting para-
phrases is multiple translations of the same text
(Barzilay and McKeown, 2001). The Para-
phrase Database (PPDB) (Ganitkevitch et al.,
2013; Pavlick et al., 2015) is a huge collection of
paraphrases extracted from bilingual parallel cor-
pora. Paraphrases are scored heuristically, and the
database is available for download in six increas-
ingly large sizes according to scores (the smallest
size being the most accurate). In addition to lex-
ical paraphrases, PPDB also consists of 140 mil-
lion syntactic paraphrases, some of which include
predicates with non-terminals as arguments.

2.2 Using Multiple Event Descriptions
Another line of work extracts paraphrases from re-
dundant comparable news articles (e.g. Shinyama

2Chirp is a paraphrase of tweet.

et al., 2002; Barzilay and Lee, 2003). The as-
sumption is that multiple news articles describing
the same event use various lexical choices, pro-
viding a good source for paraphrases. Heuristics
are applied to recognize that two news articles dis-
cuss the same event, such as lexical overlap and
same publish date (Shinyama and Sekine, 2006).
Given such a pair of articles, it is likely that predi-
cates connecting the same arguments will be para-
phrases, as in the following example:

1. GOP lawmakers introduce new health care plan
2. GOP lawmakers unveil new health care plan

Zhang and Weld (2013) and Zhang et al. (2015)
introduced methods that leverage parallel news
streams to cluster predicates by meaning, using
temporal constraints. Since this approach acquires
paraphrases from descriptions of the same event, it
is potentially more accurate than methods that ac-
quire paraphrases from the entire corpus or trans-
lation phrase table. However, there is currently no
paraphrase resource acquired in this approach.3

Finally, Xu et al. (2014) developed a supervised
model to collect sentential paraphrases from Twit-
ter. They used Twitter’s trending topic service, and
considered two tweets from the same topic as para-
phrases if they shared a single anchor word.

3 Resource Construction

We present a methodology to automatically collect
binary verbal predicate paraphrases from Twitter.
We first obtain news related tweets (§3.1) from
which we extract propositions (§3.2). For a can-
didate pair of propositions, we assume that if both
arguments can be matched then the predicates are
likely paraphrases (§3.3). Finally, we rank the
predicate pairs according to the number of in-
stances in which they were aligned (§3.4).

3.1 Obtaining News Headlines

We use Twitter as a source of readily avail-
able news headlines. The 140 characters limit
makes tweets concise, informative and indepen-
dent of each other, obviating the need to resolve
document-level entity coreference. We query the
Twitter Search API4 via Twitter Search.5 We use

3Zhang and Weld (2013) released a small collection of
10k predicate paraphrase clusters (with average cluster size
of 2.4) produced by the system.

4
https://apps.twitter.com/

5
https://github.com/ckoepp/TwitterSearch

156



Turkey intercepts the plane which took off from Moscow

subj
obj subj

prep from

Russia , furious about the plane , threatens to retaliate

prop of

prep about

subj

comp

(1) [Turkey]0 intercepts [plane]1 (2) [plane]0 took off from [Moscow]1 [Russia]0 threatens to [retaliate]1

Figure 1: PropS structures and the corresponding propositions extracted by our process. Left: multi-word
predicates and multiple extractions per tweet. Right: argument reduction.

Manafort hid payments from Ukraine party with Moscow ties [a]0 hide [a]1 Paul Manafort payments
Manafort laundered the payments through Belize [a]0 launder [a]1 Manafort payments
Send immigration judges to cities to speed up deportations to send [a]0 to [a]1 immigration judges cities
Immigration judges headed to 12 cities to speed up deportations [a]0 headed to [a]1 immigration judges 12 cities

Table 2: Examples of predicate paraphrase instances in our resource: each instance contains two tweets,
predicate types extracted from them, and the instantiations of arguments.

Twitter’s news filter that retrieves tweets contain-
ing links to news websites, and limit the search to
English tweets.

3.2 Proposition Extraction

We extract propositions from news tweets using
PropS (Stanovsky et al., 2016), which simplifies
dependency trees by conveniently marking a wide
range of predicates (e.g, verbal, adjectival, non-
lexical) and positioning them as direct heads of
their corresponding arguments. Specifically, we
run PropS over dependency trees predicted by
spaCy6 and extract predicate types (as in Table 1)
composed of verbal predicates, datives, preposi-
tions, and auxiliaries.

Finally, we employ a pre-trained argument re-
duction model to remove non-restrictive argument
modifications (Stanovsky and Dagan, 2016). This
is essential for our subsequent alignment step, as
it is likely that short and concise phrases will tend
to match more frequently in comparison to longer,
more specific arguments. Figure 1 exemplifies
some of the phenomena handled by this process,
along with the automatically predicted output.

3.3 Generating Paraphrase Instances

Following the assumption that different descrip-
tions of the same event are bound to be redun-
dant (as discussed in Section 2.2), we consider
two predicates as paraphrases if: (1) They appear
on the same day, and (2) Each of their arguments
aligns with a unique argument in the other predi-
cate, either by strict matching (short edit distance,
abbreviations, etc.) or by a looser matching (par-

6
https://spacy.io

tial token matching or WordNet synonyms).7 Ta-
ble 2 shows examples of predicate paraphrase in-
stances in the resource.

3.4 Resource Release
The resource release consists of two files:

1. Instances: the specific contexts in which the
predicates are paraphrases (as in Table 2). In
practice, to comply with Twitter policy, we
release predicate paraphrase pair types along
with their arguments and tweet IDs, and pro-
vide a script for downloading the full texts.

2. Types: predicate paraphrase pair types (as in
Table 1). The types are ranked in a descend-
ing order according to a heuristic accuracy
score:

s = count ·
(

1 +
d

N

)
where count is the number of instances in
which the predicate types were aligned (Sec-
tion 3.3), d is the number of different days in
which they were aligned, and N is the num-
ber of days since the resource collection be-
gun.

Taking into account the number of different
days in which predicates were aligned re-
duces the noise caused by two entities that
undergo two different actions on the same
day. For example, the following tweets from
the day of Chuck Berry’s death:

1. Last year when Chuck Berry turned 90
2. Chuck Berry dies at 90

7In practice, our publicly available code requires that at
least one pair of arguments will strictly match.

157



10 20 50 ∞

74

60

78

86

40.37

15.38

6.15
1.78

Accuracy Types

(a) Estimated accuracy (%) and number of types (×1K) of
predicate pairs with at least 5 instances in different score bins.

1 2 3 4 5 6 7 8 9 10

0

10

20

30

40

50

60

70

80

90

100

Accuracy Instances Types

(b) Estimated accuracy (%), number of instances (×10K)
and types (×10K) in the first 10 weeks.

Figure 2: Resource statistics after ten weeks of collection.

yield the incorrect type [a]0 turn [a]1 / [a]0
die at [a]1. While there may be several oc-
currences of that type on the same day, it is
not expected to re-occur in other news events
(in different days), yielding a low accuracy
score.

4 Analysis of Resource Quality

We estimate the quality of the resource obtained
after ten weeks of collection by annotating a sam-
ple of the extracted paraphrases.

The annotation task was carried out in Ama-
zon Mechanical Turk.8 To ensure the quality of
workers, we applied a qualification test and re-
quired a 99% approval rate for at least 1,000 prior
tasks. We assigned each annotation to 3 workers
and used the majority vote to determine the cor-
rectness of paraphrases.

We followed a similar approach to instance-
based evaluation (Szpektor et al., 2007), and let
workers judge the correctness of a predicate pair
(e.g. [a]0 purchase [a]1/[a]0 acquire [a]1) through
5 different instances (e.g. Intel purchased Mobil-
eye/Intel acquired Mobileye). We considered the
type as correct if at least one of its instance-pairs
were judged as correct. The idea that lies behind
this type of evaluation is that predicate pairs are
difficult to judge out-of-context.

Differently from Szpektor et al. (2007), we used
the instances in which the paraphrases appeared
originally, as those are available in the resource.

8
https://www.mturk.com/mturk

4.1 Quality of Extractions and Ranking

To evaluate the resource accuracy, and follow-
ing the instance-based evaluation scheme, we only
considered paraphrases that occurred in at least 5
instances (which currently constitute 10% of the
paraphrase types). We partition the types into four
increasingly large bins according to their scores
(the smallest bin being the most accurate), simi-
larly to PPDB (Ganitkevitch et al., 2013), and an-
notate a sample of 50 types from each bin. Fig-
ure 2(a) shows that the frequent types achieve up
to 86% accuracy.

The accuracy expectedly increases with the
score, except for the lowest-score bin ((0, 10])
which is more accurate than the next one
((10, 20]). At the current stage of the resource
there is a long tail of paraphrases that appeared
few times. While many of them are incorrect,
there are also true paraphrases that are infrequent
and therefore have a low accuracy score. We ex-
pect that some of these paraphrases will occur
again in the future and their accuracy score will
be strengthened.

4.2 Size and Accuracy Over Time

To estimate future usefulness, Figure 2(b) plots the
resource size (in terms of types and instances) and
estimated accuracy through each week in the first
10 weeks of collection.

The accuracy at a specific time was estimated
by annotating a sample of 50 predicate pair types
with accuracy score ≥ 20 in the resource obtained

158



drag [a]0 from [a]1 [a]0 remove from [a]1
leak [a]0 to [a]1 to share [a]0 with [a]1

oust [a]0 from [a]1 [a]0 be force out at [a]1
reveal [a]0 to [a]1 share [a]0 with [a]1

[a]0 add [a]1 [a]0 beef up [a]1
[a]0 admit to [a]1 [a]0 will attend [a]1

[a]0 announce as [a]1 [a]0 to become [a]1
[a]0 arrest in [a]1 [a]0 charge in [a]1
[a]0 attack [a]1 [a]0 clash with [a]1

[a]0 be force out at [a]1 [a]0 have be fire from [a]1
[a]0 eliminate [a]1 [a]0 slash [a]1

[a]0 face [a]1 [a]0 hit with [a]1
[a]0 mock [a]1 [a]0 troll [a]1

[a]0 open up about [a]1 [a]0 reveal [a]1
[a]0 get [a]1 [a]0 sentence to [a]1

Table 3: A sample of types from our resource that
are not found in Berant or in PPDB.

at that time, which roughly correspond to the top
ranked 1.5% types.

Figure 2(b) demonstrates that these types main-
tain a level of around 80% in accuracy. The re-
source growth rate (i.e. the number of new types)
is expected to change with time. We predict that
the resource will contain around 2 million types in
one year.9

5 Comparison to Existing Resources

The resources which are most similar to ours are
Berant (Berant, 2012), a resource of predicate
entailments, and PPDB (Pavlick et al., 2015), a re-
source of paraphrases, both described in Section 2.

We expect our resource to be more accurate than
resources which are based on the distributional ap-
proach (Berant, 2012; Lin and Pantel, 2001). In
addition, in comparison to PPDB, we specialize on
binary verbal predicates, and apply an additional
phase of proposition extraction, handling various
phenomena such as non-consecutive particles and
minimality of arguments.

Berant (2012) evaluated their resource against
a dataset of predicate entailments (Zeichner et al.,
2012), using a recall-precision curve to show the
performance obtained with a range of thresholds
on the resource score. This kind of evaluation is
less suitable for our resource; first, predicate en-
tailment is directional, causing paraphrases with
the wrong entailment direction to be labeled neg-
ative in the dataset. Second, since our resource is
still relatively small, it is unlikely to have sufficient
coverage of the dataset at that point. We therefore

9For up-to-date resource statistics, see: https://github.
com/vered1986/Chirps/tree/master/resource.

leave this evaluation to future work.
To demonstrate the added value of our resource,

we show that even in its current size, it already
contains accurate predicate pairs which are absent
from the existing resources. Rather than compar-
ing against labeled data, we use types with score
≥ 50 from our resource (1,778 pairs), which were
assessed as accurate (Section 4.2).

We checked whether these predicate pairs are
covered by Berant and PPDB. To eliminate di-
rectionality, we looked for types in both directions,
i.e. for a predicate pair (p1, p2) we searched for
both (p1, p2) and (p2, p1). Overall, we found that
67% of these types do not exist in Berant, 62%
in PPDB, and 49% in neither.

Table 3 exemplifies some of the predicate pairs
that do not exist in both resources. Specifically,
our resource contains many non-consecutive pred-
icates (e.g. reveal [a]0 to [a]1 / share [a]0 with [a]1)
that by definition do not exist in Berant.

Some pairs, such as [a]0 get [a]1 / [a]0 sentence
to [a]1, are context-specific, occurring when [a]0
is a person and [a]1 is the time they are about to
serve in prison. Given that get has a broad distri-
bution of argument instantiations, this paraphrase
and similar paraphrases are less likely to exist in
resources that rely on the distribution of arguments
in the entire corpus.

6 Conclusion

We presented a new unsupervised method to ac-
quire fairly accurate predicate paraphrases from
news tweets discussing the same event. We re-
lease a growing resource of predicate paraphrases.
Qualitative analysis shows that our resource adds
value over existing resources. In the future, when
the resource is comparable in size to the existing
resources, we plan to evaluate its intrinsic accu-
racy on annotated test sets, as well as its extrinsic
benefits in downstream NLP applications.

Acknowledgments

This work was partially supported by an Intel
ICRI-CI grant, the Israel Science Foundation grant
880/12, and the German Research Foundation
through the German-Israeli Project Cooperation
(DIP, grant DA 1600/1-1).

159



References
Amit Bagga and Breck Baldwin. 1999. Cross-

document event coreference: Annotations, experi-
ments, and observations. In Workshop on Corefer-
ence and its Applications.

Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceed-
ings of the 2003 Human Language Technol-
ogy Conference of the North American Chapter
of the Association for Computational Linguistics.
http://aclweb.org/anthology/N03-1003.

Regina Barzilay and R. Kathleen McKeown. 2001.
Extracting paraphrases from a parallel corpus.
In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics.
http://aclweb.org/anthology/P01-1008.

Jonathan Berant. 2012. Global Learning of Textual En-
tailment Graphs. Ph.D. thesis, Tel Aviv University.

Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2010. Global learning of focused entailment graphs.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics. Associ-
ation for Computational Linguistics, pages 1220–
1229. http://aclweb.org/anthology/P10-1124.

Ido Dagan, Dan Roth, and Mark Sammons. 2013. Rec-
ognizing textual entailment .

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of the 2013 Con-
ference of the North American Chapter of the
Association for Computational Linguistics: Hu-
man Language Technologies. Association for
Computational Linguistics, pages 758–764.
http://aclweb.org/anthology/N13-1092.

Zellig S Harris. 1954. Distributional structure. Word
10(2-3):146–162.

Dekang Lin and Patrick Pantel. 2001. Dirt – Discovery
of inference rules from text. In Proceedings of the
seventh ACM SIGKDD international conference on
Knowledge discovery and data mining. ACM, pages
323–328.

Ellie Pavlick, Pushpendre Rastogi, Juri Ganitke-
vitch, Benjamin Van Durme, and Chris Callison-
Burch. 2015. PPDB 2.0: Better paraphrase rank-
ing, fine-grained entailment relations, word em-
beddings, and style classification. In Proceed-
ings of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 2: Short Papers). Associa-
tion for Computational Linguistics, pages 425–430.
https://doi.org/10.3115/v1/P15-2070.

Michael Roth and Anette Frank. 2012. Aligning predi-
cate argument structures in monolingual comparable

texts: A new corpus for a new task. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics – Volume 1: Proceedings of the
main conference and the shared task, and Volume
2: Proceedings of the Sixth International Workshop
on Semantic Evaluation (SemEval 2012). Associa-
tion for Computational Linguistics, pages 218–227.
http://aclweb.org/anthology/S12-1030.

Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference. http://aclweb.org/anthology/N06-1039.

Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news
articles. In Proceedings of the second interna-
tional conference on Human Language Technology
Research. Morgan Kaufmann Publishers Inc., pages
313–318.

Gabriel Stanovsky and Ido Dagan. 2016. Annotating
and predicting non-restrictive noun phrase modifica-
tions. In Proceedings of the 54rd Annual Meeting of
the Association for Computational Linguistics (ACL
2016).

Gabriel Stanovsky, Jessica Ficler, Ido Dagan, and
Yoav Goldberg. 2016. Getting more out of
syntax with props. CoRR abs/1603.01648.
http://arxiv.org/abs/1603.01648.

Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics. As-
sociation for Computational Linguistics, pages 456–
463. http://aclweb.org/anthology/P07-1058.

Wei Xu, Alan Ritter, Chris Callison-Burch, William B
Dolan, and Yangfeng Ji. 2014. Extracting lexi-
cally divergent paraphrases from twitter. Transac-
tions of the Association for Computational Linguis-
tics 2:435–448.

Naomi Zeichner, Jonathan Berant, and Ido Da-
gan. 2012. Crowdsourcing inference-rule evalua-
tion. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Lin-
guistics (Volume 2: Short Papers). Association
for Computational Linguistics, pages 156–160.
http://aclweb.org/anthology/P12-2031.

Congle Zhang, Stephen Soderland, and Daniel S Weld.
2015. Exploiting parallel news streams for unsuper-
vised event extraction. Transactions of the Associa-
tion for Computational Linguistics 3:117–129.

Congle Zhang and Daniel S Weld. 2013. Harvest-
ing parallel news streams to generate paraphrases of
event relations. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing. Seattle, Washington, USA, pages 1776–
1786.

160


