



















































Towards Joint Morphological Analysis and Dependency Parsing of Turkish


Proceedings of the Second International Conference on Dependency Linguistics (DepLing 2013), pages 23–32,
Prague, August 27–30, 2013. c© 2013 Charles University in Prague, Matfyzpress, Prague, Czech Republic

Towards Joint Morphological Analysis and Dependency Parsing of
Turkish

Özlem Çetinoğlu and Jonas Kuhn
IMS, University of Stuttgart

Germany
{ozlem,jonas}@ims.uni-stuttgart.de

Abstract
Turkish is an agglutinative language with
rich morphology-syntax interactions. As
an extension of this property, the Turk-
ish Treebank is designed to represent sub-
lexical dependencies, which brings extra
challenges to parsing raw text. In this
work, we use a joint POS tagging and pars-
ing approach to parse Turkish raw text,
and we show it outperforms a pipeline ap-
proach. Then we experiment with incor-
porating morphological feature prediction
into the joint system. Our results show
statistically significant improvements with
the joint systems and achieve the state-of-
the-art accuracy for Turkish dependency
parsing.

1 Introduction

Turkish is a morphologically rich language (MRL)
that has been known to pose interesting research
questions to linguists and computational linguists,
including architectural issues at the morphology-
syntax interface. Today, good quality tools for
morphological analysis are available for analysing
Turkish raw text input at the word level, and
in work on the Turkish Dependency Treebank
(Oflazer et al., 2003), a representation scheme has
been developed that captures the peculiarities at
the morphology-syntax interface in a dependency
format that is formally compatible with the stan-
dard CoNLL dependency format.

So, it might seem as if all Turkish-specific chal-
lenges have been resolved, and only language-
independent data-driven methods are required
from now on (after all, the Turkish Dependency
Treebank was included in the CoNLL 2006 and
2007 Shared Tasks (Buchholz and Marsi, 2006;
Nivre et al., 2007), and several researchers work-
ing on language-independent methods have re-
ported scores on the available data).

However, Turkish still causes a considerable ar-
chitectural challenge for the standard pipeline ar-
chitecture used in data-driven dependency pars-
ing: the dependency treebank scheme for Turk-
ish is based on segments that are not identical to
the words from the raw text input, but are often
sublexical units that form parts of morphological
derivations.

While it is straightforward to train data-driven
parsers on the gold standard segmentation from
the treebank (which is what happened in the
shared tasks), any realistic application starting out
with raw text has to involve morphological dis-
ambiguation in the preprocessing which means it
is not guaranteed that treebank-compatible seg-
ment boundaries will be produced. For instance,
when training a dependency parser on predicted
POS and morphology features, the treebank is of
course used to provide the gold standard depen-
dency arcs, but with an automatic (and hence im-
perfect) morphological disambiguator, there will
be cases where the gold standard assumes two seg-
ments for a word, but morphological prediction
assumes only one. So any standard learning al-
gorithm will break down because the node sets for
the dependency graphs are incompatible.

For many languages, realistic parsing scenarios
assume gold tokens and use predicted POS (and
morphological features). For Turkish, keeping the
gold segmentation and assigning predicted POS
and morphology would converge to using an or-
acle because gold segmentation would sometimes
disambiguate morphology. Instead, realistic sce-
narios include segmentation, and a statistical mor-
phological disambiguator picks the most probable
analysis among all possibilities a morphological
analyser produces. It is the morphological anal-
ysis that determines the lemma, POS, morpholog-
ical features, and segmentation of a word is based
on the number of its word-internal derivations.

For instance, in (1), the middle word bende has

23



four morphological analyses with different lemma
and POS combinations, meaning ‘at me’, ‘on the
mole’, ‘to the dam’, and ‘servant’ respectively.1

Hence, unlike many other languages, the segmen-
tation, POS tagging, and morphological analysis
are tightly connected for Turkish.

Eryiğit et al. (2008) is the first work that ad-
dresses the segmentation problem in parsing pre-
dicted text. They set up a pipeline architecture of
a morphological analyser and disambiguator but
leave out handling the multiword expressions.2 A
recent work from Eryiğit (2012) solely focuses on
the impact of the morphological analysis and dis-
ambiguation of the Turkish treebank. Again, it
follows the standard pipeline but this time with a
treebank version that represents multiwords as de-
tached segments, which allows avoiding to use a
multiword extractor.

The major drawback of a pipeline system is
to propagate the disambiguator’s mistakes to the
parsing step. Moreover, the disambiguator cannot
take advantage of syntactic information that could
help disambiguate certain morphological analyses.

In (1), the first word kahveleri means ‘the cof-
fees (Acc)’, ‘his/her coffees’, ‘their (one) coffee’,
‘their coffees’ from (1a) to (1d). When the first
two words come together, they make a sentence
meaning ‘His/her/their coffees are at my place’.
kahveleri is still ambiguous but its dependency re-
lation is clear; bende, with morphological analysis
(1e), behaves as a copular predicate with no overt
marker and kahveleri is dependent on bende as a
subject.

When the third word içelim ‘let’s drink’ fol-
lows the former two, the meaning of the sentence
changes to ‘Let’s drink the coffees at my place’,
which also changes the morphological analysis of
kahveleri to (1a). It now behaves as the object of
the main predicate içelim. A pipeline system can-
not benefit from such a disambiguation advantage.

An alternative approach to pipeline architec-
tures is making joint decisions on morphological
disambiguation and parsing. It has been shown
that such an architecture improves constituency
parsing accuracy both for Arabic (Green and Man-

1A3pl: 3rd personal plural agreement, A3sg: 3rd per-
sonal singular agreement, Pnon: no possessives, P3sg: 3rd
personal singular possessive, P3pl: 3rd personal plural pos-
sessive, Nom: Nominative, Acc: Accusative, Loc: Locative,
Dat: Dative, Zero: No overt derivation, Pos: Positive, Opt:
Optative mood

2because of the lack of a multiword extractor. Hence the
experiments are not in a fully predicted setting.

ning, 2010) and for Hebrew (Goldberg and Tsar-
faty, 2008). On the dependency parsing front, Lee
et al. (2011) introduces a joint morphological dis-
ambiguation and dependency parsing architecture
which proves to outperform their pipeline archi-
tecture for Latin, Ancient Greek, Czech, and Hun-
garian. However it is limited to unlabelled de-
pendency parsing and initial scores are below the
state-of-the-art. On the other hand, parsers that
can jointly POS tag become more common in the
last years (Bohnet and Nivre, 2012; Hatori et al.,
2011; Li et al., 2011). Bohnet and Nivre (2012)
propose a joint POS tagger and labelled depen-
dency parser that outperforms the pipeline results
and also improves the state-of-the-art accuracy for
German, Czech, English, and Chinese.

Joint POS tagger and dependency parsers are
not originally designed for predicting morphologi-
cal features, but they provide a flexible field (POS)
where the parser is not dependent on the morpho-
logical disambiguator decisions. So the use of this
field can actually be extended to accommodating
morphological features instead of or in addition
to POS tags, which gives parsers an opportunity
to override fixed disambiguator mistakes. Hence,
those parsers approximate to a joint morphological
disambiguation and dependency parsing architec-
ture, which provides us with a testbed until gen-
uinely full-fledged joint parsers are developed.

In this paper we use Bohnet and Nivre’s (2012)
system to apply their approach to Turkish and later
to explore ways to include morphological feature
prediction into parsing. Experimental results show
that even a partial flexibility in predicting the mor-
phological features helps improve the parsing ac-
curacy statistically significantly.

The paper is structured as follows: Section 2
gives an overview on how morphological features
are used in parsing MRLs. Section 3 explains the
morphological analysis representation and its re-
lation with segmentation. Section 4 describes the
use of morphological features in joint parsing ex-
periments. The setup for experiments are given in
Section 5 and results are discussed in Section 6.
We conclude with Section 7.

2 Use of Morphological Features

Using morphological information as features in
parsing has been a commonly used method for
MRLs (Tsarfaty et al., 2010). The effect is con-
troversial: in some cases gold morphology clearly

24



(1)

Kahveleri bende içelim

a.kahve+Noun+A3pl+Pnon+Acc e.ben+PersP+A1sg+Pnon+Loc iç+Verb+Pos+Opt+A1pl
b.kahve+Noun+A3pl+P3sg+Nom f.ben+Noun+A3sg+Pnon+Loc
c.kahve+Noun+A3sg+P3pl+Nom g.bent+Noun+A3sg+Pnon+Dat
d.kahve+Noun+A3pl+P3pl+Nom h.bende+Noun+A3sg+Pnon+Nom

helps, in others its impact is little. For some
settings predicted information causes a drop, for
some settings a partial set of morphological fea-
tures improves parsing accuracy.

Ambati et al. (2010) explore ways of integrat-
ing local morphosyntactic features into Hindi de-
pendency parsing. They experiment with differ-
ent sets of features both on a graph-based and
a transition-based dependency parser. Both with
gold and predicted settings using morphological
features root, case, and suffix outperform using
POS as the only feature.

Bengoetxea and Gojenola (2010) utilise the
CoNLL-X format and MaltParser’s feature con-
figuration file to take advantage of morphological
features in parsing Basque with gold data. Their
experiments show that case and subordination type
increase parsing accuracy.

Marton et al. (2010) explore which morphologi-
cal features could be useful in dependency parsing
of Arabic. They observe the effect of features by
adding them one at a time separately and compar-
ing the outcomes. Experiments show that when
gold morphology is provided, case markers help
the most, whereas when the morphology is auto-
matically predicted the outcome is the opposite:
using case harms the results the most. When fea-
tures are combined in a greedy heuristic, using
definiteness, person, number, and gender informa-
tion improves accuracy.

To overcome the exhaustive feature space prob-
lem of Arabic, Dehdari et al. (2011) use heuristic
search algorithms for the optimal feature combi-
nation. Similar to Marton et al. (2010) they run
experiments by including one feature at a time to
their no-feature baseline, and also conduct a sec-
ond set of experiments where they remove one fea-
ture at a time from the whole feature set. They also
conclude that leaving out the predicted case im-
proved the parsing most among the possible can-
didates to remove, this time for constituency pars-
ing. In the single feature experiments, genitive cl-
itics help the most. The optimal combination they
achieve consists of the features determiner, proper
noun, genitive clitics, and negation.

Another Semitic language that is studied within
the MRLs is Hebrew. Initial results on Hebrew de-
pendency parsing (Goldberg and Elhadad, 2009)
show predicted morphological features help in a
transition-based parser with a tailored feature con-
figuration file, although scores drop in a graph-
based parser. The same authors later prove both
gold and predicted agreement features improve
accuracy for an easy-first, non-directional depen-
dency parser (Goldberg and Elhadad, 2010). Tsar-
faty and Sima’an (2010) report agreement features
are useful also for constituency parsing when they
extend the Relational Realisational (Tsarfaty and
Sima’an, 2008) models with this information.

Seeker and Kuhn (2011) focus on the internal
structures and grammatical functions of German
noun phrases. Their experiments show grammat-
ical functions are predicted with higher accuracy
when a graph-based dependency parser is pro-
vided with both gold and predicted case markers.

They further explore the effects of using case in
dependency parsing, this time for Czech and Hun-
garian as well as for German (Seeker and Kuhn,
2013). On a graph-based parser German does
not benefit much from using predicted morphol-
ogy but Czech and Hungarian clearly profit. They
also use case as a constraint on integer linear pro-
gramming (ILP) parsing models to filter out un-
grammatical case-function mappings. For all three
languages, the constrained models outperform the
unconstrained models and graph-based parser in
predicting core grammatical functions.

The research discussed in this section show case
and agreement are among the most investigated
features, and most of the time they are among the
most beneficial ones. These are the features we
also look into. But first, we describe the interac-
tion between the morphology and syntax in Turk-
ish in Section 3.

3 The Morphology-Syntax Interface in
Turkish

The motivation behind using sublexical units in
the Turkish treebank comes from its agglutina-
tive nature. Many linguistic phenomena that are

25



syntactic in other languages are represented with
derivational morphology in Turkish (Sulger et al.,
2013). For instance çekti is a one-word sentence in
Turkish meaning ‘It was a cheque’. The word çek
‘cheque’ is derived into a verb (with no overt suf-
fix) and then the past tense suffix -ti is attached. (2)
is the morphological representation of this word
where ˆDB denotes the derivational boundary:

(2) çek+Noun+A3sg+Pnon+NomˆDB+Verb+Zero
+Past+A3sg

Each sequence of inflectional features divided
by a derivational boundary is called an inflectional
group (IG hereafter). The word in (2) has two
IGs. A further example clarifies why inflectional
groups are chosen as the unit of the treebank. Fig-
ure 1 gives the dependency representation of the
sentence açık çekti ‘It was a blank cheque’. The
adjective açık ‘blank’ modifies the noun çek only,
not the derived verb çekti. A word based represen-
tation would disregard this distinction.

Açık çek - ti
açık çek +Verb
+Adj +Noun +Zero

+A3sg +Past
+Pnon +A3sg
+Nom

MODIFIER

Figure 1: The dependency representation for açık
çekti

The Turkish Treebank follows this IG nota-
tion. A word is segmented into segments from its
derivational boundaries. If it is derived n times, it
is represented as n+1 segments. The first segment
has the lemma, and the last segment has the whole
word as the surface form. The surface forms of
non-final segments are underscores. (3) gives the
treebank representation of the sentence in Figure
1 in the CoNLL format. The derived verb çekti is
represented as two segments.

The possible segmentation problem arises when
words have ambiguous morphological analyses
with different number of IGs. For instance, the
word çekti has a second interpretation with the
meaning ‘s/he pulled’ which is the past tense of
the verb çek ‘to pull’ in 3rd person singular. The
morphological representation of this sense is given
in (4).

(4) çek+Verb+Pos+Past+A3sg

Note that in this analysis, there are no deriva-
tional boundaries, hence it only has a single IG.
When the gold standard is the first interpretation
of the word çekti and a morphological prediction
suggests the second interpretation, the number of
segments do not match any more.

4 Morphological Feature Prediction

Like many other free-word-order languages, Turk-
ish has overt case markers. It is the case marker
that determines the function of a word in a sen-
tence rather than the POS of that word. For in-
stance, an accusative nominal is an object no mat-
ter if it is a noun, proper noun, or pronoun.

However, the case-function mapping is not
completely unambiguous. Nominative case is as-
sociated with subjects and indefinite direct ob-
jects. Subjects of sentential complements are gen-
itive. Dative, ablative, genitive, and instrumental
can be non-canonical objects (Çetinoğlu and Butt,
2008), although their primary function is adjunct.
In copular sentences, the nominal predicate, with
or without an overt copular suffix, can bear any
case marker except accusative.

Another morphological feature that parsing al-
gorithms can benefit from is agreement. In Turk-
ish, subjects and verbs must agree in number and
person. There is an exception to this rule: a third
person plural subject might agree with a verb in
third person singular as well as a verb in third per-
son plural.

To explore the question whether we can benefit
from case and agreement features in parsing Turk-
ish, we employ two different representation meth-
ods. First, we append case markers to nominal
POS tags3 to see if a more informative POS field
could facilitate parsing (Pos+Case). Then, with
the intuition that case markers alone could deter-
mine the function, we categorise nominals accord-
ing to CASE instead of their POS (Case).

In the implementation, in order to represent case
markers as categories we move them to the POS
field. POS tags are moved to the morphological
features field. For instance, In the CoNLL for-
mat4, çeki ‘cheque.Acc’ has normally the repre-
sentation in (5a). Appending CASE to POS results
in (5b). When CASE replaces POS, the representa-
tion is as in (5c).

3These are namely nouns, proper nouns, pronouns, nomi-
nal participals, and infinitives.

4The columns are Form, Lemma, POS, Morphological
Features respectively.

26



(3)
ID Form Lemma POS Morph. Feat. Head Dep. Rel.
1 açık açık Adj _ 2 MODIFIER
2 _ çek Noun A3sg|Pnon|Nom 3 DERIV
3 çekti _ Verb Zero|Past|A3sg 0 ROOT

(5) a. çeki çek Noun A3sg|Pnon|Acc
b. çeki çek Noun|Acc A3sg|Pnon

c. çeki çek Acc A3sg|Pnon|Noun

This representation has two benefits. We can
still use the POS tags as features for the parser, and
after parsing, it is possible to restore the POS tags
by switching them back. This allows us to evaluate
our system against the standard gold data.

When combined with a joint parsing system,
both approaches extend the use of the parser and
practically carry it to a level between POS tag-
ging and morphological analysis. We applied the
CASE-POS replacement technique to agreement
markers (Agr) hoping that the parser can learn and
predict the relation between subjects and verbs
better. We also collected the finite verbs under
the VFin umbrella instead of Verb to distin-
guish verbs with an agreement marker from non-
finite ones (VFin). We discuss the effects of those
changes in Section 6.2.

5 Experimental Setup

5.1 Data Set

We use the METU-Sabancı Turkish Treebank
(Oflazer et al., 2003) for training and ITU vali-
dation set (Eryiğit, 2007) for testing. The train-
ing and test sets consist of 5635 and 300 sen-
tences respectively. There are no separate devel-
opment sets. The original version of the treebank
contains multiword expressions5 where words that
construct the multiword are attached together with
an underscore. The POS and morphological fea-
tures of a MWE are that of the last word of the
MWE. Eryiğit et al. (2011) have created a de-
tached version of the original treebank. In the
detached version, multiword expressions are split-
ted into words, and POS and morphological fea-
tures are assigned to the new words. They are de-
pendent on the final word of the multiword with
the relation MWE. (6a) and (6b) give the origi-
nal and detached versions of söz vermiştim ‘I have
promised’, respectively. Note that if a MWE con-

5E.g., named entities, collocations, date-time expressions,
noun-verb compounds as in (6).

sists derived words they will also be represented
with multiple IGs. In our experiments we use the
detached version of the treebank.

5.2 Tools

In order to parse data with predicted segmenta-
tion, POS and morphological features, the raw
data is first passed through a morphological anal-
yser (Oflazer, 1994) and then through a morpho-
logical disambiguator (Sak et al., 2008). Heuris-
tic rules are used for some unknown types6 and
the rest of unknowns are considered to be nom-
inative proper nouns. We adopt Bohnet’s (2010)
state-of-the-art graph-based parser as our Pipeline
parser7 and Bohnet and Nivre’s (2012) transition-
based parser as our Joint parser that can jointly
handle POS tagging and dependency parsing.

5.3 Evaluation

The standard evaluation metrics labelled and unla-
belled attachement scores (LAS and UAS) (Buch-
holz and Marsi, 2006) are not applicable to com-
pare a predicted file to a gold file if the segment
sizes are different. We handle this problem by
using an evaluation tool based on IGs (Eryiğit
et al., 2008). The unlabelled attachement score
UASIG gives the ratio of IGs that are attached
to the correct head, and the labelled attachement
score LASIG gives the ratio of IGs attached to the
correct head with the correct label. In cases where
the morphology (segmentation, POS, and morpho-
logical features) of the head word is different from
the gold one, an attachement is correct only if the
dependent is attached to the correct word and the
head IG has the gold main POS. Note that when
gold segmentation and POS are used LASIG and
UASIG are identical to the standard LAS an UAS
respectively. We omit punctuation in evaluation.

6E.g. if a word ends with an apostrophe followed by the
surface form of a case marker, the string before the apostro-
phe is the root of a proper noun and the case is determined
from the surface form.

7We also ran baseline experiments with Bohnet’s
transition-based parser. The graph-based parser clearly out-
performs it in the gold setting. When the parsers are pro-
vided with predicted POS tags and morphological features,
the scores are comparable.

27



(6)

ID Form Lemma POS Morph. Feat. Head Dep. Rel.
a. 4 söz_vermiştim söz_ver Verb Pos|Narr|Past|A1sg 5 SENTENCE

b. 4 söz söz Noun A3sg|Pnon|Nom 5 MWE
5 vermiştim ver Verb Pos|Narr|Past|A1sg 6 SENTENCE

6 Experiments and Analyses

We conduct 10-fold cross validation experiments
on the training data and report the average scores
for pipeline and joint parsers. Gold settings use
gold segmentation, POS, and morphological fea-
tures, whereas in predicted settings, all this in-
formation is predicted (either by the morphologi-
cal analyser+disambiguator or by the joint parser).
For systems we observe improvements on 10-fold
cross validation experiments, we also give the test
set results.8

6.1 Pipeline Experiments

In the first set of experiments, we examine the ef-
fect of using morphological features in parsing.
Table 1 gives the average 10-fold cross validation
scores on the training data. As discussed in Sec-
tion 2, there are controversial results of using mor-
phological features in parsing MRLs: although
gold features help, predicted features might harm
the accuracy. For Turkish, Eryiğit et al. (2008)
have already shown that adding gold morphologi-
cal features to Malt parser trained on the original
treebank improves accuracy. Our findings are in
line with theirs.

The first row of Table 1 gives the graph-based
parser results when both the training and parsing
data have morphological information. The pre-
dicted LASIG is 4.5% lower than the gold one.
When the graph-based parser is trained on gold
data with morphological features, but the features
are not provided during parsing, there are 12.4%
and 10.7% LASIG drops in the gold and predicted
settings respectively. A drop in such a scenario
is of course expected, but the impact of no mor-
phology in parsing is huge as compared to many
other MRLs (e.g., Seeker and Kuhn (2013) report
6.3%, 2.4%, and only 0.4% absolute drops in LAS
for Hungarian, Czech, and German respectively).
When the morphological information is not used
in training at all, the parser can cope with the lack
of morphological information better during pars-

8For replicability, experimental settings are available at
http://www.ims.uni-stuttgart.de/~ozlem/
cetinogluDepling13.html

Gold Predicted
System LASIG UASIG LASIG UASIG
GB +T,+P 66.29 77.51 61.79 73.89
GB +T,-P 53.88 71.49 51.02 69.71
GB -T,-P 60.62 75.36 56.31 71.42

Table 1: The effect of using morphological fea-
tures on the graph-based parser. Morphologi-
cal features are used in neither training nor pars-
ing (-T,-P), used in training but not provided in
parsing (+T,-P), used both in training and pars-
ing (+T,+P). Results given are the average 10-fold
cross-validation scores on the training data.

ing. Still, the gold and predicted LASIG scores
are absolute 5-6% lower than a setting that uses
morphology both in training and parsing.

6.2 Joint Parsing Experiments
Table 2 gives the training set 10-fold cross valida-
tion average scores for systems we experimented
in this paper, as well as for previous work. It is
observed that moving CASE to the POS field helps
with a 0.3% absolute increase in the gold pipeline
settings. Joint parsing results with gold features,
are 1-1.5% absolute lower than the pipeline scores.
This is expected; the gold setting for joint parsing
is not exactly gold, as by definition the parser pre-
dicts POS tags during parsing instead of gold ones
although the segmentation and morphological fea-
tures are gold. As a result, they cannot beat purely
gold settings.

If we have a closer look at the joint systems,
we witness that only JointCase outperforms Joint.
JointPos+Case increases the tagset to be learned
and predicted from 35 to 107 which is probably
too fine-grained for the parser. Agreement mark-
ers, which are not directly related to grammati-
cal functions like CASE, have a negative impact
in the gold settings when used instead of Verb.
Still, when agreement markers are used only to
introduce an extra category, namely VFin, the
scores come closer to the baseline of joint parsing
with gold information, and even improves over the
baseline LASIG in the predicted setting.

In the pipeline approach with predicted mor-
phology, using CASE instead of nominal POS im-

28



Gold Predicted
System LASIG UASIG LASIG UASIG
Pipeline 66.29 77.51 61.79 73.89
PipelineCase 66.60 77.60 62.07 74.00
Joint 64.61 75.83 62.21 73.86
JointCase 64.92 76.27 62.58 74.35
JointPos+Case 63.99 75.45 62.02 73.76
JointAgr 63.65 74.95 61.32 73.17
JointV Fin 64.44 75.68 62.34 72.59
Ery11-Ery12 65.90 76.00 58.3/61.1 70.70

Table 2: Training set 10-fold cross validation av-
erage scores. Gold scores Ery11 are taken from
Eryiğit et al. (2011) and predicted scores Ery12
are taken from Eryiğit (2012). Ery12 (Eryiğit,
2012) gives an interval LASIG corresponding 0%
and 100% accuracy for MWE relations

Gold Predicted
System LASIG UASIG LASIG UASIG
Pipeline 68.92 78.85 64.59 76.32
PipelineCase 68.86 78.98 65.00 76.35
Joint 66.14 76.86 63.77 75.06
JointCase 67.25 78.50 65.19 77.05
Ery11-Ery12 - - 64.2/66.2 75.53

Table 3: Testset scores. Ery11 (Eryiğit et al.,
2011) does not provide gold scores for testset.
Ery12 (Eryiğit, 2012) gives an interval LASIG
corresponding 0% and 100% accuracy for MWE
relations.

proves the labelled accuracy by 0.3% absolute for
the training set. Letting the parser predict POS
in the joint system adds 0.14 points more. The
best score is achieved with JointCase which has a
0.3% absolute increase as compared to Joint. The
difference between pipeline systems and joint sys-
tems are statistically significant both for LASIG
and UASIG, in the gold setting. When pre-
dicted data is used, PipelineCase, Joint, JointCase
LASIG scores are statistically significantly better
than Pipeline (p<0.05, paired t-test).

The testset scores are given in Table 3. They
follow the training set trend, except for the Joint
system to our surprise. This is perhaps due to the
different characteristics of test and training data.
When we look at the breakdown of dependencies
from 10-fold cross validation results in Section
6.3, we discuss a recall drop in some labels when
they are parsed with the Joint parser. We do not
look at the dependency distribution of the test data
but if it is different from the training data then a
possibly similar drop in the same labels might im-
pact the overall score more. In parsing the test

data with gold features, pipeline systems statisti-
cally significantly outperform joint systems. In the
predicted setting, only Joint vs. JointCase UASIG
difference is statistically significant.

Both in Tables 2 and 3, predicted LASIG scores
from Eryiğit (2012) are given as an interval. In her
experiments, the parser is trained on the original
treebank (that is, no MWE relations are present in
the training data) and tested on the detached ver-
sion. She reports lower and upper bounds corre-
sponding 0% and 100% accuracy for MWE re-
lations. To compare our results to those of Ery-
iğit’s, we also calculate the upper bounds with
100% MWE accuracy in our best performing sys-
tem. When we accept all MWE labels correct9

we achieve 64.49% LASIG on the average score
of 10-fold cross validation on the training set and
66.46% LASIG on the testset for the JointCase
system. For both the predicted and gold systems
our parsers outperform previous work.

For comparability with other existing results,
we also trained the Pipeline parser on the origi-
nal version of the treebank which is used in the
CoNLL 2007 Shared Task. Nivre et al. (2007) re-
port 71.6% LAS on the testset (excluding punc-
tuation) for the best system (Titov and Hender-
son, 2007). Eryiğit (2012) increases the LAS to
71.98% and the Pipeline parser outperform both
systems with 72.53% LAS .

6.3 Error Analysis

For a detailed error analysis we take into account
the Pipeline, PipelineCase, Joint, and JointCase
10-fold cross validation results on the training set.
In the predicted setting, scores from these four
parsers are in ascending order (Table 2, predicted
LASIG column, first four rows). When we look
at the dependency breakdown of pipeline and joint
systems, we observe subjects and objects follow
this trend, together with question particles, nega-
tive particles, and modifiers.

The dependencies that benefit from joint pars-
ing the most are determiners. This is due to the
fact that some frequently occuring determiners are
ambiguous. For instance, O has the determiner
(‘that’) and personal pronoun (‘he/she/it’) read-
ings, and similarly bu ‘this’ is both a determiner
and a demonstrative pronoun. Joint parsing lets
the parser assign the correct POS to those words
where the morphological disambiguator fails. Let-

9through a parameter in the evaluation script

29



Dependency Precision Recall
ABLATIVE.ADJUNCT 41.9 50.3
APPOSITION 48.3 15.0
CLASSIFIER 59.1 68.1
COORDINATION 53.0 48.4
DATIVE.ADJUNCT 40.5 45.8
DETERMINER 73.5 81.3
INSTRUMENTAL.ADJUNCT 24.6 21.0
INTENSIFIER 70.7 70.7
LOCATIVE.ADJUNCT 40.4 46.0
MODIFIER 60.3 58.3
MWE 63.5 58.1
NEGATIVE.PARTICLE 67.0 45.6
OBJECT 59.9 58.2
POSSESSOR 70.9 74.5
QUESTION.PARTICLE 71.5 62.8
SENTENCE 86.6 88.0
S.MODIFIER 49.4 46.1
SUBJECT 48.9 51.0
VOCATIVE 29.6 19.5

Table 4: The dependency breakdown of the 10-
fold cross validation scores for JointCase with pre-
dicted morphological information. Precision and
recall are given in percent. Dependencies with less
than 100 occurrences are omitted.

ting the parser predict CASE instead of POS causes
some drop, but both precision and recall are still
higher than both pipeline systems.

Another dependency with Joint as the most ac-
curate system is coordination. CASE helps in
PipelineCase as compared to Pipeline, but causes
an accuracy decrease when going from Joint to
JointCase. The COORDINATION label attaches
conjunctions to their conjunct to the right. The
most frequent conjunctions comma and ve ‘and’
can be predicted with very high accuracy. When
the Joint parser is used, there are slight improve-
ments on attachements to head conjuncts with var-
ious POS tags and a systematic improvement on
attaching conjunctions to head copulars and con-
ditionals.

The precision of possessors does not change
much with different systems, but the recall drops
in Joint. That drop is recovered when JointCase is
applied. Intensifiers (e.g., particles de ‘also, too’,
bile ‘even’) also have a similar trend. Precision,
on the other hand increases with Joint.

A large subset of dependencies that suffers from
the same drop is adjuncts. Dative, ablative, loca-
tive, and instrumental adjuncts commonly have
drops in the Joint recall as compared to pipeline
systems. Their precision, however, increases.
When we look into the parser output, we see that
the Joint system has systematically mistaken by

assigning Adj to the Verb root of participles.
Then all arguments attached to this incorrectly
POS-tagged root are penalised by the evaluation
script although most of the time attachements are
correct.

The incorrect POS assignment problem disap-
pears when the joint parser is trained on the CASE
feature of nominals instead of their POS. This ex-
plains why the precision of adjuncts improves a
bit more and their recall has a jump. The only
exception is the precision drop in instrumental
adjuncts. The reason could be nouns in instru-
mental case that behave as adverbs, such as hı-
zla (speed+Ins, ‘quickly’). The parser cannot
learn to distinguish an instrumental adjunct from
an adverbial modifier when +Ins is used as POS
in JointCase.

The advantage of JointCase over Pipeline is ex-
emplified with a comparison in Figure 2. The
Pipeline and JointCase parse trees, together with
POS tags and case markers are given in (a) and
(b) respectively. The Pipeline parser relies on the
morphological disambiguator output which incor-
rectly assigns the analyses (1b) to kahveleri and
(1h) to bende. As a result, the parser assigns the
incorrect labels to both dependencies.

On the other hand, the JointCase parser replaces
the case Nom with its prediction Acc in kahveleri
and Nom replaces Loc in bende. These correc-
tions result in predicting dependencies identical to
gold ones. Note that the lemma of bende is still
incorrect, but it does not affect the attachements.

Kahveleri bende içelim
kahve bende iç
+Noun +Noun +Opt
+Nom +Nom +A1pl

SUBJECT OBJECT

Kahveleri bende içelim
kahve bende iç
+Noun +Noun +Opt
+Acc +Loc +A1pl

OBJECT

LOC.ADJ

Figure 2: The (a) Pipeline and (b) JointCase parse
trees for the example sentence (1) Kahveleri bende
içelim ‘Let’s drink coffee at my place.’

The dependency breakdown of the 10-fold cross
validation parses for JointCase with predicted mor-
phological information is given in Table 4. In the

30



Turkish treebank representation, the root of a tree
is the sentence-final punctuation. The main pred-
icate of the sentence is attached to the sentence-
final punctuation with the SENTENCE label. By
far, this label is the easiest to predict with our sys-
tems. It is followed by determiners, intersifiers,
possessors, and question particles, which are all
local dependencies. Then come classifiers, coor-
dination, modifiers, multiword expressions within
a range of 50-65% precision and recall.

Despite getting improvements with the
JointCase system, grammatical functions are still
quite low in accuracy. Except for objects, all
such labels are below 50% precision and recall.
This is due to both the free-word-order nature
of Turkish and the ambiguous case-function
mapping mentioned in Section 4.

And finally, appositions, vocatives, and instu-
mental adjuncts are at the bottom of the accuracy
ranking with scores going down to 20-30%. Their
frequencies are also low and they have different
POS and morphological features within the same
class, which complicates parsers’ learning.

7 Conclusion and Future Work

We have presented a set of experiments on pars-
ing raw Turkish text. We argue the ideal method
for parsing Turkish would be joint segmentation,
POS tagging, morphological analysis, and depen-
dency parsing. In this work we keep the segmen-
tation fixed and first show using a joint POS tag-
ging and parsing approach outperforms a pipeline
approach in a realistic scenario. Then we come
one step closer to the ideal case and attemp to in-
corporate some morphological features into joint
prediction. As a second outcome, we show cat-
egorising nominals according to CASE instead of
their POS improves parsing at all settings ( gold
vs. predicted, pipeline vs. joint). With the com-
bination of joint parsing and CASE incorporation
we not only show statistically significant improve-
ments but also achieve the state-of-the-art parsing
accuracy.

We believe these positive results prove there is
room for improvement in predicting morphologi-
cal features with a joint POS tagging and depen-
dency parsing system. Even for the joint pars-
ing experiments below the Joint baseline, more
clever ways of integration into joint prediction
might help achieve higher scores. Past research
on MRLs present such cases. Bengoetxea and Go-

jenola (2010) show a simple integration of mor-
phological features does not improve Basque pars-
ing results on the first attemp, but taking advan-
tage of the data representation and parser configu-
ration changes the impact. Similarly, Tsarfaty and
Sima’an (2010) has negative results initially for
the impact of using agreement markers on Hebrew
parsing. After they modify the way they use the
morphological information, it actually helps.

In future work, we intend to explore ways to
make more use of the joint parser and to apply the
same or similar techniques to other MRLs such as
German, Czech, and Hungarian.

We also want to add TedEval (Tsarfaty et al.,
2012), which also supports mismatching system-
gold segmentation, to our evaluation tools to ver-
ify our scores and to use a language-independent
metric in a multilingual setting.

Acknowledgments

We thank Bernd Bohnet for his help on using the
joint parser and Gülşen Eryiğit for providing us
with the IG evaluation script. This work is funded
by the Collaborative Research Centre (SFB 732)
at the University of Stuttgart.

References
Bharat Ram Ambati, Samar Husain, Sambhav Jain,

Dipti Misra Sharma, and Rajeev Sangal. 2010. Two
methods to incorporate ‘local morphosyntactic’ fea-
tures in Hindi dependency parsing. In Proc. of
the SPMRL Workshop of NAACL-HLT, pages 22–30,
Los Angeles, CA, USA.

Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency pars-
ing of Basque. In Proc. of the SPMRL Workshop of
NAACL-HLT, pages 31–39, Los Angeles, CA, USA.

Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proc.
of the EMNLP-CoNLL, pages 1455–1465, Jeju, Ko-
rea.

Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proc. of
COLING, pages 89–97, Beijing, China.

Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of CoNLL-X, pages 149–164, Stroudsburg, PA,
USA.

Özlem Çetinoğlu and Miriam Butt. 2008. Turkish non-
canonical objects. In Proc. of LFG08 Conference,
Sydney, Australia. CSLI Publications.

31



Jon Dehdari, Lamia Tounsi, and Josef van Gen-
abith. 2011. Morphological features for parsing
morphologically-rich languages: A case of Arabic.
In Proc. of the SPMRL Workshop of IWPT, pages
12–21, Dublin, Ireland.

Gülşen Eryiğit. 2007. ITU validation set for METU-
Sabancı Turkish treebank.

Gülşen Eryiğit. 2012. The impact of automatic
morphological analysis & disambiguation on depen-
dency parsing of Turkish. In Proc. of LREC, Istan-
bul, Turkey.

Gülşen Eryiğit, Joakim Nivre, and Kemal Oflazer.
2008. Dependency parsing of Turkish. Computa-
tional Linguistics, 34(3):357–389.

Gülşen Eryiğit, Tugay Ilbay, and Ozan Arkan Can.
2011. Multiword expressions in statistical depen-
dency parsing. In Proc. of the SPMRL Workshop of
IWPT, pages 45–55, Dublin, Ireland.

Yoav Goldberg and Michael Elhadad. 2009. He-
brew dependency parsing: Initial results. In Proc.
of IWPT, pages 129–133, Paris, France.

Yoav Goldberg and Michael Elhadad. 2010. Easy-first
dependency parsing of modern Hebrew. In Proc. of
the SPMRL Workshop of NAACL-HLT, pages 103–
107, Los Angeles, CA, USA.

Yoav Goldberg and Reut Tsarfaty. 2008. A single gen-
erative model for joint morphological segmentation
and syntactic parsing. In Proc. of ACL-HLT, pages
371–379, Columbus, Ohio.

Spence Green and Christopher D. Manning. 2010.
Better Arabic parsing: baselines, evaluations, and
analysis. In Proc. of COLING, pages 394–402,
Stroudsburg, PA, USA.

Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun’ichi Tsujii. 2011. Incremental joint POS tag-
ging and dependency parsing in Chinese. In Proc. of
IJCNLP, pages 1216–1224, Chiang Mai, Thailand.

John Lee, Jason Naradowsky, and David A. Smith.
2011. A discriminative model for joint morpho-
logical disambiguation and dependency parsing. In
Proc. of ACL-HLT, Portland, Oregon, USA.

Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu,
Wenliang Chen, and Haizhou Li. 2011. Joint mod-
els for Chinese POS tagging and dependency pars-
ing. In Proc. of EMNLP, pages 1180–1191, Edin-
burgh, Scotland, UK.

Yuval Marton, Nizar Habash, and Owen Rambow.
2010. Improving Arabic dependency parsing with
lexical and inflectional morphological features. In
Proc. of the SPMRL Workshop of NAACL-HLT,
pages 13–21, Los Angeles, CA, USA.

Joakim Nivre, Johan Hall, Sandra Kübler, Ryan Mac
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on depen-
dency parsing. In Proc. of the CoNLL Shared Task
Session of EMNLP-CoNLL.

Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-Tür,
and Gökhan Tür. 2003. Building a Turkish tree-
bank. In Anne Abeille, editor, Building and Exploit-
ing Syntactically-annotated Corpora. Kluwer Aca-
demic Publishers, Dordrecht.

Kemal Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Comput-
ing, 9(2):137–148.

Haşim Sak, Tunga Güngör, and Murat Saraçlar. 2008.
Turkish language resources: Morphological parser,
morphological disambiguator and web corpus. In
Proc. of GoTAL 2008, pages 417–427.

Wolfgang Seeker and Jonas Kuhn. 2011. On the role
of explicit morphological feature representation in
syntactic dependency parsing for German. In Proc.
of IWPT, pages 58–62, Dublin, Ireland.

Wolfgang Seeker and Jonas Kuhn. 2013. Morphologi-
cal and syntactic case in statistical dependency pars-
ing. Computational Linguistics, 39:23–55.

Sebastian Sulger, Miriam Butt, Tracy Holloway King,
Paul Meurer, Tibor Laczkó, György Rákosi,
Cheikh Bamba Dione, Helge Dyvik, Victoria Rosén,
Koenraad De Smedt, Agnieszka Patejuk, Özlem
Çetinoğlu, I Wayan Arka, and Meladel Mistica.
2013. Pargrambank: The pargram parallel treebank.
In Proc. of ACL, Sofia, Bulgaria.

Ivan Titov and James Henderson. 2007. Fast and ro-
bust multilingual dependency parsing with a gener-
ative latent variable model. In Proc. of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
947–951.

Reut Tsarfaty and Khalil Sima’an. 2008. Relational-
realizational parsing. In Proc. of COLING, pages
889–896, Manchester, UK.

Reut Tsarfaty and Khalil Sima’an. 2010. Modeling
morphosyntactic agreement in constituency-based
parsing of modern Hebrew. In Proc. of the SPMRL
Workshop of NAACL-HLT, pages 40–48, Los Ange-
les, CA, USA.

Reut Tsarfaty, Djamé Seddah, Yoav Goldberg, Sandra
Kuebler, Yannick Versley, Marie Candito, Jennifer
Foster, Ines Rehbein, and Lamia Tounsi. 2010. Sta-
tistical parsing of morphologically rich languages
(SPMRL) what, how and whither. In Proc. of the
SPMRL Workshop of NAACL-HLT, pages 1–12, Los
Angeles, CA, USA.

Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Joint evaluation for morphological segmen-
tation and syntactic parsing. In Proc. of ACL, Jeju,
Korea.

32


