



















































Interpreting Neural Network Hate Speech Classifiers


Proceedings of the Second Workshop on Abusive Language Online (ALW2), pages 86–92
Brussels, Belgium, October 31, 2018. c©2018 Association for Computational Linguistics

86

Interpreting Neural Network Hate Speech Classifiers

Cindy Wang
Stanford University

Department of Computer Science
cindyw@cs.stanford.edu

Abstract

Deep neural networks have been applied to
hate speech detection with apparent success,
but they have limited practical applicability
without transparency into the predictions they
make. In this paper, we perform several ex-
periments to visualize and understand a state-
of-the-art neural network classifier for hate
speech (Zhang et al., 2018). We adapt tech-
niques from computer vision to visualize sen-
sitive regions of the input stimuli and identify
the features learned by individual neurons. We
also introduce a method to discover the key-
words that are most predictive of hate speech.
Our analyses explain the aspects of neural net-
works that work well and point out areas for
further improvement.

1 Introduction

We define hate speech as “language that is used to
express hatred towards a targeted group or is in-
tended to be derogatory, to humiliate, or to insult
the members of the group” (Davidson et al., 2017).
This definition importantly does not include all in-
stances of offensive language, reflecting the chal-
lenges of automated detection in practice. For
instance, in the following examples from Twit-
ter (1) clearly expresses homophobic sentiment,
while (2) uses the same term self-referentially:

(1) Being gay aint cool ... yall just be confused
and hurt ... fags dont make it to Heaven

(2) me showing up in heaven after everyone
told me god hates fags

As in many other natural language tasks, deep
neural networks have become increasingly popu-
lar and effective within the realm of hate speech
research. However, few attempts have been made
to explain the underlying features that contribute
to their performance, essentially rendering them
black-boxes. Given the significant social, moral,
and legal consequences of incorrect predictions,

interpretability is critical for deploying and im-
proving these models.

To address this, we contribute three ways of vi-
sualizing and understanding neural networks for
text classification and conduct a case study on a
state-of-the-art model for generalized hate speech.
We 1) perform occlusion tests to investigate re-
gions of model sensitivity in the inputs, 2) identify
maximal activations of network units to visualize
learned features, and 3) identify the unigrams most
strongly associated with hate speech. Our analy-
ses explore the bases of the neural network’s pre-
dictions and discuss common classes of errors that
remain to be addressed by future work.

2 Related Work

Hate speech classification. Early approaches em-
ployed relatively simple classifiers and relied on
manually extracted features (e.g. n-grams, part-
of-speech tags, lexicons) to represent data (Di-
nakar et al., 2011; Nobata et al., 2016). Schmidt
and Wiegand (2017)’s survey of hate speech de-
tection describes various types of features used.
The classification decisions of such models are
interpretable and high-precision: Warner and
Hirschberg (2012) found that the trigram “<DET>
jewish <NOUN>” is the most significant posi-
tive feature for anti-semitic hate, while Waseem
and Hovy (2016) identified predictive character n-
grams using logistic regression coefficients. How-
ever, manually extracted feature spaces are lim-
ited in both their semantic and syntactic repre-
sentational ability. Lexical features are insuffi-
cient when abusive terms take on various dif-
ferent meanings (Kwok and Wang, 2013; David-
son et al., 2017) or are not present at all in
the case of implicit hate speech (Dinakar et al.,
2011). Syntactic features such as part-of-speech
sequences and typed dependencies fail to fully



87

capture complex linguistic forms or accurately
model context (Waseem and Hovy, 2016; Warner
and Hirschberg, 2012). Wiegand et al. (2018)
used feature-based classification to build a lexi-
con of abusive words, which is similar to the in-
terpretability task in this paper of identifying in-
dicative unigram features. While their approach is
primarily applicable to explicit abuse, they showed
that inducing a generic lexicon is important for
cross-domain detection of abusive microposts.

Neural network classifiers. The limitations of
feature engineering motivate classification meth-
ods that can implicitly discover relevant features.
Badjatiya et al. (2017) and Gambäck and Sikdar
(2017) were the first to use recurrent neural net-
works (RNNs) and convolution neural networks
(CNNs), respectively, for hate speech detection in
tweets. A comprehensive comparative study by
Zhang et al. (2018) used a combined CNN and
gated recurrent unit (GRU) network to outperform
the state-of-the-art on 6 out of 7 publicly avail-
able hate speech datasets by 1-13 F1 points. The
authors hypothesize that CNN layers capture co-
occurring word n-grams, but they do not perform
an analysis of the features that their model actu-
ally captures. Deep learning classifiers have also
been explored for related tasks such as personal
attacks and user comment moderation (Wulczyn
et al., 2017; Pavlopoulos et al., 2017). Pavlopoulos
et al. (2017) propose an RNN model with a self-
attention mechanism, which learns a set of weights
to determine the words in a sequence that are most
important for classification.

Visualizing neural networks. Existing ap-
proaches for visualizing RNNs largely involve ap-
plying dimensionality reduction techniques such
as t-SNE (van der Maaten and Hinton, 2008) to
hidden representations. Hermans and Schrauwen
(2013) and Karpathy et al. (2015) investigated the
functionality of internal RNN structures, visual-
izing interpretable activations in the context of
character-level long short-term memory (LSTM)
language models. We are interested in the
high-level semantic features identified by network
structures and are heavily influenced by the sig-
nificant body of work focused on visualizing and
interpreting CNNs. Zeiler and Fergus (2013) in-
troduced a visualization technique that projects
the top activations of CNN layers back into pixel
space. They also used partial image occlusion
to determine the area of a given image to which

Label # Examples % Examples
Hate 1430 5.8%
Offensive 19190 77.4%
Neither 4163 16.8%

Table 1: Distribution of class labels in the dataset.

Figure 1: Illustration of the CNN-GRU architecture.

the network is most sensitive. Girshick et al.
(2013) propose a non-parametric method to visu-
alize learned features of individual neurons for ob-
ject detection. We adapt these techniques to draw
meaningful insights about our problem space.

3 Case Study

Dataset. We use the dataset of 24,802 labeled
tweets made available by Davidson et al. (2017).
The tweets are labeled as one of three classes:
hate speech, offensive but not hate speech, or nei-
ther offensive nor hate speech. The distribution
of labels in the resulting dataset is shown in Ta-
ble 1. Out of the seven hate speech datasets pub-
licly available at the time of this work,1 it is the
only one that is coded for general hate speech,
rather than specific hate target characteristics such
as race, gender, or religion.

CNN-GRU model. We utilize the CNN-GRU
classifier introduced by Zhang et al. (2018), which
achieves the state-of-the-art on most hate speech
datasets including Davidson et al. (2017), and con-
tribute a Tensorflow reimplementation for future
study. The inputs to the model are tweets which
are mapped to sequences of word embeddings.
These are then fed through a 1D convolution and
max pooling to generate input to a GRU. The out-
put of the GRU is flattened by a global max pool-
ing layer, then finally passed to the softmax out-
put layer, which predicts a probability distribution
over the three classes. The model architecture is
shown in Figure 1 and described in detail in the
original paper.

1For details and descriptions of all seven datasets, see
Zhang et al. (2018).



88

Figure 2: Partial occlusion heatmaps of test examples demonstrating four types of errors made by the CNN-GRU
network. Heatmaps are plotted for the predicted class (boxed) while the true class is given below. Darker regions
denote portions of the input to which the classifier prediction is most sensitive.

4 Visualization Techniques

4.1 Partial Occlusion

Previously applied to image classification net-
works, partial occlusion involves iteratively oc-
cluding different patches of the input image and
monitoring the output of the classifier. We apply
a modified version of this technique to our CNN-
GRU model by systematically replacing each to-
ken of a given input sequence with an <unk> to-
ken.2 We then plot a heatmap of the classifier
probabilities of the true class (hate, offensive, or
neither) over the tokens in the sequence.

The resulting visualizations reveal a few major
types of errors made by the CNN-GRU (Figure
2). We observe overlocalization in many long se-
quences, particularly those misclassified as offen-
sive. This occurs when the classifier decision is
sensitive to only a single unigram or bigram rather
than the entire context, as in Figure 2(a). The net-
work loses sequence information during convolu-
tion and shows decreased sensitivity to the longer
context as a result.

Lack of localization is the opposite problem in
which the model is not sensitive to any region of
the input, shown in Figure 2(b). It occurs mostly
in longer and hate class examples. A possible ex-
planation for this type of error is that convolving
sequential data diffuses the signal of important to-
kens and n-grams.

For correctly classified examples, the sensitive
regions intuitively correspond to features like n-
grams, part-of-speech templates, and word depen-
dencies. However, incorrectly classified examples

2<unk> indicates an out-of-vocabulary word. The word
embedding for <unk> is random, whereas in-vocabulary
word embeddings encode meaning via unsupervised pre-
training.

also demonstrate sensitivity to unintuitive fea-
tures that are not helpful for classification. For
instance, Figure 2(c) shows a sensitive region that
crosses a sentence boundary.

Finally, we see a high rate of errors due to the
discretization of the hate and offensive classes.
While hate speech is largely contained within of-
fensive language, the sensitive regions for the two
classes are disparate. In Figure 2(d), the network’s
prediction that the example is offensive is highly
sensitive to the sequence “those god damn”, but
not the racial slur “chinks,” which is both hateful
and offensive.

Some of the errors we identify, such as lack of
localization and unintuitive features, can be ad-
dressed by modifying the model architecture. We
can change the way long sequences are convolved,
or restrict convolutions within phrase boundaries.
More difficult to address are the errors in which
our network is sensitive to the correct regions (or
a reasonable subset thereof) but makes incorrect
predictions. These issues stem from the nature
of the data itself, such as the complex linguistic
similarity between hate and offensive language.
Moreover, many misclassified examples contain
implicit characteristics such as sarcasm or irony,
which limits the robustness of features learned
solely from input text.

4.2 Maximum Activations

The technique of retrieving inputs that maximally
activate individual neurons has been used for im-
age networks (Zeiler and Fergus, 2013; Girshick
et al., 2013), and we adapt it to the CNN-GRU
in order to understand what features of the input
stimuli it learns to detect. For each of the units
in the final global max pooling layer of the CNN-



89

Figure 3: Examples of interpretable units from the
global max pool layer of the CNN-GRU. The inputs
with the top eight activations for each neuron are
shown, with relevant tokens bolded. In the interest of
space, some examples here are abridged and the full
version can be found in Appendix Figure 4.

GRU, we compute the unit’s activations on the en-
tire dataset and retrieve the top-scoring inputs.

Figure 3 displays the maximally activating ex-
amples for seven of 100 units in the global max
pool. We verify that the model does indeed learn
relevant features for hate speech classification,
some of which are traditional natural language
features such as the part-of-speech bigram “you
<NOUN PHRASE>.” Others, like a unit that fires
on sports references and a unit that detects Dutch-
language tweets (the result of querying for hate
keywords yankee and hoe, respectively), reflect as-
sumptions in data collection. Some units capture
features that reflect domain-specific phenomena,
such as repeated symbols or sequences of multiple
abusive keywords.

Many units are too general or not interpretable
at all. For instance, several units detect the
hate term bitch, but none of them clearly capture
the distinction between when it is used in sex-
ist and colloquial contexts. Conversely, exam-
ples containing rarer and more ambiguous slurs
like cracker do not appear as top inputs for any
unit. Overall, the CNN-GRU discovers some in-
terpretable lexical and syntactical features, but its
final layer activations overrepresent common uni-

Category Terms
Hatebase words faggot, nigger, fag, coon, teabag-

ger, cripple, spook, muzzie,
mook, jiggaboo, mutt, redskin,
dink

Hatebase plural faggots, niggers, fags, crackers,
coons, rednecks, hos, queers, col-
oreds, wetbacks, muzzies, wig-
gers, darkies

Pejorative nom-
inalization

blacks, jews, commie, lightskins,
negroes

Hate-related or
offensive

racist, fugly, hag, traitor, chucks,
goon, asss, blacc, eff, homopho-
bic, racists, nogs, muhammed,
fatherless, slurs

Hateful context
words

arrested, yelled, smoked, joints,
stoned, frat, celibate, catfished,
wedlock, sliced, kappa, trap-
pin, birthed, allegiance, menace,
commander, stamp, cyber

Hashtags (see Table 4)
Dialect varia-
tions

des, boutta, denna, waddup, boof,
ont, bestf, playen, sav, erbody,
prolli, deze, bougie

Pop culture gram, tweakin, dej, uchiha,
mewtwo, bios, fenkell, mikes,
beavis, aeropostale

Other en, waffle, moe, saltine, squid,
pacer, sharpie, skyler, sockfetish,
johns, lactose, ov, tater

Table 2: List of keywords discovered via synthetic test
examples. Terms within each category appear in order
of frequency in the corpus. Descriptions for hashtags
are from blogs such as www.socialseer.com and
cross-referenced on Twitter.

grams and fail to detect semantics at a more fine-
grained level via surrounding context.

4.3 Synthetic Test Examples
Lastly, we propose a general technique to identify
the the most indicative unigram features for a deep
model using synthetic test examples and apply it to
the CNN-GRU.

For each word in our corpus, we construct a sen-
tence of the form “they call you <word>” and
feed it as input to the CNN-GRU network. We
choose this structure to grammatically accommo-
date both nouns and adjectives, and because it is
semantically neutral compared to similar formula-
tions such as “you are <word>.” We then retrieve
the words whose test sentences are classified as
hate speech. After filtering out words that do not
appear in two or more distinct tweets (retweets are
indistinct) and words containing non-alphabetical
characters,3 this method returns 101 terms. We
manually group the terms into nine categories and

3We inspect the output to confirm that this filtering elimi-
nates only nonsense tokens and not intentional misspellings.

www.socialseer.com


90

summarize them in Table 2.
As a quantitative heuristic for the quality of the

discovered terms, we evaluate our method’s recall
on the hate speech lexicon Hatebase.4 We measure
the recall of Hatebase words, plural forms of Hate-
base words, and tweets containing Hatebase terms
and compare against a random baseline (Table 3).
The recall of our method is approximately an order
of magnitude better than random across all cate-
gories. Recall of plural forms is better than that of
base forms, which may reflect the training data’s
definition of hate speech as language that targets
a group. Notably, recall of Hatebase tweets5 is
lower than recall of individual terms regardless of
form, meaning that the Hatebase terms discovered
using our template method are not the ones that
occur most commonly in the corpus. This is rea-
sonable as several of the most common Hatebase
terms such as bitch and nigga are ones that tend to
be used colloquially rather than as slurs.

Of the non-Hatebase terms that our method dis-
covers, four are pejorative nominalizations. These
are neutral adjectives that take on pejorative mean-
ing when used as nouns, such as blacks and
jews (Palmer et al., 2017). We also find six
domain-specific hate terms in the form of hash-
tags, which tend to be non-word acronyms and
primarily used by densely connected, politically
conservative Twitter users (see Table 4). The re-
sults also include dialect-specific terms and slang
spellings, such as des and boutta, which mean
these and about to respectively. While these terms
co-occur frequently with hate speech keywords in
our corpus, they are semantically neutral, suggest-
ing that our model exhibits bias towards certain
written vernaculars. While these terms co-occur
frequently with hate speech keywords in our cor-
pus, they are semantically neutral, suggesting that
our model exhibits bias towards certain written
vernaculars.

5 Conclusion

We presented a variety of methods to understand
the prediction behavior of a neural network text
classifier and applied them to hate speech. First,

4Hatebase (https://www.hatebase.org) is an on-
line, crowd-sourced hate speech lexicon. Davidson et al.
(2017) use Hatebase queries to bootstrap the dataset we use
in this paper.

5The number of tweets containing one of the 26 discov-
ered terms divided by the number of tweets containing any
Hatebase term.

Recall Recall % Recall %
(ours) (ours) (random)

Hb terms 13/182 7.14 1.05
Hb plurals 13/91 14.29 1.06
Both 26/273 9.52 1.04

Hb tweets
1453/

6.00 1.21
24234

Table 3: Comparison between our method and a ran-
dom baseline on recall of Hatebase lexicon terms and
tweets containing Hatebase terms. The random base-
line is averaged over 10,000 trials.

Hashtag Meaning
#pjnet Patriot Journalist Network
#lnyhbt Let Not Your Heart Be Troubled,

Sean Hannity’s hashtag
#tgdn Twitter Gulag Defense Network
#httr Hail to the Redskins
#yeswedid Reference to President Obama’s

motto
#acab All Coppers Are Bastards

Table 4: Descriptions of discovered hashtag keywords,
given by blogs such as www.socialseer.com and
cross-referenced on Twitter.

we used partial occlusion of the inputs to visual-
ize the sensitivity of the network. This revealed
that the architecture loses representational capac-
ity on long inputs and suffers from lack of class
separability. We then analyzed the semantic mean-
ing of individual neurons, some of which capture
intuitively good features for our domain, though
many still appear to be random or uninterpretable.
Finally, we presented a way to discover the most
indicative hate keywords for our model. Not all
discovered terms are inherently hateful, revealing
peculiarities and biases of our problem space.

Overall, our experiments give us better insight
into the implicit features learned by neural net-
works. We lay the groundwork for future efforts
towards better modeling and data collection, in-
cluding active removal of linguistic discrimina-
tion.

Acknowledgments

Thanks firstly to Christopher Potts for many use-
ful discussions that formed the foundation for this
work in this paper. Thanks also to the three anony-
mous reviewers for their insightful feedback and
constructive suggestions. This material is based in
part upon work supported by the NSF under Grant
No. BCS-1456077.

https://www.hatebase.org
www.socialseer.com


91

References
Pinkesh Badjatiya, Shashank Gupta, Manish Gupta,

and Vasudeva Varma. 2017. Deep learning for hate
speech detection in tweets. In WWW 2017 Compan-
ion, pages 759–760. International World Wide Web
Conference Committee.

Thomas Davidson, Dana Warmsley, Michael Macy,
and Ingmar Weber. 2017. Automated hate speech
detection and the problem of offensive language.
In Proceedings of the Eleventh International AAAI
Conference on Web and Social Media, pages 512–
515.

Karthik Dinakar, Roi Reichart, and Henry Lieberman.
2011. Modeling the detection of textual cyberbul-
lying. In Fifth International AAAI Conference on
Weblogs and Social Media Workshop on the Social
Mobile Web, pages 11–17.

Björn Gambäck and Utpal Kumar Sikdar. 2017. Us-
ing convolutional neural networks to classify hate-
speech. In Proceedings of the First Workshop on
Abusive Language Online, pages 85–90. Association
for Computational Linguistics.

Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Ji-
tendra Malik. 2013. Rich feature hierarchies for ac-
curate object detection and semantic segmentation.
CoRR, abs/1311.2524.

Michiel Hermans and Benjamin Schrauwen. 2013.
Training and analysing deep recurrent neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 190–198.

Andrej Karpathy, Justin Johnson, and Fei-Fei Li. 2015.
Visualizing and understanding recurrent networks.
CoRR, abs/1506.02078.

Irene Kwok and Yuzhou Wang. 2013. Locate the hate:
Detecting tweets against blacks. In Proceedings of
the Twenty-Seventh AAAI Conference on Artificial
Intelligence, pages 1621–1622. Association for the
Advancement of Artificial Intelligence.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-SNE. Journal of Machine
Learning Research, 9:2579–2605.

Chikashi Nobata, Joel Tetreault, Achint Thomas,
Yashar Mehdad, and Yi Chang. 2016. Abusive lan-
guage detection in online user content. In Proceed-
ings of the 25th International Conference on World
Wide Web, WWW ’16, pages 145–153.

Alexis Palmer, Melissa Robinson, and Kristy Phillips.
2017. Illegal is not a noun: Linguistic form for de-
tection of pejorative nominalizations. In Proceed-
ings of the First Workshop on Abusive Language On-
line, pages 91–100. Association for Computational
Linguistics.

John Pavlopoulos, Prodromos Malakasiotis, and Ion
Androutsopoulos. 2017. Deep learning for user

comment moderation. In Proceedings of the First
Workshop on Abusive Language Online, pages 25–
35. Association for Computational Linguistics.

Anna Schmidt and Michael Wiegand. 2017. A survey
on hate speech detection using natural language pro-
cessing. In Proceedings of the Fifth International
Workshop on Natural Language Processing for So-
cial Media, pages 1–10.

William Warner and Julia Hirschberg. 2012. Detecting
hate speech on the world wide web. In Proceed-
ings of the 2012 Workshop on Language in Social
Media, pages 19–26. Association for Computational
Linguistics.

Zeerak Waseem and Dirk Hovy. 2016. Hateful sym-
bols or hateful people? predictive features for hate
speech detection on twitter. In Proceedings of the
2016 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 88–93. Associ-
ation for Computational Linguistics.

Michael Wiegand, Josef Ruppenhofer, Anna Schmidt,
and Clayton Greenberg. 2018. Inducing a lexicon
of abusive words–a feature-based approach. In Pro-
ceedings of the 2018 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, vol-
ume 1, pages 1046–1056. Association for Compu-
tational Linguistics.

Ellery Wulczyn, Nithum Thain, and Lucas Dixon.
2017. Ex machina: Personal attacks seen at scale.
In Proceedings of the 26th International Conference
on World Wide Web, pages 1391–1399. International
World Wide Web Conferences Steering Committee.

Matthew D. Zeiler and Rob Fergus. 2013. Visualizing
and understanding convolutional networks. CoRR,
abs/1311.2901.

Ziqi Zhang, David Robinson, and Jonathan Tepper.
2018. Detecting hate speech on twitter using a
convolution-gru based deep neural network. In Eu-
ropean Semantic Web Conference 2018, pages 745–
760.

A Appendix

Tr
ue

la
be

ls Hate 0.31 0.58 0.11

Offensive 0.04 0.94 0.03

Neither 0.02 0.13 0.85

Hate Offensive Neither

Predicted labels

Table 5: A summary of performance for the CNN-GRU
classifier on a held-out test set.



92

Figure 4: Version of Figure 3 with full text of each tweet. Examples of interpretable units from the global max
pool layer of the CNN-GRU. The inputs with the top eight activations for each neuron are shown, with relevant
tokens bolded.


