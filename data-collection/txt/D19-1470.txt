



















































Neural Conversation Recommendation with Online Interaction Modeling


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4633–4643,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

4633

Neural Conversation Recommendation with Online Interaction Modeling

Xingshan Zeng1,2, Jing Li3∗, Lu Wang4, Kam-Fai Wong1,2
1The Chinese University of Hong Kong, Hong Kong, China

2MoE Key Laboratory of High Confidence Software Technologies, China
3Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China
4Khoury College of Computer Sciences, Northeastern University, Boston, United States

1,2{xszeng,kfwong}@se.cuhk.edu.hk
3jing-amelia.li@polyu.edu.hk, 4luwang@ccs.neu.edu

Abstract

The prevalent use of social media leads to a
vast amount of online conversations being pro-
duced on a daily basis. It presents a con-
crete challenge for individuals to better dis-
cover and engage in social media discussions.
In this paper, we present a novel framework
to automatically recommend conversations to
users based on their prior conversation behav-
iors. Built on neural collaborative filtering,
our model explores deep semantic features that
measure how a user’s preferences match an
ongoing conversation’s context. Furthermore,
to identify salient characteristics from inter-
leaving user interactions, our model incorpo-
rates graph-structured networks, where both
replying relations and temporal features are
encoded as conversation context. Experimen-
tal results on two large-scale datasets collected
from Twitter and Reddit show that our model
yields better performance than previous state-
of-the-art models, which only utilize lexical
features and ignore past user interactions in the
conversations.

1 Introduction

Social media has profoundly revolutionized peo-
ple’s social interactions, as many individuals now
turn to online platforms to voice opinions and ex-
change ideas. Meanwhile, the abundance of in-
formation brings the problem of information ex-
plosion — the huge volume of online discussions
produced every day has far outpaced any individ-
ual’s capability of digesting them. It is hence diffi-
cult for one to discover online discussions that are
potentially of interest. To address this issue, we
study the problem of online conversation recom-
mendation, with the goal of identifying conversa-
tions that fit a user’s preferences, hence likely to
result in the user’s future engagement.

∗This work was mainly conducted when Jing Li was af-
filiated with Tencent AI Lab, Shenzhen, China.

T1[U1]: The official 9/11 story is a 
complete and total lie. Also, it is 
being used to destroy the U.S.

T2[U2]: …… The depressing thing 
is that it clearly intends to take the 

rest of the world down with it.

T4[U1]: It is not 
the U.S. it is in fact 
a shadow global 
government ......

T3[U3]:Only if the rest of 
the world allows these 
lunatics to do whatever 

they want ......

T7[U5]: Yeah, like these 
people: <URL> ?

T6[U5]: Why? Reddit
users tend to be quite 

intelligent.  Why a 
blind spot about 9/11?

T5[U4]:No, there is a 
9/11 clause in logic…..

Conversation 1

Conversation 2

[U1]: With all of your fears 
how often do you change 

your underwear? ……

[U0]: Why would you want 
limited government? ……

[U0]: we need homeland 
security to protect us, 9/11 & 
katrina were a mistake……

[U1]: That would make 
things interesting. Larry 

Nichols would need to come 
up for Hillary……

[U0]: Except nobody would 
broadcast Larry Nichols 

story……

Previous interactions of U0:

U0 later replies 
to this message!

…… 

Figure 1: Two Reddit conversation snippets on the
right. User U0, whose historical interactions with an-
other user U1 shown on the left, only engages in Con-
versation 1 (which is initialized by U1), but not Con-
versation 2 (U1 does not participate in). Red arrows
indicate in-reply-to relations, and blue arrows depict
chronological orders.

In previous studies, it has been shown that effec-
tive online conversation recommendation has the
potential to produce more positive online social in-
teraction experience (Chen et al., 2011; Zeng et al.,
2018). Prior work on this subject has focused on
post-level recommendation (Yan et al., 2012; Chen
et al., 2012), or conversation-level suggestion with
handcrafted features (Chen et al., 2011) and word
co-occurrence patterns (Zeng et al., 2018). Nev-
ertheless, they ignore the useful information em-
bedded in replying relations, where the conversa-
tion structure is formed via messages sent among
users. In this work, we examine conversation
context, and model the participants’ interactions
therein. This approach enables deep representa-
tion learning that reflects personal interests and
conversation preferences, together signaling what
conversations a user is likely to be involved in.

To illustrate how online interactions could indi-
cate users’ future conversation behavior, Figure 1



4634

shows two conversation snippets on Reddit, both
centering around the September 11 attack (9/11).
As can be seen, user U0, who had discussed the
event according to the chat history, later engaged
in Conversation 1 (C1) instead of Conversation 2
(C2). One explanation is that C1 was initialized
by user U1, whose discussion topics overlap with
U0’s, and more importantly, used to interact with
U0 in many prior discussions.

To model user preferences from their prior in-
teractions, we propose to employ graph-structured
neural networks to explicitly encode who replies
to whom at when in the conversation history. In
this way, temporal features of conversations are
also exploited to capture messages’ chronologi-
cal orders (shown in Figure 1 with blue arrows).
We then incorporate the interaction representa-
tions into a novel neural collaborative filtering
framework (He et al., 2017), which further aligns
user’s preferences with the conversation context.
Compared with existing methods that are based
on handcrafted features (Chen et al., 2011) or
Bayesian models (Zeng et al., 2018), our end-to-
end trained neural model learns to automatically
recommend conversations as well as to encode
user interests embedded in their conversation in-
teractions. To the best of our knowledge, this is the
first work to explore neural conversation recom-
mendation with online interactions explicitly en-
coded for user preference modeling.

To evaluate our model, we conduct extensive
experiments on two large-scale datasets with on-
line conversations from Twitter and Reddit1. Ex-
perimental results show that our method signifi-
cantly outperforms state-of-the-art models that do
not capture user interactions. For example, our
model obtains an MAP (Mean Average Precision)
of 0.625 on Twitter, compared with 0.591 by Zeng
et al. (2018). We further find that our model still
exhibits superior performance when the sparsity
levels of user history and conversation context are
varied, demonstrating our model’s potential ability
to handle sparse conversation records. Additional
experiments on an ablation study confirms the ef-
fectiveness of different components in our frame-
work. A case study further reveals important inter-
action features captured by our model, which in-
dicate their conversation entries and hence explain
our model’s advanced performance. Finally, we

1The datasets and codes are available at: https://
github.com/zxshamson/neural-conv-rec

investigate the challenging task of first time replies
prediction, where our model again produces sig-
nificantly better results than existing popular rec-
ommendation models.

2 Related Work

Our work is in line with conversation behavior
analysis, where studies explore user interactions
in ongoing conversations (Ritter et al., 2010) and
how they signal the conversations’ future trajec-
tory, such as continued activity (Backstrom et al.,
2013; Jiao et al., 2018; Zeng et al., 2019) and the
risk of going awry (Zhang et al., 2018). Different
from these proposals which do not model personal
interests, we study conversation recommendation
for a specific user, where we measure how a user’s
preferences match a conversation’s context.

This work is also related to user response pre-
diction (Artzi et al., 2012; Zhang et al., 2015)
and post recommendation (Duan et al., 2010; Chen
et al., 2012; Yan et al., 2012; Hong et al., 2013).
While most of these studies focus on post model-
ing, we examine conversation context to predict
user engagements, which goes beyond the post-
level prediction task. Other prior work examin-
ing conversation-level recommendation relies on
either manual features (Chen et al., 2011) or shal-
low word occurrence patterns (Zeng et al., 2018),
largely ignoring the useful features from histori-
cal user interactions. On the contrary, we utilize
online user interactions in the conversation his-
tory, to allow the inclusion of richer information of
modeling personal interests. In addition, our neu-
ral network-based model enables automatic learn-
ing for a deeper representation of user interests,
whereas existing methods require significant man-
ual efforts for model customization (Chen et al.,
2011; Zeng et al., 2018).

Furthermore, our user interaction module is in-
spired by prior work on conversation structure
modeling. Compared with popular sequential con-
versation models that focus on messages’ tempo-
ral features (Cheng et al., 2017; Jiao et al., 2018;
Zeng et al., 2019), our module explicitly encodes
the replying relationships to exploit the user con-
versation structure (Miura et al., 2018; Zayats and
Ostendorf, 2018). It is shown that such structure
indicates salient messages and can benefit various
compelling applications, e.g., conversation sum-
marization (Chang et al., 2013; Li et al., 2015) and
discussion topic extraction (Li et al., 2016, 2018).

https://github.com/zxshamson/neural-conv-rec
https://github.com/zxshamson/neural-conv-rec


4635

CNN Encoder

User(!)
Conv(")

Word Embedding

Graph Model

Linear + Tanh

Arcs 
infos

+

Conv RF
Embedding

User RF
Embedding

User CI
Embedding

×

MLP Layers

Linear + Sigmoid

…

…

+ Concatenation
Multiplication

+ +

×
+

+

#$% #$& #$|(|

Figure 2: The architecture of our neural conversation
recommendation framework, which models replying
preferences and conversation interactions to predict a
user’s future engagement in given conversations. RF:
replying factors modeling. CI: conversation interaction
modeling.

However, its effect on conversation recommenda-
tion has not been explored yet, and our work aims
to fill the gap.

3 Neural Conversation Recommendation

This section describes our neural recommenda-
tion model with interaction modeling. Figure 2
shows the overall architecture of our framework
based on neural collaborative filtering (NCF) (He
et al., 2017). Section 3.1 will present an overview
showing how our model works, where both users’
replying history and conversations’ interaction
structure will be encoded for recommendation.
Their modeling details will be given in Section 3.2
and 3.3 in turn. At last, Section 3.4 shows the over-
all model training process.

3.1 Model Overview

Here we first describe the input and output. For
training, our model is fed with a conversation
dataset C. Each conversation c ∈ C is formed with
a sequence of turns 〈t1, t2, ..., t|c|〉, where |c| de-
notes the number of turns. Each turn t is in form
of a word sequence 〈w1, w2, ..., w|t|〉with |t| being
t’s word number. Its author is represented by user
id ut. We also record each turn’s parent turn in re-
plying relations (i.e. which turn it replies to) and
chronological order (i.e. which turn posted before
it), so that the interaction patterns within a conver-

sation can be captured. We will talk more about it
in Section 3.3.

For recommendation, our model is taken a user
u and a conversation c as input, and then predict
how likely u will engage in c, conditioned on u’s
previous behavior and c’s context history.

Our goal is to predict ŷu,c ∈ [0, 1], which mea-
sures how likely user u will engage in conversa-
tion c. Here to estimate ŷu,c, two types of infor-
mation are encoded: replying history of users and
interaction structure of conversations. The former
is captured from what conversations a user previ-
ously replied to, where we learn rRFu,c to encode u’s
replying preference on c. However, such learned
representation captures user replying preferences
without diving into turn-level features and interac-
tion structure in conversations. So we utilize the
latter to explore how users interact with each other
in conversation context, which encodes words in
turns and turn interactions to produce conversa-
tion interaction representation rCIu,c, reflecting a
denser preference of a user. In Section 3.2, we
will present how to learn rRFu,c , and in Section 3.3
we learn about rCIu,c.

Coupling the rRFu,c and r
CI
u,c, we predict ŷu,c via

the formula below:

ŷu,c = σ(h
T
O[r

RF
u,c ; r

CI
u,c]) (1)

where σ(·) denotes sigmoid activation, [; ] indi-
cates concatenation operation, and hO is a learn-
able parameter. In recommendation for user u, we
rank the conversations with ŷu,c and the top N re-
sults will serve as our final output.

3.2 Replying Factors Modeling
As mentioned in Section 3.1, we first model users’
replying preferences with what conversations they
entered before. We follow the practice in He et al.
(2017) to use two embedding layers, IRFU (·) and
IRFC (·), to capture the latent factors for users and
conversations that result in user’s previous reply-
ing history. For user u, we can obtain its user em-
bedding rRFu by looking up u in I

RF
U (·). A conver-

sation embedding rRFc can be similarly obtained
from IRFC (·). Then we measure user u’s replying
preference over conversation c with the similarity
between rRFu and r

RF
c :

rRFu,c = r
RF
u � rRFc (2)

where � denotes element-wise product. As can
be seen, rRFu,c is able to encode what conversations



4636

a user engages in and analyze the factors of why
it happen, simply with general replying history.
More features will be explored via conversation
interaction modeling presented in Section 3.3.

3.3 Conversation Interaction Modeling

We first explore users’ prior interaction behavior
in conversations. A user embedding layer ICIU (·)
is hence employed, where the embedding rCIu for
user u reflects u’s interaction patterns, such as
what they used to say and whom they usually in-
teracted with. For the conversation modeling, we
adopt graph-structured networks to model the in-
teraction structure therein and yield a representa-
tion rCIc for conversation c. The effects of user-
and conversation-specific interaction features are
combined with Multilayer Perceptron (MLP):

rCIu,c = α(W
T
M (...α(W

T
1 [r

CI
u ; r

CI
c ] + b1)...) + bM ) (3)

where α(·) is ReLU-activated function (Rectified
Linear Unit) and M is the number of layers in
MLP. In the following, we will introduce how we
obtain rCIc via modeling of intra-turn features and
inter-turn interactions.

Turn-level Modeling. Here we describe how we
model turn-level representations, which combine
what content it conveys and who its author is.

Content representation is to reflect how words
appear therein, where we employ a Convolutional
Neural Network (CNN) (Kim, 2014) encoder to
model a turn’s word sequence. Specifically, given
a turn t in conversation c, we first map each word
in t into a word embedding layer (initialized with
pre-trained word vectors) to explore deep word se-
mantics. And then, to capture how a word appears
in local context with its neighbors, a CNN encoder
is exploited to generate the turn-level content rep-
resentation zt.

Next, we concatenate zt, conveying content fea-
tures, and rCIut , embedded with the interaction pat-
terns of t’s author ut, to produce a turn represen-
tation rTRt . It couples turn t’s word occurrence
patterns and its author’s history interactions with
other conversation turns. Afterwards, rTRt is de-
livered to model t’s interaction with the other turns
in c. We will describe how it is processed next.

Turn Interaction Modeling. To encode conver-
sation interaction structure, we first organize the
turns in a conversation c as a reply tree to for-
mulate who replies to whom. Each node therein

time

state gstate g-1

4/0
78,

4/5
78,

4/6
78,

4/2
78,

4/0
7

4/5
7

4/6
7

4/2
7

9:;

<=>

(a) Graph-State LSTM

Gates

LSTM

LSTM

LSTM

LSTM

ReLU ……
(N layers)

Wself

Wpre

Wsuc

(One GCN layer)(BiLSTM layer)

4/0&/01'

&/51'

&/61'

&/21' 4/2

4/6

4/5

(b) Graph Convolutional Networks

Figure 3: Two variants of our proposed graph-
structured networks.

represents a turn and the edges reflect replying re-
lations (directed from turns to replies such as the
red arrows in Figure 1). Moreover, to exploit tem-
poral information, we add another kind of edges
to indicate chronological order (such as the blue
arrows in Figure 1). In doing so, a reply tree is
extended to a directed graph (such as the one in
Figure 1), with both replying and temporal inter-
actions encoded and therefore named as an inter-
action graph. For each turn t on the graph, we dis-
tinguish its neighbors into predecessors, denoted
by Ep(t), and successors, Es(t).

Then, we employ graph-structured networks to
model the interaction structure. There are two
modeling methods discussed here: Graph-State
LSTM (Long Short-Term Memory) (henceforth
GLSTM) (Beck et al., 2018; Song et al., 2018)
and Graph Convolutional Networks (henceforth
GCN) (Kipf and Welling, 2017; Marcheggiani and
Titov, 2017), whose empirical effectiveness will
be compared in Section 5.1. Here we present their
architecture in Figure 3 and describe how they
model conversation interactions below.

Graph-State LSTM. We start with GLSTM and
show its architecture in Figure 3(a). It is an exten-
sion of LSTM from sequence to graph structure,
where a turn’s hidden states are updated condi-
tioned on both the turn-level representation rTRt



4637

and the states of all its neighbors on the graph.
The update strategy is the same as standard LSTM
(Hochreiter and Schmidhuber, 1997), except for
the following formula, which can be used in the
update of input gate, output gate, forget gate, and
content recorder:

gGLSTMt = σ(W
pxpt +W

sxst +U
phpt +U

shst + b) (4)

The first two terms explore the turn-level rep-
resentations (rTRt ) from the neighbors. The third
and forth terms capture turn interactions on the
graph. b denotes the bias. The superscripts p and s
indicate the neighbor being a predecessor or suc-
cessor. xpt takes the sum of predecessor k’s turn
representations rTRk and so does x

s
t for successors.

h∗t means the neighbors’ hidden states in their
last updates. W ∗ and U∗ are learnable parameter
weights and σ(·) means sigmoid activation. More-
over, in GLSTM, we define the state number g to
reflect the maximum order of GLSTM state transi-
tions, where the larger g indicates longer turn de-
pendency on graph paths encoded. Here due to
the space limitation, we leave out the details of
GLSTM and refer the readers to Song et al. (2018).

Afterwards, to produce conversation represen-
tation rCIc with turn interactions, we combine all
turns’ hidden states with average pooling and map
them into the same dimension (with Tanh activa-
tion) as rCIu to measure user and conversation sim-
ilarity.

Graph Convolutional Networks. Figure 3(b)
shows the architecture of GCN, which can be con-
sidered as CNN on graph. Here following previ-
ous practice (Marcheggiani and Titov, 2017), be-
fore using GCN to model turn interactions, we first
feed the turn representations rTRt into a sequen-
tial Bidirectional LSTM (BiLSTM) layer to cap-
ture the chronological turn interactions. Then, we
take the t-th hidden states of BiLSTM hLSTMt to
further capture turn t’s interaction with its neigh-
bors on interaction graph. The formula describing
this process is given as:

hGCNt =
∑

i∈Ep(t)

ωi,t(W
prehLSTMi + b

pre) +

∑
j∈Es(t)

ωj,t(W
suchLSTMj + b

suc) +

ωt,t(W
selfhLSTMt + b

self )

(5)

Here following Marcheggiani and Titov (2017),
we use different sets of parameters to fit varying
types of interactions: self interactions (self), inter-
action from predecessors to successors (pre), and

that from the other way around (suc). ωi,j is a
scalar gate controlling weights defined below:

ωi,j = σ(h
LSTM
i · V dir(i,j) + ddir(i,j)) (6)

It is to identify the neighbors affecting more to t
than others. dir(i, j) indicates the type of i-j di-
rection (pre, suc, or self).

Furthermore, to allow deep interactions to be
learned, we can stack multiple GCN layers to form
a multi-layer GCNs, where we apply a ReLU acti-
vated function between two layers. After that, we
take the similar operations as for GLSTM yield
conversation representation rCIc as c’s conversa-
tion interaction representation.

3.4 Model Training
Here we describe how we formulate our learn-
ing objective and train our model. As mentioned
above, our goal is to predict a score ŷu,c ∈ [0, 1]
indicating how likely user uwill reply to conversa-
tion c. In training, we adopt binary cross-entropy
as our learning loss with penalty given to negative
feedback (u does not engage in c ). It is because
negative feedback may happen for many unpre-
dictable reasons, such as users being too busy to
go online. Thus for conversation recommendation,
we rely more on the positive feedback and design
the weighted binary cross-entropy loss below:

L = −
∑

(u,c)∈T

[
λ ·yu,c log(ŷu,c)+(1−yu,c) log(1− ŷu,c)

]
(7)

where T is a set of training instances. yu,c is a bi-
nary label indicating whether u replied to c, and
ŷu,c is our predicted score. λ (λ > 1) is a pre-
defined parameter to trade off the weights of posi-
tive and negative instances.

In model training, the negative sampling strat-
egy is adopted (He et al., 2017), whose sampling
ratio (the number of negative samples for each
positive instance) is set to 5. Also, we pre-train
the embedding layers for both the replying factors
and conversation interaction modeling with the pa-
rameters from He et al. (2017). We will discuss the
effects of pre-training in Section 5.3.

4 Experimental Setup

Data Collection and Preprocessing. In our ex-
periments, we use datasets from two different plat-
forms: the first one is released by Zeng et al.
(2018) containing Twitter conversations formed
by tweets from the TREC 2011 microblog track



4638

Twitter Reddit
# of users 10,122 13,134
# of conversations 7,500 29,477
# of turns 38,999 109,774
Avg. # of conversations per user 1.7 5.9
Avg. # of turns per conversation 5.2 3.7
Avg. # of users per conversation 2.3 2.6

Table 1: Dataset statistics.

data2 covering a diverse set of topics; the other is
from Zeng et al. (2019), which is comprised of dis-
cussion threads about political issues on Reddit, a
popular discussion website.

The tweets in Twitter dataset were mainly
posted from Jan 23 to Feb 8, 2011, and discussion
threads in Reddit dataset were posted from Jan to
Dec, 2008. To discover the whole conversations,
we retrieved all messages with replying relations
(indicated by “parent id” property in Reddit cor-
pus, for example), and recorded their authors and
parent messages. Finally, conversations with only
one message were removed.

We applied the Glove tweet preprocessing
toolkit (Pennington et al., 2014)3 on the Twitter
dataset. As for the Reddit dataset, we performed
tokenization using open source natural language
toolkit (NLTK) (Loper and Bird, 2002), with links
replaced to a generic tag “URL” and all number to-
kens removed. We maintained a vocabulary with
all the rest characters appearing in the corpus for
both datasets, including punctuation and emoti-
cons.

Data Statistics and Analysis. The statistics of
two datasets are shown in Table 1, with more in-
formation in Figure 4. We can observe that Reddit
dataset contains more conversations, with a higher
average number of conversations per user. On the
other hand, Twitter conversations are longer, with
fewer participants. Figure 4(a) shows that most
users participate in very few conversations in both
datasets, indicating a potential sparsity problem.
In terms of conversation structure (Figure 4(b)),
most conversations only contain one path where
the replying relations precisely follow the chrono-
logical order; whereas the Reddit dataset contains
more tree-structured conversations with rich and
complex interactions.

To further illustrate the effect of a conversa-
2https://trec.nist.gov/data/tweets/
3https://nlp.stanford.edu/projects/

glove/preprocess-twitter.rb

2^0

2^3

2^6

2^9

2^12

2^15

1 4 7 10 13 >15

# 
of

 u
se

rs

# of conversations a user participated 

Twitter

Reddit

(a) User conversation dist.

0

0.2

0.4

0.6

0.8

1

One	path Two	path More	than	two

Twitter

Reddit

(b) Ratio of diff. structure

Figure 4: Distribution of number of conversations
per user (left) and proportion of different conversation
structures (right) for both datasets.

tion’s structure on its future development, we cal-
culate the likelihoods of (1) new users joining the
discussion, and (2) current participants continuing
the conversation, grouped by different conversa-
tion structures (Table 2). In general (especially on
Reddit), conversations with only one path tend to
continue within the current participants, and keep
newcomers out, whereas conversations with com-
plex structures are more likely to attract new users.

Twitter Reddit
Paths amount 1 2 > 2 1 2 > 2
New comers rate 0.18 0.20 0.16 0.30 0.45 0.50
Continue rate 0.70 0.64 0.74 0.51 0.31 0.38

Table 2: Different structures and the future trend of
conversations.

Model Setting. We follow the experimental set-
tings employed in previous work (Zeng et al.,
2018). For each conversation, we take first 75%
of the context as observation for training purpose.
The rest is equally divided into a testing set and a
development set. For negative instances, we also
split the unobserved user-conversation pairs into
three parts: 80%, 10%, and 10% for training, test-
ing, and development, respectively. Furthermore,
due to the large amount of conversations in Red-
dit dataset, we only sample 100 negative instances
uniformly from them for testing and development.

For parameters setups, we initialize the word
embedding layer with 200-dimensional Glove em-
bedding (Pennington et al., 2014), where the Twit-
ter version is used for our Twitter dataset, and the
Common Crawl version is applied on the Reddit
dataset4. Factor dimension for the RF part is set to
20, while for the CI part it is 100. For the CNN

4https://nlp.stanford.edu/projects/
glove/

https://trec.nist.gov/data/tweets/
https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb
https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb
https://nlp.stanford.edu/projects/glove/
https://nlp.stanford.edu/projects/glove/


4639

encoders, we use filter windows of 2, 3, and 4,
each with 100 feature maps. For the size of hid-
den states of our graph models, we set 200 (100 for
each direction for BiLSTM). The number of MLP
layers is 3. During training, the batch size is set to
512 and Adam optimizer (Kingma and Ba, 2014)
is adopted with an initial learning rate of 0.01. We
set the trade off weight in learning loss λ = 100.

Evaluation and Comparisons. Following Zeng
et al. (2018)’s work, we adopt mean average pre-
cision (MAP), precision at 1 (P@1), and normal-
ized Discounted Cumulative Gain at 5 (nDCG@5)
for evaluation (we also try other metrics including
P@5 and nDCG@10, and find similar trends). The
metrics are first computed for users in the datasets,
then averaged over all users.

For comparison, we first search for the best
model among different interaction modeling (Sec-
tion 5.1). We then consider two baselines: 1) rank-
ing conversations randomly (RANDOM), 2) con-
versations with more participants ranked higher
(POPULARITY). Previous work compared in-
cludes5:
• RSVM: Ranks conversations for each user

with features described in Duan et al. (2010) by
ranking SVM (Joachims, 2002).
• NCF: The neural CF model (He et al., 2017),

not utilizing any context information.
• CONVMF: A CNN-based model for recom-

mendation with reviews (Kim et al., 2016), where
we adapt to use a hierarchical two-layer CNN to
model words in turns and turn sequences.
• CR JTD: The state-of-the-art method for our

task (Zeng et al., 2018), with a Bayesian model
jointly modeling topics and discourse.

5 Experimental Results

In this section, we first evaluate the effectiveness
of varying modules for conversation interaction
modeling in Section 5.1. Then our model with the
best module is further compared with the baselines
and previous recommendation systems in Section
5.2. There we also discuss the model performance
given varying conversation context length and user
interaction sparsity. We further discuss our model
with an ablation study and a case study in Section
5.3. Finally in Section 5.4, we analyze the results
of first time replies prediction.

5We did not compare with Chen et al. (2011) since that
method mainly requires information about social relation-
ship, which is unavailable in our datasets.

Models Train Time MAPTwitter Reddit
BiLSTM 0.94 0.617 0.498
GLSTM 1.25 0.617 0.528
GCN (W/O BiLSTM) 1.03 0.619 0.530
GCN (With BiLSTM) 1.00 0.620 0.533

Table 3: Results of our model variants on development
set. The best MAP results are in bold. “Train Time”:
training time per epoch divided by that of model GCN
(With BiLSTM).

5.1 Interaction Modeling Comparison

We first compare the effects of varying interaction
modeling methods (see Section 3.3) on conversa-
tion recommendation. Table 3 displays their re-
sults on development set. In comparison, we con-
sider BiLSTM over turn sequence (only chrono-
logical order encoded and henceforth BiLSTM),
GLSTM (state number g = 6), GCN (layer num-
ber set to 3) without BiLSTM-encoded tempo-
ral representations (henceforth GCN (W/O BiL-
STM)), and the full GCN described in Section 3.3
(henceforth GCN (With BiLSTM) and layer num-
ber set to 1). The above hyper-parameters are
tuned based on the training loss.

From the results, we find that BiLSTM exhibits
the worst results for not encoding replying rela-
tions. Its difference from others are larger on Red-
dit attributed to the rich replying structure therein
(as shown in Figure 4(b)). The best performance is
achieved for GCN (With BiLSTM), with relatively
less training time. This shows the effectiveness
and efficiency to explore the order of turns with
BiLSTM and the user interactions with GCN. In
the later analysis, we will only discuss our model
that exploits GCN (With BiLSTM) for interaction
modeling.

5.2 Comparisons with Previous Work

Main Results. Table 4 shows the conversation
recommendation results with baselines and state
of the arts. Our model exhibits the best results on
both datasets, significantly outperforming all the
comparison models. It indicates the usefulness to
encode user interactions for conversation recom-
mendation. Particularly, CONVMF is able to en-
code turns’ temporal orders yet ignores how they
reply with each other in conversation history. It is
outperformed by our model, showing the benefit
to capture users’ replying patterns for predicting
what conversations will draw their engagement.



4640

Models
Twitter Reddit

MAP P@1 nDCG MAP P@1 nDCG
Baselines
RANDOM 0.006 0.001 0.002 0.040 0.010 0.022
POPULARITY 0.023 0.005 0.010 0.082 0.033 0.063
Comparisons
RSVM 0.554 0.575 0.559 0.453 0.457 0.466
NCF 0.573 0.593 0.576 0.412 0.544 0.461
CONVMF 0.579 0.596 0.583 0.485 0.532 0.520
CR JTD 0.591 0.591 0.600 0.453 0.559 0.485
OURS 0.625 0.632 0.626 0.538 0.674 0.590

Table 4: Main results on conversation recommenda-
tion. “nDCG” stands for “nDCG@5”. The best result
for each column is in bold. Our model significantly
outperforms all the comparisons (p < 0.01, paired t-
test).

We also observe that both baseline models work
poorly. It is because conversation recommenda-
tion is challenging, not possible to be well tackled
with simple ranking strategies.

In addition, we notice that CR JTD outper-
forms CONVMF on Twitter, with opposite obser-
vation made on Reddit. It is possibly because
Twitter exhibits more informal language style.
Thus CR JTD, taken bag-of-words input, can bet-
ter fit the data than CONVMF, taking word or-
ders into account. Nevertheless, our model outper-
forms them both, showing that prior interactions
among users can better signal their future reply be-
havior compared with the words they said.

The final observation is that, all comparing
methods (except for the naive baselines) perform
better on Twitter than Reddit. One reason is that
the Twitter dataset is smaller and contains fewer
users and conversations. Another possible reason
might be that the topics in the Reddit dataset are
mostly about politics, while the Twitter conversa-
tions are of diverse topics, which makes the model
easier to distinguish user interests.

Training with Varying Conversation History.
The main results are reported given the first 75%
turns as conversation history. Here we investigate
how the length of conversation history affects re-
ply preference prediction. The models are hence
trained with the first 25%, 50%, and 75% turns
as conversation history and their MAP scores are
shown in Figure 5.

As can be seen, all models exhibit better results
when trained with longer history. This shows that
users’ future conversation preference can be better
predicted with richer history data. We also observe

0.3

0.4

0.5

0.6

0.7

25% 50% 75%

RSVM ConvMF
CR_JTD Ours

(a) Twitter

0.2

0.3

0.4

0.5

0.6

25% 50% 75%

RSVM ConvMF
CR_JTD Ours

(b) Reddit

Figure 5: MAP scores (in Y-axis) of models trained
with the first 25%, 50%, and 75% turns as history.

0.5

0.6

0.7

0.8

0.9

1

All >1 >2 >3

ConvMF CR_JTD Ours

Figure 6: MAP scores (in Y-axis) for users with vary-
ing degrees of interaction history. X-axis indicates the
number of conversations users previously engage in.
“All” means recommendation for all users.

that our model obtains the best MAP on both the
50% and 75% setting, while for 25%, it is outper-
formed by CR JTD. It might be ascribed to the
sparse user interactions exhibited in 25% context,
where there are only 1 or 2 turns on average ac-
cording to Table 1.

Results for Varying User Interaction Sparsity.
Figure 4(a) has shown that most users only engage
in very few conversations. This results in severe
data sparsity in user interaction history, especially
on Twitter. We are hence interested in how models
perform on varying degree of sparsity.

Figure 6 shows the MAP scores in recommen-
dation to users engaged in varying number of con-
versations before on Twitter. As can be seen,
user interaction sparsity can largely affect rec-
ommendation performance, where all models per-
form poorly for users exhibiting less than one con-
versation entry. We also observe that our model
performs consistently better in varying degrees of
sparsity. It may because our model is able to learn
rich interactions from conversation context, which
helps alleviate the sparsity in user history.

5.3 Further Discussion

Here we further discuss what our model learns
leading to its superiority.



4641

Ablation Study. We start with an ablation study
to discuss the relative contributions of our differ-
ent components. The MAP scores of their abla-
tions are compared in Table 5. Our full model per-
forms the best, showing that all components are
useful. It is also seen that RF modeling contribute
the most, meanwhile better contributions can be
made with its parameters pre-trained. It indicates
the crucial role RF modeling plays for neural con-
versation recommendation.

Models Twitter Reddit
W/O RF modeling (Sec. 3.2) 0.509 0.239
W/O pre-training (Sec. 3.4) 0.593 0.525
W/O MLP layers (Eq. 3) 0.600 0.537
W/O gate control weights (Eq. 6) 0.608 0.530
Our full model 0.625 0.538

Table 5: MAP scores obtained by our ablations. The
best MAP results are highlighted in bold.

Case Study. To further understand how our
model predicts users’ conversation preferences,
we take the example in Figure 1 and analyze what
our model learns for it. Recall that user U0 used to
discuss a lot on 9/11 with U1, who starts C1. U0
later engages in C1 instead of C2, though it also
concerns 9/11.

U1 U2 U4 U5
rRFu 0.761 0.731 0.681 0.659
rCIu 0.763 0.453 0.287 0.737

Table 6: Cosine similarity between U0’s user embed-
dings in RF and CI modeling with others’.

Table 6 shows the similarity of user factors
learned by our RF (replying factor modeling in
Section 3.2) and CI (conversation interaction mod-
eling in Section 3.3) modules. As can be seen,
both modules learn that U0 and U1 are simi-
lar (probably referred from their frequent interac-
tions). Their joint effects result in our successful
prediction of U0 to engage in C1 rather than C2.
For the same reason, our model can further predict
that U0 is more likely to reply to T4 (in C1), which
is posted by U1, based on the similarity between
user factors and turn representations.

5.4 First Time Replies Prediction

In some scenarios, users may be more interested in
seeing new conversations, which they haven’t seen
before but potentially match their preferences. We
hence examine model performance to predict only

first time replies and show their MAP scores in
Table 7. It is observed that all models perform
poorly, which implies that recommending unseen
conversations to users is extremely difficult. This
finding is consistent with Zeng et al. (2018). How-
ever, our model still outperforms others by a large
margin. It again demonstrates the effectiveness of
modeling user interactions for recommendation.

Models Twitter Reddit
RSVM 0.002 0.049
NCF 0.033 0.038
CONVMF 0.049 0.210
CR JTD 0.090 0.075
OURS 0.160 0.212

Table 7: MAP scores for first time replies prediction.

6 Conclusion

We study neural conversation recommendation
with graph-structured networks to encode user in-
teractions. Experimental results on Twitter and
Reddit show our model significantly outperforms
the state of the arts. We also observe that compet-
itive results can still be obtained on varying con-
versation history length and user interaction spar-
sity. Further discussions analyze the contributions
of different components of our model and the use-
ful features we learn leading to our superiority.
At last, we study a challenging task of first time
replies prediction, where our model still exhibits
its effectiveness.

Acknowledgements

This work is partially supported by the follow-
ing HK grants: RGC-GRF (14232816, 14209416,
14204118, 3133237), NSFC (61877020) & ITF
(ITS/335/18). Lu Wang is supported in part by
National Science Foundation through Grants IIS-
1566382 and IIS-1813341. We thank the three
anonymous reviewers for the insightful sugges-
tions on various aspects of this work.

References
Yoav Artzi, Patrick Pantel, and Michael Gamon. 2012.

Predicting responses to microblog posts. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
602–606. Association for Computational Linguis-
tics.



4642

Lars Backstrom, Jon M. Kleinberg, Lillian Lee, and
Cristian Danescu-Niculescu-Mizil. 2013. Charac-
terizing and curating conversation threads: expan-
sion, focus, volume, re-entry. In Sixth ACM Inter-
national Conference on Web Search and Data Min-
ing, WSDM 2013, Rome, Italy, February 4-8, 2013,
pages 13–22.

Daniel Beck, Gholamreza Haffari, and Trevor Cohn.
2018. Graph-to-sequence learning using gated
graph neural networks. In Proceedings of the 56th
Annual Meeting of the Association for Computa-
tional Linguistics, ACL 2018, Melbourne, Australia,
July 15-20, 2018, Volume 1: Long Papers, pages
273–283.

Yi Chang, Xuanhui Wang, Qiaozhu Mei, and Yan
Liu. 2013. Towards twitter context summarization
with user influence models. In Sixth ACM Inter-
national Conference on Web Search and Data Min-
ing, WSDM 2013, Rome, Italy, February 4-8, 2013,
pages 527–536.

Jilin Chen, Rowan Nairn, and Ed Huai-hsin Chi. 2011.
Speak little and well: recommending conversations
in online social streams. In Proceedings of the In-
ternational Conference on Human Factors in Com-
puting Systems, CHI 2011, Vancouver, BC, Canada,
May 7-12, 2011, pages 217–226.

Kailong Chen, Tianqi Chen, Guoqing Zheng, Ou Jin,
Enpeng Yao, and Yong Yu. 2012. Collaborative per-
sonalized tweet recommendation. In Proceedings of
the 35th international ACM SIGIR Conference on
Research and development in information retrieval,
pages 661–670. ACM.

Justin Cheng, Michael Bernstein, Cristian Danescu-
Niculescu-Mizil, and Jure Leskovec. 2017. Any-
one can become a troll: Causes of trolling behavior
in online discussions. In Proceedings of the 2017
ACM Conference on Computer Supported Coopera-
tive Work and Social Computing, CSCW ’17, pages
1217–1230. ACM.

Yajuan Duan, Long Jiang, Tao Qin, Ming Zhou, and
Heung-Yeung Shum. 2010. An empirical study on
learning to rank of tweets. In Proceedings of the
23rd International Conference on Computational
Linguistics, pages 295–303. Association for Com-
putational Linguistics.

Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie,
Xia Hu, and Tat-Seng Chua. 2017. Neural collabo-
rative filtering. In Proceedings of the 26th Interna-
tional Conference on World Wide Web, WWW 2017,
Perth, Australia, April 3-7, 2017, pages 173–182.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Liangjie Hong, Aziz S Doumith, and Brian D Davison.
2013. Co-factorization machines: modeling user in-
terests and predicting individual decisions in Twit-
ter. In Proceedings of the sixth ACM International

Conference on Web Search and Data Mining, pages
557–566. ACM.

Yunhao Jiao, Cheng Li, Fei Wu, and Qiaozhu Mei.
2018. Find the conversation killers: A predictive
study of thread-ending posts. In Proceedings of the
2018 World Wide Web Conference on World Wide
Web, pages 1145–1154. International World Wide
Web Conferences Steering Committee.

Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the
Eighth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 133–
142.

Dong Hyun Kim, Chanyoung Park, Jinoh Oh, Sungy-
oung Lee, and Hwanjo Yu. 2016. Convolutional ma-
trix factorization for document context-aware rec-
ommendation. In Proceedings of the 10th ACM
Conference on Recommender Systems, Boston, MA,
USA, September 15-19, 2016, pages 233–240.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1746–1751.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Thomas N. Kipf and Max Welling. 2017. Semi-
supervised classification with graph convolutional
networks. In 5th International Conference on
Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Pro-
ceedings.

Jing Li, Wei Gao, Zhongyu Wei, Baolin Peng, and
Kam-Fai Wong. 2015. Using content-level struc-
tures for summarizing microblog repost trees. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2015, Lisbon, Portugal, September 17-21, 2015,
pages 2168–2178.

Jing Li, Ming Liao, Wei Gao, Yulan He, and Kam-Fai
Wong. 2016. Topic extraction from microblog posts
using conversation structures. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2016, August 7-12, 2016,
Berlin, Germany, Volume 1: Long Papers.

Jing Li, Yan Song, Zhongyu Wei, and Kam-Fai Wong.
2018. A joint model of conversational discourse
and latent topics on microblogs. Computational Lin-
guistics, 44(4).

Edward Loper and Steven Bird. 2002. Nltk: The natu-
ral language toolkit. In Proceedings of the ACL-02
Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing and Compu-
tational Linguistics.



4643

Diego Marcheggiani and Ivan Titov. 2017. Encod-
ing sentences with graph convolutional networks
for semantic role labeling. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2017, Copenhagen,
Denmark, September 9-11, 2017, pages 1506–1515.

Yasuhide Miura, Ryuji Kano, Motoki Taniguchi,
Tomoki Taniguchi, Shotaro Misawa, and Tomoko
Ohkuma. 2018. Integrating tree structures and graph
structures with neural networks to classify discus-
sion discourse acts. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics,
COLING 2018, Santa Fe, New Mexico, USA, August
20-26, 2018, pages 3806–3818.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of Twitter conversations. In
Human Language Technologies: Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 172–180.

Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel
Gildea. 2018. A graph-to-sequence model for amr-
to-text generation. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2018, Melbourne, Australia, July
15-20, 2018, Volume 1: Long Papers, pages 1616–
1626.

Rui Yan, Mirella Lapata, and Xiaoming Li. 2012.
Tweet recommendation with graph co-ranking. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 516–525. Association for
Computational Linguistics.

Victoria Zayats and Mari Ostendorf. 2018. Conver-
sation modeling on reddit using a graph-structured
LSTM. TACL, 6:121–132.

Xingshan Zeng, Jing Li, Lu Wang, Nicholas
Beauchamp, Sarah Shugars, and Kam-Fai Wong.
2018. Microblog conversation recommendation via
joint modeling of topics and discourse. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT 2018,
Volume 1 (Long Papers), pages 375–385.

Xingshan Zeng, Jing Li, Lu Wang, and Kam-Fai Wong.
2019. Joint effects of context and user history for
predicting online conversation re-entries. In Pro-
ceedings of the 57th Conference of the Association
for Computational Linguistics, ACL 2019, Florence,
Italy, July 28- August 2, 2019, Volume 1: Long Pa-
pers, pages 2809–2818.

Justine Zhang, Jonathan P Chang, Cristian Danescu-
Niculescu-Mizil, Lucas Dixon, Yiqing Hua, Nithum
Thain, and Dario Taraborelli. 2018. Conversations
gone awry: Detecting early signs of conversational
failure. arXiv preprint arXiv:1805.05345.

Qi Zhang, Yeyun Gong, Ya Guo, and Xuanjing Huang.
2015. Retweet behavior prediction using hierarchi-
cal Dirichlet process. In AAAI, pages 403–409.

http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162

