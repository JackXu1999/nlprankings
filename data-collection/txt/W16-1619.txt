



















































Parameterized context windows in Random Indexing


Proceedings of the 1st Workshop on Representation Learning for NLP, pages 166–173,
Berlin, Germany, August 11th, 2016. c©2016 Association for Computational Linguistics

Parameterized context windows in Random Indexing

Tobias Norlund
Schibsted Products and Technology

Västra Järnvgsgatan 21
111 64 Stockholm

Sweden
tobias.norlund@schibsted.com

David Nilsson
Nepa

Maria Skolgata 83
118 53 Stockholm

Sweden
david.nilsson@nepa.com

Magnus Sahlgren
Gavagai

Slussplan 9
111 30 Stockholm

Sweden
magnus.sahlgren@gavagai.se

Abstract

This paper introduces a parameterization
for word embeddings produced by the
Random Indexing framework. The pa-
rameterization introduces position specific
weights in the context windows, and the
approach is shown to improve the perfor-
mance in both word similarity and senti-
ment classification tasks. We also demon-
strate the relation between Random Index-
ing and Convolutional Neural Networks.

1 Introduction

Quantifying the importance of contextual infor-
mation for semantic representation is the goal of
distributional semantics, in which contextual in-
formation is used to quantify semantic similarities
between words (Turney and Pantel, 2010). How-
ever, standard practice in distributional semantics
is to weight the importance of context items based
on either its frequency (Sahlgren et al., 2016), its
distance to the focus word (Lund et al., 1995), or
its global co-occurrence statistics (Niwa and Nitta,
1994). Thus far, there has not been much work on
applying machine learning to this in order to select
useful context items for distributional semantics.

The idea with the proposed parameterization is
to weight the items in the context window based
on their usefulness for accomplishing some spe-
cific task, such as sentiment classification or word
similarity rating. In this paper, we introduce a
simple parameterization for the Random Indexing
processing model. We first show that Random In-
dexing can be formulated in terms of a convolu-
tion, in order to situate the framework in the con-
text of neural networks. We then introduce a sim-
ple parameterization of the positions on the con-
text windows, and we show that it improves the

performance of the embeddings in some word sim-
ilarity and sentiment classification tasks. 1

2 Notation

Using a vocabulary V of words wi for i =
1, . . . , |V|, we seek word embeddings vi ∈
Rd by collecting statistics from a corpus C =
{w1, . . . , wt, . . . , wN}. We will interchangeably
mix subscripts i and t of words and embeddings to
index the vocabulary and corpus respectively.

3 Random Indexing as Convolution

Random Indexing (RI) (Kanerva et al., 2000; Kan-
erva, 2009) is a distributional semantic model that
updates the embedding vectors v∗ in an online
fashion by summing the sparse random vectors
e∗ that represent the context items (these vectors
are called random index vectors and act as unique
identifiers for the context items, words in this
case):

vt ← vt +
k∑

l=−k
l 6=0

h(wt+l)et+l (1)

k is the context window size and h(w) some
weight that quantifies the importance of the con-
text item (the standard setting is h(w) = 1 for all
w). Here et+l is the random index vector to corpus
item wt+l.

Equation (1) describes the update rule of RI and
the final embeddings vi can be expressed as:

vi =
k∑

l=−k
l 6=0

N∑
t=1

wt=wi

h(wt+l)et+l (2)

To establish the equivalence between RI and
convolution, we can reformulate the update rule

1The code is available at: http://github.com/
TobiasNorlund/Attention_RI

166



in Equation (1) as follows; let h ∈ R(2k+1)×d be a
filter function where

hlj =

{
1, ∀(l, j) ∈ {(0, d2), . . . , (k − 1, d2), (k + 1, d2), . . . , (2k, d2)}
0, else

Furthermore, let S ∈ RN×d be a matrix of stacked
sparse random vectors et. Now, if we use h and
S, we can rewrite the second term of the random
indexing update rule in Equation (1):

Ztv =
2k∑
l=0

d∑
j=0

(et−l+k)v−j+dhlj . (3)

Equation (3) is a 2D discrete convolution between
S and h, hence:

Ztv = (S ∗ h)[t, v] =
∑2k

l=0

∑d
j=0(et−l+k)v−j+dhlj .

(4)
Because h has been defined with zeros everywhere
except for column d2 , Equation (4) can been seen
as a 1D convolution over each column vector in S,

Ztv = (STv ∗ hTd
2

)[t] =
2k∑
l=0

(et−l+k)vht d
2
. (5)

4 Dealing with Redundant Features

Since word embeddings (produced by RI or some
other distributional model) are constructed unsu-
pervised by collecting co-occurrence information
from a large corpus, it is likely that the resulting
embeddings are very general, which may lower
the expressiveness of the embeddings if they are
going to be used in a very specific domain. Take
the example of training a text categorization clas-
sifier within a financial context; corpus occur-
rences of the words “bank” and “stock” in the
senses of LARGE COLLECTION and INVENTORY
will likely not provide useful information for the
embeddings in this domain. In word embeddings,
different senses are represented by co-occurrences
with different context items (Cuba Gyllensten and
Sahlgren, 2015). We refer to context items that are
less useful for a specific task as redundant features
of the embeddings.

Unfortunately, it is not, in the general case, pos-
sible to know a priori which context items will
be useful to construct embeddings for a particu-
lar task. Such context (i.e. feature) selection in-
stead needs to be performed jointly with training
the classifier. When backpropagation is used as
optimization strategy of the classifier, one can also

treat the word embeddings as parameters to up-
date. It is straightforward to take the derivatives of
the objective function with respect to the input and
apply Stochastic Gradient Descent (SGD) updates
just as for the model parameters. This strategy is
well known (Zhang and Wallace, 2015), and will
be referred to as SGD Random Indexing (SGD-
RI).

5 Parameterization of context window

Another strategy is to parameterize the word em-
beddings, and to optimize those parameters jointly
with the task using backpropagation. The RI al-
gorithm, as defined in (Sahlgren et al., 2016),
weights the importance of context items based on
their relative frequency according to Equation (6):

h(wt) = exp
(
−c · f(wt)|V|

)
(6)

where c is a constant, f(wt) is the corpus fre-
quency of itemwt, and |V| is the size of the vocab-
ulary (i.e. the number of unique words seen thus
far).

We would however like to parameterize context
items not only depending on relative frequency
but also on their usefulness for the specific task at
hand. To describe the suggested parametrization,
recall the Random Indexing algorithm in Equation
(1) where we look at each word and its context in
the corpus in a streaming fashion, and construct
embedding vectors by summing the index vectors
of all words occurring in the context. A fairly ob-
vious refinement of this algorithm would be to pa-
rameterize the relative positions within the context
window depending on their usefulness for the task
at hand. Equation (7) formalizes the parameter-
ization by introducing an additional factor to the
weighting scheme:

h(wt+l) = θwtl exp
(
−c · f(wt)|V|

)
(7)

Inserting this parameterization into the update
rule in Equation (1), we get:

vt ← vt +
k∑

l=−k
l 6=0

θwtl exp
(
−c · f(wt+l)|V|

)
et+l

(8)
which is equivalent to:

167



vi =
k∑

l=−k
l 6=0

N∑
t=1

wt=wi

θwtl exp
(
−c · f(wt+l)|V|

)
et+l

(9)
By careful inspection, the θwtl can be moved

outside the inner sum, while swapping the sub-
script to i since wt = wi:

vi =
k∑

l=−k
l 6=0

θwil

N∑
t=1

wt=wi

exp
(
−c · f(wt+l)|V|

)
et+l

︸ ︷︷ ︸
ṽli

(10)
The rewrite now allows the inner sum to be cal-

culated before fitting the θwil s which makes the
algorithm much more efficient. In practice, this
means we aggregate an embedding vector ṽli for
each relative window position l, for each word wi.
Stacking these 2k context vectors into a matrix Vi
and collecting the θwil s in a vector yields:

Vi =
[
ṽ−ki . . . ṽ

−1
i ṽ

+1
i . . . ṽ

+k
i

]
(11)

θi =



θwi−k
...
θwi−1
θwi+1

...
θwi+k


(12)

Equation (10) can now be rewritten as a matrix
vector multiplication:

vi = Viθi. (13)

In other words, this suggests instead of aggre-
gating embedding vectors vi according to (9), to
aggregate matrices Vi upon parsing the corpus.
The embedding vectors are then calculated as a
multiplication with a parameter vector θi accord-
ing to (13). Note that when θi = 1 you recover
the vanilla Random Indexing embeddings.

We will refer to this strategy as Parameterized
Random Indexing (PAR-RI).

6 Example: Word Similarity

To exemplify the effectiveness of the proposed pa-
rameterization, we use the SimLex-999 (Hill et al.,

2015) test in order to see how much the Spearman
rank correlation can be improved by fitting the θis
such that cosine similarity between the embedding
vectors correspond to the similarity ratings. For-
mally, we seek to minimize the following objec-
tive function:

min
θ∗

∑
(wi,wj)∈S

1
2

(cosαij − s(wi, wj))2︸ ︷︷ ︸
f(wi,wj)

(14)

where (wi, wj) ∈ S corresponds to each word
pair in SimLex. s(wi, wj) is the SimLex similar-
ity score for the word pair (scaled to [0, 1]) and
cosαij is the cosine similarity between the word’s
corresponding vectors:

αij =
vTi vj

‖vi‖2‖vj‖2 (15)

where vi and vj are wi and wj’s corresponding
word vectors, calculated as in equation (13). Since
this is a non-convex problem, SGD is applied as
optimization strategy. Calculating the gradient of
f with respect to θi and θj is straightforward:

δf

δθi
= (cosαij − s(wi, wj)) δ cosαij

δθi
(16)

δf

δθj
= (cosαij − s(wi, wj)) δ cosαij

δθj
. (17)

Applying the chain rule, the gradient of cosαij
becomes:

δ cosαij
δθi

=
δ cosαij
δvi

δvi
δθi

δ cosαij
δvi

=
vj‖vi‖2 − vi v

T
i vj
‖vi‖2

‖vi‖22‖vj‖2
δvi
δθi

= Vi.

(18)

The expression for δ cosαijδθj is the same, but with
the subscripts interchanged. We now apply SGD
to optimize the θis iteratively using the following
update rules:

θ
(t+1)
i = θ

t
i − η

δf

δθi

θ
(t+1)
j = θ

t
j − η

δf

δθj
.

(19)

168



This procedure is performed using V∗ matrices
generated from a dump of Wikipedia with the Ran-
dom Indexing hyper-parameters listed in Table 1.
The θis are initialized to one-vectors (θ∗ = 1) and
updated according to equation (19) with a learning
rate η = 1.0 until convergence.

Table 1: Hyper-parameters for PAR-RI.
Parameter Value Description
d 2,000 Dimensionality
k 10 Window size
c 60 Constant in frequency weight

� 10 Non-zero elements in index vectorsrandomly drawn from {−1,+1}

The results are summarized in Table 2. We can
see that the Spearman correlation is drastically im-
proved with the optimized θis. This experiment
can be seen as, for each word wi, finding a linear
combination in the column space of Vi that opti-
mizes the cosine similarity of the word vectors to
match the SimLex similarity scores. It is remark-
able that optimizing the θis in the relatively small
20-dimensional (R2k) subspaces of the full word
space (R2000) yields such a big improvement.

Table 2: Results of the SimLex experiment.
Avg. error Spearman

Initial θis (θ∗ = 1) 0.28 0.21
Optimized θis 0.19 0.62

7 Example: Sentiment Classification

The improvements reported in the previous sec-
tion should motivate the parameterization to be vi-
able for improving the performance in text clas-
sification as well. In this section, we parameter-
ize the embeddings for sentiment classification us-
ing two standard benchmarks; the Pang and Lee
Sentence Polarity Dataset v1.0 (PL05) (Pang and
Lee, 2005) and the Stanford Sentiment Treebank
(SST) (Socher et al., 2013). The PL05 data con-
sists of 10,662 short movie reviews that are clas-
sified as either positive or negative. Experiments
using this dataset are split into 25% test and 75%
train/validation sets and evaluated by 5-fold cross
validation on the training/validation set. We make
two consecutive runs, in total 10 trainings, and re-
port their maximum, minimum and mean accuracy
as well as their standard deviation. The SST data
is an extension of PL05 with train/validation/test
splits provided. The dataset also provides fine-
grained labels (very positive, positive, neutral,

negative, very negative). In this study we have
however omitted the neutral labels and treated it
as a binary classification problem by merging the
very positive, positive, very negative and negative
classes into two. We report the maximum, mini-
mum and mean accuracy as well as the standard
deviation of 10 consecutive runs using the pro-
vided train/val/test splits.

We use two different classifiers in these exper-
iments. The first is a standard neural network
(referred to as MLP for Multi-Layer Perceptron)
(Rumelhart et al., 1986) with one hidden layer of
120 nodes with sigmoid activations and one sig-
moid output unit. All word vectors are normal-
ized to an l2 norm of 1 and naively summed to
produce document vectors. The weights in the
neural network are also l2 regularized with a con-
stant factor of λ = 0.001. The second classifier
is the model proposed by Kim (2014) which im-
plements a Convolutional Neural Network (CNN).
The hyper-parameters used are listed in table 3.
Like the MLP model, the word embeddings are
also normalized to unit length.

Table 3: Hyper-parameters for CNN.
Parameter Value Description

n 300 Number of filters,100 of height 3,4,5 respectively
p 0.5 Dropout rate
s 3 Filter max l2-norm

As comparison with the different RI-based em-
beddings (RI, SGD-RI, and PAR-RI), we also
include results using embeddings produced with
SGNS (Mikolov et al., 2013) and GloVe (Penning-
ton et al., 2014), both with 300-dimensional vec-
tors and a window size of 2. We also include re-
sults using embeddings randomly sampled from
a uniform U(−0.25, 0.25) distribution (RAND).
All word embeddings (except for the RAND vec-
tors) are pre-trained (unsupervised) on a dump of
Wikipedia. We list all hyper-parameters for RI,
SGNS and GloVe in table 4, 5 and 6

Table 4: Hyper-parameters for RI
Parameter Value Description
d 2,000 Dimensionality
k 2 Window size
c 60 Constant

� 10
# non-zero elements in index
vectors randomly drawn from
{−1,+1}

The results of all experiments are shown in ta-

169



Table 5: Hyper-parameters for SGNS
Parameter Value Description
d 300 Dimensionality
k 2 Window size

negative 5 Number of negative samplesper positive
down sampling no No down sampling is applied
α 0.025 Initial learning rate
iter 5 Number of iterations

Table 6: Hyper-parameters for GloVe
Parameter Value Description
d 300 Dimensionality
k 2 Window size
iter 15 Number of training iterations
x-max 10 Cutoff in weighting function

α 0.75 Constant in exponent of weightingfunction
η 0.05 Initial learning rate

ble 7 (next page).2 Comparing the various em-
beddings, it is obvious that the performance dif-
ferences are very small, and thus not likely to be
of any significant practical importance. This is es-
pecially true for the MLP experiments where the
variances reach over two points in many cases. On
the other hand, all embeddings outperform the ran-
domized RAND vectors, which demonstrates that
classification performance is improved when the
model can take semantic information into account.
The best performing embedding for the MLP clas-
sifier is PAR-RI, while SGNS performs better us-
ing the CNN model. The GloVe embeddings, de-
spite their theoretical similarities to the SGNS em-
beddings (Suzuki and Nagata, 2015), consistently
underperform both in comparison with SGNS and
PAR-RI (and, in the case of the MLP classifier,
also the SGD-RI embeddings). This is in contrast
to the experiments performed by (Zhang and Wal-
lace, 2015) where the difference was minor.

Comparing the PAR-RI embeddings with SGD-
RI and standard RI, it seems PAR-RI performs
well, with the highest mean accuracy on the SST
dataset, using the MLP model. SGD-RI improves
the results compared to the standard RI embed-
dings for the MLP model, but not for the CNN
model. Updating of the SGNS embeddings just
like SGD-RI for the CNN have also been studied
in Zhang and Wallace (2015), who report a perfor-
mance boost of about ∼0.8%. This also contrasts
to our results with SGD-RI using the CNN model,

2Since our focus in this paper is the effect of the word
embeddings, we will not comment further on the performance
differences between the MLP and CNN classifiers.

which instead decrease the performance compared
to standard RI. This could be due to the RI embed-
dings being more high dimensional than SGNS,
yielding a larger and harder parameter space to op-
timize.

Comparing our results to other reported results
in the literature, Kim (2014) and Zhang and Wal-
lace (2015) manage to push the boundaries up to
80.10 for the PL05 data, and up to 84.88 for the
SST data using SGNS embeddings pre-trained on
a much larger 100 billion tokens Google News
dataset. We believe this somewhat increased per-
formance is partly due to the bigger dataset. An-
other factor could also be that the language style
in news articles is more similar to the movie re-
views compared to Wikipedia, arguably yielding
better-suited embeddings.

8 Optimized Context Profiles

When the PAR-RI parametrization was proposed,
the hypothesis was that certain relative positions
in the context windows would be more important
in describing the context of a word than others.
The results in the two previous sections demon-
strate that the proposed parameterization is able to
improve the embeddings in both a word similarity
task, and (to a lesser extent) a sentiment classifi-
cation task. This indicates that the parameteriza-
tion is actually able to find useful context profiles
for terms used in the various test settings. In this
section, we exemplify the kinds of context profiles
learned when trained for the sentiment classifica-
tion task.

Figure 1 (on page 7) shows the learned weights
per context window position for four different ad-
jectives (top row), four different determiners (mid-
dle row), and four different nouns (bottom row).
The parameterization obviously has a larger effect
for some words than for others; as an example, the
windows for “good” and “bad” is much more pa-
rameterized than the windows for “reliable” and
“positive”, and the windows for the determiners
are in general much more parameterized than the
windows for nouns. It is interesting to note that
there is a small tendency that the windows for the
adjectives have a higher weight in the +1 posi-
tion, which is consistent with a linguistic analysis
of adjectives as qualifiers of succeeding nouns. By
contrast, the window positions for the determiners
seem to have a higher weight in the positions just
preceding the focus word, while the windows for

170



Table 7: Experiment results. Mean accuracies in %. The parentheses contain the maximum achieved
accuracy, the standard deviation and the minimum achieved accuracy of the consecutive runs

Emb + Model PL05 SST
RAND MLP 68.23 (↑ 69.58, ±0.98, ↓ 66.32) 69.89 (↑ 71.11, ±0.83, ↓ 68.64)
RI MLP 72.45 (↑ 74.91, ±2.99, ↓ 66.54) 75.13 (↑ 77.98, ±2.54, ↓ 73.75)
SGD-RI MLP 73.62 (↑ 74.16, ±0.77, ↓ 71.42) 77.91 (↑ 78.80, ±0.82, ↓ 76.11)
ATT-RI MLP 72.45 (↑ 74.83, ±2.20, ↓ 68.90) 78.03 (↑ 79.63, ±1.60, ↓ 74.46)
SGNS MLP 73.84 (↑ 74.76, ±1.14, ↓ 71.57) 77.27 (↑ 79.57, ±3.13, ↓ 70.02)
GLOVE MLP 71.29 (↑ 73.67, ±1.67, ↓ 68.60) 76.26 (↑ 77.32, ±1.66, ↓ 71.61)
RAND CNN 72.12 (↑ 72.91, ±0.50, ↓ 71.28) 76.99 (↑ 77.94, ±0.76, ↓ 75.50)
RI CNN 76.18 (↑ 76.60, ±0.35, ↓ 75.51) 81.83 (↑ 82.72, ±0.39, ↓ 81.39)
SGD-RI CNN 75.67 (↑ 76.26, ±0.63, ↓ 74.22) 81.31 (↑ 81.89, ±0.38, ↓ 80.78)
ATT-RI CNN 77.55 (↑ 78.08, ±0.31, ↓ 77.09) 81.77 (↑ 82.64, ±0.60, ↓ 80.66)
SGNS CNN 77.92 (↑ 78.34, ±0.24, ↓ 77.55) 83.44 (↑ 84.00, ±0.51, ↓ 82.00)
GLOVE CNN 77.35 (↑ 77.77, ±0.34, ↓ 76.79) 81.56 (↑ 82.06, ±0.34, ↓ 80.78)

nouns seem to have very small parameterization.
It thus seems as if the parameterization is able to
learn slightly different window profiles for differ-
ent parts of speech.

As noted in the introduction, is is common prac-
tice in distributional semantics to weight the con-
text windows by the distance to the focus word. If
this is an optimal strategy, we should see a bell-
like curve leaning to zero at the edges. Such a
shape is partially present for some of the words,
for example in “and”, “of”, “good” and “bad”, but
for most words, the weights are almost unchanged.
We believe this could be due to the vanishing gra-
dient problem where the gradient seems to vanish
deeper down the model. In addition, the less com-
mon the word is in the training set, the less it is
updated. Another interesting aspect of the learned
weights is that by inspecting the l1 norm of the
weight vectors, we get a hint of the words’ relative
importance for the given task. We can see that the
l1 norm for the words “good” and “bad” are larger
than for “the” and “of”, which feels natural for the
sentiment classification task.

9 Conclusion

This paper has introduced a simple parameteriza-
tion for the RI framework, which has also been de-
rived in terms of convolution. It parameterizes the
positions in the context windows and optimizes
with respect to the performance of the embeddings
in some given task, such as word similarity or text
classification. Our experiments show that the pro-
posed PAR-RI model is able to improve the per-
formance of the embeddings in many cases, and

that the results are competitive in comparison with
other well-known embeddings. The idea of pa-
rameterizing the window positions could also be
applied to other distributional semantic models,
such as SGNS.

We note that all embeddings used in the senti-
ment classification task produce very similar re-
sults. This indicates that in practice, the word em-
beddings included in this paper are more or less
equivalent. It is therefore doubtful whether it is
possible to draw any conclusions based on these
results regarding the question whether any single
embedding is superior to the others in the general
case.

The examples of context profiles provided as
examples of the parameterization shows some in-
teresting effects. However, training the position-
dependent weights is non-trivial, and one could
probably think of better initializations of the
weights than just one-vectors, for example using
a bell-like shape. The vanishing gradient problem
would however remain, and the weights for un-
common words will not change significantly.

The conclusion of the experiments using SGD-
RI is that updating the embeddings jointly with the
classification model using SGD does not necessar-
ily improve generalization. This is in fact not so
strange. Moving around only a subset of the words
(i.e. the words present in the training set), while
leaving the rest untouched produces an inconsis-
tent space with undefined distributional properties
between updated and non-updated embeddings. It
could therefore be an idea to use randomized em-
beddings for all words not present in the training

171



10 5 0 5 10
Window position

0.0

0.5

1.0

1.5

2.0
W

e
ig

h
t

good

10 5 0 5 10
Window position

0.0

0.5

1.0

1.5

2.0

W
e
ig

h
t

reliable

10 5 0 5 10
Window position

0.0

0.5

1.0

1.5

2.0

W
e
ig

h
t

positive

10 5 0 5 10
Window position

0.0

0.5

1.0

1.5

2.0

W
e
ig

h
t

bad

10 5 0 5 10
Window position

0.0

0.5

1.0

1.5

2.0

W
e
ig

h
t

the

10 5 0 5 10
Window position

0.0

0.5

1.0

1.5

2.0

W
e
ig

h
t

and

10 5 0 5 10
Window position

0.0

0.5

1.0

1.5

2.0

W
e
ig

h
t

of

10 5 0 5 10
Window position

0.0

0.5

1.0

1.5

2.0

W
e
ig

h
t

but

10 5 0 5 10
Window position

0.0

0.5

1.0

1.5

2.0

W
e
ig

h
t

tolkien

10 5 0 5 10
Window position

0.0

0.5

1.0

1.5

2.0

W
e
ig

h
t

show

10 5 0 5 10
Window position

0.0

0.5

1.0

1.5

2.0

W
e
ig

h
t

tv

10 5 0 5 10
Window position

0.0

0.5

1.0

1.5

2.0

W
e
ig

h
t

warfare

Figure 1: The learned weights for four adjectives (top row), four determiners (middle row), and four
nouns (bottom row).

set because they then can be regarded as approx-
imately orthogonal, and thus should not interfere
with the semantic structure.

References

Amaru Cuba Gyllensten and Magnus Sahlgren. 2015.
Navigating the semantic horizon using relative
neighborhood graphs. In Proceedings of EMNLP,
pages 2451–2460.

Felix Hill, Roi Reichart, and Anna Korhonen. 2015.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Computational Linguis-
tics, 41(4):665–695.

Pentti Kanerva, Jan Kristofersson, and Anders Holst.
2000. Random Indexing of text samples for Latent
Semantic Analysis. In Proceedings of CogSci, page
1036.

Pentti Kanerva. 2009. Hyperdimensional computing.
Cognitive Computation, 1(2):139–159.

Yoon Kim. 2014. Convolutional neural networks
for sentence classification. In Proceedings of the
2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 1746–
1751, Doha, Qatar, October. Association for Com-
putational Linguistics.

Kevin Lund, Curt Burgess, and Ruth A. Atchley.
1995. Semantic and associative priming in high-
dimensional semantic space. In Proceedings of the
17th Annual Conference of the Cognitive Science
Society, pages 660–665. Hillsdale, NJ: Erlbaum.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

Yoshiki Niwa and Yoshihiko Nitta. 1994. Co-
occurrence vectors from corpora vs. distance vec-
tors from dictionaries. In Proceedings of the 15th
Conference on Computational Linguistics - Volume

172



1, COLING ’94, pages 304–309, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of ACL,
pages 115–124.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vec-
tors for word representation. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2014), pages 1532–
1543.

David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Parallel distributed processing:
Explorations in the microstructure of cognition, vol.
1. chapter Learning Internal Representations by Er-
ror Propagation, pages 318–362. MIT Press, Cam-
bridge, MA, USA.

Magnus Sahlgren, Amaru Cuba Gyllensten, Fredrik
Espinoza, Ola Hamfors, Anders Holst, Jussi Karl-
gren, Fredrik Olsson, Per Persson, and Akshay
Viswanathan. 2016. The Gavagai Living Lexicon.
In Proceedings of LREC.

Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the conference on
empirical methods in natural language processing
(EMNLP), volume 1631, page 1642. Citeseer.

Jun Suzuki and Masaaki Nagata. 2015. A unified
learning framework of skip-grams and global vec-
tors. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics and
the 7th International Joint Conference on Natural
Language Processing (Volume 2: Short Papers),
pages 186–191, Beijing, China, July. Association for
Computational Linguistics.

Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141–188, January.

Ye Zhang and Byron Wallace. 2015. A sensitiv-
ity analysis of (and practitioners’ guide to) convo-
lutional neural networks for sentence classification.
CoRR, abs/1510.03820.

173


