



















































Dynamic Entity Representation with Max-pooling Improves Machine Reading


Proceedings of NAACL-HLT 2016, pages 850–855,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Dynamic Entity Representation with Max-pooling Improves
Machine Reading

Sosuke Kobayashi and Ran Tian and Naoaki Okazaki and Kentaro Inui
Tohoku University, Japan

{sosuke.k, tianran, okazaki, inui}@ecei.tohoku.ac.jp

Abstract

We propose a novel neural network model for
machine reading, DER Network, which ex-
plicitly implements a reader building dynamic
meaning representations for entities by gath-
ering and accumulating information around
the entities as it reads a document. Eval-
uated on a recent large scale dataset (Her-
mann et al., 2015), our model exhibits bet-
ter results than previous research, and we
find that max-pooling is suited for model-
ing the accumulation of information on enti-
ties. Further analysis suggests that our model
can put together multiple pieces of informa-
tion encoded in different sentences to an-
swer complicated questions. Our code for the
model is available at https://github.
com/soskek/der-network

1 Introduction

Machine reading systems (Poon et al., 2010;
Richardson et al., 2013) can be tested on their
ability to answer queries about contents of doc-
uments that they read, thus a central problem is
how the information of documents should be orga-
nized in the system and retrieved by the queries.
Recently, large scale datasets of document-query-
answer triples have been constructed from online
newspaper articles and their summaries (Hermann
et al., 2015), by replacing named entities in the
summaries with placeholders to form Cloze (Tay-
lor, 1953) style questions (Figure 1). These datasets
have enabled training and testing of complicated
neural network models of hypothesized machine
readers (Hermann et al., 2015; Hill et al., 2015).

( @entity1 ) @entity0 may be @entity2 in the popular 
@entity4 superhero films , but he recently dealt in some 

advanced bionic technology himself . @entity0 recently 
presented a robotic arm to young @entity7 , a @entity8 boy 

who is missing his right arm from just above his elbow . the 
arm was made by @entity12 , a … !

" [X] " star @entity0 presents a young child with a bionic 

arm!

Query !

Context !

(CNN)Robert Downey Jr. may be Iron Man in the popular 
Marvel superhero films, but he recently dealt in some 

advanced bionic technology himself. Downey recently 
presented a robotic arm to young Alex Pring, a Central 

Florida boy who is missing his right arm from just above his 
elbow. The arm was made by Limbitless Solutions, a …!

"Iron Man" star Robert Downey Jr. presents a young child 

with a bionic arm!

Raw Highlight !

Raw Article !

Answer      @entity2 !

Figure 1: A document-query-answer triple con-
structed from a news article and its bullet point sum-
mary. An entity in the summary (Robert Downey Jr.)
is replaced by the placeholder [X] to form a query.
All entities are anonymized to exclude world knowl-
edge and focus on reading comprehension.

In this paper, we hypothesize that a reader without
world knowledge can only understand a named en-
tity by dynamically constructing its meaning from
the contexts. For example, in Figure 1, a reader
reading the sentence “Robert Downey Jr. may be
Iron Man . . . ” can only understand “Robert Downey
Jr.” as something that “may be Iron Man” at this
stage, given that it does not know Robert Downey
Jr. a priori. Information about this entity can only

850



be accumulated by its subsequent occurrence, such
as “Downey recently presented a robotic arm . . . ”.
Thus, named entities basically serve as anchors to
link multiple pieces of information encoded in dif-
ferent sentences. This insight has been reflected
by the anonymization process in construction of the
dataset, in which coreferent entities (e.g. “Robert
Downey Jr.” and “Downey”) are replaced by ran-
domly permuted abstract entity markers (e.g. “@en-
tity0”), in order to prevent additional world knowl-
edge from being attached to the surface form of the
entities (Hermann et al., 2015). We, however, take it
as a strong motivation to implement a reader that dy-
namically builds meaning representations for each
entity, by gathering and accumulating information
on that entity as it reads a document (Section 2).

Evaluation of our model, DER Network, exhibits
better results than previous research (Section 3). In
particular, we find that max-pooling of entity rep-
resentations, which is intended to model the accu-
mulation of information on entities, can drastically
improve performance. Further analysis suggests
that max-pooling can help our model draw multiple
pieces of information from different sentences.

2 Model

Following Hermann et al. (2015), our model esti-
mates the conditional probability p(e|D, q), where q
is a query and D is a document. A candidate answer
for the query is denoted by e, which in this paper is
any named entity. Our model can be factorized as:

p(e|D, q) ∝ exp(v(e;D, q)Tu(q)) (1)

in which u(q) is the learned meaning for the query
and v(e;D, q) the dynamically constructed mean-
ing for an entity, depending on the document D and
the query q. We note that (1) is in contrast to the
factorization used by Hermann et al. (2015):

p(a|D, q) ∝ exp(v(a)Tu(D, q)) (2)

in which a vector u(D, q) is learned to represent
the status of a reader after reading a document and
a query, and this vector is used to retrieve an answer
by coupling with the answer vector v(a).1

1Hermann et al. (2015) models p(a|D, q) for every word to-
ken a in a document. While the approach could be more general

Factorization (2) relies on the hypothesis that
there exists a fixed vector for each candidate an-
swer representing its meaning. However, as we ar-
gued in Section 1, an entity surface does not possess
meaning; rather, it serves as an anchor to link pieces
of information about it. Therefore, we hypothesize
that the meaning representation v(e;D, q) of an en-
tity e should be dynamically constructed from its
surrounding contexts, and the meanings are “accu-
mulated” through the reader reading the document
D. We explain the construction of v(e;D, q) in
Section 2.1, and propose a max-pooling process for
modeling information accumulation in Section 2.2.

2.1 Dynamic Entity Representation

For any entity e, we take its context c as any sentence
that includes a token of e. Then, we use bidirectional
single-layer LSTMs (Hochreiter and Schmidhuber,
1997; Graves et al., 2005) to encode c into vectors.
LSTM is a neural cell that outputs a vector hc,t for
each token t in the sentence c; taking the word vector
xc,t of the token as input, each hc,t is calculated re-
currently from its precedent vector hc,t−1 or hc,t+1,
depending on the direction of the encoding. For-
mally, we write forward and backward LSTMs as:

~hc,t =
−−−−→
LSTM(xc,t, ~hc,t−1) (forward) (3)

~hc,t =
←−−−−
LSTM(xc,t, ~hc,t+1) (backward) (4)

Then, denoting the length of the sentence c as T and
the index of the entity e token as τ , we define the
dynamic entity representation de,c as the concatena-
tion of the vectors [~hc,T , ~hc,1, ~hc,τ , ~hc,τ ] encoded by
a feed-forward layer (Figure 2):

de,c = tanh(Whd[~hc,T , ~hc,1, ~hc,τ , ~hc,τ ]+bd)

in which Whd and bd respectively stand for the
learned weight matrix and bias vector of that feed-
forward layer. Index hd denotes thatWhd is a matrix
mapping h-vectors to d-vectors. Index d shows that
bd has the same dimension as d-vectors. We use this
convention throughout this paper.

Having de,c as the dynamic representation of an
entity e occurring in context c, we define vector

because it has the potential to answer other types of questions
given appropriate training data, our approach is arguably suit-
able for the specific task and natural for testing our hypothesis.

851



[bos] @ent0  may    be    ...   [eos] !

!hc,τ

!hc,τ
!hc,1

!hc,T

xc,1 xc,2 xc,3 xc,4 xc,T

de0,c

=
−−−−→
LSTM←−−−−

=
−−−−→
LSTM(

=
←−−−−
LSTM(

Figure 2: Dynamic entity representa-
tion de,c encodes LSTM outputs, mod-
eling surrounding context.

... know something about                    , accused in a string of shootings ... !

                                                       used to have tatoos indicating … !

On Thursday morning ,                   made his first court appearance ...!

de,1
de,2
de,3

dx max-pooling
c′≺c

(
Figure 3: Max-pooling takes the max value of each dimension of
dynamic entity representations, modeling accumulation of con-
text information. It is then fed to xc,τ as input to LSTMs.

v(e;D, q) for each entity as a weighted sum 2:

v(e;D, q) = Wdv
[∑
c∈D

se,c(q)de,c
]
+ bv (5)

in which se,c(q) is calculated by the attention mech-
anism (Bahdanau et al., 2015), modeling the degree
to which our reader should attend to a particular oc-
currence of an entity, given the query q. More pre-
cisely, se,c(q) is defined as the following:

se,c(q) =
exp(s′e,c(q))∑
c′ exp(s

′
e,c′(q))

(6)

s′e,c′(q) = m
T tanh(Wdmde,c′ + q) + bs (7)

where se,c(q) is calculated by taking the softmax of
s′e,c′(q), which is calculated from the dynamic entity
representation de,c′ and the query vector q. The vec-
tor m, matrixWdm, and the bias bs in (7) are learned
parameters in the attention mechanism. Vector m is
used here to map a vector value to a scalar.

The query vector3 u(q) is constructed similarly as
dynamic entity representations, using bidirectional
LSTMs4 to encode the query and then encoding the
output vectors. More precisely, if we denote the
length of the query as T and the index of the place-
holder as τ , the query vector is calculated as:

u(q) = Whq[~hq,T , ~hq,1, ~hq,τ , ~hq,τ ]+bq (8)

Then, v(e;D, q) and u(q) are used in (1) to calcu-
late probability p(e|D, q).

2Following a heuristic used in Hill et al. (2015), we add a
secondary bias b′v to v(e; D, q) if the entity e already appears
in the query q.

3u(q) and another query vector q, are calculated respec-
tively, in the same way (8) with unshared model parameters,
while sharing the parameters is also promising.

4The parameters of the bi-LSTM for queries are not shared
with the ones for entity contexts.

2.2 Max-pooling

We expect the dynamic entity representation to cap-
ture information about an entity mentioned in a sen-
tence. However, as an entity occurs multiple times
in a document, information is accumulated as sub-
sequent occurrences of the entity draw information
from previous mentions. For example, in Figure 1,
the first sentence mentioning “Robert Downey Jr.”
relates Downey to Iron Man, whereas a subsequent
mention of “Downey” also relates him to a robotic
arm. Both of the two pieces of information are
necessary to answer the query “Iron Man star [X]
presents . . . with a bionic arm”. Therefore, the dy-
namic entity representations as constructed individ-
ually from single sentences may not provide enough
information for our reader model. We thus propose
the use of max-pooling to model information accu-
mulation of dynamic entity representations.

More precisely, for each entity e, max-pooling
takes the max value of each dimension of the vec-
tors de,c′ from all preceding contexts c′ (Figure 3).
Then, in a subsequent sentence c where the entity
occurs again at index τ , we use the vector

xc,τ = Wdx max-pooling
c′≺c

(de,c′) + bx

as input for the LSTMs in (3) and (4) for encod-
ing the context. This vector xc,τ draws informa-
tion from preceding contexts, and is regarded as the
meaning of the entity e that the reader understands
so far, before reading the sentence c. It is used in
place of a vector previously randomly initialized as
a notion of e, in the construction of the new dynamic
entity representation de,c.

852



3 Evaluation

We use the CNN-QA dataset (Hermann et al., 2015)
for evaluating our model’s ability to answer ques-
tions about named entities. The dataset consists
of (D, q, e)-triples, where the document D is taken
from online news articles, and the query q is formed
by hiding a named entity e in a summarizing bullet
point of the document (Figure 1). The training set
has 90k articles and 380k queries, and both valida-
tion and test sets have 1k articles and 3k queries. An
average article has about 25 entities and 700 word
tokens. One trains a machine reading system on the
data by maximizing likelihood of correct answers.
We use Chainer5 (Tokui et al., 2015) to implement
our model6.

Experimental Settings Named entities in CNN-
QA are already recognized. For preprocessing, we
segment sentences at punctuation marks “.”, “!”, and
“?”.7 We train our model8 with hyper-parameters
lightly tuned on the validation set9, and we conduct
ablation test on several techniques that improve our
basic model.

Results As shown in Table 1, Max-pooling de-
scribed in Section 2.2 drastically improves perfor-
mance, showing the effect of accumulating informa-
tion on entities. Another technique, called “Byway”,
is based on the observation that the attention mech-
anism (5) must always promote some entity occur-
rences (since all weights sum to 1), which could be
difficult if the entity does not answer the query. To
counter this, we make an artificial occurrence for
each entity with no contexts, which serves as a by-
way to attend when no other occurrences can be rea-
sonably related to the query. This simple trick shows

5http://chainer.org/
6The implementation is available at https://github.

com/soskek/der-network.
7Text in CNN-QA are tokenized without any sentence seg-

mentations.
8Training process takes roughly a week (3-5 passes of the

training data) on a 6-core 2.4GHz Xeon CPU.
9Vector dimension: 300, Dropout: 0.3, Batch: 50, Optimiza-

tion: RMSProp with momentum (Tieleman and Hinton, 2012;
Graves, 2013) (momentum: 0.9, decay: 0.95), Learning rate:
1e-4 divided by 2.0 per epoch, Gradient clipping factor: 10. We
initialize word vectors by uniform distribution [-0.05, 0.05], and
other matrix parameters by Gaussians of mean 0 and variance
2/(# rows + # columns).

Models Valid Test
Basic Proposed Model (Basic) 0.614 0.623
Basic + Max-pooling 0.712 0.707
Basic + Byway 0.691 0.706
Basic + Byway, Max-pooling (Full) 0.708 0.720
Full + w2v-initialization 0.713 0.729
Deep LSTMs∗ 0.550 0.570
Attentive Reader∗ 0.616 0.630
Impatient Reader∗ 0.618 0.638
Memory Networks∗∗ 0.635 0.684
+ Ensemble (11 models)∗∗ 0.662 0.694

Table 1: Accuracy on CNN-QA dataset. Results
marked by ∗ are cited from Hermann et al. (2015)
and ∗∗ from Hill et al. (2015).

( @entity1 ) @entity0 may be @entity2 in the popular @entity4 
superhero films , but he recently dealt in some advanced bionic 
technology himself . !

…!

@entity7 received his robotic arm in the summer , then later had it 
upgraded to resemble a " @entity26 " arm .!

this past saturday , @entity7  received an even more impressive gift , 
from " @entity2 " himself . !

…!

the actor showed the child two arms , one from @entity0 's movies 
and one for @entity7 : a real , working robotic @entity2 arm . !

…!

" [X] " star @entity0 presents a young child with a bionic arm !e2   e2  / e7!
Max    Basic!

.58 !
!

!

!

!

!

.31 !
!

!

.11 !

!

!

!

!

1.00!
!

.75 !
!

!

!

!

!

.25 !
!

!

.00 !

Figure 4: A correct answer found by max-pooling.
Attention to each entity occurrence shown on left.

clear effects, suggesting that the attention mecha-
nism plays a key role in our model. Combining these
two techniques helps more. Further, we note that
initializing our model with pre-trained word vec-
tors10 is helpful, though world knowledge of enti-
ties has been prevented by the anonymization pro-
cess. This suggests that pre-trained word vectors
may still bring extra linguistic knowledge encoded
in ordinary words. Finally, we note that our model,
full DER Network, shows the best results compared
to several previous reader models (Hermann et al.,
2015; Hill et al., 2015), endorsing our approach as
promising. The 99% confidence intervals of the re-
sults of full DER Network and the one initialized
by word2vec on the test set were [0.700, 0.740] and
[0.708, 0.749], respectively (measured by bootstrap
tests).

10We use GoogleNews vectors from http://code.
google.com/p/word2vec/ (Mikolov et al., 2013).

853



Analysis In the example shown in Figure 4, our
basic model missed by paying little attention to the
second and third sentences, probably because it does
not mention @entity0 (Downey). In contrast, max-
pooling of @entity2 (Iron Man) draws attention to
the second and third sentences because Iron Man is
said related to Downey in the first sentence. This
helps Iron Man surpass @entity26 (Transformers),
which is the name of a different movie series in
which robots appear but Downey doesn’t. Quanti-
tatively, in the 479 samples in test set correctly an-
swered by max-pooling but missed by basic model,
the average occurrences of answer entities (8.0) is
higher than the one (7.2) in the 1782 samples cor-
rectly answered by both models. This suggests that
max-pooling especially helps samples with more en-
tity mentions.

4 Discussion

It is actually a surprise for us that deep learning mod-
els, despite their vast amount of parameters, seem
able to learn as intended by the designers. This also
indicates a potential that additional linguistic intu-
itions modeled by deep learning methods can im-
prove performances, as in the other work using max-
pooling (LeCun et al., 1998; Socher et al., 2011; Le
et al., 2012; Collobert et al., 2011; Kalchbrenner et
al., 2014), attention (Bahdanau et al., 2015; Luong
et al., 2015; Xu et al., 2015; Rush et al., 2015), etc.
In this work, we have focused on modeling a reader
that dynamically builds meanings for entities. We
believe the methodology can be inspiring to other
problems as well.

Acknowledgments

This work was supported by CREST, JST and
JSPS KAKENHI Grant Number 15H01702 and
15H05318. We would like to thank members of Pre-
ferred Infrastructure, Inc. and Preferred Networks,
Inc. for useful discussions. We also thank the anony-
mous reviewers for comments on earlier version of
this paper.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly

learning to align and translate. In Proceedings of the
3rd International Conference on Learning Represen-
tations.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493–
2537.

Alex Graves, Santiago Fernández, and Jürgen Schmidhu-
ber. 2005. Bidirectional lstm networks for improved
phoneme classification and recognition. In Proceed-
ings of the 15th International Conference on Artificial
Neural Networks: Formal Models and Their Applica-
tions - Volume Part II, pages 799–804.

Alex Graves. 2013. Generating sequences with recurrent
neural networks. CoRR, abs/1308.0850.

Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-
stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. 2015. Teaching machines to read
and comprehend. In Advances in Neural Information
Processing Systems 28, pages 1684–1692.

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason We-
ston. 2015. The goldilocks principle: Reading chil-
dren’s books with explicit memory representations.
CoRR, abs/1511.02301.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9(8):1735–
1780.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for mod-
elling sentences. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 655–665.

Quoc Le, Marc’Aurelio Ranzato, Rajat Monga, Matthieu
Devin, Kai Chen, Greg Corrado, Jeff Dean, and An-
drew Ng. 2012. Building high-level features using
large scale unsupervised learning. In Proceedings of
the 29th International Conference on Machine Learn-
ing, pages 81–88.

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE,
86(11):2278–2324.

Thang Luong, Hieu Pham, and Christopher D. Manning.
2015. Effective approaches to attention-based neural
machine translation. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1412–1421.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed repre-
sentations of words and phrases and their composition-
ality. In Advances in Neural Information Processing
Systems 26, pages 3111–3119.

854



Hoifung Poon, Janara Christensen, Pedro Domingos,
Oren Etzioni, Raphael Hoffmann, Chloe Kiddon,
Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Ste-
fan Schoenmackers, Stephen Soderland, Dan Weld,
Fei Wu, and Congle Zhang. 2010. Machine read-
ing at the university of washington. In Proceedings of
the NAACL HLT 2010 First International Workshop on
Formalisms and Methodology for Learning by Read-
ing, pages 87–95.

Matthew Richardson, Christopher J.C. Burges, and Erin
Renshaw. 2013. MCTest: A challenge dataset for the
open-domain machine comprehension of text. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 193–203.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 379–389.

Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural Infor-
mation Processing Systems 24, pages 801–809.

Wilson L. Taylor. 1953. ”cloze procedure”: a new
tool for measuring readability. Journalism Quarterly,
30:415–433.

Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture
6.5 - msprop: Divide the gradient by a running av-
erage of its recent magnitude. COURSERA: Neural
Networks for Machine Learning.

Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clay-
ton. 2015. Chainer: a next-generation open source
framework for deep learning. In Proceedings of Work-
shop on Machine Learning Systems (LearningSys) in
The 29th Annual Conference on Neural Information
Processing Systems.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell: Neu-
ral image caption generation with visual attention. In
Proceedings of the 32nd International Conference on
Machine Learning, pages 2048–2057.

855


