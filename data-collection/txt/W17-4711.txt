



















































Biasing Attention-Based Recurrent Neural Networks Using External Alignment Information


Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 108–117
Copenhagen, Denmark, September 711, 2017. c©2017 Association for Computational Linguistics

Biasing Attention-Based Recurrent Neural
Networks Using External Alignment Information

Tamer Alkhouli and Hermann Ney
Human Language Technology and Pattern Recognition Group

Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany

<surname>@i6.informatik.rwth-aachen.de

Abstract

This work explores extending attention-
based neural models to include alignment
information as input. We modify the at-
tention component to have dependence
on the current source position. The at-
tention model is then used as a lexical
model together with an additional align-
ment model to generate translation. The
attention model is trained using external
alignment information, and it is applied
in decoding by performing beam search
over the lexical and alignment hypothe-
ses. The alignment model is used to score
these alignment candidates. We demon-
strate that the attention layer is capable
of using the alignment information to im-
prove over the baseline attention model
that uses no such alignments. Our experi-
ments are performed on two tasks: WMT
2016 English→Romanian and WMT 2017
German→English.

1 Introduction

Neural machine translation (NMT) has emerged
recently as a successful end-to-end statistical ma-
chine translation approach. The best performing
NMT systems use an attention mechanism that fo-
cuses the attention of the decoder on parts of the
source sentence (Bahdanau et al., 2015). The at-
tention component is computed as an intermedi-
ate part of the model, and is trained jointly with
the rest of the model. The approach is appeal-
ing because (1) it is end-to-end, where the neural
model is trained from scratch without assistance
from other trained models, and (2) the attention
component is trained jointly with the rest of the
model, requiring no pre-computed alignments.

In this work, we raise the question whether the

attention component is self-sufficient to attend to
the source side, and if it can still benefit from ex-
plicit dependence on the alignment information.
To this end, we modify the attention model to bias
the attention layer towards the alignment informa-
tion, and evaluate the model in a generative frame-
work consisting of two steps: alignment prediction
followed by lexical translation.

Two decades ago, (Vogel et al., 1996) applied
hidden Markov models to machine translation.
The idea was based on introducing word align-
ments as hidden variables, while using the first-
order Markov assumption to simplify the depen-
dencies of the alignment sequence. The approach
decomposed the translation process using a lexi-
cal model and an alignment model. These mod-
els were simple tables enumerating all possible
translation and alignment combinations. Nowa-
days, HMM is used with IBM models to gener-
ate word alignments, which are needed to train
phrase-based systems.

Alkhouli et al. (2016) and Wang et al. (2017)
apply the hidden Markov model decomposition
using feedforward lexical and alignment neural
network models. In this work, we are interested in
using more expressive models. Namely, we lever-
age attention models as lexical models and use
them with bidirectional recurrent alignment mod-
els. These recurrent models are able to encode un-
bounded source and target context in comparison
to feedforward networks.

The attention-based translation model is condi-
tioned on the full source sentence, but it has no ex-
plicit dependence on alignments as input. We pro-
pose to bias the attention mechanism using align-
ment information, while still allowing the model
to compute attention weights dynamically. Condi-
tioning the model on the alignment information as
such makes it possible to combine with an align-
ment model in a generative story. We demonstrate

108



that the attention model can benefit from such ex-
ternal alignment information on two WMT tasks:
the 2016 English→Romanian task and the 2017
German→English task.

2 Related Work

Alignment-based neural models have explicit de-
pendence on the alignment information either at
the input or at the output of the network. They
have been extensively and successfully applied in
the literature on top of conventional phrase-based
systems (Sundermeyer et al., 2014a; Tamura et al.,
2014; Devlin et al., 2014). In this work, we focus
on using the models directly to perform standalone
neural machine translation.

Alignment-based neural models were proposed
in (Alkhouli et al., 2016) to perform neural ma-
chine translation. They mainly used feedforward
alignment and lexical models in decoding. In this
work, we investigate recurrent models instead. We
use a modified attention model as a lexical model
and apply it together with a recurrent alignment
neural model.

Deriving neural models for translation based on
the HMM framework can also be found in (Yang
et al., 2013; Yu et al., 2017). Alignment-based
neural models were also applied to perform sum-
marization and morphological inflection (Yu et al.,
2016). The work used a monotonous alignment
model, where training was done by marginaliz-
ing over the alignment hidden variables, which is
computationally expensive. In this work, we use
non-monotonous alignment models. In addition,
we train using pre-computed Viterbi alignments
which speeds up neural training. In (Yu et al.,
2017), alignment-based neural models were used
to model alignment and translation from the tar-
get to the source side (inverse direction), and a
language model was included in addition. They
showed results on a small translation task. In this
work, we present results on translation tasks con-
taining tens of millions of words. We do not in-
clude a language model in any of our systems.

There is plenty of work on modifying attention
models to capture more complex dependencies.
(Cohn et al., 2016) introduces structural biases
from word-based alignment concepts like fertility
and Markov conditioning. These are internal mod-
ifications that leave the model self-contained. Our
modifications introduce alignments as external in-
formation to the model. (Arthur et al., 2016) in-

clude lexical probabilities to bias attention. (Chen
et al., 2016; Mi et al., 2016) add an extra term
dependent on the alignments to the training ob-
jective function to guide neural training. This is
only applied during training but not during decod-
ing. Our work modifies the attention component
directly, and we can choose whether to apply the
alignment bias during decoding or not. We show
that using alignment bias during search alongside
an alignment model improves translation.

3 Alignment-Based Translation

Given a source sentence fJ1 = f1...fj ...fJ , a tar-
get sentence eI1 = e1...ei...eI , and an alignment
sequence bI1 = b1...bi...bI , where j = bi is the
source position aligned to the target position i, we
model translation using an alignment model and a
lexical model:

p(eI1|fJ1 ) =
∑

bI1

p(eI1, b
I
1|fJ1 ) (1)

≈ max
bI1

I∏

i=1

p(ei|bi, bi−11 , ei−11 , fJ1 )︸ ︷︷ ︸
lexical model

·

p(bi|bi−11 , ei−11 , fJ1 )︸ ︷︷ ︸
alignment model

Both the lexical model and the alignment model
have rich dependencies including the full source
context fJ1 , the full alignment history b

i−1
1 , and

the full target history ei−11 . The lexical model has
an extra dependence on the current source position
bi. First-order HMMs simplify the dependence
on the alignment history and limit it to the pre-
decessor alignment point bi−1. This allows an ef-
ficient computation of the sum over the alignment
sequence given in Eq. (1) using dynamic program-
ming. In this work, we stick to the maximum ap-
proximation, and keep the full dependence on the
alignment history bi−11 . We use recurrent neural
networks to model the unbounded source, target
and alignment context. Nevertheless, the models
we describe can be simplified easily to drop the
full dependence on the alignment history, in which
case integrated training using the sum can be per-
formed as suggested by Wang et al. (2017).

4 Attention-Based Translation Model

The standard attention-based translation model
has three main components: The encoder, the de-
coder, and the attention component. The model

109



fj
ei−1

didi−1

ti−1

oioi−1

embeddings

bidirectional encoder

target states

attention

p(ei|ei−11 , f
J
1 )

ri−1
mi

Figure 1: Attention model architecture.

architecture is illustrated in Fig. (1). We use
long short-term memory (LSTM) recurrent layers
throughout this work (Hochreiter and Schmidhu-
ber, 1997; Gers et al., 2000, 2003). We include a
bidirectional encoder where we sum the forward
and backward source state representations:

−→
hj = LSTM(

−→
hj−1, Ffj)

←−
hj = LSTM(

←−
hj+1, Ffj)

hj = Y
←−
hj + Z

−→
hj (2)

where Y and Z are weight matrices, F
is the source word embedding matrix, and
fj ∈ {0, 1}|Vf |×1 is the one-hot vector of the
source word at position j. |Vf | is the size of the
source vocabulary. The parameterization of the re-
current layer is abstracted away using the LSTM
notation for simplicity. We use an LSTM layer to
represent the state of the target sequence:

ti−1 = LSTM(ti−2, Eei−1) (3)

where E is the target word embedding matrix, and
ei−1 ∈ {0, 1}|Ve|×1 is the one-hot vector of the
target word at position i − 1. |Ve| is the size of
the target vocabulary. The attention weights are
normalized using the softmax function according

to the following equations:

αij =
exp(sij)∑J
j=1 exp(sij)

sij = v
T tanh(Whj +Mri−1 + a)

ri−1 = Roi−1 + Lti−1 (4)

oi−1 = Adi−1 +Bti−2
di−1 = LSTM(di−2,mi−1) (5)

mi =
J∑

j=1

αijhj

where αij denotes the normalized attention
weights, sij denotes the unnormalized attention
scores, ri−1 is the translation state computed using
the decoder state at the previous step oi−1 and the
target state ti−1 which in turn is computed using
the target word ei−1. The decoder state di is com-
puted using an LSTM over the attended source po-
sitions mi. v and a are vectors, and A, B, W , M ,
R, and L are weight matrices.

The final target word probability is computed
as a softmax function of the decoder state oi ∈
R|Ve|×1:

p(ei = w|ei−11 , fJ1 ) =
exp(oiw)∑|Ve|
v=1 exp(oiv)

5 Alignment-Biased Attention

In order to use the attention model as an
alignment-dependent lexical model, we introduce
a dependence on the alignment information bi. We
modify the attention mechanism according to the
following equation:

sij = v
T tanh(Whj +Mri−1 + a+ δj,bi c) (6)

where c is a vector, and δj,bi is the Kronecker delta:

δj,bi =

{
1, if j = bi
0, otherwise.

We also experiment with a bias term that in-
cludes the aligned source state hbi :

sij = v
T tanh(Whj +Mri−1 + a+ δj,biDhbi)

(7)

which we refer to as source alignment bias. D is
an additional weight matrix. Note that the model
will have full dependence on the alignment history
due to Eq. (5) and Eq. (4) (cf. Fig. (1)). This de-
pendency can be simplified by removing both the

110



fj
ei−1

embeddings

bidirectional encoder

qi

zi

ti−1target states

selection based on bi−1 = j

p(∆i|bi−11 , e
i−1
1 , f

J
1 )

Figure 2: Bidirectional alignment model (BAM).

recurrency in Eq. (5), and the recurrent input oi−1
that feeds ri−1 in Eq. (4). In this work, however,
we stick to the richer representation and keep the
full dependence on the alignment history.

If the alignment information is pre-computed,
e.g. through IBM/HMM training, using it as an
alignment bias might risk that the original at-
tention part will learn nothing and that it be-
comes completely dependent on the alignment in-
formation. To alleviate this problem, we include
the alignment bias term during training for some
batches and drop it for others. In our experiments,
we randomly include the bias term for 50% of the
training batches.

6 Recurrent Alignment Model

We use a recurrent alignment model to score
alignments. The model architecture is shown in
Fig. (2). Following (Alkhouli et al., 2016), the
alignment model predicts the relative jump ∆i =
bi − bi−1 from the previous source position bi−1
to the current source position bi. This model has a
bidirectional source encoder consisting of two re-
current layers (yellow), and a recurrent layer main-
taining the target state (red). The most recent tar-
get state computed including word ei−1 is paired
with the source states at position bi−1, which is a
hard alignment obtained externally and not com-
puted by the model. We pair the source state hj at
position j = bi−1 with the target state ti−1 at posi-
tion i−1 to predict the jump ∆i to the next source

position bi according to the following equations:

qi = Uti−1 + hbi−1
zi = LSTM(zi−1, qi) (8)

where U is a weight matrix, qi is the paired source
and target states, and zi is the decoder state used
to predict the jump from bi−1 to bi. hbi−1 and ti−1
are defined in Eq. (2), and Eq. (3), respectively.
Removing the recurrency in Eq. (8) results in a
first-order model over the alignment sequence.

7 Training

In this work, we train the attention and the align-
ment model separately. We obtain the alignments
using IBM/HMM training. While this breaks up
the simplicity of end-to-end training of attention
models, we want to note that this is not central to
the proposed approach. Integrated training using
the sum instead of the maximum approximation in
Eq. (1) can be performed using the Baum-Welch
algorithm similar to (Yu et al., 2017; Wang et al.,
2017), but the models need to give up the recur-
rency over the alignment information. Alterna-
tively, the maximum approximation can be used
to find the Viterbi alignments without changing
the models, where training proceeds by alternat-
ing between aligning the training data and model
estimation. In this work, however, we focus on
the modeling aspect and leave integrated training
to future work.

8 Alignment-Based Decoding

Similar to (Alkhouli et al., 2016), we combine the
lexical and alignment neural models in a beam-
based decoder. Since the models depend on the
alignment information, we also have to hypothe-
size alignments during decoding. In training, we
assume that each target position is aligned to ex-
actly one source position. During decoding, we
hypothesize all source positions for each target po-
sition. We assign the models separate weights and
obtain the best translation as follows:

eÎ1 = arg max
I,eI1

{
1

I
max
bI1

{
I∑

i=1

λ log p(ei|bi1, ei−11 , fJ1 )

+(1− λ) log p(∆i|bi−11 , ei−11 , fJ1 )
}}

(9)

where λ is the lexical model weight, which we
tune on the development set using grid search.

111



WMT 2016 WMT 2017
English Romanian German English

Sentences 604K 3.55M
Running Words 15.5M 15.8M 85M 86M
Vocabulary 92.3K 128.3K 671K 587K
Neural Network Vocabulary 56.1K 80.9K 188K 131K

Table 1: Corpora and NN statistics.

9 Experiments

9.1 Setup

This section presents experiments on two
WMT shared translation tasks: the 2016
English→Romanian task1 and the 2017
German→English task.2 The corpora statis-
tics are shown in Tab. (1). We use the full
bilingual data of the English→Romanian task.
For the German→English task, we choose the
common crawl, news commentary and European
parliament bilingual data. The data is filtered
by removing sentences longer than 100 words.
We also remove sentences where five or more
consecutive source words are unaligned according
to IBM1/HMM/IBM4 training. This is to remove
noisy sentence pairs that are frequent in the
common crawl corpus. We do not use any kind of
synthetic or back-translated data in this work.

We reduce the vocabulary size by replac-
ing singletons with the unknown token for
both English and Romanian corpora in the
English→Romanian task. Since we have more
data in the German→English task, we replace
words occurring less than 6 times in the German
corpus and less than 4 times in the English cor-
pus by the unknown token. The reduced vocabu-
laries are what we refer to as the neural network
vocabulary in Tab. (1). To handle the large out-
put vocabularies, all lexical models use a class-
factored output layer, with 1000 singleton classes
dedicated to the most frequent words, and 1000
classes shared among the rest of the words. The
classes are trained using a separate tool to opti-
mize the maximum likelihood training criterion
with the bigram assumption. The alignment model
uses a small output layer of 201 nodes, determined
by a maximum jump length of 100 (forward and
backward). We train using stochastic gradient de-
scent and halve the learning rate when the devel-
opment perplexity increases.

1http://www.statmt.org/wmt16/
2http://www.statmt.org/wmt17/

We train feedforward models to compare to
(Alkhouli et al., 2016). The models have two hid-
den layers, the first has 1000 nodes and the second
has 500 nodes. We use a 9-word source window,
and a 5-gram target history. 100 nodes are used
for word embeddings. The bidirectional alignment
models have 4 LSTM layers as shown in Fig. (2).
We use 200-node source and target word embed-
dings and 200 nodes in each LSTM layer.

The attention models also use 200-node LSTM
layers, and 200-node source and target embed-
dings. The internal dimension of the atten-
tion component is also set to 200 nodes, i.e.
v, a, c ∈ R200×1.

Each model is trained on 4-12 CPU cores using
the Intel MKL library, and takes about 2–4 days
on average to converge.

We apply attention models with alignment bias
and feedforward models in decoding using a de-
coder similar to that proposed in (Alkhouli et al.,
2016). The decoder hypothesizes each source po-
sition for every target position being translated.
Beam search is applied where the search nodes
consist of both lexical and alignment hypothe-
ses. When the attention model is applied with-
out the alignment bias term, the decoder simpli-
fies to hypothesizing lexical translations only. To
speed up decoding of long sentences, we limit
alignment hypotheses to the source positions j ∈
{i − 20, ..., i + 20}, where i is the current target
position being translated. We use a beam size of
16 in all experiments. The alignments used during
training are a result of IBM1/HMM/IBM4 training
using GIZA++ (Och and Ney, 2003).

We use grid search to optimize the lexical
model weights (cf. Eq. (9)). We find that
the attention model receives a weight of 0.8,
while the alignment model is assigned a weight
of 0.2. We tune this on the development set
of each task. We use 1000 sentence pairs of
newsdev2016 as the development set of the
English→Romanian task, and newstest2015
for tuning the German→English model weights.

112



These same datasets are used to halve the learning
rate during model training.

All translation experiments are performed us-
ing an extension of the Jane toolkit (Vilar et al.,
2010; Wuebker et al., 2012). The neural net-
works are trained using an extension of the rwthlm
toolkit (Sundermeyer et al., 2014b). All results are
measured in case-insensitive BLEU [%] (Papineni
et al., 2002) using mteval from the Moses toolkit
(Koehn et al., 2007). Case-insensitive TER [%]
scores are computed with TERCom (Snover et al.,
2006). Word classes are trained using an in-house
tool (Botros et al., 2015) similar to mkcls.

9.2 Results

We compare our proposed system to three baseline
systems on the WMT 2016 English→Romanian
task and the WMT 2017 German→English task.
The results are shown in Tab. (2). We set up a
baseline system using a feedforward lexical model
and a feedforward alignment model, to compare to
the models used in (Alkhouli et al., 2016). This is
shown in row 1. We first check the effect of us-
ing a recurrent alignment model (row 2) instead of
the feedforward model. This brings an improve-
ment of up to 1.6% BLEU. The attention baseline
(row 3) performs much better in comparison, scor-
ing up to 3.1% BLEU better than the feedforward
system. This model has no alignment bias com-
ponent. We note here that the German→English
training data size is about 5.7 times more than that
of the English→Romanian task, which can explain
the small gap in performance between the systems
in row 2 and row 3 on the German→English task,
as the feeforward networks have large hidden lay-
ers of 1000 and 500 nodes, while the recurrent
models use hidden layers of size 200.

We train an attention model by adding the align-
ment bias term in Eq. (6). We bias the attention
model randomly during training for 50% of the
training batches. During decoding, we include a
bidirectional alignment model to score the align-
ment hypotheses (rows 4, 5). The combination of
the alignment-biased attention model and the bidi-
rectional alignment model (row 4) outperforms the
standard attention model (row 3). This shows that
the model learns to use the alignment information.
We also compare to adding source alignment bias
as given by Eq. (7) (row 5). We observe no differ-
ence to the case of constant alignment bias (row
4) on these tasks. Overall, we improve BLEU by

1.7% and 1.1% on the English→Romanian and
the German→English task, respectively.

9.3 Alignment Model
In Tab. (3), we analyze the effect of the alignment
model on the system. We observe that if the align-
ment model is dropped, the attention model is un-
able to score the alignments hypothesized during
decoding on its own (row 4). If we drop the align-
ment model in decoding, we also have to exclude
the alignment bias term when computing attention
weights during decoding (row 3) (the bias term is
still included in training). In this case, the transla-
tion degrades to the baseline performance.

9.4 Block out
In Tab. (3) we also investigate the effect of
block out. On the English→Romanian task
which has less training data in comparison to
German→English, we observe that block out
helps improve the system (row 2 vs. 5). This is
because it avoids overfitting the alignment infor-
mation, allowing the attention component to learn
to attend on its own. This can be verified when
comparing row 3 to row 6: When block out is
used in training, and the attention model is used
afterwards in decoding alone without an alignment
model, it is able to perform close to the baseline at-
tention performance if block out is used. Without
using block out, the model fails to attend to the
source side properly on its own.

9.5 Alignment Quality
We analyze the word alignment quality using 504
manually word-aligned German-English sentence
pairs that were extracted from the Europarl cor-
pus (Vilar et al., 2006). In Tab. (4), we com-
pare the baseline attention system to the proposed
alignment-based system. The alignments of the
baseline attention system are generated by align-
ing each target word to the source position hav-
ing the maximum attention weight. We observe
that the baseline attention system has a high AER
in comparison to the proposed system, which re-
duces AER from 44.9% to 29.7%. This corre-
sponds to 1.1% BLEU improvement. It is worth
noting that the high AER of the baseline system
is likely because the model is not trained to align,
and that the attention weights it produces are soft
alignments. In comparison, our system uses an
alignment model that explicitly learns to model
alignments.

113



WMT En→Ro WMT De→En
newstest2016 newstest2017

lexical alignment bias
# model model term BLEU

[%]
TER

[%]
BLEU

[%]
TER

[%]

1 feedforward feedforward - 20.0 64.2 24.2 58.6
2 feedforward bidirectional - 21.6 62.7 25.5 57.6
3 attention - - 23.1 60.6 25.7 57.6
4 attention bidirectional δj,bi c 24.8 58.1 26.8 55.6
5 attention bidirectional δj,biDhbi 24.8 58.1 26.8 55.5

Table 2: Translation results on the WMT 2016 English→Romanian task and the WMT 2017
German→English task.

WMT En→Ro WMT De→En
newstest2016 newstest2017

lexical alignment decode w/ train w/
# model model align bias block out BLEU

[%]
TER

[%]
BLEU

[%]
TER

[%]

1 attention baseline - - - 23.1 60.6 25.7 57.6
2

+ alignment bias

bidirectional yes
yes

24.8 58.1 26.8 55.6
3 - no 23.1 60.6 25.7 59.4
4 - yes degenerate degenerate
5 bidirectional yes

no
23.7 59.2 26.7 55.8

6 - no degenerate degenerate

Table 3: The effect of using the alignment model in decoding and block out in training . The alignment
bias term used here is δj,bi c. Rows 1 and 2 are the same as rows 3 and 4 in Tab. (2). Block out means
including the alignment bias term for 50% of the training batches.

newstest2017 Europarl
BLEU

[%]
AER

[%]

attention baseline 25.7 44.9
proposed system 26.8 29.7

Table 4: A comparison between the WMT
German→English proposed system and the base-
line attention system in terms of the alignment er-
ror rate (AER). The attention baseline and the pro-
posed system are the same ones shown in Tab. (2),
rows 3 and 4, respectively.

To illustrate what happens when we include
the source alignment bias term, we take a
sample from the translation hypotheses of the
German→English system in Tab. (2, row 5), and
compare it to the output of the standard attention
model Tab. (2, row 3). The sample is chosen from
the development set newstest2015. The Ger-
man sentence “diese schreckliche Erfahrung wird
uns immer verfolgen .” has the reference transla-
tion “ this horrible experience will stay with us .”

In Fig. (3), we illustrate the best translation hy-
pothesis and the corresponding attention weights
produced by the standard attention model. Fig. (4)
shows the same thing for the attention model using
source alignment bias. We observe that the latter
is able to generate a good translation while being
able to attend to the source sentence in a proper
order. On the other hand, the standard attention
model has a problem in the first half of the hy-
pothesis, where it attends to the second half of the
source sentence instead. It ends up confusing the
object and the subject. A more acceptable, though
inaccurate, translation of ‘verfolgen’ under such
reordering would be ‘followed by’, but the system
fails to generate this translation.

Fig. (5) shows the curve of tuning the lexical
model weight. We observe that the weight is ro-
bust against small changes. The best results in
terms of BLEU are achieved when λ = 0.8.

114



we
are

always
pursuing

this
terrible

experience

.
EOS

diese
schreckliche
Erfahrung
wird
uns
immer
verfolgen
. EOS

Figure 3: A translation example produced by the
standard attention system in Tab. (2), row 3. EOS
denotes the sentence end symbol. The shading de-
gree corresponds to the attention weight.

this
terrible

experience
will

always
follow

us

.
EOS

diese
schreckliche
Erfahrung
wird
uns
immer
verfolgen
. EOS

Figure 4: A translation example produced by our
best system using source alignment bias, given in
Tab. (2), row 5. EOS denotes the sentence end
symbol. The shading degree corresponds to the
attention weight.

10 Conclusion

We presented a modification of the attention
model to bias it using external alignment infor-
mation. We also presented a bidirectional recur-
rent neural network alignment model to be used
alongside the proposed attention model. We used
the two models in a generative scheme of align-
ment generation followed by lexical translation.
We demonstrated improvements over the standard
attention model on two WMT tasks. We provided
evidence that enabling the alignment bias term for
all training samples makes the attention mecha-
nism overfit the alignments on non-large datasets.
To remedy this, we proposed to apply the align-
ment bias on half of the training samples, which

22

22.5

23

23.5

24

24.5

25

0.6 0.7 0.8 0.9
59

59.5

60

60.5

61

B
L

E
U

[%
]

T
E

R
[%

]

λ

BLEU[%]
TER[%]

Figure 5: Grid search tuning of the lexical weight
of the system in Tab. (2, row 4). The re-
sults are computed on the development set of the
English→Romanian task.

yielded our best system.
While this work depends on pre-computed

alignments to train the attention and alignment
models, this is not central to our approach. In
future work, we plan to perform integrated train-
ing by alternating between alignment generation
and model estimation. Alignment generation can
be performed using forced alignment where beam
search is performed over the alignment positions,
while fixing the lexical translations to the refer-
ence translation. This can eliminate the need for
pre-computing alignments using ad hoc methods
like IBM1/ HMM/IBM4 training.

Acknowledgements

The work reported in this
paper results from two
projects, SEQCLAS and
QT21. SEQCLAS has
received funding from

the European Research Council (ERC) under
the European Union’s Horizon 2020 research
and innovation programme under grant agree-
ment no 694537. QT21 has received funding
from the European Union’s Horizon 2020 research
and innovation programme under grant agreement
no 645452. The work reflects only the authors’
views and neither the European Commission nor
the European Research Council Executive Agency
are responsible for any use that may be made of the
information it contains.

Tamer Alkhouli was partly funded by the 2016
Google PhD Fellowship for North America, Eu-
rope and the Middle East.

115



The authors would like to thank Kazuki Irie for
contributing to the attention layer implementation.

References
Tamer Alkhouli, Gabriel Bretschner, Jan-Thorsten Pe-

ter, Mohammed Hethnawi, Andreas Guta, and Her-
mann Ney. 2016. Alignment-based neural machine
translation. In Proceedings of the First Conference
on Machine Translation, pages 54–65, Berlin, Ger-
many.

Philip Arthur, Graham Neubig, and Satoshi Nakamura.
2016. Incorporating discrete translation lexicons
into neural machine translation. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1557–1567, Austin,
Texas.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations, San Diego,
Calefornia, USA.

Rami Botros, Kazuki Irie, Martin Sundermeyer, and
Hermann Ney. 2015. On efficient training of word
classes and their application to recurrent neural net-
work language models. In Interspeech, pages 1443–
1447, Dresden, Germany.

Wenhu Chen, Evgeny Matusov, Shahram Khadivi, and
Jan-Thorsten Peter. 2016. Guided alignment train-
ing for topic-aware neural machine translation. In
Proceedings of the 2016 Conference of the Asso-
ciation for Machine Translation in the Americas
(AMTA), pages 121–134, Austin, Texas.

Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vy-
molova, Kaisheng Yao, Chris Dyer, and Gholamreza
Haffari. 2016. Incorporating structural alignment
biases into an attentional neural translation model.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 876–885, San Diego, California.

Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and Robust Neural Network Joint Models for
Statistical Machine Translation. In 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 1370–1380, Baltimore, MD, USA.

Felix A. Gers, Jürgen Schmidhuber, and Fred Cum-
mins. 2000. Learning to forget: Continual
prediction with LSTM. Neural computation,
12(10):2451–2471.

Felix A. Gers, Nicol N. Schraudolph, and Jürgen
Schmidhuber. 2003. Learning precise timing with
lstm recurrent networks. The Journal of Machine
Learning Research, 3:115–143.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra
Constantine, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
pages 177–180, Prague, Czech Republic.

Haitao Mi, Zhiguo Wang, and Abe Ittycheriah. 2016.
Supervised attentions for neural machine translation.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2283–2288, Austin, Texas.

Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics, 29(1):19–51.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311–318,
Philadelphia, Pennsylvania, USA.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223–231, Cambridge, Massachusetts, USA.

Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,
and Hermann Ney. 2014a. Translation Modeling
with Bidirectional Recurrent Neural Networks. In
Conference on Empirical Methods on Natural Lan-
guage Processing, pages 14–25, Doha, Qatar.

Martin Sundermeyer, Ralf Schlüter, and Hermann Ney.
2014b. rwthlm - the RWTH Aachen university neu-
ral network language modeling toolkit. In Inter-
speech, pages 2093–2097, Singapore.

Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2014. Recurrent neural networks for word align-
ment model. In 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1470–
1480, Baltimore, MD, USA.

David Vilar, Maja Popović, and Hermann Ney. 2006.
AER: Do we need to “improve” our alignments?
In Proceedings of International Workshop on Spo-
ken Language Translation, pages 205–212, Kyoto,
Japan.

David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open source hierarchi-
cal translation, extended with reordering and lexi-
con models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262–270, Uppsala, Sweden.

116



Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-Based Word Alignment in Statistical
Translation. In Proceedings of the 16th conference
on Computational linguistics, volume 2, pages 836–
841, Copenhagen, Denmark.

Weiyue Wang, Tamer Alkhouli, Derui Zhu, and Her-
mann Ney. 2017. Hybrid neural network alignment
and lexicon model in direct hmm for statistical ma-
chine translation. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics, Vancouver, Canada.

Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483–491, Mum-
bai, India.

Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Neng-
hai Yu. 2013. Word alignment modeling with con-
text dependent deep neural network. In 51st Annual
Meeting of the Association for Computational Lin-
guistics, pages 166–175, Sofia, Bulgaria.

Lei Yu, Phil Blunsom, Chris Dyer, Edward Grefen-
stette, and Tomás Kociský. 2017. The neural
noisy channel. In Proceedings of the International
Conference on Learning Representations, volume
abs/1611.02554.

Lei Yu, Jan Buys, and Phil Blunsom. 2016. Online seg-
ment to segment neural transduction. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 1307–1316,
Austin, Texas.

117


