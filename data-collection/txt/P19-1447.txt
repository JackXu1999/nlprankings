



















































Reranking for Neural Semantic Parsing


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4553–4559
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

4553

Reranking for Neural Semantic Parsing

Pengcheng Yin
Language Technologies Institute

Carnegie Mellon University
pcyin@cs.cmu.edu

Graham Neubig
Language Technologies Institute

Carnegie Mellon University
gneubig@cs.cmu.edu

Abstract
Semantic parsing considers the task of trans-
ducing natural language (NL) utterances into
machine executable meaning representations
(MRs). While neural network-based seman-
tic parsers have achieved impressive improve-
ments over previous methods, results are still
far from perfect, and cursory manual inspec-
tion can easily identify obvious problems such
as lack of adequacy or coherence of the gen-
erated MRs. This paper presents a simple
approach to quickly iterate and improve the
performance of an existing neural semantic
parser by reranking an n-best list of predicted
MRs, using features that are designed to fix
observed problems with baseline models. We
implement our reranker in a competitive neu-
ral semantic parser and test on four semantic
parsing (GEO, ATIS) and Python code gen-
eration (DJANGO, CONALA) tasks, improv-
ing the strong baseline parser by up to 5.7%
absolute in BLEU (CONALA) and 2.9% in
accuracy (DJANGO), outperforming the best
published neural parser results on all four
datasets.1

1 Introduction

Semantic parsing is the task of mapping a nat-
ural language utterance into machine executable
meaning representations (e.g., Python code). Re-
cent years have witnessed a burgeoning of ap-
plying neural network architectures for semantic
parsing, from sequence-to-sequence models (Jia
and Liang, 2016; Ling et al., 2016; Liang et al.,
2017; Suhr et al., 2018), to more complex pars-
ing paradigms guided by the structured topolo-
gies of target meaning representations (Xiao et al.
(2016); Dong and Lapata (2016); Yin and Neubig
(2017); Rabinovich et al. (2017); Krishnamurthy
et al. (2017); Zhong et al. (2017); Dong and Lap-
ata (2018); Iyer et al. (2018), inter alia).

1Code is available at pcyin.me/reranking.

download the file from url `url` and save it under file `file_name`

json.loads(['url', 'file_name', 'file_name'])

urllib.request.urlretrieve('url', 'r')

urllib.request.urlretrieve('url', 'file_name')

r = urllib.request.urlretrieve(str_0)

Input Utterance

System Predictions (n-best list of MRs)
z1

z2

Reranker Output
urllib.request.urlretrieve('url', 'file_name')

z9

…

z3

z 7! x : �35.2 z $ x : �3.4

z 7! x : �31.4 z $ x : �0.7

z $ x : �0.8

z $ x : �5.8z 7! x : �49.8

x

z3

z 7! x : �34.7

Figure 1: Illustration of the reranker with a real example
from the CONALA code generation task (Yin et al., 2018a)
with reconstruction (z 7→ x) and discriminative matching
(x↔ z) scores.

While neural network-based semantic parsers
have achieved impressive results, there is still
room for improvement. A pilot analysis of incor-
rect predictions from a competitive neural seman-
tic parser, TRANX (Yin and Neubig, 2018) indi-
cates an obvious issue of incoherence. In the real
example in Figure 1, top prediction z1 is semanti-
cally incoherent with the intent expressed in the ut-
terance. Perhaps a more interesting issue is inade-
quacy — while the predicted MRs match the over-
all intent of the utterance, they still miss or misin-
terpret crucial pieces of information (e.g., missing
or generating wrong arguments, as in z2 and z9).
Indeed, we observe that around 41% of the failure
cases of TRANX on a popular Python code gener-
ation task (DJANGO, Oda et al. (2015)) are due to
such inadequate predictions.

Although the top predictions from a semantic
parser could fall short in adequacy or coherence,
we found the parser still maintains high recall,
covering the gold-standard MR in its n-best list of
predictions most of the time2. This naturally mo-

2As we will show in § 3, our base neural semantic parser

http://pcyin.me/reranking


4554

tivates us to investigate whether the performance
of an existing neural parser can be potentially im-
proved by reranking the n-best list of candidate
MRs. In this paper, we propose a simple reranker
powered mainly by two quality-measuring fea-
tures of a candidate MR: (1) a generative recon-
struction model, which tests the coherence and
adequacy of an MR via the likelihood of recon-
structing the original input utterance from the MR;
and (2) a discriminative matching model, which
directly captures the semantic coherence between
utterances and MRs. We implement our reranker
in a strong neural semantic parser and evaluate on
both tasks of parsing NL to domain-specific logi-
cal form (GEO, ATIS) and general-purpose source
code (DJANGO, CONALA). Our reranking ap-
proach improves upon this strong parser by up to
5.7% absolute in BLEU (CONALA) and 2.9% in
accuracy (DJANGO), outperforming the best pub-
lished neural parser results on all datasets.

2 Reranking Model

Figure 1 illustrates our approach. Given an in-
put NL utterance x, we assume access to an exist-
ing neural semantic parser p(z|x), which outputs
a ranked n-best list of system-generated mean-
ing representations given x, {zi}ni=1. In prac-
tice, such an n-best list is usually generated by
approximate inference like beam search. The
reranker R(·) takes as input the n-best list of
MRs and the input utterance, and outputs the MR
ẑ with the highest reranking score, i.e., ẑ =
argmaxz∈{zi}ni=1R(z,x). We parameterize R(·)
as a (log-) linear model:

R(z,x) = α0 log p(z|x) +
K∑
k=1

αkfk(z,x), (1)

where fk is a feature function that scores a
candidate prediction z, and {α} tuned weights.
We also include the original parser score in
R(·). The idea of reranking the beam of can-
didate parses has been attempted for various
NLP tasks (Collins and Koo, 2000), and was
also previously applied for classical grammar-
driven semantic parsers. Such reranking mod-
els typically use domain-specific syntactic features
strongly coupled with the underlying parsing al-
gorithm (e.g., an indicator feature for each gram-
mar rule applied, Raymond and Mooney (2006);

registers 77.3% top-1 accuracy and 84.0% recall over the 15-
best beam on the DJANGO dataset, a 6.7% absolute gap.

download
file
from
url

ur
ll
ib

re
qu
es
t

ur
lr
et
ri
ev
e

…

…ur
l

urllibdownload

=f( , )
urlretrievedownload

=f( , )

urlurl

=f( , )

… p(x$z) = g( )…

1. Alignment 2. Comparison 3. Aggregation

Figure 2: Illustration of the discriminative matching model,
adapted from Parikh et al. (2016). Punctuations (dots, paren-
theses) in MR are omitted for clarity.

Srivastava et al. (2017)), while our reranker ap-
plies domain-general quality-measuring features
compatible with both domain-specific (e.g., λ-
calculus) and general-purpose (e.g., Python) MRs
(more in § 3). Specifically, our reranker mainly
uses two features, whose scores are given by two
external models: a reconstruction model and a
matching model.

Generative Reconstruction Feature Our re-
construction feature log p(z 7→ x) is a genera-
tive model that scores the coherence and adequacy
of an MR z using the probability of reproducing
the original input utterance x from z. Intuitively,
a good candidate MR should adequately encode
the semantics of x, leading to high reconstruction
score. The idea of using reconstruction as a qual-
ity metric is closely related to reconstruction mod-
els in auto-encoders (Vincent et al., 2008), and its
applications in semi-supervised (Yin et al., 2018b)
and weakly supervised (Cheng and Lapata, 2018)
semantic parsing, where p(z 7→x) is used to score
the quality of sampled MRs in optimization. Sim-
ilar models have also been applied for pragmatic
inference in instruction-following agents for mod-
eling the likelihood of causing the speaker to pro-
duce the utterance given an inferred action (Fried
et al., 2018), while we use p(z 7→x) as one quality-
measuring feature in our reranker.

Specifically, we implement p(z 7→x) using an
attentional sequence-to-sequence network (Luong
et al., 2015), which takes as input a tokenized MR
z. The network is augmented with a copy mecha-
nism (Gu et al., 2016), allowing out-of-vocabulary
variable names (e.g., file name in Figure 1) in z
to be directly copied to the utterance x.

Discriminative Matching Feature We use a
matching model to measure the probability of
the input utterance x and a candidate MR z
being semantically coherent to each other. In-
tuitively, for a semantically coherent parse z
(e.g., z3 in Figure 1), each sub-piece in z (e.g.,



4555

urllib.request.urlretrieve) could coarsely
match with a span (e.g., download the file) in the
utterance, and vice versa. Motivated by this obser-
vation, we implement p(x↔z) as a decomposable
attention model (Parikh et al., 2016), a discrimi-
native model which computes a semantic coher-
ence score based on the latent pairwise alignment
between tokens in x and z. Figure 2 depicts an
overview of the model, while we refer interested
readers to Parikh et al. (2016) for technical de-
tails. Intuitively, the model measures the semantic
equivalence of an utterance x and an MR z based
on pair-wise associations of tokens in x and z in
three steps: (1) an alignment step, where align-
ment scores between each pair of tokens in x and
z are computed using attention; (2) a comparison
step, where a set of representations are produced
from embeddings of pairwise aligned tokens, cap-
turing their semantic similarities; and (3) an aggre-
gation step, where all pairwise comparisons results
are combined to compute the semantic coherence
score.

Token Count Feature Besides the two primary
features introduced above, we also include an aux-
iliary token count feature |z| of an MR, which
has been shown useful in preventing a machine
translation model from favoring shorter predic-
tions (Cho et al., 2014; Och and Ney, 2002), while
we test them for reranking MRs, especially when
the target metric is BLEU (§ 3).

3 Experiment

We test on four semantic parsing and code gener-
ation benchmarks:
GEO (Zelle and Mooney, 1996) and ATIS (Deb-
orah A. Dahl and Shriber) are two closed-domain
semantic parsing datasets. The NL utterances are
geographical (GEO) and flight booking (ATIS) in-
quiries (e.g., What is the latest flight to Boston?).
The corresponding MRs are defined in λ-calculus
logical forms (e.g., argmax x (and (flight
x) (to x boston)) (departure time x))).
DJANGO (Oda et al., 2015) is a popular
Python code generation dataset consisting of
NL-annotated code from the Django framework.
Around 70% of examples are simple cases of vari-
able assignment (e.g., result = []), function
definition/invocation or condition tests, which can
be easily inferred from the verbose NL utterances
(e.g., Result is an empty list).

CONALA (Yin et al., 2018a)3 is a newly intro-
duced task for open-domain code generation. It
consists of 2, 879 examples of manually anno-
tated NL questions (e.g., Check if all elements in
list ‘my list’ are the same) and their Python solu-
tion (e.g., len(set(mylist)) == 1) on STACK
OVERFLOW. Compared with DJANGO, examples
in CONALA cover real-world NL queries issued
by programmers with diverse intents, and there-
fore are significantly more difficult due to its broad
coverage and high compositionality of target MRs.

Base Semantic Parser p(z|x) While we remark
that our reranking model is parser agnostic, in
the experiments we are primarily interested in in-
vestigating if the reranker could further improve
the performance of an already-strong semantic
parser. We use TRANX (Yin and Neubig, 2018)5,
a general-purpose open-source neural semantic
parser that maps an input utterance into MRs us-
ing a neural sequence-to-tree network, where MRs
are represented as abstract syntax trees. We leave
evaluating the performance of the reranker with
other parsers as interesting future work.

Training Reranking Model Deploying the
reranker to a benchmark dataset involves three
steps: (1) training the base parser, (2) training the
reranking features (reconstruction and matching
models), and (3) tuning the feature weights.
(1) Training Base Parser We use its pre-processed
version of the dataset shipped with the base parser
TRANX. We train TRANX using its official con-
figuration and collect the n-best list of candidates
for each example using beam search (beam size
is 5 for GEO and ATIS, and 15 for DJANGO and
CONALA).
(2) Training Reranking Features The reconstruc-
tion model is trained using standard maximum-
likelihood estimation using utterances and their
associated gold MRs in the training set. We then
chose the model with the lowest perplexity on the
development set6. To train the matching model, it
requires training examples in the form of triplets
〈x, z, y〉, consisting of an utterance x, an MR z
and a binary label y indicating whether z is a

3https://conala-corpus.github.io/
5http://pcyin.me/tranX
6GEO does not have a development set, so we randomly

sample 20% examples from its official training set for devel-
opment, and use the remaining 80% examples for training the
reranker and its features.

https://conala-corpus.github.io/
http://pcyin.me/tranX


4556

GEO ATIS DJANGO CONALA
Model ACC. Model ACC. Model ACC. Model BLEU / ACC.

RSK17 87.1 RSK17 85.9 YN17 71.6 SEQ2SEQ† 10.58 / n/a
DL18 88.2 DL18 87.7 DL18 74.1 Best Submission† 24.30 / n/a
Base Parser 88.8 ±1.0 Base Parser 87.6 ±0.1 Base Parser 77.3 ±0.4 Base Parser 24.35 ±0.4 / 2.5 ±0.7
+ Reranker 89.1 ±1.2 + Reranker 88.4 ±0.5 + Reranker 80.2 ±0.4 + Reranker 30.11 ±0.6 / 2.8 ±0.5

– recon. 88.9 ±0.9 – recon. 87.8 ±0.3 – recon. 78.1 ±0.3 – recon. 28.41 ±1.0 / 2.1 ±0.5
– match. 89.2 ±1.0 – match. 87.7 ±0.5 – match. 79.9 ±0.4 – match. 29.89 ±0.5 / 2.6 ±0.6
– t.c. 89.4 ±0.9 – t.c. 88.1 ±0.7 – t.c. 80.0 ±0.4 – t.c. 28.45 ±1.1 / 3.0 ±0.5

Oracle 90.9 ±0.6 Oracle 90.4 ±0.3 Oracle 84.0 ±0.6 Oracle 37.08∗ ±0.6 / 5.4 ±0.6

Table 1: Mean and standard deviation4over five random runs. recon., match., t.c. stand for reconstruction, matching, and token
count features, resp. †Results taken from the official CONALA leaderboard as on Feb. 25th, 2019. ∗Upper-bound corpus-BLEU
is approximated by choosing z in the n-best list with the highest sentence-level BLEU.

correct parse of x. During optimization we cre-
ate a mini-batch D by: (a) sampling a mini-batch
of 10 gold examples 〈x, z∗〉 (x and its correct
MR z∗) from the original training set and adding
those examples (with y = 1) to D; (b) for each
x in the batch, sampling an incorrect MR z′ from
the n-best list, and adding the negative example
〈x, z′, y = 0〉 to D. Validation is performed by
evaluating the classification accuracy on develop-
ment set constructed similarly.

We also made one special modification for
the discriminative matching model on DJANGO.
Different with the preprocessed version of other
datasets, OOV variable names in DJANGO cannot
be easily identified and canonicalized, which hurts
the performance of the vanilla matching model.
Therefore, for each input 〈x, z〉, we replace each
OOV token (e.g., a variable name my list) in x
and z with a unique numbered slot (e.g., VAR0).
Hence, different OOV variable names in the in-
put can still be distinguished based on their slot
IDs. We found this simple trick improved the av-
erage classification accuracy on the development
set from 77% to 81%.

(3) Tuning Feature Weights Finally, given the
trained features, we then tune the feature weights
in E.q. 1 using the minimum risk training (Smith
and Eisner, 2006) algorithm implemented in the
Travatar package (Neubig, 2013), which opti-
mizes the expected metric over candidates in the
n-best list of candidate MRs on development sets.
Steps (2) and (3) are quite efficient, and takes less
than 10 minutes on a server with a GPU.

Metric We use the standard evaluation met-
ric for each dataset: exact match accuracy for
GEO, ATIS, DJANGO and corpus-level BLEU for
CONALA.

3.1 Results

Table 1 lists evaluation results. We also report the
oracle recall over n-best list as an upper-bound
performance (last line in Table 1). First, we note
that our base parser is indeed strong, performing
competitively against existing neural systems on
all datasets. This suggests that our base parser
will serve as a reasonable testbed for the rerank-
ing model. Next, we observe that the reranker
achieves improved results across the board, clos-
ing the gap between top-1 predictions and oracle
recall. Notably, the reranker registers 2.9% ab-
solute gain in accuracy on DJANGO and 5.7% in
BLEU on CONALA, resp. This demonstrates that
reranking is an effective approach to improve the
performance of an already-strong neural parser.

We also performed a feature ablation study by
removing one feature at a time. For discussion,
we also present qualitative examples of reranking
results in Table 2. We are particularly interested
in investigating the comparative utility of the dis-
criminative matching and reconstruction features.
Interestingly, we observe that while the matching
feature seems to be important for semantic pars-
ing tasks like ATIS, the reconstruction model per-
forms generally better on two Python code genera-
tion tasks, where target MRs are much more com-
plex. We hypothesize that our matching model
based on pair-wise token associations between ut-
terances and MRs is particularly effective for sim-
pler MRs in ATIS, where there is a clear corre-
spondence between utterance spans (e.g., round
trip in Example 1, Table 2) and MR predicates
(e.g., round trip). This could also hold for some
examples in DJANGO, where the verbose NL ut-
terances could be roughly aligned with the MR

6We observe relatively high variance on GEO, possibly
due to its small size (599/279 train/test examples).



4557

ATIS Show me all round trip flight from ci0 to ci1 nonstop
z1 lambda $0 e (and (flight $0) (from

$0 ci0) (to $0 ci1) (nonstop $0)) 7

p(z|x) = −2.0 z 7→ x=−11.1 x↔ z=−14.3
z2 lambda $0 e (and (flight $0) (from $0 ci0)
(to $0 ci1) (nonstop $0) (round trip $0)) 3

p(z|x) = −2.3 z 7→ x=−4.2 x↔ z>−0.01
DJANGO If length of version does not equals to integer 5,

raise an exception
z1 raise 7

p(z|x) = −0.1 z 7→ x=−61.7 x↔ z=−5.3
z2 assert len(version) == 5 3

p(z|x) = −3.3 z 7→ x=−17.0 x↔ z=−0.5
DJANGO and truncate, return the result. return elements of

words joined in a string, separated with whitespace
z1 return str(’joined’) 7

p(z|x) = −1.6 z 7→ x=−54.6 x↔ z=−0.68
z5 return ’ ’.join(words) 3

p(z|x) = −2.6 z 7→ x=−48.8 x↔ z=−0.61
CONALA Removing duplicates in list ‘t’
z1 print(list(itertools.chain(∗t))) 7

p(z|x) = −5.9 z 7→ x=−15.5 x↔ z=−0.4
z4 list(set(t)) 3

p(z|x) = −6.7 z 7→ x=−11.3 x↔ z=−1.4

Table 2: Examples with original parser score p(z|x), recon-
struction (z 7→ x) and discriminative matching (x ↔ z)
feature scores (log scale). We show both the original top pre-
diction z1 and the highest-scored one zi by the reranker.

(e.g., Example 2). On the other hand, we observe
the reconstruction model could potentially go be-
yond surface token-wise match between NL utter-
ances and MRs, promoting more complex (longer)
candidate MRs that more adequately encode the
semantics of the input utterance7 (e.g., in Exam-
ple 3, z5 receives a much higher reconstruction
score, while the difference between the discrimi-
native matching scores is small). Therefore, on the
challenging CONALA dataset with much weaker
alignment between its succinct NL utterances and
highly compositional MRs (e.g., Example 4), the
matching model does not function as well (e.g.,
the incorrect MR z1 received a higher match-
ing score). While more careful investigation of
the relative advantage between the reconstruction
and discriminative matching features remain an in-
teresting future work, we remark that the recon-
struction model p(z 7→ x), when combined with
with the original parser score p(z|x) in E.q. (1),
also implicitly functions as a matching model that
measures the semantic similarity using the bidi-
rectional generation likelihood between x and z.
Such architecture could be an interesting future di-

7On DJANGO, the average length of top-reranked MRs by
the reconstruction and matching models is 8.6 and 7.7, resp.

rection for modeling semantic similarity.
Additionally, the auxiliary token count feature

is also effective, especially on CONALA, yielding
a +1.66 gain in BLEU by promoting longer MRs.

Finally, we investigated the failure cases where
our best-performed reranker generated incorrect
MRs. We are particularly interested in those re-
maining failed examples on simpler semantic pars-
ing tasks (GEO, ATIS), where our reranker’s accu-
racies are close to the oracle recall. For instance,
on ATIS, only 6 incorrect examples (out of a to-
tal of 49) are due to reranking error, 10 are due to
that the gold MRs are not included the n-best list,
while most (20) remaining cases are because the
specific NL patterns in testing utterances (e.g., the
temporal NL pattern flight . . . prior to . . .) are not
covered by its (small) training set. This interesting
result suggests that incorporating external linguis-
tic knowledge (e.g., Wang et al. (2014)) is impor-
tant in order to further improve the performance of
neural parsers on closed-domain semantic parsing
tasks.

4 Conclusion

We proposed a feature-based reranker for neural
semantic parsing, which achieved strong results on
three semantic parsing and code generation tasks.
In the future we plan to apply the reranker to
other parsers and more benchmark datasets. We
will also attempt to jointly train the base seman-
tic parser and the reranker by using the reranker’s
output as supervision to fine tune the base parser.

Acknowledgments

This research was supported by NSF Award No.
1815287 “Open-domain, Data-driven Code Syn-
thesis from Natural Language,” and a grant from
the Carnegie Bosch Institute.

References

Jianpeng Cheng and Mirella Lapata. 2018. Weakly-
supervised neural semantic parsing with a generative
ranker. In CoNLL.

Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder–decoder ap-
proaches. In Proceedings of SSST-8, Eighth Work-
shop on Syntax, Semantics and Structure in Statisti-
cal Translation.



4558

Michael Collins and Terry K Koo. 2000. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31.

Michael Brown William Fisher Kate Hunicke-Smith
David Pallett Christine Pao Alexander Rudnicky
Deborah A. Dahl, Madeleine Bates and Elizabeth
Shriber.

Li Dong and Mirella Lapata. 2016. Language to logical
form with neural attention. In Proceedings of ACL.

Li Dong and Mirella Lapata. 2018. coarse-to-fine de-
coding for neural semantic parsing. In Proceedings
of ACL.

Daniel Fried, Jacob Andreas, and Dan Klein. 2018.
Unified pragmatic models for generating and follow-
ing instructions. In Proceedings of NAACL.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K.
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
ACL.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and
Luke Zettlemoyer. 2018. Mapping language to
code in programmatic context. In Proceedings of
EMNLP.

Robin Jia and Percy Liang. 2016. Data recombination
for neural semantic parsing. In Proceedings of ACL.

Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gard-
ner. 2017. Neural semantic parsing with type con-
straints for semi-structured tables. In Proceedings
of EMNLP.

Chen Liang, Jonathan Berant, Quoc Le, Kenneth D.
Forbus, and Ni Lao. 2017. Neural symbolic ma-
chines: Learning semantic parsers on freebase with
weak supervision. In Proceedings of ACL.

Wang Ling, Phil Blunsom, Edward Grefenstette,
Karl Moritz Hermann, Tomás Kociský, Fumin
Wang, and Andrew Senior. 2016. Latent predictor
networks for code generation. In Proceedings of
ACL.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-
based neural machine translation. In Proceedings
of EMNLP.

Graham Neubig. 2013. Travatar: A forest-to-string
machine translation engine based on tree transduc-
ers. In Proceedings of the ACL Demonstration
Track.

Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL.

Yusuke Oda, Hiroyuki Fudaba, Graham Neubig,
Hideaki Hata, Sakriani Sakti, Tomoki Toda, and
Satoshi Nakamura. 2015. Learning to generate
pseudo-code from source code using statistical ma-
chine translation (T). In Proceedings of ASE.

Ankur Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proceed-
ings of EMNLP.

Maxim Rabinovich, Mitchell Stern, and Dan Klein.
2017. Abstract syntax networks for code generation
and semantic parsing. In Proceedings of ACL.

Ruifang Ge Raymond and J. Mooney. 2006. Discrim-
inative reranking for semantic parsing. In Proceed-
ings of COLING/ACL.

David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Pro-
ceedings of the COLING/ACL.

Shashank Srivastava, Amos Azaria, and Tom Michael
Mitchell. 2017. Parsing natural language conversa-
tions using contextual cues. In IJCAI.

Alane Suhr, Srinivasan Iyer, and Yoav Artzi. 2018.
Learning to map context-dependent sentences to ex-
ecutable formal queries. In Proceedings of NAACL-
HLT.

Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
coders. In ICML.

Adrienne Wang, Tom Kwiatkowski, and Luke Zettle-
moyer. 2014. Morpho-syntactic lexical generaliza-
tion for CCG semantic parsing. In Proceedings of
EMNLP.

Chunyang Xiao, Marc Dymetman, and Claire Gardent.
2016. Sequence-based structured prediction for se-
mantic parsing. In Proceedings of ACL.

Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan
Vasilescu, and Graham Neubig. 2018a. Learning to
mine aligned code and natural language pairs from
stack overflow. In Proceedings of MSR.

Pengcheng Yin and Graham Neubig. 2017. a syntactic
neural model for general-purpose code generation.
In Proceedings of ACL.

Pengcheng Yin and Graham Neubig. 2018. TRANX:
A transition-based neural abstract syntax parser for
semantic parsing and code generation. In Proceed-
ings of EMNLP Demonstration Track.

Pengcheng Yin, Chunting Zhou, Junxian He, and Gra-
ham Neubig. 2018b. StructVAE: Tree-structured la-
tent variable models for semi-supervised semantic
parsing. In Proceedings of ACL.

John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence - Volume
2.



4559

Victor Zhong, Caiming Xiong, and Richard Socher.
2017. Seq2SQL: Generating structured queries
from natural language using reinforcement learning.
arXiv preprint arXiv:1709.00103.


