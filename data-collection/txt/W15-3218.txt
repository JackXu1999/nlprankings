



















































QCRI@QALB-2015 Shared Task: Correction of Arabic Text for Native and Non-Native Speakers Errors


Proceedings of the Second Workshop on Arabic Natural Language Processing, pages 150–154,
Beijing, China, July 26-31, 2015. c©2014 Association for Computational Linguistics

QCRI@QALB-2015 Shared Task:Correction of Arabic Text for Native
and Non-Native Speakers’ Errors

Hamdy Mubarak, Kareem Darwish, Ahmed Abdelali
Qatar Computing Research Institute

Hamad Bin Khalifa University
Doha, Qatar

{hmubarak, kdarwish, aabdelali}@qf.org.qa

Abstract

This paper describes the error correc-
tion model that we used for the QALB-
2015 Automatic Correction of Arabic Text
shared task. We employed a case-specific
correction approach that handles specific
error types such as dialectal word substi-
tution and word splits and merges with the
aid of a language model. We also ap-
plied corrections that are specific to sec-
ond language learners that handle erro-
neous preposition selection, definiteness,
and gender-number agreement.

1 Introduction

In This paper, we provide a system description
for our submissions to the Arabic error correction
shared task (QALB-2015 Shared Task on Auto-
matic Correction of Arabic) as part of the Arabic
NLP workshop. The QALB-2015 shared task is
an extension of the first QALB shared task (Mo-
hit et al., 2014) which addressed errors in com-
ments written to Aljazeera articles by native Ara-
bic speakers (Zaghouani et al., 2014). The cur-
rent competition includes two tracks, and, in ad-
dition to errors produced by native speakers, also
includes correction of texts written by learners
of Arabic as a foreign language (L2) (Zaghouani
et al., 2015). The native track includes Alj-
train-2014, Alj-dev-2014, Alj-test-2014 texts from
QALB-2014. The L2 track includes L2-train-2015
and L2-dev-2015. This data was released for the
development of the systems. The systems were
scored on blind test sets Alj-test-2015 and L2-test-
2015.
We submitted runs to the automatic correction of
text generated by native speaker (L1) and non-
native speakers (L2). For both L1 and L2, we
employed a case-specific approach that is aided
by a language model (LM) to handle specific

error types such as dialectal word substitutions
and word splits. We also constructed a list of
corrections that we observed in the QALB-2014
data set and in the QALB-2015 training set. We
made use of these corrections to generate alter-
native corrections for words. When dealing with
L2 text, we noticed specific patterns of mistakes
mainly related to gender-number agreement, pho-
netic spellings, and definiteness. As for punctua-
tion recovery, we opted only to place periods at the
end of sentences and to correct reversed question
marks. We opted not to invest in punctuation re-
covery based on the mixed results we obtained for
the QALB-2014 shared task (Mubarak and Dar-
wish, 2014).

2 QALB L2 Corpus Error Analysis

The QALB corpus used for the task contains over
two million words of manually corrected Arabic
text. The corpus is composed of text that is pro-
duced by native speakers as well as non-native
speakers (Habash et al., 2013). While annotat-
ing the corpus, Zaghouani et al. (2014) detailed
various types of errors that were encountered and
addressed - mainly L1. Additional proposed cor-
rections for L2 errors were summarized with no
details. Understanding the error types would shed
light on their manifestations and help correct them
properly. We inspected the training and develop-
ment sets and noticed a number of potential issues
that can be summarized as follows:

1. Syntax Errors due to first language influence:
L2 learners may carry over rules from their na-
tive languages resulting in syntactic and mor-
phological errors, such as:

(a) Definiteness: In Arabic syntax, a possessive
case, idafa construct, which happens between
two words, mostly requires that the first word
be indefinite while the second be definite.
Such as the case of “

	YJ
ÒÊ�JË @ H. A�J»” (ktAb Al-

150



tlmy* 1 – ”The book of the student”). Note,
the first Arabic word doesn’t contain the defi-
nite article ”Al” while the second does. Er-
roneous application, or not, of the definite
article was common. For example, the stu-
dent may say: ”

	YJ
ÒÊ�K H. A�J»” (ktAb tlmy*) or
”

	YJ
ÒÊ�JË @ H. A�JºË@” (AlktAb Altlmy*).
(b) Gender-number agreement: Gender-number

agreement is another common error type.
The inflectional morphology of Arabic may:
embed gender-number markers in verbs as
in ” �é 	JK
YÖÏ @ ú


	æ�JJ.j. «

@” (>Ejbtny Almdynp – I

liked the city) and the learner may write
” �é 	JK
YÖÏ @ ú


	æJ.j. «

@” (>Ejbny Almdynp) with-

out the feminine marker; and the use of
feminine singular adjectives with masculine
plural inanimate nouns as in ” �éÒJ
 	¢« 	àYÓ”
(mdn EZymp – great cities) and the learner
may write ” 	àñÒJ
 	¢« 	àYÓ” (mdn EZymwn) or
” �HAÒJ
 	¢« 	àYÓ” (mdn EZymAt).

(c) Prepositions: Mixing the usage of prepo-
sitions is another typical challenge for L2
learners, as it requires good understanding of
spacio-temporal aspects of language. Thus,
L2 learners tend to mix between these prepo-
sitions as in “ �é 	JK
YÖÏ @ ú


	̄ �IÊð” (wSlt fy Alm-
dynp – I arrived in the city) instead of
” �é 	JK
YÖÏ @ úÍ@ �IÊð” (wSlt ¡lY Almdynp – I
arrived to the city).

2. Spelling errors: Grasping sounds is another
challenging issue particularly given:

(a) Letter that sound the same but written differ-
ently, such as ” �H” (t) and ” �è” (p), may lead
to erroneous spellings like ” �H@PAJ.Ó” (mbArAt
– game) instead of ” �è @PAJ.Ó” (mbArAp). Other
example letter pairs are ”” (S) and ”” (s)
and ” ” (T) and ” �H” (t)

(b) Letters that have similar shapes but a differ
number of dots on or below them. We noticed
that L2 learners often confuse letter such as:
”h. ” (j), ”h” (H), and ”p (x); and ”” (S)
and ” 	” (D). This may lead to errors such
as ” �HXA	mÌ'@ I. �. ” (Sbb AlxAdv) instead of

1Buckwalter transliteration

” �HXAmÌ'@ I. �. ” (sbb AlHAdv – the reason for
the accident).

3 Word Error Correction

In this section we describe our case-specific error
correction system that handles specific error types
with the aid of a language model (LM) generated
from an Aljazeera corpus. We built a word bigram
LM from a set of 234,638 Aljazeera articles2 that
span 10 years. Mubarak et al. (2010) reported that
spelling mistakes in Aljazeera articles are infre-
quent. We used this language model in all sub-
sequent steps.

We attempted to address specific types of er-
rors including dialectal words, word normalization
errors, and words that were erroneously split or
merged. Before applying any correction, we al-
ways consulted the LM. We handled the following
cases in order (L2 specific corrections are noted):
• Switching from English punctuation marks to

Arabic ones, namely changing: “?” → “?” and “,”
→ “,”.
• Correcting errors in definite article (È@ “Al”)

when it’s preceded by the preposition (È “l”) ex:
ÉÒªËB “lAlEml”→ ÉÒªÊË “llEml”.
• Handling common dialectal words and com-

mon word-level mistakes. To do so, we ex-
tracted all the errors and their corrections from the
QALB-2014 (train, dev, and test) and the training
split of the QALB-2015 data set. In all, we ex-
tracted 221,460 errors from this corpus. If an er-
ror had 1 seen correction and the correction was
done at least 2 times, we used the correction as a
deterministic correction. For example, the word
( �H@YgB@ “AlAHdAv” – the events) was found
86 times in this corpus, and in all cases it was
corrected to ( �H@Yg


B@ “Al > HdAv′′). There

were 10,255 such corrections. Further, we man-
ually revised words for which a specific correc-
tion was made in 60% or more of the cases (2,076
words) to extract a list of valid alternatives for each
word. For example, the word (PñÓB@ “AlAmwr”)
appeared 157 times and was corrected to (PñÓ


B@

“Al > mwr′′) in 99% of the cases. We ig-
nored the remaining seen corrections. An example
dialectal word is (ú
ÎË@ “Ally” – “this” or “that”)

2http://www.aljazeera.net

151



which could be mapped to (ø

	YË@ “Al*y”), (ú


�æË@
“Alty”), or ( 	áK


	YË @ “Al*yn”). An example of a com-
mon mistake is ( é<Ë @ Z A �	� @ “ > n$A′Allh′′ – “God
willing”) which is corrected to ( é<Ë @ Z A � 	à@ “>n
$A’ Allh”). When performing correction, given
a word appearing in our list, we either replaced
it deterministically if it had one correction, or we
consulted our LM to pick between the different al-
ternatives. When dealing with L2 data, we added
297 more deterministic errors (ex: Õç�'ð “wvm” was
always corrected to Õç�' “vm”).
• Handling split conjunctions (ð “w”) that

should be concatenated with the next word (ex:
¼A 	Jë ð “w HnAk”→ ¼A 	Jëð “wHnAk”).
• Handling errors pertaining to the different

forms of alef, alef maqsoura and ya, and ta
marbouta and ha as described in Table 1 and
Table 2. We used an approach similar to the
open suggested by Moussa et al. (Moussa et al.,
2012), and we also added the following cases,
namely attempting to replace: ð “&” with ð ð
“&w” or ñK “}w”; and ø “}” with Zø
 “y”’ or vice
versa (ex:  ðQÓ “mr&s” → ð ðQÓ “mr&ws”,
Zø
 PA

�̄ “qAry”’ → øPA�̄ “qAr}”). To generate
the alternatives for words, we normalized all
the unique words in the Aljazeera corpus, and
we constructed a reverse look-up table that has
the normalized form as the key and a list of
seen alternatives that could have generated the
normalized form. The look-up table contained
905k normalized word entries with corresponding
denormalized forms. When correcting, a word is
normalized and looked-up in the table to retrieve
possible alternatives. We used the LM to pick
the best alternative in context. Table 2 shows
examples from the look-up table for normalized
words and their alternative corrections.

• Removing repeated letters. Often people
repeat letters, particularly long vowels, for em-
phasis as in ( @ @ @Q�
J
�
J
 	k


@ “>xyyyyrAAA”) (meaning

“at last”). We corrected for elongation in a
manner similar to that of Darwish et al. (Darwish
et al., 2012). When a long vowel is repeated,
we replaced it with a either the vowel (ex.
@Q�
 	g


@ “>xyrA” – finally) or the vowel with one

repetition (ex. 	á�
K
Xñª “sEwdyyn” – Saudis)

and scored it using the LM. This was expanded
to consonants also (ex. PPPQ�
�JºK. “bkvyrrrr”
→ Q�
�JºK. “bkvyr”). If a repeated alef appeared
in the beginning of the word, we attempted to
replace it with alef lam (ex. �èPA 	k@@ “AAHDArp”
→ �èPA 	mÌ'@ “AlHDArp” – ”civilization”). A
trailing alef-hamza-alef sequence was replaced
by alef-hamza (ex. @ Z AÖÞ “smA’A” → ZAÖÞ “smA”’
(meaning “sky”)). Also, we replaced (ÉÊË “lll”)
at the beginning of word by (ÉË “ll”) (ex. �é 	ªÊÊË
“lllgp”→ �é 	ªÊË “llgp”).

• Handling grammar errors in verb suffixes to
restore missing alef (ex. ñÊª 	̄ @ “AfElw” → @ñÊª 	̄ @
“AfElwA” – do (plural); @ñÊª 	®J
 “syfElwA” →
	àñÊª 	®J
 “syfElwn” – they will do; 	àñ 	¢ 	®j�JË “ltH-

fZwn” → @ñ 	¢ 	®j�JË “ltHfZwA” – that you may
memorize/protect).
• Handling merges and splits. Often words are

concatenated erroneously. Thus, we attempted to
split all words that were at least 5 letters long after
letters that don’t change their shapes when they are
connected to the letters following them, namely
different alef forms, X “d”, 	X ”*“, P “r”, 	P “z”,
ð “w”, �è “p”, and ø “Y” (ex: A 	JK. PAK
 “yArbnA”
→ A 	JK. P AK
 “yA rbnA”). If the bigram was observed
in the LM and the LM score was higher (in con-
text) than when they were concatenated, then the
word was split. Conversely, some words were split
in the middle. We attempted to merge every two
words in sequence. If the LM score was higher
(in context) after the merge, then the two words
would be merged (ex: �H@ PA�J 	K @ “AntSAr At”→
�H@PA�J 	K @ “AntSArAt”).
• Correcting out-of-vocabulary (OOV) words.

For words that were not observed in the LM, we
attempted replacing phonetically or visually simi-
lar letters and deleting/replacing letters that appear
in dialectal words as shown in Table 3. Generated
suggestions are scored in context using the LM.
Many of these errors are common in the L2 data
set.
• For L2 data only, as we mentioned earlier

we observed errors pertaining to definiteness and
gender-number agreement. We generated possi-
ble corrections as follows: words that start with
definite article, we scored the word with and with-

152



out a definite article. We did the same with words
ending with ta marbouta (p). We also added other
alternatives for the word by adding the definite ar-
ticle and/or that ta marbouta (for words without
one or the other or neither). In all cases, we used
the LM to select the most probable alternative in
contexts.

Letter Norm. Example�
@ , @ ,


@ @ YÔg@ ← YÔg


@

>, <, | A AHmd← > Hmd
¨A 	J�̄ @ ← ¨A 	J�̄ @
AqnAE ← < qnAE
	áÓ@ ← 	áÓ

�
@

Amn← |mn
ø ø
 ø
 ñ

�̄ ← øñ�̄
Y y qSwy← qSwY�è è èXAJ
�̄ ← �èXAJ
�̄
p h qyAdh← qyAdp

ø , ð Z ÈðZÓ ← Èð ñÓ
&, } ’ ms&wl← ms’wl

diacritics null 	­�®�JÓ← 	­��®
��J �Ó

mvqf ←muvaq fK
kashida null Q�
J.»← Q�������J
J.»

kbyr← kby r
Table 1: Word Normalization.

4 Official Shared Task Experiments and
Results

We submitted 1 run for L1 errors (QCRI-1-ALJ),
and 2 runs for L2 errors (QCRI-1-L2, QCRI-2-L2)
as follows:

1. QCRI-1-ALJ: case-based correction for L1 test.

2. QCRI-2-L1: case-based correction for L2 test
file and also by adding alternatives for possible
errors in the definite article È@ ”Al” and femi-
nine mark �è ”p” as described in section 3.

3. QCRI-1-L2: case-based correction for L2 test
file with handling the definiteness or feminine
marker.

Table 4 and Table 5 report the officially submit-
ted results against the development set and test set
in order, and Table 6 reports the results of the new
system against the development set and test set of
QALB 2014 shared task.

Word Alternatives and Frequencies

ÐC«@ ÐC«@ 20352, ÐC«

@ 632, ÐC«@ 5

AElAm < ElAm 20352, > ElAm 632, AElAm 5
èPA 	k �èPA 	k 1271, èPA 	k 1

HDArh HDArp 1271, HDArh 1

Table 2: Word Alternatives.

Case Example
	  , 	 ¡�. A 	← ¡�. A 	£
Z, D DAbT← ZAbT	X , X I. ë

	YË@← I. ëYË@
d, * Al*hb← Aldhb
+ H. I. ªÊK
← I. ªÊJ
K.
b+ ylEb← bylEb
+ X I. ªÊK
← I. ªÊK
X
d+ ylEb← dylEb

+ è ,+ h I. �JºJ
← I. �JºJ
k
H+, h+ syktb← Hyktb
+ ÈAë �I	�J. Ë @ è

	Yë← �I 	�J. Ë Aë
hAl+ h*h Albnt← hAlbnt
+ ÈA« 	P


B@ úÎ«← 	P


BA«

EAl+ ElY AlArD← EAlArD
  , �H �éJ
 	J�
�KCË@← �éJ
 	�J
£CË@
t, T AllAtynyp← AllATynyp

p , h , h. 
	j�JË @← j�JË@

j, H, x AltxSS← AltHSS
¼ , � è@Pñ�J»X← è @Pñ�J�̄ X
q, k dktwrAh← dqtwrAh

ø
 + . . . + È@ h. Q
	j�JË @← ú
k. Q

	j�JË @
Al+ ... +y Altxrj← Altxrjy

Table 3: Phonetic, Dialectal, and L2 Errors

5 Conclusion

In this paper, we presented an automatic approach
for correcting Arabic text based on handling spe-
cific error types. We handled common dialectal
words, some dialectal morphological features, let-
ter normalization errors (ex. alef, ta marbouta,
etc.), and word splitting and merging. For the
L2 corpus, we also corrected letters that L2 learn-
ers often confuse because of similarity in shape or
sound, and we attempted to correct errors pertain-
ing to definiteness and gender-number agreement.
For punctuation recovery, we opted to put periods
at the end of sentences. Preliminary experiments
using fuzzy match using a character-based mod-

153



Run P R F1
QCRI-1-ALJ 84.2 49.8 62.6
(Alj-dev-2015)
QCRI-1-L2 46.3 19.2 27.1
(L2-dev-2015)
QCRI-2-L2 57.6 16.3 25.4
(L2-dev-2015)

Table 4: Official Results for Dev. Data

Run P R F1
QCRI-1-ALJ 84.74 58.10 68.94
(Alj-test-2015)
QCRI-1-L2 45.86 20.16 28.01
(L2-test-2015)
QCRI-2-L2 54.87 17.63 26.69
(L2-test-2015)

Table 5: Official Results for Test Data

els showed promising results(Sajjad et al., 2012;
Durrani et al., 2014; Darwish et al., 2014). We in-
tend to incorporate this development among others
in our on-going research. The fuzzy match algo-
rithm will correct cases like: ( Aî 	Eñ 	KY 	j�J�
 , ZAªK.


B@,

Al>bEA’ , ystxdnwnhA) to ( Aî 	EñÓY 	j�J�
 , ZAªK. P

B@,

Al<rbEA’ , ystxdmwnhA).
L2 learners present new spelling error types.

Such types may not typical spelling errors as they
may produce valid words that are erroneous in
context. Hence employing a methodology to de-
tect such cases will be of great help.Also, we
plan to handle more grammar errors for cases like:
numbers, case endings, gender-number agree-
ment, irregular (broken) plurals, and Tanween er-
rors (

	¬QåË@ 	áÓ ¨ñ	JÒÖÏ @).

References
Kareem Darwish, Walid Magdy, and Ahmed Mourad.

2012. Language processing for arabic microblog
retrieval. In Proceedings of the 21st ACM inter-
national conference on Information and knowledge
management, pages 2427–2430. ACM.

Kareem Darwish, Ahmed M. Ali, and Ahmed Abde-
lali. 2014. Query term expansion by automatic
learning of morphological equivalence patterns from
wikipedia. In SIGIR 2014 Workshop on Semantic
Matching in Information Retrieval (SMIR), volume
1204, pages 24–29. CEUR-WS.

Nadir Durrani, Hassan Sajjad, Hieu Hoang, and Philipp
Koehn. 2014. Integrating an unsupervised translit-

Run P R F1
Alj-dev-2014 65.42 62.96 64.17
Alj-test-2014 65.79 61.94 63.81

Table 6: Results for QALB 2014 Data Sets

eration model into statistical machine translation. In
Proceedings of the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics, volume 2: Short Papers, pages 148–153,
Gothenburg, Sweden, April. Association for Com-
putational Linguistics.

Nizar Habash, Behrang Mohit, Ossama Obeid, Kemal
Oflazer, Nadi Tomeh, and Wajdi Zaghouani. 2013.
Qalb: Qatar arabic language bank. In Proceedings
of Qatar Annual Research Conference (ARC-2013),
Doha, Qatar.

Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wa-
jdi Zaghouani, and Ossama Obeid. 2014. The First
QALB Shared Task on Automatic Text Correction
for Arabic. In Proceedings of EMNLP Workshop on
Arabic Natural Language Processing, Doha, Qatar,
October.

Mohammed Moussa, Mohamed Waleed Fakhr, and Ka-
reem Darwish. 2012. Statistical denormalization for
arabic text. In In Empirical Methods in Natural Lan-
guage Processing.

Hamdy Mubarak and Kareem Darwish. 2014. Auto-
matic correction of arabic text: a cascaded approach.
Arabic NLP 2014 Workshop.

Hamdy Mubarak, Mostafa Ramadan, and Ahmed Met-
wali. 2010. Spelling mistakes in arabic newspa-
pers. In Arabic Language and Scientific Researches
conference, Faculty of Arts, Ain Shams University,
Cairo, Egypt.

Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2012. A statistical model for unsupervised and
semi-supervised transliteration mining. In Proceed-
ings of the Association for Computational Linguis-
tics, ACL ’12, pages 469–477, Jeju, Korea.

Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-
sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura
Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014.
Large scale arabic error annotation: Guidelines and
framework. In Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC’14), Reykjavik, Iceland, May.

Wajdi Zaghouani, Nizar Habash, Houda Bouamor, Alla
Rozovskaya, Behrang Mohit, Abeer Heider, and Ke-
mal Oflazer. 2015. Correction annotation for non-
native arabic texts: Guidelines and corpus. In Pro-
ceedings of The 9th Linguistic Annotation Work-
shop, pages 129–139, Denver, Colorado, USA, June.
Association for Computational Linguistics.

154


