











































DENS: A Dataset for Multi-class Emotion Analysis


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 6293–6298,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

6293

DENS: A Dataset for Multi-class Emotion Analysis

Chen Liu and Muhammad Osama and Anderson de Andrade
Wattpad

Toronto, ON, Canada
cecilia, muhammad.osama, anderson@wattpad.com

Abstract

We introduce a new dataset for multi-class
emotion analysis from long-form narratives in
English. The Dataset for Emotions of Nar-
rative Sequences (DENS) was collected from
both classic literature available on Project
Gutenberg and modern online narratives avail-
able on Wattpad, annotated using Amazon
Mechanical Turk. A number of statistics
and baseline benchmarks are provided for the
dataset. Of the tested techniques, we find
that the fine-tuning of a pre-trained BERT
model achieves the best results, with an av-
erage micro-F1 score of 60.4%. Our results
show that the dataset provides a novel opportu-
nity in emotion analysis that requires moving
beyond existing sentence-level techniques.

1 Introduction

Humans experience a variety of complex emotions
in daily life. These emotions are heavily reflected
in our language, in both spoken and written forms.

Many recent advances in natural language pro-
cessing on emotions have focused on product re-
views (McAuley et al., 2015) and tweets (Mo-
hammad et al., 2018; Kant et al., 2018). These
datasets are often limited in length (e.g. by the
number of words in tweets), purpose (e.g. prod-
uct reviews), or emotional spectrum (e.g. binary
classification).

Character dialogues and narratives in story-
telling usually carry strong emotions. A memo-
rable story is often one in which the emotional
journey of the characters resonates with the reader.
Indeed, emotion is one of the most important as-
pects of narratives. In order to characterize narra-
tive emotions properly, we must move beyond bi-
nary constraints (e.g. good or bad, happy or sad).

In this paper, we introduce the Dataset for Emo-
tions of Narrative Sequences (DENS) for emotion
analysis, consisting of passages from long-form

fictional narratives from both classic literature and
modern stories in English. The data samples con-
sist of self-contained passages that span several
sentences and a variety of subjects. Each sample
is annotated by using one of 9 classes and an indi-
cator for annotator agreement.

2 Background

Using the categorical basic emotion
model (Plutchik, 1979), (Mohammad and
Kiritchenko, 2015; Mohammad, 2012) studied
creating lexicons from tweets for use in emotion
analysis. Recently, (Mohammad et al., 2018),
(Klinger et al., 2018) and (Kant et al., 2018)
proposed shared-tasks for multi-class emotion
analysis based on tweets.

Fewer works have been reported on understand-
ing emotions in narratives. Emotional Arc (Rea-
gan et al., 2016) is one recent advance in this
direction. The work used lexicons and unsuper-
vised learning methods based on unlabelled pas-
sages from titles in Project Gutenberg1.

For labelled datasets on narratives, (Alm et al.,
2005) provided a sentence-level annotated cor-
pus of childrens’ stories and (Kim and Klinger,
2018) provided phrase-level annotations on se-
lected Project Gutenberg titles.

To the best of our knowledge, the dataset in
this work is the first to provide multi-class emo-
tion labels on passages, selected from both Project
Gutenberg and modern narratives. The dataset
is available upon request for non-commercial, re-
search only purposes2.

3 Dataset

In this section, we describe the process used to col-
lect and annotate the dataset.

1
https://www.gutenberg.org/

2Please send requests to: academic dataset@wattpad.com

https://www.gutenberg.org/


6294

3.1 Plutchik’s Wheel of Emotions

The dataset is annotated based on a modified
Plutchik’s wheel of emotions.

The original Plutchik’s wheel consists of 8 pri-
mary emotions: Joy, Sadness, Anger, Fear, Antici-
pation, Surprise, Trust, Disgust. In addition, more
complex emotions can be formed by combing two
basic emotions. For example, Love is defined as a
combination of Joy and Trust (Fig. 1).

Figure 1: Plutchik’s wheel of emotions (Wikimedia,
2011)

The intensity of an emotion is also captured in
Plutchik’s wheel. For example, the primary emo-
tion of Anger can vary between Annoyance (mild)
and Rage (intense).

We conducted an initial survey based on 100
stories with a significant fraction sampled from
the romance genre. We asked readers to identify
the major emotion exhibited in each story from a
choice of the original 8 primary emotions.

We found that readers have significant difficulty
in identifying Trust as an emotion associated with
romantic stories. Hence, we modified our annota-
tion scheme by removing Trust and adding Love.
We also added the Neutral category to denote pas-
sages that do not exhibit any emotional content.

The final annotation categories for the dataset
are: Joy, Sadness, Anger, Fear, Anticipation, Sur-
prise, Love, Disgust, Neutral.

3.2 Passage Selection

We selected both classic and modern narratives in
English for this dataset. The modern narratives
were sampled based on popularity from Wattpad.
We parsed selected narratives into passages, where
a passage is considered to be eligible for annota-
tion if it contained between 40 and 200 tokens.

In long-form narratives, many non-
conversational passages are intended for transition
or scene introduction, and may not carry any
emotion. We divided the eligible passages into
two parts, and one part was pruned using selected
emotion-rich but ambiguous lexicons such as cry,
punch, kiss, etc.. Then we mixed this pruned part
with the unpruned part for annotation in order
to reduce the number of neutral passages. See
Appendix A.1 for the lexicons used.

3.3 Mechanical Turk (MTurk)

MTurk was set up using the standard sentiment
template and instructed the crowd annotators to
‘pick the best/major emotion embodied in the pas-
sage’.

We further provided instructions to clar-
ify the intensity of an emotion, such as:
“Rage/Annoyance is a form of Anger”, “Seren-
ity/Ecstasy is a form of Joy”, and “Love includes
Romantic/Family/Friendship”, along with sample
passages.

We required all annotators have a ‘master’
MTurk qualification. Each passage was labelled
by 3 unique annotators. Only passages with a
majority agreement between annotators were ac-
cepted as valid. This is equivalent to a Fleiss’s 
score of greater than 0.4.

For passages without majority agreement be-
tween annotators, we consolidated their labels us-
ing in-house data annotators who are experts in
narrative content. A passage is accepted as valid
if the in-house annotator’s label matched any one
of the MTurk annotators’ labels. The remaining
passages are discarded. We provide the fraction of
annotator agreement for each label in the dataset.

Though passages may lose some emotional con-
text when read independently of the complete nar-
rative, we believe annotator agreement on our
dataset supports the assertion that small excerpts
can still convey coherent emotions.

During the annotation process, several anno-
tators had suggested for us to include additional
emotions such as confused, pain, and jealousy,



6295

Genre Distribution (%)
Mystery/Thriller 19.7
Paranormal 16.6
Fantasy 13.2
Horror 11.3
Romance 8.7
Action/Adventure 5.3
Other 9.3

Table 1: Genre distribution of the modern narratives

which are common to narratives. As they were not
part of the original Plutchik’s wheel, we decided
to not include them. An interesting future direc-
tion is to study the relationship between emotions
such as ‘pain versus sadness’ or ‘confused versus
surprise’ and improve the emotion model for nar-
ratives.

3.4 Dataset Statistics
The dataset contains a total of 9710 passages, with
an average of 6.24 sentences per passage, 16.16
words per sentence, and an average length of 86
words.

The vocabulary size is 28K (when lowercased).
It contains over 1600 unique titles across multi-
ple categories, including 88 titles (1520 passages)
from Project Gutenberg. All of the modern nar-
ratives were written after the year 2000, with no-
table amount of themes in coming-of-age, strong-
female-lead, and LGBTQ+. The genre distribution
is listed in Table 1.

In the final dataset, 21.0% of the data has con-
sensus between all annotators, 73.5% has major-
ity agreement, and 5.48% has labels assigned after
consultation with in-house annotators.

The distribution of data points over labels with
top lexicons (lower-cased, normalized) is shown
in Table 2. Note that the Disgust category is very
small and should be discarded. Furthermore, we
suspect that the data labelled as Surprise may be
noisier than other categories and should be dis-
carded as well.

Table 3 shows a few examples labelled data
from classic titles. More examples can be found
in Table 6 in the Appendix A.2.

4 Benchmarks

We performed benchmark experiments on the
dataset using several different algorithms. In all
experiments, we have discarded the data labelled

with Surprise and Disgust.
We pre-processed the data by using the SpaCy3

pipeline. We masked out named entities with
entity-type specific placeholders to reduce the
chance of benchmark models utilizing named en-
tities as a basis for classification.

Benchmark results are shown in Table 4. The
dataset is approximately balanced after discarding
the Surprise and Disgust classes. We report the
average micro-F1 scores, with 5-fold cross valida-
tion for each technique.

We provide a brief overview of each bench-
mark experiment below. Among all of the
benchmarks, Bidirectional Encoder Representa-
tions from Transformers (BERT) (Devlin et al.,
2018) achieved the best performance with a 0.604
micro-F1 score.

Overall, we observed that deep-learning based
techniques performed better than lexical based
methods. This suggests that a method which at-
tends to context and themes could do well on the
dataset.

4.1 Bag-of-Words-based Benchmarks

We computed bag-of-words-based benchmarks
using the following methods:

• Classification with TF-IDF + Linear SVM
(TF-IDF + SVM)

• Classification with Depeche++ Emotion lex-
icons (Araque et al., 2018) + Linear SVM
(Depeche + SVM)

• Classification with NRC Emotion lexicons
(Mohammad and Turney, 2010, 2013) + Lin-
ear SVM (NRC + SVM)

• Combination of TF-IDF and NRC Emotion
lexicons (TF-NRC + SVM)

4.2 Doc2Vec + SVM

We also used simple classification models with
learned embeddings. We trained a Doc2Vec
model (Le and Mikolov, 2014) using the dataset
and used the embedding document vectors as fea-
tures for a linear SVM classifier.

4.3 Hierarchical RNN

For this benchmark, we considered a Hierarchical
RNN, following (Sordoni et al., 2015). We used
two BiLSTMs (Graves et al., 2005) with 256 units

3
https://spacy.io/

https://spacy.io/


6296

Label Gutenberg Total Top Lexicons
Neutral 318 1711 take, love, long, really, want, always, though, away, look
Fear 159 1412 left, behind, right, want, let, death, go, say, think
Sadness 195 1402 father, always, little, look, something, us, really, mother, think
Anger 192 1306 feel, much, well, man, look, us, say, something, love
Joy 241 1266 see, always, let, long, make, hand, away, get, really
Love 162 1157 hand, know, right, let, happy, get, ever, us, look
Anticipation 147 1020 know, long, life, make, get, think, blood, want, feel
Surprise 102 362 love, find, looking, know, well, much, something, door, really
Disgust 4 74 get, hand, inside, let, hate, table, men, always, make

Table 2: Dataset label distribution

Text Label
I found this was a little too close upon him, but I made it up in what follows.

He stood stock-still for a while and said nothing, and I went on thus: “You

cannot,” says I, “without the highest injustice, believe that I yielded upon all

these persuasions without a love not to be questioned, not to be shaken again

by anything that could happen afterward. If you have such dishonourable

thoughts of me, I must ask you what foundation in any of my behaviour have I

given for such a suggestion?”

Angry

She stretched hers eagerly and gratefully towards him. What had happened?

Through all the numbness of her blood, there sprang a strange new warmth

from his strong palm, and a pulse, which she had almost forgotten as a dream

of the past, began to beat through her frame. She turned around all a-tremble,

and saw his face in the glow of the coming day.

Anticipation

Ah! That moving procession that has left me by the road-side! Its fantastic

colors are more brilliant and beautiful than the sun on the undulating waters.

What matter if souls and bodies are failing beneath the feet of the ever-pressing

multitude! It moves with the majestic rhythm of the spheres. Its discordant

clashes sweep upward in one harmonious tone that blends with the music of

other worlds–to complete God’s orchestra.

Joy

Table 3: Sample data from classic titles

Model micro-F1
TF-IDF + SVM 0.450
Depeche + SVM 0.254
NRC + SVM 0.286
TF-NRC + SVM 0.458
Doc2Vec + SVM 0.403
HRNN 0.469
BiRNN + Self-Attention 0.487
ELMo + BiRNN 0.516
Fine-tuned BERT 0.604

Table 4: Benchmark results (averaged 5-fold cross val-
idation)

each to model sentences and documents. The to-
kens of a sentence were processed independently
of other sentence tokens. For each direction in the

token-level BiLSTM, the last outputs were con-
catenated and fed into the sentence-level BiLSTM
as inputs.

The outputs of the BiLSTM were connected to 2
dense layers with 256 ReLU units and a Softmax
layer. We initialized tokens with publicly avail-
able embeddings trained with GloVe (Pennington
et al., 2014). Sentence boundaries were provided
by SpaCy. Dropout was applied to the dense hid-
den layers during training.

4.4 Bi-directional RNN and Self-Attention
(BiRNN + Self-Attention)

One challenge with RNN-based solutions for text
classification is finding the best way to combine
word-level representations into higher-level repre-
sentations.



6297

Self-attention (Yang et al., 2016; Lin et al.,
2017; Sinha et al., 2018) has been adapted to text
classification, providing improved interpretability
and performance. We used (Lin et al., 2017) as the
basis of this benchmark.

The benchmark used a layered Bi-directional
RNN (60 units) with GRU cells and a dense layer.
Both self-attention layers were 60 units in size and
cross-entropy was used as the cost function.

Note that we have omitted the orthogonal reg-
ularizer term, since this dataset is relatively small
compared to the traditional datasets used for train-
ing such a model. We did not observe any signifi-
cant performance gain while using the regularizer
term in our experiments.

4.5 ELMo embedding and Bi-directional
RNN (ELMo + BiRNN)

Deep Contextualized Word Representations
(ELMo) (Peters et al., 2018) have shown recent
success in a number of NLP tasks. The unsuper-
vised nature of the language model allows it to
utilize a large amount of available unlabelled data
in order to learn better representations of words.

We used the pre-trained ELMo model (v2)
available on Tensorhub4 for this benchmark. We
fed the word embeddings of ELMo as input into a
one layer Bi-directional RNN (16 units) with GRU
cells (with dropout) and a dense layer. Cross-
entropy was used as the cost function.

4.6 Fine-tuned BERT

Bidirectional Encoder Representations from
Transformers (BERT) (Devlin et al., 2018) has
achieved state-of-the-art results on several NLP
tasks, including sentence classification.

We used the fine-tuning procedure outlined in
the original work to adapt the pre-trained uncased
BERTLARGE5 to a multi-class passage classifica-
tion task. This technique achieved the best result
among our benchmarks, with an average micro-F1
score of 60.4%.

5 Conclusion

We introduce DENS, a dataset for multi-class
emotion analysis from long-form narratives in En-
glish. We provide a number of benchmark results

4
https://tfhub.dev/google/elmo/2

5
https://tfhub.dev/google/bert_

uncased_L-24_H-1024_A-16/1

based on models ranging from bag-of-word mod-
els to methods based on pre-trained language mod-
els (ELMo and BERT).

Our benchmark results demonstrate that this
dataset provides a novel challenge in emotion
analysis. The results also demonstrate that
attention-based models could significantly im-
prove performance on classification tasks such as
emotion analysis.

Interesting future directions for this work in-
clude: 1. incorporating common-sense knowledge
into emotion analysis to capture semantic context
and 2. using few-shot learning to bootstrap and
improve performance of underrepresented emo-
tions.

Finally, as narrative passages often involve in-
teractions between multiple emotions, one avenue
for future datasets could be to focus on the multi-
emotion complexities of human language and their
contextual interactions.

References
Cecilia Alm, Dan Roth, and Richard Sproat. 2005.

Emotions from text: Machine learning for text-based
emotion prediction.

Oscar Araque, Lorenzo Gatti, Jacopo Staiano, and
Marco Guerini. 2018. Depechemood++: a bilingual
emotion lexicon built through simple yet powerful
techniques.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Alex Graves, Santiago Fernández, and Jürgen Schmid-
huber. 2005. Bidirectional lstm networks for im-
proved phoneme classification and recognition. In
International Conference on Artificial Neural Net-

works, pages 799–804. Springer.

Neel Kant, Raul Puri, Nikolai Yakovenko, and Bryan
Catanzaro. 2018. Practical text classification with
large pre-trained language models. arXiv preprint
arXiv:1812.01207.

Evgeny Kim and Roman Klinger. 2018. Who feels
what and why? annotation of a literature corpus
with semantic roles of emotions. In Proceedings of
the 27th International Conference on Computational

Linguistics, pages 1345–1359. Association for Com-
putational Linguistics.

Roman Klinger, Orphee De Clercq, Saif Mohammad,
and Alexandra Balahur. 2018. Iest: Wassa-2018
implicit emotions shared task. In Proceedings of
the 9th Workshop on Computational Approaches to

https://tfhub.dev/google/elmo/2
https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16/1
https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16/1
https://doi.org/10.3115/1220575.1220648
https://doi.org/10.3115/1220575.1220648
https://arxiv.org/abs/1810.03660
https://arxiv.org/abs/1810.03660
https://arxiv.org/abs/1810.03660
http://aclweb.org/anthology/C18-1114
http://aclweb.org/anthology/C18-1114
http://aclweb.org/anthology/C18-1114
http://aclweb.org/anthology/W18-6206
http://aclweb.org/anthology/W18-6206


6298

Subjectivity, Sentiment and Social Media Analysis,
pages 31–42, Brussels, Belgium. Association for
Computational Linguistics.

Quoc V. Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. CoRR,
abs/1405.4053.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding.

Julian McAuley, Rahul Pandey, and Jure Leskovec.
2015. Inferring networks of substitutable and com-
plementary products. In Proceedings of the 21th
ACM SIGKDD international conference on knowl-

edge discovery and data mining, pages 785–794.
ACM.

Saif M. Mohammad. 2012. #emotional tweets. In
*SEM 2012: The First Joint Conference on Lexical

and Computational Semantics – Volume 1: Proceed-

ings of the main conference and the shared task, and

Volume 2: Proceedings of the Sixth International

Workshop on Semantic Evaluation (SemEval 2012),
pages 246–255, Montréal, Canada. Association for
Computational Linguistics.

Saif M. Mohammad, Felipe Bravo-Marquez, Mo-
hammad Salameh, and Svetlana Kiritchenko. 2018.
Semeval-2018 task 1: Affect in tweets. In Pro-
ceedings of the International Workshop on Semantic

Evaluation (SemEval-2018). Association for Com-
putational Linguistics.

Saif M. Mohammad and Svetlana Kiritchenko. 2015.
Using hashtags to capture fine emotion cate-
gories from tweets. Computational Intelligence,
31(2):301–326.

Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Us-
ing mechanical turk to create an emotion lexicon. In
Proceedings of the NAACL HLT 2010 Workshop on

Computational Approaches to Analysis and Gener-

ation of Emotion in Text, CAAGET ’10, pages 26–
34, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Saif M. Mohammad and Peter D. Turney. 2013.
Crowdsourcing a word-emotion association lexicon.
29(3):436–465.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-

cessing (EMNLP), pages 1532–1543.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proc. of NAACL.

Robert Plutchik. 1979. Emotions: A general psycho-
evolutionary theory, volume 1. Psychology Press,
Taylor and Francis Group.

Andrew J. Reagan, Lewis Mitchell, Dilan Kiley,
Christopher M. Danforth, and Peter Sheridan Dodds.
2016. The emotional arcs of stories are dominated
by six basic shapes. EPJ Data Science, 5:31.

Koustuv Sinha, Yue Dong, Jackie Chi Kit Cheung, and
Derek Ruths. 2018. A hierarchical neural attention-
based text classifier. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-

guage Processing, pages 817–823. Association for
Computational Linguistics.

Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi,
Christina Lioma, Jakob Grue Simonsen, and Jian-
Yun Nie. 2015. A hierarchical recurrent encoder-
decoder for generative context-aware query sugges-
tion. In Proceedings of the 24th ACM International
on Conference on Information and Knowledge Man-

agement, pages 553–562. ACM.

Wikimedia. 2011. Robert plutchik’s wheel of emo-
tions. File: Plutchik-wheel.svg.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Compu-

tational Linguistics: Human Language Technolo-

gies, pages 1480–1489. Association for Computa-
tional Linguistics.

http://arxiv.org/abs/1405.4053
http://arxiv.org/abs/1405.4053
https://openreview.net/forum?id=BJC_jUqxe
https://openreview.net/forum?id=BJC_jUqxe
http://www.aclweb.org/anthology/S12-1033
http://aclweb.org/anthology/P16-1001
https://doi.org/10.1111/coin.12024
https://doi.org/10.1111/coin.12024
http://dl.acm.org/citation.cfm?id=1860631.1860635
http://dl.acm.org/citation.cfm?id=1860631.1860635
http://dl.acm.org/citation.cfm?id=1860631.1860635
http://aclweb.org/anthology/D18-1094
http://aclweb.org/anthology/D18-1094
https://commons.wikimedia.org/wiki/File:Plutchik-wheel.svg
https://commons.wikimedia.org/wiki/File:Plutchik-wheel.svg
https://doi.org/10.18653/v1/N16-1174
https://doi.org/10.18653/v1/N16-1174

