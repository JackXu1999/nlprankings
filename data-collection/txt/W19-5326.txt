



















































Multi-Source Transformer for Kazakh-Russian-English Neural Machine Translation


Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 267–274
Florence, Italy, August 1-2, 2019. c©2019 Association for Computational Linguistics

267

Multi-Source Transformer for
Kazakh-Russian-English Neural Machine Translation

Patrick Littell Chi-kiu Lo Samuel Larkin Darlene Stewart
NRC-CNRC

National Research Council of Canada
1200 Montreal Road, Ottawa, Ontario K1A 0R6, Canada

{Patrick.Littell|Chikiu.Lo|Samuel.Larkin|Darlene.Stewart}@nrc-cnrc.gc.ca

Abstract

We describe the neural machine translation
(NMT) system developed at the National Re-
search Council of Canada (NRC) for the
Kazakh-English news translation task of the
Fourth Conference on Machine Translation
(WMT19). Our submission is a multi-source
NMT system taking both the original Kazakh
sentence and its Russian translation as input
for translating into English.

1 Introduction

The WMT19 (Bojar et al., 2019) Kazakh-English
News Translation task presented a machine trans-
lation scenario in which parallel resources be-
tween the two languages (˜200k sentences) were
considerably fewer than parallel resources be-
tween these languages and a third language, Rus-
sian (˜14M English-Russian sentence pairs and
˜5M Kazakh-Russian pairs).

The NRC team therefore explored machine
translation pipelines that utilized the Russian re-
sources, including:

1. “Pivoting” through Russian: training an MT
system from Kazakh to Russian, and another
system from Russian to English (Fig. 1a).

2. Creating a synthetic Kazakh-English paral-
lel corpus by training a Russian-Kazakh MT
system and using it to “cross-translate”1 the
Russian-English corpus (Fig. 1b).

3. Training a multi-encoder (Libovický and
Helcl, 2017; Libovický et al., 2018) Trans-
former system (Vaswani et al., 2017) from

1We term synthetic data creation by translation between
source languages “cross-translation” to distinguish it from
“back-translation” in the sense of Sennrich et al. (2016).
Nishimura et al. (2018), which also uses source1-to-source2
translation, calls both kinds of synthetic data creation “back-
translation”, but because our pipeline uses both kinds we dis-
tinguish them with separate terms.

Kazakh/Russian to English that subsumes
both of these approaches (Fig. 1c).

Techniques (1) and (2) both involve the trans-
lation of genuine data into a synthetic translation
(into Russian in the first case, and into Kazakh in
the second case). It is, however, possible to attend
to both the original sentence and its translation
using multi-source techniques (Zoph and Knight,
2016; Libovický and Helcl, 2017; Nishimura et al.,
2018); we hypothesized that giving the system
both the originals and “cross-translations”, in
both directions (Kazakh-to-Russian and Russian-
to-Kazakh), would allow the system to make use
of the additional information available by seeing
the sources before translation.

Our multi-encoder Transformer approach per-
formed best among our submitted systems by a
considerable margin, outperforming pivoting by
4.2 BLEU and augmentation by one-way cross-
translation by 10.2 BLEU.2

2 Multilingual data

2.1 Kazakh-English
The raw bilingual Kazakh-English data provided
for the constrained news translation task consists
of web-crawled data, news commentary data and
Wikipedia article titles. In total, they account for
˜200k sentence pairs. All these data were used to
train the foundation systems for back-translation.
Since the web-crawled data is very noisy, we re-
moved all the web-crawled portion from the train-
ing data before training our final submitted sys-
tem.

For tuning and evaluating, we used the
newsdev2019-kken data set; for SMT, we

2However, these systems, as submitted, are not directly
comparable due to some additional data filtering in our final
submitted system; we will be releasing more direct compar-
isons and a more thorough description of the architecture in a
companion article.



268

(a) “Pivoting”: two systems (source-
to-L3 and L3-to-target) executed in a
pipeline

(b) Augmentation of source/target
corpus with “cross-translated” syn-
thetic data

(c) Multi-source system with augmen-
tation by cross-translation in both di-
rections

Figure 1: Approaches to utilizing a third language (“L3”) in machine translation.

split it into two sets as our internal dev and dev-
test; dev contains 1266 sentence pairs and devtest
contains the remaining 800 sentence pairs.

2.2 Kazakh-Russian

The raw bilingual Kazakh-Russian data provided
to assist in the news translation task is web-
crawled data. In total, they account for ˜5M sen-
tence pairs. All these data were used to train the
foundation systems for cross-translation.

For tuning and evaluating, we randomly se-
lected 1000 sentence pairs each for the dev and
devtest sets from the provided bilingual data. The
remaining bilingual data is de-duplicated against
the bag of 6-grams collected from the dev and de-
vtest sets. The de-duplicated bilingual data has
˜4.2M sentence pairs.

2.3 Russian-English

The raw bilingual Russian-English data we used
in our systems consists of web-crawled data, news
commentary data and Wikipedia article titles. In
total they account for ˜14M sentence pairs. All
these data were used to train the foundation sys-
tems for back-translation. Since the Paracrawl
portion of the bilingual data is very noisy, be-
fore training our final submitted system we ran our
parallel corpus filtering pipeline (Lo et al., 2018)
with YiSi-2 as the scoring function (instead of MT
+ YiSi-1) and trimmed the size of the Paracrawl
portion from 12M sentence pairs to 4M sentence
pairs.

For tuning and evaluating, we used the
newstest2017-enru data set as the dev set
and the newstest2018-enru data set as the
devtest set.

3 Data preparation

3.1 Cleaning and tokenization

Our preprocessing pipeline begins by cleaning the
UTF-8 with both Moses’ cleaning script3 and an
in-house script that performs additional white-
space, hyphen, and control character normaliza-
tion. We then proceed to normalize and tokenize
the sentences with Moses’ punctuation normaliza-
tion4 and tokenization scripts5.

3.2 Transliteration

To mitigate some of the overall complexity, and
allow greater sharing in joint BPE models and
weight tying, we first converted the Kazakh and
Russian text from Cyrillic to Roman, using offi-
cial Romanization standards using spm normalize
(Kudo, 2018) and transliteration tables from Wik-
tionary for Kazakh6 and Russian7.

3.3 Byte-pair encoding

Our BPE model is a joint one across transliter-
ated Kazakh, transliterated Russian, and English.
Using fastBPE8, we created a 90k-operation BPE
model, balancing the three languages with ˜8.2M
sentences of each, using:

• all available Kazakh from bilinugual kk-en;

• all available Kazakh from bilinugual kk-ru;
3github.com/moses-smt/

mosesdecoder/scripts/tokenizer/
remove-non-printing-char.perl

4github.com/moses-smt/mosesdecoder/
scripts/tokenizer/normalize-punctuation.
perl

5github.com/moses-smt/mosesdecoder/
scripts/tokenizer/tokenizer.perl

6en.wiktionary.org/wiki/Module:
kk-translit

7en.wiktionary.org/wiki/Module:
ru-translit

8github.com/glample/fastBPE

github.com/moses-smt/mosesdecoder/scripts/tokenizer/remove-non-printing-char.perl
github.com/moses-smt/mosesdecoder/scripts/tokenizer/remove-non-printing-char.perl
github.com/moses-smt/mosesdecoder/scripts/tokenizer/remove-non-printing-char.perl
github.com/moses-smt/mosesdecoder/scripts/tokenizer/normalize-punctuation.perl
github.com/moses-smt/mosesdecoder/scripts/tokenizer/normalize-punctuation.perl
github.com/moses-smt/mosesdecoder/scripts/tokenizer/normalize-punctuation.perl
github.com/moses-smt/mosesdecoder/scripts/tokenizer/tokenizer.perl
github.com/moses-smt/mosesdecoder/scripts/tokenizer/tokenizer.perl
en.wiktionary.org/wiki/Module:kk-translit
en.wiktionary.org/wiki/Module:kk-translit
en.wiktionary.org/wiki/Module:ru-translit
en.wiktionary.org/wiki/Module:ru-translit
github.com/glample/fastBPE


269

• all monolingual Kazakh news and wiki data;
• all available English from bilingual kk-en;
• a sample of ˜8M English sentences from

bilingual ru-en and monolingual en;

• all available Russian from bilinugual kk-ru;
• a sample of ˜3.2M Russian sentences from

bilingual ru-en and monolingual ru.

A separate vocabulary was extracted for each lan-
guage using the corpora used to create the BPE
model. The BPE model was then applied to all
training, dev and devtest data.

4 Multi-encoder transformer

We implemented a multi-source Transformer
(Vaswani et al., 2017) architecture, in the Sock-
eye (Hieber et al., 2017) framework, that combines
the output of two encoders (one for Kazakh, one
for Russian); this architecture will be described in
greater detail in a companion paper.

Our encoder combination takes place during at-
tention (that is, the attention step in which infor-
mation from the decoder and encoders are com-
bined, rather than the self-attention steps inside
each encoder and decoder); Figure 2 illustrates the
position in which the multiple sources are com-
bined into a single representation.

First, we perform multi-head scaled dot-product
attention between the the decoder and each en-
coder separately.

C(s) = MultiHead(s)
(
D,H(s),H(s)

)
(1)

MultiHead(s) (Q,K,V ) =
h∑
i

Head(s)i W
O
i

(s)
(2)

Head(s)i (Q,K,V , dk) =

A(QWQi
(s)
,KWKi

(s)
,V W Vi

(s)
, dk) (3)

A (Q,K,V , dk) = softmax
(
QK>√
dk

)
V (4)

where D = (d1,d2, · · · ,dn), di ∈ Rdmodel repre-
sents the decoder states, H = (h1,h2, · · · ,hm),
hi ∈ Rdmodel represents the outputs of the
encoder’s final self-attention layer, WQi

(s)
∈

Rdmodel×dk , WKi
(s) ∈ Rdmodel×dk , W Vi

(s) ∈

Figure 2: Multi-source attention on S sources. Each
output from the S encoders is attended to by a sepa-
rate multi-head attention layer (Eqs. 1-4), and then the
outputs of these attention layers are combined (Eq. 5).

Rdmodel×dk and WOi
(s) ∈ Rdk×dmodel are trainable

parameter matrices which project the key, query
and value into a smaller dimensionality. Together
with dk = dmodel/h, we have C(s) ∈ Rn×dmodel .

Next, we combine the outputs from the different
encoders with a simple projection and sum, similar
to what Libovický et al. (2018) refer to as “paral-
lel”:

C̃ =
S∑
i

C(i)WC
(i)

(5)

As this is essentially the same operation as the
multi-head combination in Equation (2), and no
nonlinearities intervene, we can also conceptual-
ize Equations (1)-(5) as if they were a single multi-
head attention layer with S ∗ h heads (in this case
2 ∗ 8 heads), in which each group of h heads is
constrained to attend to the output of one encoder.

We also experimented with a hierarchical atten-
tion mechanism along the lines of Libovický and
Helcl (2017) and Libovický et al. (2018), but as
this did not outperform the simpler combination
mechanism in (5) in internal testing, our submit-
ted systems utilized the latter.



270

Figure 3: The relations of all the MT systems involved in building the NRC final submitted system.

5 Experiments and results

5.1 NMT Setup
Our code extends sockeye-1.18.72 from Hieber
et al. (2017). Each source encoder has 6 lay-
ers and our decoder also has 6 layers, with a
model dimension of dmodel = 512 and 2048 hid-
den units sub-layer feed-forward networks. We
use weight tying, where the source embeddings,
the target embeddings and the target softmax
weights are tied, which implies a shared vocab.
We trained employing a cross-entropy loss with
Adam (Kingma and Ba, 2014), β1 = 0.9, β2 =
0.999, � = 1e − 8 and an initial learning rate of
0.0001, decreasing the learning by 0.7 each time
the development-set BLEU did not improve for 8
checkpoints. We optimized against BLEU using
newsdev2019-kken as the development set,
stopping early if BLEU did not improve for 32
checkpoints of 1000 updates each. The inputs and
output lengths were restricted to a maximum of 60
tokens, and mini-batches were of variable size de-
pending on sentence length, with each mini-batch
containing up to 4096 words.

5.2 SMT Setup
We trained en2kk, ru2kk and en2ru SMT sys-
tems using Portage (Larkin et al., 2010), a conven-
tional log-linear phrase-based SMT system, us-
ing the corresponding BPEed parallel corpora pre-
pared as described in Section 3. The translation
model of each SMT system uses IBM4 word align-
ments (Brown et al., 1993) with grow-diag-final-
and phrase extraction heuristics (Koehn et al.,
2003). The systems each have two n-gram lan-
guage models: a 5-gram language model (LM)
(a mixture LM in the kk2en case) trained on the
target-side of the corresponding parallel corpora

using SRILM (Stolcke, 2002), and a pruned 6-
gram LM trained on the monolingual training cor-
pora (for en2ru, trained just on news using KenLM
(Heafield, 2011); for ru2kk and en2kk, a static
mixture LM trained on all monolingual Kazakh
data using SRILM). Each SMT system also in-
cludes a hierachical distortion model, a sparse fea-
ture model consisting of the standard sparse fea-
tures proposed in Hopkins and May (2011) and
sparse hierarchical distortion model features pro-
posed in Cherry (2013), and a neural network joint
model, or NNJM, with 3 words of target con-
text and 11 words of source context, effectively a
15-gram LM (Vaswani et al., 2013; Devlin et al.,
2014). The parameters of the log-linear model
were tuned by optimizing BLEU on the develop-
ment set using the batch variant of the margin in-
fused relaxed algorithm (MIRA) by Cherry and
Foster (2012). Decoding uses the cube-pruning
algorithm of Huang and Chiang (2007) with a 7-
word distortion limit.

We then used these SMT systems to back-
translate a ˜2M sentence subselection of monolin-
gual English news into Kazakh and Russian, and
a ˜5M sentence subselection of monolingual Rus-
sian news into Kazakh, as well as cross-translating
the Russian of the ru-en parallel corpora into
Kazakh.

5.3 Building the NRC Submission System

Our final submission involved several SMT com-
ponents and several NMT components to produce
back-translations and cross-translations needed
for our multi-source submission system, as shown
in Figure 3.



271

Available Training Dev./Test BLEU
Resources Source 1 Source 2 Att. Comb. Source 1 Source 2 Dev. Test

kk-en kk+en2kk – – kk – 12.8 9.9
kk-en, ru-en kk+ru+en2kk – – kk – 15.4 12.6

kk-en, kk-ru, ru-en kk+ru2kk+en2kk – – kk – 17.9 14.8
kk-ru, ru-en pivoting 19.3 20.8

kk-en, kk-ru, ru-en kk+ru2kk+en2kk kk2ru+ru+en2ru Parallel kk kk2ru 19.6 24.2 /25.0*

Table 1: BLEU scores on WMT19 Kazakh-English news translation. en2kk denotes synthetic Kazakh back-
translated from English. ru2kk denotes synthetic Kazakh cross-translated from Russian. en2ru denotes synthetic
Russian back-translated from English. kk2ru denotes synthetic Russian cross-translated from Kazakh. * denotes an
unofficial post-competition result, a fully-trained version of our top system, which had only been partially trained
due to time constraints.

5.3.1 Synthetic cross-translations

To synthesize cross-translations, we trained 3 sys-
tems using our filtered ˜4.2M sentences of bilin-
gual Russian-Kazakh data. First, we trained a
Russian-to-Kazakh (ru2kk) SMT system and then
used it to generate ˜5M sentences of synthetic
Kazakh. Augmenting the bilingual data with the
Kazakh back-translations, we trained a Kazakh-to-
Russian NMT system to back translate ˜800k sen-
tences of monolingual Kazakh news for a ru2kk
NMT system and to cross translate ˜125k kk-en
sentences for one component of our final system.
Finally, we trained a Russian-to-Kazkah NMT
system using the bilingual data and the synthetic
Russian to cross translate ˜6M for our second com-
ponent of the final system.

5.3.2 Synthetic back-translation

A stack of another three MT systems was used
to synthesize Kazakh from English using ˜200k
of available English-Kazakh bilingual data for
training. Starting with an English-to-Kazakh
SMT system, ˜2M English sentences were back-
translated to Kazakh. Augmenting the bilingual
data with the newly generated Kazakh, we trained
a NMT Kazakh-to-English system and back trans-
lated ˜800k sentences of Kazakh news. The last
English-to-Kazakh NMT system in that stack was
trained using the bilingual data enlarged with the
˜800k previously generated back-translations. It
generated our en2kk back-translation of ˜2M sen-
tences of English news.

Our final component was accomplished by
training an English-to-Russian SMT system us-
ing ˜14.3M bilingual sentences and back translat-
ing the ˜2M sentence subselection of English news
into Russian.

5.3.3 Putting it all together

The box labelled “NRC’s Submission” in Figure
3 depicts how each sub-corpus was assembled
into the final bilingual corpora used to train our
multi-source NMT submission system. Each set
of curly braces surrounds a pair of corresponding
Kazakh and Russian sources. The first pair repre-
sents Kazakh and its cross-translation to Russian,
the second is the cross-translation of Russian-to-
Kazakh with the original Russian, and lastly we
have our sub-selected corpus back-translated into
both Kazakh and Russian.

5.4 Results

We can see in Table 1 that the full multi-
source, multi-encoder system with two-way cross-
translation (both Kazakh-to-Russian and Russian-
to-Kazakh) is significantly better than our other
systems, outperforming the pivoting system (on
the fourth line) by 4.2 BLEU and augmentation
by one-way cross-translation (on the third line) by
10.2 BLEU.

We believe this improvement over the other two
methods is due to the model being able to attend
to additional original data, to which the other sys-
tems do not have direct access. Both pivoting and
one-way synthetic augmentation involve “discard-
ing” genuine data, in that some of the original sen-
tences – Kazakh sentences in the former, and Rus-
sian sentences in the later – are never seen by the
downstream system, since they are only encoun-
tered in translation. Multi-source methods allow a
system to attend to the original data in both direc-
tions, thus capturing information that would oth-
erwise be lost in translation.

Notable in this table is the comparative im-
provement of the test scores over the dev scores,
between the pivoting (line 4) and multi-source
(line 5) systems. This can be explained, we



272

System BLEU YiSi-1 YiSi-1 srl
NEU 30.5 79.19 76.97
rug-morfessor 27.9 77.70 75.47
talp-upc-2019 24.9 75.07 72.74
NRC-CNRC 24.9 75.76 73.41
Frank-s-MT 19.8 76.17 73.87

Table 2: Automatic evaluation results for the top 5 con-
strained systems in WMT19

System Ave Ave. B
NEU 70.1 0.218
rug-morfessor 69.7 0.189
talp-upc-2019 67.1 0.113
NRC-CNRC 67.0 0.092
Frank-s-MT 65.8 0.066

Table 3: Human evaluation results for the top 5 con-
strained systems in WMT19

think, by a domain difference between the dev
and test sets, where the dev set was sampled from
the same news commentary dataset as the train-
ing data, whereas the test set comes from actual
newswire text. The scores appear to show that
the multi-source system has managed to general-
ize better to newswire text, possibly because it has
seen synthetic newswire text (synthesized from the
English-Russian dataset) and can respond more
appropriately to it.9

Tables 2 and 3 compare our multi-source sys-
tem to the other official submissions in the top 5 of
the WMT19 competition. In automatic evaluation
by BLEU, we were tied for third place, although
with a slight edge when measured by YiSi-1 (Lo,
2019); in human evaluation, we were in a statisti-
cal tie for second place. Notably, our multi-source
system was the top non-ensemble pure NMT sys-
tem, with other higher-scoring systems either be-
ing ensembles or SMT/NMT hybrids.

6 Conclusion and future work

We present the NRC submission to the WMT19
Kazakh-English news translation shared task. Our
submitted system is a multi-source, multi-encoder
neural machine translation system that takes Rus-
sian as the second source in the system. The ad-

9Note that, although we did perform additional filtering
on the training data of the multi-source system, we do not
believe this is the cause of the better performance on the test
compared to the pivoting system. In later tests, we found the
pivoting system to be relatively insensitive to this filtering
process, giving similar BLEU on both dev and test.

vantages of using the multi-source NMT archi-
tecture are that it incorporates additional informa-
tion obtained from 1) the Russian-English training
data cross translated into Kazakh, and 2) the Rus-
sian cross translated from Kazakh in the Kazakh-
Russian training data.

The drawback of this approach is the compar-
ative complexity of the pipeline, with separate
systems being trained to create back-translations
and cross-translations (including back-translations
to train those systems themselves). This com-
plexity was difficult for a human team to manage
when considered for three languages; it would be
prohibitive (without additional automation) when
making systems that involve four or more lan-
guages. Making use of the multi-source architec-
ture itself for creating back- and cross-translations
together, and sharing encoders and decoders be-
tween systems that share languages, would con-
siderably lessen the the complexity of the pipeline
and the number of distinct systems that need to be
trained.

In other future work, we want to consider addi-
tional methods of multi-source attention, as well
as other means of creating cross-linguistic syn-
thetic data beyond machine translation, for lower-
resource language pairs that do not have substan-
tial parallel data but may be, for example, closely
related.

References
Ondřej Bojar, Christian Federmann, Mark Fishel,

Yvette Graham, Barry Haddow, Matthias Huck,
Philipp Koehn, Christof Monz, Mathias Müller, and
Matt Post. 2019. Findings of the 2019 conference on
machine translation (wmt19). In Proceedings of the
Fourth Conference on Machine Translation, Volume
2: Shared Task Papers, Florence, Italy. Association
for Computational Linguistics.

Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263–311.

Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Human
Language Technologies: Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, Proceedings, June 9-14, 2013,
Westin Peachtree Plaza Hotel, Atlanta, Georgia,
USA, pages 22–31. The Association for Computa-
tional Linguistics.

Colin Cherry and George F. Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In

http://aclweb.org/anthology/N/N13/N13-1003.pdf
http://aclweb.org/anthology/N/N13/N13-1003.pdf
http://www.aclweb.org/anthology/N12-1047
http://www.aclweb.org/anthology/N12-1047


273

Human Language Technologies: Conference of the
North American Chapter of the Association of Com-
putational Linguistics, Proceedings, June 3-8, 2012,
Montréal, Canada, pages 427–436. The Association
for Computational Linguistics.

Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard M. Schwartz, and John Makhoul.
2014. Fast and robust neural network joint models
for statistical machine translation. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, ACL 2014, June 22-27,
2014, Baltimore, MD, USA, Volume 1: Long Pa-
pers, pages 1370–1380. The Association for Com-
puter Linguistics.

Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187–197, Edinburgh, Scot-
land, United Kingdom.

Felix Hieber, Tobias Domhan, Michael Denkowski,
David Vilar, Artem Sokolov, Ann Clifton, and Matt
Post. 2017. Sockeye: A toolkit for neural machine
translation. CoRR, abs/1712.05690.

Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2011, 27-31 July 2011, John McIntyre
Conference Centre, Edinburgh, UK, A meeting of
SIGDAT, a Special Interest Group of the ACL, pages
1352–1362. ACL.

Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In ACL 2007, Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics, June 23-30, 2007, Prague, Czech Republic.
The Association for Computational Linguistics.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. Cite
arxiv:1412.6980Comment: Published as a confer-
ence paper at the 3rd International Conference for
Learning Representations, San Diego, 2015.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Hu-
man Language Technology Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, HLT-NAACL 2003, Edmonton,
Canada, May 27 - June 1, 2003. The Association for
Computational Linguistics.

Taku Kudo. 2018. Subword regularization: Improv-
ing neural network translation models with multiple
subword candidates. CoRR, abs/1804.10959.

Samuel Larkin, Boxing Chen, George Foster, Ulrich
Germann, Eric Joanis, Howard Johnson, and Roland
Kuhn. 2010. Lessons from nrc’s portage system at
wmt 2010. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, WMT ’10, pages 127–132, Stroudsburg,

PA, USA. Association for Computational Linguis-
tics.

Jindřich Libovický and Jindřich Helcl. 2017. Attention
strategies for multi-source sequence-to-sequence
learning. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 196–202, Van-
couver, Canada. Association for Computational Lin-
guistics.

Jindřich Libovický, Jindřich Helcl, and David
Mareček. 2018. Input combination strategies for
multi-source transformer decoder. In Proceedings
of the Third Conference on Machine Translation:
Research Papers, pages 253–260, Belgium, Brus-
sels. Association for Computational Linguistics.

Chi-kiu Lo. 2019. YiSi - A unified semantic MT
quality evaluation and estimation metric for lan-
guages with different levels of available resources.
In Proceedings of the Fourth Conference on Ma-
chine Translation, Volume 2: Shared Task Papers,
Florence, Italy. Association for Computational Lin-
guistics.

Chi-kiu Lo, Michel Simard, Darlene Stewart, Samuel
Larkin, Cyril Goutte, and Patrick Littell. 2018. Ac-
curate semantic textual similarity for cleaning noisy
parallel corpora using semantic machine translation
evaluation metric: The NRC supervised submissions
to the parallel corpus filtering task. In Proceedings
of the Third Conference on Machine Translation:
Shared Task Papers, pages 908–916, Belgium, Brus-
sels. Association for Computational Linguistics.

Yuta Nishimura, Katsuhito Sudoh, Graham Neubig,
and Satoshi Nakamura. 2018. Multi-source neural
machine translation with data augmentation. CoRR,
abs/1810.06826.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Improving neural machine translation mod-
els with monolingual data. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
86–96, Berlin, Germany. Association for Computa-
tional Linguistics.

Andreas Stolcke. 2002. SRILM - an extensible
language modeling toolkit. In 7th International
Conference on Spoken Language Processing, IC-
SLP2002 - INTERSPEECH 2002, Denver, Col-
orado, USA, September 16-20, 2002. ISCA.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems, pages 5998–6008.

Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with large-scale
neural language models improves translation. In
Proceedings of the 2013 Conference on Empirical

http://aclweb.org/anthology/P/P14/P14-1129.pdf
http://aclweb.org/anthology/P/P14/P14-1129.pdf
https://kheafield.com/papers/avenue/kenlm.pdf
https://kheafield.com/papers/avenue/kenlm.pdf
http://arxiv.org/abs/1712.05690
http://arxiv.org/abs/1712.05690
http://www.aclweb.org/anthology/D11-1125
http://www.aclweb.org/anthology/D11-1125
http://aclweb.org/anthology/P07-1019
http://aclweb.org/anthology/P07-1019
http://aclweb.org/anthology/P07-1019
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1412.6980
http://aclweb.org/anthology/N/N03/N03-1017.pdf
http://arxiv.org/abs/1804.10959
http://arxiv.org/abs/1804.10959
http://arxiv.org/abs/1804.10959
http://dl.acm.org/citation.cfm?id=1868850.1868867
http://dl.acm.org/citation.cfm?id=1868850.1868867
https://doi.org/10.18653/v1/P17-2031
https://doi.org/10.18653/v1/P17-2031
https://doi.org/10.18653/v1/P17-2031
https://www.aclweb.org/anthology/W18-6326
https://www.aclweb.org/anthology/W18-6326
https://www.aclweb.org/anthology/W18-6481
https://www.aclweb.org/anthology/W18-6481
https://www.aclweb.org/anthology/W18-6481
https://www.aclweb.org/anthology/W18-6481
https://www.aclweb.org/anthology/W18-6481
http://arxiv.org/abs/1810.06826
http://arxiv.org/abs/1810.06826
https://doi.org/10.18653/v1/P16-1009
https://doi.org/10.18653/v1/P16-1009
http://www.isca-speech.org/archive/icslp_2002/i02_0901.html
http://www.isca-speech.org/archive/icslp_2002/i02_0901.html
http://aclweb.org/anthology/D/D13/D13-1140.pdf
http://aclweb.org/anthology/D/D13/D13-1140.pdf


274

Methods in Natural Language Processing, EMNLP
2013, 18-21 October 2013, Grand Hyatt Seattle,
Seattle, Washington, USA, A meeting of SIGDAT,
a Special Interest Group of the ACL, pages 1387–
1392. ACL.

Barret Zoph and Kevin Knight. 2016. Multi-source
neural translation. In Proceedings of NAACL-HLT,
pages 30–34.


