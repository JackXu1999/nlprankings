



















































Metrics for Evaluation of Word-level Machine Translation Quality Estimation


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 585–590,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Metrics for Evaluation of Word-Level Machine Translation Quality
Estimation

Varvara Logacheva, Michal Lukasik and Lucia Specia
Department of Computer Science

University of Sheffield, UK
{v.logacheva, m.lukasik, l.specia}@sheffield.ac.uk

Abstract

The aim of this paper is to investigate suit-
able evaluation strategies for the task of
word-level quality estimation of machine
translation. We suggest various metrics
to replace F1-score for the “BAD” class,
which is currently used as main metric.
We compare the metrics’ performance on
real system outputs and synthetically gen-
erated datasets and suggest a reliable alter-
native to the F1-BAD score — the multi-
plication of F1-scores for different classes.
Other metrics have lower discriminative
power and are biased by unfair labellings.

1 Introduction

Quality estimation (QE) of machine translation
(MT) is a task of determining the quality of an au-
tomatically translated text without any oracle (ref-
erence) translation. This task has lately been re-
ceiving significant attention: from confidence es-
timation (i.e. estimation of how confident a partic-
ular MT system is on a word or a phrase (Gan-
drabur and Foster, 2003)) it evolved to system-
independent QE and is performed at the word level
(Luong et al., 2014), sentence level (Shah et al.,
2013) and document level (Scarton et al., 2015).

The emergence of a large variety of approaches
to QE led to need for reliable ways to com-
pare them. The evaluation metrics that have
been used to compare the performance of systems
participating in QE shared tasks1 have received
some criticisms. Graham (2015) shows that Pear-
son correlation better suits for the evaluation of
sentence-level QE systems than mean absolute er-
ror (MAE), often used for this purpose. Pearson
correlation evaluates how well a system captures

1http://statmt.org/wmt15/
quality-estimation-task.html

the regularities in the data, whereas MAE essen-
tially measures the difference between the true and
the predicted scores and in many cases can be min-
imised by always predicting the average score as
given by the training set labels.

Word-level QE is commonly framed as a bi-
nary task, i.e., the classification of every translated
word as “OK” or “BAD”. This task has been eval-
uated in terms of F1-score for the “BAD” class,
a metric that favours ‘pessimistic’ systems — i.e.
systems that tend to assign the “BAD” label to
most words. A trivial baseline strategy that assigns
the label “BAD” to all words can thus receive a
high score while being completely uninformative
(Bojar et al., 2014). However, no analysis of the
word-level metrics’ performance has been done
and no alternative metrics have been proposed that
are more reliable than the F1-BAD score.

In this paper we compare existing evaluation
metrics for word-level QE, suggest a number of al-
ternatives, and show that one of these alternatives
leads to more objective and reliable results.

2 Metrics

One of the reasons word-level QE is a challeng-
ing problem is the fact that “OK” and “BAD” la-
bels are not equally important: we are generally
more interested in finding incorrect words than in
assigning a suitable category to every single word.
An ideal metric should be oriented towards the re-
call for the “BAD” class. However, the case of
F1-BAD score shows that this is not the only re-
quirement: in order to be useful the metric should
not favour pessimistic labellings, i.e., all or most
words labelled as “BAD”. Below we describe pos-
sible alternatives to the F1-BAD score.

2.1 F1-score variants
Word-level F1-scores. Since F1-BAD score is
too pessimistic, an obvious solution would be to

585



balance it with F1-score for the “OK” class. How-
ever, the widely used weighted average of F1-
scores for the two classes is not suitable as it will
be dominated by F1-OK due to labels imbalance.
Any reasonable MT system will nowadays gener-
ate texts where most words are correct, so the la-
bel distribution is very skewed towards the “OK”
class. Therefore, we suggest instead the multi-
plication of F1-scores for individual classes: it is
equal to zero if one of the components is zero, and
since both are in the [0,1] range, the overall result
will not exceed the value of any of the multipliers.

Phrase-level F1-scores. One of the features of
MT errors is their phrase-level nature. Errors are
not independent: one incorrect word can influence
the classification of its neighbours. If several ad-
jacent words are tagged as “BAD”, they are likely
to be part of an error which spans over a phrase.

Therefore, we also evaluate word-level F1-
scores and alternative metrics which are based on
correctly identified erroneous or error-free spans
of words. The phrase-level F1-score we suggest
is similar to the one used for the evaluation of
named entity recognition (NER) systems (Tjong
Kim Sang and De Meulder, 2003). There, pre-
cision is the percentage of named entities found
by a system that are correct, recall is the percent-
age of named entities present in the corpus that
are found by a system. For the QE task, instead
of named entities we have spans of erroneous (or
correct) words. Precision is the percentage of cor-
rectly identified spans among all the spans found
by a system, recall is the percentage of correctly
identified spans among the spans in the test data.

However, in NER the correct borders of a
named entity are of big importance, because fail-
ure to identify them results in an incorrect entity.
On the other hand, the actual borders of an error
span in QE are not as important: the primary goal
is to identify the erroneous region in the sentence,
the task of finding the exact borders of an error
cannot be solved unambiguously even by human
annotators (Wisniewski et al., 2013). In order to
take into account partially correct phrases (e.g. a
4-word “BAD” phrase where the first word was
tagged as “OK” by a system and the remaining
words were correctly tagged as “BAD”), we com-
pute the number of true positives as the sum of
percentages of words with correctly predicted tags
for every “OK” phrase. The number of true nega-
tives is defined analogously.

2.2 Other metrics

Matthews correlation coefficient. MCC (Pow-
ers, 2011) was used as a secondary metric in
WMT14 word-level QE shared task (Bojar et al.,
2014). It is determined as follows:

MCC =
TP × TN + FP × FN√

(TP + FP )(TP + FN)(TN + FP )(TN + FN)

where TP , TN , FP and FN are true positive,
true negative, false positive and false negative val-
ues, respectively.

This coefficient results in values in the [-1, 1]
range. If the reference and hypothesis labellings
agree on the majority of the examples, the final fig-
ure is dominated by the TP ×TN quantity, which
gets close to the value of the denominator. The
more false positives and false negatives the predic-
tor produces, the lower the value of the numerator.

Sequence correlation. The sequence correla-
tion score was used as a secondary evaluation met-
ric in the QE shared task at WMT15 (Bojar et al.,
2015). Analogously to the phrase-level F1-score,
it is based on the intersection of spans of correct
and incorrect words. It also weights the phrases
to give them equal importance and penalises the
difference in the number of phrases between the
reference and the hypothesis.

3 Metrics comparison

One of the most reliable ways of comparing met-
rics is to measure their correlation with human
judgements. However, for the word-level QE task,
asking humans to rate a system labelling or to
compare the outputs of two or more QE systems
is a very expensive process. A practical way of
getting the human judgements is the use of qual-
ity labels in downstream human tasks — i.e. tasks
where quality labels can be used as additional in-
formation and where they can influence human ac-
curacy or speed. One such a downstream task can
be computer-assisted translation, where the user
translates a sentence having automatic translation
as a draft, and word-level quality labels can high-
light incorrect parts in a sentence. Improvements
in productivity could show the degree of useful-
ness of the quality labels in this case. However,
such an experiment is also very expensive to be
performed. Therefore, we consider indirect ways
of comparing the metrics’ reliability based on pre-
labelled gold-standard test sets.

586



3.1 Comparison on real systems
One of the purposes of system comparison is to
identify the best-performing system. Therefore,
we expect a good metric to be able to distinguish
between systems as well as possible. One of the
quality criteria for a metric will thus be the num-
ber of significantly different groups of systems the
metric can identify. Another criterion to evalu-
ate metrics is to compare the real systems’ perfor-
mance with synthetic datasets for which we know
the desirable behaviour of the metrics. If a metric
gives the expected scores to all artificially gener-
ated datasets, it detects some properties of the data
which are relevant to us, so we can expect it to
work adequately also on real datasets.

Here we compare the performance of six met-
rics:

• F1-BAD — F1-score for the “BAD” class.
• F1-mult — multiplication of F1-scores for

“BAD” and “OK” classes.
• phr F1-BAD — phrase-level F1-score for the

“BAD” class.
• phr F1-mult — multiplication of phrase-

level F1-scores.
• MCC — Matthews Correlation Coefficient.
• SeqCor — Sequence Correlation.
We used these metrics to rank all systems sub-

mitted to the WMT15 QE shared task 2 (word-
level QE).2 In addition to that, we test the per-
formance of the metrics on a number of syntheti-
cally created labellings that should be ranked low
in comparison to real system labellings:

• all-bad — all words are tagged as “BAD”.
• all-good — all words are tagged as “OK”.
• optimistic — 98% words are tagged as

“OK”, with only a small number of “BAD”
labels generated: this system should have
high precision (0.9) and low recall (0.1) for
the “BAD” label.
• pessimistic — 90% words are tagged as

“BAD”: this system should have high recall
(0.9) for the “BAD” label, but low recall (0.1)
for the “OK” label.
• random — labels are drawn randomly from

the label probability distribution.

We rank the systems according to all the met-
rics and compute the level of significance for every

2Systems that took part in the shared task are listed and
described in (Bojar et al., 2015).

pair of systems with randomisation tests (Yeh,
2000) with Bonferroni correction (Abdi, 2007).
In order to evaluate the metrics’ performance we
compute the system distinction coefficient d — the
probability of two systems being significantly dif-
ferent, which is defined as the ratio between the
number of significantly different pairs of systems
and all pairs of systems. We also compute d for
the top half and for the bottom half of the ranked
systems list separately in order to check how well
each metric can discriminate between better per-
forming and worse performing systems.3

The results are shown in Table 1. For every
synthetic dataset we show the number of real sys-
tem outputs that were rated lower than this dataset,
with the rightmost column showing the sum of this
figure across all the synthetic sets.

We can see that three metrics are better at distin-
guishing synthetic results from real systems: Se-
qCor and both multiplied F1-scores. In the case
of SeqCor this result is explained by the fact that
it favours longer spans of “OK” and “BAD” la-
bels and thus penalises arbitrary labellings. The
multiplications of F1-scores have two components
which penalise different labellings and balance
each other. This assumption is confirmed by the
fact that F1-BAD scores become too pessimistic
without the “OK” component: they both favour
synthetic systems with prevailing “BAD” labels.
Phrase-F1-BAD ranks these systems the highest:
all-bad and pessimistic outperform 16 out of 17
systems according to this metric.

MCC is, in contrast, too ‘optimistic’: the opti-
mistic dataset is rated higher than most of system
outputs. In addition to that, it is not good at distin-
guishing different systems: its system distinction
coefficient is the lowest among all metric. SeqCor
and phrase-F1-multiplied, despite identifying ar-
tificial datasets, cannot discriminate between real
systems: SeqCor fails with the top half systems,
phrase-F1-multiplied is bad at finding differences
in the bottom half of the list.

Overall, F1-multiplied is the only metric that
performs well both in the task of distinguishing

3dbottom is always greater than dtop in our experiments
because better performing systems tend to have closer scores
under all metrics and more often are not significantly differ-
ent from one another. When comparing two metrics, greater
d does not imply greater dtop and dbottom: we use Bonfer-
roni correction for which the significance level depends on
the number of compared values, so a difference which is sig-
nificant when comparing eight systems, for example, can be-
come insignificant when comparing 16 systems.

587



d dtop dbottom all-bad all-good optimistic pessimistic random total
F1-BAD 0.79 0.61 0.81 4 - 1 4 1 10
F1-mult 0.81 0.57 0.75 - - 2 - 2 4
phr F1-BAD 0.86 0.61 0.78 16 - 1 16 - 33
phr F1-mult 0.75 0.54 0.47 - - 1 - - 1
MCC 0.63 0.61 0.34 - - 15 - - 15
SeqCor 0.77 0.39 0.75 - - 1 1 2 4

Table 1: Results for all metrics. Numbers in synthetic dataset columns denote the number of system
submissions that were rated lower than the corresponding synthetic dataset.

synthetic systems from real ones and in the task
of discriminating among real systems, despite the
fact that its d scores are not the best. However,
F1-BAD is not far behind: it has high values for
d scores and can identify synthetic datasets quite
often.

3.2 Comparison on synthetic datasets
The experiment described above has a notable
drawback: we evaluated metrics on the outputs of
systems which had been tuned to maximise the F1-
BAD score. This means that the system rankings
produced by other metrics may be unfairly consid-
ered inaccurate.

Therefore, we suggest a more objective met-
ric evaluation procedure which uses only synthetic
datasets. We generate datasets with different pro-
portion of errors, compute the metrics’ values and
their statistical significance and then compare the
metrics’ discriminative power. This procedure is
further referred to as repeated sampling, because
we sample artificial datasets multiple times.

Our goal is for the synthetic datasets to simulate
real systems’ output. We achieve this by using the
following procedure for synthetic data generation:

• Choose the proportion of errors to introduce
in the synthetic data.
• Collect all sequences that contain incorrect

labels from the outputs of real systems.
• Randomly choose the sequences from this set

until the overall number of errors reaches the
chosen threshold.
• Take the rest of segments from the gold-

standard labelling (so that they contain no er-
rors).

Thus our artificial datasets contain a specific
number of errors, and all of them come from real
systems. We can generate datasets with very small
differences in quality and identify metrics accord-
ing to which this difference is more significant.

Let us compare the discriminative power of
metrics m1 and m2. We choose two error thresh-
olds e1 and e2. Then we sample a relatively small
number (e.g. 100) of random datasets with e1 er-
rors. Then — 100 random datasets with e2 er-
rors. We compute the values for both metrics on
the two sets of random samples and for each met-
ric we test if the difference between the results for
the two sets is significant (we compute the statistic
significance using non-paired t-test with Bonfer-
roni correction). Since we sampled the synthetic
datasets a small number of times it is likely that
the metrics will not detect any significant differ-
ences between them. In this case we repeat the
process with a larger (e.g. 200) number of samples
and compare the p-values for two metrics again.
By gradually increasing the number of samples
at some point we will find that one of the met-
rics recognises the differences in scores as statisti-
cally significant, while another one does not. This
means that this metric has higher discriminative
power: it needs less samples to determine that the
systems they are different. The procedure is out-
lined in Algorithm 1.

In our experiments in order to make p-values
more stable we repeat each sampling round (sam-
pling of a set with ei errors 100, 200, etc. times)
1,000 times and use the average of p-values. We
used fixed sets of sample numbers: [100, 200, 500,
1000, 2000, 5000, 10,000] and error thresholds:
[30%, 30.01%, 30.05%, 30.1%, 30.2%]. The sig-
nificance level α is 0.05.

Since we compare all six metrics on five er-
ror thresholds, we have 10 p-values for each met-
ric at every sampling round. We analyse the re-
sults in the following way: for every difference in
the percentage of errors (e.g. thresholds of 30%
and 30.01% give 0.01% difference, thresholds of
30% and 30.2% — 0.2% difference), we define
the minimum number of samplings that a metric

588



0.01 0.04 0.05 0.1 0.15 0.2
F1-mult 10000 2000 2000 500 200 100
MCC 10000 2000 2000 500 200 100
F1-BAD 10000 5000 2000 1000 500 200
phr F1-mult 10000 5000 5000 1000 500 200
SeqCor 10000 5000 5000 1000 500 500
phr F1-BAD 10000 10000 5000 1000 500 500

Table 2: Repeated sampling: the minimum number of samplings required to discriminate between sam-
ples with a different proportions of errors.

Result: mx ∈ {m1, m2}, where mx — metric
with the highest discriminative power
on error thresholds e1 and e2

N ← 100
α← significance level
while p-valm1 > α and p-valm2 > α do

s1 ← N random samples with e1 errors
s2 ← N random samples with e2 errors
p-valm1 ← t-test(m1(s1),m1(s2))
p-valm2 ← t-test(m2(s1),m2(s2))
if p-valm1 < α and p-valm2 > α then

return m1
else if p-valm1 > α and p-valm2 < α
then

return m2
else

N ← N + 100
end

Algorithm 1: Repeated sampling for metricsm1,
m2 and error thresholds e1, e2.

needs to observe significant differences between
datasets which differ in this number of errors. Ta-
ble 2 shows the results. Numbers in cells are min-
imum numbers of samplings. We do not show er-
ror differences greater than 0.2 because all metrics
identify them well. All metrics are sorted by dis-
criminative power from best to worst, i.e. metrics
at the top of the table require less samplings to tell
one synthetic dataset from another.

As in the previous experiment, here the discrim-
inative power of the multiplication of F1-scores is
the highest. Surprisingly, MCC performs equally
well. Similarly to the experiment with real sys-
tems, the F1-BAD metric performs worse than
the F1-multiply metric, but here their difference is
more salient. All phrase-motivated metrics show
worse results.

4 Conclusions

The aim of this paper was to compare evaluation
metrics for word and phrase-level quality estima-
tion and find an alternative for F1-BAD score,
which has been used as primary metric in recent
research but has a number of drawbacks, in partic-
ular tendency to overrate labellings with predomi-
nantly“BAD” instances.

We found that the multiplication of F1-BAD
and F1-OK scores is more stable against “pes-
simistic” labellings and has bigger discrimina-
tive power when comparing synthetic datasets.
However, other tested metrics, including advanced
phrase-based scores, could not outperform F1-
BAD.

This work should be seen as a proxy for
real user evaluation of word-level QE metrics,
which could be done on downstream tasks (e.g.
computer-assisted translation).

References
Hervé Abdi. 2007. The bonferroni and šidák cor-

rections for multiple comparisons. Encyclopedia of
measurement and statistics, 3:103–107.

Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Aleš
Tamchyna. 2014. Findings of the 2014 workshop
on statistical machine translation. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, pages 12–58, Baltimore, Maryland, USA, June.
Association for Computational Linguistics.

Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Barry Haddow, Matthias Huck, Chris Hokamp,
Philipp Koehn, Varvara Logacheva, Christof Monz,
Matteo Negri, Matt Post, Carolina Scarton, Lucia
Specia, and Marco Turchi. 2015. Findings of the
2015 workshop on statistical machine translation.
In Proceedings of the Tenth Workshop on Statistical
Machine Translation, pages 1–46, Lisbon, Portugal.

589



Simona Gandrabur and George Foster. 2003. Confi-
dence estimation for translation prediction. In HLT-
NAACL-2003, pages 95–102, Edmonton, Canada.

Yvette Graham. 2015. Improving evaluation of ma-
chine translation quality estimation. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 1804–1813.

Ngoc Quang Luong, Laurent Besacier, and Benjamin
Lecouteux. 2014. Lig system for word level qe
task at wmt14. In WMT-2014, pages 335–341, Bal-
timore, USA, June.

David M.W. Powers. 2011. Evaluation: from preci-
sion, recall and F-measure to ROC, informedness,
markedness and correlation. Journal of Machine
Learning Technologies, 2(1):37–63.

Carolina Scarton, Liling Tan, and Lucia Specia. 2015.
Ushef and usaar-ushef participation in the wmt15 qe
shared task. In Proceedings of the Tenth Workshop
on Statistical Machine Translation, pages 336–341,
Lisbon, Portugal, September.

Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An investigation on the effectiveness of features for
translation quality estimation. In MT Summit XIV,
pages 167–174, Nice, France.

Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of CoNLL-2003, pages 142–147.

Guillaume Wisniewski, Anil Kumar Singh, Natalia Se-
gal, and François Yvon. 2013. Design and Anal-
ysis of a Large Corpus of Post-Edited Translations:
Quality Estimation, Failure Analysis and the Vari-
ability of Post-Edition. In MT Summit XIV: 14th
Machine Translation Summit, pages 117–124, Nice,
France.

Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Coling-
2000: the 18th Conference on Computational Lin-
guistics, pages 947–953, Saarbrücken, Germany.

590


