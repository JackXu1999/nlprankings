



















































Adversarial Domain Adaptation Using Artificial Titles for Abstractive Title Generation


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2197–2203
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

2197

Adversarial Domain Adaptation Using Artificial Titles
for Abstractive Title Generation

Francine Chen
FX Palo Alto Laboratory

Palo Alto, CA
chen@fxpal.com

Yan-Ying Chen
FX Palo Alto Laboratory

Palo Alto, CA
yanying@fxpal.com

Abstract

A common issue in training a deep learning,
abstractive summarization model is lack of a
large set of training summaries. This paper ex-
amines techniques for adapting from a labeled
source domain to an unlabeled target domain
in the context of an encoder-decoder model
for text generation. In addition to adversarial
domain adaptation (ADA), we introduce the
use of artificial titles and sequential training to
capture the grammatical style of the unlabeled
target domain. Evaluation on adapting to/from
news articles and Stack Exchange posts indi-
cates that the use of these techniques can boost
performance for both unsupervised adaptation
as well as fine-tuning with limited target data.

1 Introduction

Many types of textual content, such as conver-
sations and posts on chat, do not have a title or
summary. While multi-sentence extractive sum-
marization can give a sense of the content of an
article, a title or highlight is more concise. Such
short summaries can be generated using abstrac-
tive summarization with an RNN encoder-decoder
model, e.g., (Nallapati et al., 2016).

A common issue when training models for
abstractive summarization of conversations and
posts is the lack of a large set of text with sum-
maries. Obtaining good quality labeled data can
be difficult and expensive, especially if author-
generated summaries are desired. One option is
to train on data from another domain with author-
generated titles, but because of differences be-
tween domains, the performance may be less than
adequate. These differences include different vo-
cabularies, different grammatical styles, and dif-
ferent ways of expressing similar concepts. Vo-
cabulary expansion may be used to address the dif-
ferent vocabularies in source and target domains,
and adversarial domain adaptation (ADA) may be

used to merge the embedded feature representa-
tions across domains. However, ADA does not
adapt the decoder in an encoder-decoder genera-
tion model.

In this paper, we investigate the utility of these
techniques in unsupervised domain adaptation for
title generation. We also examine the use of a
limited amount of labeled training data from the
target domain, when high performance may be
required but training data is not easily available.
Our contributions include (1) proposing the use
of artificial titles for unlabeled target documents
to train a decoder to learn the grammatical style
of titles in the new domain (2) proposing to train
the decoder in a sequence of steps that encourages
the source and target embedding spaces to remain
aligned during adaptation, and (3) showing that
our model improves performance over ADA and
an expanded vocabulary alone and further, that a
limited amount of labeled target data can achieve
performance close to training on all labeled target
data.

2 Related Work

Our model draws from work on abstractive sum-
marization and unsupervised domain adaptation.
Recently, a number of neural encoder-decoder
models have been proposed for abstractive sum-
marization e.g., (Rush et al., 2015; Chen et al.,
2016a; Nallapati et al., 2016; Chopra et al., 2016;
Li et al., 2017; Narayan et al., 2018; Hsu et al.,
2018), with one of the better performing mod-
els being (See et al., 2017), which serves as our
base model. Supervised domain adaptation meth-
ods have been proposed for generative models.
(Hua and Wang, 2017) found that pre-training an
abstractive summarizer with extractive summaries
does not always improve performance, but (Chen
et al., 2015) noted that fine-tuning a model trained



2198

Figure 1: Encoder-decoder RNN model for text gen-
eration with a classifier for adversarial domain adap-
tation of the encoded representations (concepts) to an
unlabeled target domain. Gradient reversal of Ld from
the domain classifier to the encoder is indicated. The
blue/red articles represent source/target domain data.

on source domain data with limited target domain
data does improve performance.

A variety of techniques have been proposed for
unsupervised domain adaptation of deep learning
systems for classification, e.g., (Hsu et al., 2017;
Tzeng et al., 2017; Ganin et al., 2016; Chen et al.,
2016b; Ghifary et al., 2016). However, all used
the aligned encoder representation for classifica-
tion but not generation.

We adapt the domain-adversarial method for
feature alignment in an encoder proposed by
(Ganin et al., 2016). However, for text generation,
a domain-independent representation from the en-
coder, as used in domain adaptation for classifica-
tion, is not adequate. We also require the decoder
to be adapted to varying domains to generate out-
put appropriate for the target domain, an issue that
we investigate in the context of title generation.

Jointly training a translation model with mixed
labeled data from two domains can improve
performance over training on one domain only
(Pryzant et al., 2017). In contrast, our domain
adaptation method trains sequentially on data, first
with the unlabeled target domain data.

3 Domain-Adapted Title Generation

Our goal is to improve performance when labeled
data from one domain, the source, is used to train
a model which is then applied to another domain
with no or only limited labeled data, the target.

3.1 Adversarial Domain Adaptation (ADA)

The embedded representation generated by the en-
coder, which represents the “concepts” in the input
text, may differ across domains. To address this,
we adapt the method proposed by (Ganin et al.,
2016), which uses a domain classifier to force the
concept representations to align across domains.

We use an encoder-decoder RNN model with do-
main adaptation (Figure 1) for title generation. La-
beled source data is fed to the encoder and the de-
coder learns to generate summary titles. At the
same time, the source data and unlabeled target
domain data are encoded by a bidirectional LSTM
as their concept representations, and the domain
classifier tries to learn to differentiate between the
representations of two domains.

The domain classifier has two dense, 100-unit
hidden layers followed by a softmax. The con-
cept representation vector is computed as the bidi-
rectional LSTM encoder’s final forward and back-
ward hidden states concatenated into a single state.
During training, the gradient from the domain
classifier, ∂Ld∂θd , is “reversed” to be negative be-
fore being propagated back through the encoder
as −∂Ld∂θc , encouraging the embedded representa-
tions to align by adjusting the feature distributions
to maximize the loss of the domain classifier.

In contrast to the two classification losses used
by (Ganin et al., 2016) for training the model, we
use the generated sequence loss together with the
adversarial domain classifier loss:

loss =
1

T

T∑
t=0

Ly(t)− λLd (1)

where, following (See et al., 2017), the decoder
(sequence) loss

Ly(t) = −logP (w∗t ) (2)

is the negative log likelihood of the target wordw∗t
at position t. The domain classifier loss, Ld, is the
cross-entropy loss between the predicted and true
domain label probabilities,

Ld = d · logP (d̂) + (1− d) · log(1−P (d̂)). (3)

λ is a parameter relating the two losses.
We followed the schedule from (Ganin et al.,

2016) for adjusting λ for the encoder:

λp =
2

1 + exp(−10p)
− 1 (4)

λ was increased from 0.0 to 1.0 by increasing p
from 0.0 to 1.0 over 5000 iterations, at which point
we observed that the domain adaptation classi-
fier loss was reaching an asymptote. λ was then
held equal to 1.0 and training continued until val-
idation performance for title generation reached
an asymptote (when training on artificial titles or
source data) or overtraining occurred (when train-
ing on limited target data). When updating the do-
main classifier, λ was set equal to one.



2199

Figure 2: Flowchart for training a model for an unla-
beled target domain with artificial targets.

3.2 Artificial Titles

The style of the unlabeled target may be different
from the source, e.g., Stack Exchange is more ca-
sual and includes more slang than news articles.
To capture the style of the unlabeled target, “artifi-
cial” titles were synthesized. Since titles tend to be
short and encode-decoder models learn to model
sentence length, target text between 4-10 words in
length were selected. A common summary base-
line is the first few sentences of a news article e.g.
(Zajic et al., 2004; Nallapati et al., 2016); some so-
cial media sites, including Trip Advisor, Facebook
and Reddit, display the first words of long posts.
For example, this paragraph might be shown as
”The style of the unlabeled target may ...”.

The first text meeting the length requirement
was selected 90% of the time and the second text
meeting the requirement selected otherwise. For
Stack Exchange, the text was a sentence from a
post, and for news, where titles are often phrases,
the text was a clause. Training on first text only,
the loss dropped below 0.001 in less than 3k iter-
ations, indicating the model had learned to copy
from the first sentence. Use of the second text
discourages this so that both the encoder and de-
coder are trained on text from the target domain
(enabling use of an expanded, joint vocabulary
trained on both source and target) to learn its style
and vocabulary. However, the artificial titles will
generally be different from the real titles, which
may lead to lower summarization performance.

3.3 Sequential Training

Our adaptation method, ASADA, is shown in Fig-
ure 2: a) A model with a joint vocabulary is first
pre-trained on artificial titles for the unlabeled tar-
get domain (Section 3.2). b) The embedding space
of the pre-trained model is then adapted to the
source domain using ADA (Section 3.1) to con-
tinue training on the target domain with the source
domain as the auxiliary adaptation data. c) With
a joint embedding space defined, the model is
trained on the source domain, which has title-text
pairs, and the unlabeled target domain is used as
the auxiliary adaptation data to keep the model

dataset type use # train summary lengthsamples mean std dev

StackEx artif. Tart 398k 11.3 5.4filt-10 S,F 140k 6.5 1.4

News artif. Tart 287k 7.7 1.5filt-10 F 31k 9.0 1.4
filt-14 S 168k 11.9 1.8

Table 1: Statistics of the Stack Exchange and News
datasets. Tart: artificial Target; S: Source; F: fine-
tuning; filt-X: filtered for at most length X.

embedding aligned with the target data.

4 Dataset

We used data from two domains: the pub-
lic CNN/Dailymail (News) dataset used by (See
et al., 2017) and posts from 20 Stack Exchange
(StackEx) channels1 with a bias towards those that
are business related (see Appendix A for details).
To reduce training time, each article was truncated
to 200 words. We limited the data to those with
title lengths of 10 words or less for use in fine-
tuning because some were longer sentences rather
than titles. (See Table 1) The News datasets were
formatted as in (See et al., 2017). The StackEx
dataset was randomly divided into train (90%),
validation (5%) and test (5%).

5 Experiments

For all experiments, the Pointer-Generator model
(Gulcehre et al., 2016) by (See et al., 2017) was
used without coverage as our base model, since
coverage is an additional training step that would
add an additional variable to the comparisons. Al-
though coverage improves performance by reduc-
ing repetitive words, we chose to examine the ef-
fects of different domain adaptation methods with-
out it. For handling differences in vocabulary, the
vocabulary of the labeled source and unlabeled tar-
get domains were combined. The union of the 50k
most frequent terms from the training data of each
domain produced a joint vocabulary of about 85k
terms. When an individual vocabulary was used,
the size was 50k words. When sequential train-
ing was used, a model was trained until the loss
on a validation set reached an asymptote. Domain
adaptation experiments from News to StackEx and
from StackEx to News were conducted, first with-
out target domain summary titles and then with a
limited amount of target domain titles.

1https://archive.org/details/
stackexchange downloaded 05/26/2017

https://archive.org/details/stackexchange
https://archive.org/details/stackexchange


2200

id reference ordescription vocab
training data
and method

News → StackEx StackEx → News
ROUGE ROUGE

1 2 L 1 2 L
(a) See et al. S S 14.22 4.22 12.80 12.92 3.19 12.15
(b) joint vocab S+T S 15.99 4.87 14.42 10.85 2.85 10.23
(c) Ganin et al. (ADA) S+T S, SADA 16.75 5.24 15.10 12.45 3.12 11.53
(d) artif titles S+T Tart 14.28 4.87 13.26 12.02 3.58 11.06
(e) artif titles, ADA S+T Tart, SADA 16.88 5.35 15.24 14.36 3.84 13.47
(f) ASADA S+T Tart, TADAart , SADA 17.78 6.22 16.15 16.75 6.11 15.99
(g) ASADA (lead-1) S+T Tlead1, TADAlead1, S

ADA 16.46 5.30 15.01 16.16 3.36 14.64
(h) Pryzant et al.(DM) S+T S+Tart 14.63 5.00 13.49 15.13 5.32 14.51
(i) Pryzant et al. (ADM) S+T S+Tart 15.29 5.37 14.06 13.00 4.30 12.01
(j) upper bound T T 31.49 13.70 29.22 23.52 10.92 22.34

Table 2: Title generation performance of domain adaptation from Source S to Target T. (a-c) Baselines. (d-g)
Our approaches with artificial titles Tart and with lead-1 Tlead1, respectively. (h) DM: Discriminative Mixing.
(i) ADM: Adversarial Discriminative Mixing. (j) Upper bound trained on labeled target data. Training steps are
separated by commas. SADA: train on S using ADA. TADAart : train on Tart using ADA.

prev curr domains same
training training gradually labeled
data & data & or jointly data

id method method embedded? domain?
(E) Tart SADA no no
(F1) Tart TADAart yes yes
(F2) TADAart SADA yes no

Table 3: Comparison of adaptation steps with artificial
titles using one step, (E), and two step ASADA, (F1)
and (F2). (E) and (F) correspond to the models (e) and
(f) in Table 2, respectively.

5.1 Unsupervised Target Domain Adaptation

For our investigations on domain adaptation when
labeled target domain data is unavailable, models
trained on source domain labels only and with a
mix of source domain labels and artificial target
labels are our baselines.
Effect of ADA and Vocabulary The top section
of Table 2 shows baseline models trained
(a) with the source domain vocabulary [(See et al.,
2017)’s approach without coverage]
(b) with a joint vocabulary instead of the source
domain vocabulary
(c) model (b) followed by training using ADA
to the target domain [(Ganin et al., 2016)’s ap-
proach].

The mixed results using a joint vocabulary re-
flect the better coverage of the added target words
outside the source’s top-50k vocabulary when the
source is News vs. StackEx (see Appendix B).
And when a joint vocabulary (S+T) is used, ADA
(c) improves performance over training only on
the source S (b), as expected.
Effect of Artificial Titles and Sequential Train-
ing The second section of Table 2 compares ap-

proaches using artificial titles:
(d) Tart: a model pre-trained on target domain ar-
ticles/posts with artificial target domain titles
(e) Tart, SADA: model (d), further trained on the
source with ADA to the target without labels.
(f) Tart,TADAart ,SADA: ASADA. Model (d), fol-
lowed by adapting the model, which has been
trained on the target domain with non-optimal
summaries, to source data, aligning the embed-
ded representations of the two domains. Then the
model is trained on source data with ADA to the
unlabeled target to learn how to summarize while
keeping the embedded representations aligned.
(g) ASADA using the lead-1 (first) sentence in
place of Tart. The better performance in (f) sup-
ports ASADA’a use of artificial titles.

ASADA’s two-step adaptation with artificial ti-
tles performed best out of all models. The mixed
performance of training on Tart indicates the arti-
ficial title quality is lower for StackEx, (d) vs. (b).
The weakly better performance of (e) over (c) indi-
cates that applying SADA directly forgets much of
Tart. The relative improvement of ASADA over
training only on source was 25% (from News to
StackEx) and 30% (from StackEx to News). This
indicates that TADAart allows the model to remember
the vocabulary and style from Tart while learning
how to summarize by SADA.

Table 3 illustrates differences between the one-
step adaptation model (e), with id (E) and the two-
step adaptation used in ASADA (F1 and F2). In
both, the model is first trained on the target do-
main using Tart. In model (e), ADA then trains
the encoder on source only and ignores Tart, grad-
ually giving greater weight to the domain classi-
fier, which uses the target data (see Sec. 3.1). At



2201

Figure 3: Domain adaptation performance with varying
amounts of labeled StackEx (left) and News (right) data
for fine-tuning with ADA (* DA) and without (* FT).
For reference, performance when trained on all labeled
target data and no adaptation (* 100%).

the same time, the labeled data domain is switched
to the source domain, so that both the embedding
and decoder domains are abruptly changed. In
contrast, in ASADA the embedding is gradually
adapted from the target domain to jointly embed
the source and target (F1). Only then is the target
domain changed (F2).

In the third section, the labeled source is
mixed with target domain artificial titles and
trained using (Pryzant et al., 2017)’s Discrimina-
tive Mixed (DM) and Adversarial Discriminative
Mixed (ADM) machine translation models. ADM
is similar to ADA in that both use and adversarial
classifier; however, for ADM both domains have
labeled data. ASADA’s better performance indi-
cates that first pre-training with artificial titles to
learn vocabulary and style and then adapting to the
source to learn to summarize is better than jointly
mixing artificial and true titles.

5.2 Limited Target Domain Labels

We next examine adaptation performance when a
limited amount of labeled data is available for the
target domain. Our best model for each domain,
ASADA, is refined by training on various percent-
ages of the labeled target domain training data and
referred to as ‘* DA’ in Figure 3. For comparison,
a baseline model was trained using labeled source
domain data and then fine-tuned (Sun et al., 2016;
Song et al., 2017) using labeled target domain data
and is shown as ‘* FT’.

Note that (1) when labeled target domain data
is very limited, say 3,000 labeled samples, ‘* DA’
improves performance more than ‘* FT’ (2) as the
amount of labeled target data increases, the perfor-
mance with and without ADA increases, and with
30% of the target data (rightmost points) is close
to or exceeds using 100% of the target data.

Figure 4: MDS visualizations comparing embeddings
of a sample of test text produced by models (d), (e)
and (f) in Table 2. artif: model (d). artif,srcADAmid:
model (e) midway through ADA. artif,srcADA: trained
model (e). ASADA: model (f). Left: News→ StackEx.
Right: StackEx→ News.

5.3 Visualization of Adaptation Models
Embedded points produced by models (d), (e) and
(f) (see Section 5.1) are compared in the visual-
ization in Figure 4. For the one-step adaptation
model, (e), embedded points are shown partway
through adaptation with ADA (i.e., p in Eqn. (4) is
approximately 0.5) and after adaptation. The em-
bedding partway through adaptation, labeled ar-
tif,srcADAmid, has moved away from the Tart em-
bedding (model (d), labeled artif ). After adap-
tation, labeled artif,srcADA, the embedded points
are only slightly closer to the Tart embedded
points. In contrast, the ASADA (f) embedding
is closer to the Tart embedding and more com-
pact, as is Tart. This supports our hypothesis that
ASADA retains more of what was learned from
the initial target embedding than model (e)’s one-
step adaptation, contributing to ASADA’s better
performance.

6 Summary

We investigated unsupervised domain adaptation
methods for an encoder-decoder model. We pro-
posed the use of artificial titles for training a de-
coder to the target domain vocabulary and style
and sequential adversarial domain adaptation to
minimize rapid changes of the encoder embed-
ding space. Our experiments show that our pro-
posed approach performed best when compared
to baseline adaptation techniques when unsuper-
vised. And with very limited target domain labels
for fine-tuning, our model performed better than
fine-tuning a model trained on the source domain.
In the future, we would like to understand the use-
fulness of artificial titles for training the decoder
relative to other factors that may impact perfor-
mance, e.g., how similar the true titles or sum-
maries are in the different domains.



2202

References
Qian Chen, Xiao-Dan Zhu, Zhen-Hua Ling, Si Wei,

and Hui Jiang. 2016a. Distraction-based neural net-
works for modeling document. In IJCAI, pages
2754–2760.

Xie Chen, Tian Tan, Xunying Liu, Pierre Lanchantin,
Moquan Wan, Mark JF Gales, and Philip C Wood-
land. 2015. Recurrent neural network language
model adaptation for multi-genre broadcast speech
recognition. In Sixteenth Annual Conference of the
International Speech Communication Association.

Xilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie,
and Kilian Weinberger. 2016b. Adversarial deep av-
eraging networks for cross-lingual sentiment classi-
fication. arXiv preprint arXiv:1606.01614.

Sumit Chopra, Michael Auli, and Alexander M Rush.
2016. Abstractive sentence summarization with at-
tentive recurrent neural networks. In Proceedings of
the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 93–98.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, François Lavi-
olette, Mario Marchand, and Victor Lempitsky.
2016. Domain-adversarial training of neural net-
works. The Journal of Machine Learning Research,
17(1):2096–2030.

Muhammad Ghifary, W Bastiaan Kleijn, Mengjie
Zhang, David Balduzzi, and Wen Li. 2016. Deep
reconstruction-classification networks for unsuper-
vised domain adaptation. In European Conference
on Computer Vision, pages 597–613. Springer.

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati,
Bowen Zhou, and Yoshua Bengio. 2016. Pointing
the unknown words. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 140–149.

Wan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui
Min, Jing Tang, and Min Sun. 2018. A unified
model for extractive and abstractive summarization
using inconsistency loss. In Proceedings of the
19th International Conference on Computational
Linguistics (Long Papers), pages 132–141. Associa-
tion for Computational Linguistics.

Wei-Ning Hsu, Yu Zhang, and James Glass. 2017.
Unsupervised domain adaptation for robust speech
recognition via variational autoencoder-based data
augmentation. In IEEE Automatic Speech Recog-
nition and Understanding Workshop.

Xinyu Hua and Lu Wang. 2017. A pilot study of do-
main adaptation effect for neural abstractive summa-
rization. In Proceedings of the Workshop on New
Frontiers in Summarization, pages 100–106.

Piji Li, Wai Lam, Lidong Bing, and Zihao Wang. 2017.
Deep recurrent generative decoder for abstractive
text summarization. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2091–2100.

Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
Caglar Gulcehre, and Bing Xiang. 2016. Ab-
stractive text summarization using sequence-to-
sequence rnns and beyond. In Proceedings of The
20th SIGNLL Conference on Computational Natu-
ral Language Learning, pages 280–290.

Shashi Narayan, Shay B Cohen, and Mirella Lapata.
2018. Don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-
treme summarization. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1797–1807.

Reid Pryzant, Denny Britz, and Quoc Le. 2017. Ef-
fective domain mixing for neural machine transla-
tion. In Proceedings of the Second Conference on
Machine Translation, pages 118–126.

Alexander M Rush, SEAS Harvard, Sumit Chopra, and
Jason Weston. 2015. A neural attention model for
sentence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 379–389.

Abigail See, Peter J Liu, and Christopher D Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 1073–1083.

Xinhang Song, Luis Herranz, and Shuqiang Jiang.
2017. Depth cnns for rgb-d scene recognition:
Learning from scratch better than transferring from
rgb-cnns. In AAAI, pages 4271–4277.

Baochen Sun, Jiashi Feng, and Kate Saenko. 2016.
Return of frustratingly easy domain adaptation. In
AAAI, pages 2058–2065.

Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor
Darrell. 2017. Adversarial discriminative domain
adaptation. In Computer Vision and Pattern Recog-
nition (CVPR), page 4.

David Zajic, Bonnie Dorr, and Richard Schwartz. 2004.
Bbn/umd at duc-2004: Topiary. In Proceedings
of the HLT-NAACL 2004 Document Understanding
Workshop, Boston, pages 112–119.

A Stack Exchange Dataset

The Stack Exchange channels used for the dataset
are: ai (i.e., ai.stackexchange.com), android, ar-
duino, cs, datascience, emacs, engineering, free-
lancing, iot, opendata, opensource, patents, pro-
grammers, robotics, salesforce, sharepoint, travel,
unix, webapps, and workplace.



2203

Figure 5: Histograms of News and Stack Exchange vo-
cabularies showing the number of target domain joint
vocabulary word tokens that are unrepresented in the
source training data.

B Cross-Domain Vocabulary Coverage

For the expanded, joint vocabulary of source and
target, Figure 5 shows that the number of News
target tokens not represented by StackExchange
vocabulary terms is much larger than the number
of Stack Exchange target tokens not represented
by News vocabulary terms. When trained on
source only, these unrepresented target domain to-
kens are neither trained nor handled by the pointer-
generator mechanism. Adversarial Domain Adap-
tation enables training of the encoder on these tar-
get tokens. Artificial Titles enable the decoder to
be trained on these tokens.


