



















































AdaNSP: Uncertainty-driven Adaptive Decoding in Neural Semantic Parsing


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4265–4270
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

4265

AdaNSP: Uncertainty-driven Adaptive Decoding in
Neural Semantic Parsing

Xiang Zhang1,2, Shizhu He2, Kang Liu1,2, Jun Zhao1,2
1University of Chinese Academy of Sciences, Beijing, 100049, China

2National Laboratory of Pattern Recognition (NLPR), Institute of Automation
Chinese Academy of Sciences, Beijing, 100190, China
{xiang.zhang, shizhu.he, kliu, jzhao}@nlpr.ia.ac.cn

Abstract

Neural semantic parsers utilize the encoder-
decoder framework to learn an end-to-end
model for semantic parsing that transduces a
natural language sentence to the formal se-
mantic representation. To keep the model
aware of the underlying grammar in target se-
quences, many constrained decoders were de-
vised in a multi-stage paradigm, which decode
to the sketches or abstract syntax trees first,
and then decode to target semantic tokens.
We instead to propose an adaptive decoding
method to avoid such intermediate represen-
tations. The decoder is guided by model un-
certainty and automatically uses deeper com-
putations when necessary. Thus it can pre-
dict tokens adaptively. Our model outperforms
the state-of-the-art neural models and does not
need any expertise like predefined grammar or
sketches in the meantime.

1 Introduction

Semantic Parsing (SP) maps a natural language
utterance into a formal language, which is cru-
cial in abundant tasks, such as question answering
(Zettlemoyer and Collins, 2005, 2007) and code
generation (Yin and Neubig, 2017). The prevail-
ing neural semantic parsers view semantic pars-
ing as a sequence transduction task, and adopt the
encoder-decoder framework similar to machine
translation.

The distinguishing difference of semantic pars-
ing, however, is in its target sequences, which
are token sequences of well-formed semantic rep-
resentations. SQL language and lambda expres-
sions are typical examples of SP targets. The
“SELECT..FROM..WHERE” pattern in SQL and
the paired parentheses in lambda expressions are
consequences of underlying grammars. However,
standard Seq2Seq models ignore the patterns and
may give ill-formed results.

To better model the grammatical and semanti-
cal constraints, many decoding methods were de-
vised. Dong and Lapata (2018) proposed to gen-
erate tokens of an intermediate sketch first, fol-
lowed by decoding into final formal targets. Oth-
ers chose to gradually build abstract syntax trees
using a transition-based paradigm, and tokens are
generated at the tree leaves or in the middle of
the transitions (Krishnamurthy et al., 2017; Chen
et al., 2018; Yin and Neubig, 2018). There are
also some decoders comprised of several submod-
ules which are intended to generate different parts
of the semantic output, respectively (Yu et al.,
2018a,b). However, the aforementioned methods
still have the following key issue. They explicitly
require the expertise to design intermediate repre-
sentations or model structures, which is not ideal
or acceptable for scenarios with Domain Specific
Languages (DSL) or new representations because
of domain alterations and the incompleteness of
the expert knowledge.

To follow the successful idea and overcome the
above issue, we introduce a novel adaptive decod-
ing mechanism. Inspired by adaptive computing
(Graves, 2016), pervasive tokens in training data
will be generated immediately with no doubt. But
for tokens seen less often, the model may be pon-
dering and less confident, and it will be better to
carry out more computations. In this way, it is
unnecessary to pre-build any intermediate super-
vision for training, such as preprocessed sketches
(Dong and Lapata, 2018) and predesigned gram-
mars (Yin and Neubig, 2018), which must be man-
ually redesigned for an unseen kind of target lan-
guage. Furthermore, we use the model uncertainty
estimates to reflect its prediction confidence. Al-
though different uncertainty estimates have been
explored in semantic parsing (Dong et al., 2018),
we use Dropout (Srivastava et al., 2014) as the un-
certainty signal (Gal and Ghahramani, 2016) due



4266

to its simplicity, and use policy gradient algorithm
to guide the model search.

Our contributions are thus three-fold.

• We introduce the adaptive decoding mech-
anism into semantic parsing, which is well
rid of intermediate representations and easily
adaptable to new target languages.

• We adopt uncertainty estimates to bias the de-
coder search, which is not covered in archi-
tecture searching literature to our best knowl-
edge.

• Our model outperforms the state-of-the-art
neural models without other intermediate su-
pervisions.

2 Uncertainty-driven Adaptive Decoding
Model

Our semantic parser is learned from pairs of natu-
ral language sentences and formal semantic rep-
resentations. Let x = {x1, x2, . . . , xm} de-
note the words in an input sentence, and y =
{y1, y2, . . . , yn} be the tokens of the correspond-
ing target lambda expression.

2.1 Adaptive Decoding Model

We first introduce the general model for adaptive
decoding. In general, the model consists of an en-
coder, a decoder, a halting module, and the atten-
tion mechanism.

Encoder. Input words x are first embedded us-
ing an embedding matrix Wx ∈ Rd×|Vx|, where d
is the dimension of embedded vectors and Vx is the
set of all input words. We use a stacked two-layer
BiLSTM to encode the input embedding. Hidden
states from both direction at the same position of
the second layer are concatenated as final encoder
outputs {h1, . . . , hm}.

Decoder. We stack two LSTM cells as one ba-
sic decoding unit. Similarlly, we use a matrix to
embed target tokens y, yi = Wyo(yi). The to-
ken embedding will serve the input of the decod-
ing cell.

st = fLSTM

([
yt; c

e
t ; c

d
t ; flag

]
, st−1

)
(1)

where [·; ·] means the concatenation of vectors, cet
and cdt are two attention context vectors described
later, and flag is what we additionally concate-
nated to the input embedding, being either 1 or 0,

Ponder?

Ponder?

Yes

Yes

yt-1 yt yt+1

Pondering Mode

st+1st-1 st

depth2

Token Distribution

depth1

yt

yt

history
attention

encoder
attention

Figure 1: The illustration of our adaptive decoding. At-
tention and pondering mode are only shown at time
t for brevity. Every decoder will go into pondering
mode before the next timestep. The decoder cell is a
stacked two-layer LSTM and initialized by the last for-
ward states of the corresponeding encoder layer.

based on whether the model is acting in ponder-
ing mode or not, which will be introduced later.
We further apply a linear mapping and a softmax
function to the concatenation of st and attention
vectors to obtain the word predicting probabilities.
We greedily decode the tokens at testing time.

Attention. We adopt two types of attention
when decoding. One attends the decoder state
upon encoder outputs and yield the input context
vector cet ,

αet = Attn
(
st−1,

[
h
(2)
1 , · · · , h

(2)
m

])
cet = Softmax(α

d
t ) [s1, · · · , st−1] (2)

where [·, ·] means to vector stacking. The other
similarly attends the hidden state to previous de-
coder outputs, yielding the context vector cdt over
the decoding history. We use the bilinear function
for encoder attention Attn(x, y) = xTWy + b,
with trained parameters W and b, and use the
dot production function for decoding history at-
tentions Attn(x, y) = xT y.

Halting and Pondering. The key feature of our
model is to adaptively choose the decoder depth
before predicting tokens. Given the output state st



4267

from (1), the model goes into the pondering mode.
The output state st is further sent to a halting mod-
ule, which will generate a probability pt positively
correlated with the model uncertainty. We use an
MLP with ReLU and sigmoid activations for the
halting module. Then a choice is sampled from
the Bernoulli distribution determined by pt. If it
chooses to continue, we again use (1) to update the
state, meanwhile using the same embedding yt for
the input.

s
(k)
t = fLSTM

([
yt; c

e
k; c

d
k; flag

]
, s

(k−1)
t

)
(3)

where s0t = st, flag = 1, and c
e
k, c

d
k are attention

vectors recomputed with s(k−1)t using (2). The
model will keep pondering until it chooses to stop
or reaches our limit of k = 3. The final state s(k)t
will act as original st in (1) for other modules.

2.2 Uncertainty Estimates
Since the halting module outputs a Bernoulli dis-
tribution to guide the decoder, we have to pro-
vides some uncertainty quantification for train-
ing. Fortunately, Dropout (Srivastava et al., 2014)
was proved a good uncertainty estimate (Gal and
Ghahramani, 2016). It’s simple and effective that
neither the model nor the optimization algorithm
would need to be changed. We left other estimates
like those proposed in Dong et al. (2018) in future
work.

To estimate uncertainty with Dropout, we leave
the model in training mode and thus the Dropout
enabled. We run the forward pass of the equation
(3) for F times with the same inputs. Output states
are further sent to get token probabilities, q =
{p(ŷt = yt+1 | st) | Θi}Fi=1, where Θi is the set
of all pertubated parameters affected by Dropout
in the ith forward pass. We take the variance of
q to reflect model uncertainty U(st) = Var(q) as
suggested in Gal and Ghahramani (2016). We dis-
able the gradient propagation when computing the
variance such that the gradient-based optimization
is not influenced.

Note that the variance of a set of probabilities
many not be quite large in practice, we thus rescale
the variance to make it more numerically robust
Un(st) = min(γ, Var(q))/γ, where γ = 0.15 in
our case.

2.3 Learning
Our model consists of the Seq2Seq part (en-
coder, decoder, and attention) and the halting mod-

ule. For the former, we minimize the traditional
cross entropy loss with gradient decent, Jent =
E(x,y) log p(y | x).

We use the REINFORCE algorithm to opti-
mize the halting module. The module acts as
our policy network, by which the model consec-
utively make decisions from the action space A =
{Ponder, Stop}. Each time the model make a
choice a ∈ A, the uncertainty of the seq2seq part
is involved in the reward,

R(a | s(k)t )

=


Un(s

(k)
t ) if incorrect & a = 1

1− Un(s(k)t ) if correct & a = 0
0 otherwise

(4)

where a = 1 means a Ponder choice and a = 0
the other. We measure the correctness by examin-
ing the greedily decoded token if arg maxy p(y |
skt ) = yt+1. The model will be rewarded for a
Stop action if the prediction is correct, and for a
Ponder action if the prediction is incorrect. This
is similar to the ponder cost of ACT that does not
encourage excessive pondering steps.

3 Experiments

We compare our method with other models on
two datasets. Our codes could be obtained via
https://github.com/zxteloiv/AdaNSP.

3.1 Experimental Setup
Datasets. We use the preprocessed ATIS and Geo-
Query datasets kindly provided by Dong and La-
pata (2018). All natural language sentences are
converted to lower cases and stemmed with NLTK
(Bird et al., 2009). Entity mentions like city codes,
flight numbers are anonymized using numbered
placeholders.

Setups. We choose hyperparameters on the
ATIS dataset with the validation set. For the Geo-
Query dataset that doesn’t come with a validation
set, we randomly shuffle the training set and select
the top 100 records as the validation set, and the
remaining as the new training data. After choosing
the best hyperparameters, we resort back to train
on the original set. The Dropout ratio is selected
from {0.5, 0.6, 0.7, 0.8}, and the embedding di-
mension d is chosen from {64, 128, 256, 512}. We
fix the batch size to 20, and both the encoder and
decoder cell are two stacked LSTM layers. We ap-
ply scheduled sampling (Bengio et al., 2015) with



4268

the ratio 0.2 during training. We run F = 5 for-
ward passes before computing the variance. We
use Adam (Kingma and Ba, 2015) for the opti-
mizer, and use its default parameters from the pa-
per.

Evaluation. We use the logical form accuracy
as the evaluation metric, which is computed with
parsed trees of the predictions and true labels. Two
trees are considered identical as long as their struc-
tures are the same, i.e., the order to sibling pred-
icates doesn’t matter. We reuse the STree parser
code from Dong and Lapata (2018).

3.2 Results and Analysis

Our model outperforms the other comparative
neural semantic parsers on this two set. We
reuse the data from Dong and Lapata (2018) since
the datasets are identical. Results are listed in
Table 1. Our results are better than the SO-
TAs (Dong and Lapata, 2018; Yin and Neubig,
2018) even without any intermediate representa-
tions, whereas Coarse2fine defines a sketch and
TranX uses ASDL for every type of target se-
mantic sequences. We outperform Coarse2fine by
0.7% and 0.9% on GeoQuery and ATIS datasets
respectively. Although Jia and Liang (2016) has a
slightly better result on GeoQuery, they introduced
a synchronous CFG to learn new and recombi-
nated examples from the training data, which is
a novel method of data augmentation and requires
much human effort for preprocessing. For an abla-
tion test, our degenerated model without the pon-
dering part receives considerable performance de-
creases by 2.8% and 2.9% on GeoQuery and ATIS
datasets respectively.

Model Geo ATIS

ZC07 (Zettlemoyer and Collins, 2007) 86.1 84.6
λ-WASP (Wong and Mooney, 2007) 86.6 -
FUBL (Kwiatkowski et al., 2011) 88.6 82.8
TISP (Zhao and Huang, 2015) 88.9 84.2

Neural network models
Seq2Seq (Dong and Lapata, 2016) 84.6 84.2
Seq2Tree (Dong and Lapata, 2016) 87.1 84.6
JL16 (Jia and Liang, 2016) 89.3 83.3
TranX (Yin and Neubig, 2018) 88.2 86.2
Coarse2fine (Dong and Lapata, 2018) 88.2 87.7

AdaNSP (ours) 88.9 88.6
- halting module 86.1 85.7

Table 1: Results on GeoQuery and ATIS datasets

4 Related Work

Semantic Parsing. CCG or alignment-based
Parsers (Zettlemoyer and Collins, 2005, 2007;
Kwiatkowski et al., 2010; Wong and Mooney,
2006, 2007) try to model the correlation between
semantic tokens and lexical meaning of natural
language sentences. Methods based on depen-
dency trees (Ge and Mooney, 2009; Liang et al.,
2011; Reddy et al., 2016) otherwise convert out-
puts from an existing syntactic parser into seman-
tic representations, which can be easily adopted
in languages with much fewer resources than En-
glish. Recently neural semantic parsers, especially
under the encoder-decoder framework, also sprang
up (Dong and Lapata, 2016, 2018; Jia and Liang,
2016; Xiao et al., 2016). To make the model aware
of the underlying grammar of targets, people try to
exert constraints on the decoder side by sketches,
typing, grammars and runtime execution guides
(Dong and Lapata, 2018; Krishnamurthy et al.,
2017; Groschwitz et al., 2018; Wang et al., 2018).
Moreover, learning algorithms in SP like struc-
tural learning and maximum marginal likelihood
are combined with reinforcement algorithms (Guu
et al., 2017; Iyyer et al., 2017; Misra et al., 2018).

Adaptive Computing. Adaptive Computation
Times (ACT) was first proposed to adaptively
learn the depth of RNN models from data (Graves,
2016). Skip-RNN (Campos et al., 2018) used a
similar idea to equip a skipping mechanism with
existing RNN cells, which adaptively skip some
recurrent blocks along the computational graph
and thus saved many computations. BlockDrop
(Wu et al., 2018) also introduced the REINFORCE
algorithm to jointly learn a dropping policy and
discard some blocks of the ResNet by the policy
network. Recently, Dehghani et al. (2019) pro-
posed Universal Transformers (UT) as an alter-
native form of the vanilla Transformer (Vaswani
et al., 2017). It utilized ACT to control the recur-
rence times of the basic layer blocks (same param-
eters) in UT, instead of stacking different block
layers in the vanilla Transformer. This helped
UT mimic the inductive bias of RNNs and was
shown Turing-completed, and has outperformed
the vanilla Transformer in many tasks.

5 Conclusion

We present the AdaNSP that adaptively searches
the corresponding computation structure of RNNs
for semantic parsing. Our method does not need



4269

expert knowledge of intermediate structures of
the target sequences, and achieves stronger results
than the existing neural semantic parsers.

Acknowledgments

This work is supported by the Natural Sci-
ence Foundation of China (No.61533018),
the Natural Key R&D Program of China
(No.2018YFC0830101), the Natural Science
Foundation of China (No.61702512) and the
independent research project of National Labo-
ratory of Pattern Recognition. This work is also
supported by Alibaba Group through Alibaba
Innovative Research (AIR) Program, CCF-DiDi
BigData Joint Lab and CCF-Tencent Open
Research Fund.

References

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015. Scheduled Sampling for Se-
quence Prediction with Recurrent Neural Networks,
page 1171–1179. Curran Associates, Inc.

Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural language processing with Python: analyz-
ing text with the natural language toolkit. ” O’Reilly
Media, Inc.”.

Vı́ctor Campos, Brendan Jou, Xavier Giró i Nieto,
Jordi Torres, and Shih-Fu Chang. 2018. Skip RNN:
Learning to skip state updates in recurrent neural
networks. In International Conference on Learning
Representations.

Bo Chen, Le Sun, and Xianpei Han. 2018. Sequence-
to-action: End-to-end semantic graph generation for
semantic parsing. Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), 1:766–777.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals,
Jakob Uszkoreit, and Lukasz Kaiser. 2019. Uni-
versal transformers. In International Conference on
Learning Representations.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), page
33–43. Association for Computational Linguistics.

Li Dong and Mirella Lapata. 2018. Coarse-to-fine de-
coding for neural semantic parsing. Proceedings of
the 56th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
1:731–742.

Li Dong, Chris Quirk, and Mirella Lapata. 2018. Con-
fidence modeling for neural semantic parsing. Pro-
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), 1:743–753.

Yarin Gal and Zoubin Ghahramani. 2016. Dropout as a
bayesian approximation: Representing model uncer-
tainty in deep learning. In International Conference
on Machine Learning, page 1050–1059.

Ruifang Ge and Raymond J. Mooney. 2009. Learning
a compositional semantic parser using an existing
syntactic parser. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2 - Volume
2, ACL ’09, page 611–619. Association for Compu-
tational Linguistics.

Alex Graves. 2016. Adaptive computation time for
recurrent neural networks. arXiv:1603.08983 [cs].
ArXiv: 1603.08983.

Jonas Groschwitz, Matthias Lindemann, Meaghan
Fowlie, Mark Johnson, and Alexander Koller. 2018.
Amr dependency parsing with a typed semantic al-
gebra. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), page 1831–1841. Association
for Computational Linguistics.

Kelvin Guu, Panupong Pasupat, Evan Liu, and Percy
Liang. 2017. From language to programs: Bridg-
ing reinforcement learning and maximum marginal
likelihood. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), page 1051–1062. Associ-
ation for Computational Linguistics.

Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017.
Search-based neural structured learning for sequen-
tial question answering. Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), 1:1821–1831.

Robin Jia and Percy Liang. 2016. Data recombination
for neural semantic parsing. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), page
12–22. Association for Computational Linguistics.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. ICLR 2015.
ArXiv: 1412.6980.

Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gard-
ner. 2017. Neural semantic parsing with type con-
straints for semi-structured tables. Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, page 1516–1526.

Tom Kwiatkowski, Luke Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In Proceedings of the 2010 con-
ference on empirical methods in natural language

http://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf
http://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf
https://openreview.net/forum?id=HkwVAXyCW
https://openreview.net/forum?id=HkwVAXyCW
https://openreview.net/forum?id=HkwVAXyCW
https://openreview.net/forum?id=HyzdRiR9Y7
https://openreview.net/forum?id=HyzdRiR9Y7
https://doi.org/10.18653/v1/P16-1004
https://doi.org/10.18653/v1/P16-1004
http://proceedings.mlr.press/v48/gal16.html
http://proceedings.mlr.press/v48/gal16.html
http://proceedings.mlr.press/v48/gal16.html
http://dl.acm.org/citation.cfm?id=1690219.1690232
http://dl.acm.org/citation.cfm?id=1690219.1690232
http://dl.acm.org/citation.cfm?id=1690219.1690232
http://arxiv.org/abs/1603.08983
http://arxiv.org/abs/1603.08983
http://aclweb.org/anthology/P18-1170
http://aclweb.org/anthology/P18-1170
https://doi.org/10.18653/v1/P17-1097
https://doi.org/10.18653/v1/P17-1097
https://doi.org/10.18653/v1/P17-1097
https://doi.org/10.18653/v1/P17-1167
https://doi.org/10.18653/v1/P17-1167
https://doi.org/10.18653/v1/P16-1002
https://doi.org/10.18653/v1/P16-1002
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1412.6980
https://doi.org/10.18653/v1/D17-1160
https://doi.org/10.18653/v1/D17-1160
http://dl.acm.org/citation.cfm?id=1870777
http://dl.acm.org/citation.cfm?id=1870777
http://dl.acm.org/citation.cfm?id=1870777


4270

processing, page 1223–1233. Association for Com-
putational Linguistics.

Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in ccg grammar induction for semantic pars-
ing. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, page
1512–1523. Association for Computational Linguis-
tics.

Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies-Volume 1,
page 590–599. Association for Computational Lin-
guistics.

Dipendra Misra, Ming-Wei Chang, Xiaodong He, and
Wen-tau Yih. 2018. Policy shaping and general-
ized update equations for semantic parsing from de-
notations. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Pro-
cessing, page 2442–2452. Association for Compu-
tational Linguistics.

Siva Reddy, Oscar Täckström, Michael Collins, Tom
Kwiatkowski, Dipanjan Das, Mark Steedman, and
Mirella Lapata. 2016. Transforming dependency
structures to logical forms for semantic parsing.
Transactions of the Association for Computational
Linguistics, 4(0):127–140.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is
all you need. arXiv:1706.03762 [cs]. ArXiv:
1706.03762.

Chenglong Wang, Kedar Tatwawadi, Marc
Brockschmidt, Po-Sen Huang, Yi Mao, Olek-
sandr Polozov, and Rishabh Singh. 2018. Robust
text-to-sql generation with execution-guided decod-
ing. arXiv:1807.03100 [cs]. ArXiv: 1807.03100.

Yuk Wah Wong and Raymond J. Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, page 439–446. Association
for Computational Linguistics.

Yuk Wah Wong and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In Annual Meeting-
Association for computational Linguistics, vol-
ume 45, page 960.

Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar,
Steven Rennie, Larry S. Davis, Kristen Grauman,
and Rogerio Feris. 2018. Blockdrop: Dynamic in-
ference paths in residual networks. In The IEEE
Conference on Computer Vision and Pattern Recog-
nition (CVPR).

Chunyang Xiao, Marc Dymetman, and Claire Gardent.
2016. Sequence-based structured prediction for se-
mantic parsing. Proceedings Association For Com-
putational Linguistics, Berlin.

Pengcheng Yin and Graham Neubig. 2017. A syntac-
tic neural model for general-purpose code genera-
tion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 440–450. Association
for Computational Linguistics.

Pengcheng Yin and Graham Neubig. 2018. Tranx: A
transition-based neural abstract syntax parser for se-
mantic parsing and code generation. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing: System Demon-
strations, page 7–12. Association for Computational
Linguistics.

Tao Yu, Zifan Li, Zilin Zhang, Rui Zhang, and
Dragomir Radev. 2018a. Typesql: Knowledge-
based type-aware neural text-to-sql generation. Pro-
ceedings of the 2018 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Vol-
ume 2 (Short Papers), 2:588–594.

Tao Yu, Michihiro Yasunaga, Kai Yang, Rui Zhang,
Dongxu Wang, Zifan Li, and Dragomir Radev.
2018b. Syntaxsqlnet: Syntax tree networks for com-
plex and cross-domain text-to-sql task. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, page 1653–1663.
Association for Computational Linguistics.

Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the Twenty-First Con-
ference on Uncertainty in Artificial Intelligence,
UAI’05, page 658–666. AUAI Press. Event-place:
Edinburgh, Scotland.

Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed ccg grammars for parsing to
logical form. In EMNLP-CoNLL, page 678–687.

Kai Zhao and Liang Huang. 2015. Type-driven in-
cremental semantic parsing with polymorphism. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, pages 1416–1421. Association for Computa-
tional Linguistics.

http://dl.acm.org/citation.cfm?id=2145593
http://dl.acm.org/citation.cfm?id=2145593
http://dl.acm.org/citation.cfm?id=2145593
http://dl.acm.org/citation.cfm?id=2002547
http://dl.acm.org/citation.cfm?id=2002547
http://aclweb.org/anthology/D18-1266
http://aclweb.org/anthology/D18-1266
http://aclweb.org/anthology/D18-1266
http://arxiv.org/abs/1706.03762
http://arxiv.org/abs/1706.03762
http://arxiv.org/abs/1807.03100
http://arxiv.org/abs/1807.03100
http://arxiv.org/abs/1807.03100
http://dl.acm.org/citation.cfm?id=1220891
http://dl.acm.org/citation.cfm?id=1220891
http://dl.acm.org/citation.cfm?id=1220891
http://www.academia.edu/download/30624787/P07-1.pdf#page=998
http://www.academia.edu/download/30624787/P07-1.pdf#page=998
http://www.aclweb.org/anthology/P/P16/P16-1127.pdf
http://www.aclweb.org/anthology/P/P16/P16-1127.pdf
https://doi.org/10.18653/v1/P17-1041
https://doi.org/10.18653/v1/P17-1041
https://doi.org/10.18653/v1/P17-1041
http://aclweb.org/anthology/D18-2002
http://aclweb.org/anthology/D18-2002
http://aclweb.org/anthology/D18-2002
https://doi.org/10.18653/v1/N18-2093
https://doi.org/10.18653/v1/N18-2093
http://aclweb.org/anthology/D18-1193
http://aclweb.org/anthology/D18-1193
http://dl.acm.org/citation.cfm?id=3020336.3020416
http://dl.acm.org/citation.cfm?id=3020336.3020416
http://dl.acm.org/citation.cfm?id=3020336.3020416
http://www.aclweb.org/website/old_anthology/D/D07/D07-1.pdf#page=712
http://www.aclweb.org/website/old_anthology/D/D07/D07-1.pdf#page=712
http://www.aclweb.org/website/old_anthology/D/D07/D07-1.pdf#page=712
https://doi.org/10.3115/v1/N15-1162
https://doi.org/10.3115/v1/N15-1162

