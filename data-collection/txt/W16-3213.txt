



















































Detecting Visually Relevant Sentences for Fine-Grained Classification


Proceedings of the 5th Workshop on Vision and Language, pages 86–91,
Berlin, Germany, August 12 2016. c©2016 Association for Computational Linguistics

Detecting Visually Relevant Sentences for Fine-Grained Classification

Olivia Winn∗, Madhavan Kavanur Kidambi∗ and Smaranda Muresan†
∗Computer Science Department, Columbia University

† Center for Computational Learning Systems, Columbia University
olivia@cs.columbia.edu, mk3700@columbia.edu, smara@columbia.edu

Abstract

Detecting discriminative semantic at-
tributes from text which correlate with im-
age features is one of the main challenges
of zero-shot learning for fine-grained
image classification. Particularly, using
full-length encyclopedic articles as textual
descriptions has had limited success, one
reason being that such documents contain
many non-visual or unrelated sentences.
We propose a method to automatically
extract visually relevant sentences from
Wikipedia documents. Our model, based
on a convolutional neural network, is
robustly tested through ground truth
labeling obtained via Amazon Mechanical
Turk, achieving 81.73% F1 measure.

1 Introduction

Current research in multimodal fusion and cross-
modal mapping relies primarily on pre-aligned
datasets of images and their short captions or tags,
where the text is known to contain visually de-
scriptive content directly related to its image (Ba-
roni, 2016). These texts are usually manually col-
lected, and restricted in length to words, phrases,
and sentences. Using full-length documents such
as Wikipedia articles would potentially allow au-
tomated access to already available rich descrip-
tive content and would greatly aid the task of fine-
grained classification across numerous domains,
many of which have rich image datasets (such as
birds (Welinder et al., 2010), flowers (Nilsback
and Zisserman, 2008), aircraft (Maji et al., 2013),
and dogs (Khosla et al., 2011).)

Unfortunately, most full-length documents con-
tain predominantly non-visual text, making them
noisy with respect to visual information and lim-
iting the success of zero-shot learning techniques

Figure 1: Example Sentences from Wikipedia ar-
ticle on the Fish Crow.

for fine-grained classification (Elhoseiny et al.,
2013; Elhoseiny et al., 2015; Lei Ba et al., 2015).
Furthermore, the visual portion of the text often
describes objects outside the classifier’s interest,
such as the color of a bird’s eggs when the task is
identifying bird species (see Figure 1).

Thus, the question we address in this paper is
as follows: can we automatically identify visually
descriptive sentences relevant to a particular ob-
ject from documents that may contain predomi-
nantly non-visual text? We refer to this type of
sentence as ‘visually relevant’. Answering this
question would allow us to automatically build
aligned datasets of images with rich sentence-level
descriptions, removing the necessity of manually
creating aligned image-text datasets.

In this work, we focus on bird species, as this is
one of the most well-studied and challenging fine-
grained classification domains, using Wikipedia
articles as our text (Section 2). To build our com-
putational models, we must first define the notion
of ‘visually relevant’ sentences. We use the defini-

86



tion of Visually Descriptive Language (VDL) in-
troduced by Gaizauskas et al. (2015), with some
restrictions. Like VDL, we aim to identify ‘vi-
sually confirmed’ rather than ‘visually concrete’
segments of text as our descriptions correspond
to a class (the bird species) rather than a partic-
ular image. For example, a sentence describing a
bird’s feet can be a ‘visually relevant’ sentence for
a bird, though it would not be ‘visually concrete’
for an image of the bird flying with its feet hid-
den. Unlike VDL, for the scope of this paper we
are interested only in the sentences which are visu-
ally descriptive with respect to the object (i.e., bird
species). We define such sentences as containing
visually relevant language (VRL).

To build our training data, we make a simpli-
fying assumption: a sentence is only considered
to contain visually relevant language if it is in
the ‘Description’ section of the article. While
other sections may contain visually descriptive
language, we assume they describe other objects
such as the eggs. This simplifying assumption al-
lows us to approach our problem as a sentence
classification task (is a sentence VRL or non-
VRL), and provides an automatic, though noisy,
approach for labeling the training data. We collect
a dataset of 1150 Wikipedia articles about birds
to train the non-linear, non-consecutive convolu-
tion neural network architecture proposed by Lei
et al. (2015). The architecture of this particular
CNN is well suited to model sentences in our cor-
pus such as “Adults have upperparts streaked with
brown, grey, black and white” as it captures non-
consecutive grams such as “upperparts brown”,
“upperparts gray”, “streaked white”, etc.

To test our model in a robust manner, we
use crowdsourcing to manually annotate all sen-
tences as either VRL or non-VRL from an un-
seen set of 200 Wikipedia articles (for a total of
6342 sentences) (Section 2), corresponding to the
bird classes in the Caltech-UCSD Birds-200-2011
dataset (Welinder et al., 2010).

Our experiments show that the CNN model
trained on the noisy VRL dataset performs very
well when tested on a human-labeled VRL dataset:
83.4% Precision, 80.13% Recall, 81.73% F1 mea-
sure (Section 4). Our analysis highlights several
findings: 1) VRL sentences outside of the descrip-
tion section, or in documents with no Description
section, are properly labeled by the model as VRL;
2) non-VRL sentences within the Description sec-

Training Development
VRL 6355 794
non-VRL 27292 3411
Total 33647 4205

Table 1: Statistics of the Training and Dev. Sets

tion (many documents included descriptions of
birdsong in these sections) are correctly labeled by
the model as non-VRL (Section 4). The datasets,
including the crowdsourcing annotations for the
200 documents are released to the research com-
munity (http://github.com/oh-livia/
VRL_Wiki_Dataset). This dataset will be use-
ful to advance research on fine-grained classifi-
cation, given that the Caltech-UCSD Birds-200-
2011 is one of the most highly used datasets for
this task.

2 Datasets

To train our models we collected a set of 1150
Wikipedia articles of bird species. As a future
goal of this work is to correlate the extracted tex-
tual information with image data, the training doc-
uments were specifically chosen not to correspond
to the 200 birds species in the Caltech-UCSD
Birds-200-11 dataset, which were set aside as test
data. Of these 1150 documents, 690 of them con-
tained sections labeled “Description” or related
headings such as “Appearance”, which allowed us
to build our training and development sets. All
sentences in the sections labeled “Description”,
“Appearance” and “Identification” were consid-
ered instances of the VRL class and everything
else as instances of the non-VRL class; this label-
ing scheme we refer to as ‘noisy’. Table 1 shows
the statistics of the number of training and devel-
opment instances used to build the computational
models. The dataset is highly unbalanced: VRL
sentences comprise 19% of both training and de-
velopment. This skew is typical of many descrip-
tive documents, and as such provides an appropri-
ate model to train on.

To test our models we use the Wikipedia arti-
cles of the 200 birds in the Caltech-UCSD Birds-
200-11 previously collected by Elhoseiny et al.
(2013), consisting of 6342 sentences, which we
call 200V RL. To see whether our computational
models trained on the noisy VRL dataset are able
to detect VRL sentences as judged by humans, we
conducted a crowdsourcing experiment.

87



2.1 Crowdsourcing to Annotate Sentences as
Visually Relevant

We define a sentence-level annotation task, where
each sentence in a document is assigned one of the
following labels: 1 — the sentence contains visu-
ally relevant language (VRL), i.e. it is visually de-
scriptive with respect to the object under consid-
eration (birds species) (see examples (1) and (2));
and 0 — the sentence does not contain visually
relevant language (see examples (3), (4), (5)).

Label 1 (VRL sentence) is assigned when the
entire sentence is visually relevant (ex (1)) or when
it is partially visually relevant (e.g., in example (2)
only the underlined part is visually relevant):

(1) It has a black cap and a chestnut lower
belly

(2) Males give increasingly vocal displays
and show off the white markings of the
wings in flight and of the tail [...]

Label 0 (non-VRL sentence) is assigned when
the sentence describes the object of interest (bird
species) but it is not visually descriptive (ex (3)),
when it is visually descriptive but not relevant to
the object (ex (4)), or when it is neither visually
descriptive nor associated with the bird species.

(3) Males have 2 distinct types of songs -
classified as short and long songs.

(4) The egg coloring is a brown spotted
greenish-white.

(5) Finally volcanic eruptions on Tor-
ishima continues to be a threat.

In addition to the above labeling, for cases
where a Turker chose the label 1 they were asked
to provide information about the particular visu-
ally relevant text segments by specifying the bird,
the body part and the description. While these
phrase-level annotations are not used for our cur-
rent task, they could be used in future work when
joint-learning from text and images, especially to
align information related to each body part of the
bird. In addition, they could be used to build a
graph-based representation of image descriptions
similar to scene graphs (Schuster et al., 2015).

The annotation task was done at the sentence
level and each sentence was annotated by three
Turkers on Amazon Mechanical Turk. Besides
the two labels 1 and 0, the Turkers could also se-
lect “I don’t know” and provide an explanation for
why they could not determine whether or not the

sentence contains VRL. We used highly skilled
Turkers (≥ 500 completed HITS and ≥ 95% ap-
proval rate) and we paid 5 cents per HIT (each HIT
contained only one sentence). The inter-annotator
agreement was very high, with a Fleiss K score
of 0.8273. Only 8.64% of the sentences did not
have a unanimous vote. Less than 2% of the sen-
tences had at least one Turker vote ‘I don’t know’;
of these, less than 0.05% garnered one vote each
of 1, 0 and ‘I don’t know’.

To build the test set for the computational mod-
els we use majority voting (at least two annotators
selected the label). For the few cases where we
did not have majority voting (0.05% of data) we
selected the 0 label, as only one Turker voted 1
while the other two said 0 and ‘I don’t know’. This
test set, which we call 200HumV RL, contains 1248
sentences of class 1 (VRL) and 5094 sentences of
class 0 (non-VRL).

3 Detecting Visually Relevant Sentences

As mentioned earlier, our task can be framed as
a binary sentence classification problem, where
each sentence is labeled either as VRL or non-
VRL. Deep learning methods, and in particular
convolutional neural networks (CNNs), have be-
come some of the top performing methods on
various NLP tasks that can be modeled as sen-
tence classification (e.g, sentiment analysis, ques-
tion type classification) (Kim, 2014; Kalchbrenner
et al., 2014; Lei et al., 2015).

We use the non-linear, non-consecutive con-
volution neural network architecture proposed by
Lei et al. (2015), which we refer to as CNNLei.
This CNN uses tensor products to combine non-
consecutive n-grams of each sentence to create
an embedding per sentence. The non-consecutive
aspect of the n-gram allows it to capture co-
occurrence of words spread across sentences:
“yellow crown, rump and flank patch” will gener-
ate representations of the relevant noun-adjective
pairs “yellow crown”, “yellow rump”, and “yel-
low flank patch”. The tensor product is used as a
“generalized approach” to linear concatenation of
the n-grams, as concatenation is “insufficient to di-
rectly capture relevant information in the n-gram”
(Lei et al., 2015, p 1). We use the training and
development set described in Table 1 that comes
from the 690 documents with ‘Description’ head-
ings.

88



Hyperparameters and Word Vectors. The
word vectors are pre-trained on the entire set
of 1150 Wikipedia articles about birds using the
word2vec model of Mikolov et al. (2013) with a
window context of 20 words and vectors of 150
dimensions. Notice that we do not use the docu-
ments in the test set 200V RL for training the word
vectors. We chose to use domain specific text to
pre-train the word vectors in order to make sure we
are capturing domain specific semantics such as
proper word senses. Words such as “crown”, when
trained on a different corpus, would typically have
an embedding very close to words such as “roy-
alty”, “tiara”, etc; in the domain of bird descrip-
tions, “crown” maps most closely to “feathers”
and “head”. The hyperparameters for the CNN
model are: L2 regularization weight is 0.0001, n-
gram order is 3 and hidden feature dimension is
50.

4 Experimental Setup and Results

Test Datasets. We first evaluate the CNNLei
model on the 200HumV RL dataset described in
Section 2, which contains the 6342 sentences la-
beled by Turkers (class distribution: 1248 sen-
tences in class 1 and 5094 sentences in class 0).
Since our computational model was trained on the
noisy visually relevant sentences (where the labels
were determined by the ‘Description’ section of
the documents), we wanted to evaluate how the
model performed on a similarly constructed test
set. Thus, instead of considering the human labels
for the 6342 sentences, a sentence was assigned to
class 1 if it belonged to the Description, Appear-
ance or Identification sections and to class 0 oth-
erwise. We call this dataset 200NoisyV RL (class
distribution: 1258 sentences in class 1 and 5084
sentences in class 0). Note that while it seems as
if only 10 sentences changed, many of the sen-
tences in the ‘Description’ sections were labeled
by humans as class 0, and many sentences outside
these sections labeled as class 1. However, one
possible issue with the 200NoisyV RL dataset is that
some documents do not contain any description-
type sections and thus all sentences are labeled 0,
which might affect measuring the performance of
the model. Thus, we considered additional test
sets containing only the documents that had sec-
tions labeled with ‘Description’, ‘Appearance’ or
‘Identification’ (142 documents out of the origi-
nal 200 documents). Using these documents, we

constructed a dataset 142NoisyV RL, where class
1 contained sentences that were part of the three
description-type sections, and class 0 contained
all other sentences (class distribution: 1156 class
1 and 3836 class 0). In addition, we also used
the Turkers’ labels (majority voting) for the corre-
sponding sentences in these 142 documents. We
call this dataset 142HumV RL (class distribution:
992 class 1 and 4000 class 0). Since the CNN
model was trained on the noisy labeling, a rea-
sonable assumption is that the classification re-
sults would be better on the 200NoisyV RL and
142NoisyV RL datasets than on the 200HumV RL
and 142HumV RL datasets.

Baseline. As baseline, we used the same neural
bag-of-words model (nBoW) as Lei et al. (2015).
We use the same training and development sets as
for the CNN model (Table 1), along with the same
word embeddings.

Results and Discussion. Table 2 shows the re-
sults of the CNNLei model and the nBoW model
on the four datasets. The CNN model performs
slightly better than the baseline on all datasets in
terms of F1 measure, with a much better Recall
but worse Precision. Given that the end goal is
to use the extracted visually relevant sentences to-
gether with images for fine-grained classification,
and that the amount of visually relevant sentences
in a document is small with respect to the docu-
ment length, having high Recall is important.

One of the most interesting findings of this
study is that both of the computational models
perform much better on the human-labeled visu-
ally relevant datasets (200HumV RL, 142HumV RL)
than on the noisy visually relevant datasets
(200NoisyV RL, 142NoisyV RL). In particular, the
recall increases significantly (e.g., from 63.24%
on 142NoisyV RL to 80.15% on 142HumV RL using
the CNNLei model).

An error analysis highlights that the compu-
tational models are more ‘conservative’ with the
classification of VRL than the noisy labeling. As
mentioned earlier, the Description sections of the
Wikipedia articles often (though not always) con-
tain details pertaining to the birds’ song. However,
despite being trained on such a labeling, the com-
putational models do not classify most sentences
related primarily to the description of birds’ song
as VRL. This result was most likely aided by the
fact that some of the training documents contain

89



Models 200HumV RL 200NoisyV RL 142HumV RL 142NoisyV RLP(%) R(%) F(%) P(%) R(%) F(%) P(%) R(%) F(%) P(%) R(%) F(%)
CNNLei 83.40 80.13 81.73 66.06 62.96 64.47 82.94 80.15 81.52 82.04 63.24 71.42
nBoW 88.61 73.56 80.39 67.31 55.33 60.73 88.48 73.32 80.19 84.11 55.88 67.15

Table 2: Classification results on the four datasets

song descriptions outside of the description-type
sections, so the words pertaining to sound were
not correlated as strongly with the VRL class. It
is also possible that the abundance of appearance
descriptions in each description section would en-
courage the visual words to have a much stronger
effect on the ‘visualness’ of a sentence. One such
example is the sentence “The song is a series of
musical notes which sound like: wheeta wheeta
whee-tee-oh, for which a common pneumonic is
‘The red, the red T-shirt’”. Even the repetition of
the word ‘red’ is not enough to make the classifier
label the sentence as VRL.

Another type of example that explains these re-
sults are sentences that describe the weight of the
birds, such as “Recorded weights range from 0.69
to 2 kg,[...]” These sentences were part of the De-
scription section, but were not marked as VRL by
either the Turkers or the computational models.

We also analyzed some of the false positives
of the CNNLei model on the 142HumV RL and
200HumV RL datasets. One type of error comes
from sentences that are visually descriptive, but
not visually relevant, such as sentences that de-
scribe other objects like eggs. For example, the
sentence “The egg shells are of various shades
of light or bluish grey with irregular, dark brown
spots or greyish-brown splotches” was labeled
as VRL by the model but not by the Turkers.
More interesting are the false positives that con-
tain comparison words such as “clapping or click-
ing has been observed more often in females than
in males”, and words having to do with appear-
ance that do not specifically describe how the bird
looks such as “this bird is more often seen than
heard”.

5 Related Work

There are two lines of work most closely related
to ours. First, Gaizauskas et al. (2015) propose
a definition and typology of Visually Descriptive
Language (VDL). They show that humans are able
to reliably annotate text segments as containing
‘visually descriptive’ language or not, providing
evidence that standalone text can be classified by

the visualness of its contents. In our work, mo-
tivated by the end task of fine-grained classifica-
tion, we restrict the definition to ‘visually rele-
vant’. As Gaizauskas et al. (2015) do, we show
that humans can reliably annotate text as visually
relevant or not. Unlike Gaizauskas et al. (2015),
we propose a method to automatically detect visu-
ally relevant sentences from full-text documents.
Second, Dodge et al. (2012) propose a method to
separate visual text from non-visual text in image
captions. However, their method focuses just on
noun-phrases, while our approach finds visually
relevant sentences in full-length documents.

While our end result is a set of visually relevant
text descriptions, our approach is complementary
to the rich body of work on generating text de-
scriptions from images (see (Bernardi et al., 2016)
for a survey), since our method extracts such de-
scriptions from existing text.

6 Conclusion

Our work shows that it is possible to take
domain-specific full-length documents—such as
Wikipedia articles for birds species—and classify
their sentences by visual relevancy using a CNN
model trained on a noisy dataset. As many doc-
uments generally have a small proportion of vi-
sually relevant sentences, this approach automati-
cally generates high quality visually relevant tex-
tual descriptions for images to be used by zero-
shot learning approaches for fine-grained image
classification tasks (e.g., (Wang et al., 2009)).
While our study has focused on bird species,
we believe that this method is generally applica-
ble for other domains used in fine-grained clas-
sification research such as flowers and dogs (all
have associated Wikipedia articles and Descrip-
tion/Appearance sections). In future work, we
plan to use the outcomes of this work for joint
learning from text and images.

Acknowledgments

This research was funded by the NSF (award IIS-
409257). We thank the anonymous reviewers for
helpful feedback.

90



References
M. Baroni. 2016. Grounding distributional semantics

in the visual world. Language and Linguistics Com-
pass, 10(1):3–13.

Raffaella Bernardi, Ruket Cakici, Desmond Elliott,
Aykut Erdem, Erkut Erdem, Nazli Ikizler-Cinbis,
Frank Keller, Adrian Muscat, and Barbara Plank.
2016. Automatic description generation from im-
ages: A survey of models, datasets, and evalua-
tion measures. Journal of Artificial Intelligence Re-
search, 55.

Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-
sch, Margaret Mitchell, Karl Stratos, Kota Yam-
aguchi, Yejin Choi, Hal Daumé III, Alexander C
Berg, et al. 2012. Detecting visual text. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
762–772. Association for Computational Linguis-
tics.

Mohamed Elhoseiny, Babak Saleh, and Ahmed Elgam-
mal. 2013. Write a classifier: Zero-shot learning
using purely textual descriptions. In Proceedings of
the IEEE International Conference on Computer Vi-
sion, pages 2584–2591.

Mohamed Elhoseiny, Ahmed Elgammal, and Babak
Saleh. 2015. Tell and predict: Kernel classifier pre-
diction for unseen visual classes from unstructured
text descriptions. IEEE Conference on Computer Vi-
sion and Pattern Recognition Workshops (CVPR).

Robert Gaizauskas, Josiah Wang, and Arnau Ramisa.
2015. Defining visually descriptive language. In
Proceedings of the 2015 Workshop on Vision and
Language (VL15): Vision and Language Integration
Meets Cognitive Systems.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, ACL 2014, June 22-27, 2014,
Baltimore, MD, USA, Volume 1: Long Papers, pages
655–665.

Aditya Khosla, Nityananda Jayadevaprakash, Bang-
peng Yao, and Li Fei-Fei. 2011. Novel dataset for
fine-grained image categorization. In First Work-
shop on Fine-Grained Visual Categorization, IEEE
Conference on Computer Vision and Pattern Recog-
nition, Colorado Springs, CO, June.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1746–1751.

Jimmy Lei Ba, Kevin Swersky, Sanja Fidler, et al.
2015. Predicting deep zero-shot convolutional neu-
ral networks using textual descriptions. In Proceed-
ings of the IEEE International Conference on Com-
puter Vision, pages 4247–4255.

Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2015.
Molding cnns for text: Non-linear, non-consecutive
convolutions. In Proceedings of the 2015 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP).

S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and
A. Vedaldi. 2013. Fine-grained visual classification
of aircraft. Technical report.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. Advances in Neural infgormation Processing
Systems, pages 3111–3119.

M-E. Nilsback and A. Zisserman. 2008. Automated
flower classification over a large number of classes.
In Proceedings of the Indian Conference on Com-
puter Vision, Graphics and Image Processing, Dec.

Sebastian Schuster, Ranjay Krishna, Angel Chang,
Li Fei-Fei, and Christopher D Manning. 2015. Gen-
erating semantically precise scene graphs from tex-
tual descriptions for improved image retrieval. In
Proceedings of the Fourth Workshop on Vision and
Language, pages 70–80.

Josiah Wang, Katja Markert, and Mark Everingham.
2009. Learning models for object recognition from
natural language descriptions. In British Machine
Vision Conference (BMVC), volume 1, page 2.

P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff,
S. Belongie, and P. Perona. 2010. Caltech-UCSD
Birds 200. Technical Report CNS-TR-2010-001,
California Institute of Technology.

91


