



















































Syntactically Aware Neural Architectures for Definition Extraction


Proceedings of NAACL-HLT 2018, pages 378–385
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Syntactically Aware Neural Architectures for Definition Extraction

Luis Espinosa-Anke and Steven Schockaert
School of Computer Science and Informatics

Cardiff University
{espinosa-ankel,schockaerts1}@cardiff.ac.uk

Abstract

Automatically identifying definitional knowl-
edge in text corpora (Definition Extraction or
DE) is an important task with direct appli-
cations in, among others, Automatic Glos-
sary Generation, Taxonomy Learning, Ques-
tion Answering and Semantic Search. It is
generally cast as a binary classification prob-
lem between definitional and non-definitional
sentences. In this paper we present a set of
neural architectures combining Convolutional
and Recurrent Neural Networks, which are
further enriched by incorporating linguistic in-
formation via syntactic dependencies. Our ex-
perimental results in the task of sentence clas-
sification, on two benchmarking DE datasets
(one generic, one domain-specific), show that
these models obtain consistent state of the
art results. Furthermore, we demonstrate that
models trained on clean Wikipedia-like defini-
tions can successfully be applied to more noisy
domain-specific corpora.

1 Introduction

Dictionaries and glossaries are among the most
important sources of meaning for humankind.
Compiling, updating and translating them has tra-
ditionally been left mostly to domain experts and
professional lexicographers. However, the last
two decades have witnessed a growing interest in
automating the construction of lexicographic re-
sources.

Analogously, in Natural Language Processing
(NLP), lexicographic resources have proven use-
ful for a myriad of tasks, for example Word Sense
Disambiguation (Banerjee and Pedersen, 2002;
Navigli and Velardi, 2005; Agirre and Soroa,
2009; Camacho-Collados et al., 2015), Taxonomy
Learning (Velardi et al., 2013; Espinosa-Anke
et al., 2016b) or Information Extraction (Richard-
son et al., 1998; Delli Bovi et al., 2015). Moreover,

lexicographic information such as definitions con-
stitutes the cornerstone of important language re-
sources for NLP, such as WordNet (Miller et al.,
1990), BabelNet (Navigli and Ponzetto, 2012),
Wikidata (Vrandečić and Krötzsch, 2014) and ba-
sically any Wikipedia-derived resource.

In this context, systems able to address the prob-
lem of Definition Extraction (DE), i.e., identifying
definitional information spanning in free text, are
of great value both for computational lexicogra-
phy and for NLP. In the early days of DE, rule-
based approaches leveraged linguistic cues ob-
served in definitional data (Rebeyrolle and Tan-
guy, 2000; Klavans and Muresan, 2001; Malaisé
et al., 2004; Saggion and Gaizauskas, 2004; Stor-
rer and Wellinghoff, 2006). However, in order
to deal with problems like language dependence
and domain specificity, machine learning was in-
corporated in more recent contributions (Del Gau-
dio et al., 2013), which focused on encoding infor-
mative lexico-syntactic patterns in feature vectors
(Cui et al., 2005; Fahmi and Bouma, 2006; West-
erhout and Monachesi, 2007; Borg et al., 2009),
both in supervised and semi-supervised settings
(Reiplinger et al., 2012; Faralli and Navigli, 2013).

On the other hand, while encoding definitional
information using deep learning techniques has
been addressed in the past (Hill et al., 2015; No-
raset et al., 2016), to the best of our knowledge
no previous work has tackled the problem of DE
by reconciling both the linguistic lessons learned
in the past decades (e.g., the importance of lex-
ico syntactic patterns or long-distance relations
between definiendum and definiens)1 and the pro-
cessing potential of neural networks.

Thus, we propose to bridge this gap by learn-
ing high level features over candidate definitions

1Traditionally, a definienidum is a term being defined,
whereas the definiens refers to its differentiable characteris-
tics.

378



via convolutional filters, and then apply recurrent
neural networks to learn long term dependencies
over these feature maps. Without preprocessing
and only taking pretrained embeddings as input,
it is already possible to consistently obtain state
of the art results in two benchmarking datasets
for DE (one generic, one domain-specific). Fur-
ther improvements over this simple model are ob-
tained by incorporating syntactic information by
composing and embedding head-modifier syntac-
tic dependencies and dependency labels. One in-
teresting side result of our experiments is the ob-
servation that a model trained only on canonical
wikipedia-like definitions performs significantly
better in a domain-specific academic setting than a
model that has been trained on that domain, which
somewhat contradicts previously assumed notions
about the creativity of academic authors when pre-
senting and describing novel terminology.2

2 Method

The impact of deep learning methods in NLP is
today indisputable. The utilization of neural net-
works has improved the state of the art almost sys-
tematically in a wide number of tasks, from lan-
guage modeling (Bengio et al., 2003; Yih et al.,
2011; Mikolov et al., 2013) to text classification
(Kim, 2014) or machine translation (Bahdanau
et al., 2014), among many others.

In this paper we leverage two of the most popu-
lar architectures in deep learning for NLP with the
goal to predict, given an input sentence, its proba-
bility of including definitional knowledge. In our
best performing model we take advantage of Con-
volutional Neural Networks (CNNs) to learn lo-
cal features via convolved filters (LeCun et al.,
1998), and then apply to the learned feature maps
a Bidirectional Long Short Term Memory (blstm)
network (Hochreiter and Schmidhuber, 1997). In
this way, we aim at capturing ngram-wise features
(Zhou et al., 2015), which may be strong indica-
tors of definitional patterns (e.g., the classic X is
a Y pattern), combined with the learning of long-
term sequential dependencies over these learned
feature maps.

Following standard notation for sentence mod-
eling via CNNs (Kim, 2014), we let xi ∈ Rk be
the k-dimensional word vector associated to the i-
th word in an input sentence S. We use as pre-

2Code available at bitbucket.org/
luisespinosa/neural_de

trained embeddings the word2vec (Mikolov et al.,
2013) vectors trained with negative sampling on
the Google News corpus3. Each sentence is repre-
sented as an n× k matrix S , where n is the size of
the longest sentence in the corpus (using padding
where necessary). The convolution layer applies a
filter wj ∈ R(h+1)k to each ngram window of h+1
tokens. Specifically, writing xi:i+h for the con-
catenation of the word vectors xi,xi+1, ...,xi+h,
we have:

cij = f (wj · xi:i+h + bj)

where bj ∈ R is a bias term and f is the ReLu ac-
tivation function (Nair and Hinton, 2010). In total,
we use 100 such convolutional features, i.e. we use
the vector ci =

[
ci1, c

i
2, · · · , ci100

]
to encode the ith

ngram. We empirically set the length h+1 of each
ngram to 3. To reduce the size of the representa-
tion, we then use a max pooling layer with a pool
size of 4. Let us write di =

[
di1, d

i
2, · · · , di97

]
,

where dij = max(d
i
j , d

i+1
j , d

i+2
j , d

i+3
j ). The in-

put sentence S is then represented as the sequence
d1,d5,d9, ...,dn−3, which is used as the input
to a bidirectional LSTM (BLSTM) layer. Finally,
the output vectors of the final states for both di-
rections of this BLSTM are connected to a single
neuron with a sigmoid activation function. In all
the experiments reported in this paper, we classify
a sentence as definitional when the output of this
neuron yields a value which is at least 0.5.

2.1 Incorporating Syntactic Information

The role of syntax has been extensively stud-
ied for improving semantic modeling of domain
terminologies. Examples where syntactic cues
are leveraged include medical acronym expansion
(Pustejovsky et al., 2001), hyponym-hypernym
extraction and detection (Hearst, 1992; Shwartz
et al., 2016), and definition extraction either from
the web (Saggion and Gaizauskas, 2004), schol-
arly articles (Reiplinger et al., 2012), and more
recently from Wikipedia-like definitions (Boella
et al., 2014).

However, the interplay between syntactic infor-
mation and the generalization potential of neural
networks remains unexplored in definition mod-
eling, although intuitively it seems reasonable to
assume that a syntax-informed architecture should
have more tools at its disposal for discriminating

3code.google.com/archive/p/word2vec/

379



between definitional and non-definitional knowl-
edge. As an example of the importance of syntax
in encyclopedic definitions, among the definitions
contained in the WCL definition corpus (see Sec-
tion 3.1), 71% of them include the lexico-syntactic

pattern noun
subj←−−is dobj−−→ noun. To explore the

potential of syntactic information, we represent
dependency-based phrases by embedding them in
the same vector space as the pretrained word em-
beddings introduced above. This approach draws
from previous work on modeling phrases by com-
posing their parts and the relations that link them
(Socher et al., 2011, 2013, 2014).

Specifically, let Sd be the list of head-modifier
relations obtained by parsing4 sentence S. Each
relation r in Sd is a head-modifier tuple 〈h,m, l〉.
Here l denotes the dependency label of the rela-
tion (e.g., nsubj), which we represent as the vector
r = 12(h+m), with h and m the vector represen-
tations of words h and m respectively. This setting
for composing first-order head-modifier relations
is similar to the one proposed in Dyer et al. (2015)
for dependency parsing. This leads to a repre-
sention of the sentence as a sequence r1, ..., r|Sd|,
which preserves the original order of head words.
The intuition is that this “coarser” grained sort-
ing5 provides integrated semantic-syntactic infor-
mation that can be leveraged both by the convo-
lutional feature extraction step, and more impor-
tantly, by the sequential BLSTM module.

Then, for each sentence we concatenate the
dependency-based representation r1, ..., r|Sd| to
the word vector sequence x1, ...,xn, to obtain the
input to the convolutional layer of our model. It
is worth mentioning that we tried different merg-
ing schemes (concatenation, but also dot product
and averaging) at different layers, and found that
the best way to inform our neural definition ex-
tractor is to encode this syntactic information ex-
plicitly at input time. Finally, we also explore the
effect of enriching the input representation with
the information of the dependency label. For each
sentence, we enrich each head-modifier mean vec-
tor ri by concatenating them a one-hot representa-
tion of their corresponding dependency label. The
search space of these labels is 46 (e.g., nsubj or
dobj). An illustrative diagram of our proposed ar-
chitecture is provided in Figure 1.

4We use the dependency parser provided in the SpaCy
NLP library: spacy.io.

5It is coarser because in a dependency tree modifiers nat-

...

...

300

346

word 1

word n

...

word 1 word j

nsubj

word n word i

conj

...

100

...

100

...

...

embeddi ngi nput CNN Max Pool i ng bl st m

si gmoi d

dr opout dr opout

Figure 1: Architecture of our proposed definition ex-
traction model. Input may be either simple pretrained
embeddings or syntactically enriched representations
(separated by the dotted line).

3 Evaluation

3.1 Evaluation data

WCL: The WCL (World-Class Lattices) dataset
(Navigli et al., 2010) consists of manually anno-
tated Wikipedia definitions and distractors (1,871
and 2,847 respectively). These distractors are sen-
tences that also include the term (i.e., the Wikpe-
dia page title) and are what the authors call “syn-
tactically plausible false definitions”. The style
of the definitions is fairly consistent, and follows
in most cases the Aristotelian genus et differentia
structure of a definition (A is a B which C). We
list below both an example definition and one of
its distractors:

3 The Amiga is a family of personal computers
originally developed by Amiga Corporation.

7 Development on the Amiga began in 1982
with Jay Miner as the principal hardware de-
signer.

W00: Introduced in Jin et al. (2013), this cor-
pus consists of a collection of 731 definition sen-
tences compiled from the ACL-ARC anthology
(Bird et al., 2008), and 1454 distractors. Their
style is different6, as they are used mostly for
introducing and describing novel terminology in
NLP research papers. Let us show an example for
each sentence class:

urally lose their original order.
6In lexicographic terms, most definitions in this dataset

would be classified either as extensional (definition without
hypernym) or functional (define something by what it does.
instead of what it is).

380



3 Our system, SNS (pronounced “essence”),
retrieves documents related to an unre-
stricted user query and summarizes a subset
of them as selected by the user.

7 The senses with the highest confidence scores
are the senses that contribute the most to the
maximization function for the set .

3.2 Baselines

Let us provide a succint description of each com-
peting baseline. (1) WCL: An algorithm that
learns word-class lattices for modeling higher-
level features over shallow parsing and part
of speech (Navigli and Velardi, 2010). (2)
DefMiner: A CRF-based sequential modeling
system trained with lexical, terminological and
structural (e.g., document position) features (Jin
et al., 2013). (3) B&DC: A binary classifier
trained with dependency paths over input sen-
tences (Boella et al., 2014). (4) E&S: A sys-
tem based on more complex dependency-based
features (Espinosa-Anke and Saggion, 2014). (5)
LSTM-POS: An LSTM-based system which rep-
resents each sentence as a mixture of infrequent
words and frequent words’ associated part-of-
speech (Li et al., 2016).

As for our proposed models, we include results
for a CNN architecture alone (CNN), as well as
for the proposed CNN and BLSTM (C-BLSTM)
combination. For both architectures, subscripts d
or l denote the syntactically informed variant with-
out and with one-hot label encoding information,
respectively. Finally, among the many hyperpa-
rameters that can be explored, we report the im-
pact of the dimensionality of the output vectors of
the BLSTM layer, with sizes of 100 and 300. We
did not attempt to tune the other hyperparameters.

Experiment 1: In-domain 10-fold CV
In this experiment, we compare the performance
of different configurations of our proposed model
with previous contributions in a 10-fold cross val-
idation (CV) setting. The experimental results,
listed in Table 1, show that a fairly simple CNN ar-
chitecture with no preprocessing already achieves
remarkably strong results, especially for the WCL
dataset. Among our proposed systems, the overall
best performance in Wikipedia definitions is ob-
tained by the CNNl configuration. However, in-
corporating a BLSTM layer contributes towards
the best performing model on the NLP-specific

WCL W00

P R F1 P R F1

WCL 98.8 60.7 75.2 - - -

DefMiner 92.0 79.0 85.0 - - -

B&DC 88.0 76.0 81.6 - - -

E&S 85.9 85.3 85.4 - - -

LSTM-POS 90.4 92.0 91.2 - - -

CNN 91.1 92.0 91.5 33.5 68.7 44.8

CNNd 90.6 90.9 90.7 34.2 69.4 45.8

CNNl 94.2 94.2 94.2 42.8 65.5 51.3

C-BLSTM100 93.3 91.8 92.5 46.1 68.7 54.5

C-BLSTM100d 93.2 92.2 92.6 52.0 67.6 57.4

C-BLSTM100l 93.2 92.7 92.9 51.7 66.2 57.3

C-BLSTM300 93.4 92.3 92.7 48.9 64.5 54.0

C-BLSTM300d 94.3 91.0 92.6 47.3 64.0 51.9

C-BLSTM300l 94.0 90.7 92.5 50.0 64.5 53.8

Table 1: Comparative results between previous con-
tributions and different configurations of our proposed
contribution.

dataset (C-BLSTM100d). Several conclusions can
be drawn from these results. First, CNNs are ca-
pable of capturing a great deal of Wikipedia-like
definitional information. This probably owes to
the fairly recurrent linguistic structure of these
definitions. On the contrary, however, LSTMs
seem necessary in more complex scenarios, e.g.,
in those presented in the W00 dataset. Here, we
argue that long term dependencies may play an
important role, for example, for capturing cases
where a full-fledged definitions appear spanning
only over the last tokens of a sentence. Finally,
syntax seems to help for most configurations, and
for both datasets, although the difference is more
pronounced in the more challenging W00 dataset.

These differences in performance are, however,
small enough to make it difficult to draw strong
conclusions other than that neural network archi-
tectures are a sensible choice for this task, and
that syntax can play an important role depending
on the type of data to be processed. It is impor-
tant to highlight, finally, that depending on the ap-
plication, one may be more interested in having
an almost perfect precision (as in the system de-
scribed in Navigli and Velardi (2010)). For auto-
matic glossary generation from text, on the other
hand, having a more balanced model, with high re-

381



call at the expense of only slightly lower precision,
may be preferred, as automatic glossaries usually
undergo a human post-editing and revision step.

Experiment 2: Cross-domain DE

In this experiment we assess the performance of a
cross-domain model on the W00 dataset (cf. Sec-
tion 3.1). The main goal is to verify to what ex-
tent a model trained only on Wikipedia-like def-
initions can do well in a domain-specific setting.
To this end, we apply our best performing config-
uration trained on the whole WCL corpus to the
W00 dataset (WCL>W00), and compare it with
the performance of our best configuration as per
10-fold CV (C-BLSTM100d, see Table 1). This
experiment is important, for example, for learn-
ing what would be more appropriate if we were
to aim at constructing domain-specific glossaries
or at extracting highly specific semantic relations
from a domain terminology.

System Precision Recall F-Score
C-BLSTM100d 52.0 67.6 57.4
WCL>W00 69.0 71.0 70.0

Table 2: Results of our proposed model (with two dif-
ferent training schemes) on the NLP-specific W00 def-
inition dataset.

The results in Table 2 reveal that, despite differ-
ences in style, a system modeled over encyclope-
dic definitions outperforms a neural model trained
only on these idiosyncratic definitions. This might
be due to several reasons. First, because of the
slightly smaller size of this dataset. And second,
the more noisy nature of the corpus may pose a
stronger challenge for a neural model to identify
recurrent definitional patterns. Still, our experi-
mental results seem to suggest that these patterns
do exist, as evidenced by the strong performance
of the Wikipedia-trained model.

Qualitative Evaluation

We run our best performing model over a subset
of the ACL-ARC anthology (Bird et al., 2008),
specifically the subcorpus described in (Espinosa-
Anke et al., 2016a), which removed noisy sen-
tences as produced by the pdf to text conversion.

In Table 3 we show three high quality defini-
tions discovered by our model, as well as three
false positives. We may highlight the somewhat
surprising remarkable capacity of the model to
identify definitions beyond the is-a pattern (e.g.,

using the verb ‘mean’) and with long-distance de-
pendencies between subject and object. As for the
incorrect cases, we find that for this model to be
used in the automatic glossary construction task,
in addition to further refinement, it would have to
be coupled with a term extraction system so that
only definitions associated to meaningful domain
terms are extracted.

compositional grammar means that the

semantics of a a phrase is composed of

the semantics of the subphrases

f-score is the harmonic mean of recall

(r) and precision (p) percentages

silc is a language and encoding

identification system developed by

the rali laboratory at the university

of montreal

the main lesson is that complex

sentences are analysed with a proper

understanding without sacrificing

efficiency

a simple spell correction is a part

of the system (essentially 1 character

errors)

the segmentation of a translation

memory is a key feature for our system

Table 3: Examples of extracted definitions with over
0.9 confidence from a subset of the ACL-ARC corpus.

4 Conclusion

We have presented and evaluated a neural model
based on CNNs and Bidirectional LSTMs which
obtains state of the art results on two well known
definition extraction datasets. From our experi-
ments, it stems that: (1) Neural network archi-
tectures perform well for identifying definitional
text snippets in corpora, more so with syntactic in-
formation; (2) A model trained on Wikipedia is
competitive even in a domain-specific setting; and
(3) More complex linguistic structures seem to be
better captured with more complex models. As
for future work, it would be interesting to explore
whether meaningful further gains can be obtained
by performing hyperparameter tuning.

Acknowledgments

We thank the anonymous reviewers for their help-
ful comments. We would also like to thank Miguel
Ballesteros for fruitful discussions. This work was
supported by ERC Starting Grant 637277.

382



References

Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
pages 33–41.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473 .

Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In International Conference on
Intelligent Text Processing and Computational Lin-
guistics. Springer, pages 136–145.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of machine learning research
3(Feb):1137–1155.

Steven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson,
Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett
Powley, Dragomir Radev, and Yee Fan Tan. 2008.
The ACL Anthology reference corpus: A reference
dataset for bibliographic research in computational
linguistics. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC-08). European Language Resources Associ-
ation (ELRA), Marrakech, Morocco. ACL Anthol-
ogy Identifier: L08-1005.

Guido Boella, Luigi Di Caro, Alice Ruggeri, and Livio
Robaldo. 2014. Learning from syntax generaliza-
tions for automatic semantic annotation. Journal of
Intelligent Information Systems pages 1–16.

Claudia Borg, Michael Rosner, and Gordon Pace. 2009.
Evolutionary algorithms for definition extraction. In
Proceedings of the 1st Workshop in Definition Ex-
traction.

José Camacho-Collados, Mohammad Taher Pilehvar,
and Roberto Navigli. 2015. A unified multilingual
semantic representation of concepts. In ACL (1).
pages 741–751.

Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2005.
Generic soft pattern models for definitional question
answering. In Proceedings of the 28th annual inter-
national ACM SIGIR conference on Research and
development in information retrieval. ACM, pages
384–391.

Rosa Del Gaudio, Gustavo Batista, and António
Branco. 2013. Coping with highly imbalanced
datasets: A case study with definition extraction in a
multilingual setting. Natural Language Engineering
pages 1–33.

Claudio Delli Bovi, Luca Telesca, and Roberto Navigli.
2015. Large-scale information extraction from tex-
tual definitions through deep syntactic and semantic
analysis. TACL 3:529–543.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. arXiv preprint arXiv:1505.08075 .

Luis Espinosa-Anke, Roberto Carlini, Horacio Sag-
gion, and Francesco Ronzano. 2016a. Defext: A
semi supervised definition extraction tool. In Glob-
alex,.

Luis Espinosa-Anke and Horacio Saggion. 2014. Ap-
plying dependency relations to definition extraction.
In Natural Language Processing and Information
Systems, Springer, pages 63–74.

Luis Espinosa-Anke, Horacio Saggion, Francesco Ron-
zano, and Roberto Navigli. 2016b. Extasem! ex-
tending, taxonomizing and semantifying domain ter-
minologies. In Proceedings of the 30th Conference
on Artificial Intelligence (AAAI16).

Ismail Fahmi and Gosse Bouma. 2006. Learning to
identify definitions using syntactic features. In Pro-
ceedings of the EACL workshop on Learning Struc-
tured Information in Natural Language Applica-
tions.

Stefano Faralli and Roberto Navigli. 2013. Grow-
ing multi-domain glossaries from a few seeds using
probabilistic topic models. In EMNLP. pages 170–
181.

Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics.
pages 539–545.

Felix Hill, Kyunghyun Cho, Anna Korhonen, and
Yoshua Bengio. 2015. Learning to understand
phrases by embedding the dictionary. arXiv preprint
arXiv:1504.00548 .

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation
9(8):1735–1780.

Yiping Jin, Min-Yen Kan, Jun-Ping Ng, and Xiangnan
He. 2013. Mining scientific terms and their defini-
tions: A study of the ACL anthology. In Proceed-
ings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, Seattle, Washington,
USA, pages 780–790.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882 .

Judith Klavans and Smaranda Muresan. 2001. Evalu-
ation of the DEFINDER system for fully automatic
glossary construction. In Proceedings of the AMIA

383



Symposium. American Medical Informatics Associ-
ation, page 324.

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE
86(11):2278–2324.

SiLiang Li, Bin Xu, and Tong Lee Chung. 2016. Def-
inition extraction with lstm recurrent neural net-
works. In China National Conference on Chinese
Computational Linguistics. Springer, pages 177–
189.

Véronique Malaisé, Pierre Zweigenbaum, and Bruno
Bachimont. 2004. Detecting semantic relations be-
tween terms in definitions. In CompuTerm 2004 -
3rd International Workshop on Computational Ter-
minology. pages 55–62.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In HLT-NAACL. pages 746–
751.

George A Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J Miller.
1990. Introduction to WordNet: An on-line lexi-
cal database. International journal of lexicography
3(4):235–244.

Vinod Nair and Geoffrey E Hinton. 2010. Rectified
linear units improve restricted boltzmann machines.
In Proceedings of the 27th international conference
on machine learning (ICML-10). pages 807–814.

Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence 193:217–
250.

Roberto Navigli and Paola Velardi. 2005. Structural
semantic interconnections: a knowledge-based ap-
proach to word sense disambiguation. IEEE Trans-
actions on pattern analysis and machine intelligence
27(7):1075–1086.

Roberto Navigli and Paola Velardi. 2010. Learning
word-class lattices for definition and hypernym ex-
traction. In ACL. pages 1318–1327.

Roberto Navigli, Paola Velardi, and Juana Marı́a Ruiz-
Martı́nez. 2010. An annotated dataset for extracting
definitions and hypernyms from the web. In Pro-
ceedings of LREC’10. Valletta, Malta.

Thanapon Noraset, Chen Liang, Larry Birnbaum, and
Doug Downey. 2016. Definition modeling: Learn-
ing to define word embeddings in natural language.
arXiv preprint arXiv:1612.00394 .

James Pustejovsky, J Castano, Jason Zhang,
M Kotecki, and B Cochran. 2001. Robust
relational parsing over biomedical literature: Ex-
tracting inhibit relations. In Proceedings of the

Pacific symposium on biocomputing. volume 7,
pages 362–373.

Josette Rebeyrolle and Ludovic Tanguy. 2000.
Repérage automatique de structures linguistiques en
corpus : le cas des énoncés définitoires. Cahiers de
Grammaire 25:153–174.

Melanie Reiplinger, Ulrich Schäfer, and Magdalena
Wolska. 2012. Extracting glossary sentences from
scholarly articles: A comparative evaluation of pat-
tern bootstrapping and deep analysis. In Proceed-
ings of the ACL-2012 Special Workshop on Re-
discovering 50 Years of Discoveries. Association
for Computational Linguistics, Jeju Island, Korea,
pages 55–65.

Stephen D Richardson, William B Dolan, and Lucy
Vanderwende. 1998. Mindnet: acquiring and struc-
turing semantic information from text. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics-
Volume 2. Association for Computational Linguis-
tics, pages 1098–1102.

Horacio Saggion and Robert Gaizauskas. 2004. Min-
ing on-line sources for definition knowledge. In
17th FLAIRS. Miami Bearch, Florida.

Vered Shwartz, Yoav Goldberg, and Ido Dagan. 2016.
Improving hypernymy detection with an integrated
path-based and distributional method. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics, ACL 2016, August 7-
12, 2016, Berlin, Germany, Volume 1: Long Papers.

Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Y Ng. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural In-
formation Processing Systems. pages 801–809.

Richard Socher, Andrej Karpathy, Quoc V Le, Christo-
pher D Manning, and Andrew Y Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics
2:207–218.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 conference on
empirical methods in natural language processing.
pages 1631–1642.

Angelika Storrer and Sandra Wellinghoff. 2006. Au-
tomated detection and annotation of term definitions
in German text corpora. In Conference on Language
Resources and Evaluation (LREC).

Paola Velardi, Stefano Faralli, and Roberto Navigli.
2013. OntoLearn Reloaded: A graph-based algo-
rithm for taxonomy induction. Computational Lin-
guistics 39(3):665–707.

384



Denny Vrandečić and Markus Krötzsch. 2014. Wiki-
data: A free collaborative knowledge base. Commu-
nications of the ACM 57(10):78–85.

Eline Westerhout and Paola Monachesi. 2007. Com-
bining pattern-based and machine learning methods
to detect definitions for elearning purposes. In Pro-
ceedings of RANLP 2007 Workshop Natural Lan-
guage Processing and Knowledge Representation
for eLearning Environments.

Wen-tau Yih, Kristina Toutanova, John C Platt, and
Christopher Meek. 2011. Learning discriminative
projections for text similarity measures. In Proceed-
ings of the Fifteenth Conference on Computational
Natural Language Learning. Association for Com-
putational Linguistics, pages 247–256.

Chunting Zhou, Chonglin Sun, Zhiyuan Liu, and Fran-
cis Lau. 2015. A c-lstm neural network for text clas-
sification. arXiv preprint arXiv:1511.08630 .

385


