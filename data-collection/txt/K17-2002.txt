



















































Training Data Augmentation for Low-Resource Morphological Inflection


Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection, pages 31–39,
Vancouver, Canada, August 3–4, 2017. c©2017 Association for Computational Linguistics

Training Data Augmentation for Low-Resource Morphological Inflection
Toms Bergmanis∗

t.bergmanis@sms.ed.ac.uk
Katharina Kann†
kann@cis.lmu.de

Hinrich Schütze†
inquiries@cislmu.org

Sharon Goldwater∗
sgwater@inf.ed.ac.uk

∗School of Informatics
University of Edinburgh

†Center for Information & Language Processing
LMU Munich

Abstract

This work describes the UoE-LMU sub-
mission for the CoNLL-SIGMORPHON
2017 Shared Task on Universal Morpho-
logical Reinflection, Subtask 1: given a
lemma and target morphological tags, gen-
erate the target inflected form. We evalu-
ate several ways to improve performance
in the 1000-example setting: three meth-
ods to augment the training data with iden-
tical input-output pairs (i.e., autoencod-
ing), a heuristic approach to identify likely
pairs of inflectional variants from an un-
labeled corpus, and a method for cross-
lingual knowledge transfer. We find that
autoencoding random strings works sur-
prisingly well, outperformed only slightly
by autoencoding words from an unlabelled
corpus. The random string method also
works well in the 10,000-example setting
despite not being tuned for it. Among 18
submissions our system takes 1st and 6th
place in the 10k and 1k settings, respec-
tively.

1 Introduction

Morphological variation is a major contributor to
the sparse data problem in NLP, especially for
under-resourced languages. The SIGMORPHON
2016 Shared Task (Cotterell et al., 2016) and
CoNLL-SIGMORPHON 2017 Shared Task (Cot-
terell et al., 2017) aimed to inspire researchers to
develop better systems for morphological inflec-
tion across a wide range of languages with vary-
ing training resources. In 2016, when over 10,000
training examples were provided for each lan-
guage, neural network-based systems performed
considerably better than other approaches (Cot-
terell et al., 2016). The 2017 Shared Task there-
fore included settings with different amounts of

training data (100, 1000, or 10,000 examples), to
examine which approaches might work best when
data is even more limited.

We focus on the 1000-example setting of Sub-
task 1: given a lemma with its part of speech and
target morphological tags, generate the target in-
flected form. Our baseline system is the winning
system from 2016, the morphological encoder-
decoder (MED) from LMU (Kann and Schütze,
2016). We explore methods for improving its per-
formance in the face of fewer training examples,
either with or without additional data from unla-
beled corpora.1

In particular, we focus on various training data
augmentation methods. One uses a heuristic ap-
proach to identify likely pairs of inflectional vari-
ants from an unlabeled corpus in an unsuper-
vised way, and uses these as input-output pairs.
Three other methods augment the training data
with identical input-output pairs—i.e., simultane-
ously training the network to perform autoencod-
ing. We compare three types of autoencoder in-
puts: either lemmas and inflected forms from the
training data, words from an unlabeled corpus, or
random strings.

We present detailed results and comparisons for
various amounts of added training data for all
52 languages of the shared task. We find that
all methods improve considerably over the MED
baseline (7.2-10.7% across all languages; a 15.5%
improvement over the shared task baseline). Most
of the benefit comes with only 4k extra examples,
but performance continues to improve up to 16k
added examples.

After controlling for the amount of additional
data, we see only a small benefit from autoen-
coding corpus words rather than random strings.
Using hypothesized morphological variants works

1We did not pursue the 100-example setting because pre-
liminary experiments yielded such low baseline performance
that we felt additional work would be unlikely to help much.

31



as well as random strings. These results suggest
that the main advantage of all these methods is
providing a strong bias towards learning the iden-
tity transformation and/or working as regularizers,
rather than learning language-specific phonotac-
tics or morpho-phonological changes.2

Finally, following Kann et al. (2017), we
test whether cross-lingual knowledge transfer can
help, by multilingual training of joint models for
groups of up to 10 related languages. However,
we find no improvement as compared to using an
equivalent amount of random-string autoencoder
examples.

Our final submission to the shared task consists
of two submissions for Subtask 1 (Inflection): the
random string autoencoder for the medium and
high data settings of the restricted (main) track
and the corpus word autoencoder for the medium
data setting of the bonus track (track with exter-
nal monolingual corpora). All systems use 16K
autoencoder examples and an ensemble of three
training runs with majority voting.

In the high resource setting of the restricted
track, our system outperforms all 17 other submis-
sions, with an average test set accuracy of 95.32%
over 52 languages. In the medium resource set-
ting, among 18 submissions our system takes the
6th place with 81.02% (1.78% below the top sys-
tem). In the medium resource setting of the bonus
track, among 2 submissions our system comes first
with 82.37%.

2 Baseline MED System

For our baseline system (henceforth MED-
baseline or MED), we use the sequence encoder-
decoder architecture and input/output format of
the 2016 Shared Task winner (Kann and Schütze,
2016). The architecture follows Bahdanau et al.
(2015): that is, the encoder is a bidirectional gated
recurrent neural network (GRU) with attention,
and the decoder is a uni-directional GRU. For de-
tails, see Kann and Schütze (2016) and Bahdanau
et al. (2015).

The input sequence consists of space sepa-
rated characters of the input lemma—the dictio-
nary form of the word—followed by space sepa-
rated morphological tags, each prepended with a

2Of course, the morphological variants we find may be
noisy, so a better method for identifying these might still im-
prove upon random strings.

tag marker, for example:

w a l k t=V t=V.PTCP t=PRS (1)

The target output is a sequence of characters form-
ing the inflected word, e.g., w a l k i n g.

3 Augmenting the Training Data

For the Shared Task, the main competition permits
no additional resources beyond the labeled train-
ing examples given for each setting. However,
Wikipedia dumps in each language are provided
for teams who wish to explore semi-supervised
methods as well. We examine two methods that
use only the training data, and two that also incor-
porate corpus data. Finally, we explore a method
that uses multilingual resources.

3.1 No Outside Resources (AE-TD, AE-RS)
Morphologically related words are typically simi-
lar in form, and in many cases, parts of the word
are copied from the lemma to the inflected form.
As suggested by Kann and Schütze (2017), we hy-
pothesized that training MED to copy strings as
a secondary task would help with the morpholog-
ical inflection task. That is, we train the model
simultaneously on the tasks of morphological re-
inflection and sequence autoencoding, interspers-
ing inflection training examples and autoencoding
examples. This can be viewed as a form of multi-
task learning,3 and is equivalent to maximizing the
log-likelihood for both tasks:

L(θ)=
∑

(l,t,w)∈D
log pθ (w | e(l, t)) (2)

+
∑

s∈S
log pθ(s | e(s)),

where D is the labeled training data, with each ex-
ample consisting of a lemma l, a morphological
tag t and an inflected form w, and S is a set of
autoencoding examples. The function e represents
the encoder, which depends on θ.

In the setting with no outside resources we ex-
periment with two variants of the sequence au-
toencoder. The first of these, AE-TD, uses the

3Multitask learning for NLP using encoder-decoder net-
works typically assumes that the separate tasks either have
distinct encoders, distinct decoders, or both (e.g., Luong
et al., 2016; Alonso and Plank, 2017; Bollmann et al., 2017).
Here, we use the same encoder and decoder for both tasks.
In preliminary experiments, we tried pre-training the autoen-
coder instead (see, e.g., Dai and Le, 2015; Kamper et al.,
2015), but found that interspersing examples gave a clear ad-
vantage.

32



lemmas and target forms in the training data as
inputs to the autoencoder, yielding up to twice
as many autoencoder inputs as inflection training
pairs (any duplicate lemmas or target forms are in-
cluded only once).

Our second autoencoder variant, AE-RS, uses
randomly generated strings as inputs, which
means we can produce an arbitrary number of au-
toencoding examples. In this and following sys-
tems, we use the postfix XXK (e.g. 1K, 2K, 4K)
to indicate the number of additional examples gen-
erated. To obtain each example, we first choose its
length uniformly at random from the interval [4,
12] and then sample each character uniformly at
random from the alphabet of the respective lan-
guage.

Input/Output Format We generally follow the
input representation outlined in §2, except that
morphological tags are replaced with one special
tag that stands for autoencoding. The output for-
mat does not change.

3.2 Corpus Word Autoencoder (AE-CW)
In the setting where corpus data is available we
can replace randomly generated strings with cor-
pus words (system AE-CW). We hypothesized
that autoencoding words from the actual language
would give not only the benefit of learning to copy,
but also the benefit of learning the character distri-
butions typical of a given language.

To select words for the corpus word autoencod-
ing task we use Wikipedia text dumps provided for
the shared task. We filter out all words shorter than
4 characters. For each language we learn its alpha-
bet from the letters that occur in training words of
the original SIGMORPHON training sets and fil-
ter out all words that contain foreign characters.
We use the remaining words to sample uniformly
without repetition the required amount of autoen-
coding examples. The input and output formats
are the same as for the previous approaches.

3.3 Data Mining for Inflected Pairs (DM)
Our next method, DM, mines the corpus data to
create new training examples by (a) inferring new
lemma-inflected form pairs and (b) predicting the
tags of the inflected forms. We describe each step
below.

Inferring Lemma-Word Form Pairs Although
most work on unsupervised learning of morphol-
ogy has focused on decomposing words into mor-

Word 1⇔Word 2 Sim. Dif.
deceive⇔deception dece ive⇔ ption
receive⇔reception rece ive⇔ ption

perceive⇔perception perce ive⇔ ption
conceive⇔conception conce ive⇔ ption

Table 2: Pairs of related words, their similarities
and the respective differences.

phemes, their constituent parts, e.g., (Kurimo
et al., 2010; Hammarström and Borin, 2011), oth-
ers have focused on finding morphologically re-
lated words and the orthographic patterns relating
them (Schone and Jurafsky, 2000; Baroni et al.,
2002; Neuvel and Fulop, 2002; Soricut and Och,
2015):

walk⇔ walking ε⇔ ing (3)

We adopt the algorithm by Neuvel and Fu-
lop (2002) to learn Word Formation Strategies
(WFS)—frequently occurring orthographic pat-
terns that relate whole words to other whole
words. The input of this algorithm is a list of N
words4. The algorithm works by comparing each
of the N words to all other words. It first finds
word similarities as the Longest Common Sub-
sequence (LCS) between the two words. Then it
finds word differences as the orthographic differ-
ences with respect to similarities (see Table 2 for
examples). Finally, all word pairs with the same
differences have their similarities and differences
merged into one WFS. For example, words in Ta-
ble 2 sanction the following WFS:

∗##ceive⇔ ∗##ception (4)

where * and # stand for the optional and manda-
tory character wild cards respectively. The inter-
pretation of the WFS in Example 4 is: “a word that
ends with ceive preceded by 2 to 3 characters pre-
dicts another word ending with ception preceded
by the same 2 to 3 characters” (Neuvel and Fulop,
2002). Table 1 gives English WFS examples and
sample word pairs that warranted their creation.

Tag Prediction To perform labeling we make
use of two resources: (i) a word embedding based
part of speech (POS) classifier; (ii) the WFS we
learned previously.

4We choose N to be 40k and take the 40k most frequent
words from each language’s Wikipedia dump.

33



No WFS Examples
1 *********####d⇔ *********####s invited-invites, wired-wires, tried-tries
2 ********####ed⇔ ********####ing trailed-trailing, folded-folding, melted-melting
3 ********####e⇔ ********####ing violate-violating, liberate-liberating, raise-raising
4 **********####s⇔ **********####al comics comical, clinics-clinical, corrections-correctional

Table 1: Examples of word formation strategies and sample word pairs that warranted them.

Each of the original shared task training exam-
ples contains two word forms and a set of mor-
phological tags including word POS information
(see Example 1). We can use words with POS
labels and word embeddings to train a POS clas-
sifier. Namely we train a support vector ma-
chine (SVM) (Cortes and Vapnik, 1995) to pre-
dict POS labels using word embeddings as fea-
tures. We train word embeddings on the Wikipedia
text dumps provided for the task. We use Fast-
Text5 by Bojanowski et al. (2016) to train 300 di-
mensional continuous bag of words embeddings
(Mikolov et al., 2013) for all words occurring at
least 5 times.

For each training example in the SIGMOR-
PHON training data we examine each of the pre-
viously learned WFS. If the training example fits
the WFS’s orthographic constraints, we examine
all word pairs that warranted the creation of this
WFS. For a word pair to be labeled with the same
morphological tags as the training example we re-
quire that both words are classified with the same
part of speech as the original training example.

Input/Output Format To encode the additional
training examples we use the same format as for
the original ones (see §2) except we add a tag
which signals that this example has been automati-
cally extracted. We do this as a measure of caution
to avoid ambiguities introduced by potentially er-
roneous training examples.

3.4 Using Multilingual Resources (MLT)
Recently, Kann et al. (2017) showed that training
on a high-resource language can improve morpho-
logical inflection on a related low-resource lan-
guage using an encoder-decoder system like the
one here. This can be done using the same network
as above, but training on inflection examples from
multiple different languages—yet another form of
multitask learning, since the model parameters are
shared between languages. Following Kann et al.

5https://github.com/facebookresearch/
fastText/

(2017), our multilingually trained system (MLT)
uses the same input format as above, but with an
additional tag prepended to each example indicat-
ing which language it is from.

Our setup is slightly different from that of Kann
et al. (2017), since they had only 50 or 200 exam-
ples in each target language, and used only one or
two other higher-resource languages for transfer.
Here, we have similar amounts of data for each
language (1000 examples in most cases), and use
larger groups of related languages to train together
(see §4).

4 Experiments

Datasets The official shared task data consists of
sets for 52 different languages, of which 40 were
released as development languages. We used these
for our preliminary experiments to compare differ-
ent systems and quantities of additional training
data. The remaining 12 “surprise” languages were
released shortly before the test phase of the shared
task, and we report results for our best systems on
these as well.6 In the “medium” training data set-
ting, which we focus on in this work, 1000 train-
ing instances are given for each language, except
for Scottish Gaelic with only 681 instances. Addi-
tionally, development sets with 1000 instances are
available for all languages except for Basque, Ben-
gali, Haida, Welsh with 100 and Scottish Gaelic
with 50.

MED Parameters In our experiments we use
the same training method and hyper parameter set-
tings as suggested by Kann and Schütze (2016).
Namely, we use 100 hidden units for the en-
coder and decoder GRUs; 300 dimensions for en-
coder and decoder embeddings. For training we
use stochastic gradient descent, Adadelta (Zeiler,
2012), with gradient clipping threshold of 1.0 and
mini batch size of 20. When making predictions

6A detailed list of languages can be found
at https://sites.google.com/view/
conll-sigmorphon2017/task, or see Figure 2.

34



we use beam-search decoding with a beam of size
12.

Baselines We compare our results with two
baselines: the SIGMORPHON baseline7 and
MED baseline (see §2).

Additional Training Data We train the AE-
CW, AE-RS and DM systems with increasing
amounts of data: 1k, 2k, or 4k additional train-
ing examples. Four thousand training instances is
the maximum number of mined training examples
available for most languages. We train additional
AE-CW and AE-RS systems with 8k and 16k ad-
ditional training examples to see if any further per-
formance gains are obtainable.

Multilingual Training We train together lan-
guages that belong to the same language family.
This results in the following groupings: Baltic
(Latvian, Lithuanian), Celtic (Scottish Gaelic,
Welsh, Irish), Germanic (Danish, Dutch, English,
Icelandic, Faroese, German, Swedish, Norwegian-
Nynorsk, Norwegian-Bokmal), Finnic (Finnish,
Estonian), Iranian (Persian, Sorani, Kurmanji),
Romance (Catalan, French, Italian, Portuguese,
Spanish, Romanian), Semitic (Arabic, Hebrew),
Slavic (Bulgarian, Czech, Lower-Sorbian, Mace-
donian, Polish, Russian, Serbo-Croatian, Slovak,
Slovene, Ukrainian).

We perform two different multilingual training
experiments: First, we train MLT without any
additional data. In this setting MED trained on
individual languages gives a lower bound—MLT
should work no worse than MED. We also train
MLT-AE-TD in which the original training data
is used to obtain up to twice as many autoencod-
ing training examples for each language. In this
setting the training file of Slavic languages, for
example, contains 30K training examples (3K for
each language). Here, AE-TD trained on individ-
ual languages gives a lower bound on the expected
performance. In this setting we also train another
stronger and fairer baseline (AE-TD-RS-XK), for
which training files for individual languages con-
tain exactly the same number of training examples
as in the multilingual training case. For exam-
ple, the training file for MLT-AE-TD when trained
on the Baltic branch has 6k training examples—
1k original and 2k AE-TD examples for each of

7SIGMORPHON baseline: https://github.
com/sigmorphon/conll2017/tree/master/
evaluation

Figure 1: The average performance over the 40
development languages for each of the methods,
as a function of the number of added examples.

System Mean Accuracy
SIGMORPHON 65.67

MED 70.52
AE-TD 77.73

AE-CW-16K 81.20
AE-RS-16K 80.40

Table 4: The average performance over all lan-
guages except Haida and Khaling (see text).

the two Baltic languages (Latvian and Lithuanian).
So, the Latvian training file for AE-TD-RS-3K
also contains 6K training examples: 1K original
and 2K training data autoencoding examples, plus
an additional 3K random string autoencoding ex-
amples, which substitute for the absent Lithuanian
training examples. The Lithuanian training file is
constructed analogously.

5 Results and Discussion

5.1 Number of Added Examples

Figure 1 shows how the average performance over
the 40 development languages varies with the
number of added autoencoder examples, for AE-
CW, AE-RS and DM methods. For all three meth-
ods, adding more data up to 4k examples helps
a lot, after which performance rises more slowly
and mostly levels off by about 16k extra exam-
ples. (These results contrast with the inflection
results from the semi-supervised variational se-
quence autoencoder of Zhou and Neubig (2017),
where even after 150k unlabeled examples, per-
formance still appears to be increasing.) After
controlling for the amount of additional data, we
see only a small benefit from autoencoding corpus
words (AE-CW) rather than random strings (AE-

35



Figure 2: The accuracy of our best systems on all languages. We report the mean and standard error
(shown as error bars) across three separate training runs for each language. Languages are grouped by
their language families. From the top: Baltic, Celtic, Germanic, Finnic, Iranian, Romance, Semitic,
Slavic and miscellaneous languages.

36



MED MLT AE-TD MLT-AE-TD AE-TD-RS-XK
Baltic 63.25 64.45 69.15 70.80 72.20
Celtic 53.65 56.09 63.74 65.22 68.17

Germanic 66.53 73.29 73.83 75.87 76.71
Finnic 59.45 68.65 73.35 75.90 76.77

Iranian 75.83 80.83 81.53 83.60 84.87
Romance 80.72 83.65 84.90 86.10 86.67

Semitic 71.60 69.05 76.70 77.30 77.30
Slavic 63.78 77.90 75.65 80.31 79.47

Average 66.85 71.74 74.86 76.89 77.77

Table 3: Multilingual training. We report average development set accuracies, weighted by the number
of examples in each development set. The letter X in AE-TD-RS-XK, stands for the number of random
string autoencoding examples added (see §4 for more details).

RS).
Figure 1 suggests that on average, adding 1K,

2K or 4k mined training examples (DM curve)
gives a similar performance gain to what adding
the same amount of autoencoding examples would
give (AE-CW and AE-RS).

To investigate the quality of the mined train-
ing examples we conduct an experiment where for
each morphological tag in SIGMORPHON train-
ing data we pick an alternative lemma and target
form among the mined examples with the same
tag. We hypothesize that, if correct, mined exam-
ples should serve the same function as the origi-
nal ones and the average performance should not
change. On average this resulted in 500 swaps per
language. The average MED performance on the
modified training sets is about 10% lower than on
the original training files.

This suggests, that—although noisy—the
mined examples, when annotated with an addi-
tional “mining”-tag, work as model regularizers,
thus benefiting MED’s performance on the
inflection task.

Obtaining autoencoding examples, however, is
computationally simpler than mining additional
training examples, hence given the similar effects
the autoencoding approach seems preferable over
the data mining.

5.2 Best Monolingual Systems

The average development set accuracy over 50
languages8 of our best system AE-CW-16K is
81.2% (see Table 4) closely followed by AE-
RS-16K with 80.4%. For comparison, this is

8Haida and Khaling do not have Wikipedia text dumps to
train AE-CW and are thus excluded from all averages.

about 15.5% and 10.6% absolute gain over the
SIGMORPHON and MED baselines respectively.
AE-TD on average performs only 3.5% worse than
AE-CW-16K although using 87.5% fewer autoen-
coding examples and no external resources. Fig-
ure 2 shows the accuracy of our best systems on
all languages. We report the average accuracy and
standard error across three separate training runs
for each language.

We conducted a paired-samples t-test to com-
pare mean development set accuracies for 50 lan-
guages between AC-CW-16K and AC-RS-16K
systems, using 3 runs each. The test suggested that
there was a significant difference between accura-
cies of AC-CW-16K and AC-RS-16K (T (49) =
4.04, p < 0.01), although the difference is small
relative to the gains over the baselines.

5.3 Multilingual Training

Table 3 shows results for multilingual training ex-
periments. Due to the different development set
sizes (see §4) we report weighted average de-
velopment set accuracies. MLT without any ad-
ditional data is better than the MED baseline for
most related languages except Semitic languages,
on average giving about 7% improvement over the
MED baseline. MLT-AE-TD, the system in which
the original training data is used to obtain au-
toencoding training examples on average outper-
forms the conservative AE-TD baseline by 2.5%
absolute, but barely reaches the performance of
AE-TD-RS-XK baseline. AE-TD-RS-XK base-
line (for details see §4) gives performance for
each individual language if all training examples
of other languages in an MLT-AE-TD training file
are replaced by random string autoencoding exam-

37



ples. Table 3 shows that on average MLT-AE-TD
works no better than AE-TD-RS-XK. Currently, it
is unclear whether the performance gains in MLT
and MLT-AE-TD experiments are due to knowl-
edge transfer from related languages as suggested
by Kann et al. (2017), or because different lan-
guages serve as model regularizers with respect to
each other. Performance on 6 out of 8 groups of
related languages, however, suggests that random
string autoencoding is not only simpler but also a
better performing method than multilingual train-
ing.

5.4 Shared Task Submission: Test Results

We submitted two final systems to the medium-
resource track of Subtask 1 (Inflection): AE-RS-
16K for the restricted (main) track, and AE-CW-
16K for the bonus track (where external monolin-
gual corpora are permitted). The final systems are
ensembles of 3 separate training runs, and the final
answer is selected by majority voting (or chosen at
random in case of a tie).

Although we did not tune any system to the
high-resource track, we also submitted results
there, using an analogous ensemble system with
16k autoencoder at random strings in addition to
the 10k training examples. We did not submit to
the low-resource track because the results from the
AE-RS-16k method were below the baseline sys-
tem on the development set.

In the high resource setting of the restricted
track, our system achieved an average test set
accuracy of 95.32%, which is an 17.51% abso-
lute improvement over the Shared Task baseline,
and the top performance of 18 submissions. In
the medium resource setting AE-RS-16K gives
81.02%—an improvement of 16.32% absolute
over the Shared Task baseline. This result, how-
ever, is 1.78% absolute lower than the best sys-
tem’s performance, so among 18 submissions AE-
RS-16K takes the 6th place. In the medium re-
source setting of the bonus track among 2 submis-
sions AE-CW-16K comes first with 82.37%. This
is 1.35% better than our restricted track submis-
sion (AE-RS-16K), but still 0.43% worse than the
top performing system in the restricted track.

6 Conclusion

We evaluated several ways to improve the morpho-
logical inflection performance of a state-of-the-
art encoder-decoder model (MED) when relatively

few labeled examples are available. In experi-
ments on 52 languages, we showed that all meth-
ods considerably outperformed the MED baseline.
Autoencoding corpus words (AE-CW) gave the
largest improvement, but was only slightly bet-
ter than autoencoding random strings (AE-RS).
We found no benefit from cross-lingual knowledge
transfer as compared to using an equivalent num-
ber of random string autoencoder examples.

Our results suggest that the main benefit of the
various data augmentation methods is providing a
strong bias towards learning the identity transfor-
mation and/or regularizing the model, with a slight
additional benefit obtained by learning the typical
character sequences in the language. These bene-
fits can be achieved with very simple methods and
few or no additional data resources.

References
Héctor Martı́nez Alonso and Barbara Plank. 2017.

Multitask learning for semantic sequence prediction
under varying data conditions. In Proceedings of
EACL. Association for Computational Linguistics.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
ICLR.

Marco Baroni, Johannes Matiasek, and Harald Trost.
2002. Unsupervised discovery of morphologically
related words based on orthographic and semantic
similarity. In Proceedings of the ACL-02 Workshop
on Morphological and Phonological Learning. As-
sociation for Computational Linguistics.

Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-
tors with subword information. arXiv preprint
arXiv:1607.04606 .

Marcel Bollmann, Joachim Bingel, and Anders
Søgaard. 2017. Learning attention for historical
text normalization by learning to pronounce. In
Proceedings of ACL. Association for Computational
Linguistics.

Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning 20(3):273–297.

Ryan Cotterell, Christo Kirov, John Sylak-Glassman,
Géraldine Walther, Ekaterina Vylomova, Patrick
Xia, Manaal Faruqui, Sandra Kübler, David
Yarowsky, Jason Eisner, and Mans Hulden.
2017. The CoNLL-SIGMORPHON 2017 shared
task: Universal morphological reinflection in
52 languages. In Proceedings of the CoNLL-
SIGMORPHON 2017 Shared Task: Universal Mor-
phological Reinflection. Association for Computa-
tional Linguistics.

38



Ryan Cotterell, Christo Kirov, John Sylak-Glassman,
David Yarowsky, Jason Eisner, and Mans Hulden.
2016. The SIGMORPHON 2016 shared task–
morphological reinflection. In Proceedings of ACL.
Association for Computational Linguistics.

Andrew M Dai and Quoc V Le. 2015. Semi-supervised
sequence learning. In Advances in Neural Informa-
tion Processing Systems. pages 3079–3087.

Harald Hammarström and Lars Borin. 2011. Unsuper-
vised learning of morphology. Computational Lin-
guistics 37(2):309–350.

Herman Kamper, Micha Elsner, Aren Jansen, and
Sharon Goldwater. 2015. Unsupervised neural net-
work based feature extraction using weak top-down
constraints. In IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP).
IEEE, pages 5818–5822.

Katharina Kann, Ryan Cotterell, and Hinrich Schütze.
2017. One-shot neural cross-lingual transfer for
paradigm completion. In Proceedings of ACL. As-
sociation for Computational Linguistics.

Katharina Kann and Hinrich Schütze. 2016. MED: The
LMU system for the SIGMORPHON 2016 shared
task on morphological reinflection. In Proceedings
of ACL. Association for Computational Linguistics.

Katharina Kann and Hinrich Schütze. 2017. Unlabeled
data for morphological generation with character-
based sequence-to-sequence models. arXiv preprint
arXiv:1705.06106 .

Mikko Kurimo, Sami Virpioja, Ville Turunen, and
Krista Lagus. 2010. Morpho challenge competi-
tion 2005–2010: Evaluations and results. In Pro-
ceedings of the 11th Meeting of the ACL Special
Interest Group on Computational Morphology and
Phonology. Association for Computational Linguis-
tics, SIGMORPHON ’10, pages 87–95.

Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2016. Multi-task se-
quence to sequence learning. In Proceedings of
ICLR.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .

Sylvain Neuvel and Sean A Fulop. 2002. Unsuper-
vised learning of morphology without morphemes.
In Proceedings of the ACL-02 workshop on Morpho-
logical and phonological learning-Volume 6. Asso-
ciation for Computational Linguistics, pages 31–40.

Patrick Schone and Daniel Jurafsky. 2000.
Knowledge-free induction of morphology us-
ing latent semantic analysis. In Proceedings of
the 2nd Workshop on Learning Language in Logic
and the 4th Conference on Computational Natural
Language Learning (CoNLL). Association for
Computational Linguistics, pages 67–72.

Radu Soricut and Franz Josef Och. 2015. Unsu-
pervised morphology induction using word embed-
dings. In Proceedings of HLT-NAACL. Association
for Computational Linguistics, pages 1627–1637.

Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701 .

Chunting Zhou and Graham Neubig. 2017. Multi-
space variational encoder-decoders for semi-
supervised labeled sequence transduction. arxiv
preprint arXiv:1704.01691 .

39


