



















































Early and Late Combinations of Criteria for Reranking Distributional Thesauri


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 470–476,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Early and Late Combinations of Criteria for Reranking Distributional
Thesauri

Olivier Ferret
CEA, LIST, Vision and Content Engineering Laboratory,

Gif-sur-Yvette, F-91191 France.
olivier.ferret@cea.fr

Abstract

In this article, we first propose to ex-
ploit a new criterion for improving distri-
butional thesauri. Following a bootstrap-
ping perspective, we select relations be-
tween the terms of similar nominal com-
pounds for building in an unsupervised
way the training set of a classifier perform-
ing the reranking of a thesaurus. Then, we
evaluate several ways to combine thesauri
reranked according to different criteria and
show that exploiting the complementary
information brought by these criteria leads
to significant improvements.

1 Introduction

The work presented in this article aims at im-
proving thesauri built following the distributional
approach as implemented by (Grefenstette, 1994;
Lin, 1998; Curran and Moens, 2002). A part of
the work for improving such thesauri focuses on
the filtering of the components of the distribu-
tional contexts of words (Padró et al., 2014; Po-
lajnar and Clark, 2014) or their reweighting, ei-
ther by turning the weights of these components
into ranks (Broda et al., 2009) or by adapting
them through a bootstrapping method from the
thesaurus to improve (Zhitomirsky-Geffet and Da-
gan, 2009; Yamamoto and Asakura, 2010). The
other part implies more radical changes, includ-
ing dimensionality reduction methods such as La-
tent Semantic Analysis (Padó and Lapata, 2007),
multi-prototype (Reisinger and Mooney, 2010) or
exemplar-based models (Erk and Pado, 2010),
neural approaches (Huang et al., 2012; Mikolov et
al., 2013) or the adoption of a Bayesian viewpoint
(Kazama et al., 2010; Dinu and Lapata, 2010).

Our work follows (Ferret, 2012), which pro-
posed a different way from (Zhitomirsky-Geffet
and Dagan, 2009) to exploit bootstrapping by se-

lecting in an unsupervised way a set of semanti-
cally similar words from an initial thesaurus and
training from them a classifier to rerank the se-
mantic neighbors of the initial thesaurus entries.
More precisely, we propose a new criterion for this
selection, based on the similarity relations of the
components of similar compounds, and we show
two modes – early and late – of combination of
thesauri reranked from different criteria, including
ours, leading to significant further improvements.

2 Reranking a distributional thesaurus

Distributional thesauri are characterized by het-
erogeneous performance in their entries, even for
high frequency entries. This is a favorable situa-
tion for implementing a bootstrapping approach in
which the results for “good” entries are exploited
for improving the results of the other ones. How-
ever, such idea faces two problems: first, detect-
ing “good” entries; second, learning a model from
them for improving the performance of the other
entries.

The first issue consists in selecting without su-
pervision a set of positive and negative examples
of similar words that represents a good compro-
mise between its error rate and its size. Straight-
forward solutions such as using the similarity
value between an entry and its neighbors or relying
on the frequency of entries are not satisfactory in
terms of error rate. Hence, we propose in Section 3
a new method, based on the semantic composi-
tionality hypothesis of compounds, for achieving
this selection in a more indirect way and show the
interest to combine it with the criterion of (Ferret,
2012) for building a large training set with a rea-
sonable error rate.

We address the second issue by following
(Hagiwara et al., 2009), which defined a Sup-
port Vector Machine (SVM) model for deciding
whether two words are similar or not. In our con-
text, a positive example is a pair of nouns that are

470



semantically similar while a negative example is
a pair of non similar nouns. The features of each
pair of nouns are built by summing the weights of
the elements shared by their distributional repre-
sentations, which are vectors of weighted cooccur-
rents. Cooccurrents not shared by the two nouns
are given a null weight.

This SVM model is used for improving a the-
saurus by reranking its semantic neighbors as fol-
lows: for each entry E of the thesaurus, the rep-
resentation as an example of the word pair (E,
neighbor) is built for each of the neighbors of E
and submitted to the SVM model in classification
mode. Finally, all the neighbors of E are reranked
according to the value of the decision function
computed for each neighbor by the SVM model.

3 Unsupervised example selection

The evaluation of distributional thesauri shows
that a true semantic neighbor is more likely to be
found when the thesaurus entry is a high frequency
noun and the neighbor has a low rank. However,
relying only on these two criteria doesn’t lead to
a good enough set of positive examples. For in-
stance, taking as positive examples from the ini-
tial thesaurus of Section 4 the first neighbor of its
2,148 most frequent entries, the number of pos-
itive examples of (Hagiwara et al., 2009), only
leads to 44.3% of correct examples. Moreover,
this percentage exceeds 50% only when the num-
ber of examples is less than 654, which represents
a very small training set for this kind of task.

Hence, we propose a more selective approach
for choosing positive examples among high fre-
quency nouns to get a more balanced solution be-
tween the number of examples and their error rate.
This approach exploits a form of semantic compo-
sitionality hypothesis of compounds. While much
work has been done recently for defining the dis-
tributional representation of compounds by com-
posing the distributional representations of their
components (Mitchell and Lapata, 2010; Paperno
et al., 2014), we adopt a kind of reverse viewpoint
by exploiting the possibility to link the meaning
of a compound to the meaning of its components.
More precisely, we assume that the mono-terms
of two semantically related compounds with the
same syntactic role in their compound are likely
to be semantically linked themselves.

In this work, we only consider compounds hav-
ing one of these three term structures (with their

percentage of the vocabulary of compounds):

(a) <noun>mod <noun>head (30)
(b) <adjective>mod <noun>head (58)
(c) <noun>head <preposition><noun>mod (12)

Each compound Ci is represented as a pair
(Hi,Mi), where Hi stands for the head of the
compound whereas Mi represents its modifier
(mod). According to the assumption underlying
our selection procedure, if a compound (H2,M2)
is a semantic neighbor of a compound (H1,M1)
(i.e. at most its cth neighbor in a distributional the-
saurus of compounds), we can expect H1 and H2
on one hand and M1 and M2 on the other hand to
be semantically similar. Since distributional the-
sauri of compounds are far from being perfect, we
added constraints on the matching of the compo-
nents of two compounds. More precisely, the posi-
tive examples of semantically similar nouns (noun
pairs after→) are selected by the three following
rules, where H1 = H2 means that H1 is the same
word asH2 andH1 ≡ H2 means thatH2 is at most
the mth neighbor of H1 in the initial thesaurus of
mono-terms (but is different from H1):

(1) H1 ≡ H2 & M1 = M2 → (H1, H2)
(2) M1 ≡M2 & H1 = H2 → (M1, M2)
(3) M1 ≡M2 & H1 ≡ H2 → (H1, H2), (M1, M2)

The selection of negative examples is also an
important issue but benefits from the fact that the
number of semantic neighbors of an entry that
are actually semantically linked to this entry in a
distributional thesaurus quickly decreases as their
rank increase. In the experiments of Section 4,
we built negative examples from positive exam-
ples by turning each positive example (A,B) into
two negative examples: (A, rank 10 A neighbor)
and (B, rank 10 B neighbor). Choosing neighbors
with a higher rank would have guaranteed fewer
false negative examples but taking neighbors with
a rather small rank for building negative examples
is more useful in terms of discrimination.

4 Experiments and evaluation

4.1 Building of distributional thesauri
The first step of the work we present is the build-
ing of two distributional thesauri: the thesaurus of
mono-terms to improve (A2ST) and a thesaurus
of compounds (A2ST-comp). Similarly to (Ferret,
2012), they were both built from the AQUAINT-2
corpus, a 380 million-word corpus of news arti-
cles in English. The building procedure, defined

471



by (Ferret, 2010), was also identical to (Ferret,
2012), with distributional contexts compared with
the Cosine measure and made of window-based
lemmatized cooccurrents (1 word before and af-
ter) weighted by Positive Pointwise Mutual Infor-
mation (PPMI). For the thesaurus of compounds,
a preprocessing step was added to identify nom-
inal compounds in texts. This identification was
done in two steps: first, a set of compounds were
extracted from the AQUAINT-2 corpus by rely-
ing on a restricted set of morpho-syntactic pat-
terns applied by the Multiword Expression Toolkit
(mwetoolkit) (Ramisch et al., 2010); then, the
most frequent compounds in this set (frequency
> 100) were selected as reference and their oc-
currences in the AQUAINT-2 corpus were iden-
tified by applying the longest-match strategy to
the output of the TreeTagger part-of-speech tagger
(Schmid, 1994)1. Finally, distributional contexts
made of mono-terms and compounds were built as
stated above and neighbors were found for 29,174
compounds.

4.2 Example selection

We applied the three rules of Section 3 with all
the entries of our thesaurus of compounds and the
upper half in frequency of our mono-term entries.
For mono-terms, we only took the first neighbor
(m = 1) of each entry because of the rather low
performance of the initial thesaurus while for com-
pounds, a larger value (c = 3) was chosen for
enlarging the number of selected examples since
neighbors were globally more reliable (see results
of Table 2). As the selection method makes the
definition of a development set quite difficult, the
values of these two parameters were chosen in a
conservative way.

Table 1 gives for each rule and two combina-
tions of them the number of selected positive ex-
amples (#pos. ex.) and the percentage of positive
(%good pos.) and negative examples (%bad neg.)
found in our Gold Standard resource for thesaurus
evaluation. This resource results from the union of
the synonyms of WordNet 3.0 and the associated
words of the Moby thesaurus. Table 1 also gives
the same data for examples selected by the method
of (Ferret, 2012) (symmetry row, sym. for short),
based on the fact that as similarity relations are

1Longest-match strategy: if C1 is a reference compound
that is part of a reference compound C2, the identification
of an occurrence of C2 blocks out the identification of the
associated occurrence of C1.

method %good pos. %bad neg. #pos. ex.

symmetry 59.7 12.4 796

(1) 56.9 16.1 921
(2) 44.7 14.7 308
(3) 46.2 16.9 40
rules (1,2) 53.0 16.1 1,115
rules (1,2,3) 52.4 15.9 1,131

sym. + (1,2) 54.3 15.0 1,710
sym. + (1,2,3) 53.9 14.5 1,725

Table 1: Selection of examples.

symmetric, a pair of words (A,B) are more likely
to be similar if the first neighbor of A is B and the
first neighbor of B is A. The data for the union of
the examples produced by the two methods also
appear in Table 1.

Concerning the method we propose, Table 1
shows that rule (3), which is a priori the least reli-
able of the three rules as it only requires similarity
and not equality for both heads and modifiers, ac-
tually produces a very small set of examples that
tends to degrade global results. As a consequence,
only the combination of rules (1) and (2) is used
thereafter (row in bold). Table 1 also suggests that
the heads of two semantically linked compounds
are more likely to be actually linked themselves
if they have the same modifier than the modifiers
of two semantically linked compounds having the
same head. This confirms our expectation that the
head of a compound is more related to the mean-
ing of the compound than its modifier. More glob-
ally, Table 1 shows that the symmetry method has
higher results than the second one but their associ-
ation produces an interesting compromise between
the number of examples, 1,710, and its error rate,
45.7. The fact that the two methods only share 201
noun pairs also illustrates their complementarity.

4.3 Reranking evaluation

For our SVM models, we adopted the RBF kernel,
as (Hagiwara et al., 2009), and a grid search strat-
egy for optimizing both the γ and C parameters
by applying a 5-fold cross validation procedure to
our training set and adopting the precision mea-
sure as the evaluation function to optimize. The
models were built with LIBSVM (Chang and Lin,
2001) and then applied to the neighbors of our ini-
tial thesaurus.

Table 2 gives the results of the reranking for
both the method we propose, compound (comp.
for short), with examples selected by rules (1) and

472



(2), and the one of (Ferret, 2012), symmetry. In
either case, they correspond to an intrinsic evalu-
ation achieved by comparing the semantic neigh-
bors of each thesaurus entry with the synonyms
and related words of our Gold Standard resource
for that entry. 12,243 entries with frequency > 10
were present in this resource and evaluated in such
a way. As the neighbors are ranked according to
their similarity value with their entry, we adopted
the classical evaluation measures of Information
Retrieval by replacing documents with synonyms
and queries with entries: R-precision (R-prec.),
Mean Average Precision (MAP) and precision at
different cut-offs (1, 5 and 10).

More precisely, the initial row of Table 2 gives
the values of these measures for our initial the-
saurus of mono-terms while its A2ST-comp row
corresponds to the measures for our thesaurus of
compounds. It should be note that in the case of
the A2ST-comp thesaurus, the number of evalu-
ated entries is very small, restricted to 813 entries,
with also a very small number of reference syn-
onyms by entry. Hence, the results of the evalu-
ation of A2ST-comp have to be considered with
caution even if their high level for the very first se-
mantic neighbors tends to confirm the positive im-
pact of the low level of ambiguity of compounds
compared to mono-terms.

The two following rows gives the results of the
thesauri built from the best models of (Baroni et
al., 2014), B14-count for the count model, whose
main parameters are close or identical to ours,
and B14-predict for the predict model, built from
(Mikolov et al., 2013). These results first illus-
trate the known importance of corpus size, as the
(Baroni et al., 2014)’s corpus is more than 7 times
larger than ours, and the fact that for building the-
sauri, the count model is superior to the predict
model. This last observation is confirmed by the
results of the skip-gram model of (Mikolov et al.,
2013) with its best parameters2 for our corpus (5th

row), which clearly exhibits worst results than ini-
tial. For this Mikolov thesaurus and the follow-
ing reranked ones, each value corresponds to the
difference between the measure for the considered
thesaurus and the measure for the initial thesaurus.
All these differences were found statistically sig-
nificant according to a paired Wilcoxon test with
p-value < 0.05.

2word2vec -cbow 0 -size 600 -window 10 -negative 0 -hs
0 -sample 1e-5

Thesaurus R-prec. MAP P@1 P@5 P@10

initial (A2ST) 7.7 5.6 22.5 14.1 10.8
A2ST-comp 32.7 39.5 34.9 12.3 7.1

B14-count 12.5 9.8 31.9 19.6 15.2
B14-pred 10.9 8.5 30.3 18.4 13.8

Mikolov -2.2 -1.4 -6.2 -4.6 -3.8

symmetry +0.3 +0.1 +2.1 +0.8 +0.6
compound +0.1 +0.0 +2.0 +0.9 +0.6

sym.+comp. +0.3 +0.2 +2.8 +1.2 +0.9

RRF +0.7 +0.6 +3.7 +1.9 +1.4
borda +0.7 +0.5 +3.6 +1.7 +1.3
condorcet +0.5 +0.4 +3.4 +1.6 +1.2
CombSum +0.9 +0.8 +4.7 +2.2 +1.5

CS-w-Mik +1.2 +1.4 +4.2 +2.0 +1.5

Table 2: Evaluation of our initial thesaurus and its
reranked versions (values = percentages).

The analysis of the next two rows of Table 2
first shows that each criterion used for reranking
our initial thesaurus leads to a global increase of
results. The extent of this increase is quite sim-
ilar for the two criteria: symmetry slightly out-
performs compound but the difference is not sig-
nificant. This increase is higher for P@{1,5,10}
than for R-precision and MAP, which can be ex-
plained by the high number of synonyms and re-
lated words, 38.7 on average, that an entry of our
initial thesaurus has in our reference. Hence, even
a significant increase of P@{1,5,10} may have a
modest impact on R-precision and MAP as the
overall recall, equal to 9.8%, is low.

4.4 Thesaurus fusion

Having several thesauri reranked according to dif-
ferent criteria offers the opportunity to apply en-
semble methods. Such idea was already experi-
mented in (Curran, 2002) for thesauri built with
different parameters (window or syntactic based
cooccurrents, etc). We tested more particularly
two general strategies for data fusion (Atrey et al.,
2010): early and late fusions. The first one con-
sists in our case in fusing the training sets built
from our two criteria. As for each criterion, a clas-
sifier is then built from the fused training set and
applied for reranking the initial thesaurus (see the
sym.+comp. row of Table 2).

Table 3 illustrates qualitatively the impact of
this first strategy for the entry esteem. Its Word-
Net row gives all the synonyms for this entry in
WordNet while its Moby row gives the first re-
lated words for this entry in Moby. In our initial

473



WordNet respect, admiration, regard

Moby

admiration, appreciation, accep-
tance, dignity, regard, respect, ac-
count, adherence, consideration,
estimate, estimation, fame, great-
ness, homage + 79 words more

initial

cordiality, gratitude, admiration,
comradeship, back-scratching,
perplexity, respect, ruination,
appreciation, neighbourliness . . .

sym.+comp.

respect, admiration, trust, recog-
nition, gratitude, confidence, af-
fection, understanding, solidarity,
dignity, appreciation, regard, sym-
pathy, acceptance . . .

Table 3: Reranking for the entry esteem with the
early fusion strategy.

thesaurus, the first two neighbors of esteem that
are present in our reference resources are admira-
tion (rank 3) and respect (rank 7). The reranking
produces a thesaurus in which these two words ap-
pear as the first two neighbors of the entry while
its third synonym in WordNet raises from rank 22
to rank 12. Moreover, the number of neighbors
among the first 14 ones that are present in Moby
increases from 3 to 6.

The late fusion strategy relies on the methods
used in Information Retrieval for merging ranked
lists of retrieved documents. More precisely, we
experimented the Borda, Condorcet (Nuray and
Can, 2006) and Reciprocal Rank (RRF) (Cormack
et al., 2009) fusions based on ranks and the Comb-
Sum fusion based on similarity values, normalized
in our case with the Zero-one method (Wu et al.,
2006). The corresponding thesauri were built by
fusing, entry by entry, the lists of neighbors com-
ing from the initial, symmetry and compound the-
sauri.

Table 2 first shows that all the thesauri pro-
duced by our ensemble methods outperform our
first three thesauri, which confirms that initial,
symmetry and compound can bring complemen-
tary information, exploited by the fusion. It also
shows that our late fusion methods are more ef-
fective than our early fusion method. However,
no specific element advocates at this stage for a
generalization of this observation. The evaluation
reported by Table 2 also suggests that for fusing
distributional thesauri, the similarity of a neighbor
with its entry is a more relevant criterion than its
rank. Among the rank based methods, we observe

that RRF is clearly superior to condorcet but only
weakly superior to borda. Finally, the last row
of Table 2 – CS-w-Mik – illustrates one step fur-
ther the interest of ensemble methods for distribu-
tional thesauri: whereas the “Mikolov thesaurus”
gets the worst results among all the thesauri of Ta-
ble 2, adding it to the initial, symmetry and com-
pound thesauri in the CombSum method leads to
improve both R-precision and MAP, with a only
small decrease of P@1 and P@5. From a more
global perspective, it is interesting to note that our
best method, CombSum, clearly outperforms the
reranking method of (Ferret, 2013) with the same
initial starting point.

5 Conclusion and perspectives

In this article, we have presented a method based
on bootstrapping for improving distributional the-
sauri. More precisely, we have proposed a new
criterion, based on the relations of mono-terms in
similar compounds, for the unsupervised selection
of training examples used for reranking the seman-
tic neighbors of a thesaurus. We have evaluated
two different strategies for combining this crite-
rion with an already existing one and showed that a
late fusion approach based on the merging of lists
of neighbors is particularly effective compared to
an early fusion approach based on the merging of
training sets.

We plan to extend this work by studying how
the combination of the unsupervised selection of
examples and their use for training supervised
classifiers can be exploited for improving distribu-
tional thesauri through feature selection. We will
also investigated the interest of taking into account
word senses in this framework, as in (Huang et al.,
2012) or (Reisinger and Mooney, 2010).

References
Pradeep K. Atrey, M. Anwar Hossain, Abdulmotaleb

El Saddik, and Mohan S. Kankanhalli. 2010. Mul-
timodal fusion for multimedia analysis: a survey.
Multimedia Systems, 16(6):345–379.

Marco Baroni, Georgiana Dinu, and Germán
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In 52nd An-
nual Meeting of the Association for Computational
Linguistics (ACL 2014), pages 238–247, Baltimore,
Maryland.

Bartosz Broda, Maciej Piasecki, and Stan Szpakow-
icz. 2009. Rank-Based Transformation in Measur-

474



ing Semantic Relatedness. In 22nd Canadian Con-
ference on Artificial Intelligence, pages 187–190.

Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: a library for support vector machines.
http://www.csie.ntu.edu.tw/˜cjlin/
libsvm.

Gordon V. Cormack, Charles L. A. Clarke, and Ste-
fan Buettcher. 2009. Reciprocal rank fusion outper-
forms condorcet and individual rank learning meth-
ods. In 32nd International ACM SIGIR Conference
on Research and Development in Information Re-
trieval (SIGIR’09), pages 758–759.

James R. Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Work-
shop of the ACL Special Interest Group on the Lexi-
con (SIGLEX), pages 59–66, Philadelphia, USA.

James Curran. 2002. Ensemble methods for auto-
matic thesaurus extraction. In 2002 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2002), pages 222–229.

Georgiana Dinu and Mirella Lapata. 2010. Measur-
ing distributional similarity in context. In 2010 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2010), pages 1162–1172, MIT,
Massachusetts, USA.

Katrin Erk and Sebastian Pado. 2010. Exemplar-based
models for word meaning in context. In 48thth An-
nual Meeting of the Association for Computational
Linguistics (ACL 2010), short paper, pages 92–97,
Uppsala, Sweden, July.

Olivier Ferret. 2010. Testing semantic similarity mea-
sures for extracting synonyms from a corpus. In
Seventh conference on International Language Re-
sources and Evaluation (LREC’10), Valletta, Malta.

Olivier Ferret. 2012. Combining bootstrapping and
feature selection for improving a distributional the-
saurus. In 20th European Conference on Artificial
Intelligence (ECAI 2012), pages 336–341, Montpel-
lier, France.

Olivier Ferret. 2013. Identifying bad semantic
neighbors for improving distributional thesauri. In
51st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2013), pages 561–571,
Sofia, Bulgaria.

Gregory Grefenstette. 1994. Explorations in auto-
matic thesaurus discovery. Kluwer Academic Pub-
lishers.

Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko
Toyama. 2009. Supervised synonym acquisition
using distributional features and syntactic patterns.
Information and Media Technologies, 4(2):59–83.

Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word

prototypes. In 50th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL’12), pages
873–882.

Jun’ichi Kazama, Stijn De Saeger, Kow Kuroda,
Masaki Murata, and Kentaro Torisawa. 2010. A
bayesian method for robust estimation of distribu-
tional similarities. In 48th Annual Meeting of the
Association for Computational Linguistics, pages
247–256, Uppsala, Sweden.

Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In 17th International Confer-
ence on Computational Linguistics and 36th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-COLING’98), pages 768–774, Mon-
tral, Canada.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In 2013 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL HLT 2013), pages 746–751, At-
lanta, Georgia.

Jeffrey Mitchell and Mirella Lapata. 2010. Composi-
tion in distributional models of semantics. Cognitive
Science, 34(8):1388–1439.

Rabia Nuray and Fazli Can. 2006. Automatic rank-
ing of information retrieval systems using data fu-
sion. Information Processing and Management,
42(3):595–614.

Sebastian Padó and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161–199.

Muntsa Padró, Marco Idiart, Aline Villavicencio, and
Carlos Ramisch. 2014. Nothing like good old fre-
quency: Studying context filters for distributional
thesauri. In 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2014),
pages 419–424, Doha, Qatar.

Denis Paperno, Nghia The Pham, and Marco Baroni.
2014. A practical and linguistically-motivated ap-
proach to compositional distributional semantics. In
52nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2014), pages 90–99, Bal-
timore, Maryland.

Tamara Polajnar and Stephen Clark. 2014. Improv-
ing distributional semantic vectors through context
selection and normalisation. In 14th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL 2014), pages 230–238,
Gothenburg, Sweden.

Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010. mwetoolkit: a Framework for Multi-
word Expression Identification. In Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2010), Valetta, Malta, May.

475



Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL 2010), pages 109–117, Los Angeles,
California, June.

Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing.

Shengli Wu, Fabio Crestani, and Yaxin Bi. 2006. Eval-
uating score normalization methods in data fusion.
In Third Asia Conference on Information Retrieval
Technology (AIRS’06), pages 642–648. Springer-
Verlag.

Kazuhide Yamamoto and Takeshi Asakura. 2010.
Even unassociated features can improve lexical dis-
tributional similarity. In Second Workshop on
NLP Challenges in the Information Explosion Era
(NLPIX 2010), pages 32–39, Beijing, China.

Maayan Zhitomirsky-Geffet and Ido Dagan. 2009.
Bootstrapping Distributional Feature Vector Quality.
Computational Linguistics, 35(3):435–461.

476


