



















































Deception Detection in News Reports in the Russian Language: Lexics and Discourse


Proceedings of the 2017 EMNLP Workshop on Natural Language Processing meets Journalism, pages 74–79
Copenhagen, Denmark, September 7, 2017. c©2017 Association for Computational Linguistics

Deception Detection in News Reports in the Russian Language:  

Lexics and Discourse  

 

Dina Pisarevskaya 
dinabpr@gmail.com 

 

 

 

 

Abstract 

Different language markers can be 

used to reveal the differences between 

structures of truthful and deceptive 

(fake) news. Two experiments are 

held: the first one is based on lexics 

level markers, the second one on dis-

course level is based on rhetorical rela-

tions categories (frequencies). Corpus 

consists of 174 truthful and deceptive 

news stories in Russian. Support Vec-

tor Machines and Random Forest 

Classifier were used for text classifica-

tion. The best results for lexical mark-

ers we got by using Support Vector 

Ma-chines with rbf kernel (f-measure 

0.65). The model could be developed 

and be used as a preliminary filter for 

fake news detection. 

1 Introduction 

The research field of deception detection in news 

reports and automated fact checking arose in nat-

ural language processing (NLP) rather recently. It 

can be applied for linguistic expertise, fact check-

ing tools for newsrooms and news aggregators, 

tools for users. 

   We get information from different sources and 

should evaluate the reliability to avoid rumours, 

hoaxes and deceptive (fake) information in news 

reports. The word 'post-truth' was chosen as the 

Oxford Dictionaries Word of the Year 2016 and 

points that objective facts can be less influential 

than appeals to emotion and personal belief. It re-

gards political and other news of our 'post-truth 

era'. In the media community, key persons pay at-

tention to the value of truth in journalism, to the 

necessity of fact checking, to the threat of fake 

news and to the need for technical systems that 

would help diminish the problem: Almar Latour 

(The Wall Street Journal, 2016), sir Tim Berners-

Lee (worldwide web inventor, 2017), Tim Cook 

(Apple, 2017), and Mark Zuckerberg (Facebook, 

2016). 

   There are three types of fake news: serious fab-

rications, large-scale hoaxes and humorous fakes 

(Rubin et al., 2015a). OpenSources 

(www.opensources.co) suggests more news types, 

fake news among them. They are understood as 

fabricated information, disseminated deceptive 

content, or grossly distorted actual news reports. 

This definition corresponds to serious fabrica-

tions. 

   In social media, people participate in the propa-

gation of the news that they find interesting. Al-

gorithmic ranking, social bubbles and group po-

larization may lead to intentional or unintentional 

viral spread of unreliable news. Big amounts of 

news reports with misinformation spread caused 

by political reasons in 2016 in the USA (presiden-

tial election) (Allcott and Gentzkow, 2017). For 

the Russian language the problem of fake news is 

already vital since 2014 (Russian-Ukrainian dis-

course). 

2 Related Work  

Data science companies, academics, media or-

ganizations are working on computational fact 

checking for English: on fake news detection and 

real-time detection algorithms. In 2016, Google 

gave funding to FactMata and Full Fact project to 

develop automated fact checking tools. FactMata's 

(UK) project is devoted to fact checking and claim 

validation by known statistical databases . The 

Full Fact (UK) is developing an automated fact 

checking helper, using the logic of question an-

swering machines: facts from social media will be 

parsed, compared with curated known-true facts 

 

 

 

 

 

 

 

 

 

 

74



and determined as true or false. Tracer News sys-

tem (0.84 accuracy) is a noise filter for journalists 

to discover breaking news in Twitter (Liu et al., 

2016): machine learning models for noise filtering 

and event detection are implemented, NLP is also 

used. “Fake News” Classifier 

(www.classify.news) allows to score the veracity 

of an article by entering its URL. The corpus arti-

cles are based on OpenSources labels. The tool 

focuses on NLP techniques and considers both 

content (bag of words; multinomial Naive Bayes 

classifier) and context (sentiment analysis, capital-

ization and punctuation usage; Adaptive Boost-

ing). HeroX Fact Check Challenge 

(https://herox.com/factcheck/community) (2016-

2017) and FakeNewsChallenge 

(http://www.fakenewschallenge.org/) (2017) com-

petitions were held to help to create fact checking 

systems. 

   As to the winners of HeroX Fact Check Chal-

lenge, Fact Scan (1st place) can check several 

types of claims automatically, such as numerical, 

position, quote and object property claims. Claim 

Buster (2nd place) is also able to check simple 

statements; it can match claims, and it is based on 

knowledge bases. As regards FakeNewsChal-

lenge, the teams focused on the headline-text body 

relationships. Talos Intelligence team (1st place) 

used the ensemble classifier (gradient-boosted de-

cision trees and deep convolutional neural net-

work). Word embeddings, based on Google News 

pretrained vectors, were used for the neural net-

work. Such features are informative for decision 

trees: the number overlapping words between the 

headline and body text; similarities measured be-

tween the word count, bigrams and trigrams; simi-

larities measured after TF-IDF weighting and sin-

gular value decomposition. UCL Machine Read-

ing system (3rd place) is based on lexical and sim-

ilarity features fed through a multi-layer percep-

tron with one hidden layer. Features for checking 

headline-text body consistency contain three ele-

ments: a bag-of-words term frequency vector of 

the headline; a bag-of-words term frequency vec-

tor of the body; the cosine similarity of TF-IDF 

vectors of the headline and the text body. 

   Fake news may be identified on different lev-

els. Usually they are combined, from lexics and 

semantics to syntax. Most studies focus on lexics 

and semantics and some syntax principles; dis-

course and pragmatics have rarely been consid-

ered (Rubin et al., 2015b) due to their complexity. 

   On lexics level, stylistic features (part of speech 

(POS), length of words, subjectivity terms etc.) 

can be extracted that help to apart tabloid news 

(they are similar to fake news) with 0.77 accuracy 

(Lex et al., 2010). Numbers, imperatives, names 

of media persons can be extracted from news 

headlines (Clark, 2014); the numbers of these 

keywords can be used as features for classification 

with SVMs or Naive Bayes Classifier (Lary et al., 

2010). Psycholinguistics lexicons, for instance 

LIWC (Pennebaker and Francis, 1999), can be 

used in performing binary text classifications for 

truthful vs deceptive texts (0.70 accuracy) (Mihal-

cea and Strapparava, 1999) — for example, meth-

ods can be based on frequency of affective words 

or action words. On syntax level, Probability Con-

text Free Grammars can be used (0.85-0.91 accu-

racy) (Feng et al., 2012). On pragmatics level, 

pronouns with antecedents in text are more often 

used in fake news' headlines (Blom and Hansen, 

2015). On discourse level, rhetorical structures are 

used (Rubin et al., 2015b): vector space modeling 

application predicts whether a report is truthful or 

deceptive (0.63 accuracy) for English. Corpus 

consists of seriously fabricated news stories. So 

rhetorical structures and discourse constituent 

parts and their coherence relations are possible 

deception detection markers in English news. 

   As to facts in the described event, in (Sauri and 

Pustejovsky, 2012) model is based on grammati-

cal fact description structures in English and kin-

dred languages. It is implemented in De Facto, a 

factuality profiler for eventualities based on lexi-

cal types and syntax constructions. The FactBank, 

annotated corpus in English, was also created. 

FactMinder, a fact checking and analysis assistant 

based on information extraction, can help to find 

relevant information (Goasdoué et al., 2013). 

Knowledge networks like Wikipedia can be used 

for simple fact checking questions (Ciampaglia et 

al., 2015).  

   In (Hardalov et al., 2016) the combined ap-

proach for automatically distinguishing credible 

from fake news, based on different features and 

combining different levels, is presented: there are 

linguistic (n-gram), credibility-related (capitaliza-

tion, punctuation, pronoun use, sentiment polari-

ty), and semantic (embeddings and DBPedia data) 

features. The accuracy is from 0.75 to 0.99 on 3 

different datasets. 

75



   The impact of different features on deception 

detection was studied in recent works (Fitzpatrick 

et al., 2015; Rosso et al., 2017). 

   There are no automated deception detection 

tools for news reports for Russian, although the 

field of deception detection in written texts is 

studied on the Russian Deception Bank (226 

texts). The majority of research parameters are re-

lated to POS tags, lexical-semantic group, and 

other frequencies of LIWC lexicon words. The 

classifier's accuracy is 0.68 (Litvinova et al., 

2017). Hence, we should base the research for 

Russian on the experience of methods for other 

languages, keeping in mind linguistics, social and 

cultural circumstances.  

3 Research Objective 

The aim is to reveal differences between fake and 

truthful news reports using markers from different 

linguistics levels. We use POS tags, length of 

words, sentiment terms, punctuation on the lexics 

level. Deception detection requires understanding 

of complex text structures, so we use Rhetorical 

Structures Theory (RST) relations as markers on 

the discourse level. In two experiments we shall 

classify the texts from the definite corpus. 

4 Data Collection Principles 

There are no sources that contain verified samples 

of fake and truthful news for Russian, although the 

problem of fake news is annually discussed on con-

ference ''Media Literacy, Media Ecology, Media 

Education: Digital Media for the Future'' (Moscow). 

There are no Factbanks, unbiased fact checking 

websites, crowdsourcing projects, lists of truth-

ful/deceptive sources. We can rely only on the pre-

sented facts, on the factuality.  

   The daily manual monitoring of news lasted 24 

months (June 2015-June 2017). Online newspapers 

in Russian were used as sources. For balance, texts 

were from diverse sources: well-known news agen-

cies’ websites, local or topic-based news portals, 

online newspapers from different countries. News 

source mention was not included in text annotations 

to avoid biases. Blogs and social media texts, ana-

lytic journalism stories based on opinions (not on 

facts) were not taken. We selected only serious fab-

rications. News stories were carefully analyzed in 

retrospect when the factuality was already known, 

to avoid biased evaluation. In case of mutual con-

tradictions in the reports about the same event, a re-

port was added to fake cases if at the same time pe-

riod in online media existed reports with unproven 

facts and with their truthful refutation. So it was an 

intended fake and not a journalist's mistake caused 

by lack of facts. 

5 Corpus Details and Data Analysis  

The corpus consists of news reports about 48 dif-

ferent topics, with equal number of truthful and 

deceptive texts to each topic (not more than 12 

texts for one topic). It contains 174 texts. The 

whole number of tokens is 33049. The mean 

length of texts is 189.04 tokens, the median length 

is 168.5 tokens. The whole number of rhetorical 

relations in corpus is 3147. Mean number of rhe-

torical relations in text is 18.09, the median num-

ber is 16.5.  

   The corpus size is conventional for the initial re-

search on the field of automated deception detec-

tion, especially if we use the discourse level of 

language analysis, because it still requires manual 

annotation. Discourse parsers exist most notably 

for English (RASTA, SPADE, HILDA, CODRA 

etc.), and researchers do not use them even for 

English corpora when they need precise results. 

For comparison, the dataset in the paper which 

describes automated deception detection for news 

reports, based on RST, includes 144 news reports 

that were tagged manually (Rubin et al., 2015b). 

Corpus in the research about the impact of dis-

course markers on argument units classification 

(Eckle-Kohler et al., 2015) consists of 88 docu-

ments, predominantly news texts.  

   We used the following 18 normalized lexical 

markers for each text: average length of tokens; 

type-token ratio; frequency of adverbs; frequency 

of adjectives; frequency of pronouns-adverbs; fre-

quency of numerals-adjectives; frequency of pro-

nouns-adjectives; frequency of conjunctions; fre-

quency of interjections; frequency of numerals; 

frequency of particles; frequency of nouns; fre-

quency of pronouns-nouns; frequency of verbs; 

frequency of all punctuation marks; frequency of 

quotations; frequency of exclamation marks; fre-

quency of lemmas from a sentiment lexicon.  

   All POS tags were obtained with MyStem tool 

for Russian which is for free use (some form 

words were excluded from the analysis). We col-

lected seriously fabricated news reports, so we do 

not take capitalization as a feature. As there are no 

tools for sentiment polarity for Russian for free 

use, we use frequencies of lemmas from a list of 

76



5000 sentiment words from reviews (Сhetviorkin 

and Loukachevitch, 2012). 

   As to the discourse part, RST framework (Mann 

and Thompson, 1988) represents text as an hierar-

chical tree. Some parts are more essential (nucle-

us) than others (satellite). Text segments are con-

nected to each other with relations. The theory 

pretends to be universal for all languages, so we 

chose it for our research. There are no discourse 

parsers for Russian: tagging and validation were 

made manually. We used UAM CorpusTool for 

discourse-level annotation. We based the research 

on the ''classic'' set by Mann and Thompson and 

added to it some more types: so, we created 4 

types of Evidence according to the precision of 

source of information mention. News reports usu-

ally have a definite template, so a rather small 

number of relations was used. We have 33 relation 

types: 'Circumstance', 'Reason', 'Evidence1', 'Evi-

dence2',  'Evidence3',  'Evidence4', 'Contrast', 

'Restatement', 'Disjunction', 'Unconditional', 'Se-

quence', 'Motivation', 'Summary', 'Comparison', 

'Non-Volitional Cause', 'Antithesis', 'Volitional 

Cause', 'Non-Volitional Result', 'Joint', 'Elabora-

tion', 'Background', 'Solution', 'Evaluation', 'Inter-

pretation', 'Concession', 'Means', 'Conjunction', 

'Volitional Result', 'Justify', 'Condition', 'Exempli-

fy', 'Otherwise', 'Purpose'.  To avoid subjectivity of 

annotators' interpretation, we had 2 annotators and 

tried to solve this problem by preparing a precise 

manual for tagging and by developing consensus-

building procedures. We selected Krippendorff’s 

unitized alpha (0.78) as a measure of inter-

annotator agreement. 

   The first dataset is based on statistics data about 

frequencies of lexical markers for each news re-

port. The second one is based on statistics data 

about types of RST relations and their frequencies 

for each news report. In fact, we have a 'bag of re-

lation types', disregarding their order.  

   We selected two supervised learning methods 

for texts classification and  machine learning: 

Support Vector Machines (SVMs) and Random 

Forest, both realized in scikit-learn library for Py-

thon. SVMs were used with linear kernel and with 

rbf kernel. In both experiments (for both datasets) 

we used 10-fold cross-validation for estimator 

performance evaluation.  

   The baseline for all experiments is 50%, be-

cause there is the equal number of truthful and de-

ceptive texts in the corpus. 

6 Statistical Procedures 

The results of two experiments are presented in 

Table 1.  

Table 1: Results for lexical and discourse features 

 

   We can evaluate that for the first one the classifi-

cation task is solved better by SVMs (rbf kernel). 

The most significant features are: average length of 

tokens, frequency of sentiment words, frequency of 

particles, frequency of verbs. It was checked with 

Student's t-test. Although the results of the first ex-

periment are better, for the second one the classifi-

cation task is solved better by Random Forest Clas-

sifier. The most significant rhetorical relation types 

among discourse features are disjunc-

tion/conjunction, non-volitional cause, evaluation, 

elaboration.  Non-volitional cause, elaboration, 

evaluation, conjunction are more typical for decep-

tive texts. Probably authors of fake news pay more 

attention to the causation, because they want to ex-

plain an event with the internal logic, without any 

inconsistencies. 

7 Discussion 

Automated deception detection seems to be a 

promising and methodologically challenging re-

search topic, and further measures should be taken 

to find features for deception/truth detection  in 

automated news verification model for Russian. 

 Preci- 

sion 

Accuracy Recall  F-

measure 

Support Vector Machines, rbf kernel, 10-fold 

cross-validation 

Lexical 

features 

0.62 0.64 0.73 0.65 

Discourse 

features 

0.56 0.54 0.52 0.51 

Support Vector Machines, linear kernel, 10-fold 

cross-validation 

Lexical 

features 

0.62 0.61 0.62 0.60 

Discourse 

features 

0.54 0.53 0.51 0.50 

Random Forest Classifier, 10-fold cross-validation 

Lexical 

features 

0.58 0.56 0.47 0.50 

Discourse 

features 

0.62 0.57 0.52 0.54 

77



The model should be developed, learned and test-

ed on larger data collections with different topics. 

We should use a complex approach and combine 

lexics and discourse methods, also combining 

them with other linguistics and statistical methods. 

For instance, n-grams, word embeddings, psycho-

linguistics features; syntactic level features on top 

of sequences of discourse relations should be  

studied. ’The trees’ - hierarchies of RST relation 

types in texts should also be considered, to get 

better results. The extrapolation of the existing 

model to all possible news reports in Russian 

would be incorrect. But it can already be used as a  

preliminary filter for fake news detection. Results 

of its work should be double-checked, especially 

for suspicious instances. The model is also re-

stricted by the absence of tools and corpora for 

Russian, as typical for NLP tasks for Russian. The 

guidelines for gathering a corpus of obviously 

truthful/deceptive news should also be improved.  

8 Conclusions 

News verification and automated fact checking 

tend to be very important issues in our world, with 

its information warfare. The research is initial. We 

collected a corpus for Russian (174 news reports, 

truthful and fake). We held two experiments, for 

both we applied SVMs algorithm (linear/rbf ker-

nel) and Random Forest to classify the news re-

ports into 2 classes: truthful/deceptive. We used 

18 markers on lexics level, mostly frequencies of 

POS tags in texts. On discourse level we used fre-

quencies of RST relations in texts. The classifica-

tion task in the first experiment is solved better by 

SVMs (rbf kernel) (f-measure 0.65). The model 

based on RST features shows best results with 

Random Forest Classifier (f-measure 0.54) and 

should be modified. In the next research, the com-

bination of different deception detection markers 

for Russian should be taken in order to make a 

better predictive model. 

References  

 H. Allcott and M. Gentzkow. 2017. Social Media and 

Fake News in the 2016 Election. In Journal of 

Economic Perspectives. Vol. 31-2: 211-236. 

J.N. Blom and K.R. Hansen. 2015. Click bait: For-

ward-reference as lure in online news headlines.  

Journal of Pragmatics, 76: 87-100. 

I.I. Chetviorkin and N.Y. Loukachevitch. 2012. Ex-

traction of Russian Sentiment Lexicon for Product 

Meta-Domain. In Proceedings of COLING 2012: 

Technical Papers: 593–610. 

GL Ciampaglia, P. Shiralkar, LM Rocha, J. Bollen, F. 

Menczer, and A. Flammini. 2015. Computational 

Fact Checking from Knowledge Networks. PLoS 

ONE 10(6): e0128193. 

https://doi.org/10.1371/journal.pone.0128193 

R. Clark. 2014. Top 8 Secrets of How to Write an Up-

worthy Headline, Poynter, URL: 

http://www.poynter.org/news/media-

innovation/255886/top-8-secrets-of-how-to-write-

an-upworthy-headline/ 

J. Eckle-Kohler, R. Kluge, I. Gurevych. 2015. On the 

Role of Discourse Markers for Discriminating 

Claims and Premises in Argumentative Discourse, 

In Proceedings of the 2015 Conference on Empiri-

cal Methods in Natural Language Processing 

(EMNLP): 2236-2242. 

S. Feng, R. Banerjee, and Y. Choi. 2012. Syntactic 

Stylometry for Deception Detection. In Proceed-

ings 50th Annual Meeting of the Association for 

Computational Linguistics, Association for Com-

putational Linguistics, Vol. 2: Short Papers: 171–

175. 

E. Fitzpatrick, J. Bachenko, and T. Fornaciari. 2015. 

Automatic Detection of Verbal Deception. Synthe-

sis Lectures on Human Language Technologies. 

Morgan & Claypool  Publishers. 

F. Goasdoué, K. Karanasos, Y. Katsis, J. Leblay, I. 

Manolescu, and S. Zampetakis. 2013. Fact Check-

ing and Analyzing the Web. In SIGMOD - ACM In-

ternational Conference on Management of Data, 

Jun 2013, New York, United States.  

M. Hardalov, I. Koychev, P. Nakov. 2016. In Search 

of Credible News. In Artificial Intelligence: Meth-

odology, Systems, and Applications: 172-180. 

D.J. Lary, A. Nikitkov, and D. Stone. 2010. Which 

Machine-Learning Models Best Predict Online 

Auction Seller Deception Risk? American Account-

ing Association AAA Strategic and Emerging 

Technologies. 

E. Lex, A. Juffinger, and M. Granitzer. 2010. Objec-

tivity classification in online media. In Proceedings 

of the 21st ACM conference on Hypertext and hy-

permedia: 293-294. 

O. Litvinova, T. Litvinova, P. Seredin, Y. Lyell. 2017. 

Deception Detection in Russian Texts. In Proceed-

ings of the Student Research Workshop at the 15th 

Conference of the European Chapter of the Associ-

ation for Computational Linguistics: 43-52.  

X. Liu, Q. Li,  A. Nourbakhsh, R. Fang, M. Thomas, 

K. Anderson, R. Kociuba, M. Vedder, S. Pomer-

ville, R. Wudali, R. Martin, J. Duprey, A. Vachher, 

W. Keenan, and S. Shah. 2016. Reuters Tracer: A 

78



Large Scale System of Detecting & Verifying Real-

Time News Events from Twitter. In Proceedings of 

the 25th ACM International on Conference on In-

formation and Knowledge Management. Indianap-

olis, Indiana, USA, October 24-28, 2016: 207-216.  

W.C. Mann and S.A. Thompson. 1988. Rhetorical 

Structure Theory: Toward a Functional Theory of 

Text Organization, Text, vol. 8, no.3: 243-281.  

R. Mihalcea and C. Strapparava. 1999. The Lie Detec-

tor: Explorations in the Automatic Recognition of 

Deceptive Language. In Proceedings 47th Annual 

Meeting of the Association for Computational Lin-

guistics, Singapore: 309-312.  

J. Pennebaker and M. Francis. 1999. Linguistic in-

quiry and word count: LIWC, Erlbaum Publishers. 

P. Rosso and L. Cagnina. 2017. Deception Detection 

and Opinion Spam. In: A Practical Guide to Senti-

ment Analysis, Cambria, E., Das, D., Bandyopadh-

yay, S., Feraco, A. (Eds.), Socio-Affective Compu-

ting, vol. 5, Springer-Verlag: 155-171. 

V.L. Rubin, N.J. Conroy, and Y. Chen. 2015a. Decep-

tion Detection for News: Three Types of Fakes. 

Conference: ASIS T2015, At St. Louis, MO, USA.  

V.L. Rubin, N.J. Conroy, and Y.C. Chen. 2015b. To-

wards News Verification: Deception Detection 

Methods for News Discourse. In Proceedings of the 

Hawaii International Conference on System Sci-

ences (HICSS48) Symposium on Rapid Screening 

Technologies, Deception Detection and Credibility 

Assessment Symposium, January 5-8, 11 pages. 

R. Sauri and J. Pustejovsky. 2012. Are You Sure That 

This Happened? Assessing the Factuality Degree of 

Events in Text. In Computational Linguistics: 1-39.  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

79


