



















































Identification of Mentions and Relations between Bacteria and Biotope from PubMed Abstracts


Proceedings of the 4th BioNLP Shared Task Workshop, pages 64–72,
Berlin, Germany, August 13, 2016. c©2016 Association for Computational Linguistics

Identification of Mentions and Relations between Bacteria and Biotope
from PubMed Abstracts

Cyril Grouin
LIMSI, CNRS, Université Paris-Saclay

Bât 508, Campus Universitaire, F-91405 Orsay
cyril.grouin@limsi.fr

Abstract

This paper presents our participation in
the Bacteria/Biotope track from the 2016
BioNLP Shared-Task. Our methods rely
on a combination of distinct machine-
learning and rule-based systems. We used
CRF and post-processing rules to identify
mentions of bacteria and biotopes, a rule-
based approach to normalize the concepts
in the ontology and the taxonomy, and
SVM to identify relations between bac-
teria and biotopes. On the test datasets,
we achieved similar results to those ob-
tained on the development datasets: on
the categorization task, precision of 0.503
(gold standard entities) and SER of 0.827
(both NER and categorization); on the
event relation task, F-measure of 0.485
(gold standard entities, ranking third out
of 11) and of 0.192 (both NER and event
relation, ranking first); on the knowledge-
based task, mean references of 0.771 (gold
standard entities) and of 0.202 (both NER,
categorization and event relation).

1 Introduction

In this paper, we present the methods we used
while participating in the Bacteria/Biotope track
from the 2016 BioNLP Shared-Task. We partially
reused the method we designed while participating
in the previous edition of the challenge (Grouin,
2013), and we updated afterwards while designing
new experiments (Lavergne et al., 2015).

2 Background

Four teams participated in the Bacteria/Biotope
track (Bossy et al., 2015) from the 2013 BioNLP
Shared-Task.

On the entity detection and categorization
task, the best results were obtained using either
machine-learning approaches, as done by Bannour
et al. (2013) who ranked first (Slot Error Rate
(SER) of 0.661), or using syntactic hand-coded
rules, as done by Karadeniz and Özgür (2013)
who ranked second (SER=0.676). We ranked third
(SER=0.678) using CRF and normalization rules.

On the localization relation extraction task,
the best results were obtained through machine-
learning approaches. Björne and Salakoski (2013)
ranked first (F=0.42), using a system based on
Support Vector Machine (SVM), while Claveau
(2013) ranked second (F=0.40) using a lazy ma-
chine learning (kNN) approach.

3 Task description

3.1 Presentation

The 2016 Bacteria/Biotope track1 (Deléger et
al., 2016) consists in three main objectives:
(i) named entity recognition (NER) to identify
mentions of bacteria and biotopes from scientific
abstracts, (ii) categorization to normalize men-
tions of bacteria in the NCBI taxonomy and men-
tions of biotopes in the OntoBiotope ontology, and
(iii) event extraction to identify relations of local-
ization between a bacteria and a biotope.

The track is organized into three main tasks,
based on gold standard annotations of entities: a
categorization task (cat), an event extraction task
(event), and a knowledge-base population task
(kb) which combines categorization and relation
identification. Additionally, each task is com-
posed of a named entity recognition sub-task: cat-
egorization and relation identification are based
on predictions of entities (cat+ner, event+ner, and
kb+ner tasks) instead of gold standard annotations.

1http://2016.bionlp-st.org/

64



3.2 Material

3.2.1 Corpus
The corpus is composed of 215 scientific texts (ti-
tle and abstract) focusing on bacteria, extracted
from the Medline database. This corpus is split
into three datasets: training (71 texts), develop-
ment (36 texts), and test (108 texts).2 We used the
train dataset to develop our systems and to tune
our models while results produced by those sys-
tems were evaluated on the dev dataset. The test
datasets were used for the official evaluation.

3.2.2 Annotations
Bossy et al. (2016) defined three kinds of entities
(bacteria, habitat, geographical) and one type of
relation (lives in) between a bacteria and a biotope.

Entities Annotations of entities imply three
kinds of annotations: (i) single entities, (ii) em-
bedded entities, in case of different meanings, and
(iii) discontinuous entities, to deal with coordi-
nation. Figure 1 highlights discontinuous annota-
tions (throat cultures) and embedded annotations
(throat within throat cultures, and nasopharyngeal
within nasopharyngeal cultures).

18/03/2016 12:21brat

Page 1 sur 1http://127.0.0.1:8001/index.xhtml#/BioNLP2016/event-ner/BB-event+ner-8532424

Respiratory carriage of Kingella kingae among healthy children.

The role of Kingella kingae as an invasive pathogen of young children is being increasingly recognized, but the niche of the organism in the respiratory tract and its prevalence in the 

normal flora of children remain unknown. To investigate these two aspects throat and nasopharyngeal cultures were obtained every 2 weeks from two 

cohorts of children, ages 6 to 42 months on enrollment, attending a day-care center in southern Israel. To determine the age-related prevalence of K. kingae, throat cultures were 

obtained from children ages 6 months to 14 years hospitalized for elective surgery who had not received antibiotics during the previous 30 days and from 

healthy infants younger than 6 months attending a well-baby-care clinic for routine vaccinations. During an 11-month follow-up 109 of 624 (27.5%) throat cultures but none of the 

nasopharyngeal cultures obtained from 48 day-care center attendees grew K. kingae. The monthly prevalence of K. kingae ranged from 6.1 to 34.6% with December and April peaks. 

Overall 35 of 48 (72.9%) children had at least one positive culture for the organism. Among the 27 children who had > or = 2 positive cultures, continuous and intermittent patterns of 

carriage were observed. None of the colonized children experienced an invasive K. kingae infection. The prevalence of pharyngeal carriage among surgical patients was 8.0%, and the 

organism was not isolated from any of the infants younger than 6 months attending the well-baby-care clinic.

Habitat Bacteria Habitat
Lives_InLives_In

Bacteria Habitat Habitat
Lives_In

Lives_In
Lives_In

Habitat Habitat Habitat
Habitat

Habitat

Habitat

HabitatLives_In

Habitat Habitat Geographical Bacteria Habitat

Habitat

Habitat

Habitat Habitat

Habitat

Habitat

Habitat

Lives_In
Lives_In

Habitat

Habitat

Habitat

Habitat

Bacteria Bacteria
Lives_In Equiv Lives_In

Lives_In

Lives_In
Lives_In

Habitat Habitat
Lives_In

Lives_In

Habitat Bacteria Habitat Habitat
Lives_In Lives_In

Lives_In

Habitat Habitat

Habitat

1

2

brat/BioNLP2016/event-ner/BB-event+ner-8532424

Figure 1: Discontinuous and embedded annota-
tions of entities

Specific annotation rules apply for classifiers
(genus, species, strain) and generic classes (bac-
teria, cohort, in vivo, microbe, suspension) which
must not be annotated, except for specified strain
(mutants, serotypes, serovars).

Categorization The categorization focuses on
two types of entity (bacteria, habitat). Annotations
provide the ID for each mention to be normalized,
based on the NCBI taxonomy3 (Federhen, 2002)
for mentions of bacteria and the OntoBiotope on-
tology4 (Nédellec, 2016) for mentions of habitat.

2The test dataset is split into two datasets: one set of
54 files for all tasks implying a named entity recognition
(NER) process (cat+ner, event+ner, kb+ner) and a second set
of 54 files giving gold standard annotations of bacteria and
biotope for tasks without NER (cat, event, kb).

3http://www.ncbi.nlm.nih.gov/taxonomy
4http://2016.bionlp-st.org/tasks/bb2/

OntoBiotope_BioNLP-ST-2016.obo

Mentions of bacteria are normalized into only
one category while mentions of habitat can be
normalized into several categories. The catego-
rization into one or several categories for habi-
tat mentions is dependent on the structure of
the ontology, whether an “is a” relation between
category candidates exists in the ontology or
not (see figure 2). As an example, the men-
tion chicks is normalized into three categories
(“laboratory animal—000323”, “infant–002177”,
“chicken–002229”) while all mentions of mice are
normalized into one category (“laboratory mice—
002153”) since this category is related with the
category “laboratory animal—000323”.

[Term]
id: OBT:000323
name: laboratory animal
is_a: OBT:000218 ! animal

[Term]
id: OBT:002153
name: laboratory mice
is_a: OBT:001865 ! mouse
is_a: OBT:000323 ! laboratory animal

[Term]
id: OBT:002229
name: chicken
is_a: OBT:002165 ! poultry

Figure 2: Extract from the OntoBiotope ontology

Relations Annotations of relations always im-
ply one bacteria with one or several biotopes
(habitat, geographical). Figure 3 shows relations
between a bacteria and two biotopes, a geograph-
ical unit (UK) and a habitat (UK retail poultry).
According to the guidelines, even if arguments
from a relation must be as close as possible, one
can find a few cases of relations between two dis-
tant entities. The longest distance is of 1868 char-
acters, 276 words, implying 10 sentences.

18/03/2016 13:41brat

Page 1 sur 1http://127.0.0.1:8001/index.xhtml#/BioNLP2016/event-ner/BB-event+ner-23855904

Widespread acquisition of antimicrobial resistance among Campylobacter isolates from UK retail poultry and evidence for clonal expansion of resistant lineages.

Antimicrobial resistance is increasing among clinical Campylobacter cases and is common among isolates from other sources, specifically retail poultry - a major source of human 

infection. In this study the antimicrobial susceptibility of isolates from a UK-wide survey of Campylobacter in retail poultry in 2001 and 2004-5 was investigated. The occurrence of 
phenotypes resistant to tetracycline, quinolones (ciprofloxacin and naladixic acid), erythromycin, chloramphenicol and aminoglycosides was quantified. This was compared with a 
phylogeny for these isolates based upon Multi Locus Sequence Typing (MLST) to investigate the pattern of antimicrobial resistance acquisition.

Antimicrobial resistance was present in all lineage clusters, but statistical testing showed a non-random distribution. Erythromycin resistance was associated with Campylobacter coli. 
For all antimicrobials tested, resistant isolates were distributed among relatively distant lineages indicative of widespread acquisition. There was also evidence of clustering of 
resistance phenotypes within lineages; indicative of local expansion of resistant strains.

These results are consistent with the widespread acquisition of antimicrobial resistance among chicken associated Campylobacter isolates, either through mutation or horizontal gene 

transfer, and the expansion of these lineages as a proportion of the population. As Campylobacter are not known to multiply outside of the host and long-term carriage in humans is 

extremely infrequent in industrialized countries, the most likely location for the proliferation of resistant lineages is in farmed chickens.

Bacteria Geographical

Habitat
Lives_In

Lives_In

Habitat Bacteria Habitat Habitat
Lives_In Lives_In

Lives_In

Geographical Bacteria Habitat
Lives_InLives_In

Bacteria

Habitat Bacteria
Lives_In

Bacteria Habitat
Lives_In

Lives_In

Habitat

Lives_In

1

2

3

4

brat/BioNLP2016/event-ner/BB-event+ner-23855904

Figure 3: Example of relation annotations

3.2.3 Statistics
We present in table 1 the number of annotations
for each category of entities (bacteria, habitat, ge-
ographical) and relations (lives in), as well as the
number of categorizations performed in the asso-
ciated resource (OntoBiotope ontology or NCBI

65



taxonomy) in each dataset (train, dev, and test5).
The figures presented in cells with a grey back-
ground refer to the number of predictions to be
made during the challenge. While annotations
of entities are found in almost all files (one file
from the train dataset does not propose any anno-
tation), relations are found in about 80% of files
(i.e., 84 files out of 107 files from the train+dev
datasets). The number of annotated entities per file
is quite unbalanced, from 1 to 69 entities.

Annotations Train Dev
Test

#1 #2
Number of files 71 36 54 54

Bacteria 375 244 341 401
Entities Habitat 747 454 720 621

Geographical 36 38 37 27

Category
NCBI Taxon 376 245 347 401
OntoBiotope 825 535 861 681

Relations Lives in 327 223 340 314

Table 1: Number of annotations per category in
each dataset (test #1=dataset with reference an-
notations of entities, #2=dataset without annota-
tions). Grey background refers to the number of
predictions to be made during the challenge

We observed that discontinuous entities:
(i) mainly concern habitat entities (87.0%),
(ii) generally involve two entities, more rarely
three entities, and that (iii) the pivot shared by
discontinuous and continuous entities is generally
at the end of the portion (e.g., “cultures” in throat
and nasopharyngeal cultures). In the training
and development datasets (107 files), out of
1894 annotations of entities, we only found 46
discontinuous entities (i.e., 2.4% of annotations
are discontinuous entities).

4 Methods

Based on the three main objectives of the track and
the previous observations, we considered distinct
systems (cf. figure 4): named entity recognition,
categorization, and relation identification. We did
not use any of the provided supporting resources.
Due to the low number of discontinuous entities,
we decided not to process this type of annotation.

5Test #1 refers to the test dataset with gold standard an-
notations of entities (cat+ner, event+ner, kb+ner tasks) while
test #2 refers to the test dataset without annotations of entities
(cat, event, kb tasks).

4.1 Additional data
Presentation In order to improve the robustness
of our systems, we annotated a new set of 22 files.6

To produce this new set, we queried PubMed
with names of bacteria we randomly selected from
the train and development datasets: Francisella,
Lactobacillus, LVS, Mycoplasma, Rickettsia, Tri-
chomonas vaginalis and Vibro parahaemolyticus.
Among all results returned by PubMed, we kept
abstracts published in 2016 we found interesting.

Annotations We used our systems (see sec-
tions 4.2 and 4.4) to automatically pre-annotate
this dataset. One human annotator corrected and
completed the automatic pre-annotations in one
hour using the BRAT annotation tool (Stenetorp et
al., 2012). Since we were not trained to annotate
such files, even if we tried to follow the guidelines
(Bossy et al., 2016), we hope our annotations are
not too much inconsistent with annotations pro-
vided by the organizers. Our dataset includes 252
annotations of bacteria, 176 habitat, 31 geograph-
ical and 130 relations. Except for habitat and rela-
tions, this distribution is consistent with statistics
presented in table 1.

4.2 Named Entity Recognition
4.2.1 Presentation
We considered the named entity recognition
(NER) issue as a classification task, where tokens
from a text should be classified into three cate-
gories (bacteria, habitat, geographical). Our NER
system relies both on machine-learning approach
and post-processing rules.

Machine-learning Conditional Random Fields
(CRF) (Lafferty et al., 2001) are widely used for
sequence labeling tasks. Our experiments rely on
the Wapiti system (Lavergne et al., 2010), based
on the linear-chain CRFs framework.

The feature sets are: (i) the token itself, (ii) to-
ken typographic case, presence of punctuation
marks in the token, presence of digits in the to-
ken, token length, (iii) identification of the token
in the OntoBiotope ontology or in the NCBI tax-
onomy, (iv) semantic class of the token among 37

6Our additional dataset, annotated before the release of
the test datasets, is composed of files (title and abstract) cor-
responding to the following PMIDs: 1262454, 21624472,
26358917, 26510639, 26678135, 26709916, 26773254,
26901499, 26902724, 26919818, 26941131, 26941728,
26942354, 26950451, 26951983, 26961264, 26962869,
26964722, 26965788, 26965874, 26968160, 26968657.
None of our additional data are also part of the test datasets.

66



Corpus (title and abstract)
• train+dev provided datasets
• additional dataset

Additional information
• part-of-speech tag (Wapiti)
• semantic classes (Cocoa)
• unsupervised clustering (Brown)

Identification of entities
(bacteria, habitat, geographical)

• CRF model (Wapiti)
• post-processing rules
• embedded entities

Gold standard
annotations of entities

Normalization
of entities

Identification of
relations (bacteria
lives_in location)

SVM Light

Terminological resources
• OntoBiotope ontology
• NCBI taxonomy

Rules (exact match
and partial match)

Figure 4: Systems used to identify entities, normalize entities, and identify relations

pre-defined classes (Body part, Chemical, Food,
Habitat, Organism, Physiology, etc.), provided by
the Cocoa web API,7 (v) part-of-speech tag8 of the
token, and (vi) cluster ID of each token through
an automatic unsupervised clustering of all to-
kens from the train and dev datasets into 120 clus-
ters, using the algorithm designed by Brown et al.
(1992) and implemented by Liang (2005).

Since a lot of tokens from texts are not men-
tions of bacteria, habitat and geographical,9 those
unannotated tokens lead to an unbalanced distri-
bution of data. This may imply an over-training of
the CRF system of the unannotated tokens. In or-
der to reduce this over-training issue, we deleted
portions of unannotated tokens. Specifically, we
deleted parts of text composed of unannotated to-
kens, if those parts are distant of more than 16 to-
kens10 from the closest annotated token. As a con-
sequence, we kept the wholeness of the context
of annotated parts and we reduced the number of
unannotated tokens in our training set.

We tuned our system to predict widest enti-
ties since we considered that shorter entities can
easily be identified through post-processing rules.
Because embedded entities only concern habitats,
this strategy does not concern bacteria and geo-
graphical units. So that the CRF produces widest
entities, in case of embedded annotations, we only
kept the widest entities in the sample file given as
input to train the CRF model.

7Cocoa: compact cover annotator for biological noun
phrases, http://npjoint.com/annotate.php

8POS tagging was performed using an English POS CRF
model for Wapiti: https://wapiti.limsi.fr/

9Based on our tokenization, among 15 530 tokens from
the training dataset, only 2 110 of them (i.e., 13.59%) are part
of bacteria, habitat and geographical mentions.

10This distance of 16 tokens has been chosen empirically.
This threshold reduced by 23.1% the number of unannotated
tokens in the training dataset. From now on, the 2 110 anno-
tated tokens represent 20.45% of all tokens.

Post-processing In order to improve the predic-
tions we made in the previous step and to deal with
some of the specific annotation rules defined in the
guidelines (Bossy et al., 2016), we designed a few
post-processing rules:

• annotation of abbreviations (EHEC, EPEC,
LVS, MRSA, etc.), generic classes with an ini-
tial upper case (Bacteria, Bacterium), some
nomenclatural suffixes (sp., spp.), adjectives
for habitat (aquatic, nosocomial, saprophyte)
and geographical (northern, southern, etc.);

• deletion of annotations for generic classes
(bacteria, bacterial, bacterium), modifiers
(methicillin-resistant, pathogenic), some
nomenclatural suffixes (gen. nov., sp. nov.),
and 34 generic habitat terms (antibiotic,
ecosystem, world, etc.).

Embedded entities Since our CRF predicted
widest entities, we processed embedded habitat
entities through a post-processing system. For all
predictions of mentions of habitat, we searched for
shortened entities within widest entities. As an
example, based on the prediction gastric mucosa-
associated lymphoma, this simple rule allows us
to identify the single mention gastric. We thus in-
creased the coverage of the habitat mentions.

4.2.2 Design of experiments
We designed several experiments, depending on
the size of the training corpus and whether we used
or not post-processing rules and embedded entities
processing. Results are presented in section 5.1.1.
The configuration we used on the test dataset is
the following one: we trained the final CRF model
on all available annotated files (193 files),11 we

11Those annotated files came from the training dataset
(71 files), the development dataset (36 files), the additional
dataset we manually annotated (22 files), and the test #1

67



applied post-processing rules to correct the CRF
outputs, and we processed the embedded entities
through a last script.

4.3 Categorization

Exact match We performed the categorization
task using a basic rule-based approach. We
searched the mention to normalize in the Onto-
Biotope ontology (habitat) or in the NCBI tax-
onomy (bacteria), through an exact match search,
and returned the corresponding numeric identifier.

Partial match Additionally, we searched for
partial matching of mentions of bacteria in the tax-
onomy: (i) shortened versions: H. pylori vs. He-
licobacter pylori, (ii) specified versions: bacil-
lus intermedius s3-19 vs. bacillus intermedius,
and (iii) linguistics variations: plural form (lac-
tobacilli vs. lactobacillus) or adjectival derivation
(mycobacterial vs. mycobacteria). Similarly, we
searched for partial matching of mentions of habi-
tat in the ontology: (i) linguistic variations: plural
forms (patients vs. patient), hand-coded nominal-
ization of adjectives (clinical vs. clinic), (ii) split
of multi-terms into single terms (human and blood
vs. human blood), and (iii) hand-coded transfor-
mation of specific cases (adult is replaced by hu-
man adult; children is replaced by child).

Default value At last, we defined default values
for all unmatched mentions of bacteria and habi-
tat, based on the most used values in the training
and development datasets (this choice is not rele-
vant for all unmatched mentions but it allows us
to slightly improve our results). We used the tax-
onomy entry #210 (i.e., Campilobacter pylori and
Helicobacter pilori) for bacteria, and the Onto-
Biotope entry #002216 patient with infectious dis-
ease (the second most used category) for habitat.

4.4 Relation Extraction

In order to identify relations between bacteria
and biotope, we designed experiments based on
the SVM framework (Vapnik, 1995), as done by
Björne and Salakoski (2013). Our experiments
rely on the SVM Light implementation proposed

dataset (54 files). For clarification, the named entity recog-
nition evaluation (cat+ner, event+ner, kb+ner tasks) is per-
formed on the test #2 dataset, composed of different files than
the test #1 dataset. As a consequence, since there is no com-
mon files between test datasets #1 and #2, the use of the an-
notated files from the test #1 dataset to train the final CRF
model does not hedge the official evaluation.

by Joachims (1999). Since a few long distance re-
lations exist, in order to ensure the robustness of
our system, we decided to remove all relations im-
plying a distance higher than 80 tokens between
both entities from our training set. This thresh-
old produced the best results. It allows us to keep
the shortest relations from the training dataset (i.e.,
60% of all positive relations). We strictly balanced
positive and negative examples to train our model.

The feature sets are: (i) a bag of words of all
tokens from both entities to be linked, and (ii) the
distance in characters between those entities.

5 Results

5.1 Development dataset
In this section, we present the results we achieved
on the development dataset. Since we produced
outputs compatible with the BRAT annotation
tool, results were computed using the BRATe-
val evaluation tool developed by Verspoor et al.
(2013) and updated by Deléger et al. (2014). This
evaluation tool allows us to evaluate all kinds of
entities (single, embedded and discontinuous enti-
ties) as well as relations between entities.

5.1.1 Named entity recognition
Table 2 presents the results we achieved on the de-
velopment dataset in the named entity recognition
sub-task. We give both the F-measure we achieved
on each category (bacteria, habitat, geographical)
and the detailed overall results (exact match). We
designed five experiments:

1. CRF model trained on the train dataset
(71 files);

2. CRF model trained on the train+additional
datasets (93 files);

3. CRF model trained on the train+additional
datasets (93 files) using an over-training re-
duction function (we reduced the number of
tokens which must not be annotated);

4. CRF model trained on the train+additional
datasets (93 files) using an over-training re-
duction function, and post-processing rules
were applied (all categories);

5. CRF model trained on the train+additional
datasets (93 files) using an over-training re-
duction function, post-processing rules were
applied (all categories), and embedded enti-
ties (habitat) were processed.

68



Entity F-measures Overall results
# Bact Hab Geo P R F
1 0.668 0.470 0.727 0.721 0.452 0.556
2 0.769 0.462 0.739 0.753 0.488 0.592
3 0.772 0.469 0.739 0.740 0.500 0.597
4 0.785 0.469 0.739 0.747 0.504 0.602
5 0.785 0.523 0.739 0.737 0.548 0.628

Table 2: Results on the development dataset,
F-measure for each category (Bact=Bacteria,
Hab=Habitat, Geo=Geographical) and overall re-
sults (P=Precision, R=Recall, F=F-measure) de-
pending on the experiment

5.1.2 Categorization
Table 3 presents the results we achieved on the
development dataset for the categorization task.
Our evaluation only computes an exact match be-
tween the IDs from the taxonomy and the ontol-
ogy provided in the hypothesis and the reference.
This evaluation does not compute any similarity
distance within the hypothesis and reference cat-
egories. We give the overall and detailed results
for both the OntoBiotope ontology and the NCBI
taxonomy. Results are provided for two tasks:

1. categorization performed on the entities iden-
tified by our CRF system, configuration #5
(cat+ner task);

2. categorization performed on the gold stan-
dard annotations of entities (cat task).

# Evaluation P R F
1 Overall 0.404 0.286 0.335

OntoBiotope 0.509 0.360 0.422
NCBI taxonomy 0.621 0.457 0.527

2 Overall 0.456 0.412 0.433
OntoBiotope 0.570 0.515 0.541
NCBI taxonomy 0.886 0.885 0.886

Table 3: Results (exact match) on the development
dataset on the categorization tasks (P=Precision,
R=Recall, F=F-measure)

5.1.3 Relations
Table 4 presents the results we achieved (exact
match) on the development dataset in the relation
identification task. We designed four experiments:

1. SVM model trained on the train dataset
(71 files), prediction of entities from the CRF
system (event+ner task);

2. SVM model trained on the train+additional
dataset (93 files), prediction of entities from
the CRF system (event+ner task);

3. SVM model trained on the train dataset
(71 files), gold standard annotations of enti-
ties (event task);

4. SVM model trained on the train+additional
dataset (93 files), gold standard annotations
of entities (event task).

# Evaluation P R F
Entities from the CRF system (event+ner task)

1 Overall 0.171 0.213 0.189
Bacteria-Habitat 0.162 0.235 0.192
Bacteria-Geographical 0.364 0.111 0.170

2 Overall 0.190 0.213 0.201
Bacteria-Habitat 0.181 0.235 0.204
Bacteria-Geographical 0.400 0.111 0.174
Entities from the gold standard (event task)

3 Overall 0.381 0.652 0.480
Bacteria-Habitat 0.355 0.658 0.461
Bacteria-Geographical 0.622 0.622 0.622

4 Overall 0.385 0.652 0.484
Bacteria-Habitat 0.357 0.658 0.463
Bacteria-Geographical 0.657 0.627 0.639

Table 4: Results on the development dataset
on the relation identification tasks (P=Precision,
R=Recall, F=F-measure)

5.1.4 Online evaluation service
Since the online evaluation service provides a dis-
tinct evaluation (giving final scores and using dif-
ferent metrics), in order to compare the results
we achieved on both the development and the
test datasets, we present in table 5 the results we
achieved on all tasks on the development datasets
using our last configuration, as computed by the
evaluation service.

5.2 Test dataset (official results)
Table 6 presents the results we achieved on the
test dataset. Our results are similar to results ob-
tained on the development datasets. This observa-
tion highlights the robustness of our methods.

We ranked second (out of 2) on all categoriza-
tion tasks. We ranked third (out of 11) on the
event task, and first (out of 3) on the event+ner
task. At last, we were the only participant on all
knowledge-based tasks.

69



task Official results (dev)

cat
Precision

1.000

cat+ner
SER Mism Ins Del
0.702 49.85 127 314

event
Precision Recall F-measure

0.389 0.644 0.485

event+ner
SER P R F
1.486 0.216 0.201 0.208

kb
Mean references

0.7861

kb+ner
Mean references

0.2074

Table 5: Official results computed on the
development datasets (SER=Slot Error Rate,
Mism=Mismatch, Ins=Insertion, Del=Deletion,
P=Precision, R=Recall, F=F-measure)

6 Discussion

6.1 Observations

Additional data A first observation concerns
the use of additional data. Increasing the num-
ber of annotated files proved to be useful for all
machine-learning approaches. In the named en-
tity recognition task—using a CRF system—we
gained +3.6 points of F-measure (see table 2).
In the relation identification task—using a SVM
system—we gained +1.2 points of F-measure for
relations based on entities predicted by the CRF,
and +0.4 point for relations based on gold standard
entities annotations (see table 4). The advantage of
using more annotated data is real for all tasks.

Post-processing rules Despite the use of both
additional data and over-training reduction func-
tion, the CRF model achieved moderate re-
sults (F=0.597, see table 2). The use of
post-processing rules to refine the CRF outputs
slightly increased the overall results (+0.5 points,
F=0.602) and mainly impacted the bacteria cate-
gory (+1.3 points). At last, processing embedded
habitat entities with rules improved the overall re-
sults (+2.6 points, F=0.628). Using a few post-
processing rules increased by +3.1 points the over-
all results achieved through the CRF model.

Named entity recognition Our strategy based
on four steps (additional annotated data, over-
training reduction function, post-processing rules,
and embedded entities processing) allows us to

task Official results (test)

cat
Precision

0.503

cat+ner
SER Mism Ins Del
0.827 198.16 192 455

event
Precision Recall F-measure

0.388 0.646 0.485

event+ner
SER P R F
1.558 0.193 0.192 0.192

kb
Mean references

0.7714

kb+ner
Mean references

0.2024

Table 6: Official results computed on the test
dataset (SER=Slot Error Rate, Mism=Mismatch,
Ins=Insertion, Del=Deletion, P=Precision,
R=Recall, F=F-measure)

achieve quite moderate results (F=0.628, see ta-
ble 2). We failed to identify correctly entities of
habitat (F=0.523) while results are higher for both
bacteria (F=0.785) and geographical (F=0.739).

Nevertheless, when annotating additional data,
we experienced harder work for habitat than for
bacteria or geographical. As a consequence, this
type of entities is complex for both human annota-
tors and automatic systems.

Categorization The rule-based approach we de-
signed to categorize entities in both the Onto-
Biotope ontology and the NCBI taxonomy is quite
simple. Since our named entity recognition sys-
tem obtained moderate results (overall F-measure
of 0.628, see table 2), on the categorization task,
we achieved better results on the gold standard an-
notations of entities (overall F-measure of 0.446)
than on predictions of entities made by our CRF
system (overall F-measure of 0.338, see table 3).

Since we failed to categorize more habitat than
bacteria, using default categorization values (see
section 4.3) led us to obtain lower precision val-
ues for habitat, on both cat+ner (Phab=0.482 vs.
Pbact=0.714) and cat (Phab=0.518 vs. Pbact=0.983)
tasks. Moreover, the lowest recall values are also
obtained on the categorization of habitat.

6.2 Error analysis

We give in figure 5 a sample of annotations per-
formed by our system on the development dataset
(event+ner task).

70



04/04/2016 18:44brat

Page 1 sur 1http://127.0.0.1:8001/index.xhtml#/BioNLP2016/sorties/BB-event+ner-15293611

Clonal strains of Pseudomonas aeruginosa in paediatric and adult cystic fibrosis units.

Despite recent reports of clonal strains of Pseudomonas aeruginosa in cystic fibrosis (CF) units, the need for routine microbiological surveillance remains 

contentious. Sputum was collected prospectively from productive patients attending the regional paediatric and adult CF units in Brisbane, Australia. 

All P. aeruginosa isolates were typed using pulsed-field gel electrophoresis. Spirometry, anthropometrics, hospitalisations and antibiotic sensitivity data 

were recorded. The first 100 sputum samples (first 50 patients at each clinic) harboured 163 isolates of P. aeruginosa. A total of 39 patients shared a 

common strain (pulsotype 2), 20 patients shared a strain with at least one other patient and 41 patients harboured unique strains. Eight patients shared a 

strain identical to a previously reported Australian transmissible strain (pulsotype 1). Compared with the unique strain group, patients harbouring 

pulsotype 2 were younger and had poorer lung function. Treatment requirements were similar in these two groups, as were the rates of multiresistance. 

In conclusion, 59% of patients harboured a clonal strain, supporting the need for routine microbiological surveillance. In contrast to previously described 
clonal strains, the dominant pulsotype was indistinguishable from nonclonal strains with respect to both colonial morphology and multiresistance. The 
clinical significance of clonal strains remains uncertain and requires longitudinal study.

Bacteria Habitat

Habitat
Lives_In Lives_In

Lives_In

Lives_In

Bacteria Habitat
Lives_In
Lives_In Lives_In

Habitat Habitat Habitat Geographical Geographical

Lives_In
Lives_In

Lives_In

Bacteria

Lives_In
Lives_In

Habitat Habitat

Habitat

Bacteria Habitat
Lives_InLives_In

Lives_In Lives_In
Lives_In
Lives_InLives_In

Habitat Habitat Habitat Habitat

Lives_In
Lives_In

Lives_In

Geographical Habitat

Habitat

Habitat

1

2

brat/BioNLP2016/sorties/BB-event+ner-15293611

Figure 5: Sample of entities and relations predicted by our system on the development dataset (event+ner
task). The first line is the title of the scientific text while other lines are part of the first paragraph

On the NER tasks, our system failed to iden-
tify acronyms (HMDM, HMDMs, PMN, PMNs),
and all discontinuous entities since we chose to not
process this kind of entity. False negatives mainly
concern habitats: (i) single entities (paediatric),
(ii) discontinuous entities (paediatric ... cystic fi-
brosis units, regional ... adult CF units, and re-
gional paediatric ... CF units), and (iii) frontiers
errors for which annotations depend on the context
(adult cystic fibrosis units vs. only adult cystic in
our sample, cystic fibrosis (CF) units vs. cystic, or
productive patients vs. patients).

On the categorization tasks, the main errors
concern all entities we failed to categorize and
for which we gave a default value. Those enti-
ties refer to adjectives the system did not process
(pulmonary, duodenal, etc.) and complex entities
(vacuum- and modified-atmosphere-packed cold-
smoked salmon stored at 5 degrees C, categorized
as “vacuum-packed meat” in the reference). As a
consequence, each category used as a default value
obtained bad results on the development dataset:
the NCBI taxonomy entry #210 achieved 34 true
positives and 69 false positives while the Onto-
Biotope entry #002216 achieved 14 true positives,
207 false positives and 11 false negatives.

At last, on the event identification tasks, since
there is only one type of relation to identify, the er-
rors concern missing relations and too much rela-
tions. Missing relations concern geographical en-
tities (cf. missing relations between P. aeruginosa
and geographical entities Brisbane and Australia
on figure 5): due to a low number of entities in
this category (see table 1), our SVM model failed
to learn relations with geographical entities. False
positives concern cases where the context between
entities prohibits relations (Neutrophils are resis-
tant to Yersinia), and annotations done on several

lines, including between the content of the title and
the content of the other paragraphs (cf. relations
between Pseudomonas aeruginosa from the first
paragraph and habitats adult cystic and adult from
the title).

7 Conclusion

In this paper, we presented the experiments we
made while participating in the Bacteria/Biotope
track from the 2016 BioNLP Shared-Task. We
combined CRF and post-processing rules to iden-
tify entities (bacteria, habitat, geographical), in-
cluding embedded entities, and we used rules
based on exact and partial match to normalize the
entities in the NCBI taxonomy (bacteria) and the
OntoBiotope ontology (habitat). For relation ex-
traction, we used a SVM system based on a basic
set of features.

As future work, we plan to deal with discontin-
uous entities. To process this issue, we consider
that a CRF model making the distinction between
the pivot and tokens specific to each entity would
be useful. As an example, in throat and nasopha-
ryngeal cultures, the pivot is cultures while spe-
cific tokens are throat and nasopharyngeal. Post-
processing rules would bring together tokens so as
to produce the final entities (throat cultures and
nasopharyngeal cultures). Our categorization ap-
proach to search for partial matches is relatively
simple. Future work is needed to provide a better
processing of the OntoBiotope ontology, namely,
in order to take into account the “is a” relations.

At last, we estimate that using unsupervised
learning of relations may provide interesting re-
sults, especially to improve the features set used
in the SVM model.

71



References

Sondes Bannour, Laurent Audibert, and Henry Sol-
dano. 2013. Ontology-based semantic annotation:
an automatic hybrid rule-based method. In BioNLP-
ST Work Proc, pages 139–43, Sofia, Bulgaria.

Jari Björne and Tapio Salakoski. 2013. TEES
2.1: Automated Annotation Scheme Learning in the
BioNLP 2013 Shared Task. In BioNLP-ST Work
Proc, pages 16–25, Sofia, Bulgaria.

Robert Bossy, Wiktoria Golik, Zorana Ratkovic, Di-
alekti Valsamou, Philippe Bessières, and Claire
Nédellec. 2015. Overview of the gene regu-
lation network and the bacteria biotope tasks in
BioNLP’13 shared task. BMC Bioinformatics,
16(Suppl 10):S1.

Robert Bossy, Claire Nédellec, Julien Jourde, and
Mouhammadou Ba, 2016. Guidelines for Annota-
tion of Bacteria Biotopes. INRA.

Peter F Brown, Vincent J Della Pietra, Peter V de
Souza, Jenifer C Lai, and Robert L Mercer. 1992.
Class-Based n-gram Models of Natural Language.
Computational Linguistics, 18(4):467–79.

Vincent Claveau. 2013. IRISA participation to
BioNLP-ST 2013: lazy-learning and information re-
trieval for information extraction tasks. In BioNLP-
ST Work Proc, pages 188–96, Sofia, Bulgaria.

Louise Deléger, Anne-Laure Ligozat, Cyril Grouin,
Pierre Zweigenbaum, and Aurélie Névéol. 2014.
Annotation of specialized corpora using a compre-
hensive entity and relation scheme. In Proc of
LREC, pages 1267–74, Reykjavik, Iceland.

Louise Deléger, Robert Bossy, Estelle Chaix,
Mouhamadou Ba, Arnaud Ferré, Philippe Bessières,
and Claire Nédellec. 2016. Overview of the Bac-
teria Biotope Task at BioNLP Shared Task 2016.
In Proceedings of the 4th BioNLP Shared Task
workshop, Berlin, Germany, August. Association
for Computational Linguistics.

Scott Federhen. 2002. The Taxonomy Project. In Jo-
hanna McEntyre and Jim Ostell, editors, The NCBI
Handbook, chapter 4. National Center for Biotech-
nology Information, Bethesda, MD, 2nd edition.

Cyril Grouin. 2013. Building A Contrasting Taxa Ex-
tractor for Relation Identification from Assertions:
BIOlogical Taxonomy & Ontology Phrase Extrac-
tion System. In BioNLP-ST Workshop Proc, pages
144–52, Sofia, Bulgaria.

Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. In B. Schölkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning, chapter 11, pages 169–
184. MIT Press, Cambridge, MA.

İlknur Karadeniz and Arzucan Özgür. 2013. Bacte-
ria Biotope Detection, Ontology-based Normaliza-
tion, and Relation Extraction using Syntactic Rules.
In BioNLP-ST Work Proc, pages 170–7, Sofia, Bul-
garia.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic models for segmenting and labeling
sequence data. In Proc of ICML, pages 282–9,
Williamstown, MA.

Thomas Lavergne, Olivier Cappé, and François Yvon.
2010. Practical Very Large Scale CRFs. In Proc of
ACL, pages 504–13, Uppsala, Sweden.

Thomas Lavergne, Cyril Grouin, and Pierre Zweigen-
baum. 2015. The contribution of co-reference reso-
lution to supervised relation detection between bac-
teria and biotopes entities. BMC Bioinformatics,
16(Suppl 10):S6.

Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master’s thesis, Massachusetts Insti-
tute of Technology.

Claire Nédellec. 2016. The OntoBiotope Ontology.
Institut National de la Recherche Agronomique.

Pontus Stenetorp, Sampo Pyysalo, Goran Topić,
Tomoko Ohta, Sophia Ananiadou, and Juni’chi Tsu-
jii. 2012. BRAT: a Web-based Tool for NLP-
Assisted Text Annotation. In Proc of EACL Demon-
strations, pages 102–7, Avignon, France. ACL.

Vladimir N Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag.

Karin Verspoor, Antonio Jimeno Yepes, Lawrence
Cavedon, Tara McIntosh, Asha Herten-Crabb, Zo
Thomas, and John-Paul Plazzer. 2013. Annotating
the Biomedical Literature for the Human Variome.
In Database: The Journal of Biological Databases
and Curation.

72


