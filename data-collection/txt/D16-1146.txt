



















































Lifted Rule Injection for Relation Embeddings


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1389–1399,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Lifted Rule Injection for Relation Embeddings

Thomas Demeester
Ghent University - iMinds

Ghent, Belgium
tdmeeste@intec.ugent.be

Tim Rocktäschel and Sebastian Riedel
University College London

London, UK
{t.rocktaschel,s.riedel}@cs.ucl.ac.uk

Abstract

Methods based on representation learning cur-
rently hold the state-of-the-art in many natural
language processing and knowledge base in-
ference tasks. Yet, a major challenge is how to
efficiently incorporate commonsense knowl-
edge into such models. A recent approach reg-
ularizes relation and entity representations by
propositionalization of first-order logic rules.
However, propositionalization does not scale
beyond domains with only few entities and
rules. In this paper we present a highly ef-
ficient method for incorporating implication
rules into distributed representations for au-
tomated knowledge base construction. We
map entity-tuple embeddings into an approxi-
mately Boolean space and encourage a partial
ordering over relation embeddings based on
implication rules mined from WordNet. Sur-
prisingly, we find that the strong restriction
of the entity-tuple embedding space does not
hurt the expressiveness of the model and even
acts as a regularizer that improves general-
ization. By incorporating few commonsense
rules, we achieve an increase of 2 percentage
points mean average precision over a matrix
factorization baseline, while observing a neg-
ligible increase in runtime.

1 Introduction

Current successful methods for automated knowl-
edge base construction tasks heavily rely on learned
distributed vector representations (Nickel et al.,
2012; Riedel et al., 2013; Socher et al., 2013; Chang
et al., 2014; Neelakantan et al., 2015; Toutanova et
al., 2015; Nickel et al., 2015; Verga et al., 2016;

Verga and McCallum, 2016). Although these mod-
els are able to learn robust representations from
large amounts of data, they often lack common-
sense knowledge. Such knowledge is rarely explic-
itly stated in texts but can be found in resources
like PPDB (Ganitkevitch et al., 2013) or WordNet
(Miller, 1995).

Combining neural methods with symbolic com-
monsense knowledge, for instance in the form of
implication rules, is in the focus of current research
(Rocktäschel et al., 2014; Wang et al., 2014; Bow-
man et al., 2015; Wang et al., 2015; Vendrov et al.,
2016; Hu et al., 2016; Rocktäschel and Riedel, 2016;
Cohen, 2016). A recent approach (Rocktäschel et
al., 2015) regularizes entity-tuple and relation em-
beddings via first-order logic rules. To this end, ev-
ery first-order rule is propositionalized based on ob-
served entity-tuples, and a differentiable loss term is
added for every propositional rule. This approach
does not scale beyond only a few entity-tuples and
rules. For example, propositionalizing the rule ∀x :
isMan(x)⇒ isMortal(x) would result in a very
large number of loss terms on a large database.

In this paper, we present a method to incorporate
simple rules while maintaining the computational
efficiency of only modeling training facts. This
is achieved by minimizing an upper bound of the
loss that encourages the implication between rela-
tions to hold, entirely independent from the num-
ber of entity pairs. It only involves representa-
tions of the relations that are mentioned in rules, as
well as a general rule-independent constraint on the
entity-tuple embedding space. In the example given
above, if we require that every component of the

1389



vector representation of isMan is smaller than the
corresponding component of relation isMortal,
then we can show that the rule holds for any non-
negative representation of an entity-tuple. Hence our
method avoids the need for separate loss terms for
every ground atom resulting from propositionaliz-
ing rules. In statistical relational learning this type
of approach is often referred to as lifted inference
or learning (Poole, 2003; Braz, 2007) because it
deals with groups of random variables at a first-order
level. In this sense our approach is a lifted form of
rule injection. This allows for imposing large num-
bers of rules while learning distributed representa-
tions of relations and entity-tuples. Besides drasti-
cally lower computation time, an important advan-
tage of our method over Rocktäschel et al. (2015) is
that when these constraints are satisfied, the injected
rules always hold, even for unseen but inferred facts.
While the method presented here only deals with im-
plications and not general first-order rules, it does
not rely on the assumption of independence between
relations, and is hence more generally applicable.

Our contributions are fourfold: (i) we develop a
very efficient way of regularizing relation represen-
tations to incorporate first-order logic implications
(§3), (ii) we reveal that, against expectation, map-
ping entity-tuple embeddings to non-negative space
does not hurt but instead improves the generaliza-
tion ability of our model (§5.1) (iii) we show im-
provements on a knowledge base completion task by
injecting mined commonsense rules from WordNet
(§5.3), and finally (iv) we give a qualitative analysis
of the results, demonstrating that implication con-
straints are indeed satisfied in an asymmetric way
and result in a substantially increased structuring of
the relation embedding space (§5.6).

2 Background

In this section we revisit the matrix factorization re-
lation extraction model by Riedel et al. (2013) and
introduce the notation used throughout the paper.
We choose the matrix factorization model for its
simplicity as the base on which we develop impli-
cation injection.

Riedel et al. (2013) represent every relation r ∈
R (selected from Freebase (Bollacker et al., 2008)
or extracted as textual surface pattern) by a k-

dimensional latent representation r ∈ Rk. A par-
ticular relation instance or fact is the combination
of a relation r and a tuple t of entities that are en-
gaged in that relation, and is written as 〈r, t〉. We
write O as the set of all such input facts available
for training. Furthermore, every entity-tuple t ∈ T
is represented by a latent vector t ∈ Rk (with T the
set of all entity-tuples in O).

Model F by Riedel et al. (2013) measures the
compatibility between a relation r and an entity-
tuple t using the dot product r>t of their respec-
tive vector representations. During training, the
representations are learned such that valid facts re-
ceive high scores, whereas negative ones receive low
scores. Typically no negative evidence is available
at training time, and therefore a Bayesian Personal-
ized Ranking (BPR) objective (Rendle et al., 2009)
is used. Given a pair of facts fp := 〈rp, tp〉 6∈ O and
fq := 〈rq, tq〉 ∈ O, this objective requires that

r>p tp ≤ r>q tq. (1)

The embeddings can be trained by minimizing a
convex loss function `R that penalizes violations
of that requirement when iterating over the training
set. In practice, each positive training fact 〈r, tq〉 is
compared with a randomly sampled unobserved fact
〈r, tp〉 for the same relation. The overall loss can
hence be written as

LR =
∑

〈r,tq〉∈O
tp∈T , 〈r,tp〉6∈O

`R
(
r>[tp − tq]

)
. (2)

and measures how well observed valid facts are
ranked above unobserved facts, thus reconstructing
the ranking of the training data. We will hence-
forth call LR the reconstruction loss, to make a dis-
tinction with the implication loss that we will intro-
duce later. Riedel et al. (2013) use the logistic loss
`R(s) := − log σ(−s), where σ(s) := (1 + e−x)−1
denotes the sigmoid function. In order to avoid over-
fitting, an L2 regularization term on the r and t em-
beddings is added to the reconstruction loss. The
overall objective to minimize hence is

LF = LR + α
(∑

r‖r‖22 +
∑

t‖t‖22
)

(3)

where α is the regularization strength.

1390



3 Lifted Injection of Implications

In this section, we show how an implication

∀t ∈ T : 〈rp, t〉 ⇒ 〈rq, t〉, (4)

can be imposed independently of the entity-tuples.
For simplicity, we abbreviate such implications as
rp ⇒ rq (e.g., professorAt⇒ employeeAt).

3.1 Grounded Loss Formulation
The implication rule can be imposed by requiring
that every tuple t ∈ T is at least as compatible with
relation rp as with rq. Written in terms of the latent
representations, eq. (4) therefore becomes

∀t ∈ T : r>p t ≤ r>q t (5)

If 〈rp, t〉 is a true fact with a high score r>p t, and
the fact 〈rq, t〉 has an even higher score, it must also
be true, but not vice versa. We can therefore inject
an implication rule by minimizing a loss term with
a separate contribution from every t ∈ T , adding
up to the total loss if the corresponding inequality
is not satisfied. In order to make the contribution of
every tuple t to that loss independent of the magni-
tude of the tuple embedding, we divide both sides of
the above inequality by ‖t‖1. With t̃ := t/‖t‖1, the
implication loss for the rule rp ⇒ rq can be written
as

LI =
∑

∀t∈T
`I
(
[rp − rq]>t̃

)
(6)

for an appropriate convex loss function `I , similarly
to eq. (2). In practice, the summation can be reduced
to those tuples that occur in combination with rp or
rq in the training data. Still, the propositionalization
in terms of training facts leads to a heavy computa-
tional cost for imposing a single implication, simi-
lar to the technique introduced in Rocktäschel et al.
(2015). Moreover, with that simplification there is
no guarantee that the implication between both re-
lations would generalize towards inferred facts not
seen during training.

3.2 Lifted Loss Formulation
The problems mentioned above can be avoided if
instead of LI , a tuple-independent upper bound is
minimized. Such a bound can be constructed, pro-
vided all components of t are restricted to a non-
negative embedding space, i.e., T ⊆ Rk,+. If this

holds, Jensen’s inequality allows us to transform
eq. (6) as follows

LI =
∑

∀t∈T
`I

( k∑

i=1

t̃i [rp − rq]>1i
)

(7)

≤
k∑

i=1

`I
(
[rp − rq]>1i

) ∑

∀t∈T
t̃i (8)

where 1i is the unit vector along dimension i in
tuple-space. This is allowed because the {t̃i}ki=1
form convex coefficients (t̃i > 0, and

∑
i t̃i = 1),

and `I is a convex function. If we define

LUI :=
k∑

i=1

`I
(
[rp − rq]>1i

)
(9)

we can write
LI ≤ βLUI (10)

in which β is an upper bound on
∑

t t̃i. One such
bound is |T |, but others are conceivable too. In prac-
tice we rescale β to a hyper-parameter β̃ that we use
to control the impact of the upper bound to the over-
all loss. We call LUI the lifted loss, as it no longer
depends on any of the entity-tuples; it is grounded
over the unit tuples 1i instead.

The implication rp ⇒ rq can thus be imposed by
minimizing the lifted loss LUI . Note that by mini-
mizing LUI , the model is encouraged to satisfy the
constraint rp ≤ rq on the relation embeddings,
where ≤ denotes the component-wise comparison.
In fact, a sufficient condition for eq. (5) to hold, is

rp ≤ rq and ∀t ∈ T : t ≥ 0 (11)

with 0 the k-dimensional null vector. This corre-
sponds to a single relation-specific loss term, and
the general restriction T ⊆ Rk,+ on the tuple-
embedding space.

3.3 Approximately Boolean Entity Tuples

In order to impose implications by minimizing a
lifted loss LUI , the tuple-embedding space needs to
be restricted to Rk,+. We have chosen to restrict the
tuple space even more than required, namely to the
hypercube t ∈ [0, 1]k, as approximately Boolean
embeddings (Kruszewski et al., 2015). The tuple

1391



embeddings are constructed from real-valued vec-
tors e, using the component-wise sigmoid function

t = σ(e), e ∈ Rk. (12)

For minimizing the loss, the gradients are hence
computed with respect to e, and the L2 regulariza-
tion is applied to the components of e instead of t.

Other choices for ensuring the restriction t ≥ 0
in eq. (11) are possible, but we found that our ap-
proach works better in practice than those (e.g., the
exponential transformation proposed by Demeester
et al. (2016)). It can also be observed that the unit
tuples over which the implication loss is grounded,
form a special case of approximately Boolean em-
beddings.

In order to investigate the impact of this restric-
tion even when not injecting any rules, we introduce
model FS: the original model F, but with sigmoidal
entity-tuples:

LFS =
∑

〈r,tq〉∈O
tp∈T , 〈r,tp〉6∈O

`R
(
r>[σ(ep)− σ(eq)]

)

+ α
(∑

r‖r‖22 +
∑

e‖e‖22
)

(13)

Here, ep and eq are the real-valued representations
as in eq. (12), for tuples tp and tq, respectively.

With the above choice of a non-negative tuple-
embedding space we can now state the full lifted rule
injection model (FSL):

LFSL = LFS + β̃
∑

I∈I
LUI (14)

LUI denotes a lifted loss term for every rule in a set
I of implication rules that we want to inject.

3.4 Convex Implication Loss
The logistic loss `R (see §2) is not suited for im-
posing implications because once the inequality in
eq. (11) is satisfied, the components of rp and rq do
not need to be separated any further. However, with
`R this would continue to happen due to the small
non-zero gradient. In the reconstruction loss LR
this is a desirable effect which further separates the
scores for positive from negative examples. How-
ever, if an implication is imposed between two re-
lations that are almost equivalent according to the

training data, we still want to find almost equivalent
embedding vectors. Hence, we propose to use the
loss

`I(s) = max(0, s+ δ) (15)

with δ a small positive margin to ensure that the gra-
dient does not disappear before the inequality is ac-
tually satisfied. We use δ = 0.01 in all experiments.

The main advantage of the presented approach
over earlier methods that impose the rules in a
grounded way (Rocktäschel et al., 2015; Wang et
al., 2015) is the computational efficiency of impos-
ing the lifted loss. Evaluating LUI or its gradient for
one implication rule is comparable to evaluating the
reconstruction loss for one pair of training facts. In
typical applications there are much fewer rules than
training facts and the extra computation time needed
to inject these rules is therefore negligible.

4 Related Work

Recent research on combining rules with learned
vector representations has been important for new
developments in the field of knowledge base com-
pletion. Rocktäschel et al. (2014) and Rocktäschel
et al. (2015) provided a framework to jointly maxi-
mize the probability of observed facts and proposi-
tionalized first-order logic rules. Wang et al. (2015)
demonstrated how different types of rules can be
incorporated using an Integer Linear Programming
approach. Wang and Cohen (2016) learned em-
beddings for facts and first-order logic rules using
matrix factorization. Yet, all of these approaches
ground the rules in the training data, limiting their
scalability towards large rule sets and KBs with
many entities. As argued in the introduction, this
forms an important motivation for the lifted rule in-
jection model put forward in this work, which by
construction does not suffer from that limitation.
Wei et al. (2015) proposed an alternative strategy to
tackle the scalability problem by reasoning on a fil-
tered subset of grounded facts.

Wu et al. (2015) proposed to use a path ranking
approach for capturing long-range interactions be-
tween entities, and to add these as an extra loss term,
besides the loss that models pairwise relations. Our
model FSL differs substantially from their approach,
in that we consider tuples instead of separate enti-
ties, and we inject a given set of rules. Yet, by cre-

1392



ating a partial ordering in the relation embeddings
as a result of injecting implication rules, model FSL
can also capture interactions beyond direct relations.
This will be demonstrated in §5.3 by injecting rules
between surface patterns only and still measuring an
improvement on predictions for structured Freebase
relations.

Combining logic and distributed representations
is also an active field of research outside of au-
tomated knowledge base completion. Recent ad-
vances include the work by Faruqui et al. (2014),
who injected ontological knowledge from WordNet
into word representations. Furthermore, Vendrov et
al. (2016) proposed to enforce a partial ordering in
an embeddings space of images and phrases. Our
method is related to such order embeddings since
we define a partial ordering on relation embeddings.
However, to ensure that implications hold for all
entity-tuples we also need a restriction on the entity-
tuple embedding space and derive bounds on the
loss. Another important contribution is the recent
work by Hu et al. (2016), who proposed a frame-
work for injecting rules into general neural network
architectures, by jointly training on the actual targets
and on the rule-regularized predictions provided by
a teacher network. Although quite different at first
sight, their work could offer a way to use our model
in various neural network architectures, by integrat-
ing the proposed lifted loss into the teacher network.

This paper builds upon our previous workshop
paper (Demeester et al., 2016). In that work,
we tested different tuple embedding transforma-
tions in an ad-hoc manner. We used approxi-
mately Boolean representations of relations instead
of entity-tuples, strongly reducing the model’s de-
grees of freedom. We now derive the FSL model
from a carefully considered mathematical transfor-
mation of the grounded loss. The FSL model only
restricts the tuple embedding space, whereby rela-
tion vectors remain real valued. Furthermore, previ-
ous experiments were performed on small-scale ar-
tificial datasets, whereas we now test on a real-world
relation extraction benchmark.

Finally, we explicitly discuss the main differ-
ences with respect to the strongly related work from
Rocktäschel et al. (2015). Their method is more gen-
eral, as they cover a wide range of first-order logic
rules, whereas we only discuss implications. Lifted

rule injection beyond implications will be studied in
future research contributions. However, albeit less
general, our model has a number of clear advan-
tages:

Scalability – Our proposed model of lifted rule
injection scales according to the number of implica-
tion rules, instead of the number of rules times the
number of observed facts for every relation present
in a rule.

Generalizability – Injected implications will
hold even for facts not seen during training, because
their validity only depends on the order relation im-
posed on the relation representations. This is not
guaranteed when training on rules grounded in train-
ing facts by Rocktäschel et al. (2015).

Training Flexibility – Our method can be trained
with various loss functions, including the rank-based
loss as used in Riedel et al. (2013). This was not
possible for the model of Rocktäschel et al. (2015)
and already leads to an improved accuracy as seen
from the zero-shot learning experiment in §5.2.

Independence Assumption – In Rocktäschel et
al. (2015) an implication of the form ap ⇒ aq for
two ground atoms ap and aq is modeled by the log-
ical equivalence ¬(ap ∧ ¬aq), and its probability
is approximated in terms of the elementary proba-
bilities π(ap) and π(aq) as 1 − π(ap)

(
1 − π(aq)

)
.

This assumes the independence of the two atoms ap
and aq, which may not hold in practice. Our ap-
proach does not rely on that assumption and also
works for cases of statistical dependence. For ex-
ample, the independence assumption does not hold
in the trivial case where the relations rp and rq in
the two atoms are equivalent, whereas in our model,
the constraints rp ≤ rq and rp ≥ rq would simply
reduce to rp = rq.

5 Experiments and Results

We now present our experimental results. We start
by describing the experimental setup and hyperpa-
rameters. Before turning to the injection of rules,
we compare model F with model FS, and show that
restricting the tuple embedding space has a regu-
larization effect, rather than limiting the expressive-
ness of the model (§5.1). We then demonstrate that
model FSL is capable of zero-shot learning (§5.2),
and show that injecting high-quality WordNet rules

1393



Test relation # R13-F F FS FSL

person/company 106 0.75 0.73 0.74 0.77
location/containedby 73 0.69 0.62 0.70 0.71
person/nationality 28 0.19 0.20 0.20 0.21
author/works written 27 0.65 0.71 0.69 0.65
person/place of birth 21 0.72 0.69 0.72 0.70
parent/child 19 0.76 0.77 0.81 0.85
person/place of death 19 0.83 0.85 0.83 0.85
neighborhood/neighborhood of 11 0.70 0.67 0.63 0.62
person/parents 6 0.61 0.53 0.66 0.66
company/founders 4 0.77 0.73 0.64 0.67
sports team/league 4 0.59 0.44 0.43 0.56
team owner/teams owned 2 0.38 0.64 0.64 0.61
team/arena stadium 2 0.13 0.13 0.13 0.12
film/directed by 2 0.50 0.18 0.17 0.13
broadcast/area served 2 0.58 0.83 0.83 1.00
structure/architect 2 1.00 1.00 1.00 1.00
composer/compositions 2 0.67 0.64 0.51 0.50
person/religion 1 1.00 1.00 1.00 1.00
film/produced by 1 0.50 1.00 1.00 0.33

Weighted MAP 0.67 0.65 0.67 0.69

Table 1: Weighted mean average precision for our
reimplementation of the matrix factorization model
(F) compared to restricting the entity-pair space (FS)
and injecting WordNet rules (FSL). Model F results
by Riedel et al. (2013) are denoted as R13-F.

leads to an improved precision (§5.3). We proceed
with a visual illustration of the relation embeddings
with and without injected rules (§5.4), provide de-
tails on time efficiency of the lifted rule injection
method (§5.5), and show that it correctly captures
the asymmetry of implication rules (§5.6).

All models were implemented in Tensor-
Flow (Abadi et al., 2015). We use the hyperparam-
eters of Riedel et al. (2013), with k = 100 hidden
dimensions and a weight of α = 0.01 for the L2
regularization loss. We use ADAM (Kingma and
Ba, 2014) for optimization with an initial learning
rate of 0.005 and a mini-batch size of 8192. The
embeddings are initialized by sampling uniformly
from [−0.1, 0.1] and we use β̃ = 0.1 for the
implication loss throughout our experiments.

5.1 Restricted Embedding Space

Before incorporating external commonsense knowl-
edge into relation representations, we were curious
how much we lose by restricting the entity-tuple
space to approximately Boolean embeddings. We
evaluate our models on the New York Times dataset
introduced by Riedel et al. (2013). Surprisingly, we
find that the expressiveness of the model does not

suffer from this strong restriction. From Table 1 we
see that restricting the tuple-embedding space seems
to perform slightly better (FS) as opposed to a real-
valued tuple-embedding space (F), suggesting that
this restriction has a regularization effect that im-
proves generalization. We also provide the original
results for model F by Riedel et al. (2013) (denoted
as R13-F) for comparison. Due to a different im-
plementation and optimization procedure, the results
for our model F and R13-F are not identical.

Inspecting the top relations for a sampled dimen-
sion in the embedding space reveals that the rela-
tion space of model FS more closely resembles clus-
ters than that of model F (Table 2). We hypothesize
that this might be caused by approximately Boolean
entity-tuple representations in model FS, resulting in
attribute-like entity-tuple vectors that capture which
relation clusters they belong to.

5.2 Zero-shot Learning
The zero-shot learning experiment performed in
Rocktäschel et al. (2015) leads to an important find-
ing: when injecting implications with right-hand
sides for Freebase relations for which no or very lim-
ited training facts are available, the model should be
able to infer the validity of Freebase facts for those
relations based on rules and correlations between
textual surface patterns.

We inject the same hand-picked relations as used
by Rocktäschel et al. (2015), after removing all
Freebase training facts. The lifted rule injection
(model FSL) reaches a weighted MAP of 0.35,
comparable with 0.38 by the Joint model from
Rocktäschel et al. (2015) (denoted R15-Joint). Note
that for this experiment we initialized the Freebase
relations implied by the rules with negative random
vectors (sampled uniformly from [−7.9,−8.1]). The
reason is that without any negative training facts for
these relations, their components can only go up due
to the implication loss, and we do not want to get
values that are too high before optimization.

Figure 1 shows how the relation extraction perfor-
mance improves when more Freebase relation train-
ing facts are added. It effictively measures how
well the proposed models, matrix factorization (F),
propositionalized rule injection (R15-Joint), and our
model (FSL), can make use of the provided rules
and correlations between textual surface form pat-

1394



Table 2: Top patterns for a randomly sampled dimension in non-restricted and restricted embedding space .

Model F (non-restricted) Model FS (restricted)

nsubj<-represent->dobj rcmod->return->prep->to->pobj
appos->member->prep->of->pobj->team->nn nn<-return->prep->to->pobj
nsubj<-die->dobj nsubj<-return->prep->to->pobj
nsubj<-speak->prep->about->pobj rcmod->leave->dobj
appos->champion->poss nsubj<-quit->dobj

0.0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
Fraction of Freebase Training Facts

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

w
M

A
P

Figure 1: Weighted MAP for injecting hand-picked
rules as a function of the fraction of Freebase train-
ing facts. Comparison between model F (lowest, in
blue), R15-Joint (middle, in green) and model FSL
(highest, in red).

terns and increased fractions of Freebase training
facts. Although FSL starts at a lower performance
than R15-Joint when no Freebase training facts are
present, it outperforms R15-Joint and a plain matrix
factorization model by a substantial margin when
provided with more than 7.5% of Freebase train-
ing facts. This indicates that, in addition to being
much faster than R15-Joint, it can make better use
of provided rules and few training facts. We at-
tribute this to the Bayesian personalized ranking loss
instead of the logistic loss used in Rocktäschel et
al. (2015). The former is compatible with our rule-
injection method, but not with the approach of max-
imizing the expectation of propositional rules used
by R15-Joint.

5.3 Injecting Knowledge from WordNet

The main purpose of this work is to be able to
incorporate rules from external resources for aid-

ing relation extraction. We use WordNet hyper-
nyms to generate rules for the NYT dataset. To
this end we iterate over all surface form patterns
in the dataset and attempt to replace words in
the pattern by their hypernyms. If the result-
ing pattern is contained in the dataset, we gen-
erate the corresponding rule. For instance, we
generate a rule appos->diplomat->amod ⇒
appos->official->amod since both patterns
are contained in the NYT dataset and we know from
WordNet that a diplomat is an official. This leads to
427 rules from WordNet that we subsequently anno-
tate manually to obtain 36 high-quality rules. Note
that none of these rules directly imply a Freebase re-
lation. Although the test relations all originate from
Freebase, we still hope to see improvements by tran-
sitive effects, i.e., better surface form representations
that in turn help to predict Freebase facts.

We show results obtained by injecting these
WordNet rules in Table 1 (column FSL). The
weighted MAP measure increases by 2% with
respect to model FS, and 4% compared to our reim-
plementation of the matrix factorization model F.
This demonstrates that imposing a partial ordering
based on implication rules can be used to incorpo-
rate logical commonsense knowledge and increase
the quality of information extraction systems. Note
that our evaluation setting guarantees that only
indirect effects of the rules are measured, i.e., we
do not use any rules directly implying test relations.
This shows that injecting such rules influences
the relation embedding space beyond only the
relations explicitly stated in the rules. For example,
injecting the rule appos<-father->appos
⇒ poss<-parent->appos can contribute
to improved predictions for the test relation
parent/child.

1395



(a) (b)

Figure 2: Visualization of embeddings (columns) for
the relations that appear in the high-quality Word-
Net rules, (a) without and (b) with injection of these
rules. Values range from -1 (orange) via 0 (white) to
1 (purple). Best viewed in color.

5.4 Visualizing Relation Embeddings

We provide a visual inspection of how the structure
of the relation embedding space changes when rules
are imposed. We select all relations involved in the
WordNet rules, and gather them as columns in a sin-
gle matrix, sorted by increasing `1 norm (values in
the 100 dimensions are similarly sorted). Figures 2a
and 2b show the difference between model F (with-
out injected rules) and FSL (with rules). The val-
ues of the embeddings in model FSL are more po-
larized, i.e., we observe stronger negative or posi-
tive components than for model F. Furthermore, FSL
also reveals a clearer difference between the left-
most (mostly negative, more specific) and right-most
(predominantly positive, more general) embeddings
(i.e., a clearer separation between positive and nega-
tive values in the plot), which results from imposing
the order relation in eq. (11) when injecting implica-
tions.

5.5 Efficiency of Lifted Injection of Rules

In order to get an idea of the time efficiency of in-
jecting rules, we measure the time per epoch when
restricting the program execution to a single 2.4GHz
CPU core. We measure on average 6.33s per epoch
without rules (model FS), against 6.76s and 6.97s

when injecting the 36 high-quality WordNet rules
and the unfiltered 427 rules (model FSL), respec-
tively. Increasing the amount of injected rules from
36 to 427 leads to an increase of only 3% in compu-
tation time, even though in our setup all rule losses
are used in every training batch. This confirms the
high efficiency of our lifted rule injection method.

5.6 Asymmetric Character of Implications

In order to demonstrate that injecting implications
conserves their asymmetric nature, we perform the
following experiment. After incorporating high-
quality Wordnet rules rp ⇒ rq into model FSL we
select all of the tuples tp that occur with relation rp
in a training fact 〈rp, tp〉. Matching these with re-
lation rq should result in high values for the scores
r>q tp, if the implication holds. If however the tuples
tq are selected from the training facts 〈rq, tq〉, and
matched with relation rp, the scores r>p tq should
be much lower if the inverse implication does not
hold (in other words, if rq and rp are not equiva-
lent). Table 3 lists the averaged results for 5 example
rules, and the average over all relations in WordNet
rules, both for the case with injected rules (model
FSL), and without rules (model FS). For easier com-
parison, the scores are mapped to the unit interval
via the sigmoid function. This quantity σ(r>t) is
often interpreted as the probability that the corre-
sponding fact holds (Riedel et al., 2013), but be-
cause of the BPR-based training, only differences
between scores play a role here. After injecting
rules, the average scores of facts inferred by these
rules (i.e., column σ(r>q tp) for model FSL) are al-
ways higher than for facts (incorrectly) inferred by
the inverse rules (column σ(r>p tq) for model FSL).
In the fourth example, the inverse rule leads to high
scores as well (on average 0.79, vs. 0.98 for the ac-
tual rule). This is due to the fact that the daily and
newspaper relations are more or less equivalent,
such that the components of rp are not much below
those of rq. For the last example (the ambassador
⇒ diplomat rule), the asymmetry in the implica-
tion is maintained, although the absolute scores are
rather low for these two relations.

The results for model FS reflect how strongly the
implications in either direction are latently present
in the training data. We can only conclude that
model FS manages to capture the similarity be-

1396



rule model FSL model FS
rp ⇒ rq σ(r>q tp) σ(r>p tq) σ(r>q tp) σ(r>p tq)

appos->party->amod ⇒ appos->organization->amod 0.99 0.22 0.70 0.86
poss<-father->appos ⇒ poss<-parent->appos 0.96 0.00 0.72 0.89
appos->prosecutor->nn ⇒ appos->lawyer->nn 0.99 0.01 0.87 0.80
appos->daily->amod ⇒ appos->newspaper->amod 0.98 0.79 0.90 0.86
appos->ambassador->amod ⇒ appos->diplomat->amod 0.31 0.05 0.93 0.84

average over 36 high-quality Wordnet rules 0.95 0.28 0.74 0.70

Table 3: Average of σ(r>q t) over all inferred facts 〈rq, tp〉 for tuples tp from training items for relation rp,
and vice versa, for Wordnet implications rp ⇒ rq, and model FSL (injected rules) vs. model FS (no rules).

tween relations, but not the asymmetric character
of implications. For example, purely based on the
training data, it appears to be more likely that the
parent relation implies the father relation, than
vice versa. This again demonstrates the importance
and added value of injecting external rules capturing
commonsense knowledge.

6 Conclusions

We presented a novel, fast approach for incorporat-
ing first-order implication rules into distributed rep-
resentations of relations. We termed our approach
‘lifted rule injection’, as it avoids the costly ground-
ing of first-order implication rules and is thus inde-
pendent of the size of the domain of entities. By
construction, these rules are satisfied for any ob-
served or unobserved fact. The presented approach
requires a restriction on the entity-tuple embedding
space. However, experiments on a real-world dataset
show that this does not impair the expressiveness of
the learned representations. On the contrary, it ap-
pears to have a beneficial regularization effect.

By incorporating rules generated from WordNet
hypernyms, our model improved over a matrix fac-
torization baseline for knowledge base completion.
Especially for domains where annotation is costly
and only small amounts of training facts are avail-
able, our approach provides a way to leverage exter-
nal knowledge sources for inferring facts.

In future work, we want to extend the proposed
ideas beyond implications towards general first-
order logic rules. We believe that supporting con-
junctions, disjunctions and negations would enable
to debug and improve representation learning based
knowledge base completion. Furthermore, we want
to integrate these ideas into neural methods beyond
matrix factorization approaches.

Acknowledgments

This work was supported by the Research Founda-
tion - Flanders (FWO), Ghent University - iMinds,
Microsoft Research through its PhD Scholarship
Programme, an Allen Distinguished Investigator
Award, and a Marie Curie Career Integration Award.

References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene

Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado,
Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay
Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey
Irving, Michael Isard, Yangqing Jia, Rafal Jozefow-
icz, Lukasz Kaiser, Manjunath Kudlur, Josh Leven-
berg, Dan Mané, Rajat Monga, Sherry Moore, Derek
Murray, Chris Olah, Mike Schuster, Jonathon Shlens,
Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul
Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fer-
nanda Viégas, Oriol Vinyals, Pete Warden, Martin
Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang
Zheng. 2015. TensorFlow: Large-scale machine
learning on heterogeneous systems. Software avail-
able from tensorflow.org.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management of
data, pages 1247–1250. ACM.

Samuel R Bowman, Christopher Potts, and Christopher D
Manning. 2015. Recursive neural networks can learn
logical semantics. In Proceedings of the 3rd Workshop
on Continuous Vector Space Models and their Compo-
sitionality (CVSC).

Rodrigo De Salvo Braz. 2007. Lifted First-order Proba-
bilistic Inference. Ph.D. thesis, Champaign, IL, USA.
AAI3290183.

Kai-Wei Chang, Wen-tau Yih, Bishan Yang, and Christo-
pher Meek. 2014. Typed tensor decomposition of

1397



knowledge bases for relation extraction. In EMNLP,
pages 1568–1579.

William. W. Cohen. 2016. TensorLog: A Differentiable
Deductive Database. ArXiv e-prints, May.

Thomas Demeester, Tim Rocktäschel, and Sebastian
Riedel. 2016. Regularizing relation representations
by first-order implications. In NAACL Workshop on
Automated Knowledge Base Construction (AKBC).

Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris
Dyer, Eduard Hovy, and Noah A Smith. 2014.
Retrofitting word vectors to semantic lexicons. arXiv
preprint arXiv:1411.4166.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. Ppdb: The paraphrase
database. In Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics (HLT-NAACL), pages 758–764.

Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard
Hovy, and Eric Xing. 2016. Harnessing deep
neural networks with logic rules. arXiv preprint
arXiv:1603.06318.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

German Kruszewski, Denis Paperno, and Marco Baroni.
2015. Deriving boolean structures from distributional
vectors. Transactions of the Association for Computa-
tional Linguistics, 3:375–388.

George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39–41.

Arvind Neelakantan, Benjamin Roth, and Andrew Mc-
Callum. 2015. Compositional vector space mod-
els for knowledge base completion. arXiv preprint
arXiv:1504.06662.

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing yago: scalable machine
learning for linked data. In Proceedings of the 21st
international conference on World Wide Web, pages
271–280. ACM.

Maximilian Nickel, Kevin Murphy, Volker Tresp, and
Evgeniy Gabrilovich. 2015. A review of relational
machine learning for knowledge graphs: From multi-
relational link prediction to automated knowledge
graph construction. arXiv preprint arXiv:1503.00759.

David Poole. 2003. First-order probabilistic inference.
In Proceedings of the 18th International Joint Confer-
ence on Artificial Intelligence (IJCAI), pages 985–991,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.

Steffen Rendle, Christoph Freudenthaler, Zeno Gantner,
and Lars Schmidt-Thieme. 2009. BPR: Bayesian per-
sonalized ranking from implicit feedback. In Proceed-
ings of the Twenty-Fifth Conference on Uncertainty in

Artificial Intelligence (UAI), pages 452–461, Arling-
ton, Virginia, United States. AUAI Press.

Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL), pages 74–84.

Tim Rocktäschel and Sebastian Riedel. 2016. Learn-
ing knowledge base inference with neural theorem
provers. In NAACL Workshop on Automated Knowl-
edge Base Construction (AKBC).

Tim Rocktäschel, Matko Bosnjak, Sameer Singh, and Se-
bastian Riedel. 2014. Low-dimensional embeddings
of logic. In ACL Workshop on Semantic Parsing.

Tim Rocktäschel, Sameer Singh, and Sebastian Riedel.
2015. Injecting Logical Background Knowledge into
Embeddings for Relation Extraction. In Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL).

Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
sor networks for knowledge base completion. In
Advances in Neural Information Processing Systems
(NIPS).

Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-
fung Poon, Pallavi Choudhury, and Michael Gamon.
2015. Representing text for joint embedding of text
and knowledge bases. In EMNLP.

Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Ur-
tasun. 2016. Order-embeddings of images and lan-
guage. arXiv preprint, abs/1511.06361.

Patrick Verga and Andrew McCallum. 2016. Row-less
universal schema. In NAACL Workshop on Automated
Knowledge Base Construction (AKBC).

Patrick Verga, David Belanger, Emma Strubell, Ben-
jamin Roth, and Andrew McCallum. 2016. Multilin-
gual relation extraction using compositional universal
schema. In Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics (HLT-NAACL), pages 886–896. ACL.

William Yang Wang and William W. Cohen. 2016.
Learning first-order logic embeddings via matrix fac-
torization. In Proceedings of the 25th International
Joint Conference on Artificial Intelligence (IJCAI
2015), New York, NY, July. AAAI.

William Yang Wang, Kathryn Mazaitis, and William W
Cohen. 2014. Structure learning via parameter learn-
ing. In Proceedings of the 23rd ACM International
Conference on Conference on Information and Knowl-
edge Management, pages 1199–1208. ACM.

Quan Wang, Bin Wang, and Li Guo. 2015. Knowledge
base completion using embeddings and rules. In Pro-

1398



ceedings of the 24th International Conference on Ar-
tificial Intelligence (IJCAI), pages 1859–1865. AAAI
Press.

Zhuoyu Wei, Jun Zhao, Kang Liu, Zhenyu Qi, Zhengya
Sun, and Guanhua Tian. 2015. Large-scale knowl-
edge base completion: Inferring via grounding net-
work sampling over selected instances. In Proceed-
ings of the 24th ACM International on Conference
on Information and Knowledge Management (CIKM),
pages 1331–1340. ACM.

Fei Wu, Jun Song, Yi Yang, Xi Li, Zhongfei Zhang,
and Yueting Zhuang. 2015. Structured embedding
via pairwise relations and long-range interactions in
knowledge base. In AAAI Conference on Artificial In-
telligence, pages 1663–1670.

1399


