











































Adversarially Regularising Neural NLI Models to Integrate Logical Background Knowledge


Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 65–74
Brussels, Belgium, October 31 - November 1, 2018. c©2018 Association for Computational Linguistics

65

Adversarially Regularising Neural NLI Models
to Integrate Logical Background Knowledge

Pasquale Minervini
University College London

p.minervini@cs.ucl.ac.uk

Sebastian Riedel
University College London

s.riedel@cs.ucl.ac.uk

Abstract

Adversarial examples are inputs to machine
learning models designed to cause the model
to make a mistake. They are useful for under-
standing the shortcomings of machine learn-
ing models, interpreting their results, and for
regularisation. In NLP, however, most ex-
ample generation strategies produce input text
by using known, pre-specified semantic trans-
formations, requiring significant manual ef-
fort and in-depth understanding of the prob-
lem and domain. In this paper, we investi-
gate the problem of automatically generating
adversarial examples that violate a set of given
First-Order Logic constraints in Natural Lan-
guage Inference (NLI). We reduce the prob-
lem of identifying such adversarial examples
to a combinatorial optimisation problem, by
maximising a quantity measuring the degree
of violation of such constraints and by using a
language model for generating linguistically-
plausible examples. Furthermore, we propose
a method for adversarially regularising neu-
ral NLI models for incorporating background
knowledge. Our results show that, while the
proposed method does not always improve
results on the SNLI and MultiNLI datasets,
it significantly and consistently increases the
predictive accuracy on adversarially-crafted
datasets – up to a 79.6% relative improve-
ment – while drastically reducing the num-
ber of background knowledge violations. Fur-
thermore, we show that adversarial examples
transfer among model architectures, and that
the proposed adversarial training procedure
improves the robustness of NLI models to ad-
versarial examples.

1 Introduction

An open problem in Artificial Intelligence is quan-
tifying the extent to which algorithms exhibit in-
telligent behaviour (Levesque, 2014). In Machine
Learning, a standard procedure consists in esti-

mating the generalisation error, i.e. the predic-
tion error over an independent test sample (Hastie
et al., 2001). However, machine learning models
can succeed simply by recognising patterns that
happen to be predictive on instances in the test
sample, while ignoring deeper phenomena (Rimell
and Clark, 2009; Paperno et al., 2016).

Adversarial examples are inputs to machine
learning models designed to cause the model to
make a mistake (Szegedy et al., 2014; Goodfel-
low et al., 2014). In Natural Language Processing
(NLP) and Machine Reading, generating adversar-
ial examples can be really useful for understanding
the shortcomings of NLP models (Jia and Liang,
2017; Kannan and Vinyals, 2017) and for regular-
isation (Minervini et al., 2017).

In this paper, we focus on the problem of gener-
ating adversarial examples for Natural Language
Inference (NLI) models in order to gain insights
about the inner workings of such systems, and reg-
ularising them. NLI, also referred to as Recog-
nising Textual Entailment (Fyodorov et al., 2000;
Condoravdi et al., 2003; Dagan et al., 2005), is a
central problem in language understanding (Katz,
1972; Bos and Markert, 2005; van Benthem, 2008;
MacCartney and Manning, 2009), and thus it is es-
pecially well suited to serve as a benchmark task
for research in machine reading. In NLI, a model
is presented with two sentences, a premise p and a
hypothesis h, and the goal is to determine whether
p semantically entails h.

The problem of acquiring large amounts of la-
belled data for NLI was addressed with the cre-
ation of the SNLI (Bowman et al., 2015) and
MultiNLI (Williams et al., 2017) datasets. In
these processes, annotators were presented with
a premise p drawn from a corpus, and were re-
quired to generate three new sentences (hypothe-
ses) based on p, according to the following crite-
ria: a) Entailment – h is definitely true given p (p



66

entails h); b) Contradiction – h is definitely not
true given p (p contradicts h); and c) Neutral – h
might be true given p. Given a premise-hypothesis
sentence pair (p, h), a NLI model is asked to clas-
sify the relationship between p and h – i.e. either
entailment, contradiction, or neutral. Solving NLI
requires to fully capture the sentence meaning by
handling complex linguistic phenomena like lexi-
cal entailment, quantification, co-reference, tense,
belief, modality, and lexical and syntactic ambigu-
ities (Williams et al., 2017).

In this work, we use adversarial examples for:
a) identifying cases where models violate existing
background knowledge, expressed in the form of
logic rules, and b) training models that are robust
to such violations.

The underlying idea in our work is that NLI
models should adhere to a set of structural con-
straints that are intrinsic to the human reasoning
process. For instance, contradiction is inherently
symmetric: if a sentence p contradicts a sentence
h, then h contradicts p as well. Similarly, entail-
ment is both reflexive and transitive. It is reflexive
since a sentence a is always entailed by (i.e. is true
given) a. It is also transitive, since if a is entailed
by b, and b is entailed by c, then a is entailed by c
as well.

Example 1 (Inconsistency). Consider three sen-
tences a, b and c each describing a situation, such
as: a) “The girl plays”, b) “The girl plays with a
ball”, and c) “The girl plays with a red ball”. Note
that if a is entailed by b, and b is entailed by c, then
also a is entailed by c. If a NLI model detects that
b entails a, c entails b, but c does not entail a, we
know that it is making an error (since its results are
inconsistent), even though we may not be aware of
the sentences a, b, and c and the true semantic re-
lationships holding between them. 4

Our adversarial examples are different from
those used in other fields such as computer vi-
sion, where they typically consist in small, seman-
tically invariant perturbations that result in dras-
tic changes in the model predictions. In this pa-
per, we propose a method for generating adver-
sarial examples that cause a model to violate pre-
existing background knowledge (Section 4), based
on reducing the generation problem to a combina-
torial optimisation problem. Furthermore, we out-
line a method for incorporating such background
knowledge into models by means of an adversar-
ial training procedure (Section 5).

Our results (Section 8) show that, even though
the proposed adversarial training procedure does
not sensibly improve accuracy on SNLI and
MultiNLI, it yields significant relative improve-
ment in accuracy (up to 79.6%) on adversarial
datasets. Furthermore, we show that adversarial
examples transfer across models, and that the pro-
posed method allows training significantly more
robust NLI models.

2 Background

Neural NLI Models. In NLI, in particu-
lar on the Stanford Natural Language In-
ference (SNLI) (Bowman et al., 2015) and
MultiNLI (Williams et al., 2017) datasets, neu-
ral NLI models – end-to-end differentiable models
that can be trained via gradient-based optimisation
– proved to be very successful, achieving state-
of-the-art results (Rocktäschel et al., 2016; Parikh
et al., 2016; Chen et al., 2017).

Let S denote the set of all possible sentences,
and let a = (a1, . . . , a`a) ∈ S and b =
(b1, . . . , b`b) ∈ S denote two input sentences –
representing the premise and the hypothesis – of
length `a and `b, respectively. In neural NLI mod-
els, all words ai and bj are typically represented
by k-dimensional embedding vectors ai,bj ∈ Rk.
As such, the sentences a and b can be encoded by
the sentence embedding matrices a ∈ Rk×`a and
b ∈ Rk×`b , where the columns ai and bj respec-
tively denote the embeddings of words ai and bj .

Given two sentences a, b ∈ S, the goal of a NLI
model is to identify the semantic relation between
a and b, which can be either entailment, contra-
diction, or neutral. For this reason, given an in-
stance, neural NLI models compute the following
conditional probability distribution over all three
classes:

pΘ( · | a, b) = softmax(scoreΘ(a,b)) (1)

where scoreΘ : Rk×`a × Rk×`b → R3 is a model-
dependent scoring function with parameters Θ,
and softmax(x)i = exp{xi}/

∑
j exp{xj} de-

notes the softmax function.
Several scoring functions have been proposed in

the literature, such as the conditional Bidirectional
LSTM (cBiLSTM) (Rocktäschel et al., 2016), the
Decomposable Attention Model (DAM) (Parikh
et al., 2016), and the Enhanced LSTM model
(ESIM) (Chen et al., 2017). One desirable qual-
ity of the scoring function scoreΘ is that it should



67

be differentiable with respect to the model param-
eters Θ, which allows the neural NLI model to be
trained from data via back-propagation.

Model Training. Let D = {(x1, y1), . . . ,
(xm, ym)} represent a NLI dataset, where xi de-
notes the i-th premise-hypothesis sentence pair,
and yi ∈ {1, . . . ,K} their relationship, where
K ∈ N is the number of possible relationships –
in the case of NLI, K = 3. The model is trained
by minimising a cross-entropy loss JD on D:

JD(D,Θ) = −
m∑
i=1

K∑
k=1

1{yi = k} log(ŷi,k) (2)

where ŷi,k = pΘ(yi = k | xi) denotes the proba-
bility of class k on the instance xi inferred by the
neural NLI model as in Eq. (1).

In the following, we analyse the behaviour of
neural NLI models by means of adversarial exam-
ples – inputs to machine learning models designed
to cause the model to commit mistakes. In com-
puter vision models, adversarial examples are cre-
ated by adding a very small amount of noise to
the input (Szegedy et al., 2014; Goodfellow et al.,
2014): these perturbations do not change the se-
mantics of the images, but they can drastically
change the predictions of computer vision mod-
els. In our setting, we define an adversary whose
goal is finding sets of NLI instances where the
model fails to be consistent with available back-
ground knowledge, encoded in the form of First-
Order Logic (FOL) rules. In the following sec-
tions, we define the corresponding optimisation
problem, and propose an efficient solution.

3 Background Knowledge

For analysing the behaviour of NLI models, we
verify whether they agree with the provided back-
ground knowledge, encoded by a set of FOL rules.
Note that the three NLI classes – entailment, con-
tradiction, and neutrality – can be seen as binary
logic predicates, and we can define FOL formulas
for describing the formal relationships that hold
between them.

In the following, we denote the predicates asso-
ciated with entailment, contradiction, and neutral-
ity as ent, con, and neu, respectively. By doing so,
we can represent semantic relationships between
sentences via logic atoms. For instance, given
three sentences s1, s2, s3 ∈ S, we can represent

NLI Rules

R1 > ⇒ ent(X1, X1)
R2 con(X1, X2)⇒ con(X2, X1)
R3 ent(X1, X2)⇒ ¬con(X2, X1)
R4 neu(X1, X2)⇒ ¬con(X2, X1)
R5 ent(X1, X2) ∧ ent(X2, X3)⇒ ent(X1, X3)

Table 1: First-Order Logic rules defining desired
properties of NLI models: Xi are universally
quantified variables, and operators ∧, ¬, and> de-
note logic conjunction, negation, and tautology.

the fact that s1 entails s2 and s2 contradicts s3 by
using the logic atoms ent(s1, s2) and con(s2, s3).

Let X1, . . . , Xn be a set of universally quanti-
fied variables. We define our background knowl-
edge as a set of FOL rules, each having the follow-
ing body⇒ head form:

body(X1, . . . , Xn)⇒ head(X1, . . . , Xn), (3)

where body and head represent the premise and
the conclusion of the rule – if body holds, head
holds as well. In the following, we consider the
rules R1, . . . ,R5 outlined in Table 1. Rule R1
enforces the constraint that entailment is reflex-
ive; rule R2 that contradiction should always be
symmetric (if s1 contradicts s2, then s2 contra-
dicts s1 as well); rule R5 that entailment is tran-
sitive; while rules R3 and R4 describe the formal
relationships between the entailment, neutral, and
contradiction relations.

In Section 4 we propose a method to automat-
ically generate sets of sentences that violate the
rules outlined in Table 1 – effectively generating
adversarial examples. Then, in Section 5 we show
how we can leverage such adversarial examples by
generating them on-the-fly during training and us-
ing them for regularising the model parameters, in
an adversarial training regime.

4 Generating Adversarial Examples

In this section, we propose a method for efficiently
generating adversarial examples for NLI models
– i.e. examples that make the model violate the
background knowledge outlined in Section 3.

4.1 Inconsistency Loss

We cast the problem of generating adversarial ex-
amples as an optimisation problem. In particular,
we propose a continuous inconsistency loss that



68

measures the degree to which a set of sentences
causes a model to violate a rule.

Example 2 (Inconsistency Loss). Consider the
rule R2 in Table 1, i.e. con(X1, X2) ⇒
con(X2, X1). Let s1, s2 ∈ S be two sentences:
this rule is violated if, according to the model, a
sentence s1 contradicts s2, but s2 does not con-
tradict s1. However, if we just use the final deci-
sion made by the neural NLI model, we can sim-
ply check whether the rule is violated by two given
sentences, without any information on the degree
of such a violation.

Intuitively, for the rule being maximally vi-
olated, the conditional probability associated to
con(s1, s2) should be very high (≈ 1), while the
one associated to con(s2, s1) should be very low
(≈ 0). We can measure the extent to which the rule
is violated – which we refer to as inconsistency
loss JI – by checking whether the probability of
the body of the rule is higher than the probability
of its head:

JI(S = {X1 7→ s1, X2 7→ s2})
= [pΘ(con | s1, s2)− pΘ(con | s2, s1)]+

where S is a substitution set that maps the vari-
ables X1 and X2 in R2 to the sentences s1 and
s2, [x]+ = max(0, x), and pΘ(con | si, sj) is the
(conditional) probability that si contradicts sj ac-
cording to the neural NLI model. Note that, in
accordance with the logic implication, the incon-
sistency loss reaches its global minimum when the
probability of the body is close to zero – i.e. the
premise is false – and when the probabilities of
both the body and the head are close to one – i.e.
the premise and the conclusion are both true. 4

We now generalise the intuition in Ex. 2 to any
FOL rule. Let r = (body ⇒ head) denote an
arbitrary FOL rule in the form described in Eq. (3),
and let vars(r) = {X1, . . . , Xn} denote the set of
universally quantified variables in the rule r.

Furthermore, let S = {X1 7→ s1, . . . , Xn 7→
sn} denote a substitution set, i.e. a mapping from
variables in vars(r) to sentences s1, . . . , sn ∈ S.
The inconsistency loss associated with the rule r
on the substitution set S can be defined as:

JI(S) = [p(S; body)− p(S; head)]+ (4)

where p(S; body) and p(S; head) denote the
probability of body and head of the rule, after re-
placing the variables in r with the corresponding

sentences in S. The motivation for the loss in
Eq. (4) is that logic implications can be understood
as “whenever the body is true, the head has to be
true as well”. In terms of NLI models, this trans-
lates as “the probability of the head should at least
be as large as the probability of the body”.

For calculating the inconsistency loss in Eq. (4),
we need to specify how to calculate the probabil-
ity of head and body. The probability of a single
ground atom is given by querying the neural NLI
model, as in Eq. (1). The head contains a single
atom, while the body can be a conjunction of mul-
tiple atoms. Similarly to Minervini et al. (2017),
we use the Gödel t-norm, a continuous generali-
sation of the conjunction operator in logic (Gupta
and Qi, 1991), for computing the probability of the
body of a clause:

pΘ(a1 ∧ a2) = min{pΘ(a1), pΘ(a2)}

where a1 and a2 are two clause atoms.
In this work, we cast the problem of generating

adversarial examples as an optimisation problem:
we search for the substitution set S = {X1 7→
s1, . . . , Xn 7→ sn} that maximises the inconsis-
tency loss in Eq. (4), thus (maximally) violating
the available background knowledge.

4.2 Constraining via Language Modelling
Maximising the inconsistency loss in Eq. (4) may
not be sufficient for generating meaningful adver-
sarial examples: they can lead neural NLI mod-
els to violate available background knowledge, but
they may not be well-formed and meaningful.

For such a reason, in addition to maximising the
inconsistency loss, we also constrain the perplex-
ity of generated sentences by using a neural lan-
guage model (Bengio et al., 2000). In this work,
we use a LSTM (Hochreiter and Schmidhuber,
1997) neural language model pL(w1, . . . , wt) for
generating low-perplexity adversarial examples.

4.3 Searching in a Discrete Space
As mentioned earlier in this section, we cast the
problem of automatically generating adversarial
examples – i.e. examples that cause NLI models
to violate available background knowledge – as an
optimisation problem. Specifically, we look for
substitutions sets S = {X1 7→ s1, . . . , Xn 7→ sn}
that jointly: a) maximise the inconsistency loss
described in Eq. (4), and b) are composed by sen-
tences with a low perplexity, as defined by the neu-
ral language model in Section 4.2.



69

The search objective can be formalised by the
following optimisation problem:

maximise
S

JI(S)

subject to log pL(S) ≤ τ
(5)

where log pL(S) denotes the log-probability of the
sentences in the substitution set S, and τ is a
threshold on the perplexity of generated sentences.

For generating low-perplexity adversarial ex-
amples, we take inspiration from Guu et al. (2017)
and generate the sentences by editing prototypes
extracted from a corpus. Specifically, for search-
ing substitution sets whose sentences jointly have
a high probability and are highly adversarial, as
measured the inconsistency loss in Eq. (4), we
use the following procedure, also described in Ap-
pendix A.4: a) we first sample sentences close to
the data manifold (i.e. with a low perplexity), by
either sampling from the training set or from the
language model; b) we then make small variations
to the sentences – analogous to adversarial images,
which consist in small perturbations of training ex-
amples – so to optimise the objective in Eq. (5).

When editing prototypes, we consider the fol-
lowing perturbations: a) change one word in one
of the input sentences; b) remove one parse sub-
tree from one of the input sentences; c) insert one
parse sub-tree from one sentence in the corpus in
the parse tree of one of the input sentences.

Note that the generation process can easily lead
to ungrammatical or implausible sentences; how-
ever, these will be likely to have a high perplexity
according to the language model (Section 4.2), and
thus they will be ruled out by the search algorithm.

5 Adversarial Regularisation

We now show one can use the adversarial exam-
ples to regularise the training process. We propose
training NLI models by jointly: a) minimising the
data loss (Eq. (2)), and b) minimising the incon-
sistency loss (Eq. (4)) on a set of generated adver-
sarial examples (substitution sets).

More formally, for training, we jointly minimise
the cross-entropy loss defined on the data JD(Θ)
and the inconsistency loss on a set of generated
adversarial examples maxS JI(S; Θ), resulting in
the following optimisation problem:

minimise
Θ

JD(D,Θ) + λmax
S
JI(S; Θ)

subject to log pL(S) ≤ τ
(6)

Premise A man in a suit walks through a train station.
Hypothesis Two boys ride skateboard.

Type Contradiction

Premise Two boys ride skateboard.
Hypothesis A man in a suit walks through a train station.

Type Contradiction

Premise Two people are surfing in the ocean.
Hypothesis There are people outside.

Type Entailment

Premise There are people outside.
Hypothesis Two people are surfing in the ocean.

Type Neutral

Table 2: Sample sentences from an Adversarial
NLI Dataset generated using the DAM model, by
maximising the inconsistency loss JI .

where λ ∈ R+ is a hyperparameter specifying the
trade-off between the data loss JD (Eq. (2)), and
the inconsistency loss JI (Eq. (4)), measured on
the generated substitution set S.

In Eq. (6), the regularisation term
maxS JI(S; Θ) has the task of generating
the adversarial substitution sets by maximising
the inconsistency loss. Furthermore, the con-
straint log pL(S) ≤ τ ensures that the perplexity
of generated sentences is lower than a threshold
τ . For this work, we used the max aggregation
function. However, other functions can be used
as well, such as the sum or mean of multiple
inconsistency losses.

For minimising the regularised loss in Eq. (6),
we alternate between two optimisation processes –
generating the adversarial examples (Eq. (5)) and
minimising the regularised loss (Eq. (6)). The al-
gorithm is outlined in Appendix A.4: at each itera-
tion, after generating a set of adversarial examples
S, it computes the gradient of the regularised loss
in Eq. (6), and updates the model parameters via a
gradient descent step.

6 Creating Adversarial NLI Datasets

We crafted a series of datasets for assessing the ro-
bustness of the proposed regularisation method to
adversarial examples. Starting from the SNLI test
set, we proceeded as follows. We selected the k
instances in the SNLI test set that maximise the in-
consistency loss in Eq. (4) with respect to the rules
in R1, R2, R3, and R4 in Table 1. We refer to the
generated datasets as Akm, where m identifies the
model used for selecting the sentence pairs, and k
denotes number of examples in the dataset.



70

For generating each of the Akm datasets,
we proceeded as follows. Let D =
{(x1, yi), . . . , (xn, yn)} be a NLI dataset (such
as SNLI), where each instance xi = (pi, hi) is a
premise-hypothesis sentence pair, and yi denotes
the relationship holding between pi and hi. For
each instance xi = (pi, hi), we consider two
substitution sets: Si = {X1 7→ pi, X2 7→ hi} and
S′i = {X1 7→ hi, X2 7→ pi}, each corresponding
to a mapping from variables to sentences.

We compute the inconsistency score associated
to each instance xi in the dataset D as JI(Si) +
JI(S′i). Note that the inconsistency score only de-
pends on the premise pi and hypothesis hi in each
instance xi, and it does not depend on its label yi.

After computing the inconsistency scores for all
sentence pairs inD using a modelm, we select the
k instances with the highest inconsistency score,
we create two instances xi = (pi, hi) and x̂i =
(hi, pi), and add both (xi, yi) and (x̂i, ŷi) to the
dataset Akm. Note that, while yi is already known
from the datasetD, ŷi is unknown. For this reason,
we find ŷi by manual annotation.

7 Related Work

Adversarial examples are receiving a considerable
attention in NLP; their usage, however, is consid-
erably limited by the fact that semantically invari-
ant input perturbations in NLP are difficult to iden-
tify (Buck et al., 2017).

Jia and Liang (2017) analyse the robustness of
extractive question answering models on exam-
ples obtained by adding adversarially generated
distracting text to SQuAD (Rajpurkar et al., 2016)
dataset instances. Belinkov and Bisk (2017) also
notice that character-level Machine Translation
are overly sensitive to random character manipu-
lations, such as typos. Hosseini et al. (2017) show
that simple character-level modifications can dras-
tically change the toxicity score of a text. Iyyer
et al. (2018) proposes using paraphrasing for gen-
erating adversarial examples. Our model is fun-
damentally different in two ways: a) it does not
need labelled data for generating adversarial ex-
amples – the inconsistency loss can be maximised
by just making an NLI model produce inconsistent
results, and b) it incorporates adversarial examples
during the training process, with the aim of train-
ing more robust NLI models.

Adversarial examples are also used for as-
sessing the robustness of computer vision mod-

Model Original Regularised
Valid. Test Valid. Test

M
ul

tiN
L

I cBiLSTM 61.52 63.95 66.98 66.68
DAM 72.78 73.28 73.57 73.51
ESIM 73.66 75.22 75.72 75.80

SN
L

I cBiLSTM 81.41 80.99 82.27 81.12
DAM 86.96 86.29 87.08 86.43
ESIM 87.83 87.25 87.98 87.55

Table 3: Accuracy on the SNLI and MultiNLI
datasets with different neural NLI models before
(left) and after (right) adversarial regularisation.

Model Rule |B| |B ∧ ¬H| Violations (%)

cBiLSTM

R1 1,098,734 261,064 23.76 %
R2 174,902 80,748 46.17 %
R3 197,697 24,294 12.29 %
R4 176,768 33,435 18.91 %

DAM

R1 1,098,734 956 00.09 %
R2 171,728 28,680 16.70 %
R3 196,042 11,599 05.92 %
R4 181,597 29,635 16.32 %

ESIM

R1 1,098,734 10,985 01.00 %
R2 177,950 17,518 09.84 %
R3 200,852 6,482 03.23 %
R4 170,565 17,190 10.08 %

Table 4: Violations (%) of rules R1,R2,R3,R4
from Table 1 on the SNLI training set, yield by
cBiLSTM, DAM, and ESIM.

els (Szegedy et al., 2014; Goodfellow et al., 2014;
Nguyen et al., 2015), where they are created by
adding a small amount of noise to the inputs that
does not change the semantics of the images, but
drastically changes the model predictions.

8 Experiments

We trained DAM, ESIM and cBiLSTM on the
SNLI corpus using the hyperparameters provided
in the respective papers. The results provided by
such models on the SNLI and MultiNLI validation
and tests sets are provided in Table 3. In the case
of MultiNLI, the validation set was obtained by
removing 10,000 instances from the training set
(originally composed by 392,702 instances), and
the test set consists in the matched validation set.

Background Knowledge Violations. As a first
experiment, we count the how likely our model is
to violate rules R1,R2,R3,R4 in Table 1.

In Table 4 we report the number sentence pairs
in the SNLI training set where DAM, ESIM and
cBiLSTM violate R1,R2,R3,R4. In the |B|
column we report the number of times the body



71

Model
Dataset A100DAM A500DAM A1000DAM A100ESIM A500ESIM A1000ESIM A100cBiLSTM A500cBiLSTM A1000cBiLSTM

DAMAR 83.33 79.15 79.37 71.35 72.19 70.05 93.00 88.99 86.00
DAM 47.40 47.93 51.66 55.73 60.94 60.88 81.50 77.37 75.28

ESIMAR 89.06 86.00 85.08 78.12 76.04 73.32 96.50 91.92 88.52
ESIM 72.40 74.59 76.92 52.08 58.65 60.78 87.00 84.34 82.05

cBiLSTMAR 85.42 80.39 78.74 73.96 70.52 65.39 92.50 88.38 83.62
cBiLSTM 56.25 59.96 61.75 47.92 53.23 53.73 51.50 52.83 53.24

Table 5: Accuracy of unregularised and regularised neural NLI models DAM, cBiLSTM, and ESIM, and
their adversarially regularised versions DAMAR, cBiLSTMAR, and ESIMAR, on adversarial datasets
Akm.

of the rule holds, according to the model. In the
|B ∧ ¬H| column we report the number of times
where the body of the rule holds, but the head does
not – which is clearly a violation of available rules.

We can see that, in the case of rule R1 (reflex-
ivity of entailment), DAM and ESIM make a rel-
atively low number of violations – namely 0.09
and 1.00 %, respectively. However, in the case of
cBiLSTM, we can see that, each sentence s ∈ S
in the SNLI training set, with a 23.76 % chance,
s does not entail itself – which violates our back-
ground knowledge.

With respect to R2 (symmetry of contradic-
tion), we see that none of the models is completely
consistent with the available background knowl-
edge. Given a sentence pair s1, s2 ∈ S from the
SNLI training set, if – according to the model – s1
contradicts s2, a significant number of times (be-
tween 9.84% and 46.17%) the same model also
infers that s2 does not contradict s1. This phe-
nomenon happens 16.70 % of times with DAM,
9.84 % of times with ESIM, and 46.17 % with
cBiLSTM: this indicates that all considered mod-
els are prone to violating R2 in their predictions,
with ESIM being the more robust.

In Appendix A.2 we report several examples of
such violations in the SNLI training set. We se-
lect those that maximise the inconsistency loss de-
scribed in Eq. (4), violating rules R2 and R3. We
can notice that the presence of inconsistencies is
often correlated with the length of the sentences.
The model tends to detect entailment relationships
between longer (i.e., possibly more specific) and
shorter (i.e., possibly more general) sentences.

8.1 Generation of Adversarial Examples

In the following, we analyse the automatic gen-
eration of sets of adversarial examples that make
the model violate the existing background knowl-

0.0 10 4 10 3 10 2 10 1 1.0
Regularisation Parameter 

0

5

10

Vi
ol

at
io

ns
 (%

)

Number of violations (%) made by ESIM
con(X1, X2) con(X2, X1)
ent(X1, X2) ¬con(X2, X1)
neut(X1, X2) ¬con(X2, X1)

ent(X1, X1)

Figure 1: Number of violations (%) to rules in Ta-
ble 1 made by ESIM on the SNLI test set.

edge. We search in the space of sentences by ap-
plying perturbations to sampled sentence pairs, us-
ing a language model for guiding the search pro-
cess. The generation procedure is described in
Section 4.

The procedure was especially effective in gen-
erating adversarial examples – a sample is shown
in Table 6. We can notice that, even though
DAM and ESIM achieve results close to human
level performance on SNLI, they are likely to fail
when faced with linguistic phenomena such as
negation, hyponymy, and antonymy. Gururangan
et al. (2018) recently showed that NLI datasets
tend to suffer from annotation artefacts and lim-
ited linguistic variations: this allows NLI mod-
els to achieve nearly-human performance by cap-
turing repetitive patterns and idiosyncrasies in a
dataset, without being able of effectively captur-
ing textual entailment. This is visible, for instance,
in example 5 of Table 6, where the model fails
to capture the hyponymy relation between “male”
and “man”, incorrectly predicting an entailment in
place of a neutral relationship. Furthermore, it is
clear that models lack commonsense knowledge,
such as the relation between “pushing” and “car-
rying” (example 1), and being outside and swim-
ming (example 2). Generating such adversarial



72

Adversarial Example Prediction Inconsistency

1
s1 A man in uniform is pushing a medical bed. s1

0.72−−→s2 .01 .92
s2 a man is pushing carrying something. s2

0.93−−→s1

1
s1 A dog swims in the water s1

0.78−−→s2 .00 .99
s2 A dog is swimming outside. s2

0.99−−→s1

2
s1 A young man is sledding down a snow covered hill on a green sled. s1

0.98−−→s2 .00 .97
s1 A man is sledding down to meet his daughter. s2

1.00−−→s1

3
s1 A woman sleeps on the ground. A boy and girl play in a pool. s1

0.94−−→s2 .00 .82
s2 Two kids are happily playing in a swimming pool. s2

0.85−−→s1

4
s1 The school is having a special event in order to show the american culture on how other cultures are dealt with in parties. s1

0.96−−→s2 .01 .63
s2 A school dog is hosting an event. s2

0.66−−→s1

s1 A boy is drinking out of a water fountain shaped like a woman. s1
0.96−−→s2

5 s2 A male is getting a drink of water. s2
0.93−−→s3 .00 .94

s3 A male man is getting a drink of water. s1
0.97−−→s3

Table 6: Inconsistent results produced by DAM on automatically generated adversarial examples. The
notation segment one segment two denotes that the corruption process removes “segment one” and intro-
duced “segment two” in the sentence, and s1

p−→s2 indicates that DAM classifies the relation between s1
and s2 as contradiction, with probability p. We use different colours for representing the contradiction,
entailment and neutral classes. Examples 1, 2, 3, and 4 violate the rule R2, while example 5 violates the
rule R5. .00 .99 indicates that the corruption process increases the inconsistency loss from .00 to .99,
and the red boxes are used for indicating mistakes made by the model on the adversarial examples.

examples provides us with useful insights on the
inner workings of neural NLI models, that can be
leveraged for improving the robustness of state-of-
the-art models.

8.2 Adversarial Regularisation

We evaluated whether our approach for integrat-
ing logical background knowledge via adversar-
ial training (Section 5) is effective at reducing
the number of background knowledge violations,
without reducing the predictive accuracy of the
model. We started with pre-trained DAM, ESIM,
and cBiLSTM models, trained using the hyperpa-
rameters published in their respective papers.

After training, each model was then fine-tuned
for 10 epochs, by minimising the adversarially
regularised loss function introduced in Eq. (6). Ta-
ble 3 shows results on the SNLI and MultiNLI
development and test set, while Fig. 1 shows the
number of violations for different values of λ,
where regularised models are much more likely to
make predictions that are consistent with the avail-
able background knowledge.

We can see that, despite the drastic reduc-
tion of background knowledge violations, the im-
provement may not be significant, supporting the
idea that models achieving close-to-human per-
formance on SNLI and MultiNLI may be captur-
ing annotation artefacts and idiosyncrasies in such

datasets (Gururangan et al., 2018).

Evaluation on Adversarial Datasets. We eval-
uated the proposed approach on 9 adversarial
datasets Akm, with k ∈ {100, 500, 1000}, gen-
erated following the procedure described in Sec-
tion 6 – results are summarised in Table 5. We
can see that the proposed adversarial training
method significantly increases the accuracy on
the adversarial test sets. For instance, consider
A100DAM: prior to regularising (λ = 0), DAM
achieves a very low accuracy on this dataset – i.e.
47.4%. By increasing the regularisation parameter
λ ∈ {10−4, 10−3, 10−2, 10−1}, we noticed sensi-
ble accuracy increases, yielding relative accuracy
improvements up to 75.8% in the case of DAM,
and 79.6% in the case of cBiLSTM.

From Table 5 we can notice that adversarial ex-
amples transfer across different models: an unreg-
ularised model is likely to perform poorly also on
adversarial datasets generated by using different
models, with ESIM being the more robust model
to adversarially generated examples. Furthermore,
we can see that regularised models are generally
more robust to adversarial examples, even when
those were generated using different model archi-
tectures. For instance we can see that, while cBiL-
STM is vulnerable also to adversarial examples
generated using DAM and ESIM, its adversari-



73

ally regularised version cBiLSTMAR is generally
more robust to any sort of adversarial examples.

9 Conclusions

In this paper, we investigated the problem of auto-
matically generating adversarial examples that vi-
olate a set of given First-Order Logic constraints in
NLI. We reduced the problem of identifying such
adversarial examples to an optimisation problem,
by maximising a continuous relaxation of the vio-
lation of such constraints, and by using a language
model for generating linguistically-plausible ex-
amples. Furthermore, we proposed a method for
adversarially regularising neural NLI models for
incorporating background knowledge.

Our results showed that the proposed method
consistently yields significant increases to the pre-
dictive accuracy on adversarially-crafted datasets
– up to a 79.6% relative improvement – while
drastically reducing the number of background
knowledge violations. Furthermore, we showed
that adversarial examples transfer across model
architectures, and the proposed adversarial train-
ing procedure produces generally more robust
models. The source code and data for re-
producing our results is available online, at
https://github.com/uclmr/adversarial-nli/.

Acknowledgements

We are immensely grateful to Jeff Mitchell, Jo-
hannes Welbl, and the whole UCL Machine Read-
ing research group for all useful discussions, in-
puts, and ideas. This work has been supported by
an Allen Distinguished Investigator Award, and a
Marie Curie Career Integration Award.

References

Yonatan Belinkov and Yonatan Bisk. 2017. Synthetic
and natural noise both break neural machine transla-
tion. CoRR, abs/1711.02173.

Yoshua Bengio, Réjean Ducharme, and Pascal Vincent.
2000. A neural probabilistic language model. In
Advances in Neural Information Processing Systems
13, Papers from Neural Information Processing Sys-
tems (NIPS) 2000, pages 932–938. MIT Press.

Johan van Benthem. 2008. A brief history of natural
logic. In M. Chakraborty, B. Löwe, M. Nath Mi-
tra, and S. Sarukki, editors, Logic, Navya-Nyaya
and Applications: Homage to Bimal Matilal. Col-
lege Publications.

Johan Bos and Katja Markert. 2005. Recognis-
ing textual entailment with logical inference. In
HLT/EMNLP 2005, Human Language Technology
Conference and Conference on Empirical Methods
in Natural Language Processing, Proceedings of the
Conference, pages 628–635. The Association for
Computational Linguistics.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2015, pages 632–642. The Association for Compu-
tational Linguistics.

Christian Buck, Jannis Bulian, Massimiliano Cia-
ramita, Andrea Gesmundo, Neil Houlsby, Wojciech
Gajewski, and Wei Wang. 2017. Ask the right ques-
tions: Active question reformulation with reinforce-
ment learning. CoRR, abs/1705.07830.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017. Enhanced LSTM for
natural language inference. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2017, pages 1657–1668.
Association for Computational Linguistics.

Cleo Condoravdi, Dick Crouch, Valeria de Paiva, Rein-
hard Stolle, and Daniel G. Bobrow. 2003. Entail-
ment, intensionality and text understanding. In Pro-
ceedings of the HLT-NAACL 2003 Workshop on Text
Meaning, pages 38–45.

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entailment
challenge. In Machine Learning Challenges, Eval-
uating Predictive Uncertainty, Visual Object Clas-
sification and Recognizing Textual Entailment, First
PASCAL Machine Learning Challenges Workshop,
MLCW 2005, volume 3944 of LNCS, pages 177–
190. Springer.

Yaroslav Fyodorov, Yoad Winter, and Nissim Francez.
2000. A natural logic inference system. In Proceed-
ings of the of the 2nd Workshop on Inference in Com-
putational Semantics.

Ian J. Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2014. Explaining and harnessing adver-
sarial examples. CoRR, abs/1412.6572.

M. M. Gupta and J. Qi. 1991. Theory of t-norms
and fuzzy inference methods. Fuzzy Sets Syst.,
40(3):431–450.

Suchin Gururangan, Swabha Swayamdipta, Omer
Levy, Roy Schwartz, Samuel R. Bowman, and
Noah A. Smith. 2018. Annotation artifacts in natu-
ral language inference data. CoRR, abs/1803.02324.

Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren,
and Percy Liang. 2017. Generating sentences by
editing prototypes. CoRR, abs/1709.08878.

https://github.com/uclmr/adversarial-nli/


74

Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2001. The Elements of Statistical Learning.
Springer Series in Statistics. Springer New York Inc.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Hossein Hosseini, Baicen Xiao, and Radha Pooven-
dran. 2017. Deceiving google’s cloud video intel-
ligence API built for summarizing videos. In 2017
IEEE Conference on Computer Vision and Pattern
Recognition Workshops, CVPR Workshops, pages
1305–1309. IEEE Computer Society.

Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke
Zettlemoyer. 2018. Adversarial example generation
with syntactically controlled paraphrase networks.
CoRR, abs/1804.06059.

Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.
In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2017, pages 2011–2021. Association for Computa-
tional Linguistics.

Anjuli Kannan and Oriol Vinyals. 2017. Adver-
sarial evaluation of dialogue models. CoRR,
abs/1701.08198.

J.J. Katz. 1972. Semantic theory. Studies in language.
Harper & Row.

Hector J. Levesque. 2014. On our best behaviour. Ar-
tif. Intell., 212:27–35.

Bill MacCartney and Christopher D Manning. 2009.
An extended model of natural logic. In Proceed-
ings of the of the Eighth International Conference
on Computational Semantics, Tilburg, Netherlands.

Pasquale Minervini, Thomas Demeester, Tim Rock-
täschel, and Sebastian Riedel. 2017. Adversarial
sets for regularising neural link predictors. In Pro-
ceedings of the Thirty-Third Conference on Uncer-
tainty in Artificial Intelligence, UAI 2017. AUAI
Press.

Anh Mai Nguyen, Jason Yosinski, and Jeff Clune.
2015. Deep neural networks are easily fooled: High
confidence predictions for unrecognizable images.
In IEEE Conference on Computer Vision and Pat-
tern Recognition, CVPR 2015, pages 427–436. IEEE
Computer Society.

Denis Paperno, Germán Kruszewski, Angeliki Lazari-
dou, Quan Ngoc Pham, Raffaella Bernardi, San-
dro Pezzelle, Marco Baroni, Gemma Boleda, and
Raquel Fernández. 2016. The LAMBADA dataset:
Word prediction requiring a broad discourse context.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2016.
The Association for Computer Linguistics.

Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In (Su et al.,
2016), pages 2249–2255.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100, 000+ questions for
machine comprehension of text. In (Su et al., 2016),
pages 2383–2392.

Laura Rimell and Stephen Clark. 2009. Port-
ing a lexicalized-grammar parser to the biomedi-
cal domain. Journal of Biomedical Informatics,
42(5):852–865.

Tim Rocktäschel, Edward Grefenstette, Karl Moritz
Hermann, Tomas Kocisky, and Phil Blunsom. 2016.
Reasoning about entailment with neural attention.
In International Conference on Learning Represen-
tations (ICLR).

Jian Su et al., editors. 2016. Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2016. The Association
for Computational Linguistics.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and
Rob Fergus. 2014. Intriguing properties of neural
networks. In International Conference on Learning
Representations.

Adina Williams, Nikita Nangia, and Samuel R. Bow-
man. 2017. A broad-coverage challenge corpus for
sentence understanding through inference. CoRR,
abs/1704.05426.


