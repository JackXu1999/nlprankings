



















































Entity Disambiguation by Knowledge and Text Jointly Embedding


Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 260–269,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Entity Disambiguation by Knowledge and Text Jointly Embedding

Wei Fang1,2, Jianwen Zhang3, Dilin Wang4, Zheng Chen3, Ming Li1,2
1SYSU-CMU Joint Institute of Engineering,

School of Electronics and Information Technology, Sun Yat-Sen University
2SYSU-CMU Shunde International Joint Research Institute

{fangwei7@mail2, liming46@mail}.sysu.edu.cn
3Microsoft, Redmond, WA

{jiazhan, zhengc}@microsoft.com
4Computer Science, Dartmouth College

dilin.wang.gr@dartmouth.edu

Abstract

For most entity disambiguation systems,
the secret recipes are feature representa-
tions for mentions and entities, most of
which are based on Bag-of-Words (BoW)
representations. Commonly, BoW has
several drawbacks: (1) It ignores the
intrinsic meaning of words/entities; (2)
It often results in high-dimension vector
spaces and expensive computation; (3) For
different applications, methods of design-
ing handcrafted representations may be
quite different, lacking of a general guide-
line. In this paper, we propose a different
approach named EDKate. We first learn
low-dimensional continuous vector repre-
sentations for entities and words by jointly
embedding knowledge base and text in the
same vector space. Then we utilize these
embeddings to design simple but effec-
tive features and build a two-layer disam-
biguation model. Extensive experiments
on real-world data sets show that (1) The
embedding-based features are very effec-
tive. Even a single one embedding-based
feature can beat the combination of sev-
eral BoW-based features. (2) The superi-
ority is even more promising in a difficult
set where the mention-entity prior cannot
work well. (3) The proposed embedding
method is much better than trivial imple-
mentations of some off-the-shelf embed-
ding algorithms. (4) We compared our
EDKate with existing methods/systems
and the results are also positive.

1 Introduction

Entity disambiguation is the task of linking entity
mentions in unstructured text to the correspond-
ing entities in a knowledge base. For example,
in the sentence “Michael Jordan is newly elected
as AAAI fellow”, the mention “Michael Jordan”
should be linked to “Michael I. Jordan” (Berke-
ley Professor) rather than “Michael Jordan” (NBA
Player). Formally, given a set of mentions M =
{m1,m2, ..., mk} (specified or detected automati-
cally) in a document d, for each mention mi ∈ M ,
the task is to find the correct entity ei in the knowl-
edge base (KB) K to which the mention mi refers.

There are various methods proposed for the
problem in the past decades. But generally
speaking, an entity disambiguation method is
commonly composed of three stages/components.
(1) Constructing representations for men-
tions/entities from raw data, often as the form
of sparse vectors. (2) Extracting features for
disambiguation models based on the represen-
tations of mentions and entities constructed in
stage (1). (3) Optimizing the disambiguation
model by empirically setting or learning weights
on the extracted features, e.g., by training a
classifier/ranker. There exist few features directly
defined by heuristics, skipping the first stage. For
example, string similarity or edit distance between
a mention surface and an entity’s canonical
form (Cucerzan, 2011; Cassidy et al., 2011), and
the prior probability of a mention surface being
some entity, etc. However, they are the minority
as it is difficult for human to design such features.

Almost all the existing methods focus on the
second or the third stages while the importance of
the first stage is often overlooked. The common
practice to deal with the first stage of representa-

260



tions is defining handcrafted BoW representations.
For example, an entity is often represented by a
sparse vector of weights on the n-grams contained
in the description text of the entity, i.e., the stan-
dard Bag-of-Words (BoW) representation. TF-
IDF is often used to set the weights. There are sev-
eral variants for this way, e.g., using selected key
phrases or Wikipedia in-links/out-links instead of
all n-grams as the dimensions of the vectors (Rati-
nov et al., 2011). The problem is more challeng-
ing when representing a mention. The common
choice is using the n-gram vector of the surround-
ing text. Obviously the information of the local
text window is too limited to well represent a men-
tion. In practice, there is another constraint, the
representations of entities and mentions should be
in the same space, i.e., the dimensions of the vec-
tors should be shared. This constraint makes the
representation design more difficult. How to de-
fine such representations and the features based on
them almost become the secrete sauce of a disam-
biguation system. For example, Cucerzan (2007)
uses Wikipedia anchor surfaces and “Category”
values as dimensions and designed complex mech-
anisms to represent words, mentions and entities
as sparse vectors on those dimensions.

BoW representations have several intrinsic
drawbacks: First, the semantic meaning of a di-
mension is largely ignored. For example, “cat”,
“cats” and “tree” are equally distant under one-
hot BoW representations. Second, BoW repre-
sentations often introduce high dimension vector
spaces and lead to expensive computation. Third,
for different applications, methods of designing
handcrafted representations may be quite differ-
ent, lacking of a general guideline. The intuitive
questions like “why using n-grams, Wikipedia
links or category values as dimensions” and “why
using TF-IDF as weights” are hinting us it is very
likely these handcrafted representations are not the
best and there should be some better representa-
tions.

In this paper we focus on the first stage, the
problem of representations. Inspired by the re-
cent works on word embedding (Bengio et al.,
2003; Collobert et al., 2011; Mikolov et al., 2013a;
Mikolov et al., 2013b), knowledge embedding
(Bordes et al., 2011; Bordes et al., 2013; Socher et
al., 2013; Wang et al., 2014b) and joint embedding
KBs and texts (Wang et al., 2014a; Zhong and
Zhang, 2015), we propose to learn representations

for entity disambiguation. Specifically, from KBs
and texts, we jointly embed entities and words
into the same low-dimensional continuous vector
space. The embeddings are obtained by optimiz-
ing a global objective considering all the infor-
mation in the KBs and texts thus the intrinsic se-
mantics of words and entities are believed to be
preserved during the embedding. Then we design
simple but effective features based on embeddings
and build a two-layer disambiguation model. We
conduct extensive experiments on real-word data
sets and exhibit the effectiveness of our words and
entities’ representation.

2 Related Work

Entity Disambiguation Entity disambiguation
methods roughly fall into two categories: local
approaches and collective approaches. Local ap-
proaches disambiguate each mention in a docu-
ment separately. For example, Bunescu and Pasca
(2006) compare the context of each mention with
the Wikipedia categories of an entity candidate;
Milne and Witten (2008) come up with the con-
cept “unambiguous link” and make it convenient
to compute entity relatedness. Differently, collec-
tive approaches require all entities in a document
“coherent” in semantic, measured by some objec-
tive functions. Cucerzan (2007) proposes a topic
representation for document by aggregating topic
vectors of all entity candidates in the document.
Kulkarni et al. (2009) model pair-wise coherence
of entity candidates for two different mentions and
use hill-climbing algorithm to get a proximate so-
lution. Hoffart et al. (2011) treat entity disam-
biguation as the task of finding a dense subgraph
which contain all mention nodes and exactly one
mention-entity edge for each mention from a large
graph.

Most methods above design various represen-
tations for mentions and entities. For example,
based on Wikipedia, Cucerzan (2007) uses anchor
surfaces to represent entities in “context space”
and use items in the category boxes to represent
entities in “topic space”. For mentions, he takes
context words among a fixed-size window around
the mention as the context vector. Kulkarni et al.
(2009) exploit sets of words, sets of word counts
and sets of TF-IDFs to represent entities. Rati-
nov et al. (2011) express entities with extensive
in-links and out-links in Wikipedia.

In recent years, some works are considering

261



how to apply neural network to disambiguate enti-
ties from context. For example, He et al. (2013)
use feed-forward network to represent context
based on BoW input while Sun et al. (2015) turn
to convolution network directly based on the orig-
inal word2vec (Mikolov et al., 2013a). However,
they pay little attention to design effective word
and entity representations. In this paper, we focus
on learning representative word and entity vectors
for disambiguation.
Embedding Word embedding aims to learn con-
tinuous vector representation for words. Word em-
beddings are usually learned from unlabeled text
corpus by predicting context words surrounded or
predicting the current word given context words
(Bengio et al., 2003; Collobert et al., 2011;
Mikolov et al., 2013a; Mikolov et al., 2013b).
These embeddings can usually catch syntactic and
semantic relations between words.

Recently knowledge embedding also becomes
popular. The goal is to embed entities and rela-
tions of knowledge graphs into a low-dimension
continuous vector space while certain properties
in the graph are preserved (Bordes et al., 2011;
Bordes et al., 2013; Socher et al., 2013; Wang et
al., 2014a; Wang et al., 2014b). To connect word
embedding and knowledge embedding, (Wang et
al., 2014a) propose to align these two spaces by
Wikipedia anchors and names of entities. (Zhong
and Zhang, 2015) conduct alignment by entities’
description.

3 Disambiguation by Embedding

In this part, we first refine current joint embed-
ding techniques to train word and entity embed-
dings from Freebase and Wikipedia texts for dis-
ambiguation tasks. Then in section 3.2, we design
simple features based on embeddings. Finally in
section 3.3, we propose a two-layer disambigua-
tion model to balance mention-entity prior and
other features.

3.1 Embeddings Jointly Learning

We mainly base the joint learning framework on
(Wang et al., 2014a)’s joint model and also utilize
the alignment technique from (Zhong and Zhang,
2015) to better align word and entity embeddings
into a same space. Furthermore, we optimize
the embedding for disambiguation from two as-
pects. First, we add url-anchor (entity-entity) co-
occurrence from Wikipedia. Second, we refine

the traditional negative sampling part to have enti-
ties in candidate list more probable to be sampled,
which aims to discriminate entity candidates from
each other.

3.1.1 Knowledge Model
A knowledge base K is usually composed of a set
of triplets (h, r, t), where h, t ∈ E (the set of en-
tities) and r ∈ R (the set of relations). Here, we
follow (Wang et al., 2014a) to use h, r, t to denote
the embeddings of h, r, t respectively. And score
a triplet in this way:

z(h, r, t) = b− 1
2
∥h + r − t∥2 (1)

where b is a constant for numerical stability in the
approximate optimization stage described in 3.1.5.
Then normalize z and define:

Pr(h|r, t) = exp{z(h, r, t)}∑
h̃∈E exp{z(h̃, r, t)}

(2)

Pr (r|h, t) and Pr (t|h, r) are also defined in a
similar way. And the likelihood of observing a
triplet is:

Ltriplet(h, r, t) = log Pr(h|r, t) + log Pr(r|h, t)
+ log Pr(t|h, r)

(3)
Then the goal is to maximize the likelihood of all
triplets in the whole knowledge graph:

LK =
∑

(h,r,t)∈K
Ltriplet(h, r, t) (4)

3.1.2 Text Model
In text model, to be compatible with the knowl-
edge model, a pair of co-occurrence words is
scored in this way:

z(w, v) = b− 1
2
∥w − v∥2 (5)

where w and v represent co-occurrence of two
words in a context window; w and v represent the
corresponding embeddings for w and v. Then nor-
malize z(w, v) and give a probability representa-
tion:

Pr(w|v) = exp{z(w, v)}∑
w̃∈V exp{z(w̃, v)}

(6)

where V is our vocabulary. Then the goal of the
text model is to maximize the likelihood of all
word co-occurrence pairs:

LT =
∑
(w,v)

[log Pr(w|v) + log Pr(v|w)] (7)

262



3.1.3 Alignment Model
Alignment model guarantees the vectors of enti-
ties and words are in the same space, i.e., the sim-
ilarity/distance between an entity vector and an
word vector is meaningful. We combine all the
three alignment models proposed in (Wang et al.,
2014a) and (Zhong and Zhang, 2015).

Alignment by Wikipedia Anchors (Wang et
al., 2014a). Mentions are replaced with the enti-
ties they link to and word-word co-occurrence be-
comes word-entity co-occurrence.

LAA =
∑

(w,a),a∈A
[log Pr(w|ea) + log Pr(ea|w)]

(8)
where A denotes the set of anchors and ea denotes
the entity behind the anchor a.

Alignment by Names of Entities (Wang et al.,
2014a). For each triplet (h, r, t), h or t are re-
placed with their corresponding names, so we get
(wh, r, t), (h, r, wt) and (wh, r, wt), where wh de-
notes the name of h and wt denotes the name of
t.

LAN =
∑

(h,r,t)

[Ltriplet(wh, r, t) + Ltriplet(h, r, wt)

+ Ltriplet(wh, r, wt)]
(9)

Alignment by Entities’ Description (Zhong
and Zhang, 2015). This alignment utilizes the co-
occurrence of Wikipedia url and words in the de-
scription of that url page, which is similar to the
PV-DBOW model in (Le and Mikolov, 2014).

LAD =
∑
e∈E

∑
w∈De

[log Pr(e|w) + log Pr(w|e)]

(10)
where De denotes the description of an entity e.
To clarify again, “url” is equivalent with “en-
tity” in this paper. Combine these three kinds of
alignment techniques, we get the whole alignment
model:

LA = LAA + LAN + LAD (11)

3.1.4 Url-Anchor Co-occurrence
For entity disambiguation, the entity relatedness
graph is useful to capture the “topics” of an entity
in Wikipedia. Thus we also hope to encode such
information into our embedding. Specifically we
further incorporate “url-anchor” co-occurrence to
the training objective. “url” stands for the url of a

Wikipedia page and “anchor” stands for the hyper-
links of anchor fields in that page.

LU =
∑
e∈E

∑
a∈ADe

[Pr(e|ea) + Pr(ea|e)] (12)

where ADe stands for all anchors in Wikpedia
page De. Pr(e|ea) and Pr(ea|e) are defined simi-
larly as equation 6.

Considering knowledge model, text model,
alignment model and url-anchor co-occurrence all
together, we get the overall objective (likelihood)
to maximize:

L = LK + LT + LA + LU (13)

3.1.5 Negative Sampling Refinement

In training phase, to avoid the computation of
the normalizer in equation (2) and (6), we fol-
low (Mikolov et al., 2013b) to transform the origin
softmax-like objective to a simpler binary classi-
fication objective, which aims to distinguish ob-
served data from noise.

To optimize for entity disambiguation, when us-
ing the context words to predict an anchor (en-
tity), i.e., optimizing Pr(ea|w), rather than uni-
formly sampling negatives from the vocabulary as
(Mikolov et al., 2013b), we conduct our sampling
according to the candidates’ prior distribution.

3.2 Disambiguation Features Design

With the embeddings we train above, many entity
disambiguation methods can directly take them
as the words and entities’ representation and re-
define their features. In this section, we only de-
sign some simple features to illustrate the capabil-
ity of the embeddings in disambiguation. In the
section of experiment, we can observe that even a
single embedding-based feature can beat the com-
bination of several BoW-based features.

3.2.1 Mention-Entity Prior

This feature is directly counted from Wikipedia’s
anchor fields and measures the link probability of
an entity e given a mention m. Prior is a strong in-
dicator (Fader et al., 2009) to select the correct en-
tity. However, it is unwise to take prior as a feature
all the time because prior usually get a very large
weight, which overfits the training data. Later in
this paper, we will propose a classifier to tell when
to use the prior or not.

263



3.2.2 Global Context Relatedness (E-GCR)
This feature comes from the hypothesis that the
true entity of a mention will coincide with the
meaning of most of the other words in the same
document. So this feature sums up all idf-
weighted relatedness scores between an entity
candidate and each context word, then average
them:

∀e ∈ Γ(m),E −GCR(e, d|m) =
1
|d|

∑
w∈d

idf(w) · Ω(e, w) (14)

where Γ(m) denotes the entity candidate set of
mention m; d denotes the document containing
m; Ω(e, w) denotes a distance-based relatedness
b− 12 ∥e − w∥2, which is compatible with the em-
bedding model.

3.2.3 Local Context Relatedness (E-LCR)
E-GCR can only coarsely rank topic-related can-
didates to one of the top positions. But sometimes
there is nearly no relation between the true entity
and the topic of the document:
Ex.1 “Stefani, a DJ at The Zone 101.5 FM in
Phoenix, AZ, sent me an awesome MP3 of the in-
terview...”
In this example, E-GCR will link AZ to AZ (rap-
per) because the context is all about music al-
though Phoenix should be a strong hint to link AZ
to Arizona.

To avoid this kind of errors, we design a fea-
ture to describe the relatedness between an en-
tity candidate and some important words around
the mention. To identify these words, we
turn to dependency parser provided by Stanford
CoreNLP (Manning et al., 2014). Formulate this
feature:

∀e ∈ Γ(m),E − LCR(e, d|m) =
1

|Sdepend|
∑

w∈Sdepend
Ω(e, w) (15)

where Sdepend is the set consisting all adjacent
words of m in the dependency graph of the doc-
ument d.

3.2.4 Local Entity Coherence (E-LEC)
In practice, there are usually many casual men-
tions linked to an entity, such as w v for West Vir-
ginia.
Ex.2 “We would like to welcome you to the official

website for the city of Chester, w v. ”
In this case, “w v” should be a strong hint for the
disambiguation of “Chester”. However, “w”, “v”
or “w v” is too casual to catch useful informa-
tion if we only take their lexical expression. So
we should not only take the relative surface forms
but also their entity candidates into consideration.
Then the entity “West Virginia” will be quite help-
ful to link “Chester” to “Chester, West Virginia”
This feature is similar to the previous collective
or topic-coherence methods. And our local entity
coherence is more accurate because we only con-
sider relative mentions/entities around rather than
all entities in a document. Formulate this feature:

∀e ∈ Γ(m), E − LEC(e, d|m) =
1

|Sdepend|
∑

w∈Sdepend maxe′∈Γ(w),e′ ̸=e
Ω(e, e′) (16)

3.3 Two-layer Disambiguation Model

To balance the usage of prior and other features,
we propose a two-layer disambiguation model. It
includes two steps: (1) Build a binary classifier to
give a probability pconf denoting the confidence to
use prior only. Features used to construct this clas-
sifier are E-GCR, mention word itself and context
words in a window sized 4 around the mention. (2)
If pconf achieve a designated threshold ξ, we only
adopt prior to select the best candidate, otherwise
we only consider other embedding-based features
described in section 3.2. Formulate this model:

∀m, e∗ =


arg max
e∈Γ(m)

prior(e|m), pconf ≥ ξ

arg max
e∈Γ(m)

∑|F |
i wi · fi, pconf < ξ

(17)
where e∗ is the entity we choose for the mention
m.

4 Experiments

In the experiments, we first compare our
embedding-based features with some traditional
BoW-based features. Then we illustrate the ca-
pability of the two-layer disambiguation model.
After that we compare our embedding technique
EDKate in entity disambiguation tasks with some
other straightforward work-arounds. Finally we
incorporate mention detection and construct a dis-
ambiguation system to compare with other exist-
ing systems.

264



4.1 Data
We take Freebase as the KB, full Wikipedia corpus
as text corpus. For comparison, we also use some
small benchmark corpus for testing purpose.

4.1.1 Wikipedia
We adopt the Wikipedia dumped from Feb. 13th,
2015. With the raw htmls, we first filter out
non-English and non-entity pages. Then we ex-
tract text and anchors information according to
the html templates. After the preprocessing pro-
cedures, we get 4,532,397 pages with 93,299,855
anchors. Furthermore, we split the remained pages
into training, developing and testing sets with pro-
portion 8:1:1. In some experiments, only “valid
entities” will be considered and a “(filtered)” tag
will be added to the name of the dataset. For sta-
tistical summary, please refer to Table 1.

4.1.2 Valid Entities
In some experiments, we limit our KB entities
to the Wikipedia training set and remove enti-
ties which are mentioned less than 3 times in
Wikipedia training set for efficiency. We call the
remaining entities “valid entities”.

4.1.3 Knowledge Base
We use Freebase dumped from Feb. 13th, 2015
as our knowledge base. We only want to link
mentions to Wikipedia entities so we filter out
triplets whose head or tail entity isn’t covered by
Wikipedia. Finally we get 99,980,159 triplets.
If we only consider valid entities, there are
37,606,158.

4.1.4 Small Benchmark Corpus
Besides Wikipedia, we also evaluate our
embedding-based method in some small bench-
mark datasets. KBP 2010 comes from the KBP’s
annual tracks held by TAC and contains only
one mention in one document. AQUAINT is
originally collected by (Milne and Witten, 2008)
and mimics the structure of Wikipedia. MSNBC
is taken from (Cucerzan, 2007) and focuses on
news wire text; ACE is collected by (Ratinov et
al., 2011) from the ACE co-reference dataset. For
statistics in detail, see Table 1.

4.1.5 Difficult Set
We find that in all the data sets, large part of
the examples can be simply well solved by the
mention-entity prior without considering any con-
texts. But there indeed exist some examples the

Dataset # documents # mentions

Wiki:all 4,532,397 93,299,855
Wiki:all (filtered) 2,476,438 52,422,949

Wiki:train (filtered) 1,567,080 37,956,309
Wiki:develop (filtered) 454,906 7,248,850

Wiki:test (filtered) 454,452 7,217,790
Wiki:test (filtered, difficult) 454,452 1,069,428

KBP 2010 1020 1020
KBP 2010 (filtered) 780 780

KBP 2010 (filtered, difficult) 780 183
AQUAINT 50 727

ACE 35 257
MSNBC 20 747

Table 1: Statistics for each corpus

prior cannot work well. We think disambiguation
should pay more attention to this part of exam-
ples rather than the part where prior already works
well. Thus from the testing sets “Wikipedia:test
(filtered)” and “KBP 2010 (filtered)”, we collect
the cases where prior cannot rank the correct en-
tity to top 1 and construct the separate “difficult”
set.

4.2 Embedding Training
We use stochastic gradient descent (SGD) to opti-
mize the objective (see equation (13)). We set the
dimension of word and entity embeddings to 150
and initialize each element of an embedding with
a random number near 0. For the constant b, we
empirically set it to 7.

In knowledge model, we use Freebase as our
knowledge base. We don’t set a fix epoch num-
ber and the knowledge training thread will not ter-
minate until the text training thread stop. Further-
more, we also adapt the learning rate in knowledge
training to that in text training. When a triplet
(h, r, t) is considered, the numbers of negative
samples to construct (h̃, r, t), (h, r̃, t) and (h, r, t̃)
are all 10, in which h̃ and t̃ are uniformly sampled
from E while r̃ is uniformly sampled from R.

In text model, we use the filtered Wikipedia
training set as our text corpus. We set the number
of epoch to 6 and set initial learning rate to 0.025,
which will decreases linearly with the training pro-
cess. When a word is encountered, we take words
inside a 5-word-window as co-occurred words.
For each co-occurred word, we sample 30 nega-
tives from the unigram distribution raised to the
3/4rd power.

In alignment model, “alignment by Wikipedia
anchors” and “alignment by entity names” can be
absorbed into text model and knowledge model re-

265



spectively. For “alignment by entity’s description,
we sample 10 negatives in Pr(e|w) and 30 nega-
tives in Pr(w|e).

For Pr(ea|w) in “url-anchor co-occurrence”,
we sample 20 negatives from the candidate list
of the anchor mention and 10 negatives from the
whole entity set.

To balance the training process, we give knowl-
edge model 10 threads and text model 20 threads.
We adopt the share-memory scheme like (Bordes
et al., 2013) and don’t apply locks.

4.3 Comparison between Embedding-based
and BoW-based Feature

We set up this experiment to exhibit the expres-
siveness of our embeddings. We compare E-GCR
(global context relatedness) with some traditional
BoW-based features. Moreover, in this experi-
ment, we report the results on “difficult set” where
the mention-entity prior fails. Following the same
metric used in (Cucerzan, 2011), we take accuracy
to evaluate the disambiguation performance, that
is, the fraction of all mentions for which the cor-
rect entity is ranked to top 1.

4.3.1 Implementation
For embedding-based features, we only consider
the E-GCR, which is more comparable with the
BoW-based features B-CS and B-TS we use
here because they all consider the whole docu-
ment as context. These BoW-based features in-
clude: (1)Mention-Entity Prior; (2)BoW Con-
text Similarity (B-CS). This feature is proposed
by (Cucerzan, 2011). First, for each entity in
Wikipedia, take all surface forms of anchors in that
page as its representation vector. Then compute
scalar product between this representation vec-
tor and the context word vector of a given men-
tion; (3)BoW Topic Similarity (B-TS). First con-
struct the topic vector for each entity from cate-
gory boxes like (Cucerzan, 2011). Then compute
scalar product between topic vector and context
word vector of a given mention.

4.3.2 Results
From Table 2, we get (1) E-GCR can beat the com-
bination of several BoW-based features. This is
mainly because, embeddings training owns a in-
ner optimizaiton objective and embraces the infor-
mation of these BoW-based representations. (2)
Embedding-based feature appear robust and sig-
nificantly outperform BoW-based features in diffi-

Feature
Wiki:test KBP 2010
(filtered) (filtered)

overall difficult overall difficult

Prior 0.8488 0 0.7645 0
Prior+B-CS 0.8545 0.0587 0.7645 0.0308

Prior+B-CS+B-TS 0.8680 0.1609 0.7907 0.1437
B-CS 0.6375 0.3159 0.3648 0.3210

B-CS+B-TS 0.6793 0.3943 0.5422 0.4491
E-GCR 0.8738 0.5183 0.8445 0.6358

Table 2: Comparison between Embedding-based
Feature and BoW-based Feature

Model Type
Wiki:test KBP 2010
(filtered) (filtered)

overall difficult overall difficult

Linear 0.8671 0.1310 0.7791 0.0617
Two-layer 0.8931 0.4795 0.8474 0.5140

Table 3: Comparison between Linear Disam-
biguation Model and Two-layer Model

cult set, which indicate that these BoW represen-
tation cannot well catch the information in these
cases while embeddings are still expressive . (3)
Unlike the situation in difficult set, the gap be-
tween “E-GCR” and “Prior+B-CS+B-TS” is not
so large mainly because “difficult set” only occupy
a small proportion and prior would cover the draw-
backs of BoW-based features.

This experiment hints us to pay more attention
to the difficult set, which is helpful to improve the
overall performance.

4.4 Comparison between Linear and
Two-layer Disambiguation Model

In this section we evaluate the quality of the
two-layer disambiguation model and compare it
with the linear disambiguation model (Cucerzan,
2011). Moreover, we also report results in the “dif-
ficult set” defined above to see whether our two-
layer model could balance prior and other features
or not. The features we use here are prior and E-
GCR. Accuracy is used as the evaluation metric.

4.4.1 Implementation

We use logistic regression for both models. For
the two-layer model, we first apply the prior clas-
sification and get pconf . Here we set the threshold
ξ to 0.95 according to experiments in development
set.

266



Figure 1: Precision-Recall Curves for Prior Clas-
sification

4.4.2 Results
From Figure 1, we see that the classifier is quite
good at classifying positive cases to the correct
class. From Table 3, we observe that our two-
layer model receive a promising result in overall
and difficult set against the linear model. This ev-
idence indicates the prior classifier works and the
two-layer model can balance the usage of prior and
other features.

4.5 Comparison between Different
Embeddings

This experiment compares our embeddings tech-
nique (EDKate) with some other methods:
(Mikolov et al., 2013b), (Wang et al., 2014a) and
(Zhong and Zhang, 2015). Here, We only consider
the cases given mentions and take accuracy as the
evaluation metric.

4.5.1 Implementation
For (Mikolov et al., 2013b), we directly take their
word2vec model and replace anchor surface with
the entity symbol in the training corpus. In this
way, we get embeddings for words and entities.
For (Wang et al., 2014a) and (Zhong and Zhang,
2015), we completely follow the model in the orig-
inal paper. For our method. We use the knowl-
edge model and text model described in (Wang et
al., 2014a) and combine the alignment techniques
in both of (Wang et al., 2014a) and (Zhong and
Zhang, 2015). Moreover, we add “url-anchor” co-
occurence to the training objective and refine the
negative sampling method by having entities in
candidates list more probable to be sampled. In
this experiment, we only use E-GCR as our fea-
ture for simplicity.

4.5.2 Results
From Table 4, we observe that (Wang et al., 2014a)
outperform (Mikolov et al., 2013b), which indi-
cates the introduction of some structure informa-

Embedding Version Wiki: test KBP 2010(filtered) (filtered)

(Mikolov et al., 2013b) 0.8062 0.7311
(Wang et al., 2014a) 0.8283 0.7922

(Zhong and Zhang, 2015) 0.8355 0.7965
EDKate 0.8738 0.8445

Table 4: Comparison between Different Embed-
dings

Method Accuracy on KBP 2010

(Lehmann et al., 2010) 0.806
(He et al., 2013) 0.809
(Sun et al., 2015) 0.839
(Cucerzan, 2011) 0.873

EDKate 0.889

Table 5: Comparison with other reported results
on KBP 2010

tion like knowledge base is quite beneficial. And
the utilization of description message for enti-
ties improve the performance as well. For our
method EDKate, we further take advantages of
“url-anchor” co-occurrence and special sampling
method, which make embeddings more expressive
and guarantee the performance.

4.6 Comparison with Reported Results on
KBP 2010

In this section, we will compare our result on KBP
2010 with other existing reported results, in which
Cucerzan (2011) holds the best record in KBP
2010 so far; Lehmann et al. (2010) rank first in the
2010 competition while He et al. (2013) and Sun
et al. (2015) adopt neural-network-based methods.
We still use accuracy as the evaluation metric be-
cause KBP 2010 specifies the input mentions. Be-
cause some papers only report the accuracy with
3 decimal places, we unify all results to 3 decimal
places.

4.6.1 Implementation
We take the dataset “Wikipedia:all” to train em-
beddings here and use all features we defined in
section 3.2. In this experiment, we adopt the unfil-
tered version of KBP 2010 as the test corpus.

4.6.2 Results
Table 5 shows that EDKate outperforms the cur-
rent best record (Cucerzan, 2011) in KBP 2010
dataset. Sun et al. (2015) apply convolution neu-
ral network and take advantages of (Mikolov et al.,
2013a)’s word2vec as input but it seems not so ef-

267



System AQUAINT ACE MSNBC

WikipediaMiner 0.8361 0.7276 0.6849
Wikifier v1 0.8394 0.7725 0.7488

EDKate 0.8515 0.8079 0.7550
Wikifier v2 0.8888 0.8530 0.8120

Table 6: Comparison with other Wikification sys-
tems in BoT F1 metric

fective as ours, which shows the importance of the
embedding quality in this disambiguation task.

4.7 Comparison with Other Wikification
Systems

In this section, we equip EDKate with mention de-
tection and compare our system with Wikipedi-
aMiner (Milne and Witten, 2008), Wikifier v1
(Ratinov et al., 2011) and Wikifier v2 (Cheng and
Roth, 2013). For the evaluation metric, we adopt
the Bag-of-Title (BoT) F1 evaluation metric which
is used in all other systems we choose here.

4.7.1 Implementation
We first make use of all mentions in the mention-
entity table to construct a Trie-tree, which is used
to detect mentions in input text. To remove noise,
we simply retain mentions which contain at least
one noun and filter mentions that completely con-
sist of stop words. Then we apply our disambigua-
tion technique to the mentions detected. The same
as experiment 4.6, we make use of all features de-
scribed in section 3.2 here.

4.7.2 Results
Table 6 shows that our embedding-based method
EDKate is better than two popular systems but
cannot outperform Wikifier v2 in these three
datasets. It should be mentioned that Wikifier v2
is largely based on Wikifier v1 and its magic is
to add relational inference with some handcrafted
rules. Actually, the embedding methods can per-
forms well to model relations (Wang et al., 2014a),
so the idea to introduce relational information into
our current framework is promising and will be the
future work.

5 Conclusion

In this paper, we propose to refine a knowledge
and text joint learning framework for entity dis-
ambiguation tasks and learn semantics-rich em-
beddings for words and entities. Then we design
some simple embedding-based features and build

a two-layer disambiguation model. Extensive ex-
periments show that our embeddings are very ex-
pressive and is quite helpful in the entity disam-
biguation tasks.

References
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and

Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.

Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning structured embed-
dings of knowledge bases. In Conference on Artifi-
cial Intelligence, number EPFL-CONF-192344.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in Neural Information
Processing Systems, pages 2787–2795.

Razvan C Bunescu and Marius Pasca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proceedings of European Chapter of the As-
sociation for Computational Linguistics, volume 6,
pages 9–16.

Taylor Cassidy, Zheng Chen, Javier Artiles, Heng Ji,
Hongbo Deng, Lev-Arie Ratinov, Jing Zheng, Ji-
awei Han, and Dan Roth. 2011. Cuny-uiuc-sri tac-
kbp2011 entity linking system description. In Pro-
ceedings of Text Analysis Conference.

Xiao Cheng and Dan Roth. 2013. Relational inference
for wikification. In Proceedings of Conference on
Empirical Methods in Natural Language Process-
ing.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, volume 7, pages 708–
716.

Silviu Cucerzan. 2011. Tac entity linking by perform-
ing full-document entity extraction and disambigua-
tion. In Proceedings of Text Analysis Conference,
volume 2011.

Anthony Fader, Stephen Soderland, Oren Etzioni, and
Turing Center. 2009. Scaling wikipedia-based
named entity disambiguation to arbitrary web text.
In Proceedings of the International Joint Confer-
ences on Artificial Intelligence Workshop on User-
contributed Knowledge and Artificial Intelligence:
An Evolving Synergy, Pasadena, CA, USA, pages
21–26.

268



Zhengyan He, Shujie Liu, Mu Li, Ming Zhou, Longkai
Zhang, and Houfeng Wang. 2013. Learning entity
representation for entity disambiguation. In ACL
(2), pages 30–34.

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fürstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 782–792.

Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective annota-
tion of wikipedia entities in web text. In Proceed-
ings of Special Interest Group on Knowledge Dis-
covery and data Mining, pages 457–466.

Quoc V Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. arXiv
preprint arXiv:1405.4053.

John Lehmann, Sean Monahan, Luke Nezda, Arnold
Jung, and Ying Shi. 2010. Lcc approaches to
knowledge base population at tac 2010. In Proceed-
ings of Text Analysis Conference.

Christopher D Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural lan-
guage processing toolkit. In Proceedings of Asso-
ciation for Computational Linguistics, pages 55–60.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed represen-
tations of words and phrases and their composition-
ality. In Advances in neural information processing
systems, pages 3111–3119.

David Milne and Ian H Witten. 2008. Learning to link
with wikipedia. In Proceedings of Information and
knowledge management, pages 509–518.

Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to wikipedia. In Proceedings of Asso-
ciation for Computational Linguistics, pages 1375–
1384.

Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
sor networks for knowledge base completion. In Ad-
vances in Neural Information Processing Systems,
pages 926–934.

Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhen-
zhou Ji, and Xiaolong Wang. 2015. Modeling men-
tion, context and entity with neural networks for en-
tity disambiguation. In Proceedings of the Twenty-
Fourth International Joint Conference on Artificial
Intelligence (IJCAI), pages 1333–1339.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014a. Knowledge graph and text jointly em-
bedding. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics,
pages 1591–1601.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014b. Knowledge graph embedding by
translating on hyperplanes. In Proceedings of As-
sociation for the Advancement of Artificial Intelli-
gence, pages 1112–1119.

Huaping Zhong and Jianwen Zhang. 2015. Aligning
knowledge and text embeddings by entity descrip-
tions. In Proceedings of Conference on Empirical
Methods in Natural Language Processing.

269


