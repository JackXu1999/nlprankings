



















































CoNLL-2017 Shared Task


Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 119–125,
Vancouver, Canada, August 3-4, 2017. c© 2017 Association for Computational Linguistics

TurkuNLP: Delexicalized Pre-training of Word Embeddings for
Dependency Parsing

Jenna Kanerva1,2, Juhani Luotolahti1,2, and Filip Ginter1
1Turku NLP Group

2University of Turku Graduate School (UTUGS)
University of Turku, Finland

jmnybl@utu.fi, mjluot@utu.fi, figint@utu.fi

Abstract

We present the TurkuNLP entry in the
CoNLL 2017 Shared Task on Multilingual
Parsing from Raw Text to Universal De-
pendencies. The system is based on the
UDPipe parser with our focus being in ex-
ploring various techniques to pre-train the
word embeddings used by the parser in or-
der to improve its performance especially
on languages with small training sets. The
system ranked 11th among the 33 partici-
pants overall, being 8th on the small tree-
banks, 10th on the large treebanks, 12th on
the parallel test sets, and 26th on the sur-
prise languages.

1 Introduction

In this paper we describe the TurkuNLP entry
in the CoNLL 2017 Shared Task on Multilin-
gual Parsing from Raw Text to Universal De-
pendencies (Zeman et al., 2017). The Univer-
sal Dependencies (UD) treebank collection (Nivre
et al., 2017b) has 70 treebanks for 50 languages
with cross-linguistically consistent annotation. Of
these, the 63 treebanks which have at least 10,000
tokens in their test section are used for training and
testing the systems. Further, a parallel corpus con-
sisting of 1,000 sentences in 14 languages was de-
veloped as an additional test set, and finally, the
shared task included test sets for four “surprise”
languages not known until a week prior to the test
phase of the shared task (Nivre et al., 2017a). No
training data was provided for these languages —
only a handful of sentences was given as an ex-
ample. As an additional novelty, participation in
the shared task involved developing an end-to-end
parsing system, from raw text to dependency trees,
for all of the languages and treebanks. The partic-
ipants were provided with automatically predicted

word and sentence segmentation as well as mor-
phological tags for the test sets, which they could
choose to use as an alternative to developing own
segmentation and tagging. These baseline seg-
mentations and morphological analyses were pro-
vided by UDPipe v1.1 (Straka et al., 2016).

In addition to the manually annotated treebanks,
the shared task organizers also distributed a large
collection of web-crawled text for all but one of
the languages in the shared task, totaling over 90
billion tokens of fully dependency parsed data.
Once again, these analyses were produced by the
UDPipe system. This automatically processed
large dataset was intended by the organizers to
complement the manually annotated data and, for
instance, support the induction of word embed-
dings.

As an overall strategy for the shared task, we
chose to build on an existing parser and focus
on exploring various methods of pre-training the
parser and especially its embeddings, using the
large, automatically analyzed corpus provided by
the organizers. We expected this strategy to be
particularly helpful for languages with only a lit-
tle training data. On the other hand we put only
a minimal effort into the surprise languages. We
also chose to use the word and sentence segmen-
tation of the test datasets, as provided by the or-
ganizers. As we will demonstrate, the results of
our system correlate with the focus of our efforts.
Initially, we focused on the latest ParseySaurus
parser (Alberti et al., 2017), but due to the magni-
tude of the task and restrictions on time, we finally
used the UDPipe parsing pipeline of Straka et al.
(2016) as the basis of our efforts.

2 Word Embeddings

The most important component of our system
is the novel techniques we used to pre-train the

119



word embeddings. Our word embeddings com-
bine three important aspects: 1) delexicalized syn-
tactic contexts for inducing word embeddings, 2)
word embeddings built from character n-grams,
and 3) post-training injection and modification of
embeddings for unseen words.

2.1 Delexicalized Syntactic Contexts
Word embeddings induced from large text corpora
have been a key resource in many NLP task in
recent years. In many common tools for learn-
ing word embeddings, such as word2vec (Mikolov
et al., 2013), the context for a focus word is a slid-
ing window of words surrounding the focus word
in linear order. Levy and Goldberg (2014) extend
the context with dependency trees, where the con-
text is defined as the words nearby in the depen-
dency tree with additionally the dependency rela-
tion attached to the context words, for example in-
terresting/amod.

In Kanerva et al. (2017), we show that word
embeddings trained in a strongly syntactic fash-
ion outperform standard word2vec embeddings in
dependency parsing. In particular, the context is
fully delexicalized — instead of using words in the
word2vec output layer, only part-of-speech tags,
morphological features and syntactic functions are
predicted. This delexicalized syntactic context is
shown to lead to higher performance as well as
generalize better across languages.

In our shared task submission we build on top
of the previous work and optimize the embeddings
even closer to the parsing task: We extend the orig-
inal delexicalized context to also predict parsing
actions of a transition-based parser. From the ex-
isting parse trees in the raw data collection, we
create the transition sequence used to produce the
tree and for each word collect features describing
the actions taken when the word is on top-2 posi-
tions of the stack, e.g. if the focus word is first on
stack, what is the next action. Our word–context
pairs are illustrated in Table 1. In this way, we
strive to build embeddings which relate together
words which appear in similar configurations of
the parser.

2.2 Word embeddings from character
n-grams

It is known that the initial embeddings affect the
performance of neural dependency parsers and
pre-training the embedding matrix prior to train-
ing has an important effect on the performance of

interesting question
amod

root

input context
interesting ADJ
interesting Degree=Pos
interesting amod
question NOUN
question Number=Sing
question root
question Dep amod

interesting stack1 shift
interesting stack2 left-arc
interesting stack2 left amod
question stack1 left-arc
question stack1 left amod
question stack1 root

character four-grams
interesting, char $int, char inte, char nter,
char tere, char eres, char rest, char esti,
char stin, char ting, char ing$
question, char $que, char ques, char uest
char esti, char stio, char tion, char ion$

Table 1: Delexicalized contexts and character n-
grams for word embeddings

neural systems (Chen and Manning, 2014; Andor
et al., 2016).

Since the scope of languages in the task is large,
the embeddings used for this dependency pars-
ing task need to be able to be representative of
languages with both large and small available re-
sources, and also they need to be able to cap-
ture morphological information for languages with
complex morphologies as well as those with less
morphological variation.

To address better the needs of small languages
with very little of data available as well as mor-
phologically rich languages, we build our word
embedding models with methods used in the pop-
ular fastText1 representation of Bojanowski et al.
(2016). They suggest, instead of tokens, to use
character n-grams as the basic units in building
embeddings for words. First, words are turned into
character n-grams and embeddings are learned
separately for each of these character n-grams.
Secondly, word vectors are assembled from these

1https://github.com/facebookresearch/
fastText

120



trained character embeddings by averaging all
character n-grams present in a word. To make
the embeddings more informative, the n-grams in-
clude special markers for the beginning and the
end of the token, allowing the model for exam-
ple to learn special embeddings for word suffixes
which are often used as inflectional markers. In
addition to the n-grams, a vector for the full word
is also trained, and when final word vectors are
produced, the full word vector is treated similarly
as other n-grams and is averaged as part of the fi-
nal product along with the rest. This allows for the
potential benefits of token level embeddings to be
materialized in our model.

Table 1 demonstrates the splitting of a word
into character four-grams with special start and
end markers. When learning embeddings for
these character n-grams, context for each n-gram
is replicated from the original word context, i.e.
each character n-gram created from the word in-
teresting gets the delexicalized context assigned
for that word, namely ADJ, Degree=Pos, amod,
stack1 shift, stack2 left-arc, stack2 left amod in
the example Table 1.

This procedure offers multiple advantages. One
of them is the ability to construct embeddings for
previously unseen words, a common occurrence
especially with languages with small training cor-
pora. With these character n-gram embeddings
we are basically able to build an embedding for
any word, except very few cases where we do
not find any of the character n-grams from our
trained model. Another advantage of this em-
bedding scheme is its better ability to address the
morphological variation compared to plain token
based embeddings.

2.3 Data and Parameters

For the training of word embeddings for each lan-
guage we took training data from the treebank
training section and the automatically analyzed
raw corpus (Ginter et al., 2017). Using also the
treebank training section is important for very
small languages where there is very little of raw
data, especially for Old Church Slavonic where
the raw data has only 29,000 tokens compared to
37,500 tokens in the treebank training set, while
for big languages it barely makes any difference as
the treebank data gets buried under the mass of raw
data. For each language we build character n-gram

embedding models using word2vecf software2 by
Levy and Goldberg (2014) with negative sam-
pling, skip-gram architecture, embeddings dimen-
sionality of 100, delexicalized syntactic contexts
explained in Section 2.1 and character n-grams of
length 3-6. The maximum size of raw data used
is limited to 50 million unique sentences in or-
der to keep the training times bearable, and sen-
tences longer that 30 tokens are discarded. Lan-
guages with only very limited resources we run 10
training iterations, but for rest only one iteration
is used. These character n-gram models explained
in detail in Section 2.2 can then be used to build
word embeddings for arbitrary words, only requir-
ing that at least one of the extracted character n-
grams is present in our embedding model.

For parsing we also used the parameters opti-
mized for UDPipe baseline system and changed
only parts related to pre-trained embeddings. As
we do not perform further parameter optimiza-
tion, we trained our models always using the full
training set, also in cases where different devel-
opment sets were not provided. For small tree-
banks without development data, we did not test
our models in advance but trusted methods tested
on other treebanks to generalize also for these. For
each language we include 100-dim word embed-
dings trained using our methods described in Sec-
tions 2.1 and 2.2. Additionally, pre-trained fea-
ture embeddings are trained for upos+feat com-
binations included in the xpostag column. These
embeddings are trained using the transition actions
as delixicalized context, and vectors for full fea-
ture combinations are constructed from individual
features using the same character n-gram method
as in the word embeddings (one feature is now the
same as one character n-gram).

3 Parsing Pipeline

Our submission builds on top of the UDPipe pars-
ing pipeline by Straka et al. (2016). We use data
segmented by the UDPipe baseline systems as our
system input, and then morphological analysis and
syntactic parses are produced with our own UD-
Pipe models. The UDPipe morphological tagger
(MorphoDiTa (Straková et al., 2014)) is run as-is
with parameters optimized for the baseline system
(released together with the baseline models). The
only exception is that we replaced the language-
specific postag (xpostag) column with a combined

2https://github.com/BIU-NLP/word2vecf

121



universal postag (upostag) and morphological fea-
tures (feats), and trained the tagger to produce this
instead of the language-specific postags. We did
not expect this to affect the tagging accuracy, but
instead it was used to provide pre-trained feature
embeddings for the parser.

We further modified the UDPipe parser to allow
including new embedding matrices after the model
has been trained. This gives us an easy way to add
embeddings for arbitrary words without a need of
training a new parsing model. As we are able to
create word embeddings for almost any word us-
ing our character n-gram embeddings described in
Section 2.2, we are able to collect vocabulary from
the data we are parsing, create vectors for previ-
ously unseen words and inject these vectors into
the parsing model. This method essentially elimi-
nates all out of vocabulary words.

3.1 Post-training modifications of word
embeddings

The UDPipe parser uses the common technique
of adjusting the word embeddings during train-
ing. The magnitude of the change imposed by
the parser depends on the frequency of the word
in the training data and, naturally, only words seen
in the training set are subject to this training-phase
adjustment. Therefore, we implemented a step
whereby we transfer the parser adjustments onto
the words not seen in the training data. For ev-
ery such unseen word, we calculate its transla-
tion in the vector space by summing the parser-
induced changes of known words in its neighbor-
hood. These are weighted by their cosine similar-
ity with the unknown word, using a linear function
mapping similarities in the [0.5, 1.0] interval into
weights in the [0.0, 1.0] range. I.e. known words
with similarity below 0.5 do not contribute, and
thereafter the weighting is linear. The overall ef-
fect of this modification observed in our develop-
ment runs was marginal.

3.2 Parallel test sets (PUD)
Parallel test sets are parsed with a model trained on
the default treebank of a language (the one without
any treebank-specific suffix). For many languages
only one treebank exists and no choice is needed,
but for some there are two or even more choices.
We chose to use these treebanks without tree-
bank suffixes as the very first treebank included
for a language will receive just the language code
without the treebank suffix while newer treebanks

will get a distinguishable treebank suffix. It then
means that the default treebanks without suffixes
have been part of the UD collection longer, many
of these originating from the Google’s Universal
Treebank collection (McDonald et al., 2013). We
hypothesized these treebanks to be more harmo-
nized to the UD guidelines and apply better to the
new test sets.

3.3 Surprise languages

In this work we did not concentrate on parsing
the four surprise languages, and only used a very
naive approach to complete the submission of all
required test sets. For each surprise language we
simply picked one existing model among all mod-
els trained for regular treebanks. We parsed the
small sample of example sentences (about 20 sen-
tences for each language) with all existing mod-
els, and picked the one which maximized the LAS
score (Kazakh for Buryat, Galician-TreeGal for
Kurmanji, Portuguese for North Sami and Slove-
nian for Upper Sorbian) without doing any tree-
bank size, language family or related language
evaluation. The only change in the parsing model
is that during parsing, we mask all word embed-
dings, this way preventing the parser to use the
vector for unknown word too often. This makes
our parsing model delexicalized as all word em-
beddings are zeroed after training and not used in
parsing, with the exception that parsing model is
trained using information from word embeddings.

4 Results

The participating systems are tested using TIRA
platform (Potthast et al., 2014), where the system
must be deployed on a virtual machine and the test
sets are processed without direct access to the data.
Overall rank of our system in the official evalu-
ation is 11th out of 33 participating teams with
a macro-averaged labeled attachment score (LAS)
of 68.59%.3 On macro-LAS score across all tree-
banks, we are clearly behind the winning system
(Stanford, 76.30%), but our pre-trained word em-
beddings are able to improve over the baseline
UDPipe by 0.24% points on average. When look-
ing only at treebanks with very little of training
data we gain on average 2.38% over the baseline
system. The same number for the big treebanks

3Full list of the official results can be found
at http://universaldependencies.org/
conll17/results.html.

122



only is +1.15%, +0.23% for parallel test sets and
-16.55% for surprise languages. Based on these
numbers we can clearly see that we managed to
get a substantial improvement over the baseline
system on very small languages where we also as-
sumed our word embeddings to be most helpful.
Instead, our very naive approach for handling sur-
prise languages is clearly not sufficient, and a bet-
ter approach should have been implemented. De-
tailed results of our system are shown in Table 2.

On official evaluation our system ranked sixth
on universal part-of-speech tagging and second
on morphological features. We see modest im-
provement of +0.22% (upos) and +0.27% (feats)
over the baseline models. As word embeddings
are not used in tagging and we use the same pa-
rameters as the baseline system, the only modifi-
cation we did in tagging is that instead of using
language-specific postags (xpostag), we concate-
nated universal postag and morphological features
into xpostag column and trained the tagger to pro-
duce this concatenation.

5 Conclusions

We have presented our entry in the CoNLL 2017
Shared Task on Multilingual Parsing from Raw
Text to Universal Dependencies, with the 11th
rank of 33 participants. During the development,
we have focused on exploring various ways of
pre-training the embeddings used by the parser,
as well as providing embeddings also for the
unknown words in the data to be parsed. In
particular, we have proposed a method of pre-
training the embeddings using an entirely delex-
icalized output layer of the word2vec skip-gram
model. This mode of pre-training the embed-
dings is shown to be superior to the usual approach
of pre-training with the standard word2vec skip-
gram with negative sampling. We have also ex-
plored, here with only a minimal improvement to
the score, the possibility of post-hoc application
of the adjustments effected by the parser on the
word embeddings during the training phase. All
our components used in this paper are freely avail-
able at https://github.com/TurkuNLP/
conll17-system.

Acknowledgments

This work was supported by the Finnish Academy
and the Kone Foundation. Computational re-
sources were provided by CSC – IT Center for

Science, Finland.

References
Chris Alberti, Daniel Andor, Ivan Bogatyy, Michael

Collins, Dan Gillick, Lingpeng Kong, Terry Koo,
Ji Ma, Mark Omernick, Slav Petrov, et al. 2017.
Syntaxnet models for the conll 2017 shared task.
arXiv preprint arXiv:1703.04929 .

Daniel Andor, Chris Alberti, David Weiss, Aliaksei
Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
malized transition-based neural networks. arXiv
preprint arXiv:1603.06042 .

Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-
tors with subword information. arXiv preprint
arXiv:1607.04606 .

Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In EMNLP. pages 740–750.

Filip Ginter, Jan Hajič, Juhani Luotolahti, Milan
Straka, and Daniel Zeman. 2017. CoNLL 2017
shared task - automatically annotated raw texts
and word embeddings. LINDAT/CLARIN
digital library at the Institute of Formal
and Applied Linguistics, Charles University.
http://hdl.handle.net/11234/1-1989.

Jenna Kanerva, Sampo Pyysalo, and Filip Ginter.
2017. Delexicalized contexts for pure dependency-
based word embeddings. In Proceedings of the In-
ternational Conference on Dependency Linguistics
(Depling’17). Under review.

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of ACL.

Ryan T McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith B Hall, Slav Petrov, Hao Zhang, Os-
car Täckström, et al. 2013. Universal dependency
annotation for multilingual parsing. In ACL (2).
pages 92–97.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proceedings of ICLR.

Joakim Nivre, Željko Agić, Lars Ahrenberg, et al.
2017a. Universal dependencies 2.0 CoNLL 2017
shared task development and test data. LIN-
DAT/CLARIN digital library at the Institute of For-
mal and Applied Linguistics, Charles University.
http://hdl.handle.net/11234/1-2184.

Joakim Nivre et al. 2017b. Universal Dependencies
2.0. LINDAT/CLARIN digital library at the Insti-
tute of Formal and Applied Linguistics, Charles Uni-
versity, Prague, http://hdl.handle.net/
11234/1-1983. http://hdl.handle.net/11234/1-
1983.

123



Big Treebanks
Treebank Rank LAS F1 Treebank Rank LAS F1
ar 17 65.74 hr 9 78.57 
bg 11 84.85 hu 10 65.61 
ca 11 85.64 id 12 74.87 
cs 13 83.48 it 15 85.66 
cs_cac 8 84.28 ja 15 72.81 
cs_cltt 11 73.83 ko 11 66.93 
cu 11 65.43 la_ittb 9 78.99 
da 12 74.61 la_proiel 11 59.86 
de 17 69.32 lv 8 62.13 
el 13 79.93 nl 14 69.59 
en 11 76.68 nl_lassysmall 14 79.06 
en_lines 8 74.77 no_bokmaal 13 83.60 
en_partut 13 74.48 no_nynorsk 10 82.35
es 15 81.79 pl 12 80.11 
es_ancora 15 84.15 pt 10 82.91 
et 10 59.79 pt_br 12 86.36 
eu 13 70.22 ro 10 80.71 
fa 20 76.54 ru 17 74.69 
fi 9 75.82 ru_syntagrus 13 86.79 
fi_ftb 9 75.59 sk 12 74.72 
fr 12 80.61 sl 8 82.77 
fr_sequoia 10 81.12 sv 12 77.35
gl 17 77.66 sv_lines 13 74.46 
got 9 61.52 tr 13 54.69 
grc 8 59.83 ur 14 77.06 
grc_proiel 7 68.04 vi 14 38.07 
he 16 57.50 zh 12 58.71
hi 7 87.75 

All 10 74.19 
Parellel Test Sets Surprise Test Sets
Treebank Rank LAS F1 Treebank Rank LAS F1
ar_pud 27 42.34 bxr 26 14.22 
cs_pud 11 80.02 hsb 25 34.67 
de_pud 17 66.78 kmr 23 22.19 
en_pud 9 79.61 sme 27 10.99 
es_pud 12 78.02 All 26 20.52 
fi_pud 8 79.61 
fr_pud 14 74.17 Small Treebanks
hi_pud 8 51.87 Treebank Rank LAS F1
it_pud 12 84.18 fr_partut 12 78.83 
ja_pud 19 76.09 ga 7 64.25 
pt_pud 12 74.09 gl_treegal 9 66.47 
ru_pud 8 69.11 kk 2 28.31 
sv_pud 16 69.90 la 9 47.91
tr_pud 15 34.09 la_ittb 9 78.99 
All 12 68.56 la_proiel 11 59.86 

sl_sst 11 47.50 
All treebanks ug 6 36.51 
Treebank Rank LAS F1 uk 7 63.70 
All 11 68.59 All 8 54.18 

Table 2: Rank and labeled attachment score for our system in the official evaluation.

124



Martin Potthast, Tim Gollub, Francisco Rangel, Paolo
Rosso, Efstathios Stamatatos, and Benno Stein.
2014. Improving the reproducibility of PAN’s
shared tasks: Plagiarism detection, author iden-
tification, and author profiling. In Evangelos
Kanoulas, Mihai Lupu, Paul Clough, Mark Sander-
son, Mark Hall, Allan Hanbury, and Elaine Toms,
editors, Information Access Evaluation meets Mul-
tilinguality, Multimodality, and Visualization. 5th
International Conference of the CLEF Initiative
(CLEF 14). Springer, Berlin Heidelberg New York,
pages 268–299. https://doi.org/10.1007/978-3-319-
11382-1 22.

Milan Straka, Jan Hajič, and Jana Straková. 2016. UD-
Pipe: trainable pipeline for processing CoNLL-U
files performing tokenization, morphological anal-
ysis, POS tagging and parsing. In Proceedings
of the 10th International Conference on Language
Resources and Evaluation (LREC 2016). European
Language Resources Association, Portoro, Slovenia.

Jana Straková, Milan Straka, and Jan Hajič. 2014.
Open-Source Tools for Morphology, Lemmatiza-
tion, POS Tagging and Named Entity Recogni-
tion. In Proceedings of 52nd Annual Meeting
of the Association for Computational Linguistics:
System Demonstrations. Association for Computa-
tional Linguistics, Baltimore, Maryland, pages 13–
18. http://www.aclweb.org/anthology/P/P14/P14-
5003.pdf.

Daniel Zeman, Martin Popel, Milan Straka, Jan
Hajič, Joakim Nivre, Filip Ginter, Juhani Luotolahti,
Sampo Pyysalo, Slav Petrov, Martin Potthast, Fran-
cis Tyers, Elena Badmaeva, Memduh Gökırmak,
Anna Nedoluzhko, Silvie Cinková, Jan Hajič jr.,
Jaroslava Hlaváčová, Václava Kettnerová, Zdeňka
Urešová, Jenna Kanerva, Stina Ojala, Anna Mis-
silä, Christopher Manning, Sebastian Schuster, Siva
Reddy, Dima Taji, Nizar Habash, Herman Leung,
Marie-Catherine de Marneffe, Manuela Sanguinetti,
Maria Simi, Hiroshi Kanayama, Valeria de Paiva,
Kira Droganova, Hěctor Martı́nez Alonso, Hans
Uszkoreit, Vivien Macketanz, Aljoscha Burchardt,
Kim Harris, Katrin Marheinecke, Georg Rehm,
Tolga Kayadelen, Mohammed Attia, Ali Elkahky,
Zhuoran Yu, Emily Pitler, Saran Lertpradit, Michael
Mandl, Jesse Kirchner, Hector Fernandez Alcalde,
Jana Strnadova, Esha Banerjee, Ruli Manurung, An-
tonio Stella, Atsuko Shimada, Sookyoung Kwak,
Gustavo Mendonça, Tatiana Lando, Rattima Nitis-
aroj, and Josie Li. 2017. CoNLL 2017 Shared Task:
Multilingual Parsing from Raw Text to Universal
Dependencies. In Proceedings of the CoNLL 2017
Shared Task: Multilingual Parsing from Raw Text to
Universal Dependencies. Association for Computa-
tional Linguistics.

125


