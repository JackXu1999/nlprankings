



















































Generative Event Schema Induction with Entity Disambiguation


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 188–197,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Generative Event Schema Induction with Entity Disambiguation

Kiem-Hieu Nguyen1, 2 Xavier Tannier3, 1 Olivier Ferret2 Romaric Besançon2
(1) LIMSI-CNRS

(2) CEA, LIST, Laboratoire Vision et Ingnierie des Contenus, F-91191, Gif-sur-Yvette
(3) Univ. Paris-Sud

{nguyen,xtannier}@limsi.fr, {olivier.ferret,romaric.besancon}@cea.fr

Abstract

This paper presents a generative model to
event schema induction. Previous meth-
ods in the literature only use head words
to represent entities. However, elements
other than head words contain useful in-
formation. For instance, an armed man
is more discriminative than man. Our
model takes into account this information
and precisely represents it using proba-
bilistic topic distributions. We illustrate
that such information plays an important
role in parameter estimation. Mostly, it
makes topic distributions more coherent
and more discriminative. Experimental
results on benchmark dataset empirically
confirm this enhancement.

1 Introduction

Information Extraction was initially defined (and
is still defined) by the MUC evaluations (Grish-
man and Sundheim, 1996) and more specifically
by the task of template filling. The objective of
this task is to assign event roles to individual tex-
tual mentions. A template defines a specific type
of events (e.g. earthquakes), associated with se-
mantic roles (or slots) hold by entities (for earth-
quakes, their location, date, magnitude and the
damages they caused (Jean-Louis et al., 2011)).

Schema induction is the task of learning these
templates with no supervision from unlabeled text.
We focus here on event schema induction and con-
tinue the trend of generative models proposed ear-
lier for this task. The idea is to group together
entities corresponding to the same role in an event
template based on the similarity of the relations
that these entities hold with predicates. For ex-
ample, in a corpus about terrorist attacks, enti-
ties that are objects of verbs to kill, to attack can
be grouped together and characterized by a role

named VICTIM. The output of this identification
operation is a set of clusters of which members
are both words and relations, associated with their
probability (see an example later in Figure 4).
These clusters are not labeled but each of them
represents an event slot.

Our approach here is to improve this initial idea
by entity disambiguation. Some ambiguous enti-
ties, such as man or soldier, can match two differ-
ent slots (victim or perpetrator). An entity such as
terrorist can be mixed up with victims when arti-
cles relate that a terrorist has been killed by police
(and thus is object of to kill). Our hypothesis is
that the immediate context of entities is helpful for
disambiguating them. For example, the fact that
man is associated with armed, dangerous, heroic
or innocent can lead to a better attribution and def-
inition of roles. We then introduce relations be-
tween entities and their attributes in the model by
means of syntactic relations.

The document level, which is generally a cen-
ter notion in topic modeling, is not used in our
generative model. This results in a simpler, more
intuitive model, where observations are generated
from slots, that are defined by probabilistic dis-
tributions on entities, predicates and syntactic at-
tributes. This model offers room for further exten-
sions since multiple observations on an entity can
be represented in the same manner.

Model parameters are estimated by Gibbs sam-
pling. We evaluate the performance of this ap-
proach by an automatic and empiric mapping be-
tween slots from the system and slots from the ref-
erence in a way similar to previous work in the
domain.

The rest of this paper is organized as follows:
Section 2 briefly presents previous work; in Sec-
tion 3, we detail our entity and relation represen-
tation; we describe our generative model in Sec-
tion 4, before presenting our experiments and eval-
uations in Section 5.

188



2 Related Work

Despite efforts made for making template fill-
ing as generic as possible, it still depends heav-
ily on the type of events. Mixing generic
processes with a restrictive number of domain-
specific rules (Freedman et al., 2011) or exam-
ples (Grishman and He, 2014) is a way to reduce
the amount of effort needed for adapting a sys-
tem to another domain. The approaches of On-
demand information extraction (Hasegawa et al.,
2004; Sekine, 2006) and Preemptive Information
Extraction (Shinyama and Sekine, 2006) tried to
overcome this difficulty in another way by exploit-
ing templates induced from representative docu-
ments selected by queries.

Event schema induction takes root in work
on the acquisition from text of knowledge struc-
tures, such as the Memory Organization Pack-
ets (Schank, 1980), used by early text under-
standing systems (DeJong, 1982) and more re-
cently by Ferret and Grau (1997). First attempts
for applying such processes to schema induc-
tion have been made in the fields of Informa-
tion Extraction (Collier, 1998), Automatic Sum-
marization (Harabagiu, 2004) and event Question-
Answering (Filatova et al., 2006; Filatova, 2008).

More recently, work after (Hasegawa et al.,
2004) has developed weakly supervised forms
of Information Extraction including schema in-
duction in their objectives. However, they have
been mainly applied to binary relation extraction
in practice (Eichler et al., 2008; Rosenfeld and
Feldman, 2007; Min et al., 2012). In parallel,
several approaches were proposed for perform-
ing specifically schema induction in already ex-
isting frameworks: clause graph clustering (Qiu
et al., 2008), event sequence alignment (Reg-
neri et al., 2010) or LDA-based approach relying
on FrameNet-like semantic frames (Bejan, 2008).
More event-specific generative models were pro-
posed by Chambers (2013) and Cheung et al.
(2013). Finally, Chambers and Jurafsky (2008),
Chambers and Jurafsky (2009), Chambers and Ju-
rafsky (2011), improved by Balasubramanian et al.
(2013), and Chambers (2013) focused specifically
on the induction of event roles and the identifica-
tion of chains of events for building representa-
tions from texts by exploiting coreference resolu-
tion or the temporal ordering of events. All this
work is also linked to work about the induction of
scripts from texts, more or less closely linked to

Attributes Head Triggers
#1 [armed:amod] man [attack:nsubj,

kill:nsubj]
#2 [police:nn] station [attack:dobj]
#3 [] policeman [kill:dobj]
#4 [innocent:amod, man [wound:dobj]

young:amod]

Figure 1: Entity representation as tuples of ([at-
tributes], head, [triggers]).

events, such as (Frermann et al., 2014), (Pichotta
and Mooney, 2014) or (Modi and Titov, 2014).

The work we present in this article is in line
with Chambers (2013), which will be described in
more details in Section 5, together with a quanti-
tative and qualitative comparison.

3 Entity Representation

An entity is represented as a triple containing: a
head word h, a list A of attribute relations and a
list T of trigger relations. Consider the following
example:

(1) Two armed men attacked the police station
and killed a policeman. An innocent young
man was also wounded.

As illustrated in Figure 1, four entities, equiva-
lent to four separated triples, are generated from
the text above. Head words are extracted from
noun phrases. A trigger relation is composed
of a predicate (attack, kill, wound) and a depen-
dency type (subject, object). An attribute rela-
tion is composed of an argument (armed, police,
young) and a dependency type (adjectival, nomi-
nal or verbal modifier). In the relationship to trig-
gers, a head word is argument, but in the relation-
ship to attributes, it is predicate. We use Stanford
NLP toolkit (Manning et al., 2014) for parsing and
coreference resolution.

A head word is extracted if it is a nominal or
proper noun and it is related to at least one trig-
ger; pronouns are omitted. A trigger of an head
word is extracted if it is a verb or an eventive noun
and the head word serves as its subject, object, or
preposition. We use the categories noun.EVENT
and noun.ACT in WordNet as a list of eventive
nouns. A head word can have more than one trig-
ger. These multiple relations can come from a syn-
tactic coordination inside a single sentence, as it
is the case in the first sentence of the illustrating
example. They can also represent a coreference

189



h

t

π

φ

 uni(1,K)

#tuples

aθ

s

dir(α)

dir(β)

dir(γ)

A

T

Figure 2: Generative model for event induction.

chain across sentences, as we use coreference res-
olution to merge the triggers of mentions corefer-
ing to the same entity in a document. Coreferences
are useful sources for event induction (Chambers
and Jurafsky, 2011; Chambers, 2013). Finally, an
attribute is extracted if it is an adjective, a noun or
a verb and serves as an adjective, verbal or nom-
inal modifier of a head word. If there are several
modifiers, only the closest to the head word is se-
lected. This “best selection” heuristic allows to
omit non-discriminative attributes for the entity.

4 Generative Model

4.1 Model Description
Figure 2 shows the plate notation of our model.
For each triple representing an entity e, the model
first assigns a slot s for the entity from an uni-
form distribution uni(1,K). Its head word h is
then generated from a multinominal distribution
πs. Each ti of event trigger relations Te is gen-
erated from a multinominal distribution φs. Each
aj of attribute relations Ae is similarly generated
from a multinominal distribution θs. The distri-
butions θ, π, and φ are generated from Dirichlet
priors dir(α), dir(β) and dir(γ) respectively.

Given a set of entities E, our model (π, φ, θ) is
defined by

Pπ,φ,θ(E) =
∏
e∈E

Pπ,φ,θ(e) (2)

where the probability of each entity e is defined by

Pπ,φ,θ(e) = P (s)
× P (h|s)
×

∏
t∈Te

P (t|s)

×
∏
a∈Ae

P (a|s) (3)

The generative story is as follows:

for slot s← 1 to K do
Generate an attribute distribution θs from a
Dirichlet prior dir(α);
Generate a head distribution πs from a Dirichlet
prior dir(β);
Generate a trigger distribution φs from a Dirichlet
prior dir(γ);

end
for entity e ∈ E do

Generate a slot s from a uniform distribution
uni(1,K);
Generate a head h from a multinominal distribution
πs;
for i← 1 to |Te| do

Generate a trigger ti from a multinominal
distribution φs;

end
for j ← 1 to |Ae| do

Generate an attribute aj from a multinominal
distribution φs;

end
end

4.2 Parameter Estimation
For parameter estimation, we use the Gibbs sam-
pling method (Griffiths, 2002). The slot variable
s is sampled by integrating out all the other vari-
ables.

Previous models (Cheung et al., 2013; Cham-
bers, 2013) are based on document-level topic
modeling, which originated from models such as
Latent Dirichlet Allocation (Blei et al., 2003).
Our model is, instead, independent from docu-
ment contexts. Its input is a sequence of entity
triples. Document boundary is only used in a post-
processing step of filtering (see Section 5.3 for
more details). There is a universal slot distribu-
tion instead of each slot distribution for one doc-
ument. Furthermore, slot prior is ignored by us-
ing a uniform distribution as a particular case of
categorical probability. Sampling-based slot as-
signment could depend on initial states and ran-
dom seeds. In our implementation of Gibbs sam-
pling, we use 2,000 burn-in of overall 10,000 it-
erations. The purpose of burn-in is to assure that
parameters converge to a stable state before esti-
mating the probability distributions. Moreover, an
interval step of 100 is applied between consecutive
samples in order to avoid too strong coherence.

Particularly, for tracking changes in probabili-
ties resulting from attribute relations, we ran in
the first stage a specific burn-in with only heads
and trigger relations. This stable state was then
used as initialization for the second burn-in in

190



 0.0005

 0.001

 0.0015

 0.002

 0.0025

 0.003

 0.0035

 0.004

 10  20  30  40  50  60  70  80  90  100

P
(t

e
rr

o
ri
s
t|
A

T
T

A
C

K
v
ic

ti
m

)

BURN_IN iterations (x20)

Using attributes
No attribute

(a) P (terrorist|ATTACK victim)

 0

 0.005

 0.01

 0.015

 0.02

 0.025

 0.03

 0.035

 0.04

 10  20  30  40  50  60  70  80  90  100

P
(t

e
rr

o
ri
s
t|
A

T
T

A
C

K
p
e
rp

e
tr

a
to

r)

BURN_IN iterations (x20)

Using attributes
No attribute

(b) P (terrorist|ATTACK perp)

 0

 0.05

 0.1

 0.15

 0.2

 0.25

 0.3

 10  20  30  40  50  60  70  80  90  100

P
(k

ill
:d

o
b
j|
A

T
T

A
C

K
v
ic

ti
m

)

BURN_IN iterations (x20)

Using attributes
No attribute

(c) P (kill : dobj|ATTACK victim)

 0

 0.005

 0.01

 0.015

 0.02

 10  20  30  40  50  60  70  80  90  100

P
(k

ill
:d

o
b
j|
A

T
T

A
C

K
p
e
rp

e
tr

a
to

r)

BURN_IN iterations (x20)

Using attributes
No attribute

(d) P (kill : dobj|ATTACK perp)

Figure 3: Probability convergence when using attributes in sampling. The use of attributes is started
at point 50 (i.e., 50% of burn-in phase). The dotted line shows convergence without attributes; the
continuous line shows convergence with attributes.

which attributes, heads, and triggers were used al-
together. This specific experimental setting made
us understand how the attributes modified distri-
butions. We observed that non-ambiguous words
or relations (i.e. explode, murder:nsubj) were only
slightly modified whereas probabilities of ambigu-
ous words such as man, soldier or triggers such as
kill:dobj or attack:nsubj converged smoothly to a
different stable state that was semantically more
coherent. For instance, the model interestingly re-
alized that even if a terrorist was killed (e.g. by
police), he was not actually a real victim of an at-
tack. Figure 3 shows probability convergences of
terrorist and kill:dobj given ATTACK victim and
ATTACK perpetrator.

5 Evaluations

In order to compare with related work, we eval-
uated our method on the Message Understanding
Conference (MUC-4) corpus (Sundheim, 1991)
using precision, recall and F-score as conventional

metrics for template extraction.
In what follows, we first introduce the MUC-

4 corpus (Section 5.1.1), we detail the mapping
technique between learned slots and reference
slots (5.1.2) as well as the hyper-parameters of
our model (5.1.3). Next, we present a first exper-
iment (Section 5.2) showing how using attribute
relations improves overall results. The second ex-
periment (Section 5.3) studies the impact of doc-
ument classification. We then compare our re-
sults with previous approaches, more particularly
with Chambers (2013), from both quantitative and
qualitative points of view (Section 5.4). Finally,
Section 5.5 is dedicated to error analysis, with a
special emphasis on sources of false positives.

5.1 Experimental Setups
5.1.1 Datasets
The MUC-4 corpus contains 1,700 news articles
about terrorist incidents happening in Latin Amer-
ica. The corpus is divided into 1,300 documents

191



for the development set and four test sets, each
containing 100 documents.

We follow the rules in the literature to guarantee
comparable results (Patwardhan and Riloff, 2007;
Chambers and Jurafsky, 2011). The evaluation fo-
cuses on four template types – ARSON, ATTACK,
BOMBING, KIDNAPPING – and four slots – Perpe-
trator, Instrument, Target, and Victim. Perpetrator
is merged from Perpetrator Individual and Perpe-
trator Organization. The matching between sys-
tem answers and references is based on head word
matching. A head word is defined as the right-
most word of the phrase or as the right-most word
of the first ‘of’ if the phrase contains any. Op-
tional templates and slots are ignored when calcu-
lating recall. Template types are ignored in eval-
uation: this means that a perpetrator of BOMBING
in the answers could be compared to a perpetrator
of ARSON, ATTACK, BOMBING or KIDNAPPING in
the reference.

5.1.2 Slot Mapping
The model learns K slots and assigns each entity
in a document to one of the learned slots. Slot
mapping consists in matching each reference slot
to an equivalent learned slot.

Note that among the K learned slots, some are
irrelevant while others, sometimes of high quality,
contain entities that are not part of the reference
(spatio-temporal information, protagonist context,
etc.). For this reason, it makes sense to have much
more learned slots than expected event slots.

Similarly to previous work in the literature, we
implemented an automatic empirical-driven slot
mapping. Each reference slot was mapped to
the learned slot that performed the best on the
task of template extraction according to the F-
score metric. Here, two identical slots of two
different templates, such as ATTACK victim and
KIDNAPPING victim, must to be mapped sepa-
rately. Figure 4 shows the most common words of
two learned slots which were mapped to BOMB-
ING instrument and KIDNAPPING victim. This
mapping is then kept for testing.

5.1.3 Parameter Tuning
We first tuned hyper-parameters of the models on
the development set. The number of slots was set
to K = 35. Dirichlet priors were set to α = 0.1,
β = 1 and γ = 0.1. The model was learned from
the whole dataset. Slot mapping was done on tst1
and tst2. Outputs from tst3 and tst4 were eval-

BOMBING instrument
Attributes Heads Triggers

car:nn bomb explode:nsubj
powerful:amod fire hear:dobj
explosive:amod explosion place:dobj

dynamite:nn blow cause:nsubj
heavy:amod charge set:dobj

KIDNAPPING victim
Attributes Heads Triggers

several:amod people arrest:dobj
other:amod person kidnap:dobj

responsible:amod man release:dobj
military:amod member kill:dobj
young:amod leader identify:prep as

Figure 4: Attribute, head and trigger distributions
learned by the model HT+A for learned slots that
were mapped to BOMBING instrument and KID-
NAPPING victim.

uated using references and were averaged across
ten runs.

5.2 Experiment 1: Using Entity Attributes
In this experiment, two versions of our model are
compared: HT+A uses entity heads, event trigger
relations and entity attribute relations. HT uses
only entity heads and event triggers and omits at-
tributes.

We studied the gain brought by attribute re-
lations with a focus on their effect when coref-
erence information was available or was miss-
ing. The variations on the model input are named
single, multi and coref. Single input has only
one event trigger for each entity. A text like
an armed man attacked the police station and
killed a policeman results in two triples for the
entity man: (armed:amod, man, attack:nsubj) and
(armed:amod, man, kill:nsubj). In multi input, one
entity can have several event triggers, leading for
the text above to the triple (armed:amod, man, [at-
tack:nsubj, kill:nsubj]). The coref input is richer
than multi in that, in addition to triggers from the
same sentence, triggers linked to the same coref-
ered entity are merged together. For instance, if
man in the above example corefers with he in
He was arrested three hours later, the merged
triple becomes (armed:amod, man, [attack:nsubj,
kill:nsubj, arrest:dobj]). The plate notations of
these model+data combinations are given in Fig-
ure 5.

Table 1 shows a consistent improvement when
using attributes, both with and without corefer-
ences. The best performance of 40.62 F-score is
obtained by the full model on inputs with coref-

192



h

t

π

φ

 uni(1,K)

#tuples

s

(a)

h

t

π

φ

 uni(1,K)

#tuples

s

T

(b)

h

t

π

φ

 uni(1,K)

#tuples

aθ

s

A

(c)

h

t

π

φ

 uni(1,K)

#tuples

aθ

s

A

T

(d)

Figure 5: Model variants (Dirichlet priors are omitted for simplicity): 5a) HT model ran on single data.
This model is equivalent to 5b) with T=1; 5b) HT model ran on multi data; 5c) HT+A model ran on
single data; 5d) HT+A model ran on multi data.

Data HT HT+A
P R F P R F

Single 29.59 51.17 37.48 30.22 52.41 38.33
Multi 29.32 52.21 37.52 30.82 51.68 38.55
Coref 39.99 53.53 40.01 32.42 54.59 40.62

Table 1: Improvement from using attributes.

erences. Using both attributes in the model and
coreference to generate input data results in a gain
of 3 F-score points.

5.3 Experiment 2: Document Classification
In the second experiment, we evaluated our model
with a post-processing step of document classifi-
cation.

The MUC-4 corpus contains many “irrelevant”
documents. A document is irrelevant if it contains
no template. Among 1,300 documents in the de-
velopment set, 567 are irrelevant. The most chal-
lenging part is that there are many terrorist entities,
e.g. bomb, force, guerrilla, occurring in irrelevant
documents. That makes filtering out those docu-
ments important, but difficult. As document clas-

sification is not explicitly performed by our model,
a post-processing step is needed. Document clas-
sification is expected to reduce false positives in ir-
relevant documents while not dramatically reduc-
ing recall.

Given a document d with slot-assigned entities
and a set of mapped slots Sm resulting from slot
mapping, we have to decide whether this docu-
ment is relevant or not. We define the relevance
score of a document as:

relevance(d) =
∑

e∈d:se∈Sm
∑

t∈Te P (t|se)∑
e∈d

∑
t∈Te P (t|se)

(4)

where e is an entity in the document d; se is the
slot value assigned to e; and t is an event trigger in
the list of triggers Te.

The equation (4) defines the score of an entity as
the sum of the conditional probabilities of triggers
given a slot. The relevance score of the document
is proportional to the score of the entities assigned
to mapped slots. If this relevance score is higher
than a threshold λ, then the document is consid-
ered as relevant. The value of λ = 0.02 was tuned

193



System P R F
HT+A 32.42 54.59 40.62
HT+A + doc. classification 35.57 53.89 42.79
HT+A + oracle classification 44.58 54.59 49.08

Table 2: Improvement from document classifica-
tion as post-processing.

on the development set by maximizing the F-score
of document classification.

Table 2 shows the improvement when applying
document classification. The precision increases
as false positives from irrelevant documents are fil-
tered out. The loss of recall comes from relevant
documents that are mistakenly filtered out. How-
ever, this loss is not significant and the overall F-
score finally increases by 5%. We also compare
our results to an “oracle” classifier that would re-
move all irrelevant documents while preserving all
relevant ones. The performance of this oracle clas-
sification shows that there are some room for fur-
ther improvement from document classification.

Irrelevant document filtering is a technique ap-
plied by most supervised and unsupervised ap-
proaches. Supervised methods prefer relevance
detection at sentence or phrase-level (Patwardhan
and Riloff, 2009; Patwardhan and Riloff, 2007).
As for several unsupervised methods, Chambers
(2013) includes document classification in his
topic model. Chambers and Jurafsky (2011) and
Cheung et al. (2013) use the learned clusters to
classify documents by estimating the relevance of
a document with respect to a template from post-
hoc statistics about event triggers.

5.4 Comparison to State-of-the-Art
For comparing in more depth our results to the
state-of-the-art in the literature. we reimple-
mented the method proposed in Chambers (2013)
and integrated our attribute distributions into his
model (as shown in Figure 6).

The main differences between this model and
ours are the following:

1. The full template model of Chambers (2013)
adds a distribution ψ linking events to docu-
ments. This makes the model more complex
and maybe less intuitive since there is no rea-
son to connect documents and slots (a docu-
ment may contain references to several tem-
plates and slot mapping does not depend on
document level). A benefit of this document

System P R F
Cheung et al. (2013) 32 37 34
Chambers and Jurafsky (2011) 48 25 33
Chambers (2013) (paper values) 41 41 41
HT+A + doc. classification 36 54 43

Table 3: Comparison to state-of-the-art unsuper-
vised systems.

distribution is that it leads to a free classifi-
cation of irrelevant documents, thus avoid-
ing a pre- or post-processing for classifica-
tion. However, this issue of document rel-
evance is very specific to the MUC corpus
and the evaluation method; In a more general
use case, there would be no “irrelevant” doc-
uments, only documents on various topics.

2. Each entity is linked to an event variable e.
This event generates a predicate for each
entity mention (recall that mentions of an
entity are all occurrences of this entity in
the documents, for example in a corefer-
ence chain). Our work instead focus on
the fact that a probabilistic model could
have multiple observations at the same po-
sition. Multiple triggers and multiple at-
tributes are treated equally. The sources
of multiple attributes and multiple triggers
are not only from document-level corefer-
ences but also from dependency relations (or
even from domain-level entity coreferences if
available). Hence, our model arguably gener-
alizes better in terms of both modeling and
input data.

3. Chambers (2013) applies a heuristic con-
straint during the sampling process, impos-
ing that subject and object of the same predi-
cate (e.g. kill:nsubj and kill:dobj) are not dis-
tributed into the same slot. Our model does
not require this heuristic.

Some details concerning data preprocessing and
model parameters are not fully specified by Cham-
bers (2013); for this reason, our implementation
of the model (applied on the same data) leads
to slightly different results than those published.
That is why we present the two results here (pa-
per values in Table 3, reimplementation values in
Table 4).

Table 3 shows that our model outperforms the
others on recall by a large margin. It achieves the

194



h

t

π

φ  ψ

#tuples

s

M

e

p

#docs

τ

(a)

h

t

π

φ  ψ

#tuples

aθ

s

A

M

e

p

#docs

τ

(b)

Figure 6: Variation of Chambers (2013) model: 6a) Original model; 6b) Original model + attribute
distributions.

Chambers (2013) P R F
Original reimpl. 38.65 42.68 40.56
Original reimpl. + Attribute 39.25 43.68 41.31

Table 4: Performance on reimplementation of
Chambers (2013).

best overall F-score. In addition, as stated by our
experiments, precision could be further improved
by more sophisticated document classification. In-
terestingly, using attributes also proves to be use-
ful in the model proposed by Chambers (2013) (as
shown in Table 4).

5.5 Error Analysis
We performed an error analysis on the output of
HT+A + doc. classification to detect the origin
of false positives (FPs). 38% of FPs are mentions
that never occur in the reference. Within this 38%,
attacker and killer are among the most frequent er-
rors. These words could refer to a perpetrator of an
attack. These mentions, however, do not occur in
the reference, possibly because human annotators
consider them as too generic terms. Apart from
such generic terms, other assignments are obvious
errors of the system, e.g. window, door or wall as
physical target; action or massacre as perpetrator;
explosion or shooting as instrument. These kinds
of errors are due to the fact that in our model, as in
the one of Chambers (2013), the number of slots
is fixed and is not equivalent to the real number of
reference slots.

On the other hand, 62% of FPs are mentions of

entities that occur at least once in the reference.
On top of the list are perpetrators such as guer-
rilla, group and rebel. The model is capable of as-
signing guerrilla to attribution slot if it is accom-
panied by a trigger like announce:nsubj. How-
ever, triggers that describe quasi-terrorism events
(e.g. menace, threatening, military conflict) are
also grouped into perpetrator slots. Similarly,
mentions of frequent words such as bomb (instru-
ment), building, house, office (targets) tend to be
systematically grouped into these slots, regardless
of their relations. Increasing the number of slots
(to sharpen their content) does not help overall.
This is due to the fact that the MUC corpus is
very small and is biased towards terrorism events.
Adding a higher level of template type as in Cham-
bers (2013) partially solves the problem but makes
recall decrease (as shown in Table 3).

6 Conclusions and Perspectives

We presented a generative model for representing
the roles played by the entities in an event tem-
plate. We focused on using immediate contexts of
entities and proposed a simpler and more effective
model than those proposed in previous work. We
evaluated this model on the MUC-4 corpus.

Even if our results outperform other unsuper-
vised approaches, we are still far from results ob-
tained by supervised systems. Improvements can
be obtained by several ways. First, the character-
istics of the MUC-4 corpus are a limiting factor.
The corpus is small and roles are similar from a
template to another, which does not reflect reality.

195



A bigger corpus, even partially annotated but pre-
senting a better variety of templates, could lead to
very different approaches.

As we showed, our model comes with a unified
representation of all types of relations. This opens
the way to the use of multiple types of relations
(syntactic, semantic, thematic, etc.) to refine the
clusters.

Last but not least, the evaluation protocol, that
became a kind of de facto standard, is very much
imperfect. Most notably, the way of finally map-
ping with reference slots can have a great influence
on the results.

Acknowledgment

This work was partially financed by the Foun-
dation for Scientific Cooperation “Campus Paris-
Saclay” (FSC) under the project Digiteo ASTRE
No. 2013-0774D.

References
Niranjan Balasubramanian, Stephen Soderland,

Mausam, and Oren Etzioni. 2013. Generating
Coherent Event Schemas at Scale. In 2013 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP 2013), pages 1721–1731,
Seattle, Washington, USA, October.

Cosmin Adrian Bejan. 2008. Unsupervised Discovery
of Event Scenarios from Texts. In Twenty-First In-
ternational Florida Artificial Intelligence Research
Society Conference (FLAIRS 2008), pages 124–129,
Coconut Grove, Florida.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022, March.

Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised Learning of Narrative Event Chains. In
ACL-08: HLT, pages 789–797, Columbus, Ohio,
June.

Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised Learning of Narrative Schemas and their
Participants. In Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP (ACL-IJCNLP’09), pages 602–610, Suntec,
Singapore, August.

Nathanael Chambers and Dan Jurafsky. 2011.
Template-Based Information Extraction without the
Templates. In 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL 2011), pages 976–986,
Portland, Oregon, USA, June.

Nathanael Chambers. 2013. Event Schema Induction
with a Probabilistic Entity-Driven Model. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1797–
1807, Seattle, Washington, USA, October.

Kit Jackie Chi Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic Frame Induction. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 837–846.

R. Collier. 1998. Automatic Template Creation for
Information Extraction. Ph.D. thesis, University of
Sheffield.

Gerald DeJong. 1982. An overview of the FRUMP
system. In W. Lehnert and M. Ringle, editors,
Strategies for natural language processing, pages
149–176. Lawrence Erlbaum Associates.

Kathrin Eichler, Holmer Hemsen, and Günter Neu-
mann. 2008. Unsupervised Relation Extraction
From Web Documents. In 6th Conference on Lan-
guage Resources and Evaluation (LREC’08), Mar-
rakech, Morocco.

Olivier Ferret and Brigitte Grau. 1997. An Aggre-
gation Procedure for Building Episodic Memory.
In 15th International Joint Conference on Artificial
Intelligence (IJCAI-97), pages 280–285, Nagoya,
Japan.

Elena Filatova, Vasileios Hatzivassiloglou, and Kath-
leen McKeown. 2006. Automatic Creation of
Domain Templates. In 21st International Confer-
ence on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Lin-
guistics (COLING-ACL 2006), pages 207–214, Syd-
ney, Australia.

Elena Filatova. 2008. Unsupervised Relation Learning
for Event-Focused Question-Answering and Domain
Modelling. Ph.D. thesis, Columbia University.

Marjorie Freedman, Lance Ramshaw, Elizabeth
Boschee, Ryan Gabbard, Gary Kratkiewicz, Nico-
las Ward, and Ralph Weischedel. 2011. Ex-
treme Extraction – Machine Reading in a Week. In
2011 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2011), pages 1437–
1446, Edinburgh, Scotland, UK., July.

Lea Frermann, Ivan Titov, and Manfred Pinkal. 2014.
A Hierarchical Bayesian Model for Unsupervised
Induction of Script Knowledge. In 14th Conference
of the European Chapter of the Association for Com-
putational Linguistics (EACL 2014), pages 49–57,
Gothenburg, Sweden, April.

Tom Griffiths. 2002. Gibbs sampling in the genera-
tive model of Latent Dirichlet Allocation. Technical
report, Stanford University.

196



Ralph Grishman and Yifan He. 2014. An Informa-
tion Extraction Customizer. In Petr Sojka, Ale Hork,
Ivan Kopeek, and Karel Pala, editors, 17th Inter-
national Conference on Text, Speech and Dialogue
(TSD 2014), volume 8655 of Lecture Notes in Com-
puter Science, pages 3–10. Springer International
Publishing.

Ralph Grishman and Beth Sundheim. 1996. Mes-
sage Understanding Conference-6: A Brief History.
In 16th International Conference on Computational
linguistics (COLING’96), pages 466–471, Copen-
hagen, Denmark.

Sanda Harabagiu. 2004. Incremental Topic Repre-
sentation. In Proceedings of the 20th International
Conference on Computational Linguistics (COL-
ING’04), Geneva, Switzerland, August.

Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-
man. 2004. Discovering Relations among Named
Entities from Large Corpora. In 42nd Meeting
of the Association for Computational Linguistics
(ACL’04), pages 415–422, Barcelona, Spain.

Ludovic Jean-Louis, Romaric Besanon, and Olivier
Ferret. 2011. Text Segmentation and Graph-based
Method for Template Filling in Information Extrac-
tion. In 5th International Joint Conference on Nat-
ural Language Processing (IJCNLP 2011), pages
723–731, Chiang Mai, Thailand.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP Natural Lan-
guage Processing Toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60, Baltimore, USA, jun.

Bonan Min, Shuming Shi, Ralph Grishman, and Chin-
Yew Lin. 2012. Ensemble Semantics for Large-
scale Unsupervised Relation Extraction. In 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2012, pages
1027–1037, Jeju Island, Korea.

Ashutosh Modi and Ivan Titov. 2014. Inducing neural
models of script knowledge. In Eighteenth Confer-
ence on Computational Natural Language Learning
(CoNLL 2014), pages 49–57, Ann Arbor, Michigan.

Siddharth Patwardhan and Ellen Riloff. 2007. Ef-
fective Information Extraction with Semantic Affin-
ity Patterns and Relevant Regions. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL
2007), pages 717–727, Prague, Czech Republic,
June.

Siddharth Patwardhan and Ellen Riloff. 2009. A Uni-
fied Model of Phrasal and Sentential Evidence for
Information Extraction. In Proceedings of the 2009

Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2009), pages 151–160.

Karl Pichotta and Raymond Mooney. 2014. Statistical
script learning with multi-argument events. In 14th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL 2014),
pages 220–229, Gothenburg, Sweden.

Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2008.
Modeling Context in Scenario Template Creation.
In Third International Joint Conference on Natural
Language Processing (IJCNLP 2008), pages 157–
164, Hyderabad, India.

Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning Script Knowledge with Web
Experiments. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2010),
pages 979–988, Uppsala, Sweden, July.

Benjamin Rosenfeld and Ronen Feldman. 2007. Clus-
tering for unsupervised relation identification. In
Sixteenth ACM conference on Conference on in-
formation and knowledge management (CIKM’07),
pages 411–418, Lisbon, Portugal.

Roger C. Schank. 1980. Language and memory. Cog-
nitive Science, 4:243–284.

Satoshi Sekine. 2006. On-demand information
extraction. In 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics (COLING-ACL 2006), pages 731–738, Sydney,
Australia.

Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive Information Extraction using Unrestricted Rela-
tion Discovery. In HLT-NAACL 2006, pages 304–
311, New York City, USA.

Beth M. Sundheim. 1991. Third Message Understand-
ing Evaluation and Conference (MUC-3): Phase 1
Status Report. In Proceedings of the Workshop on
Speech and Natural Language, HLT ’91, pages 301–
305.

197


