



















































Relation Module for Non-Answerable Predictions on Reading Comprehension


Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 747–756
Hong Kong, China, November 3-4, 2019. c©2019 Association for Computational Linguistics

747

Relation Module for Non-answerable Prediction
on Reading Comprehension

Kevin Huang, Yun Tang, Jing Huang, Xiaodong He, and Bowen Zhou
JD AI Research, Mountain View, CA

{kevin.huang3, yun.tang, jing.huang,
xiadong.he, bowen.zhou}@jd.com

Abstract
Machine reading comprehension (MRC) has
attracted significant amounts of research atten-
tion recently, due to an increase of challenging
reading comprehension datasets. In this pa-
per, we aim to improve a MRC model’s abil-
ity to determine whether a question has an an-
swer in a given context (e.g. the recently pro-
posed SQuAD 2.0 task). Our solution is a re-
lation module that is adaptable to any MRC
model. The relation module consists of both
semantic extraction and relational information.
We first extract high level semantics as objects
from both question and context with multi-
head self-attentive pooling. These semantic
objects are then passed to a relation network,
which generates relationship scores for each
object pair in a sentence. These scores are
used to determine whether a question is non-
answerable. We test the relation module on
the SQuAD 2.0 dataset using both the BiDAF
and BERT models as baseline readers. We ob-
tain 1.8% gain of F1 accuracy on top of the
BiDAF reader, and 1.0% on top of the BERT
base model. These results show the effective-
ness of our relation module on MRC.

1 Introduction

Ever since the release of many challenging large
scale datasets for machine reading comprehension
(MRC) (Rajpurkar et al., 2016; Joshi et al., 2017;
Trischler et al., 2016; Yang et al., 2018; Reddy
et al., 2018; Jia and Liang, 2017), there have been
correspondingly many models for these datasets
(Yu et al., 2018; Seo et al., 2017; Liu et al., 2018b;
Hu et al., 2017; Xiong et al., 2017; Wang et al.,
2018; Liu et al., 2018c; Tay et al., 2018). Know-
ing what you don’t know (Rajpurkar et al., 2018) is
important in real applications of reading compre-
hension. Unanswerable questions are common-
place in the real world, and SQuAD 2.0 was re-
leased specifically to target this problem (see Fig-
ure 1 for an example of non-answerable ques-
tions).

Example 1
Context: Each year, the southern California area has
about 10,000 earthquakes. Nearly all of them are so small
that they are not felt. Only several hundred are greater
than magnitude 3.0, and only about 1520 are greater than
magnitude 4.0. The magnitude 6.7 1994 Northridge
earthquake was particularly destructive, causing a sub-
stantial number of deaths, injuries, and structural col-
lapses. It caused the most property damage of any earth-
quake in U.S. history, estimated at over $20 billion.
Question: What earthquake caused $20 million in dam-
age?
Answer: None.

Figure 1: An example of non-answerable question in
SQuAD 2.0. Highlighted words are the output from
the BERT base model. The true answer is “None”.

One problem that most of the early MRC read-
ers have in common is the inability to predict
non-answerable questions. Readers on the popular
SQuAD dataset have to be modified in order to ac-
commodate a non-answerable possibility. Current
methods on SQuAD 2.0 generally attempt to learn
a single fully connected layer (Clark and Gardner,
2018; Liu et al., 2018a; Devlin et al., 2018) in or-
der to determine whether a question/context pair is
answerable. This leaves out relational information
that may be useful for determining answerabil-
ity. We believe that relationships between different
high-level semantics in the context are helpful to
make better answerable or unanswerable decision.
For example, “Northridge earthquake” is mistak-
enly taken as the answer to the question about
what earthquake caused $20 million in damage.
Because “$20 billon” is positioned far away from
“Northridge earthquake”, it is hard for a model
to link these two concepts together and recognize
the mismatch of “$20 million” in the question and
“$20 billion” in the context.

Motivated by exploiting high level semantic re-
lationships in the context, our first step is to ex-
tract meaningful high-level semantics from ques-
tion/context. Multi-head self-attentive pooling



748

(Lin et al., 2017) has shown to be able to ex-
tract different views of a sentence with multiple
heads. Each head from the multi-head self atten-
tive pooling has different weights on the context
with learned parameters. This allows each head
to act as a filter in order to emphasize part of the
context. By summing up the weighted context, we
obtain a vector representing an instance of a high-
level semantic, which we can call it an “object”.
With multiple heads, we generate different seman-
tic objects, which are then fed in to a relation net-
work.

Relation networks (Santoro et al., 2017) are
specifically designed to model relationships be-
tween pairs of objects. In the case of reading
comprehension, an object would ideally be phrase
level semantics within a sentence. Relation net-
works are able to accomplish modeling these re-
lationships by constraining the network to learn a
score for each pair of these objects. After learn-
ing all of the pairwise scores, the relation network
then summarizes all of the relations to a single
vector. By taking a weighted sum of all of the re-
lation scores that the sentence has, we generate a
non-answerable score that is trained jointly with
answer span scores from any MRC model to de-
termine non-answerability.

In addition, we add in plausible answers from
unanswerable examples to help train the relation
module. These plausible answers help the base
model learn a better span prediction and are also
used to help guide our object extractor to extract
relevant semantics. We train a separate layer for
start-end probabilities based on the plausible an-
swers. We then augment the context vector with
hidden states from this layer. This allows the
multi-head self-attentive pooling to focus on ob-
jects related to the proposed answer span, and dif-
ferentiate from other objects that are not as rele-
vant in the context.

In summary we propose a new relation module
dedicated to learning relationships between high-
level semantics and deciding whether a question is
answerable. Our contributions are four-fold:

1. Introduce the concept of using multi-head
self-attentive pooling outputs as high level se-
mantic objects.

2. Exploit relation networks to model the rela-
tionships between different objects in a con-
text. We then summarize these relationships
to get a final decision.

3. Introduce a separate feed-forward layer
trained on plausible answers so that we can
augment the context vector passed into the
object extractor. This results in the object ex-
tractor extracting phrases more relevant to the
proposed answer span.

4. Combining all of the above into a flexible re-
lation module that can be added to the end
of a question answering model to boost non-
answerable prediction.

To our knowledge, this is the first case of utiliz-
ing an object extractor to extract high level seman-
tics, and a relation networks to encode relation-
ships between these semantics in reading compre-
hension. Our results show improvement on top of
the baseline BiDAF model and the state-of-the-art
reader based on BERT, on the SQuAD 2.0 task.

2 Related Work

Relation Networks (RN) were first proposed by
(Santoro et al., 2017) in order to help neural mod-
els to reason over the relationships between two
objects. Relation networks learn relationships be-
tween objects by learning a pairwise score for each
object pair. Relation networks have been applied
to CLEVR (Johnson et al., 2017) as well as bAbI
(Weston et al., 2015). In the CLEVR dataset, the
object inputs to the relation network are visual ob-
jects in an image, extracted by a CNN, and in bAbI
the object inputs are sentence encodings. In both
tasks, the relation network is then used to compute
a relationship score over these objects. Relation
Networks were further applied to general reason-
ing by training the model on images (You et al.,
2018).

MAC (Memory, Attention and Composition)
networks (Hudson and Manning, 2018) are differ-
ent models that have also been shown to learn re-
lations from the CLEVR dataset. MAC networks
operate with read and write cells. Each cell would
compute a relation score between a knowledge
base and question and write it into memory. Mul-
tiple read and write cells are strung together se-
quentially in order to model long chains of multi-
hop reasoning. Although MAC networks do not
explicitly reason between pairwise objects as re-
lation networks do, MAC networks are an inter-
esting way of generating multi-hop reasoning be-
tween objects within a context.



749

Figure 2: Relation Module on BERT. S and E are hidden states trained by plausible answers. We then concatenate
S and E with the contextual representation to feed into the object extractor. After we obtain the extracted objects,
we then feed into a Relation Network and pass it down for NA predictions.

Another similar line of work investigated pre-
training relationship embeddings across word
pairs on large unlabelled corpus (Jameel et al.,
2018; Joshi et al., 2018). These pre-trained pair-
wise relational embeddings were added to the at-
tention layers of BiDAF, where higher level ab-
stract reasoning occurs. The paper showed an im-
pressive gain of 2.7% on the SQuAD 2.0 develop-
ment set on top of their version of BiDAF.

Many MRC models have been adapted to work
on SQuAD 2.0 recently (Hu et al., 2019; Liu et al.,
2018a; Sun et al., 2018; Devlin et al., 2018). (Hu
et al., 2019) added a separately trained answer ver-
ifier for no-answer detection with their Mnemonic
Reader. The answer sentence that is proposed by
the reader and the question are passed to three
combinations of differently configured verifiers
for fine-grained local entailment recognition. (Liu
et al., 2018a) just added one layer as the unanswer-
able binary classifier to their SAN reader. (Sun
et al., 2018) proposed the U-net with a univer-
sal node that encodes the fused information from
both the question and passage. The summary U-
node, question vector and two context vectors are
passed to predict whether the question is answer-
able. Plausible answers were used for no-answer
pointer prediction, while in our approach, plausi-
ble answers were used to augment context vector
for object extraction that later help the no-answer
prediction.

Pretraining embeddings on large unlabelled cor-
pus has been shown to improve many downstream
tasks (Peters et al., 2018; Howard and Ruder,
2018; Alec et al., 2018). The recently released

BERT (Devlin et al., 2018) greatly increased the
F1 scores on the SQuAD 2.0 leaderboard. BERT
consists of stacked Transformers (Vaswani et al.,
2017), that are pre-trained on vast amounts of un-
labeled data with a masked language model. The
masked language model helps finetuning on down-
stream tasks, such as SQuAD 2.0. BERT models
contains a special CLS token which is helpful for
the SQuAD 2.0 task. This CLS token is trained
to predict if a pair of sentences follow each other
during the pre-training, which helps encode entail-
ment information between the sentence pair. Due
to a strong masked language model to help predict
answers and a strong CLS token to encode entail-
ment, BERT models are the current state-of-the art
for SQuAD 2.0.

3 Relation Module

Our relation module is flexible, and can be placed
on top of any MRC model. We now describe the
relation module in detail.

3.1 Augmenting Inputs

Figure 2 shows our relation module on top of the
base reader BERT. In addition to the original start-
end prediction layers trained from true answers in
the base reader, we include a separate start-end
prediction layer, with separate parameters, trained
specifically on plausible and true answers avail-
able in SQuAD 2.0. The context output C from
BERT is projected into two hidden state layers S
and E, where C, S and E ∈ RL×h, L is the
context length and h is the hidden size. The S
and E layers are then projected down to a hidden



750

dimension of 1, and trained with Cross-Entropy
Loss against the plausible and true answer starts
and ends. The hidden states S and E of this layer
are concatenated with the last context layer output
C and projected back to the original dimension to
obtain the augmented context vector X , which is
fused with start-end span information.

S = tanh(CW1 + b1) (1)

E = tanh(CW2 + b2) (2)

X = [C;S;E]W (3)

where [;;] is concatenation of multiple tensors and
X ∈ RL×h. This process is shown in Figure 2,
where S and E are hidden states trained on plau-
sible and true answer spans. This tensor X and
the last question layer output Q are passed to the
object extractor layer.

3.2 Object Extractor
The augmented context tensor X (and separately,
question tensorQ) is passed through the object ex-
tractor to generate object representations from the
tensor. We pass the inputs through a multi-head
self-attentive pooling layer. This object extractor
can be thought of as a set filters extracting out ar-
eas of interest within a sentence. We multiply the
input tensorX with a multi-head self attention ma-
trix A which is defined as

A = Softmax(W4σ(W3X
T )) (4)

O = AX (5)

where W3 ∈ Rh×h, and W4 ∈ Rn×h; σ is an ac-
tivation function, such as tanh; n is the number
of heads, and h is the hidden dimension. The out-
put O ∈ Rn×h contains the n objects with hidden
dimension h that are passed to the next layer.

3.2.1 Object Extraction Regularization
In order to help encourage the multiple heads to
extract different meaningful semantics in the text,
a regularization loss (Xia et al., 2018) is intro-
duced to encourage each head to attend to slightly
different sections of the context. Overlapping ob-
jects centered on the answer span are expected,
due to information fused from S and E, but we do
not want the entire weight distribution of the head
to be solely focused on the answer span. As we
show in later figures, many heads heavily weight
the answer span, but also weight information rel-
evant to the answer span needed to make a bet-
ter non-answerable prediction. Our regularization

Figure 3: Illustration of a Relation Network. The gθ is
a MLP to score relationships between pairs

term also helps prevent the multi-headed attentive
pooling from learning a noisy distribution over all
of the context. This regularization loss is defined
as

Laux = α||AAT − I||2 (6)

where A is the weight matrix for the attention
heads and I is the identity matrix. α is set to be
0.0005 in our experiments.

3.3 Relation Networks
Extracted objects are subsequently passed to a re-
lation network. We use two layer MLP gθ (in Fig-
ure 3) as a scoring function to compute the simi-
larity between objects. In the question-answering
task, the context contains the contextual informa-
tion necessary to determine whether a question is
answerable. Phrases and ideas from various parts
of the context need to come together in order to
fully understand whether or not a question is an-
swerable. Therefore our relation module takes all
pairs of context objects to score, and use the ques-
tion objects to guide the scoring function. We use
2 question heads q0, q1, so our scoring function is:

ri =
n∑
j=0

ωi,j ∗ gθ(oi, oj , q0, q1) (7)

z =
n∑
i=0

γi ∗ fφ(ri) (8)

where the outputs ri is the weighted sum of the
relation values for object oi from O, and z is a
summarized relation vector. The weights Ωi =
[ωi,0, ..., ωi,n] and Γ = [γ0, ..., γn] are computed
by projecting down the relations scores into a hid-
den size of 1, and applying softmax.

Ωi = Softmax(gθ(oi, :, q0, q1)wg) (9)

Γ = Softmax(fφ(:)wf ) (10)



751

gθ and fφ are two layer MLP with activation func-
tion tanh to compute and aggregate relational
scores. Figure 3 shows the process of a single rela-
tion network, where two context objects and ques-
tion objects are passed in to gθ to obtain the output
z.

We project the weighted sum of fφ with a lin-
ear layer to a single value as a representation of
the non-answerable score. This score is combined
with the start/end logits from the base reader, and
trained jointly with the reader’s cross-entropy loss.
By training jointly, the model is able to make a bet-
ter prediction based on the confidence of the span
prediction, as well as the confidence based on the
non-answerable score from the relation module.

4 Question Answering Baselines

We test the relation module on top of our own Py-
Torch implementation of the BiDAF model (Seo
et al., 2017), as well as the recent released BERT
base model (Devlin et al., 2018) for the SQuAD
2.0 task. For both of these models, we obtain im-
provement from adding the relation module. Note
that, we do not test our relation module on top of
the current leaderboard, as the details are not yet
out. We also do not test on top of BERT + Syn-
thetic Self Training (Devlin, 2019) due to lack of
computational resources available. We are show-
ing the effectiveness of our method and not trying
to compete with the top of the leaderboard.

4.1 BiDAF

We implement the baseline BiDAF model for
SQuAD 2.0 task (Clark and Gardner, 2018) with
some modifications: adding features that are com-
monly used in question answering tasks such as
TF-IDF, POS/NER tagging, etc, and the auxiliary
training losses from (Hu et al., 2019). These mod-
ifications to the original BiDAF bring about 3.8%
gain of F1 on the SQuAD 2.0 development set (see
Table 1).

The input to the relation module is the con-
text vector that is generated from the bi-directional
attention flow layer. This context layer is aug-
mented with the hidden states of linear layers
trained against plausible answers, which also takes
the context layer from the attention flow layer as
input. This configuration is shown in Figure 4.

Figure 4: Relation Module applied on BiDAF.

4.2 BERT

BERT is a masked language model pre-trained
on large amounts of data that is the core compo-
nent of all of the current state-of-the-art models
on the SQuAD 2.0 task. The input to BERT is
the concatenation of a question and context pair
in the form of [“CLS”; question; “SEP”; context;
“SEP”]. BERT comes with its own special “CLS”
token, which is pre-trained on a next sentence pair
objective in order to encode entailment informa-
tion between the two sentences during the pre-
training scheme.

We leverage this “CLS” node with the relation
module by concatenating it with the output of our
Relation Module, and projecting the values down
to a single dimension. This combines the infor-
mation stored in the “CLS” token that has been
learned from the pre-training, as well as the infor-
mation that we learn through our relation module.
We allow gradients to be passed through all layers
of BERT, and finetune the initialized weights with
the SQuAD 2.0 dataset.

5 Experiments

We experiment on the SQuAD 2.0 dataset (Ra-
jpurkar et al., 2018) which contains question and
context examples that are crowd-sourced from
Wikipedia. Each example contains an answer span
in the passage, or an empty string, indicating that
an answer doesn’t exist. The results are reported
on the SQuAD 2.0 development set.



752

Model EM(%) F1(%)
(Clark and Gardner, 2018) 61.9 64.8
Our Implementation of BiDAF 65.7 68.6
BiDAF + Relation Module 67.7 70.4
BERT-base 73.6 76.6
BERT-base + Relation Module 74.2 77.6
BERT-large 78.9 82.1
BERT-large + Relation Module 79.2 82.6

Table 1: Model performance on SQuAD 2.0 develop-
ment set averaged over three random seeds.

Model EM(%) F1(%)
BERT-base 70.7 74.4
BERT-base +Answer Verifier 71.7 75.5
BERT-base + Relation Module 73.2 76.8

Table 2: SQuAD 2.0 leaderboard numbers on the
BERT-base Models. Our model shows improvement
over the public BERT-base models on the official eval-
uation.

We use the following parameters in our BiDAF
experiment: 16 context heads, 2 question heads.
We set our regularization loss weight for the object
extractor to be 0.0005. We use Adam optimizer
(Kingma and Ba, 2014), with a start learning rate
of 0.0008 and decay the learning rate by 0.5 with
a patience of 3 epochs. We add auxiliary losses for
plausible answers, and re-rank the non-answerable
loss as in (Hu et al., 2019).

BERT comes in two different sizes, a BERT-
base model (comprising of roughly 110 million
parameters), and a BERT-large model (comprising
of roughly 340 million parameters). We use the
BERT-base model to run our experiments due to
the limited computing resources that training the
BERT-large model would take. We only use the
BERT-large model to show that we still get im-
provements with the relation module. The relation
module on top of the BERT-base model only con-
tains roughly 10 million parameters.

We use the BERT-base model to run our ex-
periments with the same hyper-parameters given
on the official BERT GitHub repository. We use
16 context objects, 2 question heads, and a regu-
larization loss of 0.0005. We also show that on
top of the BERT-large model, on the development
set, our relation module still obtains performance
gain1. We use the same number of objects, and the
same regularization losses for the BiDAF model
experiments.

1We do not have enough time to get official SQuAD 2.0
evaluation results for the large BERT models.

Answerable Non-Answerable
BERT-base 81.5 78.3
+ Relation Module 82.1 82.1

Table 3: Prediction accuracies on answerable and non-
answerable questions on development set.

Table 1 presents the results of the baseline read-
ers with and without the relation module on the
SQuAD development set. Our proposed relation
module improves the overall F1 and EM accuracy:
2.0% gain on EM and 1.8% gain on F1 on the
BiDAF, as well as 0.8% gain on EM and 1.0%
gain on F1 on the BERT-base model. Our rela-
tion module is able to take relational information
between object-pairs and form a better no-answer
prediction than a model without it. The module
obtains less gain (0.5% gain of F1) on BERT large
model due to the better performance of BERT
large model. This module is reader independent
and works for any reading comprehension model
related to non-answerable tasks.

Table 2 presents performance of three BERT-
base models with minimum additions taken from
the official SQuAD 2.0 leaderboard. We see that
our relation module gives more gain than an An-
swer Verifier on top of the BERT-base model. Our
module gains 1.3% F1 over the Answer Verifier.

Since our relation module is designed to help
a MRC model’s ability to judge non-answerable
questions, we examine the accuracy when a ques-
tion is answerable and when a question is non-
answerable. Table 3 compares these accuracy
numbers for these questions with and without the
relation module on top of the BERT-base model.
The relation module improves prediction accu-
racy for both types of questions, and with more
accuracy gain on the non-answerable questions:
close to 4% gain on the non-answerable questions,
which is more than 200 non-answerable questions
are correctly predicted.

6 Ablation Study

We conduct an ablation study to show how dif-
ferent components of the relation module affects
the overall performance for the BERT-base model.
First we test only adding plausible answers on
top of the BERT-base model, in order to quan-
tify the gain in span prediction that adding these
extra answers in would give. We show that with
just adding plausible answers, the average of the



753

Model EM(%) F1(%)
BERT-base 73.6 76.6
BERT-base+Plausible Answers 73.5 76.9
BERT-base+RM-Plausible Answers 73.6 76.9
BERT-base+RM (4 heads) 74.1 77.4
BERT-base+RM (16 heads) 74.2 77.6
BERT-base+RM (64 heads) 74.0 77.2

Table 4: Ablation study on our Relation Module. We
experiment with just having plausible answers, just
having relation network, and different number of heads
for the objects extracted by the relation network. Each
of these values are averaged over three random seeds.

three seeds gain only about a 0.3 F1. This gain
in F1 is due to the BERT layers being fine-tuned
on more answer span data that we provide. Next
we study the effects of removing augmenting the
context vector with plausible answers. We feed
the output of our BERT-base model directly into
the object extractor and subsequently to the rela-
tion network. This quantifies the effect of forcing
the self-attentive heads to focus on a plausible an-
swer span. We notice that this performs compa-
rably to just adding plausible answers, also with
only around a 0.3 F1 gain.

Finally, we conduct a study to see the effects
of different number of heads on our relation mod-
ule. We experiment with 4, 16, and 64 heads, with
16 heads performing the best out of these three
configurations. Having too few heads hinders the
performance due to not enough information be-
ing propagated for the relation network to operate
on. Having too many heads will introduce redun-
dant information, as well as incorporating extrane-
ous noise for our model to sift through to generate
meaningful relations.

7 Analysis

In order to gain better understanding on how the
relation module helps on the unanswerable predic-
tion, we examine the objects extracted from the
multi-head self-attentive pooling. This is to check
whether the relevant semantics are extracted for
the relation network. Examples are selected from
the development set for data analysis.

In Example 1, the BERT-base model incorrectly
outputs “Northridge earthquake” (in red) as the an-
swer. However, after adding our relation module,
the model rejects this possible answer and outputs
a non-answerable prediction.

The two objects from the question highly at-
tend to token “million” (see the bottom subplot

Example 1
Context: Each year, the southern California area has
about 10,000 earthquakes. Nearly all of them are so small
that they are not felt. Only several hundred are greater
than magnitude 3.0, and only about 1520 are greater than
magnitude 4.0. The magnitude 6.7 1994 Northridge
earthquake was particularly destructive, causing a sub-
stantial number of deaths, injuries, and structural col-
lapses. It caused the most property damage of any earth-
quake in U.S. history, estimated at over $20 billion.
Question: What earthquake caused $20 million in dam-
age?
Answer: None.

Figure 5: In each subplot, each row represents one ob-
ject from our object extractor; for each object we high-
light the top 5 tokens with highest weights in the entire
context and question. We show the two windows where
the majority of these top 5 weights occur. For exam-
ple, the top purple object in the context looks at key
phrases such as “##ridge earthquake” in the top sub-
plot and “billion” in the middle subplot; the blue object
in the question looks at “20 million in” in the bottom
subplot.

in Figure 5). The top row purple object covers
token “1994” , “##ridge earthquake” in the pos-
sible answer span window, and “billion” near the
end of the context window. We hypothesize that
the relation network rejects the possible answer
“Northridge earthquake” due to the mismatch of
“million” in the question objects and “billion” in
the purple context object, and relation scores from
all other object pairs.

Example 2 shows another example of non-
answerable question and context pair. The BERT-
base model incorrectly outputs “input encoding”
(in red) as its prediction, while adding our rela-
tion module on the BERT-base model predicts cor-
rectly that the question is not answerable. Fig-



754

Example 2
Context: Even though some proofs of complexity-
theoretic theorems regularly assume some concrete
choice of input encoding, one tries to keep the discus-
sion abstract enough to be independent of the choice of
encoding. This can be achieved by ensuring that different
representations can be transformed into each other effi-
ciently.
Question: What is the abstract choice typically assumed
by most complexity-theoretic theorems?
Answer: None.

Figure 6: In each subplot, each row represents one ob-
ject from our object extractor; for each object we high-
light the top 5 tokens with highest weights in the entire
context and question. We show a window where the
majority of the top 5 weights occur. For example, there
are numerous objects in the context window that look
at the key phrase “some concrete” in the top subplot;
the two objects in the question look at the key phrase
“the abstract” in the bottom subplot.

ure 6 gives a visual illustration of objects extracted
from context and question. In Figure 6, the up-
per plot illustrates the 16 semantic objects shown
in this context window and the lower plot illus-
trates the two semantic objects from the question.
We see that from the upper plot, “some concrete”
and “input encoding” are highlighted, while in
the lower plot, “what”, “the abstract”, “most” are
highlighted. The mismatch of “the abstract” from
the question objects and “some concrete” from the
context objects helps indicate that the question is
unanswerable.

8 Conclusion

In this work we propose a new relation module that
can be applied on any MRC reader and help in-
crease the prediction accuracy on non-answerable
questions. We extract high level semantics from
multi-head self-attentive pooling. The semantic
object pairs are fed into the relation network which
makes a guided decision as to whether a ques-
tion is answerable. In addition we augment the
context vector with plausible answers, allowing

us to extract objects focused on the proposed an-
swer span, and differentiate from other objects that
are not as relevant in the context. Our results on
the SQuAD 2.0 dataset using the relation mod-
ule on both BiDAF and BERT models show im-
provements from the relation module. These re-
sults prove the effectiveness of our relation mod-
ule.

For future work, we plan to generalize the rela-
tion module to other aspects of question answer-
ing, including span prediction or multi-hop rea-
soning.

9 Acknowledgements

We would like to thank Robin Jia and Pranav Ra-
jpurkar for running the SQuAD evaluation on our
submitted models.

References
Alec, Karthik Radford, Tim Narsimhan, Salimans,

Illya, and Sutskever. 2018. Improving language un-
derstanding with generative pre-training.

Christopher Clark and Matt Gardner. 2018. Simple
and effective multi-paragraph reading comprehen-
sion. Association for Computational Linguistics.

Jacob Devlin. 2019. Bert: Pre-training of deep bidi-
rectional transformers for language understanding.
Lecture Slides.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Jeremy Howard and Sebastian Ruder. 2018. Fine-tuned
language models for text classification. Association
for Computational Linguistics, abs/1801.06146.

Minghao Hu, Yuxing Peng, and Xipeng Qiu. 2017.
Mnemonic reader for machine comprehension. 27th
International Joint Conference on Artificial Intelli-
gence, abs/1705.02798.

Minghao Hu, Furu Wei, Yuxing Peng, Zhen Huang,
Nan Yang, and Ming Zhou. 2019. Read + verify:
Machine reading comprehension with unanswerable
questions. Association for the Advancement of Arti-
ficial Intelligence.

Drew A. Hudson and Christopher D. Manning. 2018.
Compositional attention networks for machine rea-
soning. International Conference on Learning Rep-
resentations 2018, abs/1803.03067.

Shoaib Jameel, Zied Bouraoui, and Steven Schockaert.
2018. Unsupervised learning of distributional rela-
tion vectors. In Association of Computational Lin-
guistics.

http://arxiv.org/abs/1710.10723
http://arxiv.org/abs/1710.10723
http://arxiv.org/abs/1710.10723
https://nlp.stanford.edu/seminar/details/jdevlin.pdf
https://nlp.stanford.edu/seminar/details/jdevlin.pdf
http://arxiv.org/abs/1801.06146
http://arxiv.org/abs/1801.06146
http://arxiv.org/abs/1705.02798
http://arxiv.org/abs/1803.03067
http://arxiv.org/abs/1803.03067


755

Robin Jia and Percy Liang. 2017. Adversarial ex-
amples for evaluating reading comprehension sys-
tems. Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing,
abs/1707.07328.

Justin Johnson, Bharath Hariharan, Laurens van der
Maaten, Li Fei-Fei, C. Lawrence Zitnick, and
Ross B. Girshick. 2017. CLEVR: A diagnostic
dataset for compositional language and elementary
visual reasoning. Conference on Computer Vision
and Pattern Recognition.

Mandar Joshi, Eunsol Choi, Omer Levy, Daniel S.
Weld, and Luke Zettlemoyer. 2018. pair2vec: Com-
positional word-pair embeddings for cross-sentence
inference. CoRR, abs/1810.08854.

Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. Association for Computational Linguistics.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding.

Xiaodong Liu, Wei Li, Yuwei Fang, Aerin Kim, Kevin
Duh, and Jianfeng Gao. 2018a. Stochastic answer
networks for squad 2.0. Association for Computa-
tional Linguistics.

Xiaodong Liu, Yelong Shen, Kevin Duh, and Jian-
feng Gao. 2018b. Stochastic answer networks
for machine reading comprehension. CoRR,
abs/1712.03556.

Xiaodong Liu, Yelong Shen, Kevin Duh, and Jian-
feng Gao. 2018c. Stochastic answer networks for
machine reading comprehension. In Association of
Computational Linguistics, pages 1705–1714. Asso-
ciation for Computational Linguistics.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proc. of NAACL.

Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don’t know: Unanswerable ques-
tions for squad. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics, Association of Computational Linguistics
2018, Melbourne, Australia, July 15-20, 2018, Vol-
ume 2: Short Papers, pages 784–789.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100, 000+ questions for
machine comprehension of text. Proceedings of the
2016 Conference on Empirical Methods in Natural
Language Processing.

Siva Reddy, Danqi Chen, and Christopher D. Manning.
2018. Coqa: A conversational question answering
challenge. CoRR, abs/1808.07042.

Adam Santoro, David Raposo, David G Barrett, Ma-
teusz Malinowski, Razvan Pascanu, Peter Battaglia,
and Timothy Lillicrap. 2017. A simple neural net-
work module for relational reasoning. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, Advances
in Neural Information Processing Systems 30, pages
4967–4976. Curran Associates, Inc.

Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
flow for machine comprehension. In Proceedings of
International Conference on Learning Representa-
tions.

Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. 2018.
U-net: Machine reading comprehension with unan-
swerable questions. Association for the Advance-
ment of Artificial Intelligence.

Yi Tay, Luu Anh Tuan, Siu Cheung Hui, and Jian Su.
2018. Densely connected attention propagation for
reading comprehension. In Proceedings of Neural
Information Processing Systems.

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2016. Newsqa: A machine compre-
hension dataset. CoRR, abs/1611.09830.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proceedings for Neural Information
Processing Systems, abs/1706.03762.

Wei Wang, Ming Yan, and Chen Wu. 2018. Multi-
granularity hierarchical attention fusion networks
for reading comprehension and question answering.
In Association of Computational Linguistics, pages
1705–1714. Association for Computational Linguis-
tics.

Jason Weston, Antoine Bordes, Sumit Chopra, and
Tomas Mikolov. 2015. Towards ai-complete ques-
tion answering: A set of prerequisite toy tasks.
CoRR, abs/1502.05698.

Congying Xia, Chenwei Zhang, Xiaohui Yan,
Yi Chang, and Philip S. Yu. 2018. Zero-shot user
intent detection via capsule neural networks. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing.

Caiming Xiong, Victor Zhong, and Richard Socher.
2017. DCN+: mixed objective and deep residual
coattention for question answering. In Proceedings
of International Joint Conferences on Artificial In-
telligence.

http://arxiv.org/abs/1707.07328
http://arxiv.org/abs/1707.07328
http://arxiv.org/abs/1707.07328
http://arxiv.org/abs/1612.06890
http://arxiv.org/abs/1612.06890
http://arxiv.org/abs/1612.06890
http://arxiv.org/abs/1810.08854
http://arxiv.org/abs/1810.08854
http://arxiv.org/abs/1810.08854
http://arxiv.org/abs/1705.03551
http://arxiv.org/abs/1705.03551
http://arxiv.org/abs/1705.03551
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1712.03556
http://arxiv.org/abs/1712.03556
https://aclanthology.info/papers/P18-2124/p18-2124
https://aclanthology.info/papers/P18-2124/p18-2124
http://arxiv.org/abs/1606.05250
http://arxiv.org/abs/1606.05250
http://arxiv.org/abs/1808.07042
http://arxiv.org/abs/1808.07042
http://papers.nips.cc/paper/7082-a-simple-neural-network-module-for-relational-reasoning.pdf
http://papers.nips.cc/paper/7082-a-simple-neural-network-module-for-relational-reasoning.pdf
http://arxiv.org/abs/1611.01603
http://arxiv.org/abs/1611.01603
http://arxiv.org/abs/1611.09830
http://arxiv.org/abs/1611.09830
http://arxiv.org/abs/1706.03762
http://arxiv.org/abs/1706.03762
http://arxiv.org/abs/1502.05698
http://arxiv.org/abs/1502.05698
http://arxiv.org/abs/1711.00106
http://arxiv.org/abs/1711.00106


756

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
gio, William W. Cohen, Ruslan Salakhutdinov, and
Christopher D. Manning. 2018. Hotpotqa: A dataset
for diverse, explainable multi-hop question answer-
ing. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing,
pages 2369–2380. Association for Computational
Linguistics.

Haoxuan You, Yifan Feng, Xibin Zhao, Changqing
Zou, Rongrong Ji, and Yue Gao. 2018. Pvrnet:
Point-view relation neural network for 3d shape
recognition. the 33th AAAI Conference on Artificial
Intelligence (AAAI2019), abs/1812.00333.

Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui
Zhao, Kai Chen, Mohammad Norouzi, and Quoc V.
Le. 2018. Qanet: Combining local convolution with
global self-attention for reading comprehension. In
Proceedings of International Conference on Learn-
ing Representations.

http://arxiv.org/abs/1812.00333
http://arxiv.org/abs/1812.00333
http://arxiv.org/abs/1812.00333
http://arxiv.org/abs/1804.09541
http://arxiv.org/abs/1804.09541

