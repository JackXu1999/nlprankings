



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 906–916
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1084

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 906–916
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1084

Apples to Apples: Learning Semantics of Common Entities
Through a Novel Comprehension Task

Omid Bakhshandeh
University of Rochester

omidb@cs.rochester.edu

James F. Allen
University of Rochester

Institute for Human and Machine Cognition
james@cs.rochester.edu

Abstract

Understanding common entities and their
attributes is a primary requirement for
any system that comprehends natural
language. In order to enable learn-
ing about common entities, we intro-
duce a novel machine comprehension
task, GuessTwo: given a short paragraph
comparing different aspects of two real-
world semantically-similar entities, a sys-
tem should guess what those entities are.
Accomplishing this task requires deep lan-
guage understanding which enables infer-
ence, connecting each comparison para-
graph to different levels of knowledge
about world entities and their attributes.
So far we have crowdsourced a dataset
of more than 14K comparison paragraphs
comparing entities from a variety of cat-
egories such as fruits and animals. We
have designed two schemes for evaluation:
open-ended, and binary-choice prediction.
For benchmarking further progress in the
task, we have collected a set of paragraphs
as the test set on which human can accom-
plish the task with an accuracy of 94.2%
on open-ended prediction. We have im-
plemented various models for tackling the
task, ranging from semantic-driven to neu-
ral models. The semantic-driven approach
outperforms the neural models, however,
the results indicate that the task is very
challenging across the models.

1 Introduction

In the past few years, there has been great progress
on core NLP tasks (e.g., parsing and part of speech
tagging) which has renewed interest in primary
language learning tasks which require text under-

standing and reasoning, such as machine compre-
hension (Schoenick et al., 2016; Hermann et al.,
2015; Rajpurkar et al., 2016; Mostafazadeh et al.,
2016). Our question is how far have we got in
learning basic concepts of the world through lan-
guage comprehension. If we look at the large
body of work on extracting knowledge from un-
structured corpora, we will see that they often lack
some very basic pieces of information. For ex-
ample, let us focus on the basic concept of ap-
ple, the fruit. What do the state-of-the-art sys-
tems and resources know about an apple? None
of the state-of-the-art knowledge bases (Speer and
Havasi, 2012; Carlson et al., 2010; Fader et al.,
2011) include much precise information about the
fact that apples have an edible skin, vary from
sweet to sour, are round, and relatively the same
size of a fist. Moreover, there is no clear approach
on how to extract such information, if any, from
trained word embeddings. This paper focuses on
how we can automatically learn about various at-
tributes of such generic entities in the world.

A key observation motivating this work is that
we can learn more detail about objects when they
are compared to other similar objects. When we
compare things we often contrast, that is, we count
their similarities along with their dissimilarities.
This results in covering the primary attributes and
aspects of objects. As humans, we tend to recall
and mention the difference between things (say
green skin vs. red skin in apples) as opposed to
absolute measures (say the existence of skin). In-
terestingly, there is evidence that human knowl-
edge is structured by semantic similarity and the
relations among objects are defined by their rel-
ative perceptual and conceptual properties, such
as their form, function, behavior, and environ-
ment (Collins and Loftus, 1975; Tversky and Gati,
1978; Cree and Mcrae, 2003). Our idea is to lever-
age comparison as a way of naturally learning

906

https://doi.org/10.18653/v1/P17-1084
https://doi.org/10.18653/v1/P17-1084


about common world concepts and their specific
attributes.

Comparison, where we name the similarities
and differences between things, is a unique cogni-
tive ability in humans1 which requires memorizing
facts, experiencing things and integration of con-
cepts of the world (Hazlitt, 1933). It is clear that
developing AI systems that are capable of compre-
hending comparison is crucial. In this paper, in or-
der to enable learning through comparison, we in-
troduce a new language comprehension task which
requires understanding different attributes of basic
entities that are being compared.

The contributions of this paper are as fol-
lows: (1) To equip learning about common enti-
ties through comparison comprehension, we have
crowdsourced a dataset of more than 14K com-
parison paragraphs comparing entities from nine
broad categories (Section 2). This resource will
be expanded over time and will be released to
the public. (2) We introduce a novel task called
GuessTwo, in which given a short paragraph com-
paring two entities, a system should guess what
the two things are. (Section 3). To make system-
atic benchmarking on the task possible, we vet a
collection of comparison paragraphs to obtain a
test set on which human performs with an accu-
racy 94.2%. (3) We present a host of neural ap-
proaches and a novel semantic-driven model for
tackling the GuessTwo task (Sections 4, 5). Our
experiments show that the semantic approach out-
performs the neural models. The results strongly
suggest that closing the gap between system and
human performances requires richer semantic pro-
cessing (Section 6). We hope that this work will
establish a new base for a machine comprehension
test that requires systems to go beyond informa-
tion extraction and towards levels of performing
basic reasoning.

2 Data Collection

To enable learning about common entities, we
aimed to create a dataset which meets the follow-
ing goals:

1. The dataset should be a collection of high-
quality documents which are rich in compar-

1It has been suggested (Hazlitt, 1933) that children un-
der seven years old cannot name differences between simple
things such as peach and apple. This further shows that the
ability for comparison develops at a later age and is cogni-
tively complex.

ing and contrasting entities using their vari-
ous attributes and aspects.

2. The comparisons in the dataset should in-
volve everyday non-technical concepts, mak-
ing their comprehension easy and common-
sense for a human.

After many experiments with scraping existing
Web resources, we decided to crowdsource the
comparison paragraphs using Amazon Mechani-
cal Turk2 (Mturk). We prompt the crowd workers
as follows: “Your task is to compare two given
items in one simple language paragraph so that a
knowledgeable person who reads it can guess what
the two things are”. The workers were instructed
to compare only the major and well-known aspects
of the two entities. We also asked them to use X
and Y for anonymously referring to the two enti-
ties. Table 1 shows three examples of our crowd-
sourced comparison paragraphs. As these exam-
ples show, the paragraphs are very contentful and
rich in comparison which meets our initial goals
in the dataset creation.

Entity Pair Selection. The choice of the two
entities which should be compared against each
other plays a key role in the quality of the col-
lected dataset. It is evident that naturally, we com-
pare two things which are semantically similar, yet
have some dissimilarities3, such as jam and jelly.
Given the goals of our task, we experimented with
concrete nouns which share a common taxonomy
class. We choose semantic classes which have at
least five well-known entities. So far, we have
covered nine broad categories as shown in Fig-
ure 2, with 21 subcategories shown in Figure 3.
We use Wikipedia item categories and the Word-
Net (Miller, 1995) ontology for identifying en-
tities from each subcategory. Then, we choose
the most common entities by looking up their fre-
quency on Google Web 1T N-grams4. We manu-
ally inspected the frequency-filtered list to make
sure that the entities are rather easy to describe
without getting technical. Given the list of enti-
ties, we paired each entity with at most five and
at least three other entities from the same subcate-
gory. We also include inter-subcategory compar-

2www.mturk.com
3Tversky’s (1978) analysis of similarity suggests that sim-

ilarity statements compare objects that belong to the same
class of things.

4https://catalog.ldc.upenn.edu/
ldc2006t13

907



Comparison Paragraph Entity X Entity Y

Both X and Y are fruits and a variety of apples. X and Y are generally similar
in size. X are dark red in color when ripe, while Y are a bright green color. X
is sweeter and softer than Y in taste and texture, sometimes starchy. Y are tart
and somewhat stringy. Y is often used in cooking, whereas X is not.

Red Delicious
Apple
Fruit

Granny Smith
Apple
Fruit

The X and Y are two types of vehicles. X is a smaller vehicle than Y. The
X has two wheels while Y has none. The X travels on roadways and smooth
surfaces, whereas Y is capable of flying. Only one or two people are able to
ride on X at once, while Y can carry more people.

Motorcycle
Motor Vehicle
Vehicle

Helicopter
Aircraft
Vehicle

X and Y are both types of world cuisines. X incorporates a lot of pasta dishes
and sauces, with basil, tomato, and cheese being major ingredients. Y consists
of many curries and stir fried dishes, with coconut and lemongrass being used
often. Y is generally spicier and more aromatic than X. X is a European
cuisine, while Y is an Asian cuisine.

Italian Cuisine
Cuisine
Cuisine

Thai Cuisine
Cuisine
Cuisine

Table 1: Examples from the GuessTwo comprehension dataset. Also provided with the dataset is the
subcategory and the broad category of the entities which are listed below the entity names in this Table.

Figure 1: An example illustrating the entity pair
matching process.

ison for a handful of entities at the boundaries.
Figure 1 illustrates our entity pair matching pro-
cess with an example on subcategories ‘apple’ and
‘citrus’.

Data Quality Control. Our task of free-form
writing is trickier than many other tasks such as
tagging on Mturk. To instruct the non-expert
workers, we designed a qualification test on Mturk
in which the workers had to judge whether or not a
given paragraph is acceptable according to our cri-
teria. We used three carefully selected paragraphs
to be a part of the qualification test. Moreover,
to further ensure the quality of the submissions,
one of our team members qualitatively browsed
through the submissions and gave the workers de-
tailed feedback before approving their paragraphs.

For each pair of entities, we collected eight
comparison paragraphs from different workers.
Given that different workers have different per-
spectives on what the major aspects to be com-
pared are, collecting multiple paragraphs helps
further enriching our dataset. We constrained the
paragraphs to be at least 250 characters and at
most 850 characters. Table 2 shows the basic
statistics of our dataset. In this Table, we also in-
cluded the median number of adjectives (includ-

ing comparatives) per paragraph as a measure of
descriptiveness of the comparison paragraphs. As
a point of reference, the median number of adjec-
tives in a random Wikipedia paragraph of the same
length is 5.

Figure 2: Distribution of broad category
of the entities.

Figure 3: Distribution of subcategory of
the entities.

Given the quality control we have in place, our
data collection is going slowly. So far we have col-
lected 14,142 paragraphs; however, we are aiming

908



Number of total approved paragraphs 14,142
Number of workers participated 649
Average number of paragraphs by one worker 21.7
Average work time among workers (minutes) 17.3
Median work time among workers (minutes) 6.4
Payment per paragraph (cents) 50
Number of broad entity categories 9
Number of entity sub-categories 24
Number of unique entities 920
Number of unique pairs compared 1974
Median number of sentences per paragraph 7
Median number of tokens per paragraph 70
Median number of adjectives per paragraph 7

Table 2: Statistics of the GuessTwo dataset as of
April 2017.

Figure 4: An example showing the entity pairs in
the test and training sets.

to expand the resource over time.

Test Set Creation. In order to enable bench-
marking on the task, we assessed the quality of
a random sample of GuessTwo paragraphs as fol-
lows: we show the paragraph to three human
workers on Mturk and ask them to guess what the
two things are. Then, we choose 520 paragraphs
for which all three workers have made exactly cor-
rect guesses for both entities. The test set will also
be expanded along with the further data collection.

We divided the rest of the GuessTwo dataset
into training and validation sets, with a 90%/10%
split. To ensure that the test set requires some
level of basic reasoning, our training set does not
share any exact entity pairs with the validation or
test set. This further enforces systems to learn
about entities indirectly by processing across para-
graphs. For instance, as shown in Figure 4, at test
time, a system should be able to guess a compari-
son involving the entities blood orange vs. lemon
by having seen comparisons of blood orange vs.
tangerine and tangerine vs. lemon.

Our dataset will be released to the pub-
lic through https://omidb.github.io/
guesstwo/.

3 The GuessTwo Task Definition

We define the following two different schemes for
the GuessTwo task:
• Open-ended GuessTwo. Given a short para-
graph P which compares two entities X and Y,
guess what the two entities are. The scope of this
prediction is the set of all entities appearing in the
training dataset.
• Binary Choice GuessTwo. Given a short para-
graph P which compares two entities X and Y,
and two nominals n1 and n2, choose 0 if n1 =
X and n2 = Y, choose 1 otherwise.

We speculate that system which can success-
fully tackle the GuessTwo task, has achieved two
major objectives: (1) Has successfully learned the
knowledge about entities stored in any form (e.g.,
continuous-space representation or symbolic) (2)
Has a basic natural language understanding ca-
pability, using which, it can comprehend a para-
graph and access its knowledge. We predict that
our training dataset has enough detailed informa-
tion about entities for learning the required knowl-
edge for tackling the task. Given the design of
our dataset, at test time, a system should perform
some level of reasoning to go beyond understand-
ing only one paragraph.

4 Neural Models

In this Section we present various end-to-end neu-
ral models for tackling the task of GuessTwo.

Continuous Bag-of-words Language Model.
This model computes the probability of a sequence
of consecutive words in context. The premise is
that the probability of a paragraph with the correct
realization of X and Y should be higher than the
a paragraph with incorrect realizations. In order
to compute the probability of a word given a con-
text we use Continuous Bag-of-words (CBOW)
(Mikolov et al., 2013a) which models the follow-
ing conditional probability:

p(w|C(w), θ) (1)

here, C(w) is the context of the word w and θ is
the model parameters. Then, the probability of a
sequence of words (in a paragraph) is computed as
follows:

n∏

i=1

p(wi|C(wi), θ) (2)

We define context to be a window of five words.
Figure 5a summarizes this model. We train this

909



(a) The CBOW model.

(b) The CNN open-ended model.

(c) The CNN binary-choice model.

Figure 5: Various neural models for tackling the task of GuessTwo.

Figure 6: The Encoder-Decoder model.

model on two datasets: (1) A collection5 of pro-
cessed Wikipedia articles. Wikipedia articles often
include definitions and descriptions of variety of
items, which can provide a reasonable resource for
our task. (2) the GuessTwo training dataset. We
call these models CBOW-Wikipedia and CBOW-
GuessTwo respectively.

At test time, for open-ended prediction we find
the two nominals which maximize the following
probability:

argmax
x,y

n∏

i=1

p(wi|C(wi)x,y, θ) (3)

where C(wi)x,y indicates the context in which any
occurrences of X have been replaced with x and
Y’s have been replaced with y. For binary choice
classification, we use the same modeling except
that we only consider x = n1, y = n2 and x =
n2, y = n1.

Encoder-Decoder Recurrent Neural Net
5http://mattmahoney.net/dc/text8.zip

(RNN). This model is a sequence-to-sequence
generation model (Cho et al., 2014; Sutskever
et al., 2014) that maps an input sequence to an
output sequence using an encoder-decoder RNN
with attention (Bahdanau et al., 2014). The en-
coder RNN processes the comparison paragraph
and the decoder generates the first item followed
by the second item (Figure 6). The paragraph
is encoded into a state vector of size 512. This
vector is then set as the initial recurrent state of
the decoder. We tune the model parameters on the
validation set, where we set the number of layers
to 2. The model is trained end-to-end, using
Stochastic Gradient Descent with early stopping.

For open-ended prediction, we use beam search
with beam-width = 25 and then output the two
tokens with the highest probability. For binary
choice classification, we use the same model
where we set the encoder RNN inputs to the in-
put paragraph tokens, then, we set the input of the
decoder RNN once to [n1, n2] and next to [n2,
n1]. After running the network forward, we take
the probability of the decoder logits and choose
the ordering which has the highest probability.

Convolutional Neural Network (CNN) En-
coder. As shown in the Figure 5b, this model first
uses a Convolutional Neural Network (CNN) (Le-
Cun and Bengio, 1998) for encoding the paragraph
(Kim, 2014). We train a simple CNN with one
layer of convolution on top of pre-trained word
vectors. Here we use the word vectors trained by

910



Be - Be

The set

Physical object - X Physical object - Y

Both

Fruit - Apple

Neutral1 Neutral

Sequence1 Sequence

Operator

Figure 7: Semantic parsing for the sentence Both
X and Y are apples.

Skip-gram model (Mikolov et al., 2013b) on 100
billion words of Google News6. For open-ended
prediction, the output of CNN is fed forward and
transformed into a 300 dimension vector. Then,
we use a softmax layer to get the probability of
each of the possible nominals forX andY. For bi-
nary choice classification, we use the same archi-
tecture and settings as above. Additionally, we en-
code each nominal into a 300-dimensional vector,
which then gets concatenated with the paragraph
vector. Figure 5c shows this model.

5 Semantic-driven Model

In this Section we present a semantic-driven ap-
proach which models the comparison paragraph
using semantic features and is capable of perform-
ing basic reasoning across paragraphs.

5.1 Representing Paragraphs
The question is, given a comparison paragraph,
what is the best representation which can enable
further reasoning? The comparison paragraphs of-
ten have complex syntactic and semantic struc-
tures, which might be challenging for many off-
the-shelf NLP tools to process. For instance, con-
sider the sentence X is much sweeter in taste than
Y. Although a dependency parser provides a lot
of information regarding how the individual words
relate grammatically, it does not give us any in-
formation regarding how Y’s sweetness (which is
elided from the sentence and is implicit) relates to
X’s. As another processing technique, if we use
the standard information extraction methods for
extracting and representing syntactic triplets (ar-
gument1, relation, argument2) (Fader et al., 2014;
Etzioni et al., 2011), we will extract a triplet such

6https://code.google.com/archive/p/
word2vec/

as X is sweeter which shares the same shortcom-
ings.

Our approach for better representation of com-
parison paragraphs starts with a broad-coverage
semantic parser (Banarescu et al., 2013; Bos,
2008; Allen et al., 2008). A semantic parser maps
an input sentence to its formal meaning represen-
tation, operating at the generic natural language
level. Here we use the TRIPS7 (Allen et al., 2008)
broad-coverage semantic parser. TRIPS provides
a very rich semantic structure; mainly it provides
sense disambiguated deep structures augmented
with semantic ontology types. Figure 7 shows
an example TRIPS semantic parse. In this graph
representation, each node specifies a word in bold
along with its corresponding ontology type on its
left. The edges in the graph are semantic roles8.
As you can see, this semantic parse represents the
sentence by decoupling the token ‘both’ and at-
tributing the property of ‘be apple’ to both X and
Y.

In our comparison paragraphs there are two ma-
jor types of sentences:
• Sentences with Absolute Information. These
sentences contain direct information about the en-
tities, such as X is red or Both X and Y are very
sweet. From each absolute sentence, we extract
frames which describe the absolute attributes of
the corresponding entity. We define a frame to
be a subgraph of a semantic parse which involves
exactly one entity and all of its semantic roles.
Relying on the deep semantic features offered by
the semantic parser, we perform negation propaga-
tion9 and sequence decoupling, among others fea-
tures. For example, given a sentence which has
a sequence, as the one depicted in Figure 7, we
perform sequence decoupling and extract the two
frames [X Be Apple] and [Y Be Apple].
• Sentences with Relative Information. These
sentences contain relative information about the
two entities, for instance, X is somewhat sweeter
thanY. As opposed to the sentences with absolute
information, we cannot extract frames from sen-
tences with comparisons directly. Various proper-
ties of entities can be associated with an abstract
scale, such as ‘size’ or ‘sweetness’, on which dif-

7http://trips.ihmc.us/parser/cgi/parse
8Refer to http://trips.ihmc.us/parser/

LFDocumentation.pdf for the full list of semantic roles
in TRIPS parser.

9A common construction which needs negation propaga-
tion is Neither X nor Y are ... .

911



Comparative>

X is sweet -er than Y.

Scale/+

Figure

Ground

Figure 8: The comparison construction predicted
for the sentence X is sweeter than Y.

ferent entities can be compared. In order to extract
such scales and the relative standing of items on
them we use the structured prediction model pre-
sented in Bakhshandeh et al. (2016), which given
a sentence predicts its comparison structures. Fig-
ure 8 shows an example predicate-argument struc-
ture that is predicted by this model. We use pre-
trained model on the annotated corpus (Bakhshan-
deh et al., 2016) of comparison structures.

Given a comparison structure such as the one
presented in Figure 8, we can extract the informa-
tion that on the scale of ‘sweetness’ X is higher
thanY. It is clear that one can build a large knowl-
edge base of such relations by reading large col-
lections of comparison paragraphs. We populate
our knowledge base of relative information about
entities as follows: First, we predict the compar-
ison structure of each sentence and then extract a
binary relation≺s which shows the relation on the
scale of s. Second, for any scale s, we apply tran-
sitivity on its entities. As shown in equation 4,
the binary relation ≺s is transitive over the set of
all entities, A. This process, called closure, en-
ables us do basic reasoning and derives implicit
relations on scales from explicit relations.

∀s ∈ S ∀x, y, z ∈ A : (x ≺s y ∧ y ≺s z)
=⇒ x ≺s z (4)

The product of this step is a structured knowl-
edge base on entity ordering which we call the or-
dering lattice. Figure 9 shows an example partial
ordering lattice inferred by our model, where the
sweetness of Golden Delicious can be compared
to Granny Smith through their direct link with Red
Delicious.

5.2 Modeling

Given a paragraph P , we first extract the set of all
the absolute information frames for X and Y (as
described above), called FX(P ) and FY(P ). Sec-
ond, for the sentences with relative information,

Figure 9: The inferred partial ordering lattice com-
paring the sweetness of different apples.

we extract all the binary relations ≺s∈ R(P ) that
should hold between X and Y. Then, our objec-
tive is to find two realizations for X and Y that
maximize the following:

argmax
x,y

p(x|FX(P )) + p(y|FY(P ))

s.t. ∀ ≺s∈ R(P ) : x ≺s y (5)
In order to compute the p(x|FX(P )) and

p(y|FY(P )) scores we used Regularized Gradient
Boosting (XGBoost) classifier (Friedman, 2000),
which uses a regularized model formulation to
limit overfitting. We directly use each frame in the
FX(P ) and FY(P ) sets as the classifier features.
We use Integer Linear Programming (ILP) for for-
mulating the constraints as follows: for each rela-
tion r ∈ R on the scale s, we lookup the scale
s in the ordering lattice and make the blacklist
B(P ) containing each pair of entities which do
not satisfy the relation r. Our ordering lattice does
not have perfect complete information, hence, we
have Open World Assumption and only prune our
search space not to include the already observed
pairs which violate the relation. our ILP objective
function will be the following:

argmax
b,b′

∑

x∈N
bx p(x|FX(P )) +

∑

y∈N
b′y p(y|FY(P ))

s.t. ∀ (j, j′) ∈ B(P ) : bj + b′j′ ≤ 1 (6)
whereN is the set of all possible realizations and b
and b′ are the binary indicator variables, so bx = 1
indicates the realization of x for X.

912



In the case of open-ended prediction, the maxi-
mization presented in Equation 6 is carried out on
the set N . In the case of binary choice classifica-
tion, however, only the two choices of n1 and n2
are considered in the maximization.

6 Results

We evaluate all the models presented in Sections 4
and 5 using the following accuracy measure:

#correct predictions of both entities
#test cases

(7)

As for the open-ended prediction we compute the
nominator of the accuracy measure using three
various matching methods on both entities: (1)
exact-match, (2) subcategory match, (3) broad cat-
egory match.

As Table 3 shows, the semantic model outper-
forms all the neural models. Moreover, the ILP
constraints have been very effective in directing
the system in the correct search space. Among
the neural models, the Encoder-Decoder RNN
model performs noticeably better than other mod-
els when matching the subcategory and broad cat-
egory. According to the exact-matching, neither
of the CBOW models could guess any of the two
test entities correctly. Overall, it is evident that
the end-to-end neural models have not been able
to generalize well and learn about the attributes of
entities across various training paragraphs. This
can be partly due to not being trained on large
enough comparison training dataset. The seman-
tic model, however, could outperform the neural
models using the same amount of data. To a de-
gree, this is because the semantic model leverages
the basic language understanding capabilities of-
fered by the semantic parser.

It is also important to note that our seman-
tic approach is not only capable of binary and
open-ended prediction, but it also offers two by-
products that can be used as knowledge in a vari-
ety of other tasks: (1) a set of the most important
absolute information frames which can be chosen
based on feature importance in the classification,
(2) the partial ordering lattice of entities. Over-
all, the results strongly suggest that the GuessTwo
task is challenging, with the open-ended scheme
being the most challenging. There is a wide gap
between human and system performance on this
task, which makes it a very promising task for the
community to pursue.

Model Binary Open-ended
Exact. Subcat.

Human 100.0 94.2 100.0
CBOW-Wikipedia 51.9 0.0 1.5
CBOW-GuessTwo 51.7 0.0 1.1
Encoder-Decoder RNN 58.8 2.9 6.8
CNN 57.6 1.9 2.5
Semantic (no constraints) 61.5 10.5 38.5
Semantic (with ILP constraints) 69.2 11.7 40.4

Table 3: System accuracy results on the GuessTwo
test set. A random baseline on binary choice task
achieves 51%. The open-ended evaluation has
two columns: exact-match (exact) and subcate-
gory match (subcat), respectively.

7 Related Work

The task of Machine Comprehension (MC) has
gained a significant attention over the past few
years. The major driver for MC has been the
publicly available benchmarking datasets. A va-
riety of MC tasks have been introduced in the
community (Richardson et al.; Hermann et al.,
2015; Rajpurkar et al., 2016; Hill et al., 2015),
in which the system reads a short text and an-
swers a few multiple-choice questions. The read-
ing comprehension involved in these tests ranges
from reading a short fictional story (Richardson
et al.) to reading a short news article (Hermann
et al., 2015). In comparison, in the GuessTwo
task the reading comprehension involves reading
a short comparison paragraph and one can say the
multiple-choice question is the constant What are
X and Y?

The CNN/DailyMail dataset consists of more
than 100K short news articles with the questions
automatically created from the bullet-point sum-
maries of the original article. This dataset uses
fill-in-the-blank-style questions such as ‘Producer
X will not press charges against Jeremy Clark-
son’ where the system should choose among all
the anonymized entities in the corresponding para-
graph to fill in X. The Stanford Question Answer-
ing (SQuAD) dataset is another recent machine
comprehension test with over 500 Wikipedia ar-
ticles and +100,000 crowdsourced questions. The
answer to every question in this dataset is a span
of text from the corresponding reading passage.

Human accuracy on CNN/DailyMail is esti-
mated to be around 75% (Chen et al., 2016) with
the current state-of-the-art at 76.1 on CNN (Sor-
doni et al., 2016), and 75.8 on DailyMail (Chen
et al., 2016). The human F1 score on SQuAD

913



dataset is reported to be at 86.8%, with the cur-
rent state-of-the-art achieving 82.9%. Given these
statistics, neither of these datasets leave enough
room for further research. Given that in both these
tasks the answer to the question is directly found
in the provided passage, we argue that the commu-
nity requires a more challenging MC task which
goes beyond matching and needs some level of in-
ference across passages. The GuessTwo task re-
quires basic reasoning and inference across para-
graphs for comprehending various aspects of enti-
ties relative to one another.

Another interesting task is MCTest (Richard-
son et al.), which is a reading comprehension
test with 660 fictional stories as the passage and
four questions per story. The human-level per-
formance on MCTest is estimated to be around
90%, with the state-of-the-art achieving an accu-
racy of 70% (Wang et al., 2015). MCTest is also
proven to be challenging, however, given its very
limited training data, further progress on the task
has been hindered. Yet another relevant QA task
is the Allen AI Science Challenge (Clarke et al.,
2010; Schoenick et al., 2016), which is a dataset
of multiple-choice questions and answers from a
standardized 8th grade science exam. The ques-
tions can range from simple fact lookup to com-
plex ones which require extensive world knowl-
edge and commonsense reasoning. This task re-
quires machine reading of a variety of resources
such as textbooks and goes beyond reading a cou-
ple of passages.

8 Conclusion

We introduced the novel task of GuessTwo, in
which given a short paragraph comparing two
common entities, a system should guess what the
two entities are. The comparison paragraphs of-
ten have complex semantic structures which make
this comprehension task demanding. Furthermore,
guessing the two entities requires a system to go
beyond only understanding one given passage and
requires reasoning across paragraphs, which is one
of the most under-explored, yet crucial, capabili-
ties of an intelligent agent.

So far, we have crowdsourced a dataset of more
than 14K comparison paragraphs comparing enti-
ties from nine major categories. For benchmark-
ing the progress, we filter a collection of these
paragraphs to create a test set, on which humans
perform with an accuracy of 94.2%. For contin-

uing our data collection, we would like to have
a targeted entity pair selection where we partic-
ularly collect the missing relations in our partial
ordering lattice. We believe that this process can
help developing more effective systems. For the
most recent statistics of the dataset and the best
performing systems please check this website.

We presented a host of neural models and a
novel semantic-driven approach for tackling the
task of GuessTwo. Our experiments show that the
semantic approach outperforms the neural mod-
els by a large margin. The poor performance of
the neural models we experimented with can mo-
tivate designing new architectures which are ca-
pable of performing basic reasoning across para-
graphs. The results strongly suggest that bridging
the gap between system and human performance
on this task requires models with richer language
representation and reasoning capabilities. As a fu-
ture work, we would like to explore the feasibility
of marrying our semantic and neural models to ex-
ploit the benefits that each of them has to offer.

9 Acknowledgments

This work was supported in part by Grant
W911NF-15-1-0542 with the US Defense Ad-
vanced Research Projects Agency (DARPA) and
the Army Research Office (ARO). We would like
to thank Linxiuzhi Yang for her help in the data
collection and anonymous reviewers for their in-
sightful comments on this work. We specially
thank William de Beaumont for his invaluable
feedback on this paper. We also thank the inputs
from Steven Piantadosi, Brad Mahon, and Gregory
Carlson on cognitive aspects of comparison.

References
James F. Allen, Mary Swift, and Will de Beau-

mont. 2008. Deep semantic analysis of
text. In Proceedings of the 2008 Conference
on Semantics in Text Processing. Associa-
tion for Computational Linguistics, Strouds-
burg, PA, USA, STEP ’08, pages 343–354.
http://dl.acm.org/citation.cfm?id=1626481.1626508.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR
abs/1409.0473. http://arxiv.org/abs/1409.0473.

Omid Bakhshandeh, Alexis Cornelia Wellwood, and
James Allen. 2016. Learning to jointly predict el-
lipsis and comparison structures. In Proceedings
of The 20th SIGNLL Conference on Computational

914



Natural Language Learning. Association for Com-
putational Linguistics, Berlin, Germany, pages 62–
74. http://www.aclweb.org/anthology/K16-1007.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representa-
tion for sembanking. In Proceedings of the
7th Linguistic Annotation Workshop and Interoper-
ability with Discourse. Association for Computa-
tional Linguistics, Sofia, Bulgaria, pages 178–186.
http://www.aclweb.org/anthology/W13-2322.

Johan Bos. 2008. Wide-coverage semantic analysis
with boxer. In Johan Bos and Rodolfo Delmonte,
editors, Semantics in Text Processing. STEP 2008
Conference Proceedings. College Publications, Re-
search in Computational Semantics, pages 277–286.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka, and Tom M. Mitchell.
2010. Toward an architecture for never-ending lan-
guage learning. In In AAAI.

Danqi Chen, Jason Bolton, and Christopher D. Man-
ning. 2016. A thorough examination of the cnn/-
daily mail reading comprehension task. In Associa-
tion for Computational Linguistics (ACL).

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078 .

James Clarke, Dan Goldwasser, Ming-Wei Chang,
and Dan Roth. 2010. Driving semantic pars-
ing from the world’s response. In Proceedings
of the Fourteenth Conference on Computa-
tional Natural Language Learning. Associa-
tion for Computational Linguistics, Strouds-
burg, PA, USA, CoNLL ’10, pages 18–27.
http://dl.acm.org/citation.cfm?id=1870568.1870571.

Allan M. Collins and Elizabeth F. Loftus. 1975. A
spreading-activation theory of semantic processing.
Psychological Review 82(6):407 – 428.

George S. Cree and Ken Mcrae. 2003. Analyzing the
factors underlying the structure and computation of
the meaning of chipmunk, cherry, chisel, cheese, and
cello (and many other such concrete nouns). Journal
of Experimental Psychology: General 132(2):163–
201+.

Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam Mausam. 2011.
Open information extraction: The second gener-
ation. In Proceedings of the Twenty-Second In-
ternational Joint Conference on Artificial Intelli-
gence - Volume Volume One. AAAI Press, IJCAI’11,
pages 3–10. https://doi.org/10.5591/978-1-57735-
516-8/IJCAI11-012.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information
extraction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Association for Computational Linguistics, Strouds-
burg, PA, USA, EMNLP ’11, pages 1535–1545.
http://dl.acm.org/citation.cfm?id=2145432.2145596.

Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2014. Open question answering over curated and
extracted knowledge bases. In Proceedings of
the 20th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. ACM,
New York, NY, USA, KDD ’14, pages 1156–1165.
https://doi.org/10.1145/2623330.2623677.

Jerome H. Friedman. 2000. Greedy function approx-
imation: A gradient boosting machine. Annals of
Statistics 29:1189–1232.

V. Hazlitt. 1933. The psychology of in-
fancy. E.P. Dutton and company, inc.
https://books.google.com/books?id=I8svAAAAYAAJ.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems. pages 1693–
1701.

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. 2015. The goldilocks principle: Reading
children’s books with explicit memory representa-
tions. International Conference on Learning Repre-
sentations (ICLR) .

Yoon Kim. 2014. Convolutional neural networks
for sentence classification. In Alessandro Mos-
chitti, Bo Pang, and Walter Daelemans, edi-
tors, Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Process-
ing, EMNLP 2014, October 25-29, 2014, Doha,
Qatar, A meeting of SIGDAT, a Special Inter-
est Group of the ACL. ACL, pages 1746–1751.
http://aclweb.org/anthology/D/D14/D14-1181.pdf.

Yann LeCun and Yoshua Bengio. 1998. The
handbook of brain theory and neural net-
works. MIT Press, Cambridge, MA, USA,
chapter Convolutional Networks for Images,
Speech, and Time Series, pages 255–258.
http://dl.acm.org/citation.cfm?id=303568.303704.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word repre-
sentations in vector space. CoRR abs/1301.3781.
http://arxiv.org/abs/1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S
Corrado, and Jeff Dean. 2013b. Distributed
representations of words and phrases and their
compositionality. In C. J. C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K. Q. Wein-
berger, editors, Advances in Neural Information

915



Processing Systems 26, Curran Associates, Inc.,
pages 3111–3119. http://papers.nips.cc/paper/5021-
distributed-representations-of-words-and-phrases-
and-their-compositionality.pdf.

George A. Miller. 1995. Wordnet: A lexical
database for english. Commun. ACM 38(11):39–41.
https://doi.org/10.1145/219717.219748.

Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016. A cor-
pus and cloze evaluation for deeper understanding
of commonsense stories. In Proceedings of the
2016 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies. Association for Com-
putational Linguistics, San Diego, California, pages
839–849. http://www.aclweb.org/anthology/N16-
1098.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text .

Matthew Richardson, Christopher J. C. Burges, and
Erin Renshaw. ???? Mctest: A challenge dataset for
the open-domain machine comprehension of text.
pages 193–203.

Carissa Schoenick, Peter Clark, Oyvind Tafjord,
Peter D. Turney, and Oren Etzioni. 2016.
Moving beyond the turing test with the allen
AI science challenge. CoRR abs/1604.04315.
http://arxiv.org/abs/1604.04315.

Alessandro Sordoni, Phillip Bachman, and Yoshua
Bengio. 2016. Iterative alternating neural atten-
tion for machine reading. CoRR abs/1606.02245.
http://arxiv.org/abs/1606.02245.

Robert Speer and Catherine Havasi. 2012. Repre-
senting general relational knowledge in concept-
net 5. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Mehmet Uur
Doan, Bente Maegaard, Joseph Mariani, Asun-
cion Moreno, Jan Odijk, and Stelios Piperidis, ed-
itors, Proceedings of the Eight International Con-
ference on Language Resources and Evaluation
(LREC’12). European Language Resources Associ-
ation (ELRA), Istanbul, Turkey.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems 27: Annual Conference on Neural In-
formation Processing Systems 2014, December 8-
13 2014, Montreal, Quebec, Canada. pages 3104–
3112. http://papers.nips.cc/paper/5346-sequence-
to-sequence-learning-with-neural-networks.

Amos Tversky and Itamar Gati. 1978. Studies of simi-
larity. Cognition and categorization 1(1978):79–98.

Hai Wang, Mohit Bansal, Kevin Gimpel, and David A.
McAllester. 2015. Machine comprehension with
syntax, frames, and semantics. In Proceedings
of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Lan-
guage Processing, ACL 2015, July 26-31, 2015, Bei-
jing, China, Volume 2: Short Papers. The Asso-
ciation for Computer Linguistics, pages 700–706.
http://aclweb.org/anthology/P/P15/P15-2115.pdf.

916


	Apples to Apples: Learning Semantics of Common Entities Through a Novel Comprehension Task

