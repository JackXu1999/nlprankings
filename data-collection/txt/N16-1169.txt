



















































Automatic Generation and Scoring of Positive Interpretations from Negated Statements


Proceedings of NAACL-HLT 2016, pages 1431–1441,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Automatic Generation and Scoring of Positive Interpretations
from Negated Statements

Eduardo Blanco and Zahra Sarabi
Human Intelligence and Language Technologies Lab

University of North Texas
Denton, TX, 76203

eduardo.blanco@unt.edu, zahrasarabi@my.unt.edu

Abstract

This paper presents a methodology to extract
positive interpretations from negated state-
ments. First, we automatically generate plau-
sible interpretations using well-known gram-
mar rules and manipulating semantic roles.
Second, we score plausible alternatives ac-
cording to their likelihood. Manual annota-
tions show that the positive interpretations are
intuitive to humans, and experimental results
show that the scoring task can be automated.

1 Introduction

Negation is an intricate phenomenon present in all
human languages (Hoeksema, 2000), and studied
from a theoretical perspective since Aristotle. Ac-
quiring and understanding negation is more chal-
lenging than language in general: children acquire
negation after learning to communicate (Nordmeyer
and Frank, 2013), and adults take longer to pro-
cess negative sentences than positive ones (Clark
and Chase, 1972). In any given language, humans
communicate in positive terms most of the time, and
use negation to express something unusual or an ex-
ception (Horn, 1989).

In classical logic, negation is a simple unary op-
erator that reverses the truth value of a proposi-
tion. In natural language, negation is always marked
(Horn and Wansing, 2015) and it is used to re-
verse polarity, i.e., turning something affirmative
into negative, or something negative into affirma-
tive. Albeit most sentences are affirmative, negation
is rather ubiquitous (Morante and Sporleder, 2012):
In scientific papers, 13.76% of statements contain a
negation (Szarvas et al., 2008); in product reviews,
19% (Councill et al., 2010); and in a selection of

Conan Doyle stories, 22.23% (Morante and Daele-
mans, 2012). In health records, 12.3% of concepts
are tagged as negated (Elkin et al., 2005); and in
OntoNotes (Hovy et al., 2006), 10.15% of state-
ments contain a verb negated with not, n’t or never.

From a theoretical perspective, it is accepted that
negation conveys positive meaning (Rooth, 1992;
Huddleston and Pullum, 2002). For example, when
reading (1) John doesn’t eat meat, humans intu-
itively understand that (1a) John eats something
other than meat, and (1b) Some people eat meat, but
not John. Extracting positive interpretations from
negated statements automatically is not straightfor-
ward: a negated statement may convey one or more
positive interpretations, and not all positive interpre-
tations are equally likely. For example, from (2)
They didn’t order the right parts, it is very likely that
(2a) They ordered the wrong parts, but (2b) Some-
body ordered the right parts, but not they is unlikely.

This paper presents a methodology to automati-
cally extract and score positive interpretations from
negated statements, as intuitively done by humans
when reading text. A key feature of the work pre-
sented here is that it is not tied to any existing ap-
proach to extract meaning from text—we generate
positive interpretations in plain text, and these pos-
itive interpretations can be semantically represented
with any existing approach. The main contributions
are: (1) procedure to automatically generate plausi-
ble positive interpretations from negated statements,
(2) annotations scoring plausible positive interpre-
tations,1 and (3) experiments detailing results with
several combinations of features, as well as gold-
standard and predicted linguistic information.

1Available at http://hilt.cse.unt.edu/

1431



2 Background and Definitions

Negation is well-understood in grammars, the valid
ways to form a negation are well-documented
(Quirk et al., 2000; van der Wouden, 1997). Nega-
tion can be expressed by verbs (e.g., avoid doing any
look-up), nouns (e.g., the absence of any phonic se-
quence), adjectives (e.g., it is pointless to argue with
a fool), adverbs (e.g., I never tried Persian food be-
fore), prepositions (e.g., you can always exchange it
without a problem), determiners (e.g., the new law
has no direct implications to international shipping),
pronouns (e.g., nobody will keep election promises).

Huddleston and Pullum (2002) distinguish four
negation types:

• Verbal if the marker of negation is grammati-
cally associated with a verb, e.g., I did not see
anything, non-verbal if it is associated with a
dependent of the verb, e.g., I saw nothing.

• Synthetic if the negation mark has a func-
tion besides marking a negation, e.g.,
[Nobody]AGENT liked it, analytic otherwise,
e.g., Not many people liked it.

• Clausal if the negation yields a negative clause,
e.g., The terms aren’t negotiable, subclausal
otherwise, e.g., The terms are non-negotiable.

• Ordinary if the negation indicates that some-
thing is not the case, e.g., That car does not
drive smooth, metalinguistic if it does not dis-
pute the truth but rather reformulates a state-
ment, e.g., That TV is not small, it is tiny.

In this paper, we target verbal, analytic, clausal
and both ordinary and metalinguistic negation.

2.1 Positive Interpretations.

In philosophy and linguistics, it is generally
accepted that negation conveys positive mean-
ings (Horn, 1989). These positive meanings range
from implicatures, i.e., what is suggested in an ut-
terance even though neither expressed nor strictly
implied (Blackburn, 2008), to entailments. Other
terms used in the literature include implied mean-
ings (Mitkov, 2005), implied alternatives (Rooth,
1985) and semantically similars (Agirre et al.,
2013). We do not strictly fit into any of this terminol-
ogy, we reveal positive interpretations as intuitively
done by humans when reading text.

2.2 Scope and Focus.

From a theoretical perspective, it is accepted that
negation has scope and focus, and that the focus—
not just the scope—yields positive interpretations
(Horn, 1989; Rooth, 1992; Taglicht, 1984). Scope
is “the part of the meaning that is negated” and fo-
cus “the part of the scope that is most prominently or
explicitly negated” (Huddleston and Pullum, 2002).

Consider the following statement in the context
of the recent refuge crisis: (3) Mr. Haile was
not looking for heaven in Europe. By definition,
scope refers to “all elements whose individual falsity
would make the negated statement strictly true”, and
focus is “the element of the scope that is intended to
be interpreted as false to make the overall negative
true” (Huddleston and Pullum, 2002). The falsity of
any of the truth conditions below makes statement
(3) true, thus the scope of the negation is (3a–3d):

3a. Somebody was looking for something some-
where. [verb looking]

3b. Mr. Haile was looking for something some-
where. [AGENT of looking, Mr. Haile]

3c. Somebody was looking for heaven some-
where. [THEME of looking, heaven]

3d. Somebody was looking for something in Eu-
rope. [LOCATION of looking, in Europe]

Determining the focus is almost always more
challenging than the scope. The challenge lies on
determining which of the truth conditions (3a–3d)
is intended to be interpreted as false to make the
negated statement true: all of them qualify, but some
are more likely. A natural reading of statement (3)
suggests that Mr. Haile was looking for something (a
regular life, a job, etc.) in Europe, but not heaven.
Determining that the focus is heaven, i.e., that ev-
erything in statement (3) is actually positive except
the THEME of looking, is the key to reveal the in-
tended positive interpretation. It is worth noting
that other foci yield unlikely interpretations, e.g.,
Somebody was looking for heaven in Europe, but not
Mr. Haile (3b, AGENT), Mr. Haile was looking for
heaven somewhere, but not in Europe (3d, LOCA-
TION). Note that (1) scope on its own does not yield
positive interpretations, and (2) some negated state-
ments convey several likely positive interpretations,
e.g., statement (1) in Section 5, Table 3.

1432



3 Previous Work

Within computational linguistics, approaches to pro-
cess negation are shallow, or target scope and focus
detection. Popular semantic representations such as
semantic roles (Palmer et al., 2005; Baker et al.,
1998) or AMR (Banarescu et al., 2013) do not reveal
the positive interpretations we target in this paper.
Shallow approaches are usually application-specific.
In sentiment and opinion analysis, negation has been
reduced to marking as negated all words between a
negation cue and the first punctuation mark (Pang et
al., 2002), or within a five-word window of a nega-
tion cue (Hu and Liu, 2004). The examples through-
out this paper show that these techniques are insuffi-
cient to reveal implicit positive interpretations.

3.1 Scope Annotation and Detection
Scope of negation detection has received a lot of
attention, mostly using two corpora: BioScope in
the medical domain (Szarvas et al., 2008) and CD-
SCO (Morante and Daelemans, 2012). BioScope
annotates negation cues and linguistic scopes exclu-
sively in biomedical texts. CD-SCO annotates nega-
tion cues, scopes, and negated events or properties
in selected Conan Doyle stories.

There have been several supervised proposals to
detect the scope of negation using BioScope and
CD-SCO (Özgür and Radev, 2009; Øvrelid et al.,
2010). Automatic approaches are mature (Abu-
Jbara and Radev, 2012): F-scores are 0.96 for nega-
tion cue detection, and 0.89 for negation cue and
scope detection (Velldal et al., 2012; Li et al., 2010).
Outside BioScope and CD-SCO, Reitan et al. (2015)
present a negation scope detector for tweets, and
show that it improves sentiment analysis. As shown
in Section 2, scope detection is insufficient to reveal
positive interpretations from negated statements.

3.2 Focus Annotation and Detection
While focus of negation has been studied for
decades in philosophy and linguistics (Section 2),
corpora and automated tools are scarce. Blanco and
Moldovan (2011) annotate focus of negation in the
3,993 negations marked with ARGM-NEG semantic
role in PropBank (Palmer et al., 2005). Their an-
notations, PB-FOC, were used in the *SEM-2012
Shared Task (Morante and Blanco, 2012). Their
guidelines require annotators to choose as focus the

semantic role that “is most prominently negated” or
the verb. If several roles may be the focus, they
prioritize “the one that yields the most meaningful
implicit [positive] information”, but do not specify
what most meaningful means. Consider again state-
ment (1) John doesn’t eat meat. Their approach
would determine that the focus is the THEME of eat,
meat, because it arguably yields the “most meaning-
ful implicit [positive] information” (using our termi-
nology, positive interpretation): John eats something
other than meat. By design, they ignore other valid
positive interpretations, e.g., Some people eat meat,
but not John. In this paper, we improve upon their
work: instead of extracting the “most meaningful”
positive interpretation from a negated statement, we
generate several positive interpretations and score
them according to their likelihood.

Anand and Martell (2012) present a complimen-
tary approach to annotate focus of negation. They
refine PB-FOC and argue that positive interpre-
tations arising from scalar implicatures and neg-
raising predicates should be separated from those
arising from focus detection. According to their an-
notations, 27.4% of negations with a focus annotated
in PB-FOC do not actually have a focus. Blanco
and Moldovan (2012) introduce the concept of fine-
grained foci and refine the annotations in PB-FOC
by annotating foci at the token level, and Matsuyoshi
et al. (2014) annotate focus of negation in Japanese.
In this paper, we are not concerned about annotat-
ing focus of negation per se, but about extracting
positive interpretations from negated statements as
intuitively understood by humans.

Automatic systems to detect the focus of nega-
tion (and reveal up to one positive interpretation)
in English texts are trained using PB-FOC. Blanco
and Moldovan (2011) obtain an accuracy of 65.5
using supervised machine learning and features de-
rived from gold-standard linguistic information, and
Blanco and Moldovan (2014) report an F-measure
of 64.1. Rosenberg and Bergler (2012) report an F-
measure of 58.4 using 4 linguistically sound heuris-
tics and predicted linguistic information, and Zou
et al. (2014) an F-measure of 65.62 using contex-
tual discourse information. Unlike the work pre-
sented here, none of these systems attempts to ex-
tract and rank several positive interpretations from
one negated statement.

1433



Neg. statement In 1995, Murdoch bought [the rest of Star TV]ARG1 that [he]ARG0 did [not]ARGM-NEG [own]verb

Positive
counterpart

after Step 1 In 1995, Murdoch bought the rest of Star TV that he did own
after Step 2 In 1995, Murdoch bought the rest of Star TV that he owned
after Step 3 In 1995, Murdoch bought the rest of Star TV that he owned (no change)

Plausible
positive
interpretations

from ARG0 In 1995, Murdoch bought the rest of Star TV that [somebody] owned, but not he
from ARG1 In 1995, Murdoch bought [something] that he owned, but not the rest of Star TV
from verb In 1995, Murdoch bought the rest of Star TV that he [some verb], but not owned

Table 1: Negated statement, and automatically generated positive counterpart and plausible positive interpretations.

4 Corpus Creation

Our goal is to create a corpus of negated statements
and their positive interpretations as intuitively un-
derstood by humans. We put a strong emphasis on
automation. First, given a negated statement, we
automatically generate plausible positive interpreta-
tions following a battery of linguistically motivated
deterministic rules (Section 4.1). Second, we collect
manual annotation to score the plausible positive in-
terpretations according to their likelihood (Section
4.2). We then use these manually obtained scores to
learn models that automatically score positive inter-
pretations (Section 6).

We decided to work on top of OntoNotes (Hovy
et al., 2006) instead of plain text or other corpora
for several reasons. First, OntoNotes includes gold
linguistic annotations such as part-of-speech tags,
parse trees and semantic roles. Second, state-of-
the-art role labelers trained with Propbank achieve
F-measures of 0.835 (Lewis et al., 2015), and we
use semantic roles to generate positive interpreta-
tions. Third, unlike BioScope, CD-SCO and PB-
FOC (Section 2), OntoNotes includes sentences
from several genres, e.g., newswire, broadcast news
and conversations, magazines, the web.

4.1 Generating Positive Interpretations

OntoNotes2 is a large corpus containing 63,918 sen-
tences. Annotating all positive interpretations from
all negations is outside of the scope of this paper.
Instead, we target selected representative negations.
Selecting Negated Statements. We first selected all
verbs negated with ARGM-NEG semantic role and
obtained 6,617 verbal negations. After examining
the negated verbs, it became clear that negation is
not uniformly distributed across verbs in OntoNotes,

2We use the CoNLL-2011 Shared Task distribution (Pradhan
et al., 2011), http://conll.cemantix.org/2011/

it roughly follows Zipf’s law. In order to allevi-
ate the annotation effort while accounting for all
negated verbs in OntoNotes, we randomly selected
up to 5 negations for each verb. The number of
negated statements selected is 600.
Converting Negated Statements into Their Posi-
tive Counterparts. We apply 3 steps inspired after
the grammatical rules to form negation detailed by
Huddleston and Pullum (2002, Ch. 9):

1. Remove the negation mark by removing the to-
kens within ARGM-NEG semantic role.

2. Remove auxiliaries, expand contractions, and
fix third-person singular and past tense. For ex-
ample (before: after), doesn’t go: goes, didn’t
go: went, won’t go: will go, We use a standard
list of irregular verbs,3 and grammar rules to
convert to third-person singular and past tense
based on orthographic patterns.

3. Rewrite negatively-oriented polarity-sensitive
items. For example (before: after), any-
one: someone, any longer: still, yet: al-
ready. at all: somewhat. We use the cor-
respondences between negatively-oriented and
positively-oriented polarity-sensitive items by
Huddleston and Pullum (2002, pp. 831).

Generating Positive Interpretations. Once the
positive counterpart is obtained, we generate posi-
tive interpretations by rewriting each semantic role
or the (originally negated) verb. Among others,
we use the following rewriting rules: ARG0–ARG4:
someone / some people / something, ARGM-TMP:
at some point of time, ARGM-LOC: somewhere,
ARGM-MNR: in some manner, ARGM-CAU: because
of something and ARGM-PRP: to do something. Ad-
ditionally, if the semantic role starts with a prepo-
sition, we also include it, e.g., gave [to John]ARG2 :

3https://en.wikipedia.org/wiki/English_
irregular_verbs

1434



gave to someone, but not John. This methodol-
ogy generated 1,888 positive interpretations from
the 600 selected negations (average: 3.15).

Table 1 exemplifies the 3 steps to transform a
negated statement into its positive counterpart, and
the positive interpretations generated. Additional
examples are provided in Table 3.

We acknowledge that some of the positive inter-
pretations we generate automatically are not as spe-
cific or intuitive as carefully crafted, manually gen-
erated interpretations could be. For example, from
[John]ARG0 does[n’t]ARGM-NEG [know]verb [the de-
tails about how they met]ARG1 , the proposed method-
ology would generate, among others, John knows
about something, but not about the details of how
they met. A better interpretations that we do not gen-
erate is John knows something about how they met,
but not the details. We argue that generating inter-
pretations automatically is the only option in order
to incorporate this work into an NLP pipeline, and
reserve for future work generating positive interpre-
tations beyond rewriting semantic roles.

4.2 Ranking Positive Interpretations

Once positive interpretations were automatically
generated, we asked annotators to rank them. An-
notators were presented with one negated statement
and one positive interpretation at a time, and were
asked Given the negated statement above, do you
think the statement [positive interpretation] below
is true? They only had access to the text in the
original negated statement, the positive interpreta-
tion, and the previous and next sentences as context.
We did not display semantic role information for the
original negated statement, its positive counterpart
or the semantic role from which the positive inter-
pretation was generated. As we shall see (Section
5.1), context often helps scoring interpretations.

Annotators were required to answer with a score
from 0 to 5, were 0 means absolutely disagree and
5 means absolutely agree. We did not provide de-
scriptions for intermediate scores or used additional
categorical labels. This simple guidelines were suf-
ficient to reliably score plausible positive interpreta-
tions automatically generated.

Sem. role # % sent Mean SD
ARG0 392 65.17% 4.07 1.03
ARG1 520 86.00% 4.55 0.98
ARG2 95 15.83% 4.40 1.31
ARG3 2 0.33% 4.50 0.50
ARG4 5 0.83% 5.00 0.00
ARGM-ADV 101 15.67% 2.41 1.72
ARGM-CAU 13 2.17% 2.46 1.91
ARGM-DIR 11 1.83% 2.00 2.30
ARGM-EXT 12 2.00% 3.75 1.53
ARGM-LOC 21 3.33% 3.76 1.69
ARGM-MNR 37 6.00% 4.54 1.03
ARGM-PRP 7 1.16% 4.43 0.71
ARGM-TMP 81 12.50% 3.99 1.52
Verb 600 100.00% 2.17 1.41
All 1,888 100.00% 3.52 1.63

Table 2: Basic corpus analysis. For each semantic role, we
show the number of positive interpretations generated (#), the

percentage of sentences for which a positive interpretation is

generated (% sent), and the mean and standard deviation (SD)

of the annotated scores.

5 Corpus Analysis

On average, we generated 3.15 positive interpreta-
tions per negation (standard deviation: 0.82), and
74% of negations have at least one interpretation
scored 4 or higher. Basic counts and statistics for
the annotations are provided in Table 2. Over-
all, we annotated 1,888 positive interpretations gen-
erated from 600 sentences, or equivalently, from
600 verbs negated with ARGM-NEG semantic role
in OntoNotes. Overall mean score is 3.52 (out of
5) and overall standard deviation, 1.63. The 25th
percentile is 2.0, the 50th percentile is 4.0 and the
75th percentile is 5.0. These numbers show that
most positive interpretations automatically gener-
ated are deemed likely by annotators, and over 25%
are scored with a 5 (out of 5).

In general, positive interpretations generated from
numbered roles (ARG0–ARG4) are scored higher
than the ones generated from modifiers (ARGM-
ADV, ARGM-CAU, ARGM-DIR, etc.). Also, positive
interpretations generated from infrequent roles are
generally ranked higher, e.g., ARG4 and ARG3 vs.
ARG0, ARGM-PRP and ARGM-MNR vs. ARGM-ADV.
Annotation Quality. In order to ensure annotation
quality, we calculated inter-annotator Pearson cor-
relation. Kappa and other agreement measures de-

1435



Negated statement, context if relevant to determining scores, and all positive interpretations Score

1

Context, previous statement: That change will obviously impact third and fourth quarter earnings for the
industry in general, he added.
Negated statement: [He]ARG0 did[n’t]ARGM-NEG [forecast]verb [Phillips’ results]ARG1.
Context, next statement: But security analysts say Phillips will be among the companies hard-hit by weak
chemical prices and will probably post a drop in third-quarter earnings.
- Interpretation 1.1 [ARG0]: Somebody forecasted Phillips’ results, but not he. 5
- Interpretation 1.2 [ARG1]: He forecasted something, but not Phillips’ results. 5

2 Negated statement: In 1995, Murdoch bought [the rest of Star TV]ARG1 [he]ARG0 did [not]ARGM-NEG [own]verb.
- Interpretation 2.1 (ARG0): In 1995, Murdoch bought the rest of Star TV somebody owned, but not he. 5
- Interpretation 2.2 (ARG1): In 1995, Murdoch bought something he owned, but not the rest of Star TV. 0

3 Negated statement: [You]ARG0 [can]ARGM-MOD[not]ARGM-NEG [run]verb [a country with 23 million people]ARG1
[with revenues of 16 billion to 20 billion dollars]ARGM-MNR.
- Interpretation 3.1 (ARG0): Somebody can run a country with 23 million people with revenues of 16
billion to 20 billion dollars, but not You.

1

- Interpretation 3.21 (ARG1): You can run something with revenues of 16 billion to 20 billion dollars,
but not a country with 23 million people.

5

- Interpretation 3.3 (ARGM-MNR): You can run a country with 23 million people in some manner, but
not with with revenues of 16 billion to 20 billion dollars.

5

4 Negated statement: Do [not]ARGM-NEG [utter]verb [a word]ARG1 .
- Interpretation 4.1 (ARG1): Utter something, but not a word. 0

Table 3: Annotation examples. We show all positive interpretations automatically generated and their scores (out of 5).

signed for categorical labels are not well-suited for
our annotation task, since not all disagreements be-
tween numeric scores are the same, e.g., 4 vs. 5
should be counted as relatively high agreement, and
1 vs. 5 should be counted as high disagreement.
Overall Pearson correlation is 0.761.

5.1 Annotation Examples

Table 3 presents annotation examples. We show the
original negated statement including semantic role
annotations from OntoNotes (square brackets), all
positive interpretations automatically generated, and
their scores. We also include context (previous and
next sentence) if it helps determining scores.

Example (1) shows that context sometimes is vital
to scoring plausible positive interpretations. Given
He didn’t forecast Phillips’ results in isolation, it is
uncertain if he forecasted anything at all, or whether
somebody forecasted Phillips’ results. However, the
previous statement makes certain (5/5) the interpre-
tation generated from ARG1: he forecasted earnings
for the industry in general. Similarly, the next state-
ment makes very likely (5/5) the interpretation gen-
erated from ARG0: other people (security analysts)
made forecasts about Phillips. In this example, 2

positive interpretations are generated from one nega-
tion, and they are assigned the highest score (5/5).

The positive interpretations generated from exam-
ple (2) can be annotated without context. Somebody
other than Murdoch had to own the rest of Star TV
that he bought in 1995 (score 5/5), and people can-
not buy what they already own (score 0/5).

Example (3) presents a positive interpretation
generated from ARG0 that is scored low (1/5); re-
call that the mean score for interpretations gener-
ated from ARG0 is 4.07 and the standard deviation
1.03 (Table 2). In this negated statement, the indef-
inite you refers to an unspecified person, thus it is
not the case that somebody can run a country with
23 million people with revenues of 16 billion to 20
billion dollars (Interpretation 3.1, score 1/5). Inter-
pretations 3.2 and 3.3, however, are scored high: an-
notators correctly understood that given the negated
statement, it is the case that You can run something
with revenues of 16 billion to 20 billion dollars, but
not a country with 23 million people (you can run a
country with less people with that revenue), and You
can run a country with 23 million people in some
manner, but not with revenues of 16 billion to 20 bil-
lion dollar (you could run it with more revenue).

1436



Finally, example (4) presents a short statement
from which only one positive interpretations is gen-
erated. Annotators were asked whether given Do not
utter a word, they think Utter something, but not a
word is true. They correctly annotated that this posi-
tive interpretations is invalid (score 0/5). Indeed, the
negated statement in example (4) can only be inter-
preted as an order to not utter anything.

6 Learning to Score Positive
Interpretations

We follow a standard supervised machine learning
approach. The 1,888 positive interpretations along
with their scores become instances, and we divide
them into training (80%) and test splits (20%) mak-
ing sure that all interpretations generated from a sen-
tence are assigned to either the training or test splits.
Note that splitting instances randomly would not be
sound: training with some interpretations generated
from a negated statement, and testing with the rest
of interpretations generated from the same statement
would be an unfair evaluation.

We trained a Support Vector Machine (SVM)
for regression with RBF kernel using scikit-
learn (Pedregosa et al., 2011), which in turn uses
LIBSVM (Chang and Lin, 2011). The feature set
and SVM parameters (C and γ) were tuned using
10-fold cross-validation with the training set, and re-
sults were calculated using the test set.

6.1 Feature Selection

We tried features derived exclusively from the
negated statement from which the positive inter-
pretation was generated, more specifically, we ex-
tract features from the negated verb or semantic
role (sem role) used to generate the positive inter-
pretation, from both of them (verb-sem role) and
from the verb-argument structure of verb (verbarg-
struct), i.e., all semantic roles of verb.

Verb features are straightforward and account for
the verb word form and part-of-speech tag.

Sem role features include the label of the seman-
tic role from which the positive interpretation was
generated (sem role label), its length (number of to-
kens), and the word form and part-of-speech tag of
its head. Additionally, we add standard features in
semantic role labeling (Gildea and Jurafsky, 2002):

the syntactic nodes (NP, PP, etc.) of the semantic
role and its parent in the parse tree, as well as the
left and right siblings, if any.

Verb-sem role features are also standard in role
labeling. We include a flag indicating whether the
verb occurs before or after the semantic role in the
negated statement (not in the positive counterpart),
syntactic node of the lowest common ancestor be-
tween verb and sem role, and the syntactic path.

Finally, verbarg-struct features encode character-
istics of the verb-argument structure to which verb
and sem role belong to. Namely, we added flags in-
dicating semantic role presence, and features indi-
cating the first and last semantic roles in order of ap-
pearance in the negated statement. We also included
the syntactic nodes of semantic roles and their heads.

We exemplify all features with Interpretation 3.1
generated from Statement 3 in Table 3:

• Verb features: verb wf=run, verb pos=VBP.
• Sem role features: label=ARG0, num tokens=1,

head wf=You, head pos=PRP, synt node=NP,
synt node parent=S, synt node left=NONE,
synt node right=VP

• Verb-sem role features: direction=after, low-
est ancestor=S, synt path=VP+VP+S-NP

• Verbarg-struct features: ARG0=1, ARGM-
MOD=1, ARG1=1, ARGM-MNR=1, first=ARG0,
last=ARGM-MNR, ARG1 head=country,
ARG1 pos=NN, etc.

7 Experimental Results

We report results obtained with several combina-
tions of features in Table 5. We detail results ob-
tained with features extracted from gold-standard
and predicted linguistic annotations (part-of-speech
tags, parse trees, semantic roles, etc.) as annotated
in the gold and auto files from the CoNLL-2011
Shared Task release of OntoNotes (Pradhan et al.,
2011). All models are trained with gold-standard
linguistic annotations, and tested with either gold-
standard or predicted linguistic annotations.
Testing with gold-standard linguistic annota-
tions. Using only the label of the semantic role
from which the positive interpretation was gener-
ated (sem role label), yields a Pearson correlation
of 0.603. Using Verb features is virtually useless
(Pearson correlation: -0.025), this is due to the

1437



Source Feature description
Verb Word form and part-of-speech tag of verb

sem role
Semantic role label of sem role, sem role label
Number of tokens in sem role
Word form and part-of-speech tag of head of sem role
Syntactic node of sem role, its parent and left and right siblings in the parse tree

verb-sem role
Whether verb occurs before or after than sem role in the negated statement
Syntactic node of lowest common ancestor of verb and sem role
Syntactic paths from verb to sem role

verbarg-struct
Flags indicating whether verb has each possible semantic role
Semantic role labels of the first and last roles of verb
Syntactic nodes and heads of each semantic role attaching to verb

Table 4: Features used to score positive interpretations from negated statements. Features are extracted from the negated verb
or semantic role (sem role) from which the positive interpretation was generated, from both of them (verb-sem role), or from the

verb-argument structure of verb (verbarg-struct), i.e., from all semantic roles of verb.

Gold Predicted
sem role label feature 0.603 0.642
verb features -0.025 -0.022
verb + sem role features 0.630 0.648
verb + sem role + verb-sem role features 0.627 0.638
verb + sem role + verb-sem role + verbarg-struct features 0.642 0.650

Table 5: Pearson correlations in the test set using gold-standard and predicted linguistic annotations (part-of-speech tags, parse
trees and semantic roles). Results are provided using sem role label as only feature, and using several features incrementally.

Number of test instances with gold-standard linguistic annotations is 378, and with predicted annotations, 268.

fact that our corpus includes at most 5 instances
for each negated verb in OntoNotes (Section 4).
Adding sem role features yields a correlation of
0.630, and incorporating verb-sem role features is
useless (Pearson: 0.627). Considering all features,
however, yields the highest correlation, 0.642.

Testing with predicted linguistic annotations. We
assigned 20% of instances to the test split, totalling
378 instances (Section 6). However, some of these
instances cannot be obtained using predicted role la-
bels: a missing or incorrect semantic role will un-
equivocally lead to positive interpretations that are
not in our corpus and thus evaluation is not straight-
forward. Results presented with predicted linguistic
annotations are calculated using only the 268 (out of
378) positive interpretations that are generated from
predicted semantic roles and are also generated (and
thus annotated in our corpus) from gold-standard
linguistic annotations.

Results using only sem role label feature (0.642)
are better or very similar than using any combina-
tion of features (0.638–0.650) except Verb features
alone, which perform poorly as explained earlier.

These results should be taken with a grain of salt:
the number of test instances is much lower (268 vs.
378). Additionally, these 268 test instances corre-
spond to positive interpretations generated from se-
mantic roles that were predicted correctly automat-
ically. Role labels are predicted better for shorter
sentences without complicated syntactic structure;
positive interpretations for this kind sentences are
also easier to score, e.g., Statement 4 in Table 3.

8 Conclusions

Humans intuitively understand negated statements
in positive terms when reading text. This pa-
per presents an automated methodology to generate
plausible positive interpretations from verbal nega-
tion, and score them based on their likelihood. We
use simple grammar rules and manipulate semantic
roles to generate positive interpretations. Experi-
mental results show that these interpretations can be
scored automatically using standard supervised ma-
chine learning techniques.

An annotation effort shows that most positive
interpretations automatically generated are likely

1438



(scores ≥4), thus the amount of positive meaning
revealed by the methodology presented here is sub-
stantial (on average, 3.15 interpretations are gener-
ated per negation). We believe that more annotations
and a learning algorithm that scores jointly all posi-
tive interpretations generated from each negation (as
opposed to individually) would yield better results.

References

Amjad Abu-Jbara and Dragomir Radev. 2012. Umichi-
gan: A conditional random field model for resolving
the scope of negation. In Proceedings of the First Joint
Conference on Lexical and Computational Semantics-
Volume 1: Proceedings of the main conference and
the shared task, and Volume 2: Proceedings of the
Sixth International Workshop on Semantic Evaluation,
pages 328–334. Association for Computational Lin-
guistics.

Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity. In Second Joint
Conference on Lexical and Computational Semantics
(*SEM), Volume 1: Proceedings of the Main Confer-
ence and the Shared Task: Semantic Textual Similarity,
pages 32–43, Atlanta, Georgia, USA, June. Associa-
tion for Computational Linguistics.

Pranav Anand and Craig Martell. 2012. Annotating
the focus of negation in terms of questions under dis-
cussion. In Proceedings of the Workshop on Extra-
Propositional Aspects of Meaning in Computational
Linguistics, ExProM ’12, pages 65–69, Stroudsburg,
PA, USA. Association for Computational Linguistics.

Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of the 17th international conference on Computa-
tional Linguistics, Montreal, Canada.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguistic
Annotation Workshop and Interoperability with Dis-
course, pages 178–186, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.

Simon Blackburn. 2008. The Oxford Dictionary of Phi-
losophy. Oxford University Press.

Eduardo Blanco and Dan Moldovan. 2011. Semantic
representation of negation using focus detection. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 581–589, Portland, Ore-

gon, USA, June. Association for Computational Lin-
guistics.

Eduardo Blanco and Dan Moldovan. 2012. Fine-
grained focus for pinpointing positive implicit mean-
ing from negated statements. In Proceedings of the
2012 Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 456–465, Montréal,
Canada, June. Association for Computational Linguis-
tics.

Eduardo Blanco and Dan Moldovan. 2014. Retriev-
ing implicit positive meaning from negated statements.
Natural Language Engineering, 20:501–535, 10.

Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3):27:1–27:27, May.

H. H. Clark and W. G. Chase. 1972. On the process of
comparing sentences against pictures. Cognitive Psy-
chology, 3(3):472–517, July.

Isaac Councill, Ryan McDonald, and Leonid Velikovich.
2010. What’s great and what’s not: learning to clas-
sify the scope of negation for improved sentiment anal-
ysis. In Proceedings of the Workshop on Negation and
Speculation in Natural Language Processing, pages
51–59, Uppsala, Sweden, July. University of Antwerp.

Peter L. Elkin, Steven H. Brown, Brent A. Bauer, Casey
S. Husser, William Carruth, Larry R. Bergstrom, and
Dietlind L. Wahner-Roedler. 2005. A controlled trial
of automated classification of negation from clinical
notes. BMC Medical Informatics and Decision Mak-
ing, 5(13).

Daniel Gildea and Daniel Jurafsky. 2002. Auto-
matic labeling of semantic roles. Comput. Linguist.,
28(3):245–288, September.

Jack Hoeksema. 2000. Negative polarity items: trigger-
ing, scope,and c-command. In L. Horn and Y. Kato,
editors, Negation and polarity, chapter 4, pages 115–
146. Oxford University Press, Oxford.

Laurence R. Horn and Heinrich Wansing. 2015. Nega-
tion. In Edward N. Zalta, editor, The Stanford Ency-
clopedia of Philosophy. Summer 2015 edition.

Laurence R. Horn. 1989. A natural history of negation.
Chicago University Press, Chicago.

Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% Solution. In NAACL ’06: Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers on XX,
pages 57–60, Morristown, NJ, USA. Association for
Computational Linguistics.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In KDD ’04: Proceedings
of the tenth ACM SIGKDD international conference

1439



on Knowledge discovery and data mining, pages 168–
177, New York, NY, USA. ACM.

Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press, April.

Mike Lewis, Luheng He, and Luke Zettlemoyer. 2015.
Joint a* ccg parsing and semantic role labelling. In
Empirical Methods in Natural Language Processing.

Junhui Li, Guodong Zhou, Hongling Wang, and Qiaom-
ing Zhu. 2010. Learning the Scope of Negation via
Shallow Semantic Parsing. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (Coling 2010), pages 671–679, Beijing, China,
August. Coling 2010 Organizing Committee.

Suguru Matsuyoshi, Ryo Otsuki, and Fumiyo Fukumoto.
2014. Annotating the focus of negation in japanese
text. In Nicoletta Calzolari, Khalid Choukri, Thierry
Declerck, Hrafn Loftsson, Bente Maegaard, Joseph
Mariani, Asuncion Moreno, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC’14), pages 1743–1750, Reykjavik, Ice-
land, May. European Language Resources Association
(ELRA). ACL Anthology Identifier: L14-1606.

Ruslan Mitkov. 2005. The Oxford handbook of compu-
tational linguistics. Oxford University Press.

Roser Morante and Eduardo Blanco. 2012. *SEM 2012
Shared Task: Resolving the Scope and Focus of Nega-
tion. In Proceedings of the First Joint Conference on
Lexical and Computational Semantics (*SEM 2012),
pages 265–274, Montréal, Canada, June.

Roser Morante and Walter Daelemans. 2012.
Conandoyle-neg: Annotation of negation in conan
doyle stories. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Evalu-
ation, Istanbul.

Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special issue.
Comput. Linguist., 38(2):223–260, June.

Ann E Nordmeyer and Michael C Frank. 2013. Measur-
ing the comprehension of negation in 2-to 4-year-old
children. Proceedings of the 35th Annual Conference
of the Cognitive Science Society. Austin, TX: Cognitive
Science Society.

Lilja Øvrelid, Erik Velldal, and Stephan Oepen. 2010.
Syntactic Scope Resolution in Uncertainty Analysis.
In Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pages
1379–1387, Beijing, China, August. Coling 2010 Or-
ganizing Committee.

Arzucan Özgür and Dragomir R. Radev. 2009. Detect-
ing Speculations and their Scopes in Scientific Text.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages

1398–1407, Singapore, August. Association for Com-
putational Linguistics.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71–106.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In Proceedings of the 2002
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79–86. Association for Com-
putational Linguistics, July.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825–
2830.

Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1–27, Portland,
Oregon, USA, June. Association for Computational
Linguistics.

Randolph Quirk, Sidney Greenbaum, and Geoffrey
Leech. 2000. A comprehensive grammar of the En-
glish language. Longman, London.

Johan Reitan, Jørgen Faret, Björn Gambäck, and Lars
Bungum. 2015. Negation scope detection for twitter
sentiment analysis. In Proceedings of the 6th Work-
shop on Computational Approaches to Subjectivity,
Sentiment and Social Media Analysis, pages 99–108,
Lisboa, Portugal, September. Association for Compu-
tational Linguistics.

Mats Rooth. 1985. Association with focus. Ph.D. the-
sis, Dept. of Linguistics, University of Massachusetts,
Amherst.

Mats Rooth. 1992. A theory of focus interpretation. Nat-
ural language semantics, 1(1):75–116.

Sabine Rosenberg and Sabine Bergler. 2012. Ucon-
cordia: Clac negation focus detection at *sem 2012.
In *SEM 2012: The First Joint Conference on Lexi-
cal and Computational Semantics – Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 294–300, Montréal, Canada, 7-8 June. Associa-
tion for Computational Linguistics.

György Szarvas, Veronika Vincze, Richárd Farkas, and
János Csirik. 2008. The BioScope corpus: annotation

1440



for negation, uncertainty and their scopein biomedical
texts. In Proceedings of BioNLP 2008, pages 38–45,
Columbus, Ohio, USA. ACL.

Josef Taglicht. 1984. Message and emphasis: On fo-
cus and scope in English, volume 15. Addison-Wesley
Longman Limited.

Ton van der Wouden. 1997. Negative contexts: colloca-
tion, polarity, and multiple negation. Routledge, Lon-
don.

Erik Velldal, Lilja Ovrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers, and the role of syntax. Comput. Linguist.,
38(2):369–410, June.

Bowei Zou, Guodong Zhou, and Qiaoming Zhu. 2014.
Negation focus identification with contextual dis-
course information. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 522–530,
Baltimore, Maryland, June. Association for Computa-
tional Linguistics.

1441


