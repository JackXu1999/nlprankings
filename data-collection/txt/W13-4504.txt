Proceedings of the Workshop on Language Processing and Crisis Information 2013, pages 26–35,

Nagoya, Japan, 14 October 2013. c(cid:13)2013 Asian Federation of Natural Language Processing

26

A Framework and Tool for

Collaborative Extraction of Reliable Information

Graham Neubig1, Shinsuke Mori2, Masahiro Mizukami1

1Graduate School of Information Science, Nara Institute of Science and Technology

8916-5 Takayama-cho, Ikoma-shi, Nara, Japan

2Academic Center for Computing and Media Studies, Kyoto University

Yoshida Honmachi, Sakyo-ku, Kyoto, Japan

Abstract

This research proposes a framework for ef-
ﬁcient information extraction and ﬁltering
in situations where 1) extreme reliability
is important, 2) the amount of information
to be combed through is massive, and 3)
we can expect a relatively large number of
human workers to be available. In particu-
lar, we are motivated by needs in times of
crisis, and assume that in order to ensure
the high level of reliability required, it will
be necessary to have at least one human
worker conﬁrm all extracted information.
Given this setting, we propose a method
to improve the efﬁciency of manual veri-
ﬁcation by deciding which information to
present to workers using machine learn-
ing techniques. Even given this efﬁcient
search framework, the amount of informa-
tion on the internet is still too much for one
user to handle, so we additionally create a
web-based framework that allows for col-
laborative work, and an algorithm that al-
lows for this framework to work on large
data in real-time. We perform an eval-
uation using data from Twitter after the
Great East Japan Earthquake, and com-
pare efﬁciency using both traditional key-
word search and the proposed learning-
based method.

1 Introduction
In times of crisis, internet sites, and particularly
social networks such as Twitter,1 overﬂow with in-
formation, with some reports noting an increase
of activity by as much as 20 fold (Miyabe et al.,
2012a). This information spans all genres, from
questions or comments about the state of affairs,
statements of opinion, emotional pleas, or even the

1http://twitter.com/ retrieved on 2013-4-9.

spread of false rumors (Qu et al., 2009; Mendoza
et al., 2010). Perhaps the information of the most
interest is that which helps either crisis-responders
or evacuees get a better grasp of situation (Vieweg
et al., 2010), and this is particularly true when the
information is provided directly by people in the
disaster-affected areas (Starbird et al., 2012).

“there is water at

However, distinguishing useful

information
the evacuation cen-
(e.g.
ter in Sendai high school”) from unreliable or
non-actionable information (e.g. “just arrived at
the evacuation center, so tired...”)
takes a large
amount of human effort. Luckily, however, the
effort of good-willed internet users is one thing
that is often plentiful in times of crisis. There
have been many success stories where volunteers
have banded together to turn natural language data
into machine-readable format (Starbird and Stam-
berger, 2010), translate crisis-related information
(Munro, 2010), gather survivor lists from evacu-
ation sites and enter them into a central database
(Google Japan, 2011), or even annotate data for
the creation of specialized information extraction
systems (Neubig et al., 2011). Given the large
amount of work required in these collaborative ef-
forts, it is common for as many as hundreds of vol-
unteers to be involved in any single task.

On the other hand, examinations of the types
of information provided on social networks after
crises have shown that the number of possible in-
formation extraction tasks is large (20-30 by Cor-
vey et al. (2012)’s classiﬁcation). Information re-
quirements also vary greatly from situation to sit-
uation, with the direction of the wind being impor-
tant during the Oklahoma wildﬁres, and radiation
measurements being important after the nuclear
meltdown following the Great East Japan Earth-
quake (Vieweg et al., 2010; Doan et al., 2012).
While a large number of volunteers may be mo-
bilized for a single task, scaling this approach to
tens or hundreds of disparate tasks has not proven

27

possible given in a timely fashion. However, by
increasing the efﬁciency of each volunteer, it is
possible to reduce the overall number of volun-
teers needed, thus increasing the potential to tackle
a much larger number of tasks in the short time-
frame allowed after a disaster.

As a result, there have been a number of works
that attempt to remove the requirement for man-
ual labor by automating the information extrac-
tion process. For example, it has been noted that
it may be possible to automatically identify in-
formation that contributes to situational awareness
in general (Verma et al., 2011), or for more pin-
point tasks such as identifying information about
safety of evacuees (Neubig et al., 2011), evacu-
ation routes (Ishino et al., 2012), or information
providers in disaster affected areas (Starbird et al.,
2012). While these systems are quite promising,
taking human workers out of the loop completely
raises questions regarding the reliability of the in-
formation provided.

Given this background, in this work we examine
a framework that enables teams of volunteers to
identify useful information in a fashion that is efﬁ-
cient, collaborative, and highly reliable. In partic-
ular, to ensure reliability, we assume that all infor-
mation provided must be checked by at least one
volunteer. However, we increase the efﬁciency of
this manual veriﬁcation by learning a classiﬁer to
decide which pieces of information are likely to
be relevant and should be presented to volunteers.
Each time a new piece of information is labeled
as either relevant or irrelevant to the task at hand,
the classiﬁer is updated to be more accurate at the
task. Finally, to take advantage of collaborative
work, we implement the proposed framework in a
web interface that can be used collaboratively by
many workers simultaneously.

Overall, while each of the individual compo-
nents described below are not novel (quite stan-
dard, in fact), our work makes four major contri-
butions: 1) combining these techniques into one
over-spanning framework for efﬁcient information
extraction (Section 2), 2) proposal and evaluation
of the information extraction framework in a col-
laborative setting, something that has not covered
extensively in previous work (Section 3), 3) a rel-
atively extensive manual evaluation of the frame-
work on real large-scale data in times of crisis
(Section 4), and 4) an open-source implementation

of the proposed framework.2

2 Information Filtering/Extraction

Framework

As mentioned in the previous section, the over-
all goal of this paper is to efﬁciently extract reli-
able information from the internet. To formalize
this notion, we deﬁne the target of the information
extraction system as a collection of documents
D = {D1, . . . , DI}. From a given document
Di we would like to gather all pieces of infor-
mation Ti = {ti,1, . . . , ti,J} relevant to our given
task. Each piece of information is vector contain-
ing K slots to be ﬁlled ti,j = {ti,j,1, . . . , ti,j,K}.
In addition, we deﬁne U = {u1, . . . , uI} where
each ui corresponds to document Di and indicates
whether there is at least one piece of useful infor-
mation in the document ui := (|Ti| > 0).
To give a concrete example, let us assume that
the information we are interested in is evacua-
tion areas after a crisis, and the target that we
are extracting from is Twitter posts. In this case,
each document Di would be a Twitter post, and
ui would be a binary variable indicating whether
there is any useful information in the post. ti,j
would be a set of entries about a particular evacu-
ation site, where each column may indicate traits
of the evacuation site such as “city,” “address,” and
“current status.”

In the following two sections, we describe the
proposed framework for ﬁnding this information
in two steps: ﬁltering, where we estimate the
usefulness ui of each document, and extraction,
where we extract the information Ti from docu-
ments that pass the ﬁltering process. In particu-
lar we focus on ﬁltering useful documents from a
large document collection, and use a simple man-
ual extraction process.

2.1 Information Filtering
We ﬁrst describe two approaches to information
ﬁltering: a baseline of keyword search, and our
improved method based on relevance feedback.

2.1.1 Keyword Search
Almost any attempt
to ﬁnd information in a
large document collection will start with keyword
search, where documents are retrieved according
to a user query Q. In the terminology that we in-
troduced above, this means that ui will be true ei-
2Available at http://phontron.com/webigator

28

ther when all of the keywords match
ui := (|Di ∩ Q| = |Q|)

or when at least one of the keywords matches

ui := (|Di ∩ Q| > 0).

(1)

(2)

While this technique is extremely simple, it has
also proven useful in actual rescue efforts that
monitor social networks for information in times
of crisis (Aida et al., 2012).
2.1.2 Information Filtering using Classiﬁers
However, keyword search is clearly not sophis-
ticated enough to adequately make the decision
whether a particular document contains informa-
tion useful to a particular task, with the “and”
search in Equation (1) ﬁltering out too many docu-
ments, and the “or” search in Equation (2) picking
up too much noise. As a solution to this problem,
it is common to use machine learning to create
more sophisticated classiﬁers (Sebastiani, 2002).
Here, we overview classiﬁers in the case of bi-
nary classiﬁcation between ui = 1 (true) and
ui = 0 (false). In this case, we deﬁne N feature
functions ϕn(Di) that express various character-
istics of the document Di. Each feature function
is assigned a weight λn and the weighted sum of
feature functions is the document’s score s(Di)

N∑

s(Di) =

λnϕn(Di).

n=1

(3)

to learn the weights λ

In the case of s(Di) ≥ 0, Di is classiﬁed as a
positive example, and in the case of s(Di) < 0,
Di is classiﬁed as a negative example.
In order
=
(λ1, λ2, . . . , λK), a corpus of documents D
is annotated with labels U∗, and a classiﬁer such
as support vector machines (SVMs) or naive
Bayes classiﬁers is used to train the weight values
(Joachims, 1998). In this research, we use naive
Bayes classiﬁers as they are extremely fast to
learn and perform reasonably well on document
classiﬁcation tasks (Dumais et al., 1998). In naive
Bayes classiﬁers, we calculate the conditional
probability of label u given the feature ϕn

P (u = 1|ϕn) = c(ϕn, u

∗

= 1)/c(ϕn)

where we slightly abuse notation for clarity by
deﬁning c(ϕn) to be the sum of all values of

∗

ϕn(Di) for labeled documents and c(ϕn, u
= 1)
to be the same sum for documents labeled with
∗
= 1. The probability of a label given the docu-
u
ment is the product of the feature probabilities
P (u = 1|ϕn)ϕn(Di)/Z

P (ui = 1|Di) =

∏

n

where Z is normalizes the probabilities to add to 1

Z = P (ui = 1|Di) + P (ui = 0|Di).

We can deﬁne score s(Di) as the log odds of
P (ui = 1|Di)
s(Di) = log P (ui = 1|Di) − log P (ui = 0|Di)
which allows us to deﬁne each weight λn as

λn = log c(ϕn, u

∗
− log c(ϕn, u

= 1)
∗

= 0).

However, zero counts for either positive or nega-
tive labels will cause the log odds to be negative or
positive inﬁnity, so in many cases, the counts are
augmented with a pseudo-count α for smoothing
(Mackay and Petoy, 1995):
∗
− log(c(ϕn, u

= 1) + α)
∗

λn = log(c(ϕn, u

= 0) + α).

(4)

It should also be noted that while standard clas-
siﬁers are trained using the document labels U, it
is also possible to directly label the features ϕn
(Melville et al., 2009; Settles, 2011). In this case,
let l(ϕn, u
= 1) be a function that is 1 if feature
ϕn is labeled positive, and 0 otherwise. We fur-
ther augment Equation (4) with a pseudo-count β
in the case of labeled features

∗

∗
λn = log(c(ϕn, u
− log(c(ϕn, u
∗

∗
= 1) + α + βl(ϕn, u
∗
= 0) + α + βl(ϕn, u

= 1))

= 0)).
(5)

In other words, if a positively labeled feature ex-
ists in a particular document Di, that document
will have a higher chance of being labeled posi-
tive. This is useful in our situation, as any clas-
siﬁer we build will likely be combined with key-
word search as described in the previous section,
and the features corresponding to these keywords
can automatically be labeled as positive.
The bottleneck in the construction of these clas-
siﬁers is the manual creation of the labels U∗,

29

which takes a signiﬁcant amount of time and ef-
fort. Fortunately, there has been some movement
for disaster preparation to create labeled corpora in
the crisis-related domain (Verma et al., 2011; Neu-
big et al., 2011; Corvey et al., 2012). However, as
all pre-constructed resources, these will be neces-
sarily limited to tasks forseen before an actual dis-
aster occurs, and also limited by the language of
the resources (i.e. English or Japanese).

2.1.3 On-the-ﬂy Information Filtering with

Relevance Feedback

In the framework in this paper, we propose using
a different approach that requires no prior creation
of labels U∗ (and thus no foresight into the infor-
mation that may be necessary in any particular sit-
uation), but also can take advantage of machine
learning techniques to improve the accuracy of in-
formation ﬁltering. In order to do so, we start with
no labels and simple keyword search, but utilize
the framework of relevance feedback (Zhou and
Huang, 2003) to iteratively improve the classiﬁer.
The iterative process is as follows:
Search: From all unlabeled documents in D with
at least one matching keyword, the system
selects M documents with the highest score
s(Di) and displays them to the user.

Extraction/Feedback: The user extracts infor-
mation Ti from each document Di (as de-
scribed in the following section), and notes
through the interface whether the document
∗
had any useful information (u
i = 1) or did
∗
not (u
i = 0).

Learning: Once the user ﬁnishes extracting infor-
mation from the M documents, the new la-
bels are submitted to the learning algorithm,
weights are updated, and we return to step 1.

This process is extremely simple, but also satis-
ﬁes a number of desiderata for our system. First,
before any examples are labeled, it is possible to
use simple keyword search as a starting point, so
users can start search immediately without any
prior labeling of data.3 However, we still have the
potential to greatly improve efﬁciency by learn-
ing a classiﬁer that can reduce the number of false
3It is also theoretically possible, and likely useful to up-
date the keywords during the annotation process. This is sup-
ported by the interface, but we decided to avoid keyword up-
dating in our experiments to reduce the number of factors
inﬂuencing results.

∗
examples (u
i = 0) that the user has to view.
Second, the labeling criterion is extremely intu-
itive:
if useful information that can be added to
the database is found, the label is positive, and if
useful information is not found the label is neg-
ative. Thus, users can essentially perform search
exactly as they would normally, but the accuracy
of the search results improves after each set of M
documents is viewed and labeled.

2.2 Information Extraction
The ﬁnal part of the framework is the process of
extracting information Ti from each document Di.
This is an interesting problem in its own right,
with large amounts of work on automated meth-
ods (Sarawagi, 2008). There are also some works
on the extraction of highly reliable information
by including a human in the extraction practice
by either writing regular expressions (Caruana et
al., 2000), or by correcting mistakes made by au-
tomatic extraction methods (Kristjansson et al.,
2004; Culotta and McCallum, 2005). While these
works are relevant to our current task, our evalua-
tion experiments are run on very short documents,
for which the relevant information can manually
be read, copied, and pasted into a spreadsheet with
relatively high speed. Thus, in this work, the ex-
traction of information from relevant documents is
performed entirely by hand, and we leave expan-
sion to more sophisticated methods to future work.

3 Collaborative Interface

While the method described in the previous sec-
tion allows for efﬁcient and reliable information
ﬁltering for a single worker over small-scale data,
it cannot be trivially applied to larger data. The
reason for this is two-fold. First, to implement the
method described in the previous section, every
example must be re-scored every time the classi-
ﬁer weights change, which results in a wait time
between each example that linearly increases with
the size of data at hand. Second, the previously de-
scribed method assumes that there is only a single
worker, but there are physical limits on the amount
of data that a single worker can handle.

In this section, we further improve the frame-
work presented in the previous section by imple-
menting it as a streaming and collaborative in-
formation aggregation interface. The framework
handles information streams by not re-scoring ev-
ery possibly example, but performing greedy re-

30

formation they want to extract (e.g.
“informa-
tion about evacuation areas within Miyagi prefec-
ture”). Next, the users choose one or more key-
words related to this information (e.g. “miyagi”,
“evacuation”). The group leader then creates a lo-
cation where all of the users can aggregate infor-
mation to the speciﬁed topic using an online docu-
ment management service such as Google Docs,4
or a special-purpose web site. Every user in the
group then accesses the annotation interface, la-
bels instances, and inputs the useful information
found in positive instances into the location where
the information is being aggregated. To ensure
that multiple users are not viewing the same in-
formation, each document is only displayed to a
single user.

3.2 Work Flow from the Server Side
When the group leader speciﬁes the ﬁrst keywords
to be used in the task, the computation server
starts to acquire examples from a web information
source such as Twitter. There is a certain amount
of overhead required to run even a simple classi-
ﬁer such as that described in Section 2.1.2, so to
increase the efﬁciency of information retrieval we
ﬁrst perform a simple keyword ﬁltering step that
removes all documents that do not contain at least
one instance of one of the speciﬁed keywords.

Given the keyword-ﬁltered document stream,
the server then calculates the features and current
scores for each of the incoming documents. These
documents are inserted into a cache ordered in de-
scending order of score. In cases where it is neces-
sary to save memory, the cache size can be limited
appropriately, with low scoring candidates being
omitted from the cache and permanently removed
from consideration.

Similarly to the previous section, when the user
labels an example, this label is fed back to the sys-
tem and weights are retrained appropriately. How-
ever, re-scoring every document in the cache each
time the weights are updated will result in a large
time lag at each update.
In order to reduce this
lag, we propose a method for approximately dis-
covering the highest scoring example in the cache
without re-scoring every example.

Speciﬁcally, we would like to display high-
scoring examples to the user, but unless we re-
calculate scores for the entire cache on every
model update there is a possibility that some of

4http://docs.google.com retrieved on 2013-4-9.

Figure 1: Overview of the proposed framework

Figure 2: Example of the annotation interface

scoring of only candidates to be presented to work-
ers and keeping a limited number of top-scoring
candidates in a memory cache. The framework
is collaborative as it is implemented through a
multi-user web interface that communicates with
the scoring and aggregation server running in the
background. An overview of the framework is
shown in Figure 1.

We also show an example of the annotation in-
terface in Figure 2. On the task page, there is a
place to view and enter the keywords for the cur-
rent task, and several (in this case, ﬁve) examples
to be viewed and labeled by the worker. Once the
worker has viewed all instances, and clicked either
“+” for positive or “-” for negative, the labels can
be submitted to the server to update the weights.

3.1 Work Flow from the User Side
We assume a use case where we have a small
group of users, and one of the users is designated
as the leader of the group.

Before the users commence the information ﬁl-
tering process, the group decides the type of in-

31

the scores in the cache will be calculated by an
older version of the model. In order to solve this
problem in an efﬁcient manner, we note that even
though the scores may change somewhat, in most
cases the order of the scores will remain more-or-
less the same as it was before the update. We fur-
ther take advantage of this by greedily searching
for the highest scoring example according to the
following process:

1. According to the (potentially old) scores in
the cache, ﬁnd the highest scoring example
d1 and second-highest scoring example d2.

2. Re-calculate d1’s score according to the cur-

rent version of the model.

3. Compare the score of d1 to the score of the

example d2 according to the old model.5

4. If d1’s new score is higher than d2’s old
score, return d1 as the highest-scoring exam-
ple. Otherwise, re-insert d1 into the cache
and return to step 1.

It should be noted that this search is greedy, and
thus may make mistakes when d1’s score is larger
than d2, but not larger than examples that occur
farther down in the cache.

4 Experimental Evaluation

To evaluate the effectiveness of our proposed
framework and tool for extraction of highly reli-
able information, we perform a series of experi-
mental evaluations. As extraction of highly reli-
able information is particularly important in times
of crisis, we used data provided as part of the Great
East Japan Earthquake Big Data Workshop6 in-
cluding all actual Japanese tweets from Twitter for
one week after the earthquake starting at March
11th, 2011, 14:45, a total of 179 million tweets.

We specify three information extraction tasks as
shown in Table 1, and use these as the targets for
5We use d2’s old model score because the cache order will
change the most when the user labels an example as negative.
In these cases, if both d1 and d2 are similar to the negatively
labeled example, both will see a large reduction in score, so
we d1’s new score will be much smaller than d2’s old score,
but not necessarily smaller than d2’s new score. To ensure
that we continue penalizing high-scoring instances that are
similar to the negatively labeled instance, we continue updat-
ing the cache until we ﬁnd an example that both had a high
score according to the old model (and thus a high position in
the cache), and a high score according to the new model.

6https://sites.google.com/site/prj311/

Figure 3: Results for three tasks with regards to
pieces of information extracted and the 5-minute
rolling average percentage of presented tweets that
contained useful information. The horizontal axis
is time in minutes.

our experiments. The ﬁrst task consists of ﬁnd-
ing information about evacuation areas or rescue
supplies that may be useful to those in disaster-
affected areas, and the other two tasks are related
to ﬁnding posts either requesting or providing in-
formation about the safety of evacuees.

Given our goal of efﬁcient and reliable identi-
ﬁcation and extraction of information as stated in
Section 1, we use as our evaluation measure the
number of tweets able to be veriﬁed by workers
in 30 minutes. In addition, to more closely simu-
late the actual situation of the tool being used in a
crisis-response setting, all workers were asked to
ﬁll in a web form indicating information such as
“location,” “situation,” or “name” in accordance
to the information that would likely be useful for
each of the tasks.

Given this data and these tasks, we perform two
rounds of experiments to compare the efﬁciency
of the learning-based interface compared to simple
keyword search (Section 4.1) and the efﬁcacy of
collaborative work (Section 4.2).

32

Type

Keywords

Evacuation/Rescue Supplies 避難所 (evacuation area), 給水 (water supplies), 炊き出し (food supplies)
Safety Info. Request
Safety Info. Provision

連絡 (contact), 取れない (cannot), 待って (waiting)
連絡 (contact), 無事 (safe)

Table 1: Filtered information and corresponding keywords

4.1 Evaluation of Learning-based

Information Filtering

First, we perform an evaluation of the information
ﬁltering interface described in Section 2.1.3. As
features, we use character 1-to-5-grams,7 and a
naive Bayes classiﬁer, as it is extremely efﬁcient
to both classify and update. In Equation (5), we
set α = 1 and β = 5. As a baseline system, we
use simple keyword search. All results were pro-
vided by a single user who had time to practice
using the interface before results were recorded.

We show the results for the three tasks in Fig-
ure 3. From the results indicating the number of
pieces of useful information extracted on the left
side of the graph, we can see that the proposed
learning capability improves the efﬁciency, with
increases ranging from 35%-159%. This increase
can be largely attributed to an increase in the infor-
mation ﬁltering accuracy, or the number of docu-
ments displayed to the user that have at least one
piece of useful information. The rolling average
of accuracy is shown on the right side of Figure 3.
We can see that there is a signiﬁcant differ-
ence in the information ﬁltering accuracy be-
tween tasks, and this affects the gain afforded
by the learning capability. Speciﬁcally, “Safety
Info.
Provision” has lower accuracy than the
other tasks, largely because it is difﬁcult to distin-
guish between provision of information (“I heard
that XXX is safe”) and requests for information
(“I wonder if XXX is safe”), while the latter is
much more common in the corpus (approximately
ﬁve times according to Murakami and Hagiwara
(2012)’s estimate). Thus, for more difﬁcult tasks
improving the accuracy of the classiﬁers could
lead to further improvements in the gains provided
by the proposed technique.

4.2 Evaluation of the Collaborative Interface
In addition, to evaluate the collaborative web inter-
face described in Section 3, we performed exper-

7Character n-grams remove the effect of analysis mis-
takes that occur when performing Japanese tokenization on
non-standard text such Twitter.

Figure 4: Information veriﬁed by 1, 2, or 3 users

iments in which multiple annotators worked col-
laboratively on an information ﬁltering task. The
experimental setup is identical to that described
in the previous section, but we focus only on the
Evacuation/Rescue Supplies task. As a compari-
son, we compare results for when 1, 2, or 3 users
work collaboratively on a single information ﬁl-
tering task, performing two experiments for each
number of users.

The result of this experiment is shown in Fig-
ure 4. After 30 minutes of work, a single user
had extracted an average of 43 pieces, two users
had extracted 103 pieces, and three users had ex-
tracted 129 pieces of useful information and added
them to the shared aggregation site. Thus, we
can see that increasing the number of users results
in an approximately linear increase in the amount
of information extracted, conﬁrming the effective-
ness of allowing multiple users to work on a single
task, and share the results of labeling with a single
classiﬁer. As each worker works largely indepen-
dently, we hypothesize that this trend will continue
for even larger numbers of users.

In Figure 5 we show the improvement in efﬁ-
ciency of information extraction as each run pro-
gresses. From the graph, we can see that in all
cases, the efﬁciency at the end of the run has in-
creased by 1.3-2.0 times over that achieved in the
initial ﬁve minutes. Figure 6 displays the rolling
average of positive examples, and we can see that

33

tract useful information from the large and noisy
web, with a focus of information extraction from
Twitter during crisis situations. As a result, we
found that the proposed framework led to an in-
crease in efﬁciency of 35%-159% over simple
keyword search, with further gains possible when
more than one user participates in the information
extraction process.

As future work, we can think of expansions to
multi-class information extraction problems.
In
this paper, we limited our experiments to situa-
tions where each classiﬁer is trained to identify
a single type of information, so identifying three
types of information will require three separate
rounds of classiﬁer training. While this is a sim-
ple setup for users to understand, it is inefﬁcient in
the case of a large number of classes, so it is worth
examining the possibilities of extracting multiple
types of information in a single process.

Another interesting line of work is the provision
of extracted information in an easy-to-consume
form for people in disaster areas through a QA sys-
tem that can be accessed through telephone when
other communication tools such as the internet are
not available (Kazama et al., 2012).

Assessing the reliability of information on the
web is an important challenge, particularly in
times of crisis (Mendoza et al., 2010). Work to
automatically assess the reliability of information
may focus on classiﬁers assessing the text, user,
topic, and dispersal patterns of the said informa-
tion (Castillo et al., 2011) or comments on so-
cial networks casting doubt on said information’s
veracity (Miyabe et al., 2012b). These methods
could be combined with our information extrac-
tion method to further ensure the reliability of the
extracted information.

There are also a number of improvements that
could be made to the extraction algorithm itself.
For example, while in this work we used a simple
Naive Bayes classiﬁer, there are classiﬁers devel-
oped speciﬁcally for the task of classifying posi-
tive examples (Sch¨olkopf et al., 2001), which may
increase the accuracy of information identiﬁca-
tion. Other promising directions include the ap-
plication of more advanced information extraction
techniques, the identiﬁcation of information that
is identical to information that has already been
extracted, or application to crowd-sourcing plat-
forms such as Mechanical Turk.

Figure 5: Average pieces of information added by
one user in each time frame

Figure 6: The percentage of examples labeled as
positive in each trial run

the percentage of positive examples labeled in-
creases drastically over time, with accuracy near
100% achieved in four out of six trials, and all tri-
als achieving accuracy over 60%. However, com-
pared to this large increase in the accuracy of the
examples presented to users, the increase in the
amount of information extracted is small. This is
because even after positive information has been
identiﬁed, there is a small but ﬁxed amount of
work required to enter the useful information into
the information aggregation site. As a result, we
can expect that further improvements in informa-
tion extraction efﬁciency can be achieved by auto-
matically extracting candidates to ﬁll in each col-
umn of information to be extracted, and make it
possible for a human to simply press a button to
verify the information if it happens to be correct.

5 Conclusion/Future Work

In this paper, we presented a framework to efﬁ-
ciently, reliably, and collaboratively ﬁlter and ex-

34

Acknowledgments

The authors sincerely thank Shingo Murakami for
his assistance in developing the web interface dur-
ing the Great East Japan Earthquake Big Data
Workshop. We would also like to thank all of the
subjects who lent their time to participate in man-
ual annotation experiments, and the oranizers of
the Great East Japan Earthquake Big Data Work-
shop and Twitter for providing the data that made
the experiments possible.

References
Shin Aida, Yasutaka Shindoh, and Masao Utiyama.
2012. Regarding the creation of the “Great East
Japan Earthquake rescue request information extrac-
tion site” and rescue activities (in Japanese).
In
Proc. 18th NLP.

Jun’ichi Kazama, Stijn De Saeger, Kentaro Torisawa,
Jun Goto, and Istvan Varga. 2012. An attempt to
apply a QA system to information in times of crisis
(in Japanese). In Proc. 18th NLP.

Trausti Kristjansson, Aron Culotta, Paul Viola, and An-
drew McCallum. 2004. Interactive information ex-
traction with constrained conditional random ﬁelds.
In Proc. AAAI.

David J.C. Mackay and Linda C. Bauman Petoy. 1995.
A hierarchical Dirichlet language model. Natural
Language Engineering, 1.

Prem Melville, Wojciech Gryc, and Richard D.
Lawrence. 2009. Sentiment analysis of blogs by
combining lexical knowledge with text classiﬁca-
tion. In Proc. KDD.

Marcelo Mendoza, Barbara Poblete,

and Carlos
Castillo. 2010. Twitter under crisis: can we trust
what we RT? In Proceedings of the First Workshop
on Social Media Analytics (SOMA), pages 71–79.

Rich Caruana, Paul G. Hodor, and John Rosenberg.
In

2000. High precision information extraction.
Proc. of the KDD-2000 Workshop on Text Mining.

Mai Miyabe, Asako Miura, and Eiji Aramaki. 2012a.
Use trend analysis of Twitter after the Great East
Japan Earthquake. In Proc. CSCW, pages 175–178.

Carlos Castillo, Marcelo Mendoza, and Barbara
Poblete. 2011. Information credibility on Twitter.
In Proc. WWW, pages 675–684.

William J Corvey, Sudha Verma, Sarah Vieweg, Martha
Palmer, and James H Martin. 2012. Foundations of
a multilayer annotation framework for Twitter com-
In Proc. LREC,
munications during crisis events.
pages 21–27.

Aron Culotta and Andrew McCallum. 2005. Reduc-
ing labeling effort for structured prediction tasks. In
Proc. AAAI.

Son Doan, Bao-Khanh Ho Vo, and Nigel Collier. 2012.
An analysis of Twitter messages in the 2011 Tohoku
earthquake. In Electronic Healthcare, pages 58–66.

Susan Dumais, John Platt, David Heckerman, and
Mehran Sahami. 1998.
Inductive learning algo-
rithms and representations for text categorization. In
Proc. CIKM, pages 148–155.

Google Japan. 2011. Requesting your help to register
shared survivor lists to Person Finder (in Japanese).
http://googlejapan.blogspot.com/
2011/03/blog-post_17.html.

Mai Miyabe, Ayana Umeshima, Akiyo Nadamoto, and
Eiji Aramaki.
2012b. Rumor cloud: Gathering
rumors by extracting correction information men-
tioned by humans (in Japanese). In Proc. 18th NLP.

Robert Munro. 2010. Crowdsourced translation for
emergency response in Haiti: the global collabora-
tion of local knowledge. In Proc. AMTA Workshop
on Collaborative Crowdsourcing for Translation.

Koji Murakami and Masato Hagiwara. 2012. A de-
tailed analysis of a safety information Tweet corpus
and observations about its annotation (in Japanese).
In Proc. 18th NLP.

Graham Neubig, Yuichiroh Matsubayashi, Masato
Hagiwara, and Koji Murakami. 2011. Safety in-
formation mining - what can NLP do in a disaster
In Proc. IJCNLP, pages 965–973, Chiang Mai,
-.
Thailand, November.

Yan Qu, Philip Fei Wu, and Xiaoqing Wang. 2009.
Online community response to major disaster: A
study of Tianya forum in the 2008 Sichuan earth-
quake. In Proc. HICSS, pages 1–11. IEEE.

Sunita Sarawagi. 2008. Information extraction. Foun-

dations and trends in databases, 1(3):261–377.

Aya Ishino, Shuhei Odawara, Hidetsugu Nanba, and
Toshiyuki Takezawa. 2012. Extracting transporta-
tion information and trafﬁc problems from tweets
during a disaster. In Proc. IMMM, pages 91–96.

Bernhard Sch¨olkopf, John C. Platt, John Shawe-Taylor,
Alex J. Smola, and Robert C. Williamson. 2001.
Estimating the support of a high-dimensional distri-
bution. Neural computation, 13(7):1443–1471.

Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many rel-
evant features. In ECML-98.

Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Comput. Surv.,
34(1).

35

Burr Settles. 2011. Closing the loop: Fast, interactive
semi-supervised annotation with queries on features
and instances. In Proc. EMNLP.

Kate Starbird and Jeannie Stamberger. 2010. Tweak
the tweet: Leveraging microblogging proliferation
with a prescriptive syntax to support citizen report-
ing. In Proc. ISCRAM.

Kate Starbird, Grace Muzny, and Leysia Palen. 2012.
Learning from the crowd: Collaborative ﬁltering
techniques for identifying on-the-ground Twitterers
during mass disruptions. In Proc. ISCRAM.

Sudha Verma, Sarah Vieweg, William J Corvey, Leysia
Palen, James H Martin, Martha Palmer, Aaron
Schram, and Kenneth M Anderson. 2011. Nat-
ural language processing to the rescue?: Extract-
ing’situational awareness’ tweets during mass emer-
gency. Proc. ICWSM.

Sarah Vieweg, Amanda L Hughes, Kate Starbird, and
Leysia Palen. 2010. Microblogging during two nat-
ural hazards events: what Twitter may contribute to
In Proc. CHI, pages 1079–
situational awareness.
1088.

Xiang Sean Zhou and Thomas S. Huang. 2003. Rele-
vance feedback in image retrieval: A comprehensive
review. Multimedia systems, 8(6).

