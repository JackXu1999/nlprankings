



















































MC^2: Multi-perspective Convolutional Cube for Conversational Machine Reading Comprehension


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6185–6190
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

6185

MC2: Multi-perspective Convolutional Cube
for Conversational Machine Reading Comprehension

Xuanyu Zhang
College of Information Science and Technology

Beijing Normal University, Beijing, 100875, China
xyz@mail.bnu.edu.cn

Abstract

Conversational machine reading comprehen-
sion (CMRC) extends traditional single-turn
machine reading comprehension (MRC) by
multi-turn interactions, which requires ma-
chines to consider the history of conversa-
tion. Most of models simply combine pre-
vious questions for conversation understand-
ing and only employ recurrent neural networks
(RNN) for reasoning. To comprehend con-
text profoundly and efficiently from different
perspectives, we propose a novel neural net-
work model, Multi-perspective Convolutional
Cube (MC2). We regard each conversation as
a cube. 1D and 2D convolutions are integrated
with RNN in our model. To avoid models pre-
viewing the next turn of conversation, we also
extend causal convolution partially to 2D. Ex-
periments on the Conversational Question An-
swering (CoQA) dataset show that our model
achieves state-of-the-art results.

1 Introduction

Conversation is one of the most important ap-
proaches for humans to acquire information. Dif-
ferent from traditional machine reading compre-
hension (MRC), conversational machine reading
comprehension (CMRC) requires machines to an-
swer multiple follow-up questions according to
a passage and dialogue history. However, these
questions usually have complicated linguistic phe-
nomena, such as co-reference, ellipsis and so on.
Only considering conversation context profoundly
can we answer the current question correctly.

Recently, many CMRC datasets, such as CoQA
(Reddy et al., 2019) and QuAC (Choi et al., 2018),
are proposed to enable models to understand pas-
sages and answer questions in dialogue. Here is
an example from the CoQA dataset in Figure 1.
We can observe that the second and third questions
omit key information. It is impossible for both hu-

Billy went to the farm to buy some beef for his brother's 
birthday. When he arrived there, he saw that all six of the 
cows were sad and had brown spots. The cows were all 
eating their breakfast in a big grassy meadow … …

History:

Who went to the farm?

Billy

Why?

To buy some beef

Q1

Q2
A1

A2

For what?

His brother's birthday

Q3

A3

Passage: 

Figure 1: An example in the CoQA dataset.

mans and machines to understand such questions
without dialogue history.

Most of existing methods consider conversation
history by prepending previous questions and an-
swers to the current question, such as BiDAF++
(Yatskar, 2019), DrQA+PGNet (Reddy et al.,
2019), SDNet (Zhu et al., 2018) and so on. How-
ever, the latent semantic information of dialogue
history is neglected. And the model may confuse
some unrelated questions and answers in a sen-
tence. Although FlowQA (Huang et al., 2019)
utilizes intermediate representations of previous
conversation, the flow mechanism can not synthe-
size the information of different words in different
turns of conversation simultaneously. Moreover,
previous models only use recurrent neural network
(RNN) as their main skeleton, which is not parallel
due to recurrent nature. And RNN can only grasp
information from two directions, either forward or
backward. But for conversation, humans usually
consider history from different perspectives and
answer questions comprehensively.

To address these issues, we propose a novel
model, i.e. Multi-perspective Convolutional Cube
(MC2). Every conversation is represented as a



6186

Interaction
Reasoning

Layer

Answer
Prediction

Layer

Passage Question(t-th)

Embedding Embedding

RNNContextual
Encoding

Layer

Perspective Ⅰ

Perspective Ⅱ

Perspective Ⅰ

Perspective Ⅲ-1

Perspective Ⅰ

Perspective Ⅲ-1

Attention

Attention

Attention

start end

①

②

③

④

⑤

⑥

RNN

Perspective Ⅰ

Attention

Figure 2: MC2 structure overview.

cube, three dimensions of which are question an-
swering (QA) turns, passage words and hidden
states of words, separately. For one thing, convo-
lutional neural networks (CNN) can extract local
information effectively across dimensions in par-
allel. Introducing CNN to RNN allows the model
to take into account local and global features effi-
ciently. For another thing, machines can compre-
hend conversation history more deeply from dif-
ferent perspectives by fusing 1D and 2D convo-
lutions in our model. In addition, to avoid infor-
mation leakage of the next turn of dialogue, we
extend causal convolution to 2D. Experiments on
the Conversational Question Answering (CoQA)
dataset show that our model improves the result of
the published state-of-the-art model by 3.2%.

2 Approaches

In this section, we propose our novel model, MC2,
for the task of conversational machine reading
comprehension, which can be formulated as fol-
lows. For one conversation, given a passage with
n tokens P = {pi}ni=1 and multiple questions with
c turns Q = {Qt}ct=1, machines need to give the
corresponding answers A = {At}ct=1. The t-th
question with m tokens is Qt = {qtj}mj=1. The
neural network is required to model the probabil-
ity distribution p(At|Q≤t, P ) for the t-th QA turn
in the conversation. As shown in Figure 2, there

are three main layers in our model, i.e., contextual
encoding layer, interaction reasoning layer and an-
swer prediction layer. Our proposed cube is used
in the middle layer. For convenience, we will il-
lustrate our model from bottom to top.

2.1 Contextual Encoding Layer

The purpose of this layer is to extract useful infor-
mation for upper layers. We embed questions and
passages into a sequence of vectors with the latest
contextualized model, BERT (Devlin et al., 2019),
separately. Instead of fine-tuning BERT with ex-
tra scoring layers, we fix the weights of BERT like
SDNet (Zhu et al., 2018) and aggregate L hidden
layers generated by BERT as contextualized em-
bedding for all BPE (Sennrich et al., 2016) tokens.

To introduce other linguistic features token by
words and facilitate answer selection, we choose
the first token of a word in BPE to represent the
word. Generally, the first token is often the root
of the word and can represent main meaning of
the whole word. And it also contains information
of rest tokens in the word with the bidirectional
structure of BERT. Besides, we split the long sen-
tence by shorter windows and combine them again
when the sentence exceeds the maximum length of
pre-trained BERT.

In detail, suppose hli ∈ Rd is the l-th hidden
layer of the first BPE token in the i-th word. We
collapse all hidden layers generated by BERT into
a single vector for each word following ELMo
(Peters et al., 2018). The contextualized em-
bedding for the i-th word is ei = γ

∑L
l=0 αlh

l
i,

where γ is designed to scale the vector and αl
is softmax-normalized weight for the l-th layer.
These weights are all trainable. To be consis-
tent with the number of turns of question EQ =
{eQt,j}mj=1ct=1 ∈ Rc×m×d, the passage ePi is ex-
panded c times to EP = {ePt,i}ni=1ct=1 ∈ Rc×n×d.

To incorporate other linguistic information,
three additional features are utilized for each word
pi in the passage following Chen et al. (2017), i.e.
part-of-speech (POS) tags, named entity recog-
nition (NER) tags and aligned question embed-
dings. The embeddings of POS eposi and NER
eneri are learned for different tags, separately. And
aligned question embeddings can be obtained in
Eq. 1. Following Huang et al. (2018), we use
f(x, y) = ReLU(Ux)TDReLU(Uy) as the at-
tention score function between x, y, where D is
a diagonal matrix and D, U are trainable.



6187

Passage W
o

rd
s

Q
A

 Tu
rn

s

Perspective Ⅰ Perspective Ⅱ Perspective Ⅲ-1 Perspective Ⅲ-2

Q
A

 Tu
rn

s

Q
A

 Tu
rn

s

Figure 3: Different perspectives of the cube.

sij = f(e
P
t,i, e

Q
t,j)

aij = exp(s
i
j)/

∑m
k=1

exp(sik)

eattnt,i =
∑m

j=1
aije

Q
t,j

(1)

We then concatenate these features and embed-
dings to rPt,i for passages and employ bidirectional
RNN to refine the question to rQt,j .

rPt,i = [e
P
t,i; e

pos
t,i ; e

ner
t,i ; e

attn
t,i ]

rQt,j = BiRNN(r
Q
t,j−1, e

Q
t,j)

(2)

2.2 Interaction Reasoning Layer
This layer plays an important role in our model,
which aims to incorporate question information
into passage representation further and reason
from different perspectives by our proposed con-
volutional cube. The cube represents the hidden
states of passages in a conversation. We will de-
scribe these perspectives in Figure 3 in the order of
¬ to ± in Figure 2. To consider global context of
each turn besides local information across differ-
ent dimensions, Perspective I equipped with RNN
is inserted before other CNN perspectives.

We first observe the cube from Perspective I
and feed the hidden states of the cube rPt,i to bi-
directional RNN for each turn of conversation
cPt,i = BiRNN(c

P
t,i−1, r

P
t,i). Then the cube is

viewed from Perspective II along QA turns for dif-
ferent words, separately. Since the (t+1)-th turn of
information can not be used when processing the
t-th turn, we employ 1D causal convolution (Oord
et al., 2016) to the cube by moving the padding
at the end to the beginning. And the representa-
tion of the cube can be updated from cPt,i into c̄

P
t,i.

After viewed from these two perspectives (¬ ­ in
Figure 2), the hidden states of every word in pas-
sages grasp information from two dimensions of
the cube.

Next, we observe the cube from Perspective I
again to fuse previous hidden states and generate

global context ĉPt,i for each turn of conversation.
To reason from more dimensions simultaneously,
2D CNN is utilized to generate hidden states of
the cube hPt,i along the dimension of both QA turns
and passage words from Perspective III-1. Differ-
ent from other models, three kinds of information
can be considered comprehensively by this pro-
cess: the same word in different QA turns, differ-
ent words in the same QA turn and different words
in different QA turns. Similar to 1D CNN above,
the 2D CNN also requires to be unidirectional on
the dimension of QA turns to avoid information
leakage. But it is more reasonable to capture bidi-
rectional information on the dimension of passage
words. We thus extend traditional causal convolu-
tion partially to 2D CNN by moving padding only
on one dimension. These two perspectives (® ¯
in Figure 2) strengthen the representation of our
cube further.

For questions in this layer, we pass them as
the input to another RNN for reasoning hQt,j =

BiRNN(hQt,j−1, r
Q
t,j). Then we employ the atten-

tion score function mentioned above to integrate
new information of questions to passages.

sij = f([e
P
t,i; c

P
t,i; ĉ

P
t,i], [e

Q
t,j ; r

Q
t,j ;h

Q
t,j ])

aij = exp(s
i
j)/

∑m
k=1

exp(sik)

hattnt,i =
∑m

j=1
aijh

Q
t,j

(3)

As shown in Figure 2, we repeat the process of
® ¯ in ° ± for deeper understanding and rea-
soning. RNN takes [hPt,i;h

attn
t,i ] and generates h̄

P
t,i

from Perspective I. Then 2D CNN generates h̃Pt,i
from Perspective III-1. We use self-attention to
enhance the current passage representation as fol-
lows:

sij = f([c
P
t,i; ĉ

P
t,i; h̄

P
t,i], [c

P
t,j ; ĉ

P
t,j ; h̄

P
t,j ])

aij = exp(s
i
j)/

∑n
k=1

exp(sik)

hselft,i =
∑n

j=1
aij h̄

P
t,j

(4)



6188

Model
In-domain Out-of-domain

Overall
Child. Liter Mid-High. News Wiki Reddit Science

PGNet 49.0 43.3 47.5 47.5 45.1 38.6 38.1 44.1
DrQA 46.7 53.9 54.1 57.8 59.4 45.0 51.0 52.6
DrQA+PGNet 64.2 63.7 67.1 68.3 71.4 57.8 63.1 65.1
Augmt. DrQA 66.0 63.3 66.2 71.0 71.3 57.7 63.0 65.4
BiDAF++ 66.5 65.7 70.2 71.6 72.6 60.8 67.1 67.8
FlowQA 73.7 71.6 76.8 79.0 80.2 67.8 76.1 75.0
SDNet 75.4 73.9 77.1 80.3 83.1 69.8 76.8 76.6
MC2 78.4 76.7 81.1 83.0 84.8 73.8 80.6 79.8
Human 90.2 88.4 89.8 88.6 89.9 86.7 88.1 88.8

Table 1: Model and human performance (% in F1 score) on the CoQA test set.

0

10

20

30

40

50

60

70

80

90

DrQA BiDAF++ FlowQA SDNet MC²
(ours)

Out 47.9 63.8 71.8 73.1 77.1

△F1 6.6 5.6 4.5 4.9 3.8

In
-d

o
m

ai
n

 F
1

 s
co

re 54.5

69.4

76.3 78.0
80.9

Figure 4: F1 score of models on in-domain and
out-of-domain parts of the CoQA test set.

At last, we view the cube from Perspective I
again to synthesize the global information ĥPt,i =

BiRNN(ĥPt,i−1, [h̄
P
t,i; h̃

P
t,i;h

self
t,i ]).

2.3 Answer Prediction Layer

This layer is the top one of our model. We use
similar methods (Chen et al., 2017; Huang et al.,
2019; Zhu et al., 2018) to predict the position of
the answer in the passage. We project the ques-
tion representation into one vector for each turn
of dialogue ĥQt =

∑m
j=1 at,jh

Q
t,j , where at,j =

exp(WhQt,j)/
∑m

k=1 exp(Wh
Q
t,k) and W is train-

able. Then two different bilinear attention func-
tions are used to estimate the probability of the
start and end according to ĥPt,i and ĥ

Q
t . We choose

the position of the maximum product of these two
probabilities as the best span. For other answer

types, such as yes, no and unknown, we condense
the passage representation ĥPt,i to ĥ

P
t like questions

and classify the answer according to [ĥPt ; ĥ
Q
t ].

To train the cube, we minimize the sum of the
negative log probabilities of the ground truth start
position, end position and answer type by the pre-
dicted distributions.

3 Experiments

3.1 Data and Metric

We conduct our experiments on the CoQA (Reddy
et al., 2019), a large-scale CMRC dataset anno-
tated by human. It consists of 127k questions with
answers collected from 8k conversations over text
passages. As shown in Table 1, it covers seven
diverse domains (five of them are in-domain and
two are out-of-domain). The out-of-domain pas-
sages only appear in the test set. Aligned with the
official evaluation, F1 score is used as the metric,
which measures the overlap between the predic-
tion and the ground truth at word level.

3.2 Implementation Details

We use pre-trained BERTLARGE model for con-
textualized embeddings, the dimension of which
is 1024. And spaCy is applied for tokenization,
part-of-speech and named entity recognition. The
last turn of the answer is added to the next turn
as guidance in the dataset. Each batch contains
one cube for one conversation. We employ LSTM
as the structure of RNN, the hidden size of which
is 250 throughout our model. The kernel size is
set to 5 and 3 for 1D and 2D CNN, respectively.
And the dropout rate is set to 0.4. The Adamax
(Kingma and Ba, 2015) is used as our optimizer
with 0.1 learning rate.



6189

51

53

55

57

59

61

63

65

67

69

71

73

75

77

79

81

1 3 5 7 9 11 13 15 17 19 21 23 25 27 29

F1
 S

co
re

Epoch

MC² (ours)

SDNet

SDNet ⃰

FlowQA

BiDAF++

DrQA+PGNet

Figure 5: F1 score on the CoQA dev set under
different training epochs. 1

Configuration F1 ∆ F1
MC2 81.266 -
w/o ­ ¯ ± 77.363 -3.903
w/o ­ 80.718 -0.548
w/o ¯ 80.867 -0.399
w/o ± 80.849 -0.417
replace ­ with ¯ 80.932 -0.334
replace ¯ with ­ 80.473 -0.793
replace ± with III-2 81.087 -0.179
exchange ­ with ¯ 81.102 -0.164

Table 2: Ablation study on the CoQA dev set.
(­ ¯ ± come from Fig. 2. III-2 comes from Fig. 3.)

3.3 Result

We compare our MC2 with other baseline models 2

in Table 1: PGNet (See et al., 2017), DrQA (Chen
et al., 2017), DrQA+PGNet (Reddy et al., 2019),
Augmented DrQA (Reddy et al., 2019), BiDAF++
(Yatskar, 2019), FlowQA (Huang et al., 2019) and
SDNet (Zhu et al., 2018). Our model achieves sig-
nificant improvement over these published mod-
els. Comparing with the previous state-of-the-art
model, SDNet, our model outperforms it by 3.2%
on F1 score. And SDNet also takes pre-trained
BERT as embedding without fine-tuning. Espe-
cially, our single model surpasses the ensemble
model of both FlowQA and SDNet.

Figure 4 shows the gap between in-domain and
out-of-domain on the test set. Although all mod-

1SDNet comes from experiments of the original author.
SDNet∗ refers to the proportion of Fig. 2 in the original paper.

2We only consider published models on the CoQA. Al-
though some models perform better on the leaderboard re-
cently, they usually focus on fine-tuning BERT model.

els perform worse on out-of-domain datasets com-
pared to in-domain datasets, our model only drops
3.8% on F1 score. It is the smallest drop between
in-domain and out-of-domain among all models,
which proves that our model has very good gener-
alization ability. Besides, our model achieves the
best performance on both in-domain and out-of-
domain datasets.

The learning curve is shown in Figure 5. It re-
flects the performance of models under different
training epochs on the development set. We can
observe that our model completely surpasses SD-
Net at every epoch. And it outperforms all base-
line models only after 5 epochs and achieves the
best performance after 18 epochs. Especially, our
model achieves 72.472% on F1 score only after
the first epoch, which is about 10% to 20% higher
than SDNet. Thus with fewer training epochs, our
model still can perform well.

3.4 Ablation Studies

To study how each perspective of our proposed
cube contributes to the performance, we conduct
an ablation analysis on the development set in Ta-
ble 2. The results show that removing all CNN
perspectives of the cube, i.e. ­ ¯ ± in Figure 2,
will cause a substantial performance drop (3.90%
on F1 score). And removing any of them also re-
sults in marginal decrease in performance. It is
clear that the improvement of reading from differ-
ent perspectives simultaneously is larger than that
of the sum of reading from single perspective sep-
arately. Besides, replacing 2D CNN (Perspective
III-1) with 1D CNN (Perspective II) also causes a
significant decline of performance (0.79% on F1
score). We also explore 3D CNN (Perspective III-
2), but it brings no improvement as expected.

4 Conclusion

In this paper, we introduce Multi-perspective Con-
volutional Cube (MC2), a novel model for con-
versational machine reading comprehension. The
cube is viewed from different perspectives to fully
understand the history of conversation. By inte-
grating CNN with RNN, fusing 1D and 2D con-
volutions, extending causal convolution to 2D, our
model achieves the best results among published
models on the CoQA dataset without fine-tuning
BERT. We will study further the capability of our
approaches on other datasets and tasks in the fu-
ture work.



6190

References
Danqi Chen, Adam Fisch, Jason Weston, and Antoine

Bordes. 2017. Reading Wikipedia to answer open-
domain questions. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers).

Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-
tau Yih, Yejin Choi, Percy Liang, and Luke Zettle-
moyer. 2018. QuAC: Question answering in con-
text. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers).

Hsin-Yuan Huang, Eunsol Choi, and Wen-tau Yih.
2019. FlowQA: Grasping flow in history for conver-
sational machine comprehension. In Proceedings of
the 7th International Conference on Learning Rep-
resentations.

Hsin-Yuan Huang, Chenguang Zhu, Yelong Shen, and
Weizhu Chen. 2018. Fusionnet: Fusing via fully-
aware attention with application to machine compre-
hension. In International Conference on Learning
Representations.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceed-
ings of the 3rd International Conference on Learn-
ing Representations.

Aaron van den Oord, Sander Dieleman, Heiga Zen,
Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray
Kavukcuoglu. 2016. Wavenet: A generative model
for raw audio. SSW, 125.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers).

Siva Reddy, Danqi Chen, and Christopher D. Manning.
2019. CoQA: A conversational question answering
challenge. Transactions of the Association for Com-
putational Linguistics, 7.

Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers).

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with

subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers).

Mark Yatskar. 2019. A qualitative comparison of
CoQA, SQuAD 2.0 and QuAC. In Proceedings of
the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long
and Short Papers).

Chenguang Zhu, Michael Zeng, and Xuedong Huang.
2018. SDNet: Contextualized attention-based
deep network for conversational question answering.
arXiv preprint arXiv:1812.03593.


