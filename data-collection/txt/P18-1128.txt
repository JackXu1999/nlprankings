



















































The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1383–1392
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

1383

The Hitchhiker’s Guide to Testing Statistical Significance in Natural
Language Processing

Rotem Dror Gili Baumer
Faculty of Industrial Engineering and Management, Technion, IIT

{rtmdrr@campus|sgbaumer@campus|segevs@campus|roiri}.technion.ac.il

Segev Shlomov Roi Reichart

Abstract

Statistical significance testing is a standard sta-
tistical tool designed to ensure that experimen-
tal results are not coincidental. In this opin-
ion/theoretical paper we discuss the role of statis-
tical significance testing in Natural Language Pro-
cessing (NLP) research. We establish the funda-
mental concepts of significance testing and discuss
the specific aspects of NLP tasks, experimental se-
tups and evaluation measures that affect the choice
of significance tests in NLP research. Based on
this discussion, we propose a simple practical pro-
tocol for statistical significance test selection in
NLP setups and accompany this protocol with a
brief survey of the most relevant tests. We then
survey recent empirical papers published in ACL
and TACL during 2017 and show that while our
community assigns great value to experimental re-
sults, statistical significance testing is often ig-
nored or misused. We conclude with a brief dis-
cussion of open issues that should be properly ad-
dressed so that this important tool can be applied
in NLP research in a statistically sound manner1.

1 Introduction

The field of Natural Language Processing (NLP)
has recently made great progress due to the data
revolution that has made abundant amounts of tex-
tual data from a variety of languages and linguis-
tic domains (newspapers, scientific journals, so-
cial media etc.) available. This, together with the
emergence of a new generation of computing re-
sources and the related development of Deep Neu-
ral Network models, have resulted in dramatic im-
provements in the capabilities of NLP algorithms.

1The code for all statistical tests detailed in this pa-
per is found on: https://github.com/rtmdrr/
testSignificanceNLP.git

The extended reach of NLP algorithms has also
resulted in NLP papers giving much more empha-
sis to the experiment and result sections by show-
ing comparisons between multiple algorithms on
various datasets from different languages and do-
mains. This emphasis on empirical results high-
lights the role of statistical significance testing in
NLP research: if we rely on empirical evalua-
tion to validate our hypotheses and reveal the cor-
rect language processing mechanisms, we better
be sure that our results are not coincidental.

This paper aims to discuss the various aspects of
proper statistical significance testing in NLP and
to provide a simple and sound guide to the way this
important tool should be used. We also discuss the
particular challenges of statistical significance in
the context of language processing tasks.

To facilitate a clear and coherent presentation,
our (somewhat simplified) model of an NLP paper
is one that presents a new algorithm and makes the
hypothesis that this algorithm is better than a pre-
vious strong algorithm, which serves as the base-
line. This hypothesis is verified in experiments
where the two algorithms are applied to the same
datasets (test sets), reasoning that if one algorithm
is consistently better than the other, hopefully with
a sufficiently large margin, then it should also be
better on future, currently unknown, datasets. Yet,
the experimental differences might be coinciden-
tal. Here comes statistical significance testing into
the picture: we have to make sure that the prob-
ability of falsely concluding that one algorithm is
better than the other is very small.

We note that in this paper we do not deal with
the problem of drawing valid conclusions from
multiple comparisons between algorithms across a
large number of datasets , a.k.a. replicability anal-
ysis (see (Dror et al., 2017)). Instead, our focus
is on a single comparison: how can we make sure
that the difference between the two algorithms, as

https://github.com/rtmdrr/testSignificanceNLP.git
https://github.com/rtmdrr/testSignificanceNLP.git


1384

observed in an individual comparison, is not coin-
cidental. Statistical significance testing of each in-
dividual comparison is the basic building block of
replicability analysis – its accurate performance is
a pre-condition for any multiple dataset analysis.

Statistical significance testing (§ 2) is a well re-
searched problem in the statistical literature. How-
ever, the unique structured nature of natural lan-
guage data is reflected in specialized evaluation
measures such as BLEU (machine translation, (Pa-
pineni et al., 2002)), ROUGE (extractive summa-
rization, (Lin, 2004)), UAS and LAS (dependency
parsing, (Kübler et al., 2009)). The distribution of
these measures is of great importance to statistical
significance testing. Moreover, certain properties
of NLP datasets and the community’s evaluation
standards also affect the way significance testing
should be performed. An NLP-specific discussion
of significance testing is hence in need.

In § 3 we discuss the considerations to be made
in order to select the proper statistical significance
test in NLP setups. We propose a simple deci-
sion tree algorithm for this purpose, and survey the
prominent significance tests – parametric and non-
parametric – for NLP tasks and data.

In § 4 we survey the current evaluation and sig-
nificance testing practices of the community. We
provide statistics collected from the long papers
of the latest ACL proceedings (Barzilay and Kan,
2017) as well as from the papers published in the
TACL journal during 2017. Our analysis reveals
that there is still a room for improvement in the
way statistical significance is used in papers pub-
lished in our top-tier publication venues. Particu-
larly, a large portion of the surveyed papers do not
test the significance of their results, or use incor-
rect tests for this purpose.

Finally, in § 5 we discuss open issues. A par-
ticularly challenging problem is that while most
significance tests assume the test set consists of
independent observations, most NLP datasets con-
sist of dependent data points. For example, many
NLP standard evaluation sets consist of sentences
coming from the same source (e.g. newspaper) or
document (e.g. newspaper article) or written by
the same author. Unfortunately, the nature of these
dependencies is hard to characterize, let alone to
quantify. Another important problem is how to test
significance when cross-validation, a popular eval-
uation methodology in NLP papers, is performed.

Besides its practical value, we hope this paper

will encourage further research into the role of
statistical significance testing in NLP and on the
questions that still remain open.

2 Preliminaries

In this section we provide the required preliminar-
ies for our discussion. We start with a formal def-
inition of statistical significance testing and pro-
ceed with an overview of the prominent evaluation
measures in NLP.

2.1 Statistical Significance Testing

In this paper we focus on the setup where the per-
formance of two algorithms,A andB, on a dataset
X , is compared using an evaluation measure M.
Let us denote M(ALG,X) as the value of the
evaluation measure M when algorithm ALG is
applied to the dataset X . Without loss of gener-
ality, we assume that higher values of the measure
are better. We define the difference in performance
between the two algorithms according to the mea-
sureM on the dataset X as:

δ(X) =M(A,X)−M(B,X). (1)

In this paper we will refer to δ(X) as our test
statistic. Using this notation we formulate the fol-
lowing statistical hypothesis testing problem:2

H0 :δ(X) ≤ 0
H1 :δ(X) > 0.

In order to decide whether or not to reject the
null hypothesis, that is reaching the conclusion
that δ(X) is indeed greater than 0, we usually
compute a p−value for the test. The p−value is
defined as the probability, under the null hypoth-
esis H0, of obtaining a result equal to or more
extreme than what was actually observed. For
the one-sided hypothesis testing defined here, the
p−value is defined as:

Pr(δ(X) ≥ δobserved|H0).

Where δobserved is the performance difference be-
tween the algorithms (according toM) when ap-
plied to X . The smaller the p-value, the higher
the significance, or, in other words, the stronger

2For simplicity we consider a one-sided hypothesis, it can
be easily re-formulated as a double-sided hypothesis.



1385

the indication provided by the data that the null-
hypothesis, H0, does not hold. In order to de-
cide whetherH0 should be rejected, the researcher
should pre-define an arbitrary, fixed threshold
value α, a.k.a the significance level. Only if
p−value < α then the null hypothesis is rejected.

In significance (or hypothesis) testing we con-
sider two error types. Type I error refers to the
case where the null hypothesis is rejected when it
is actually true. Type II error refers to the case
where the null hypothesis is not rejected although
it should be. A common approach in hypothe-
sis testing is to choose a test that guarantees that
the probability of making a type I error is up-
per bounded by the test significance level α, men-
tioned above, while achieving the highest possible
power: i.e. the lowest possible probability of mak-
ing a type II error.

2.2 Evaluation Measures in NLP

Evaluation
Measure

ACL 17 TACL 17

F-scores 78 (39.8%) 9 (25.71%)
Accuracy 67 (34.18%) 13 (37.14%)
Precision/
Recall

44 (22.45%) 6 (17.14%)

BLEU 26 (13.27%) 4 (11.43%)
ROUGE 12 (6.12%) 0 (0%)
Pearson/ Spear-
man correla-
tions

4 (2.04%) 6 (17.14%)

Perplexity 7 (3.57%) 2 (5.71%)
METEOR 6 (3.06%) 1 (2.86%)
UAS+LAS 1 (0.51%) 3 (8.57%)

Table 1: The most common evaluation measures
in (long) ACL and TACL 2017 papers, ordered by
ACL frequency. For each measure we present the
total number of papers where it is used and the
fraction of papers in the corresponding venue.

In order to draw valid conclusions from the ex-
periments formulated in § 2.1 it is crucial to ap-
ply the correct statistical significance test. In § 3
we explain that the choice of the significance test
is based, among other considerations, on the dis-
tribution of the test statistics, δ(X). From equa-
tion 1 it is clear that δ(X) depends on the evalu-
ation measure M. We hence turn to discuss the
evaluation measures employed in NLP.

In § 4 we analyze the (long) ACL and TACL

2017 papers, and observe that the most commonly
used evaluation measures are the 12 measures that
appear in Table 1. Notice that seven of these
measures: Accuracy, Precision, Recall, F-score,
Pearson and Spearman correlations and Perplexity,
are not specific to NLP. The other five measures:
BLEU (Papineni et al., 2002), ROUGE (Lin,
2004), METEOR (Banerjee and Lavie, 2005),
UAS and LAS (Kübler et al., 2009), are unique
measures that were developed for NLP applica-
tions. BLEU and METEOR are standard evalu-
ation measures for machine translation, ROUGE
for extractive summarization, and UAS and LAS
for dependency parsing. While UAS and LAS are
in fact accuracy measures, BLEU, ROUGE and
METEOR are designed for tasks where there are
several possible outputs - a characteristic property
of several NLP tasks. In machine translation, for
example, a sentence in one language can be trans-
lated in multiple ways to another language. Conse-
quently, BLEU takes an n-gram based approach on
the surface forms, while METEOR considers only
unigram matches but uses stemming and controls
for synonyms.

All 12 measures return a real number, either in
[0, 1] or in R. Notice though that accuracy may
reflect an average over a set of categorical scores
(observations), e.g., in document-level binary sen-
timent analysis where every document is tagged as
either positive or negative. In other cases, the in-
dividual observations are also continuous. For ex-
ample, when comparing two dependency parsers,
we may want to understand how likely it is, given
our results, that one parser will do better than the
other on a new sentence. In such a case we will
consider the sentence-level UAS or LAS differ-
ences between the two parsers on all the sentences
in the test set. Such sentence level UAS or LAS
scores - the individual observations to be consid-
ered in the significance test - are real-valued.

With the basic concepts clarified, we are ready
to discuss the considerations to be made when
choosing a statistical significance test.

3 Statistical Significance in NLP

The goal of this section is to detail the considera-
tions involved in the selection of a statistical sig-
nificance test for an NLP application. Based on
these considerations we provide a practical recipe
that can be applied in order to make a good choice.
In order to make this paper a practical guide for



1386

the community, we also provide a short descrip-
tion of the significance tests that are most relevant
for NLP setups.

3.1 Parametric vs. Non-parametric Tests

As noted above, a major consideration in the se-
lection of a statistical significance test is the dis-
tribution of the test statistic, δ(X), under the null
hypothesis. If the distribution is known, then the
suitable test will come from the family of para-
metric tests, that uses this distribution in order to
achieve powerful results (i.e., low probability of
making a type II error, see § 2). If the distribu-
tion is unknown then any assumption made by a
test may lead to erroneous conclusions and hence
we should rely on non-parametric tests that do not
make any such assumption. While non-parametric
tests may be less powerful than their paramet-
ric counterparts, they do not make unjustified as-
sumptions and are hence statistically sound even
when the test statistic distribution is unknown.

But how can one know the test statistic dis-
tribution? One possibility is to apply tests de-
signed to evaluate the distribution of a sample of
observations. For example, the Shapiro-Wilk test
(Shapiro and Wilk, 1965) tests the null hypothesis
that a sample comes from a normally distributed
population, the Kolmogorov-Smirnov test quanti-
fies the distance between the empirical distribu-
tion function of the sample and the cumulative
distribution function of the reference distribution,
and the Anderson-Darling test (Anderson and Dar-
ling, 1954) tests whether a given sample of data is
drawn from a given probability distribution. As
discussed below, there seems to be other heuris-
tics that are used in practice but are not often men-
tioned in research papers.

In what follows we discuss the prominent para-
metric and non-parametric tests for NLP setups.
Based on this discussion we end this section with
a simple decision tree that aims to properly guide
the significance test choice process.

3.2 Prominent Significance Tests

3.2.1 Parametric Tests
Parametric significance tests assume that the test
statistic is distributed according to a known dis-
tribution with defined parameters, typically the
normal distribution. While this assumption may
be hard to verify (see discussion above), when it
holds, these parametric tests have stronger statis-

tical power compared to non-parametric tests that
do not make this assumption (Fisher, 1937).

Here we discuss the prominent parametric test
for NLP setups - the paired student’s t-test.

Paired Student’s t-test This test assesses
whether the population means of two sets of mea-
surements differ from each other, and is based on
the assumption that both samples come from a nor-
mal distribution (Fisher, 1937).

In practice, t-test is often applied with evalua-
tion measures such as accuracy, UAS and LAS,
that compute the mean number of correct predic-
tions per input example. When comparing two de-
pendency parsers, for example, we can apply the
test to check if the averaged difference of their
UAS scores is significantly larger than zero, which
can serve as an indication that one parser is better
than the other.

Although we have not seen this discussed in
NLP papers, we believe that the decision to use
the t-test with these measures is based on the Cen-
tral Limit Theorem (CLT). CLT establishes that, in
most situations, when independent random vari-
ables are added, their properly normalized sum
tends toward a normal distribution even if the orig-
inal variables themselves are not normally dis-
tributed. That is, accuracy measures in structured
tasks tend to be normally distributed when the
number individual predictions (e.g. number of
words in a sentence when considering sentence-
level UAS) is large enough.

One case where it is theoretically justified to
employ the t-test is described in (Sethuraman,
1963). The authors prove that for large enough
data, the sampling distribution of a certain func-
tion of the Pearson’s correlation coefficient fol-
lows the Student’s t-distribution with n − 2 de-
grees of freedom. With the recent surge in word
similarity research with word embedding models,
this result is of importance to our community.

For other evaluation measures, such as F-score,
BLEU, METEOR and ROUGE that do not com-
pute means, the common practice is to assume
that they are not normally distributed (Yeh, 2000;
Berg-Kirkpatrick et al., 2012). We believe this
issue requires a further investigation and suggest
that it may be best to rely on the normality tests
discussed in § 3.1 when deciding whether or not to
employ the t-test.



1387

3.2.2 Non-parametric Tests
When the test statistic distribution is unknown,
non-parametric significance testing should be
used. The non-parametric tests that are com-
monly used in NLP setups can be divided into two
families that differ with respect to their statistical
power and computational complexity.

The first family consists of tests that do not con-
sider the actual values of the evaluation measures.
The second family do consider the values of the
measures: it tests repeatedly sample from the test
data, and estimates the p-value based on the test
statistic values in the samples. We refer to the first
family as the family of sampling-free tests and to
the second as the family of sampling-based tests.

The two families of tests reflect different pref-
erences with respect to the balance between
statistical power and computational efficiency.
Sampling-free tests do not consider the evaluation
measure values, only higher level statistics of the
results such as the number of cases in which each
of the algorithms performs better than the other.
Consequently, their statistical power is lower than
that of sampling-based tests that do consider the
evaluation measure values. Sampling-based tests,
however, compensate for the lack of distributional
assumptions over the data with re-sampling – a
computationally intensive procedure. Sampling-
based methods are hence not the optimal candi-
dates for very large datasets.

We consider here four commonly used
sampling-free tests: the sign test and two of its
variants, and the wilcoxon signed-rank test.

Sign test This test tests whether matched pair
samples are drawn from distributions with equal
medians. The test statistic is the number of exam-
ples for which algorithm A is better than algorithm
B, and the null hypothesis states that given a new
pair of measurements (e.g. evaluations (ai, bi) of
the two algorithms on a new test example), then ai
and bi are equally likely to be larger than the other
(Gibbons and Chakraborti, 2011).

The sign test has limited practical implications
since it only checks if algorithm A is better than
B and ignores the extent of the difference. Yet,
it has been used in a variety of NLP papers (e.g.
(Collins et al., 2005; Chan et al., 2007; Rush et al.,
2012)). The assumptions of this test is that the
data samples are i.i.d, the differences come from
a continuous distribution (not necessarily normal)
and that the values are ordered.

The next test is a special case of the sign test for
binary classification (or a two-tailed sign test).

McNemar’s test (McNemar, 1947) This test is
designed for paired nominal observations (binary
labels). The test is applied to a 2× 2 contingency
table, which tabulates the outcomes of two algo-
rithms on a sample of n examples. The null hy-
pothesis for this test states that the marginal prob-
ability for each outcome (label one or label two)
is the same for both algorithms. That is, when ap-
plying both algorithms on the same data we would
expect them to be correct/incorrect on the same
proportion of items. Under the null hypothesis,
with a sufficiently large number of disagreements
between the algorithms, the test statistic has a dis-
tribution of χ2 with one degree of freedom. This
test is appropriate for binary classification tasks,
and has been indeed used in such NLP works
(e.g. sentiment classificaiton, (Blitzer et al., 2006;
Ziser and Reichart, 2017)). The Cochran’s Q test
(Cochran, 1950) generalizes the McNemar’s test
for multi-class classification setups.

The sign test and its variants consider only pair-
wise ranks: which algorithm performs better on
each test example. In NLP setups, however, we
also have access to the evaluation measure val-
ues, and this allows us to rank the differences be-
tween the algorithms. The Wilcoxon signed-rank
test makes use of such a rank and hence, while it
does not consider the evaluation measure values, it
is more powerful than the sign test and its variants.

Wilcoxon signed-rank test (Wilcoxon, 1945)
Like the sign test variants, this test is used when
comparing two matched samples (e.g. UAS values
of two dependency parsers on a set of sentences).
Its null hypothesis is that the differences follow a
symmetric distribution around zero. First, the ab-
solute values of the differences are ranked. Then,
each rank gets a sign according to the sign of the
difference. The Wilcoxon test statistic sums these
signed ranks. The test is actually applicable for
most NLP setups and it has been used widely (e.g.
(Søgaard et al., 2014; Søgaard, 2013; Yang and
Mitchell, 2017)) due to its improved power com-
pared to the sign test variants.

As noted above, sampling-free tests trade sta-
tistical power for efficiency. Sampling-based
methods take the opposite approach. This
family includes two main methods: permuta-
tion/randomization tests (Noreen, 1989) and the



1388

paired bootstrap (Efron and Tibshirani, 1994).

Pitman’s permutation test This test estimates
the test statistic distribution under the null hypoth-
esis by calculating the values of this statistic un-
der all possible labellings (permutations) of the
test set. The (two-sided) p-value of the test is
calculated as the proportion of these permutations
where the absolute difference was greater than or
equal to the absolute value of the difference in the
output of the algorithm.

Obviously, permutation tests are computation-
ally intensive due to the exponentially large num-
ber of possible permutations. In practice, ap-
proximate randomization tests are used where a
pre-defined limited number of permutations are
drawn from the space of all possible permuta-
tions, without replacements (see, e.g. (Riezler and
Maxwell, 2005) in the context of machine trans-
lation). The bootstrap test (Efron and Tibshirani,
1994) is based on a closely related idea.

Paired bootstrap test This test is very similar
to approximate randomization of the permutation
test, with the difference that the sampling is done
with replacements (i.e., an example from the orig-
inal test data can appear more than once in a sam-
ple). The idea of bootstrap is to use the samples as
surrogate populations, for the purpose of approx-
imating the sampling distribution of the statistic.
The p-value is calculated in a similar manner to
the permutation test.

Bootstrap was used with a variety of NLP tasks,
including machine translation, text summarization
and semantic parsing (e.g. (Koehn, 2004; Li et al.,
2017; Wu et al., 2017; Ouchi et al., 2017)). The
test is less effective for small test sets, as it as-
sumes that the test set distribution does not deviate
too much from the population distribution.

Clearly, Sampling-based methods are computa-
tionally intensive and can be intractable for large
datasets, even with modern computing power. In
such cases, sampling-free methods form an avail-
able alternative.

3.3 Significance Test Selection

With the discussion of significance test families
- parametric vs. non-parametric (§ 3.1), and the
properties of the actual significance tests (§ 3.2)
we are now ready to provide a simple recipe for
significance test selection in NLP setups. The de-
cision tree in Figure 1 provides an illustration.

Does the test
statistic come
from a known
distribution?

Use a para-
metric test

Is the data
size small ?

Use bootstrap
or random-
ization test

Use sampling-
free non-

parametric test

Yes No

Yes No

Figure 1: Decision tree for statistical significance
test selection.

If the distribution of the test statistic is
known, then parametric tests are most appropri-
ate. These tests are more statistically powerful
and less computationally intensive compared to
their non-parametric counterparts. The stronger
statistical power of parametric tests stems from
the stronger, parametric assumptions they make,
while the higher computational demand of some
non-parametric tests is the result of their sampling
process.

When the distribution of the test statistic is un-
known, the first non-parametric family of choice
is that of sampling-based tests. These tests con-
sider the actual values of the evaluation measures
and are not restricted to higher order properties
(e.g. ranks) of the observed values – their statis-
tical power is hence higher. As noted in (Riezler
and Maxwell, 2005), in the case where the distri-
butional assumptions of the parametric tests are vi-
olated, sampling-based tests have more statistical
power than parametric tests.

Nonetheless, sampling-based tests are compu-
tationally intensive – the exact permutation test,
for example, requires the generation of all 2n data
permutations (where n is the number of points in
the dataset). To overcome this, approximate ran-
domization can be used, as was done, e.g., by
Yeh (2000) for test sets of more than 20 points.
The other alternative for very large datasets are
sampling-free tests that are less powerful but are
computationally feasible.

In what follows we check whether recent ACL
and TACL papers follow these guidelines.



1389

4 Survey of ACL and TACL papers

General Statistics ACL ’17 TACL ’17
Total number of pa-
pers

196 37

# relevant (experimen-
tal) papers

180 33

# different tasks 36 15
# different evaluation
measures

24 19

Average number of
measures per paper

2.34 2.1

# papers that do not
report significance

117 15

# papers that report
significance

63 18

# papers that report
significance but use
the wrong statistical
test

6 0

# papers that report
significance but do not
mention the test name

21 3

# papers that have to
report replicability

110 19

# papers that report
replicability

3 4

# papers that perform
cross validation

23 5

Table 2: Statistical significance statistics for em-
pirical ACL and TACL 2017 papers.

We analyzed the long papers from the pro-
ceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (ACL17,
(Barzilay and Kan, 2017)), a total of 196 pa-
pers, and the papers from the Transactions of the
Association of Computational Linguistics journal
(TACL17), Volume 5, Issue 1, a total of 37 pa-
pers. We have focused on empirical papers where
at least one comparison between methods was per-
formed.

Table 2 presents the main results from our sur-
vey. The top part of the table presents general
statistics of our dataset. In both conference and
journal papers, the variety of different NLP tasks
is quite large: 36 tasks in ACL 2017 and 15 tasks
in TACL. Interestingly, in almost every paper in
our survey the researchers chose to analyze their
results using more than one evaluation measure,

Statistical Test ACL ’17 TACL ’17
Bootstrap 6 1
t-test 17 2
Wilcoxon 3 0
Chi square 3 1
Randomization 3 1
McNemar 2 3
Sign 2 3
Permutation 1 4

Table 3: Number of times each of the prominent
statistical significance tests in ACL and TACL
2017 papers was used. 42 ACL and 15 TACL pa-
pers reported the significance test name. 5 ACL
papers mentioned an unrecognized test name.

with an average of 2.34 (ACL) and 2.1 (TACL).
Table 1 presents the most common of these evalu-
ation measures.

The lower part of Table 2 depicts the disturb-
ing reality of statistical significance testing in our
research community. Out of the 180 experimen-
tal long papers of ACL 2017, only 63 papers in-
cluded a statistical significance test. Moreover, out
of these 63 papers 21 did not mention the name
of the significance test they employed. Of the 42
papers that did mention the name of the signifi-
cance test, 6 used the wrong test according to the
considerations discussed in § 3.3 In TACL, where
the review process is presumably more strict and
of higher quality, out of 33 experimental papers,
15 did not include statistical significance testing,
and all the papers that report significance and men-
tioned the name of the test used a valid test.

While this paper focuses on the correct choice
of a significance test, we also checked whether the
papers in our sample account for the effect of mul-
tiple hypothesis testing when testing statistical sig-
nificance (see (Dror et al., 2017)). When testing
multiple hypotheses, as in the case of comparing
the participating algorithms across a large number
of datasets, the probability of making one or more
false claims may be very high, even if the proba-
bility of drawing an erroneous conclusion in each
individual comparison is small. In ACL 2017, out

3We considered the significance test to be inappropriate in
three cases: 1. Using the t-test when the evaluation measure
is not an average measure; 2. Using the t-test for a classifi-
cation task (i.e. when the observations are categorical rather
then continuous), even if the evaluation measure is an aver-
age measure; and 3. Using a Boostrap test with a small test
set size.



1390

of 110 papers that used multiple datasets only 3
corrected for multiplicity (all using the Bonferroni
correction). In TACL, the situation is slightly bet-
ter with 4 papers correcting for multiplicity out of
19 that should have done that.

Regarding the statistical tests that were used in
the papers that did report significance (Table 3), in
ACL 2017 most of the papers used the Student’s
t-test that assumes the data is i.i.d and that the test
statistics are normally distributed. As discussed in
§ 3 this is not the case in many NLP applications.
Gladly, in TACL, t-test is not as prominent.

One final note is about the misuse of the word
significant. We noticed that in a considerable
number of papers this word was used as a syn-
onym for words such as important, considerable,
meaningful, substantial, major, notable etc. We
believe that we should be more careful when us-
ing this word, ideally keeping its statistical sense
and using other, more general words to indicate a
substantial impact.

We close this discussion with two important
open issues.

5 Open Questions

In this section we would like to point on two issues
that remain open even after our investigation. We
hope that bringing these issues to the attention of
the research community will encourage our fellow
researchers to come up with appropriate solutions.

The first open issue is that of dependent obser-
vations. An assumption shared by the statistical
significance tests described in § 3, that are com-
monly used in NLP setups, is that the data samples
are independent and identically distributed. This
assumption, however, is rarely true in NLP setups.

For example, the popular WSJ Penn Treebank
corpus (Marcus et al., 1993) consists of 2,499 ar-
ticles from a three year Wall Street Journal (WSJ)
collection of 98,732 stories. Obviously, some of
the sentences included in the corpus come from
the same article, were written by the same author
or were reviewed before publication by the same
editor. As another example, many sentences in the
Europarl parallel corpus (Koehn, 2005) that is very
popular in the machine translation literature are
taken from the same parliament discussion. An in-
dependence assumption between the sentences in
these corpora is not likely to hold.

This dependence between test examples vio-
lates the conditions under which the theoretical

guarantees of the various tests were developed.
The impact of this phenomenon on our results
is hard to quantify, partly because it is hard to
quantify the nature of the dependence between
test set examples in NLP datasets. Some papers
are even talking about abandoning the null hy-
pothesis statistical significance test approach due
to this hard-to-meet assumption (Koplenig, 2017;
McShane et al., 2017; Carver, 1978; Leek et al.,
2017). In our opinion, this calls for a future col-
laboration with statisticians in order to better un-
derstand the extent to which existing popular sig-
nificance tests are relevant for NLP, and to develop
alternative tests if necessary.

Another issue that deserves some thought is that
of cross-validation. To increase the validity of re-
ported results, it is customary in NLP papers to
create a number of random splits of the experimen-
tal corpus into train, development and test portions
(see Table 2). For each such split (fold), the tested
algorithms are trained and tuned on the training
and development datasets, respectively, and their
results on the test data are recorded. The final re-
ported result is typically the average of the test set
results across the splits. Some papers also report
the fraction of the folds for which one algorithm
was better than the others. While cross-validation
is surely a desired practice, it is challenging to re-
port statistical significance when it is employed.
Particularly, the test sets of the different folds are
obviously not independent – their content is even
likely to overlap.

One solution we would like to propose here
is based on replicability analysis (Dror et al.,
2017). This paper proposes a statistical sig-
nificance framework for multiple comparisons
performed with dependent test sets, using the
KBonferroni estimator for the number of datasets
with significant effect. One statistically sound way
to test for significance when a cross-validation
protocol is employed is hence to calculate the p-
value for each fold separately, and then to per-
form replicability analysis for dependent datasets
with KBonferroni. Only if this analysis rejects
the null hypothesis in all folds (or in more than a
predefined threshold number of folds), the results
should be declared significant. Here again, further
statistical investigation may lead to additional, po-
tentially better, solutions.



1391

6 Conclusions

We discussed the use of significance testing in
NLP. We provided the main considerations for sig-
nificance test selection, and proposed a simple test
selection protocol. We then surveyed the state of
significance testing in recent top venue papers and
concluded with open issues. We hope this paper
will serve as a guide for NLP researchers and,
not less importantly, that it will encourage discus-
sions and collaborations that will contribute to the
soundness and correctness of our research.

References
Theodore W Anderson and Donald A Darling. 1954.

A test of goodness of fit. Journal of the American
statistical association 49(268):765–769.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings
of the acl workshop on intrinsic and extrinsic evalu-
ation measures for machine translation and/or sum-
marization.

Regina Barzilay and Min-Yen Kan. 2017. Proceed-
ings of the 55th annual meeting of the association
for computational linguistics (volume 1: Long pa-
pers). In Proceedings of ACL.

Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statis-
tical significance in nlp. In Proceedings of EMNLP-
CoNLL.

John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of EMNLP.

Ronald Carver. 1978. The case against statistical
significance testing. Harvard Educational Review
48(3):378–399.

Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves statis-
tical machine translation. In Proceedings of ACL.

William G Cochran. 1950. The comparison of percent-
ages in matched samples. Biometrika 37(3/4):256–
266.

Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL.

Rotem Dror, Gili Baumer, Marina Bogomolov, and Roi
Reichart. 2017. Replicability analysis for natural
language processing: Testing significance with mul-
tiple datasets. Transactions of the Association for
Computational Linguistics 5:471–486.

Bradley Efron and Robert J Tibshirani. 1994. An intro-
duction to the bootstrap. CRC press.

Ronald Aylmer Fisher. 1937. The design of experi-
ments. Oliver And Boyd; Edinburgh; London.

Jean Dickinson Gibbons and Subhabrata Chakraborti.
2011. Nonparametric statistical inference. In
International encyclopedia of statistical science,
Springer, pages 977–979.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
MT summit.

Alexander Koplenig. 2017. Against statistical signifi-
cance testing in corpus linguistics. Corpus Linguis-
tics and Linguistic Theory .

Sandra Kübler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing. Synthesis Lectures on
Human Language Technologies 1(1):1–127.

Jeff Leek, Blakeley B McShane, Andrew Gelman,
David Colquhoun, Michèle B Nuijten, and Steven N
Goodman. 2017. Five ways to fix statistics. Nature
551(7682):557–559.

Junhui Li, Deyi Xiong, Zhaopeng Tu, Muhua Zhu, Min
Zhang, and Guodong Zhou. 2017. Modeling source
syntax for neural machine translation. In Proceed-
ings of ACL.

Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics 19(2):313–330.

Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika 12(2):153–157.

Blakeley B McShane, David Gal, Andrew Gelman,
Christian Robert, and Jennifer L Tackett. 2017.
Abandon statistical significance. arXiv preprint
arXiv:1709.07588 .

Eric W Noreen. 1989. Computer intensive methods
for hypothesis testing: An introduction. Wiley, New
York.

Hiroki Ouchi, Hiroyuki Shindo, and Yuji Matsumoto.
2017. Neural modeling of multi-predicate interac-
tions for japanese predicate argument structure anal-
ysis. In Proceedings of ACL.



1392

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
ACL.

Stefan Riezler and John T Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for mt. In Proceedings of the ACL workshop on
intrinsic and extrinsic evaluation measures for ma-
chine translation and/or summarization.

Alexander Rush, Roi Reichart, Michael Collins, and
Amir Globerson. 2012. Improved parsing and pos
tagging using inter-sentence consistency constraints.
In Proceedings of EMNLP-CoNLL.

J Sethuraman. 1963. The Advanced Theory of Statis-
tics, Volume 2: Inference and Relationship. JSTOR.

Samuel Sanford Shapiro and Martin B Wilk. 1965.
An analysis of variance test for normality (complete
samples). Biometrika 52(3/4):591–611.

Anders Søgaard. 2013. Estimating effect size across
datasets. In Proceedings of NAACL-HLT .

Anders Søgaard, Anders Johannsen, Barbara Plank,
Dirk Hovy, and Héctor Martı́nez Alonso. 2014.
What’s in a p-value in nlp? In Proceedings of
CoNLL.

Frank Wilcoxon. 1945. Individual comparisons by
ranking methods. Biometrics bulletin 1(6):80–83.

Shuangzhi Wu, Dongdong Zhang, Nan Yang, Mu Li,
and Ming Zhou. 2017. Sequence-to-dependency
neural machine translation. In Proceedings of ACL.

Bishan Yang and Tom Mitchell. 2017. Leveraging
knowledge bases in lstms for improving machine
reading. In Proceedings of ACL.

Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of COLING.

Yftah Ziser and Roi Reichart. 2017. Neural structural
correspondence learning for domain adaptation. In
Proceedings of CoNLL 2017.


