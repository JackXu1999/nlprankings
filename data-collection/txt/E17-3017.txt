



















































Nematus: a Toolkit for Neural Machine Translation


Proceedings of the EACL 2017 Software Demonstrations, Valencia, Spain, April 3-7 2017, pages 65–68
c©2017 Association for Computational Linguistics

Nematus: a Toolkit for Neural Machine Translation

Rico Sennrich† Orhan Firat? Kyunghyun Cho‡ Alexandra Birch†
Barry Haddow† Julian Hitschler¶ Marcin Junczys-Dowmunt† Samuel Läubli§

Antonio Valerio Miceli Barone† Jozef Mokry† Maria Nădejde†
†University of Edinburgh ?Middle East Technical University

‡New York University ¶Heidelberg University §University of Zurich

Abstract

We present Nematus, a toolkit for Neu-
ral Machine Translation. The toolkit pri-
oritizes high translation accuracy, usabil-
ity, and extensibility. Nematus has been
used to build top-performing submissions
to shared translation tasks at WMT and
IWSLT, and has been used to train systems
for production environments.

1 Introduction

Neural Machine Translation (NMT) (Bahdanau et
al., 2015; Sutskever et al., 2014) has recently es-
tablished itself as a new state-of-the art in machine
translation. We present Nematus1, a new toolkit
for Neural Machine Translation.

Nematus has its roots in the dl4mt-tutorial.2 We
found the codebase of the tutorial to be compact,
simple and easy to extend, while also produc-
ing high translation quality. These characteristics
make it a good starting point for research in NMT.
Nematus has been extended to include new func-
tionality based on recent research, and has been
used to build top-performing systems to last year’s
shared translation tasks at WMT (Sennrich et al.,
2016) and IWSLT (Junczys-Dowmunt and Birch,
2016).

Nematus is implemented in Python, and based
on the Theano framework (Theano Develop-
ment Team, 2016). It implements an attentional
encoder–decoder architecture similar to Bahdanau
et al. (2015). Our neural network architecture dif-
fers in some aspect from theirs, and we will dis-
cuss differences in more detail. We will also de-
scribe additional functionality, aimed to enhance
usability and performance, which has been imple-
mented in Nematus.

1available at https://github.com/rsennrich/nematus
2
https://github.com/nyu-dl/dl4mt-tutorial

2 Neural Network Architecture

Nematus implements an attentional encoder–
decoder architecture similar to the one described
by Bahdanau et al. (2015), but with several imple-
mentation differences. The main differences are as
follows:

• We initialize the decoder hidden state with
the mean of the source annotation, rather than
the annotation at the last position of the en-
coder backward RNN.

• We implement a novel conditional GRU with
attention.

• In the decoder, we use a feedforward hidden
layer with tanh non-linearity rather than a
maxout before the softmax layer.

• In both encoder and decoder word embed-
ding layers, we do not use additional biases.

• Compared to Look, Generate, Update de-
coder phases in Bahdanau et al. (2015), we
implement Look, Update, Generate which
drastically simplifies the decoder implemen-
tation (see Table 1).

• Optionally, we perform recurrent Bayesian
dropout (Gal, 2015).

• Instead of a single word embedding at each
source position, our input representations al-
lows multiple features (or “factors”) at each
time step, with the final embedding being the
concatenation of the embeddings of each fea-
ture (Sennrich and Haddow, 2016).

• We allow tying of embedding matrices (Press
and Wolf, 2017; Inan et al., 2016).

We will here describe some differences in more
detail:

65



Table 1: Decoder phase differences
RNNSearch (Bahdanau et al., 2015) Nematus (DL4MT)
Phase Output - Input Phase Output - Input
Look cj ← sj−1,C Look cj ← sj−1, yj−1,C
Generate yj ← sj−1, yj−1, cj Update sj ← sj−1, yj−1, cj
Update sj ← sj−1, yj , cj Generate yj ← sj , yj−1, cj

Given a source sequence (x1, . . . , xTx) of
length Tx and a target sequence (y1, . . . , yTy) of
length Ty, let hi be the annotation of the source
symbol at position i, obtained by concatenating
the forward and backward encoder RNN hidden
states, hi = [

−→
h i;
←−
h i], and sj be the decoder hid-

den state at position j.

decoder initialization Bahdanau et al. (2015)
initialize the decoder hidden state s with the last
backward encoder state.

s0 = tanh
(
Winit

←−
h 1
)

with Winit as trained parameters.3 We use the
average annotation instead:

s0 = tanh

(
Winit

∑Tx
i=1 hi
Tx

)
conditional GRU with attention Nematus im-
plements a novel conditional GRU with attention,
cGRUatt. A cGRUatt uses its previous hidden state
sj−1, the whole set of source annotations C =
{h1, . . . ,hTx} and the previously decoded symbol
yj−1 in order to update its hidden state sj , which
is further used to decode symbol yj at position j,

sj = cGRUatt (sj−1, yj−1,C)

Our conditional GRU layer with attention
mechanism, cGRUatt, consists of three compo-
nents: two GRU state transition blocks and an
attention mechanism ATT in between. The first
transition block, GRU1, combines the previous de-
coded symbol yj−1 and previous hidden state sj−1
in order to generate an intermediate representation
s′j with the following formulations:

s′j = GRU1 (yj−1, sj−1) = (1− z′j)� s′j + z′j � sj−1,
s′j = tanh

(
W′E[yj−1] + r

′
j � (U′sj−1)

)
,

r′j = σ
(
W′rE[yj−1] + U

′
rsj−1

)
,

z′j = σ
(
W′zE[yj−1] + U

′
zsj−1

)
,

where E is the target word embedding matrix, s′j
is the proposal intermediate representation, r′j and

3All the biases are omitted for simplicity.

z′j being the reset and update gate activations. In
this formulation, W′, U′, W′r, U′r, W′z , U′z are
trained model parameters; σ is the logistic sigmoid
activation function.

The attention mechanism, ATT, inputs the en-
tire context set C along with intermediate hidden
state s′j in order to compute the context vector cj
as follows:

cj =ATT
(
C, s′j

)
=

Tx∑
i

αijhi,

αij =
exp(eij)∑Tx

k=1 exp(ekj)
,

eij =vᵀa tanh
(
Uas′j + Wahi

)
,

where αij is the normalized alignment weight be-
tween source symbol at position i and target sym-
bol at position j and va,Ua,Wa are the trained
model parameters.

Finally, the second transition block, GRU2, gen-
erates sj , the hidden state of the cGRUatt, by look-
ing at intermediate representation s′j and context
vector cj with the following formulations:

sj = GRU2
(
s′j , cj

)
= (1− zj)� sj + zj � s′j ,

sj =tanh
(
Wcj + rj � (Us′j)

)
,

rj =σ
(
Wrcj + Urs

′
j

)
,

zj =σ
(
Wzcj + Uzs

′
j

)
,

similarly, sj being the proposal hidden state,
rj and zj being the reset and update gate
activations with the trained model parameters
W,U,Wr,Ur,Wz,Uz .
Note that the two GRU blocks are not individu-
ally recurrent, recurrence only occurs at the level
of the whole cGRU layer. This way of combining
RNN blocks is similar to what is referred in the
literature as deep transition RNNs (Pascanu et al.,
2014; Zilly et al., 2016) as opposed to the more
common stacked RNNs (Schmidhuber, 1992; El
Hihi and Bengio, 1995; Graves, 2013).

deep output Given sj , yj−1, and cj , the out-
put probability p(yj |sj , yj−1, cj) is computed by
a softmax activation, using an intermediate repre-
sentation tj .

p(yj |sj ,yj−1, cj) = softmax (tjWo)
tj = tanh (sjWt1 + E[yj−1]Wt2 + cjWt3)

Wt1,Wt2,Wt3,Wo are the trained model pa-
rameters.

66



hello 0.946

0.056

world 0.957

0.100

World 0.010

4.632

. 0.030

3.609

! 0.928

0.175

... 0.014

4.384

<eos> 0.999

3.609

world 0.684

5.299

HI 0.007

4.920

<eos> 0.994

4.390

Hey 0.006

5.107

<eos> 0.999

0.175

0

Figure 1: Search graph visualisation for DE→EN
translation of "Hallo Welt!" with beam size 3.

3 Training Algorithms

By default, the training objective in Nematus is
cross-entropy minimization on a parallel training
corpus. Training is performed via stochastic gra-
dient descent, or one of its variants with adaptive
learning rate (Adadelta (Zeiler, 2012), RmsProp
(Tieleman and Hinton, 2012), Adam (Kingma and
Ba, 2014)).

Additionally, Nematus supports minimum risk
training (MRT) (Shen et al., 2016) to optimize to-
wards an arbitrary, sentence-level loss function.
Various MT metrics are supported as loss function,
including smoothed sentence-level BLEU (Chen
and Cherry, 2014), METEOR (Denkowski and
Lavie, 2011), BEER (Stanojevic and Sima’an,
2014), and any interpolation of implemented met-
rics.

To stabilize training, Nematus supports early
stopping based on cross entropy, or an arbitrary
loss function defined by the user.

4 Usability Features

In addition to the main algorithms to train and
decode with an NMT model, Nematus includes
features aimed towards facilitating experimenta-
tion with the models, and their visualisation. Var-
ious model parameters are configurable via a
command-line interface, and we provide extensive
documentation of options, and sample set-ups for
training systems.

Nematus provides support for applying single
models, as well as using multiple models in an en-
semble – the latter is possible even if the model
architectures differ, as long as the output vocabu-
lary is the same. At each time step, the probability

distribution of the ensemble is the geometric aver-
age of the individual models’ probability distribu-
tions. The toolkit includes scripts for beam search
decoding, parallel corpus scoring and n-best-list
rescoring.

Nematus includes utilities to visualise the atten-
tion weights for a given sentence pair, and to vi-
sualise the beam search graph. An example of the
latter is shown in Figure 1. Our demonstration will
cover how to train a model using the command-
line interface, and showing various functionalities
of Nematus, including decoding and visualisation,
with pre-trained models.4

5 Conclusion

We have presented Nematus, a toolkit for Neural
Machine Translation. We have described imple-
mentation differences to the architecture by Bah-
danau et al. (2015); due to the empirically strong
performance of Nematus, we consider these to be
of wider interest.

We hope that researchers will find Nematus an
accessible and well documented toolkit to support
their research. The toolkit is by no means limited
to research, and has been used to train MT systems
that are currently in production (WIPO, 2016).

Nematus is available under a permissive BSD
license.

Acknowledgments

This project has received funding from the Euro-
pean Union’s Horizon 2020 research and innova-
tion programme under grant agreements 645452
(QT21), 644333 (TraMOOC), 644402 (HimL) and
688139 (SUMMA).

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural Machine Translation by Jointly
Learning to Align and Translate. In Proceedings of
the International Conference on Learning Represen-
tations (ICLR).

Boxing Chen and Colin Cherry. 2014. A Systematic
Comparison of Smoothing Techniques for Sentence-
Level BLEU. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, pages 362–367,
Baltimore, Maryland, USA.

4Pre-trained models for 8 translation directions are avail-
able at http://statmt.org/rsennrich/wmt16_systems/

67



Michael Denkowski and Alon Lavie. 2011. Me-
teor 1.3: Automatic Metric for Reliable Optimiza-
tion and Evaluation of Machine Translation Sys-
tems. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 85–91, Ed-
inburgh, Scotland.

Salah El Hihi and Yoshua Bengio. 1995. Hierarchical
Recurrent Neural Networks for Long-Term Depen-
dencies. In Nips, volume 409.

Yarin Gal. 2015. A Theoretically Grounded Appli-
cation of Dropout in Recurrent Neural Networks.
ArXiv e-prints.

Alex Graves. 2013. Generating sequences
with recurrent neural networks. arXiv preprint
arXiv:1308.0850.

Hakan Inan, Khashayar Khosravi, and Richard Socher.
2016. Tying Word Vectors and Word Classifiers: A
Loss Framework for Language Modeling. CoRR,
abs/1611.01462.

Marcin Junczys-Dowmunt and Alexandra Birch. 2016.
The University of Edinburgh’s systems submis-
sion to the MT task at IWSLT. In The Interna-
tional Workshop on Spoken Language Translation
(IWSLT), Seattle, USA.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Razvan Pascanu, Çağlar Gülçehre, Kyunghyun Cho,
and Yoshua Bengio. 2014. How to Construct Deep
Recurrent Neural Networks. In International Con-
ference on Learning Representations 2014 (Confer-
ence Track).

Ofir Press and Lior Wolf. 2017. Using the Output Em-
bedding to Improve Language Models. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), Valencia, Spain.

Jürgen Schmidhuber. 1992. Learning complex, ex-
tended sequences using the principle of history com-
pression. Neural Computation, 4(2):234–242.

Rico Sennrich and Barry Haddow. 2016. Linguistic
Input Features Improve Neural Machine Translation.
In Proceedings of the First Conference on Machine
Translation, Volume 1: Research Papers, pages 83–
91, Berlin, Germany.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Edinburgh Neural Machine Translation Sys-
tems for WMT 16. In Proceedings of the First Con-
ference on Machine Translation, Volume 2: Shared
Task Papers, pages 368–373, Berlin, Germany.

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Minimum
Risk Training for Neural Machine Translation. In

Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), Berlin, Germany.

Milos Stanojevic and Khalil Sima’an. 2014. BEER:
BEtter Evaluation as Ranking. In Proceedings of the
Ninth Workshop on Statistical Machine Translation,
pages 414–419, Baltimore, Maryland, USA.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to Sequence Learning with Neural Net-
works. In Advances in Neural Information Process-
ing Systems 27: Annual Conference on Neural Infor-
mation Processing Systems 2014, pages 3104–3112,
Montreal, Quebec, Canada.

Theano Development Team. 2016. Theano: A Python
framework for fast computation of mathematical ex-
pressions. arXiv e-prints, abs/1605.02688.

Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture
6.5 - rmsprop.

WIPO. 2016. WIPO Develops Cutting-Edge
Translation Tool For Patent Documents, Oct.
http://www.wipo.int/pressroom/en/
articles/2016/article\_0014.html.

Matthew D Zeiler. 2012. ADADELTA: an
adaptive learning rate method. arXiv preprint
arXiv:1212.5701.

Julian Georg Zilly, Rupesh Kumar Srivastava,
Jan Koutník, and Jürgen Schmidhuber. 2016.
Recurrent highway networks. arXiv preprint
arXiv:1607.03474.

68


