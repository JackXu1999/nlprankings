



















































Adversarial Learning of Privacy-Preserving Text Representations for De-Identification of Medical Records


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5829–5839
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

5829

Adversarial Learning of Privacy-Preserving Text Representations
for De-Identification of Medical Records

Max Friedrich1, Arne Köhn2, Gregor Wiedemann1, Chris Biemann1

1Language Technology Group, Universität Hamburg
{2mfriedr,gwiedemann,biemann}@informatik.uni-hamburg.de

2Department of Language Science and Technology, Saarland University
koehn@coli.uni-saarland.de

Abstract

De-identification is the task of detecting pro-
tected health information (PHI) in medical
text. It is a critical step in sanitizing electronic
health records (EHRs) to be shared for re-
search. Automatic de-identification classifiers
can significantly speed up the sanitization pro-
cess. However, obtaining a large and diverse
dataset to train such a classifier that works well
across many types of medical text poses a chal-
lenge as privacy laws prohibit the sharing of
raw medical records. We introduce a method
to create privacy-preserving shareable repre-
sentations of medical text (i.e. they contain
no PHI) that does not require expensive man-
ual pseudonymization. These representations
can be shared between organizations to create
unified datasets for training de-identification
models. Our representation allows training a
simple LSTM-CRF de-identification model to
an F1 score of 97.4%, which is comparable
to a strong baseline that exposes private infor-
mation in its representation. A robust, widely
available de-identification classifier based on
our representation could potentially enable
studies for which de-identification would oth-
erwise be too costly.

1 Introduction

Electronic health records (EHRs) are are valuable
resource that could potentially be used in large-
scale medical research (Botsis et al., 2010; Birk-
head et al., 2015; Cowie et al., 2017). In addition
to structured medical data, EHRs contain free-text
patient notes that are a rich source of information
(Jensen et al., 2012). However, due to privacy
and data protection laws, medical records can only
be shared and used for research if they are sani-
tized to not include information potentially identi-
fying patients. The PHI that may not be shared in-
cludes potentially identifying information such as
names, geographic identifiers, dates, and account

numbers; the American Health Insurance Porta-
bility Accountability Act1 (HIPAA, 1996) defines
18 categories of PHI. De-identification is the task
of finding and labeling PHI in medical text as a
step toward sanitization. As the information to
be removed is very sensitive, sanitization always
requires final human verification. Automatic de-
identification labeling can however significantly
speed up the process, as shown for other annota-
tion tasks in e.g. Yimam (2015).

Trying to create an automatic classifier for de-
identification leads to a “chicken and egg prob-
lem” (Uzuner et al., 2007): without a compre-
hensive training set, an automatic de-identification
classifier cannot be developed, but without ac-
cess to automatic de-identification, it is difficult to
share large corpora of medical text in a privacy-
preserving way for research (including for train-
ing the classifier itself). The standard method of
data protection compliant sharing of training data
for a de-identification classifier requires humans to
pseudonymize protected information with substi-
tutes in a document-coherent way. This includes
replacing e.g. every person or place name with
a different name, offsetting dates by a random
amount while retaining date intervals, and replac-
ing misspellings with similar misspellings of the
pseudonym (Uzuner et al., 2007).

In 2019, a pseudonymized dataset for de-
identification from a single source, the i2b2 2014
dataset, is publicly available (Stubbs and Uzuner,
2015). However, de-identification classifiers
trained on this dataset do not generalize well to
data from other sources (Stubbs et al., 2017).
To obtain a universal de-identification classifier,
many medical institutions would have to pool their
data. But, preparing this data for sharing us-
ing the document-coherent pseudonymization ap-

1https://legislink.org/us/pl-104-191

mailto:2mfriedr@informatik.uni-hamburg.de
mailto:gwiedemann@informatik.uni-hamburg.de
mailto:biemann@informatik.uni-hamburg.de
mailto:koehn@coli.uni-saarland.de
https://legislink.org/us/pl-104-191


5830

James was admitted to
St. Thomas. . .

Raw patient notes

[James]Patient was admitted
to [St. Thomas]Hosp. . .

PHI-labeled patient notes

[Henry]Patient was admitted
to [River Clinic]Hosp. . .

Pseudonymized patient notes µ

[���]Patient ��� ���
��� [��� ���]Hosp. . .

Private vector representation
of patient notes µ

PHI labeling /
de-identification

Pse
udo

nym
izat

ion

Non-reversible

transformation




f

3

Figure 1: Sharing training data for de-identification. PHI annotations are marked with [brackets]. Upper alter-
native: traditional process using manual pseudonymization. Lower alternative: our approach of sharing private
vector representations. The people icon represents tasks done by humans; the gears icon represents tasks done by
machines; the lock icon represents privacy-preserving artifacts. Manual pseudonymization is marked with a dollar
icon to emphasize its high costs.

proach requires large human effort (Dernoncourt
et al., 2017).

To address this problem, we introduce an ad-
versarially learned representation of medical text
that allows privacy-preserving sharing of training
data for a de-identification classifier by transform-
ing text non-reversibly into a vector space and
only sharing this representation. Our approach
still requires humans to annotate PHI (as this is
the training data for the actual de-identification
task) but the pseudonymization step (replacing
PHI with coherent substitutes) is replaced by the
automatic transformation to the vector representa-
tion instead. A classifier then trained on our rep-
resentation cannot contain any protected data, as it
is never trained on raw text (as long as the repre-
sentation does not allow for the reconstruction of
sensitive information). The traditional approach to
sharing training data is conceptually compared to
our approach in Fig. 1.

2 Related Work

Our work builds upon two lines of research: firstly
de-identification, as the system has to provide
good de-identification performance, and secondly
adversarial representation learning, to remove all
identifying information from the representations
to be distributed.

2.1 Automatic De-Identification
Analogously to many natural language process-
ing tasks, the state of the art in de-identification
changed in recent years from rule-based systems
and shallow machine learning approaches like
conditional random fields (CRFs) (Uzuner et al.,

2007; Meystre et al., 2010) to deep learning meth-
ods (Stubbs et al., 2017; Dernoncourt et al., 2017;
Liu et al., 2017).

Three i2b2 shared tasks on de-identification
were run in 2006 (Uzuner et al., 2007), 2014
(Stubbs et al., 2015), and 2016 (Stubbs et al.,
2017). The organizers performed manual pseu-
donymization on clinical records from a single
source to create the datasets for each of the tasks.
An F1 score of 95% has been suggested as a target
for reasonable de-identification systems (Stubbs
et al., 2015).

Dernoncourt et al. (2017) first applied a
long short-term memory (LSTM) (Hochreiter and
Schmidhuber, 1997) model with a CRF output
component to de-identification. Transfer learn-
ing from a larger dataset slightly improves perfor-
mance on the i2b2 2014 dataset (Lee et al., 2018).
Liu et al. (2017) achieve state-of-the-art perfor-
mance in de-identification by combining a deep
learning ensemble with a rule component.

Up to and including the 2014 shared task, the
organizers emphasized that it is unclear if a sys-
tem trained on the provided datasets will general-
ize to medical records from other sources (Uzuner
et al., 2007; Stubbs et al., 2015). The 2016 shared
task featured a sight-unseen track in which de-
identification systems were evaluated on records
from a new data source. The best system achieved
an F1 score of 79%, suggesting that systems at
the time were not able to deliver sufficient per-
formance on completely new data (Stubbs et al.,
2017).



5831

2.2 Adversarial Representation Learning

Fair representations (Zemel et al., 2013; Hamm,
2015) aim to encode features of raw data that al-
lows it to be used in e.g. machine learning algo-
rithms while obfuscating membership in a pro-
tected group or other sensitive attributes. The
domain-adversarial neural network (DANN) ar-
chitecture (Ganin et al., 2016) is a deep learning
implementation of a three-party game between a
representer, a classifier, and an adversary compo-
nent. The classifier and the adversary are deep
learning models with shared initial layers. A gra-
dient reversal layer is used to worsen the represen-
tation for the adversary during back-propagation:
when training the adversary, the adversary-specific
part of the network is optimized for the adversarial
task but the shared part is updated against the gra-
dient to make the shared representation less suit-
able for the adversary.

Although initially conceived for use in domain
adaptation, DANNs and similar adversarial deep
learning models have recently been used to obfus-
cate demographic attributes from text (Elazar and
Goldberg, 2018; Li et al., 2018) and subject iden-
tity (Feutry et al., 2018) from images. Elazar and
Goldberg (2018) warn that when a representation
is learned using gradient reversal methods, contin-
ued adversary training on the frozen representa-
tion may allow adversaries to break representation
privacy. To test whether the unwanted informa-
tion is not extractable from the generated informa-
tion anymore, adversary training needs to continue
on the frozen representation after finishing train-
ing the system. Only if after continued adversary
training the information cannot be recovered, we
have evidence that it really is not contained in the
representation anymore.

3 Dataset and De-Identification Model

We evaluate our approaches using the i2b2 2014
dataset (Stubbs and Uzuner, 2015), which was re-
leased as part of the 2014 i2b2/UTHealth shared
task track 1 and is the largest publicly available
dataset for de-identification today. It contains
1304 free-text documents with PHI annotations.
The i2b2 dataset uses the 18 categories of PHI de-
fined by HIPAA as a starting point for its own set
of PHI categories. In addition to the HIPAA set
of categories, it includes (sub-)categories such as
doctor names, professions, states, countries, and
ages under 90.

Hyperparameter Value

Pre-trained embeddings FastText, GloVe
Casing feature Yes
Batch size 32
Number of LSTM layers 2
LSTM units per layer/dir. 128
Input embedding dropout 0.1
Variational dropout 0.25
Dropout after LSTM 0.5
Optimizer Nadam
Gradient norm clipping 1.0

Table 1: Hyperparameter configuration of our de-
identification model.

We compare three different approaches: a non-
private de-identification classifier and two privacy-
enabled extensions, automatic pseudonymization
(Section 4) and adversarially learned representa-
tions (Section 5).

Our non-private system as well as the privacy-
enabled extensions are based on a bidirectional
LSTM-CRF architecture that has been proven to
work well in sequence tagging (Huang et al., 2015;
Lample et al., 2016) and de-identification (Der-
noncourt et al., 2017; Liu et al., 2017). We only
use pre-trained FastText (Bojanowski et al., 2017)
or GloVe (Pennington et al., 2014) word embed-
dings, not explicit character embeddings, as we
suspect that these may allow easy re-identification
of private information if used in shared represen-
tations. In place of learned character features,
we provide the casing feature from Reimers and
Gurevych (2017) as an additional input. The fea-
ture maps words to a one-hot representation of
their casing (numeric, mainly numeric, all lower,
all upper, initial upper, contains digit, or other).

Table 1 shows our raw de-identification model’s
hyperparameter configuration that was determined
through a random hyperparameter search.

4 Automatic Pseudonymization

To provide a baseline to compare our primary ap-
proach against, we introduce a naı̈ve word-level
automatic pseudonymization approach that ex-
ploits the fact that state-of-the-art de-identification
models (Liu et al., 2017; Dernoncourt et al., 2017)
as well as our non-private de-identification model
work on the sentence level and do not rely on doc-
ument coherency. Before training, we shuffle the



5832

James was admitted · · ·

· · ·

Representation Model

· · ·

De-Identification Model

· · ·

Adversary Model

Tokens

Emb.

Represent.

De-identification output Adversary output

Figure 2: Simplified visualization of the adversarial
model architecture. Sequences of squares denote real-
valued vectors, dotted arrows represent possible addi-
tional real or fake inputs to the adversary. The cas-
ing feature that is provided as a second input to the de-
identification model is omitted for legibility.

training sentences and replace all PHI tokens with
a random choice of a fixed numberN of their clos-
est neighbors in an embedding space (including
the token itself), as determined by cosine distance
in a pre-computed embedding matrix.

Using this approach, the sentence

[James] was admitted to [St. Thomas]

may be replaced by

[Henry] was admitted to [Croix Scott].

While the resulting sentences do not necessarily
make sense to a reader (e.g. “Croix Scott” is not
a realistic hospital name), its embedding represen-
tation is similar to the original. We train our de-
identification model on the transformed data and
test it on the raw data. The number of neighbors
N controls the privacy properties of the approach:
N = 1 means no pseudonymization; setting N to
the number of rows in a precomputed embedding
matrix delivers perfect anonymization but the re-
sulting data may be worthless for training a de-
identification model.

5 Adversarial Representation

We introduce a new data sharing approach that is
based on an adversarially learned private repre-

sentation and improves on the pseudonymization
from Section 4. After training the representation
on an initial publicly available dataset, e.g. the
i2b2 2014 data, a central model provider shares
the frozen representation model with participating
medical institutions. They transform their PHI-
labeled raw data into the pseudonymized repre-
sentation, which is then pooled into a new pub-
lic dataset for de-identification. Periodically, the
pipeline consisting of the representation model
and a trained de-identification model can be pub-
lished to be used by medical institutions on their
unlabeled data.

Since both the representation model and the re-
sulting representations are shared in this scenario,
our representation procedure is required to prevent
two attacks:

A1. Learning an inverse representation model that
transforms representations back to original
sentences containing PHI.

A2. Building a lookup table of inputs and their ex-
act representations that can be used in known
plaintext attacks.

5.1 Architecture

Our approach uses a model that is composed of
three components: a representation model, the de-
identification model from Section 3, and an adver-
sary. An overview of the architecture is shown in
Fig. 2.

The representation model maps a sequence of
word embeddings to an intermediate vector rep-
resentation sequence. The de-identification model
receives this representation sequence as an input
instead of the original embedding sequence. It re-
tains the casing feature as an auxiliary input. The
adversary has two inputs, the representation se-
quence and an additional embedding or represen-
tation sequence, and a single output unit.

5.2 Representation

To protect against A1, our representation must be
invariant to small input changes, like a single PHI
token being replaced with a neighbor in the em-
bedding space. Again, the number of neighborsN
controls the privacy level of the representation.

To protect against A2, we add a random element
to the representation that makes repeated trans-
formations of one sentence indistinguishable from
representations of similar input sentences.



5833

We use a bidirectional LSTM model to imple-
ment the representation. It applies Gaussian noise
N with zero mean and trainable standard devia-
tions to the input embeddings E and the output
sequence. The model learns a standard deviation
for each of the input and output dimensions.

R = Nout + LSTM(E +Nin) (1)

In a preliminary experiment, we confirmed
that adding noise with a single, fixed standard
deviation is not a viable approach for privacy-
preserving representations. To change the cosine
similarity neighborhoods of embeddings at all, we
need to add high amounts of noise (more than
double of the respective embedding matrix’s stan-
dard deviation), which in turn results in unreal-
istic embeddings that do not allow training a de-
identification model of sufficient quality.

In contrast to the automatic pseudonymization
approach from Section 4 that only perturbs PHI
tokens, the representation models in this approach
processes all tokens to represent them in a new
embedding space. We evaluate the representation
sizes d ∈ {50, 100, 300}.

5.3 Adversaries

We use two adversaries that are trained on tasks
that directly follow from A1 and A2:

T1. Given a representation sequence and an em-
bedding sequence, decide if they were ob-
tained from the same sentence.

T2. Given two representation sequences (and
their cosine similarities), decide if they were
obtained from the same sentence.

We generate the representation sequences for the
second adversary from a copy of the representa-
tion model with shared weights. We generate real
and fake pairs for adversarial training using the au-
tomatic pseudonymization approach presented in
Section 4, limiting the number of replaced PHI to-
kens to one per sentence.

The adversaries are implemented as bidirec-
tional LSTM models with single output units. We
confirmed that this type of model is able to learn
the adversarial tasks on random data and raw word
embeddings in preliminary experiments. To use
the two adversaries in our architecture, we aver-
age their outputs.

R

AD

1.

R

AD

2.

R

AD

3. a)

R

AD

3. b)

Figure 3: Visualization of Feutry et al.’s three-part
training procedure. The adversarial model layout fol-
lows Fig. 2: the representation model is at the bottom,
the left branch is the de-identification model and the
right branch is the adversary. In each step, the thick
components are trained while the thin components are
frozen.

5.4 Training

We evaluate two training procedures: DANN
training (Ganin et al., 2016) and the three-part pro-
cedure from Feutry et al. (2018).

In DANN training, the three components are
trained conjointly, optimizing the sum of losses.
Training the de-identification model modifies the
representation model weights to generate a more
meaningful representation for de-identification.
The adversary gradient is reversed with a gradi-
ent reversal layer between the adversary and the
representation model in the backward pass, caus-
ing the representation to become less meaningful
for the adversary.

The training procedure by Feutry et al. (2018)
is shown in Fig. 3. It is composed of three phases:

P1. The de-identification and representation
models are pre-trained together, optimizing
the de-identification loss ldeid.

P2. The representation model is frozen and the
adversary is pre-trained, optimizing the ad-
versarial loss ladv.

P3. In alternation, for one epoch each:

(a) The representation is frozen and both
de-identification model and adversary
are trained, optimizing their respective
losses ldeid and ladv.

(b) The de-identification model and adver-
sary are frozen and the representation is
trained, optimizing the combined loss

lrepr = ldeid + λ|ladv − lrandom| (2)

In each of the first two phases, the respective val-
idation loss is monitored to decide at which point



5834

the training should move on to the next phase.
The alternating steps in the third phase each last
one training epoch; the early stopping time for the
third phase is determined using only the combined
validation loss from Phase P3b.

Gradient reversal is achieved by optimizing the
combined representation loss while the adversary
weights are frozen. The combined loss is moti-
vated by the fact that the adversary performance
should be the same as a random guessing model,
which is a lower bound for anonymization (Feutry
et al., 2018). The term |ladv− lrandom| approaches 0
when the adversary performance approaches ran-
dom guessing2. λ is a weighting factor for the two
losses; we select λ = 1.

6 Experiments

To evaluate our approaches, we perform experi-
ments using the i2b2 2014 dataset.

Preprocessing: We apply aggressive tokeniza-
tion similarly to Liu et al. (2017), including split-
ting at all punctuation marks and mid-word e.g. if
a number is followed by a word (“25yo” is split
into “25”, “yo”) in order to minimize the amount
of GloVe out-of-vocabulary tokens. We extend
spaCy’s3 sentence splitting heuristics with addi-
tional rules for splitting at multiple blank lines as
well as bulleted and numbered list items.

Deep Learning Models: We use the Keras
framework4 (Chollet et al., 2015) with the Tensor-
Flow backend (Abadi et al., 2015) to implement
our deep learning models.

Evaluation: In order to compare our results to
the state of the art, we use the token-based bi-
nary HIPAA F1 score as our main metric for de-
identification performance. Dernoncourt et al.
(2017) deem it the most important metric: decid-
ing if an entity is PHI or not is generally more im-
portant than assigning the correct category of PHI,
and only HIPAA categories of PHI are required to
be removed by American law. Non-PHI tokens are
not incorporated in the F1 score. We perform the
evaluation with the official i2b2 evaluation script5.

2In the case of binary classification: Lrandom = − log 12 .
3https://spacy.io
4https://keras.io
5https://github.com/kotfic/i2b2_

evaluation_scripts

Model F1 (%)

Our non-private FastText 97.67
Our non-private GloVe 97.24
Our non-private GloVe + casing 97.62

Dernoncourt et al. (LSTM-CRF) 97.85
Liu et al. (ensemble + rules) 98.27

Our autom. pseudon. FastText 96.75
Our autom. pseudon. GloVe 96.42

Our adv. repr. FastText 97.40
Our adv. repr. GloVe 96.89

Table 2: Binary HIPAA F1 scores of our non-private
(top) and private (bottom) de-identification approaches
on the i2b2 2014 test set in comparison to non-private
the state of the art. Our private approaches use N =
100 neighbors as a privacy criterion.

7 Results

Table 2 shows de-identification performance re-
sults for the non-private de-identification classifier
(upper part, in comparison to the state of the art) as
well as the two privacy-enabled extensions (lower
part). The results are average values out of five
experiment runs.

7.1 Non-private De-Identification Model

When trained on the raw i2b2 2014 data, our mod-
els achieve F1 scores that are comparable to Der-
noncourt et al.’s results. The casing feature im-
proves GloVe by 0.4 percentage points.

7.2 Automatic Pseudonymization

For both FastText and GloVe, moving training PHI
tokens to random tokens from up to theirN = 200
closest neighbors does not significantly reduce de-
identification performance (see Fig. 4). F1 scores
for both models drop to around 95% when se-
lecting from N = 500 neighbors and to around
90% when using N = 1000 neighbors. With
N = 100, the FastText model achieves anF1 score
of 96.75% and the GloVe model achieves an F1
score of 96.42%.

7.3 Adversarial Representation

We do not achieve satisfactory results with the
conjoint DANN training procedure: in all cases,
our models learn representations that are not suf-
ficiently resistant to the adversary. When training
the adversary on the frozen representation for an

https://spacy.io
https://keras.io
https://github.com/kotfic/i2b2_evaluation_scripts
https://github.com/kotfic/i2b2_evaluation_scripts


5835

100 101 102 103

Number of neighbors N

0.900

0.925

0.950

0.975

1.000
B

in
ar

y
H

IP
A

A
F1

sc
or

e

De-identification performance

FastText
GloVe

Figure 4: F1 scores of our models when trained on
automatically pseudonymized data where PHI tokens
are moved to one of different numbers of neighbors N .
The gray dashed line marks the 95% target F1 score.

additional 20 epochs, it is able to distinguish real
from fake input pairs on a test set with accura-
cies above 80%. This confirms the difficulties of
DANN training as described by Elazar and Gold-
berg (2018) (see Section 2.2).

In contrast, with the three-part training proce-
dure, we are able to learn a representation that al-
lows training a de-identification model while pre-
venting an adversary from learning the adversar-
ial tasks, even with continued training on a frozen
representation.

Figure 5 (left) shows our de-identification re-
sults when using adversarially learned representa-
tions. A higher number of neighbors N means a
stronger invariance requirement for the representa-
tion. For values ofN up to 1 000, our FastText and
GloVe models are able to learn representations that
allow training de-identification models that reach
or exceed the target F1 score of 95%. However,
training becomes unstable for N > 500: at this
point, the adversary is able to break the represen-
tation privacy when trained for an additional 50
epochs (Fig. 5 right).

Our choice of representation size d ∈
{50, 100, 300} does not influence de-identification
or adversary performance, so we select d = 50 for
further evaluation. For d = 50 and N = 100, the
FastText model reaches an F1 score of 97.4% and
the GloVe model reaches an F1 score of 96.89%.

8 De-Identification Performance

In the following, we discuss the results of our
models with regard to our goal of sharing sensi-
tive training data for automatic de-identification.
Overall, privacy-preserving representations come
at a cost, as our best privacy-preserving model

scores 0.27 points F1 score lower than our best
non-private model; we consider this relative in-
crease of errors of less than 10% as tolerable.

Raw Text De-Identification: We find that the
choice of GloVe or FastText embeddings does
not meaningfully influence de-identification per-
formance. FastText’s approach to embedding
unknown words (word embeddings are the sum
of their subword embeddings) should intuitively
prove useful on datasets with misspellings and un-
grammatical text. However, when using the ad-
ditional casing feature, FastText beats GloVe only
by 0.05 percentage points on the i2b2 test set. In
this task, the casing feature makes up for GloVe’s
inability to embed unknown words.

Liu et al. (2017) use a deep learning ensemble
in combination with hand-crafted rules to achieve
state-of-the-art results for de-identification. Our
model’s scores are similar to the previous state
of the art, a bidirectional LSTM-CRF model with
character features (Dernoncourt et al., 2017).

Automatically Pseudonymized Data: Our
naı̈ve automatic word-level pseudonymization
approach allows training reasonable de-iden-
tification models when selecting from up to
N = 500 neighbors. There is almost no decrease
in F1 score for up to N = 20 neighbors for both
the FastText and GloVe model.

Adversarially Learned Representation: Our
adversarially trained vector representation allows
training reasonable de-identification models (F1
scores above 95%) when using up to N = 1000
neighbors as an invariance requirement. The ad-
versarial representation results beat the automatic
pseudonymization results because the representa-
tion model can act as a task-specific feature ex-
tractor. Additionally, the representations are more
general as they are invariant to word changes.

9 Privacy Properties

In this section, we discuss our models with respect
to their privacy-preserving properties.

Embeddings: When looking up embedding
space neighbors for words, it is notable that many
FastText neighbors include the original word or
parts of it as a subword. For tokens that occur
as PHI in the i2b2 training set, on average 7.37
of their N = 100 closest neighbors in the Fast-
Text embedding matrix contain the original token



5836

101 102 103

Number of neighbors N

0.96

0.98

1.00

B
in

ar
y

H
IP

A
A

F1
sc

or
e

De-identification performance

101 102 103

Number of neighbors N

0.50

0.75

1.00

A
cc

ur
ac

y

Adversary accuracy

FastText
GloVe

Figure 5: Left: de-identification F1 scores of our models using an adversarially trained representation with different
numbers of neighbors N for the representation invariance requirement. Right: mean adversary accuracy when
trained on the frozen representation for an additional 50 epochs. The figure shows average results out of five
experiment runs.

as a subword. When looking up neighbors using
GloVe embeddings, the value is 0.44. This may
indicate that FastText requires stronger perturba-
tion (i.e. higher N ) than GloVe to sufficiently ob-
fuscate protected information.

Automatically Pseudonymized Data: The
word-level pseudonymization does not guarantee
a minimum perturbation for every word, e.g. in a
set of pseudonymized sentences using N = 100
FastText neighbors, we found the phrase

[Florida Hospital],

which was replaced with

[Miami-Florida Hosp].

Additionally, the approach may allow an adver-
sary to piece together documents from the shuf-
fled sentences. If multiple sentences contain sim-
ilar pseudonymized identifiers, they will likely
come from the same original document, undoing
the privacy gain from shuffling training sentences
across documents. It may be possible to infer the
original information using the overlapping neigh-
bor spaces. To counter this, we can re-introduce
document-level pseudonymization, i.e. moving all
occurrences of a PHI token to the same neighbor.
However, we would then also need to detect mis-
spelled names as well as other hints to the actual
tokens and transform them similarly to the orig-
inal, which would add back much of the com-
plexity of manual pseudonymization that we try
to avoid.

Adversarially Learned Representation: Our
adversarial representation empirically satisfies a
strong privacy criterion: representations are in-
variant to any protected information token being

replaced with any of its N neighbors in an em-
bedding space. When freezing the representation
model from an experiment run using up to N =
500 neighbors and training the adversary for an ad-
ditional 50 epochs, it still does not achieve higher-
than-chance accuracies on the training data. Due
to the additive noise, the adversary does not over-
fit on its training set but rather fails to identify any
structure in the data.

In the case of N = 1000 neighbors, the repre-
sentation never becomes stable in the alternating
training phase. The adversary is always able to
break the representation privacy.

10 Conclusions & Future Work

We introduced a new approach to sharing train-
ing data for de-identification that requires lower
human effort than the existing approach of
document-coherent pseudonymization. Our ap-
proach is based on adversarial learning, which
yields representations that can be distributed since
they do not contain private health information.
The setup is motivated by the need of de-
identification of medical text before sharing; our
approach provides a lower-cost alternative than
manual pseudonymization and gives rise to the
pooling of de-identification datasets from hetero-
geneous sources in order to train more robust clas-
sifiers. Our implementation and experimental data
are publicly available6.

As precursors to our adversarial representation
approach, we developed a deep learning model
for de-identification that does not rely on ex-
plicit character features as well as an automatic

6https://github.com/maxfriedrich/
deid-training-data

https://github.com/maxfriedrich/deid-training-data
https://github.com/maxfriedrich/deid-training-data


5837

word-level pseudonymization approach. A model
trained on our automatically pseudonymized data
with N = 100 neighbors loses around one per-
centage point in F1 score when compared to the
non-private system, scoring 96.75% on the i2b2
2014 test set.

Further, we presented an adversarial learning
based private representation of medical text that
is invariant to any PHI word being replaced with
any of its embedding space neighbors and con-
tains a random element. The representation allows
training a de-identification model while being ro-
bust to adversaries trying to re-identify protected
information or building a lookup table of repre-
sentations. We extended existing adversarial rep-
resentation learning approaches by using two ad-
versaries that discriminate real from fake sequence
pairs with an additional sequence input.

The representation acts as a task-specific fea-
ture extractor. For an invariance criterion of up to
N = 500 neighbors, training is stable and adver-
saries cannot beat the random guessing accuracy
of 50%. Using the adversarially learned represen-
tation, de-identification models reach an F1 score
of 97.4%, which is close to the non-private system
(97.67%). In contrast, the automatic pseudonymi-
zation approach only reaches an F1 score of 95.0%
at N = 500.

Our adversarial representation approach en-
ables cost-effective private sharing of training data
for sequence labeling. Pooling of training data for
de-identification from multiple institutions would
lead to much more robust classifiers. Eventually,
improved de-identification classifiers could help
enable large-scale medical studies that eventually
improve public health.

Future Work: The automatic pseudonymiza-
tion approach could serve as a data augmenta-
tion scheme to be used as a regularizer for de-
identification models. Training a model on a com-
bination of raw and pseudonymized data may re-
sult in better test scores on the i2b2 test set, possi-
bly improving the state of the art.

Private character embeddings that are learned
from a perturbed source could be an interesting ex-
tension to our models.

In adversarial learning with the three-part train-
ing procedure, it might be possible to tune the λ
parameter and define a better stopping condition
that avoids the unstable characteristics with high
values for N in the invariance criterion. A fur-

ther possible extension is a dynamic noise level
in the representation model that depends on the
LSTM output instead of being a trained weight.
This might allow using lower amounts of noise for
certain inputs while still being robust to the adver-
sary.

When more training data from multiple sources
become available in the future, it will be possible
to evaluate our adversarially learned representa-
tion against unseen data.

Acknowledgments

This work was partially supported by BWFG
Hamburg within the “Forum 4.0” project as part
of the ahoi.digital funding line.

De-identified clinical records used in this re-
search were provided by the i2b2 National
Center for Biomedical Computing funded by
U54LM008748 and were originally prepared for
the Shared Tasks for Challenges in NLP for Clin-
ical Data organized by Özlem Uzuner, i2b2 and
SUNY.

References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene

Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Mané, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Vasude-
van, Fernanda Viégas, Oriol Vinyals, Pete Warden,
Martin Wattenberg, Martin Wicke, Yuan Yu, and Xi-
aoqiang Zheng. 2015. TensorFlow: Large-scale ma-
chine learning on heterogeneous systems. Accessed
May 31, 2019.

Guthrie S. Birkhead, Michael Klompas, and Nirav R.
Shah. 2015. Uses of electronic health records for
public health surveillance to advance public health.
Annual Review of Public Health, 36(1):345–359.
PMID: 25581157.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Taxiarchis Botsis, Gunnar Hartvigsen, Fei Chen, and
Chunhua Weng. 2010. Secondary use of EHR:
Data quality issues and informatics opportunities.
AMIA Summits on Translational Science Proceed-
ings, 2010:1–5.

https://www.tensorflow.org/
https://www.tensorflow.org/
https://doi.org/10.1146/annurev-publhealth-031914-122747
https://doi.org/10.1146/annurev-publhealth-031914-122747
https://transacl.org/ojs/index.php/tacl/article/view/999
https://transacl.org/ojs/index.php/tacl/article/view/999
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041534/
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041534/


5838

François Chollet et al. 2015. Keras. Accessed May 31,
2019.

Martin R. Cowie, Juuso I. Blomster, Lesley H. Cur-
tis, Sylvie Duclaux, Ian Ford, Fleur Fritz, Saman-
tha Goldman, Salim Janmohamed, Jörg Kreuzer,
Mark Leenay, Alexander Michel, Seleen Ong, Jill P.
Pell, Mary Ross Southworth, Wendy Gattis Stough,
Martin Thoenes, Faiez Zannad, and Andrew Za-
lewski. 2017. Electronic health records to facilitate
clinical research. Clinical Research in Cardiology,
106(1):1–9.

Franck Dernoncourt, Ji Young Lee, Özlem Uzuner,
and Peter Szolovits. 2017. De-identification of pa-
tient notes with recurrent neural networks. Journal
of the American Medical Informatics Association,
24(3):596–606.

Yanai Elazar and Yoav Goldberg. 2018. Adversarial
removal of demographic attributes from text data.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
11–21, Brussels, Belgium. Association for Compu-
tational Linguistics.

Clément Feutry, Pablo Piantanida, Yoshua Bengio, and
Pierre Duhamel. 2018. Learning anonymized repre-
sentations with adversarial neural networks. arXiv
preprint arXiv:1802.09386.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, François Lavi-
olette, Mario Marchand, and Victor Lempitsky.
2016. Domain-adversarial training of neural net-
works. Journal of Machine Learning Research,
17(1):2096–2030.

Jihun Hamm. 2015. Preserving privacy of continuous
high-dimensional data with minimax filters. In Pro-
ceedings of the Eighteenth International Conference
on Artificial Intelligence and Statistics, volume 38 of
Proceedings of Machine Learning Research, pages
324–332, San Diego, CA, USA. PMLR.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-
rectional LSTM-CRF models for sequence tagging.
arXiv preprint arXiv:1508.01991.

Peter B. Jensen, Lars J. Jensen, and Søren Brunak.
2012. Mining electronic health records: towards
better research applications and clinical care. Na-
ture Reviews Genetics, 13:395–405.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 260–270, San Diego, CA, USA. Association
for Computational Linguistics.

Ji Young Lee, Franck Dernoncourt, and Peter
Szolovits. 2018. Transfer Learning for Named-
Entity Recognition with Neural Networks. In
Proceedings of the Eleventh International Confer-
ence on Language Resources and Evaluation, pages
4470–4473, Miyazaki, Japan. European Language
Resources Association.

Yitong Li, Timothy Baldwin, and Trevor Cohn. 2018.
Towards robust and privacy-preserving text repre-
sentations. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 25–30, Melbourne,
Australia. Association for Computational Linguis-
tics.

Zengjian Liu, Buzhou Tang, Xiaolong Wang, and
Qingcai Chen. 2017. De-identification of clinical
notes via recurrent neural network and conditional
random field. Journal of Biomedical Informatics,
75(S):S34–S42.

Stephane M. Meystre, F. Jeffrey Friedlin, Brett R.
South, Shuying Shen, and Matthew H. Samore.
2010. Automatic de-identification of textual docu-
ments in the electronic health record: a review of
recent research. BMC Medical Research Methodol-
ogy, 10(1):70.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1532–1543, Doha, Qatar. Associ-
ation for Computational Linguistics.

Nils Reimers and Iryna Gurevych. 2017. Op-
timal hyperparameters for deep LSTM-networks
for sequence labeling tasks. arXiv preprint
arXiv:1707.06799.

Amber Stubbs, Michele Filannino, and Özlem Uzuner.
2017. De-identification of psychiatric intake
records: Overview of 2016 CEGS N-GRID shared
tasks track 1. Journal of Biomedical Informatics,
75(S):S4–S18.

Amber Stubbs, Christopher Kotfila, and Özlem Uzuner.
2015. Automated systems for the de-identification
of longitudinal clinical narratives: Overview of 2014
i2b2/UTHealth shared task track 1. Journal of
Biomedical Informatics, 58:11–19.

Amber Stubbs and Özlem Uzuner. 2015. Annotating
longitudinal clinical narratives for de-identification:
The 2014 i2b2/UTHealth corpus. Journal of
Biomedical Informatics, 58:20–29.

The United States Congress. 1996. Health insurance
portability and accountability act of 1996. Accessed
May 31, 2019.

Özlem Uzuner, Yuan Luo, and Peter Szolovits. 2007.
Evaluating the state-of-the-art in automatic de-
identification. Journal of the American Medical In-
formatics Association, 14(5):550–563.

https://keras.io
https://doi.org/10.1007/s00392-016-1025-6
https://doi.org/10.1007/s00392-016-1025-6
https://doi.org/10.1093/jamia/ocw156
https://doi.org/10.1093/jamia/ocw156
https://aclweb.org/anthology/D18-1002
https://aclweb.org/anthology/D18-1002
https://arxiv.org/abs/1802.09386
https://arxiv.org/abs/1802.09386
http://www.jmlr.org/papers/volume17/15-189/15-189.pdf
http://www.jmlr.org/papers/volume17/15-189/15-189.pdf
http://proceedings.mlr.press/v38/hamm15.html
http://proceedings.mlr.press/v38/hamm15.html
https://doi.org/10.1162/neco.1997.9.8.1735
https://arxiv.org/abs/1508.01991
https://arxiv.org/abs/1508.01991
https://doi.org/10.1038/nrg3208
https://doi.org/10.1038/nrg3208
https://aclweb.org/anthology/N16-1030
http://www.lrec-conf.org/proceedings/lrec2018/pdf/878.pdf
http://www.lrec-conf.org/proceedings/lrec2018/pdf/878.pdf
https://aclweb.org/anthology/P18-2005
https://aclweb.org/anthology/P18-2005
https://doi.org/10.1016/j.jbi.2017.05.023
https://doi.org/10.1016/j.jbi.2017.05.023
https://doi.org/10.1016/j.jbi.2017.05.023
https://doi.org/10.1186/1471-2288-10-70
https://doi.org/10.1186/1471-2288-10-70
https://doi.org/10.1186/1471-2288-10-70
https://aclweb.org/anthology/D14-1162
https://aclweb.org/anthology/D14-1162
https://arxiv.org/abs/1707.06799
https://arxiv.org/abs/1707.06799
https://arxiv.org/abs/1707.06799
https://doi.org/10.1016/j.jbi.2017.06.011
https://doi.org/10.1016/j.jbi.2017.06.011
https://doi.org/10.1016/j.jbi.2017.06.011
https://doi.org/10.1016/j.jbi.2015.07.020
https://doi.org/10.1016/j.jbi.2015.07.020
https://doi.org/10.1016/j.jbi.2015.07.020
https://doi.org/10.1016/j.jbi.2015.07.020
https://doi.org/10.1016/j.jbi.2015.07.020
https://doi.org/10.1016/j.jbi.2015.07.020
https://legislink.org/us/pl-104-191
https://legislink.org/us/pl-104-191
https://www.ncbi.nlm.nih.gov/pubmed/17600094
https://www.ncbi.nlm.nih.gov/pubmed/17600094


5839

Seid Muhie Yimam. 2015. Narrowing the loop: Inte-
gration of resources and linguistic dataset develop-
ment with interactive machine learning. In Proceed-
ings of the 2015 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Student Research Workshop, pages 88–95,
Denver, CO, USA. Association for Computational
Linguistics.

Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and
Cynthia Dwork. 2013. Learning fair representa-
tions. In Proceedings of the 30th International Con-
ference on Machine Learning, volume 28 of Pro-
ceedings of Machine Learning Research, pages 325–
333, Atlanta, GA, USA. PMLR.

https://doi.org/10.3115/v1/N15-2012
https://doi.org/10.3115/v1/N15-2012
https://doi.org/10.3115/v1/N15-2012
http://proceedings.mlr.press/v28/zemel13.html
http://proceedings.mlr.press/v28/zemel13.html

