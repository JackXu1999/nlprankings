



















































Non-projective Dependency-based Pre-Reordering with Recurrent Neural Network for Machine Translation


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 846–856,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Non-projective Dependency-based Pre-Reordering with Recurrent
Neural Network for Machine Translation

Antonio Valerio Miceli-Barone
Università di Pisa

Largo B. Pontecorvo, 3
56127 Pisa, Italy

miceli@di.unipi.it

Giuseppe Attardi
Università di Pisa

Largo B. Pontecorvo, 3
56127 Pisa, Italy

attardi@di.unipi.it

Abstract

The quality of statistical machine
translation performed with phrase
based approaches can be increased by
permuting the words in the source
sentences in an order which resem-
bles that of the target language. We
propose a class of recurrent neu-
ral models which exploit source-side
dependency syntax features to re-
order the words into a target-like or-
der. We evaluate these models on
the German-to-English and Italian-to-
English language pairs, showing sig-
nificant improvements over a phrase-
based Moses baseline. We also com-
pare with state of the art German-to-
English pre-reordering rules, showing
that our method obtains similar or bet-
ter results.

1 Introduction

Statistical machine translation is typically
performed using phrase-based systems
(Koehn et al., 2007). These systems can
usually produce accurate local reordering
but they have difficulties dealing with the
long-distance reordering that tends to occur
between certain language pairs (Birch et al.,
2008).

The quality of phrase-based machine trans-
lation can be improved by reordering the
words in each sentence of source-side of the
parallel training corpus in a ”target-like” or-
der and then applying the same transforma-
tion as a pre-processing step to input strings
during execution.

When the source-side sentences can be ac-
curately parsed, pre-reordering can be per-
formed using hand-coded rules. This ap-
proach has been successfully applied to
German-to-English (Collins et al., 2005) and
other languages. The main issue with it is that
these rules must be designed for each spe-
cific language pair, which requires consider-
able linguistic expertise.

Fully statistical approaches, on the other
hand, learn the reordering relation from word
alignments. Some of them learn reordering
rules on the constituency (Dyer and Resnik,
2010) (Khalilov and Fonollosa, 2011) or pro-
jective dependency (Genzel, 2010), (Lerner
and Petrov, 2013) parse trees of source sen-
tences. The permutations that these meth-
ods can learn can be generally non-local
(i.e. high distance) on the sentences but lo-
cal (parent-child or sibling-sibling swaps) on
the parse trees. Moreover, constituency or
projective dependency trees may not be the
ideal way of representing the syntax of non-
analytic languages such as German or Ital-
ian, which could be better described using
non-projective dependency trees (Bosco and
Lombardo, 2004). Other methods, based on
recasting reordering as a combinatorial opti-
mization problem (Tromble and Eisner, 2009),
(Visweswariah et al., 2011), can learn to gen-
erate in principle arbitrary permutations, but
they can only make use of minimal syntactic
information (part-of-speech tags) and there-
fore can’t exploit the potentially valuable
structural syntactic information provided by
a parser.

In this work we propose a class of reorder-
ing models which attempt to close this gap by

846



exploiting rich dependency syntax features
and at the same time being able to process
non-projective dependency parse trees and
generate permutations which may be non-
local both on the sentences and on the parse
trees.
We represent these problems as sequence pre-
diction machine learning tasks, which we ad-
dress using recurrent neural networks.

We applied our model to reorder German
sentences into an English-like word order as
a pre-processing step for phrase-based ma-
chine translation, obtaining significant im-
provements over the unreordered baseline
system and quality comparable to the hand-
coded rules introduced by Collins et al.
(2005). We also applied our model to Italian-
to-English pre-reordering, obtaining a con-
siderable improvement over the unreordered
baseline.

2 Reordering as a walk on a
dependency tree

In order to describe the non-local reordering
phenomena that can occur between language
pairs such as German-to-English and Italian-
to-English, we introduce a reordering frame-
work similar to (Miceli Barone and Attardi,
2013), based on a graph walk of the depen-
dency parse tree of the source sentence. This
framework doesn’t restrict the parse tree to be
projective, and allows the generation of arbi-
trary permutations.

Let f ≡ ( f1, f2, . . . , fL f ) be a source sen-
tence, annotated by a rooted dependency
parse tree: ∀j ∈ 1, . . . , L f , hj ≡ PARENT(j)

We define a walker process that walks from
word to word across the edges of the parse
tree, and at each steps optionally emits the
current word, with the constraint that each
word must be eventually emitted exactly
once.
Therefore, the final string of emitted words f ′
is a permutation of the original sentence f ,
and any permutation can be generated by a
suitable walk on the parse tree.

2.1 Reordering automaton

We formalize the walker process as a non-
deterministic finite-state automaton.
The state v of the automaton is the tuple v ≡

(j, E, a) where j ∈ 1, . . . , L f is the current ver-
tex (word index), E is the set of emitted ver-
tices, a is the last action taken by the automa-
ton.
The initial state is: v(0) ≡ (root f , {}, null)
where root f is the root vertex of the parse tree.

At each step t, the automaton chooses one
of the following actions:

• EMIT: emit the word f j at the current
vertex j. This action is enabled only if the
current vertex has not been already emit-
ted:

j /∈ E
(j, E, a) EMIT→ (j, E ∪ {j}, EMIT)

(1)

• UP: move to the parent of the current
vertex. Enabled if there is a parent and
we did not just come down from it:

hj 6= null, a 6= DOWNj
(j, E, a) UP→ (hj, E, UPj)

(2)

• DOWNj′ : move to the child j′ of the cur-
rent vertex. Enabled if the subtree s(j′)
rooted at j′ contains vertices that have
not been already emitted and if we did
not just come up from it:

hj′ = j, a 6= UPj′ , ∃k ∈ s(j′) : k /∈ E
(j, E, a)

DOWNj′→ (j′, E, DOWNj′)
(3)

The execution continues until all the vertices
have been emitted.

We define the sequence of states of the
walker automaton during one run as an execu-
tion v̄ ∈ GEN( f ). An execution also uniquely
specifies the sequence of actions performed
by the automation.

The preconditions make sure that all execu-
tion of the automaton always end generating
a permutation of the source sentence. Fur-
thermore, no cycles are possible: progress is
made at every step, and it is not possible to
enter in an execution that later turns out to be
invalid.
Every permutation of the source sentence can
be generated by some execution. In fact, each
permutation f ′ can be generated by exactly
one execution, which we denote as v̄( f ′).

847



We can split the execution v̄( f ′) into a se-
quence of L f emission fragments v̄j( f ′), each
ending with an EMIT action.
The first fragment has zero or more DOWN∗
actions followed by one EMIT action, while
each other fragment has a non-empty se-
quence of UP and DOWN∗ actions (always
zero or more UPs followed by zero or more
DOWNs) followed by one EMIT action.

Finally, we define an action in an execution
as forced if it was the only action enabled at
the step where it occurred.

2.2 Application

Suppose we perform reordering using a
typical syntax-based system which pro-
cesses source-side projective dependency
parse trees and is limited to swaps between
pair of vertices which are either in a parent-
child relation or in a sibling relation. In such
execution the UP actions are always forced,
since the ”walker” process never leaves a sub-
tree before all its vertices have been emitted.

Suppose instead that we could perform re-
ordering according to an ”oracle”. The ex-
ecutions of our automaton corresponding to
these permutations will in general contain
unforced UP actions. We define these ac-
tions, and the execution fragments that ex-
hibit them, as non-tree-local.

In practice we don’t have access to a
reordering ”oracle”, but for sentences pairs
in a parallel corpus we can compute heuristic
”pseudo-oracle” reference permutations of
the source sentences from word-alignments.

Following (Al-Onaizan and Pap-
ineni, 2006), (Tromble and Eisner, 2009),
(Visweswariah et al., 2011), (Navratil et al.,
2012), we generate word alignments in both
the source-to-target and the target-to-source
directions using IBM model 4 as imple-
mented in GIZA++ (Och et al., 1999) and then
we combine them into a symmetrical word
alignment using the ”grow-diag-final-and”
heuristic implemented in Moses (Koehn et
al., 2007).

Given the symmetric word-aligned corpus,
we assign to each source-side word an in-
teger index corresponding to the position of
the leftmost target-side word it is aligned to
(attaching unaligned words to the following

aligned word) and finally we perform a sta-
ble sort of source-side words according to this
index.

2.3 Reordering example

Consider the segment of a German sentence
shown in fig. 1. The English-reordered
segment ”die Währungsreserven anfangs
lediglich dienen sollten zur Verteidigung”
corresponds to the English: ”the reserve as-
sets were originally intended to provide
protection”.

In order to compose this segment from the
original German, the reordering automaton
described in our framework must perform a
complex sequence of moves on the parse tree:

• Starting from ”sollten”, de-
scend to ”dienen”, descent to
”Währungsreserven” and finally
to ”die”. Emit it, then go up to
”Währungsreserven”, emit it and
go up to ”dienen” and up again to
”sollten”. Note that the last UP is
unforced since ”dienen” has not been
emitted at that point and has also un-
emitted children. This unforced action
indicates non-tree-local reordering.

• Go down to ”anfangs”. Note that the
in the parse tree edge crosses another
edge, indicating non-projectivity. Emit
”anfangs” and go up (forced) back to
”sollten”.

• Go down to ”dienen”, down to ”zur”,
down to ”lediglich” and emit it. Go
up (forced) to ”zur”, up (unforced) to
”dienen”, emit it, go up (unforced) to
”sollten”, emit it. Go down to ”dienen”,
down to ”zur” emit it, go down to
”Verteidigung” and emit it.

Correct reordering of this segment would
be difficult both for a phrase-based system
(since the words are further apart than both
the typical maximum distortion distance and
maximum phrase length) and for a syntax-
based system (due to the presence of non-
projectivity and non-tree-locality).

848



Figure 1: Section of the dependency parse tree of a German sentence.

3 Recurrent Neural Network
reordering models

Given the reordering framework described
above, we could try to directly predict the ex-
ecutions as Miceli Barone and Attardi (2013)
attempted with their version of the frame-
work. However, the executions of a given
sentence can have widely different lengths,
which could make incremental inexact decod-
ing such as beam search difficult due to the
need to prune over partial hypotheses that
have different numbers of emitted words.

Therefore, we decided to investigate a dif-
ferent class of models which have the prop-
erty that state transition happen only in corre-
spondence with word emission. This enables
us to leverage the technology of incremental
language models.

Using language models for reordering is
not something new (Feng et al., 2010), (Dur-
rani et al., 2011), (Bisazza and Federico, 2013),
but instead of using a more or less standard
n-gram language model, we are going to base
our model on recurrent neural network language
models (Mikolov et al., 2010).

Neural networks allow easy incorpora-
tion of multiple types of features and can
be trained more specifically on the types
of sequences that will occur during decod-
ing, hence they can avoid wasting model
space to represent the probabilities of non-
permutations.

3.1 Base RNN-RM

Let f ≡ ( f1, f2, . . . , fL f ) be a source sentence.
We model the reordering system as a deter-
ministic single hidden layer recurrent neural
network:

v(t) = τ(Θ(1) · x(t) + ΘREC · v(t− 1)) (4)

where x(t) ∈ Rn is a feature vector associated
to the t-th word in a permutation f ′, v(0) ≡
vinit, Θ(1) and ΘREC are parameters1 and τ(·)
is the hyperbolic tangent function.

If we know the first t− 1 words of the per-
mutation f ′ in order to compute the proba-
bility distribution of the t-th word we do the
following:

• Iteratively compute the state v(t − 1)
from the feature vectors x(1), . . . , x(t −
1).

• For the all the indices of the words that
haven’t occurred in the permutation so
far j ∈ J(t) ≡ ([1, L f ] − īt−1:), compute
a score hj,t ≡ ho(v(t − 1), xo(j)), where
xo(·) is the feature vector of the candidate
target word.

• Normalize the scores using the logistic
softmax function: P( Īt = j| f , īt−1:, t) =

exp(hj,t)
∑j′∈J(t) exp(hj′ ,t)

.

The scoring function ho(v(t− 1), xo(j)) ap-
plies a feed-forward hidden layer to the fea-
ture inputs xo(j), and then takes a weighed
inner product between the activation of this
layer and the state v(t − 1). The result is
then linearly combined to an additional fea-
ture equal to the logarithm of the remaining
words in the permutation (L f − t),2 and to a
bias feature:

hj,t ≡< τ(Θ(o) · xo(j)), θ(2) � v(t− 1) >
+ θ(α) · log(L f − t) + θ(bias)

(5)

where hj,t ≡ ho(v(t− 1), xo(j)).
1we don’t use a bias feature since it is redundant

when the layer has input features encoded with the
”one-hot” encoding

2since we are then passing this score to a softmax of
variable size (L f − t), this feature helps the model to
keep the score already approximately scaled.

849



We can compute the probability of an entire
permutation f ′ just by multiplying the proba-
bilities for each word: P( f ′| f ) = P( Ī = ī| f ) =
∏

L f
t=1 P( Īt = īt| f , t)

3.1.1 Training

Given a training set of pairs of sentences and
reference permutations, the training problem
is defined as finding the set of parameters
θ ≡ (vinit, Θ(1), θ(2), ΘREC, Θ(o), θ(α), θ(bias))
which minimize the per-word empirical
cross-entropy of the model w.r.t. the reference
permutations in the training set. Gradients
can be efficiently computed using backpropa-
gation through time (BPTT).

In practice we used the following training
architecture:
Stochastic gradient descent, with each train-
ing pair ( f , f ′) considered as a single mini-
batch for updating purposes. Gradients com-
puted using the automatic differentiation fa-
cilities of Theano (Bergstra et al., 2010) (which
implements a generalized BPTT). No trun-
cation is used. L2-regularization 3. Learn-
ing rates dynamically adjusted per scalar pa-
rameter using the AdaDelta heuristic (Zeiler,
2012). Gradient clipping heuristic to prevent
the ”exploding gradient” problem (Graves,
2013). Early stopping w.r.t. a validation set to
prevent overfitting. Uniform random initial-
ization for parameters other than the recur-
rent parameter matrix ΘREC. Random initial-
ization with echo state property for ΘREC, with
contraction coefficient σ = 0.99 (Jaeger, 2001),
(Gallicchio and Micheli, 2011).

Training time complexity is O(L2f ) per sen-
tence, which could be reduced to O(L f ) using
truncated BTTP at the expense of update ac-
curacy and hence convergence speed. Space
complexity is O(L f ) per sentence.

3.1.2 Decoding

In order to use the RNN-RM model for pre-
reordering we need to compute the most

likely permutation
∗
f ′ of the source sentence

f :
∗
f ′ ≡ argmax

f ′∈GEN( f )
P( f ′| f ) (6)

3λ = 10−4 on the recurrent matrix, λ = 10−6 on the
final layer, per minibatch.

Solving this problem to the global optimum is
computationally hard4, hence we solve it to a
local optimum using a beam search strategy.

We generate the permutation incrementally
from left to right. Starting from an initial
state consisting of an empty string and the ini-
tial state vector vinit, at each step we generate
all possible successor states and retain the B-
most probable of them (histogram pruning),
according to the probability of the entire pre-
fix of permutation they represent.

Since RNN state vectors do not decompose
in a meaningful way, we don’t use any hy-
pothesis recombination.
At step t there are L f − t possible succes-
sor states, and the process always takes ex-
actly L f steps5, therefore time complexity is
O(B · L2f ) and space complexity is O(B).
3.1.3 Features
We use two different feature configurations:
unlexicalized and lexicalized.

In the unlexicalized configuration, the state
transition input feature function x(j) is com-
posed by the following features, all encoded
using the ”one-hot” encoding scheme:

• Unigram: POS(j), DEPREL(j), POS(j) ∗
DEPREL(j). Left, right and parent un-
igram: POS(k), DEPREL(k), POS(k) ∗
DEPREL(k), where k is the index of re-
spectively the word at the left (in the
original sentence), at the right and the
dependency parent of word j. Unique
tags are used for padding.

• Pair features: POS(j) ∗ POS(k), POS(j) ∗
DEPREL(k), DEPREL(j) ∗ POS(k),
DEPREL(j) ∗ DEPREL(k), for k defined
as above.

• Triple features POS(j) ∗ POS(le f tj) ∗
POS(rightj), POS(j) ∗ POS(le f tj) ∗
POS(parentj), POS(j) ∗ POS(rightj) ∗
POS(parentj).

• Bigram: POS(j) ∗ POS(k), POS(j) ∗
DEPREL(k), DEPREL(j) ∗ POS(k)
where k is the previous emitted word in
the permutation.

4NP-hard for at least certain choices of features and
parameters

5actually, L f − 1, since the last choice is forced

850



• Topological features: three binary fea-
tures which indicate whether word j
and the previously emitted word are in
a parent-child, child-parent or sibling-
sibling relation, respectively.

The target word feature function xo(j) is
the same as x(j) except that each feature is
also conjoined with a quantized signed dis-
tance6 between word j and the previous emit-
ted word. Feature value combinations that
appear less than 100 times in the training set
are replaced by a distinguished ”rare” tag.

The lexicalized configuration is equivalent
to the unlexicalized one except that x(j) and
xo(j) also have the surface form of word j (not
conjoined with the signed distance).

3.2 Fragment RNN-RM

The Base RNN-RM described in the previ-
ous section includes dependency informa-
tion, but not the full information of reorder-
ing fragments as defined by our automa-
ton model (sec. 2). In order to determine
whether this rich information is relevant to
machine translation pre-reordering, we pro-
pose an extension, denoted as Fragment RNN-
RM, which includes reordering fragment fea-
tures, at expense of a significant increase of
time complexity.
We consider a hierarchical recurrent neural
network. At top level, this is defined as the
previous RNN. However, the x(j) and xo(j)
vectors, in addition to the feature vectors de-
scribed as above now contain also the final
states of another recurrent neural network.
This internal RNN has a separate clock and
a separate state vector. For each step t of
the top-level RNN which transitions between
word f ′(t− 1) and f ′(t), the internal RNN is
reinitialized to its own initial state and per-
forms multiple internal steps, one for each ac-
tion in the fragment of the execution that the
walker automaton must perform to walk be-
tween words f ′(t− 1) and f ′(t) in the depen-
dency parse (with a special shortcut of length
one if they are adjacent in f with monotonic
relative order).

6values greater than 5 and smaller than 10 are quan-
tized as 5, values greater or equal to 10 are quantized as
10. Negative values are treated similarly.

The state transition of the inner RNN is de-
fined as:

vr(t) = τ(Θ(r1) · xr(tr)+ ΘrREC · vr(tr− 1))(7)
where xr(tr) is the feature function for the
word traversed at inner time tr in the execu-
tion fragment. vr(0) = vinitr , Θ(r1) and ΘrREC
are parameters.
Evaluation and decoding are performed es-
sentially in the same was as in Base RNN-
RM, except that the time complexity is now
O(L3f ) since the length of execution fragments
is O(L f ).
Training is also essentially performed in the
same way, though gradient computation is
much more involved since gradients prop-
agate from the top-level RNN to the inner
RNN. In our implementation we just used the
automatic differentiation facilities of Theano.

3.2.1 Features
The unlexicalized features for the inner RNN
input vector xr(tr) depend on the current
word in the execution fragment (at index tr),
the previous one and the action label: UP,
DOWN or RIGHT (shortcut). EMIT actions
are not included as they always implicitly oc-
cur at the end of each fragment.
Specifically the features, encoded with the
”one-hot” encoding are: A ∗ POS(tr) ∗
POS(tr − 1), A ∗ POS(tr) ∗ DEPREL(tr −
1), A ∗ DEPREL(tr) ∗ POS(tr − 1), A ∗
DEPREL(tr) ∗ DEPREL(tr − 1).
These features are also conjoined with the
quantized signed distance (in the original
sentence) between each pair of words.
The lexicalized features just include the surface
form of each visited word at tr.

3.3 Base GRU-RM

We also propose a variant of the Base RNN-
RM where the standard recurrent hidden
layer is replaced by a Gated Recurrent Unit
layer, recently proposed by Cho et al. (2014)
for machine translation applications.
The Base GRU-RM is defined as the Base
RNN-RM of sec. 3.1, except that the recur-
rence relation 4 is replaced by fig. 2

Features are the same of unlexicalized Base
RNN-RM (we experienced difficulties train-
ing the Base GRU-RM with lexicalized fea-
tures).

851



vrst(t) = π(Θ
(1)
rst · x(t) + ΘRECrst · v(t− 1))

vupd(t) = π(Θ
(1)
upd · x(t) + ΘRECupd · v(t− 1))

vraw(t) = τ(Θ(1) · x(t) + ΘREC · v(t− 1)� vupd(t))
v(t) = vrst(t)� v(t− 1) + (1− vrst(t))� vraw(t)

(8)

Figure 2: GRU recurrence equations. vrst(t) and vupd(t) are the activation vectors of the ”reset”
and ”update” gates, respectively, and π(·) is the logistic sigmoid function.

.

Training is also performed in the same
way except that we found more benefi-
cial to convergence speed to optimize using
Adam (Kingma and Ba, 2014) 7 rather than
AdaDelta.
In principle we could also extend the Frag-
ment RNN-RM into a Fragment GRU-RM,
but we did not investigate that model in this
work.

4 Experiments

We performed German-to-English pre-
reordering experiments with Base RNN-RM
(both unlexicalized and lexicalized), Frag-
ment RNN-RM and Base GRU-RM.

In order to validate the experimental re-
sults on a different language pair, we addi-
tionally performed an Italian-to-English pre-
reordering experiment with the Base GRU-
RM, after assessing that this was the model
that obtained the largest improvement on
German-to-English.

4.1 Setup

The German-to-English baseline phrase-
based system was trained on the Europarl v7
corpus (Koehn, 2005). We randomly split it
in a 1,881,531 sentence pairs training set, a
2,000 sentence pairs development set (used
for tuning) and a 2,000 sentence pairs test
set. The English language model was trained
on the English side of the parallel corpus
augmented with a corpus of sentences from
AP News, for a total of 22,891,001 sentences.
The baseline system is phrase-based Moses
in a default configuration with maximum
distortion distance equal to 6 and lexicalized
reordering enabled. Maximum phrase size is

7with learning rate 2 · 10−5 and all the other hyper-
parameters equal to the default values in the article.

equal to 7.
The language model is a 5-gram
IRSTLM/KenLM.
The pseudo-oracle system was trained on
the training and tuning corpus obtained by
permuting the German source side using
the heuristic described in section 2.2 and is
otherwise equal to the baseline system.
In addition to the test set extracted from
Europarl, we also used a 2,525 sentence
pairs test set (”news2009”) a 3,000 sentence
pairs ”challenge” set used for the WMT 2013
translation task (”news2013”).

The Italian-to-English baseline system was
trained on a parallel corpus assembled from
Europarl v7, JRC-ACQUIS v2.2 (Steinberger
et al., 2006) and additional bilingual articles
crawled from online newspaper websites8, to-
taling 3,081,700 sentence pairs, which were
split into a 3,075,777 sentence pairs phrase-
table training corpus, a 3,923 sentence pairs
tuning corpus, and a 2,000 sentence pairs test
corpus.

Non-projective dependency parsing for our
models, both for German and Italian was
performed with the DeSR transition-based
parser (Attardi, 2006).

We also trained a German-to-English
Moses system with pre-reordering performed
by Collins et al. (2005) rules, implemented by
Howlett and Dras (2011).
Constituency parsing for Collins et al. (2005)
rules was performed with the Berkeley parser
(Petrov et al., 2006). For Italian-to-English
we did not compare with a hand-coded
reordering system as we are not aware of
any strong pre-reordering baseline for this
language pair.

For our experiments, we extract approxi-

8Corriere.it and Asianews.it

852



mately 300,000 sentence pairs from the Moses
training set based on a heuristic confidence
measure of word-alignment quality (Huang,
2009), (Navratil et al., 2012). We randomly
removed 2,000 sentences from this filtered
dataset to form a validation set for early stop-
ping, the rest were used for training the pre-
reordering models.

4.2 Results

The hidden state size s of the RNNs was set to
100 while it was set to 30 for the GRU model,
validation was performed every 2,000 train-
ing examples. After 50 consecutive validation
rounds without improvement, training was
stopped and the set of training parameters
that resulted in the lowest validation cross-
entropy were saved.
Training took approximately 1.5 days for the
unlexicalized Base RNN-RM, 2.5 days for the
lexicalized Base RNN-RM and for the unlexi-
calized Base GRU-RM and 5 days for the un-
lexicalized Fragment RNN-RM on a 24-core
machine without GPU (CPU load never rose
to more than 400%).

Decoding was performed with a beam size
of 4. Decoding the whole German corpus
took about 1.0-1.2 days for all the models ex-
cept Fragment RNN-RM for which it took
about 3 days. Decoding for the Italian corpus
for the Base GRU-RM took approximately 1.5
days.

Effects on monolingual reordering score
are shown in fig. 3 (German) and fig. 4
(Italian), effects on translation quality are
shown in fig. 5 (German-to-English) and fig.
6 (Italian-to-English)9.

4.3 Discussion and analysis

All our German-to-English models signifi-
cantly improved over the phrase-based base-
line, performing as well as or almost as well
as (Collins et al., 2005), which is an interesting
result since our models doesn’t require any
specific linguistic expertise.

Surprisingly, the lexicalized version of Base
RNN-RM performed worse than the unlexi-

9Although the baseline systems were trained on the
same datasets used in Miceli Barone and Attardi (2013),
the results are different since we used a different ver-
sion of Moses

calized one. This goes contrary to expectation
as neural language models are usually lexical-
ized and in fact often use nothing but lexical
features.

The unlexicalized Fragment RNN-RM was
quite accurate but very expensive both dur-
ing training and decoding, thus it may not be
practical.

The unlexicalized Base GRU-RM per-
formed very well, especially on the Europarl
dataset (where all the scores are much higher
than the other datasets) and it never per-
formed significantly worse than the unlexi-
calized Fragment RNN-RM which is much
slower.

We also performed exploratory experi-
ments with different feature sets (such as
lexical-only features) but we couldn’t obtain
a good training error. Larger network sizes
should increase model capacity and may pos-
sibly enable training on simpler feature sets.

The Italian-to-English experiment with
Base GRU-RM confirmed that this model per-
forms very well on a language pair with dif-
ferent reordering phenomena than German-
to-English.

5 Conclusions

We presented a class of statistical syntax-
based, non-projective, non-tree-local pre-
reordering systems for machine translation.
Our systems processes source sentences
parsed with non-projective dependency
parsers and permutes them into a target-
like word order, suitable for translation
by an appropriately trained downstream
phrase-based system.

The models we proposed are completely
trained with machine learning approaches
and is, in principle, capable of generating ar-
bitrary permutations, without the hard con-
straints that are commonly present in other
statistical syntax-based pre-reordering meth-
ods.
Practical constraints depend on the choice
of features and are therefore quite flexible,
allowing a trade-off between accuracy and
speed.

In our experiments with the RNN-RM and
GRU-RM models we managed to achieve
translation quality improvements compara-

853



Reordering BLEU improvement
none 62.10
unlex. Base RNN-RM 64.03 +1.93
lex. Base RNN-RM 63.99 +1.89
unlex. Fragment RNN-RM 64.43 +2.33
unlex. Base GRU-RM 64.78 +2.68

Figure 3: German ”Monolingual” reordering scores (upstream system output vs. ”oracle”-
permuted German) on the Europarl test set. All improvements are significant at 1% level.

Reordering BLEU improvement
none 73.11
unlex. Base GRU-RM 81.09 +7.98

Figure 4: Italian ”Monolingual” reordering scores on the Europarl test set. All improvements
are significant at 1% level.

Test set system BLEU improvement
Europarl baseline 33.00
Europarl ”oracle” 41.80 +8.80
Europarl Collins 33.52 +0.52
Europarl unlex. Base RNN-RM 33.41 +0.41
Europarl lex. Base RNN-RM 33.38 +0.38
Europarl unlex. Fragment RNN-RM 33.54 +0.54
Europarl unlex. Base GRU-RM 34.15 +1.15
news2013 baseline 18.80
news2013 Collins NA NA
news2013 unlex. Base RNN-RM 19.19 +0.39
news2013 lex. Base RNN-RM 19.01 +0.21
news2013 unlex. Fragment RNN-RM 19.27 +0.47
news2013 unlex. Base GRU-RM 19.28 +0.48
news2009 baseline 18.09
news2009 Collins 18.74 +0.65
news2009 unlex. Base RNN-RM 18.50 +0.41
news2009 lex. Base RNN-RM 18.44 +0.35
news2009 unlex. Fragment RNN-RM 18.60 +0.51
news2009 unlex. Base GRU-RM 18.58 +0.49

Figure 5: German-to-English RNN-RM translation scores. All improvements are significant at
1% level.

Test set system BLEU improvement
Europarl baseline 29.58
Europarl unlex. Base GRU-RM 30.84 +1.26

Figure 6: Italian-to-English RNN-RM translation scores. Improvement is significant at 1% level.

854



ble to those of the best hand-coded pre-
reordering rules.

References

Yaser Al-Onaizan and Kishore Papineni. 2006.
Distortion models for statistical machine trans-
lation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Com-
putational Linguistics, ACL-44, pages 529–536,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Giuseppe Attardi. 2006. Experiments with a mul-
tilanguage non-projective dependency parser.
In Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, CoNLL-X ’06,
pages 166–170, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.

James Bergstra, Olivier Breuleux, Frédéric
Bastien, Pascal Lamblin, Razvan Pascanu,
Guillaume Desjardins, Joseph Turian, David
Warde-Farley, and Yoshua Bengio. 2010.
Theano: a CPU and GPU math expression
compiler. In Proceedings of the Python for Scien-
tific Computing Conference (SciPy), June. Oral
Presentation.

Alexandra Birch, Miles Osborne, and Philipp
Koehn. 2008. Predicting success in machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’08, pages 745–754, Stroudsburg,
PA, USA. Association for Computational Lin-
guistics.

Arianna Bisazza and Marcello Federico. 2013.
Efficient solutions for word reordering in
German-English phrase-based statistical ma-
chine translation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 440–451, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.

Cristina Bosco and Vincenzo Lombardo. 2004.
Dependency and relational structure in tree-
bank annotation. In COLING 2004 Recent Ad-
vances in Dependency Grammar, pages 1–8.

Kyunghyun Cho, Bart van Merriënboer, Dzmitry
Bahdanau, and Yoshua Bengio. 2014. On
the properties of neural machine translation:
Encoder-decoder approaches. arXiv preprint
arXiv:1409.1259.

Michael Collins, Philipp Koehn, and Ivona
Kučerová. 2005. Clause restructuring for sta-
tistical machine translation. In Proceedings of
the 43rd annual meeting on association for computa-
tional linguistics, pages 531–540. Association for
Computational Linguistics.

Nadir Durrani, Helmut Schmid, and Alexander
Fraser. 2011. A joint sequence translation
model with integrated reordering. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 1045–1054. Associ-
ation for Computational Linguistics.

Chris Dyer and Philip Resnik. 2010. Context-free
reordering, finite-state translation. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, HLT ’10, pages
858–866, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Minwei Feng, Arne Mauser, and Hermann Ney.
2010. A source-side decoding sequence model
for statistical machine translation. In Conference
of the Association for Machine Translation in the
Americas (AMTA).

C. Gallicchio and A. Micheli. 2011. Architectural
and markovian factors of echo state networks.
Neural Networks, 24(5):440 – 456.

Dmitriy Genzel. 2010. Automatically learning
source-side reordering rules for large scale ma-
chine translation. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, COLING ’10, pages 376–384, Stroudsburg,
PA, USA. Association for Computational Lin-
guistics.

Alex Graves. 2013. Generating sequences
with recurrent neural networks. arXiv preprint
arXiv:1308.0850.

Susan Howlett and Mark Dras. 2011. Clause
restructuring for SMT not absolutely helpful.
In Proceedings of the 49th Annual Meeting of the
Assocation for Computational Linguistics: Human
Language Technologies, pages 384–388.

Fei Huang. 2009. Confidence measure for word
alignment. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 2-
Volume 2, pages 932–940. Association for Com-
putational Linguistics.

Herbert Jaeger. 2001. The echo state ap-
proach to analysing and training recurrent neu-
ral networks-with an erratum note. Bonn, Ger-
many: German National Research Center for Infor-
mation Technology GMD Technical Report, 148:34.

Maxim Khalilov and José AR Fonollosa. 2011.
Syntax-based reordering for statistical ma-
chine translation. Computer speech & language,
25(4):761–788.

Diederik Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. arXiv
preprint arXiv:1412.6980.

855



Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen,
Christine Moran, Richard Zens, Chris Dyer,
Ondřej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings
of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL
’07, pages 177–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Philipp Koehn. 2005. Europarl: A parallel cor-
pus for statistical machine translation. In Con-
ference Proceedings: the tenth Machine Translation
Summit, pages 79–86, Phuket, Thailand. AAMT,
AAMT.

Uri Lerner and Slav Petrov. 2013. Source-side
classifier preordering for machine translation.
In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP
’13).

Antonio Valerio Miceli Barone and Giuseppe At-
tardi. 2013. Pre-reordering for machine
translation using transition-based walks on de-
pendency parse trees. In Proceedings of the
Eighth Workshop on Statistical Machine Transla-
tion, pages 164–169, Sofia, Bulgaria, August.
Association for Computational Linguistics.

Tomas Mikolov, Martin Karafiát, Lukas Bur-
get, Jan Cernockỳ, and Sanjeev Khudanpur.
2010. Recurrent neural network based lan-
guage model. In INTERSPEECH, pages 1045–
1048.

Jiri Navratil, Karthik Visweswariah, and Anan-
thakrishnan Ramanathan. 2012. A com-
parison of syntactic reordering methods for
english-german machine translation. In COL-
ING, pages 2043–2058.

Franz Josef Och, Christoph Tillmann, Hermann
Ney, et al. 1999. Improved alignment models
for statistical machine translation. In Proc. of the
Joint SIGDAT Conf. on Empirical Methods in Nat-
ural Language Processing and Very Large Corpora,
pages 20–28.

Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, compact,
and interpretable tree annotation. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 433–440. Association for Computational
Linguistics.

Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Dniel Varga. 2006. The jrc-acquis: A multi-
lingual aligned parallel corpus with 20+ lan-
guages. In Proceedings of the 5th International

Conference on Language Resources and Evaluation
(LREC’2006), Genoa, Italy.

Roy Tromble and Jason Eisner. 2009. Learning
linear ordering problems for better translation.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing: Vol-
ume 2 - Volume 2, EMNLP ’09, pages 1007–1016,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Karthik Visweswariah, Rajakrishnan Rajku-
mar, Ankur Gandhe, Ananthakrishnan Ra-
manathan, and Jiri Navratil. 2011. A word
reordering model for improved machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’11, pages 486–496, Stroudsburg,
PA, USA. Association for Computational
Linguistics.

Matthew D Zeiler. 2012. Adadelta: An adap-
tive learning rate method. arXiv preprint
arXiv:1212.5701.

856


