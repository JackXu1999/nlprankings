



















































Homograph Disambiguation through Selective Diacritic Restoration


Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 49–59
Florence, Italy, August 1, 2019. c©2019 Association for Computational Linguistics

49

Homograph Disambiguation Through Selective Diacritic Restoration

Sawsan Alqahtani,1,2 Hanan Aldarmaki,1 Mona Diab1,3
1The George Washington University

2Princess Nourah Bint Abdul Rahman University
3AWS, Amazon AI

sawsanq@gwu.edu, aldarmaki@gwu.edu, diabmona@amazon.com

Abstract
Lexical ambiguity, a challenging phenomenon
in all natural languages, is particularly preva-
lent for languages with diacritics that tend to
be omitted in writing, such as Arabic. Omit-
ting diacritics leads to an increase in the num-
ber of homographs: different words with the
same spelling. Diacritic restoration could the-
oretically help disambiguate these words, but
in practice, the increase in overall sparsity
leads to performance degradation in NLP ap-
plications. In this paper, we propose ap-
proaches for automatically marking a sub-
set of words for diacritic restoration, which
leads to selective homograph disambiguation.
Compared to full or no diacritic restoration,
these approaches yield selectively-diacritized
datasets that balance sparsity and lexical dis-
ambiguation. We evaluate the various selec-
tion strategies extrinsically on several down-
stream applications: neural machine transla-
tion, part-of-speech tagging, and semantic tex-
tual similarity. Our experiments on Arabic
show promising results, where our devised
strategies on selective diacritization lead to a
more balanced and consistent performance in
downstream applications.

1 Introduction

Lexical ambiguity, an inherent phenomenon in
natural languages, refers to words or phrases that
can have multiple meanings. In written text, lex-
ical ambiguity can be roughly characterized into
two categories: polysemy and homonymy. A pol-
ysemous word has multiple senses that express
different but related meanings (e.g. ‘head’ as an
anatomical body part, or as a person in charge),
whereas homonyms are different words that hap-
pen to have the same spelling (e.g. ‘bass’ as an in-
strument vs. a fish) (Löbner, 2013). Homographs
are words that have the same spelling but may have
different pronunciation and meaning.

A diacritic is a mark that is added above, below,
or within letters to indicate pronunciation, vowels,

or other functions. For languages that use diacrit-
ical marks, such as Arabic or Hebrew, the orthog-
raphy is typically under-specified for such marks,
i.e. the diacritics are omitted. This phenomenon
exacerbates the lexical ambiguity problem since
it increases the rate of homographs. For exam-
ple, without considering context, the undiacritized
Arabic word ktb may refer to any of the follow-
ing diacritized variants:1 katab I.

��
J
�
» “wrote”, ku-

tub I.
��
J
�
» “books”, or kutib I.

�
J�

�
» “was written”.

As an illustrative analogy in English, dropping
vowels in a word such as pan yields the under-
specified token pn which can be mapped to pin,
pan, pun, pen. It should be noted that even after
fully specifying words with their relevant diacrit-
ics, homonyms such as “bass” are still ambiguous;
likewise in Arabic, the fully-specified word bayot
�

I
�
�

�
K. can either mean “verse” or “house”.

In this paper, we devise strategies to automati-
cally identify and disambiguate a subset of homo-
graphs that result from omitting diacritics. While
context is often sufficient for determining the
meaning of ambiguous words, explicitly restoring
missing diacritics should provide valuable addi-
tional information for homograph disambiguation.
This process, diacritization, would render the re-
sulting text comparable to that of languages whose
words are orthographically fully specified such as
English.

Past studies have focused on developing models
for automatic diacritic restoration that can be used
as a pre-processing step for various applications
such as text-to-speech (Ungurean et al., 2008) and
reading comprehension (Hermena et al., 2015). In
theory, restoring all diacritics should also help im-
prove the performance of NLP applications such
as machine translation. However, in practice,

1We adopt Buckwalter Transliteration encod-
ing into Latin script for rendering Arabic text
http://www.qamus.org/transliteration.htm.



50

full diacritic restoration results in increased spar-
sity and out-of-vocabulary words, which leads to
degradation in performance (Diab et al., 2007;
Alqahtani et al., 2016). The main objective of this
work is to find a sweet spot between zero and full
diacritization in order to reduce lexical ambigu-
ity without increasing sparsity. We propose selec-
tive diacritization, a process of restoring diacritics
to a subset of the words in a sentence sufficient
to disambiguate homographs without significantly
increasing sparsity. Selective diacritization can be
viewed as a relaxed variant of word sense disam-
biguation since only homographs that arise from
missing diacritics are disambiguated.2

Intrinsically evaluating the quality of a devised
selective diacritization scheme against a gold set
is challenging since it is difficult to obtain a
dataset that exhibits consistent selective diacritiza-
tion with reliable inter-annotator agreement (Za-
ghouani et al., 2016b; Bouamor et al., 2015),
thereby necessitating an empirical automatic in-
vestigation. Hence, in this work, we evaluate the
proposed selective diacritization schemes extrin-
sically on various semantic and syntactic down-
stream NLP applications: Semantic Textual Simi-
larity (STS), Neural Machine Translation (NMT),
and Part-of-Speech (POS) tagging. We compare
our selective strategies against two baselines full
diacritization and zero diacritics applied on all the
words in the text. We use Modern Standard Arabic
(MSA) as a case-study.3

Our approach is summarized as follows: we
start with full diacritic restoration of a large cor-
pus, then apply different unsupervised methods to
identify the words that are ambiguous when undi-
acritized. This results in a dictionary where each
word is assigned an ambiguity label (ambiguous
vs. unambiguous). Selectively-diacritized datasets
can then be constructed by restoring the full dia-
critics only to the words that are identified as am-
biguous.

The contribution of this paper is threefold:

1. We introduce automatic selective diacritiza-
tion as a viable step in lexical disambiguation
and provide an encouraging baseline for fu-
ture developments towards optimal diacriti-

2Identifying empirically successful selective diacritiza-
tion strategies can help discover optimal diacritization
schemes; however, this direction is currently beyond the
scope of this work.

3Proposed methodologies can be applied to other lan-
guages where diacritics are omitted.

zation. Section 2 describes existing work
towards optimal diacritization and how they
differ from our approach;

2. We propose several unsupervised data-driven
methods for the automatic identification of
ambiguous words;

3. We evaluate and analyze the impact of par-
tial sense disambiguation (i.e. selective dia-
critic restoration of identified homographs) in
downstream applications for MSA.

2 Related Work

We are concerned mainly with studies that target
word disambiguation through the use of diacrit-
ics/accents restoration. Homograph disambigua-
tion through accents has been explored previously
in several studies with the use of different rule-
based and machine-learning approaches for lan-
guages such as Arabic, Spanish, Igbo, and Viet-
namese (Ezeani et al., 2017; Nguyen et al., 2012;
Nivre et al., 2017; Said et al., 2013; Tufiş and
Chiţu, 1999).

Bouamor et al. (2015) conducted a pilot study
where they asked human annotators to add the
minimum number of diacritics sufficient to dis-
ambiguate homographs. However, attempts to
provide human annotation for selective diacriti-
zation resulted in low inter-annotator agreement
due to the annotators’ subjectivity and different
linguistic understanding of the words and con-
texts (Bouamor et al., 2015; Zaghouani et al.,
2016b). To address this issue, Zaghouani et al.
(2016b) used a morphological disambiguation
tool, MADAMIRA (Pasha et al., 2014), to iden-
tify candidate words that may need disambigua-
tion. A word was considered ambiguous if
MADAMIRA generates multiple high-scoring di-
acritic alternatives, and human annotators were
asked to select from these alternatives or man-
ually edit the diacritics if none of the options
was deemed correct. This resulted in a signifi-
cant increase in inter-annotator agreement. Our
work differs in two aspects: first, we develop au-
tomatic methods for ambiguity detection based
on word usage. We then restore the diacritics
for all occurrences of these ambiguous words,
whereas in (Zaghouani et al., 2016b), the same
word may be tagged as ambiguous in some cases
but not ambiguous in other cases depending on



51

context, which makes it harder to generalize to
new datasets.

Yarowsky (1994) developed an accent restora-
tion algorithm for Spanish and French that speci-
fies the accent patterns for ambiguous words (i.e.
multiple accent patterns). Our intuition is different
than that of Yarowsky (1994) in two ways. First,
they added diacritics to all words that have more
than one diacritic pattern while we add the diacrit-
ics for only a subset of candidate words. Second,
they used context for adding diacritics, while we
use context to isolate words that require diacrit-
ics, for which we apply an off-the-shelf diacritic
restoration model.

Rather than restoring all diacritics in the writ-
ten text, the idea of adding diacritics sufficient
to resolving lexical ambiguity was initially intro-
duced in (Diab et al., 2007). They developed sev-
eral linguistically-based partial schemes and eval-
uated their methods in Statistical Machine Trans-
lation. They found that fully diacritizing texts
led to performance degradation due to sparseness
while no diacritization increased the lexical am-
biguity rate. Similar results have been found in
(Alqahtani et al., 2016), where several other ba-
sic diacritic patterns were investigated. Although
the impact of diacritics in machine translation was
promising, the development of partial schemes
does not show significant improvements over the
non-diacritized and fully-diacritized baselines.

Alnefaie and Azmi (2017) introduced a par-
tial diacritization scheme for MSA based on the
output of a morphological analyzer in addition
to WordNet (Black et al., 2006), and Alqahtani
et al. (2018) created a lexical resource that as-
signed an ambiguity label for each word, where a
word is considered ambiguous if it has more than
one diacritic possibility, with and without consid-
ering its part-of-speech tag. However, both (Alne-
faie and Azmi, 2017; Alqahtani et al., 2018) did
not evaluate their methods empirically to demon-
strate their effectiveness for NLP applications.
Hanai and Glass (2014) similarly developed three
linguistically-based partial diacritic schemes for
automatic speech recognition and found statisti-
cally significant improvement over the baseline.
However, their work is focused on improving word
pronunciations whereas we focus on word sense
disambiguation. Ezeani et al. (2017) discussed the
impact of adding accents for each and every word
in Igbo language, potentially increasing the per-

formance for machine translation and word sense
disambiguation.

All of the aforementioned approaches either ap-
ply full diacritics on all words whenever appro-
priate or derive partial diacritic schemes based on
linguistic understanding; crucially these partial di-
acritic schemes are applied to all words in a sen-
tence.4 Our devised strategies differ in that we ap-
ply full diacritization to a select set of tokens in the
text. Our work is related to these previous stud-
ies in the sense that we reduce the search space of
candidate words that could benefit from full or par-
tial diacritization without increasing sparsity. Fur-
thermore, the novelty of this work lies in utilizing
automatic unsupervised methods to identify such
words.

3 Approach

3.1 Selective Diacritization

Selective diacritization is the process of restoring
diacritics to a subset of words in a text corpus.
Manually annotating words in a dataset with bi-
nary ambiguity labels (ambiguous vs. unambigu-
ous) is challenging due to the difficulty in defin-
ing ambiguous words that would benefit from di-
acritics (Zaghouani et al., 2016b). Therefore, we
propose several techniques to automatically iden-
tify ambiguous words for selective diacritization.
Since it is common to use distributed word vec-
tor representations in downstream tasks, we de-
fine ambiguity in terms of distributional similarity
among diacritized word variants. Our intuition is
that variants with low distributional similarity are
more likely to benefit from diacritization to disam-
biguate their meanings and tease apart their con-
text variations. On the other hand, word variants
with highly similar contexts tend to have very sim-
ilar distributional representations, which results in
unnecessary redundancy and sparsity if all variants
are kept.

Based on this definition, we developed sev-
eral context-based approaches to identify candi-
date ambiguous word types and generate a set of
dictionaries with ambiguity labels (AmbigDict),
where each word is marked as either ambiguous

4For instance, the undiacritized sentence bEd ywm
ÐñK
 YªK. “after a day” would be diacritized as baEod yawom

Ð
�

ñ
�
K
 Y

�
ª

�
K. when fully diacritized, bEod ywom Ð

�
ñK
 Y

�
ªK. ((Diab

et al., 2007; Alqahtani et al., 2016)’s SUK scheme) when par-
tially diacritized, baEod ywm ÐñK
 Y

�
ª

�
K. when selectively dia-

critized.



52

or unambiguous. The proposed approaches can
be classified by the type of tokens used to create
the AmbigDict: diacritized (AmbigDict-DIAC) or
undiacritized (AmbigDict-UNDIAC). For exam-
ple an entry in AmbigDict-UNDIAC would be
“Elm” ÕÎ«: ambiguous; “ktb” I.

�
J»: unambiguous,

whereas in AmbigDict-DIAC would be “Ealam”
Õ
�
Î

�
«: ambiguous; “kutub” I.

��
J
�
»: unambiguous.

3.2 AmbigDict-UNDIAC Generation
We explore two methods for creating ambiguity
dictionaries from undiacritized text: using a mor-
phological analyzer, and unsupervised sense in-
duction.

Multiple Morphological Variants (MULTI):
The number of diacritic alternatives for a word
can be a clue to determine whether a word is am-
biguous due to missing diacritics (Alqahtani et al.,
2018). In this approach, context is not consid-
ered, but rather the output of a morphological an-
alyzer applied to the text. We leverage the mor-
phological analyzer component of MADAMIRA
(Pasha et al., 2014) to generate all possible valid
diacritic variants of a word whether these variants
are present in the corpus or not. If an undiacritized
word has more than one possible diacritic variant,
we consider it ambiguous. We use this context-
independent approach as a baseline.

Sense Induction Based Approach (SENSE):
Selective diacritization is related to word sense
disambiguation, however we only target disam-
biguation through diacritic restoration. Tech-
niques used in automatic word sense induction
can be used as a basis for identifying ambiguous
words. Using undiacritized text, we apply an off-
the-shelf system for word sense induction devel-
oped by Pelevina et al. (2017), which uses the
Chinese Whispers algorithm (Biemann, 2006) to
identify senses of a graph constructed by comput-
ing the word similarities (highest cosine similari-
ties) through using word as well as context embed-
dings. We apply the first three steps described in
Pelevina et al. (2017) but we do not use the gen-
erated sense-based embeddings; we only use the
system to identify the words with multiplw senses.
We set the three parameters as follows: the graph
size N to 200, the inventory granularity n to 400,
and the minimum number of clusters (senses) k to
5.5 A word type is deemed ambiguous if it appears

5We tuned these parameters empirically.

in more than one cluster.

3.3 AmbigDict-DIAC Generation

We explore clustering and translation based meth-
ods to create ambiguity dictionaries from dia-
critized text.

Clustering-based Approaches (CL): Similar
in spirit to SENSE, we apply unsupervised clus-
tering to our corpora to induce AmbigDict. How-
ever, unlike SENSE, we apply clustering to dia-
critized data. Our intuition is that dissimilar words
are likely to occur in different contexts, and there-
fore likely to be in different clusters. Therefore,
we tag words as ambiguous if diacritized variants
of the same underlying undiacritized form appear
in different clusters.

As a preprocessing step, we apply a full contex-
tualized diacritization tool to the underlying cor-
pora. We leverage the MADAMIRA tool (Pasha
et al., 2014) to produce fully diacritized text (for
every token in the data) covering both types of di-
acritic restoration: lexical and syntactic. The lat-
ter covers syntactic case and mood diacritics. In
this study, we are only concerned with lexical am-
biguity; Moreover, MADAMIRA has a very high
diacritic error rate in syntactic diacritic restora-
tion (15%) compared to (3.5%) for lexical dia-
critic restoration. Hence, we drop the predicted
word final syntactic diacritics resulting in a dia-
critization scheme similar to the partial scheme in
(Diab et al., 2007; Alqahtani et al., 2016), namely,
FULL-CM. In FULL-CM, every token is fully lex-
ically diacritized (e.g. the fully diacritized words
Ealama �Õ

�
Î

�
« and Ealamu �Õ

�
Î

�
« differ in their syntac-

tic diacritics and are mapped to Ealam Õ
�
Î

�
« “flag”

in FULL-CM).
Given this diacritized corpus, we apply three

different standard clustering approaches: Brown6

(Brown et al., 1992) (CL-BR), K-means7 (Ka-
nungo et al., 2002) (CL-KM), and Gaussian
Mixture via Expectation Maximization (CL-EM)8

(Dempster et al., 1977). We tune the number of
clusters for downstream tasks; in particular, we
empirically investigate the performance on the de-

6https://github.com/percyliang/brown-cluster
7We use “sickit-learn” version 0.18.1. We use the value

1 for both random state and n init and the default values for
the remaining parameters.

8We use “sickit-learn” version 0.18.1. with the follow-
ing parameters: max iter=1000, random state=1, and covari-
ance type=spherical



53

velopment set in the downstream tasks for differ-
ent number of clusters.

Translation-based Approaches (TR): Transla-
tion can be used as a basis for word sense induc-
tion (Diab and Resnik, 2002; Ng et al., 2003) since
words across different languages tend to have dis-
parate senses. Following a similar intuition, we
use English translations from a parallel corpus as a
trigger to divide the set of diacritic possibilities of
a word into multiple subsets. The intuition here is
that homographs worth disambiguating are those
that are likely to be translated differently. We
leverage an English MSA parallel corpus, where
the MSA is diacritized in the Full-CM scheme us-
ing MADAMIRA (the same preprocessing step for
CL described above). In this approach, diacritized
variants that share the same English translations
are considered unambiguous, whereas those that
are typically translated to different English words
are considered ambiguous. To that end, we first
align the sentences at the token level and gener-
ate word translation probabilities using fast-align
(Dyer et al., 2013), which is a log-linear reparam-
eterization of IBM Model 2 (Brown et al., 1993).
If a word shares any translation with its diacritized
variant in the top N most likely translations, we
consider it unambiguous (e.g. Ealam Õ

�
Î

�
« ‘flag”

and Ealima �ÕÎ�
�
« ‘learned” are unambiguous since

they do not share top translations). Otherwise, the
word is tagged as ambiguous. We tune N to in-
clude 1, 5, 10, and all translations.

4 Evaluation

Once we have generated the two variants of Am-
bigDict (AmbigDict-UNDIAC and AmbigDict-
DIAC), we evaluate their efficacy extrinsically on
downstream applications. For all downstream ap-
plications, training and test data are preprocessed
using MADAMIRA (Pasha et al., 2014) with the
FULL-CM diacritization scheme where we only
keep lexical diacritics.9 Then the data is filtered
based on the AmbigDict of choice; namely, only
word tokens in the text deemed ambiguous accord-
ing to each AmbigDict maintain their full diacrit-
ics (as generated by MADAMIRA) while the un-
ambiguous words are kept undiacritized.

9Full diacritics are included except inflectional diacritics
that reflect the syntactic positions of words within sentences
but do not alter meaning.

4.1 Datasets

For MULTI, SENSE, CR, we use a combination of
four Modern Standard Arabic (MSA) datasets that
vary in genre and domain and add up to ∼50M
tokens: Gigaword 5th edition, distributed by Lin-
guistic Data Consortium (LDC), Wikipedia dump
2016, Corpus of Contemporary Arabic (CCA)
(Zaghouani et al., 2016a; Al-Sulaiti and Atwell,
2006), and LDC Arabic Tree Bank (ATB).10 For
TR, we use an Arabic-English parallel dataset
which includes ∼60M tokens and is created from
53 LDC catalogs. For data cleaning, we replace
e-mails and URLs with a unified token and use
SPLIT tool (Al-Badrashiny et al., 2016) to clean
UTF8 characters (e.g. Latin and Chinese), remove
diacritics in the original data, and separate punc-
tuation, symbols, and numbers in the text, and re-
place them with separate unified tokens. We split
long sentences (more than 150 words) by punctu-
ation and then remove all sentences that are still
longer than 150 words. We use D3 style (i.e. all
affixes are separated) (Pasha et al., 2014) for Ara-
bic tokenization without normalizing characters.
For English, we lower case all characters and use
TreeTagger (Schmid, 1999) for tokenization. We
used SkipGram word embeddings (Mikolov et al.,
2013), where applicable.

4.2 Extrinsic Evaluation

We evaluate the effectiveness of the proposed ap-
proaches using three applications: Semantic Tex-
tual Similarity (STS), Neural Machine Translation
(NMT), and Part-of-Speech (POS) tagging. We
used different significance testing methods appro-
priate for each application with p = 0.05.

4.2.1 Semantic Textual Similarity (STS)
STS is a benchmark evaluation task (Cer et al.,
2017), where the objective is to predict the sim-
ilarity score between a pair of sentences. Per-
formance is typically evaluated using the Pearson
correlation coefficient against human judgments.
We used the William test (Graham and Baldwin,
2014) for significance testing. We experiment with
an unsupervised system based on matrix factoriza-
tion developed by (Guo and Diab, 2012; Guo et al.,
2014), which generates sentence embeddings from
a word-sentence co-occurrence matrix, then com-
pare them using cosine similarity.We use a dimen-
sion size of 700. To train the model, we use the

10Parts 1, 2, 3, 5, 6, 7, 10, 11, and 12



54

Arabic dataset released for SemEval-2017 task 1
(Cer et al., 2017). Since the training dataset is
small, we augment it by randomly selecting sen-
tences from the dataset (∼1,655,922) described in
Section 4.1 where the chosen sentences have to
satisfy the following conditions: the number of
words lie between 5 and 150; and, the minimum
frequency for each word is 2. We apply these con-
ditions in the diacritized data since it suffers more
from sparseness, and then use their undiacritized
correspondents in the undiacritized setting.

4.2.2 Neural Machine Translation (NMT)
We build a BiLSTM-LSTM encoder-decoder ma-
chine translation system as described in (Bah-
danau et al., 2014) using OpenNMT (Klein et al.,
2014). We use 300 as input dimension for both
source and target vectors, 500 as hidden units,
and 0.3 for dropout. We initialize words with
embeddings trained using FastText (Bojanowski
et al., 2017) on the selectively-diacritized dataset
described in Section 4.1. We train the model us-
ing SGD with max gradient norm of 1 and learn-
ing rate decay of 0.5. We use the Web Inven-
tory of Transcribed and Translated Talks (WIT),
which is made available for IWSLT 2016 (Mauro
et al., 2012). We use BLEU (Papineni et al., 2002)
for evaluation, and bootstrap re-sampling and ap-
proximate randomization for significance testing
(Clark et al., 2011).

4.2.3 POS tagging
POS tagging is the task of determining the syntac-
tic role of a word (i.e. part of speech) within a
sentence. We use a BiLSTM-CRF architecture to
train a POS tagger using the implementation pro-
vided by (Reimers and Gurevych, 2017), with 300
as dimension size, initialized using the same em-
beddings we use in NMT. We used ATB datasets
parts 1,2, and 3 to train the models with Universal
Dependencies POS tags, version 2 (Nivre et al.,
2016). We use word-level accuracy for evaluation,
and t-test (Fisher, 1935; Dror et al., 2018) for sig-
nificance testing.

4.3 Automatic Diacritization

For generating the various AmbigDict approaches,
we used either fully diacritized versions, with-
out case and mood related diacritics,11 or undia-
critized versions of the datasets. Since it is ex-

11FULL-CM diacritization scheme, where we only keep
lexical diacritics.

pensive to obtain enormous human-annotated dia-
critized datasets, we use the morphological analy-
sis and disambiguation tool, MADAMIRA version
2016 2.1 (Pasha et al., 2014)

4.4 AmbigDict Statistics

Table 1 shows the number of identified ambigu-
ous words using each approach. Note that the to-
tal vocabulary sizes vary due to either different
datasets (e.g. for TR) or different preprocessing
(e.g. MULTI is based on undiacritized text). For
a given corpus, the number of ambiguous words
identified by MULTI can be viewed as an estimate
of the upper bound on ambiguous words due to di-
acritics. In MULTI, words that have no valid anal-
ysis generated by MADAMIRA are filtered; this
resulted in significant drop of the number of types
since the dataset includes noisy and infrequent in-
stances.

Dictionary Types % Ambig Words
AmbigDict-UNDIAC

MULTI 168,384 33.82
SENSE 467,953 8.50

AmbigDict-DIAC
CL 497,222 8.70 - 8.98
TR 36,533 27.58

Table 1: Vocabulary size and percentage of ambiguous
entries in AmbigDict-DIAC and AmbigDict-UNDIAC.

4.5 Results and Analysis

Dictionary STS NMT POS
NONE 0.608 27.1 97.99%
FULL-CM 0.593 26.8 98.06%

AmbigDict-UNDIAC
MULTI 0.591 27.0 98.11%*
SENSE 0.598 27.1 97.97%

AmbigDict-DIAC
CL-BR 0.601 27.1 98.09%
CL-KM 0.608 27.2 98.05%
CL-EM 0.617* 27.1 98.05%
TR 0.616* 27.3* 97.94%

Table 2: Performance with selectively-diacritized
datasets in downstream applications. Bold numbers
indicate approaches with higher performance than the
best performing baseline. * refers to approaches with
statistically-significant performance gains against the
best performing baseline.

Table 2 shows the performance of all strategies
in downstream tasks. Comparing baselines NONE



55

and FULL-CM, we observe that applications that
require semantic understanding (STS and NMT)
show better performance when the dataset is undi-
acritized, whereas POS tagging yields better per-
formance with the fully diacritized dataset.

The differences in performance between the
baselines are significant across all tasks. In all
tasks, at least one of the selective diacritization
schemes leads to performance gains compared
to both baselines. However, the choice of best
performing selective diacritization scheme varies
across tasks. In general, AmbigDict-DIAC ap-
proaches provide more promising results on se-
mantic related applications.

TR and CL-EM approaches yield the highest
performance in two of the applications (STS and
NMT), while MULTI and CL-BR achieved the
highest performance in POS tagging. Incidentally,
MULTI has the highest rate of ambiguous words,
which leads to more disambiguation through di-
acritization. This is consistent with the observa-
tion that diacritization is useful for syntactic tasks
like POS tagging, as observed through the base-
lines. In all other tasks, all selective diacritization
schemes performed significantly higher than full
diacritization.

Homograph Evaluation: We compared the per-
formance of the various schemes on subsets of the
test sets that include homographs, which are iden-
tified from the FULL-CM version of the training
datasets. For STS and NMT evaluation, we kept
only the test sentences that contain at least one ho-
mograph. For POS word-level evaluation, we only
considered the homographs. Table 3 shows homo-
graph performance across applications. The per-
formance on these subsets follow the same trend
as the overall results illustrated in Table 2 except
for POS tagging, where FULL-CM achieved the
comparable performance to the selective schemes.
Note, however, that almost all schemes achieved
higher POS tagging accuracy than NONE in these
subsets, and almost all schemes achieved compa-
rable or higher performance than FULL-CM in
STS and NMT, with TR significantly outperform-
ing the rest of the schemes as well as the baselines.
This illustrates the usefulness of selective diacriti-
zation for balancing homograph disambiguation
and sparsity compared to full or no diacritization.

Frequent POS Tag Performance: POS tagging
labels each word in the sentence as opposed to

Dictionary STS NMT POS
NONE 0.590 27.4 98.26%
FULL-CM 0.575 27 98.70%

AmbigDict-UNDIAC
MULTI 0.574 27.2 98.65%
SENSE 0.581 27.3 98.37%

AmbigDict-DIAC
CL-BR 0.584 27.4 98.59%
CL-KM 0.591 27.5 98.52%
CL-EM 0.60* 27.4 98.47%
TR 0.597* 27.6* 98.22%

Table 3: Performance of selectively-diacritized
datasets on homographs. Bold numbers indicate
approaches with higher performance than the best
performing baseline. * refers to approaches with
statistically-significant performance gains against the
best performing baseline.

NMT and STS which are evaluated at the sen-
tence level. Thus, we compared the best perform-
ing scheme (MULTI) and the baselines in terms
of their per tag performance on the four most
frequent tags: verbs, nouns, adjectives, and ad-
verbs. Table 4 shows the results of the baselines
and MULTI. For verbs and nouns, MULTI has
better performance than both baselines followed
by FULL-CM. For adjectives and adverbs, NONE
followed by MULTI have better performance than
FULL-CM. While FULL-CM has overall higher
accuracy, these results indicate that selective dia-
critization is a better approach for the most fre-
quent tags, possibly due to reduced sparsity com-
pared with FULL-CM.

OOV Performance: We measured the POS tag-
ging performance on Out-of-Vocabulary (OOV)
words to measure the effect of sparsity on perfor-
mance. We consider a word OOV if it does not
occur in the fully-diacritized training set. FULL-
CM achieved 87.43% tag accuracy, while NONE
achieved 87.56%. Using the MULTI scheme,
the POS tagging accuracy on OOV words was
87.51%, which falls between the two baselines, as
expected.

The results above indicate that using a selec-
tive diacritization scheme like MULTI can achieve
a desirable balance between disambiguation and
sparsity, such that better performance can be
achieved in the frequent cases without increasing
sparsity and OOV rates.



56

Scheme Verb Noun Adj Adv
MULTI 95.98% 97.63% 94.43% 97.05%
NONE 95.08% 97.45% 94.71% 98.08%
FULL-CM 95.87% 97.56% 94.40% 96.79%

Table 4: POS Tagging performance per most frequent
tag. Bold scores indicate the highest score in a column.

Type Example

part-of- $ak ½
�
�

 “doubt” (noun)

speech $ak∼
�
½

�
�

 “doubted” (verb)

action >a*okur Q
�
»

�	
X

�
@ “remember”

direction >u*ak∼ir Q
��
»

�	
X

�
@ “remind”

number $uyuwEiy∼ayon
	á
�
�


��
J
«� ñ

�
J


�
�

 “communists”

$uyuwEiy∼iyn 	á�

��
J
«� ñ

�
J


�
�

 “communists”

Table 5: Examples of ambiguous word pairs detected
by the clustering approaches.

4.6 Properties of Ambiguity Dictionaries

Clustering-Based Ambiguity: While MULTI,
TR, and SENSE approaches have intuitive jus-
tifications, the clustering approaches are based
entirely on distributional features. We analyzed
some of the results qualitatively to shed light
on types of words that are deemed ambiguous
through clustering. While the various cluster-
ing approaches resulted in different labeling, their
overall statistics and patterns were highly similar.
Using a random subset of words from these CL
dictionaries, we extracted the examples shown in
Table 5, which shows some of the most common
types of ambiguity. Note that the detected words
are either semantically ambiguous (e.g. deriva-
tions or distinct lemmas) or syntactically ambigu-
ous (e.g. part-of-speech).

Diacritic Pattern Complexity: We investigated
whether there are regular diacritic patterns among
words that were considered ambiguous by CL and
TR. Both approaches are data-driven, and we ap-
plied them on different corpora, so we investigated
their degree of agreement. To do so, we abstracted
the diacritic patterns for words in the vocabulary
by converting all characters other than diacritics
to a unified token “C”, then we collected statis-
tics of patterns of word pairs that are deemed am-
biguous vs. unambiguous. For example, the am-
biguous pair “katab” I.

��
J
�
» and “kutib” I.

�
J�

�
» have

the pattern CaCaC-CuCiC. For CL methods, the
number of unique diacritic patterns of unambigu-

Pattern Pair Example
CaC∼aC Ear∼aD 	��Q �« “make wider”
CuCiC EuriD 	Q

�

�
« “has been shown”

CaCiCaCoC ba$iEayon 	á��

�
ª

�
�

�
�. “ugly” (dual)

CaCiCiCC ba$iEiyn 	á�
ª�
�

�

�
�. “ugly” (plural)

Table 6: Examples of consistent diacritic patterns of
ambiguous words between CL and TR approaches.

ous word pairs (i.e. falling in the same cluster)
were between 197-219 patterns, whereas patterns
of ambiguous pairs were between 813-872. The
majority of patterns between unambiguous words
also occurred between ambiguous words. For TR,
while most patterns were labeled unambiguous,
around 300 patterns were always labeled ambigu-
ous. We did not find overarching semantic or
syntactic rules that consistently explain ambigu-
ity tags. However, a number of patterns (∼ 20)
were always tagged as ambiguous by both TR and
CL approaches. Table 6 shows a sample of these
patterns with examples.

5 Discussion & Conclusion

We investigated selective diacritization as a vi-
able technique for reducing lexical ambiguity us-
ing Arabic as a case study. To our knowledge,
this is the first work that shows encouraging results
with automatic selective diacritization schemes in
which the devised approaches evaluated on several
downstream applications. Our findings demon-
strate that partial diacritization achieves a balance
between homograph disambiguation and sparsity
effects; the performance using selective diacritiza-
tion always approached the best of both extremes
in each application, and sometimes surpassed the
performance of both baselines, which is consistent
with our intuition of balancing sparsity and disam-
biguation for improving overall performance.

While the increase in performance was not con-
sistent across all tasks, the results provide an em-
pirical evidence of the viability of automatic par-
tial diacritization, especially since manual efforts
in this vein had been rather challenging. We be-
lieve that the approaches described in this paper
could help advance the efforts towards optimal di-
acritization schemes, which are currently mostly
based on linguistic features. We analyzed some
patterns that were recognized as ambiguous using
our best-performing schemes, and showed some
consistencies in the diacritic patterns, although the



57

results were not conclusive. We believe that a
deeper analysis of these patterns may help shed
light on the lexical ambiguity phenomenon in ad-
dition to allowing further improvements in selec-
tive diacritization.

References
Mohamed Al-Badrashiny, Arfath Pasha, Mona Diab,

Nizar Habash, Owen Rambow, Wael Salloum, and
Ramy Eskander. 2016. SPLIT: Smart preprocessing
(Quasi) language independent tool. In International
Conference on Language Resources and Evaluation
(LREC).

Latifa Al-Sulaiti and Eric Atwell. 2006. The design
of a corpus of contemporary Arabic. International
Journal of Corpus Linguistics, 11(2):135–171.

Rehab Alnefaie and Aqil M. Azmi. 2017. Automatic
minimal diacritization of Arabic texts. In 3rd Inter-
national Conference on Arabic Computational Lin-
guistics (ACLing).

Sawsan Alqahtani, Mona Diab, and Wajdi Zaghouani.
2018. ARLEX: A large scale comprehensive lexical
inventory for Modern Standard Arabic. In OSACT
3: The 3rd Workshop on Open-Source Arabic Cor-
pora and Processing Tools.

Sawsan Alqahtani, Mahmoud Ghoneim, and Mona
Diab. 2016. Investigating the impact of various par-
tial diacritization schemes on Arabic-English sta-
tistical machine translation. In International As-
sociation for Machine Translation in the Americas
(AMTA).

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations.

Chris Biemann. 2006. Chinese whispers: an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In Proceedings
of the first workshop on graph based methods for
natural language processing. Association for Com-
putational Linguistics.

William Black, Sabri Elkateb, Horacio Rodriguez,
Musa Alkhalifa, Piek Vossen, Adam Pease, and
Christiane Fellbaum. 2006. Introducing the Arabic
WordNet project. In Proceedings of the third inter-
national WordNet conference.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics.

Houda Bouamor, Wajdi Zaghouani, Mona Diab, Os-
sama Obeid, Kemal Oflazer, Mahmoud Ghoneim,

and Abdelati Hawwari. 2015. A pilot study on Ara-
bic multi-genre corpus diacritization. In Proceed-
ings of the Second Workshop on Arabic Natural Lan-
guage Processing.

Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.

Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational linguistics, 19(2):263–311.

Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017. Semeval-2017
task 1: Semantic textual similarity-multilingual and
cross-lingual focused evaluation. In SemEval work-
shop at ACL.

Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 176–181.

Arthur P Dempster, Nan M Laird, and Donald B Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the royal sta-
tistical society., 39:1–38.

Mona Diab, Mahmoud Ghoneim, and Nizar Habash.
2007. Arabic diacritization in the context of sta-
tistical machine translation. In Proceedings of MT-
Summit.

Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel cor-
pora. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
255–262.

Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Re-
ichart. 2018. The hitchhikers guide to testing statis-
tical significance in natural language processing. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics, volume 1,
pages 1383–1392.

Chris Dyer, Victor Chahuneau, and Noah A Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 644–648.

Ignatius Ezeani, Mark Hepple, and Ikechukwu
Onyenwe. 2017. Lexical disambiguation of Igbo us-
ing diacritic restoration. In Proceedings of the 1st
Workshop on Sense, Concept and Entity Represen-
tations and their Applications, pages 53–60.

Ronald Aylmer Fisher. 1935. The design of experi-
ments. Oliver And Boyd.

http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1607.04606v2
http://arxiv.org/abs/1607.04606v2
http://arxiv.org/abs/1708.00055
http://arxiv.org/abs/1708.00055
http://arxiv.org/abs/1708.00055


58

Yvette Graham and Timothy Baldwin. 2014. Testing
for significance of increased correlation with human
judgment. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 172–176.

Weiwei Guo and Mona Diab. 2012. Modeling sen-
tences in the latent space. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics, pages 864–872.

Weiwei Guo, Wei Liu, and Mona Diab. 2014. Fast
tweet retrieval with compact binary codes. In Pro-
ceedings of COLING 2014, the 25th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 486–496.

Tuka Al Hanai and James R Glass. 2014. Lexical
modeling for Arabic ASR: A systematic approach.
In Fifteenth Annual Conference of the International
Speech Communication Association.

Ehab W Hermena, Denis Drieghe, Sam Hellmuth, and
Simon P Liversedge. 2015. Processing of Arabic
diacritical marks: Phonological–syntactic disam-
biguation of homographic verbs and visual crowding
effects. Journal of Experimental Psychology: Hu-
man Perception and Performance, 41(2).

Tapas Kanungo, David M Mount, Nathan S Netanyahu,
Christine D Piatko, Ruth Silverman, and Angela Y
Wu. 2002. An efficient k-means clustering algo-
rithm: Analysis and implementation. IEEE Trans-
actions on Pattern Analysis & Machine Intelligence,
pages 881–892.

G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M.
Rush. 2014. OpenNMT: Open-Source Toolkit for
Neural Machine Translation. In Proceedings of ACL
2017, System Demonstrations.

Sebastian Löbner. 2013. Understanding semantics.
Routledge.

Cettolo Mauro, Girardi Christian, and Federico Mar-
cello. 2012. Wit3: Web inventory of transcribed and
translated talks. In Conference of European Associ-
ation for Machine Translation, pages 261–268.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space.

Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003.
Exploiting parallel texts for word sense disambigua-
tion: an empirical study. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics, pages 455–462.

Minh Trung Nguyen, Quoc Nhan Nguyen, and
Hong Phuong Nguyen. 2012. Vietnamese diacritics
restoration as sequential tagging. In IEEE RIVF In-
ternational Conference on Computing Communica-
tion Technologies, Research, Innovation, and Vision
for the Future, pages 1–6.

Joakim Nivre, Marie-Catherine De Marneffe, Filip
Ginter, Yoav Goldberg, Jan Hajic, Christopher D
Manning, Ryan McDonald, Slav Petrov, Sampo
Pyysalo, Natalia Silveira, Reut Tsarfaty, and Daniel
Zeman. 2017. A Bambara tonalization system for
word sense disambiguation using differential cod-
ing, segmentation and edit operation filtering. In
Proceedings of the Eighth International Joint Con-
ference on Natural Language Processing, volume 1,
pages 694–703.

Joakim Nivre, Marie-Catherine De Marneffe, Filip
Ginter, Yoav Goldberg, Jan Hajic, Christopher D
Manning, Ryan T McDonald, Slav Petrov, Sampo
Pyysalo, Natalia Silveira, et al. 2016. Universal
dependencies v1: A multilingual treebank collec-
tion. In Proceedings of the Tenth International
Conference on Language Resources and Evaluation
(LREC).

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th annual meeting on association for com-
putational linguistics, pages 311–318.

Arfath Pasha, Mohamed Al-Badrashiny, Mona T Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow, and Ryan Roth.
2014. MADAMIRA: A fast, comprehensive tool for
morphological analysis and disambiguation of Ara-
bic. In LREC, volume 14, pages 1094–1101.

Maria Pelevina, Nikolay Arefyev, Chris Biemann, and
Alexander Panchenko. 2017. Making sense of word
embeddings. In Proceedings of the 1st Workshop on
Representation Learning for NLP on Association for
Computational Linguistics.

Nils Reimers and Iryna Gurevych. 2017. Report-
ing score distributions makes a difference: perfor-
mance study of LSTM-networks for sequence tag-
ging. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 338–348.

Ahmed Said, Mohamed El-Sharqwi, Achraf Chalabi,
and Eslam Kamal. 2013. A hybrid approach for
Arabic diacritization. In International Conference
on Application of Natural Language to Information
Systems, pages 53–64.

Helmut Schmid. 1999. Improvements in part-of-
speech tagging with an application to German. In
Natural Language Processing Using Very Large
Corpora, pages 13–25.

Dan Tufiş and Adrian Chiţu. 1999. Automatic diacrit-
ics insertion in Romanian texts. In Proceedings of
the International Conference on Computational Lex-
icography COMPLEX, volume 99, pages 185–194.

Cătălin Ungurean, Dragoş Burileanu, Vladimir
Popescu, Cristian Negrescu, and Aurelian Dervis.

http://arxiv.org/abs/1701.02810
http://arxiv.org/abs/1701.02810
http://arxiv.org/abs/1301.3781
http://arxiv.org/abs/1301.3781
http://arxiv.org/abs/1708.03390
http://arxiv.org/abs/1708.03390
http://arxiv.org/abs/1707.09861
http://arxiv.org/abs/1707.09861
http://arxiv.org/abs/1707.09861
http://arxiv.org/abs/1707.09861


59

2008. Automatic diacritic restoration for a TTS-
based e-mail reader application. UPB Scientific
Bulletin, Series C, 70(4):3–12.

David Yarowsky. 1994. Decision lists for lexical ambi-
guity resolution: application to accent restoration in
Spanish and French. In Proceedings of the 32nd an-
nual meeting on Association for Computational Lin-
guistics, pages 88–95.

Wajdi Zaghouani, Houda Bouamor, Abdelati Hawwari,
Mona T Diab, Ossama Obeid, Mahmoud Ghoneim,
Sawsan Alqahtani, and Kemal Oflazer. 2016a.
Guidelines and framework for a large scale Ara-
bic diacritized corpus. In The Tenth International
Conference on Language Resources and Evaluation
(LREC), page 36373643.

Wajdi Zaghouani, Abdelati Hawwari, Sawsan Alqah-
tani, Houda Bouamor, Mahmoud Ghoneim, Mona
Diab, and Kemal Oflazer. 2016b. Using ambigu-
ity detection to streamline linguistic annotation. In
Proceedings of the Workshop on Computational Lin-
guistics for Linguistic Complexity (CL4LC), pages
127–136.


