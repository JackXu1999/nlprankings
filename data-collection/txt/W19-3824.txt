



















































On Measuring Gender Bias in Translation of Gender-neutral Pronouns


Proceedings of the 1st Workshop on Gender Bias in Natural Language Processing, pages 173–181
Florence, Italy, August 2, 2019. c©2019 Association for Computational Linguistics

173

On Measuring Gender Bias in Translation of Gender-neutral Pronouns

Won Ik Cho1, Ji Won Kim2, Seok Min Kim1, and Nam Soo Kim1
Department of Electrical and Computer Engineering and INMC1

{wicho,smkim}@hi.snu.ac.kr, nkim@snu.ac.kr
Department of Linguistics2

kimjiwon08@snu.ac.kr
Seoul National University

1 Gwanak-ro, Gwanak-gu, Seoul, Korea, 08826

Abstract

Ethics regarding social bias has recently
thrown striking issues in natural language pro-
cessing. Especially for gender-related topics,
the need for a system that reduces the model
bias has grown in areas such as image caption-
ing, content recommendation, and automated
employment. However, detection and evalua-
tion of gender bias in the machine translation
systems are not yet thoroughly investigated,
for the task being cross-lingual and challeng-
ing to define. In this paper, we propose a
scheme for making up a test set that evalu-
ates the gender bias in a machine translation
system, with Korean, a language with gender-
neutral pronouns. Three word/phrase sets are
primarily constructed, each incorporating pos-
itive/negative expressions or occupations; all
the terms are gender-independent or at least
not biased to one side severely. Then, addi-
tional sentence lists are constructed concern-
ing formality of the pronouns and politeness
of the sentences. With the generated sentence
set of size 4,236 in total, we evaluate gender
bias in conventional machine translation sys-
tems utilizing the proposed measure, which is
termed here as translation gender bias index
(TGBI). The corpus and the code for evalua-
tion is available on-line1.

1 Introduction

Gender bias in natural language processing (NLP)
has been an issue of great importance, especially
among the areas including image semantic role la-
beling (Zhao et al., 2017), language modeling (Lu
et al., 2018), and coreference resolution (Lu et al.,
2018; Webster et al., 2018). Along with these, the
bias in machine translation (MT) was also claimed
recently regarding the issue of gender dependency
in the translation incorporating occupation (Prates
et al., 2018; Kuczmarski and Johnson, 2018). That

1https://github.com/nolongerprejudice/tgbi

Figure 1: Occupation gender bias shown in some KR-
EN (Korean-English) translation systems. Note that un-
like this figure, Yale romanization is utilized in the rest
of this paper.

is, the prejudice within people, e.g., cops are usu-
ally men or nurses are usually women, which is
inherent in corpora, assigns bias to the MT models
trained with them.

State-of-the-art MT systems or the ones in ser-
vice are based on large-scale corpora that incorpo-
rate various topics and text styles. Usually, sen-
tence pairs for training are fed into the seq2seq
(Sutskever et al., 2014; Bahdanau et al., 2014) or
Transformer (Vaswani et al., 2017)-based models,
where the decoding process refers to thought vec-
tor of the source data to infer a plausible transla-
tion (Cho et al., 2014). Under some circumstances,
this may incur an association of gender-specified
pronouns (in the target) and gender-neutral ones
(in the source) for lexicon pairs that frequently col-
locate in the corpora. We claim that this kind of
phenomenon seriously threatens the fairness of a
translation system, in the sense that it lacks gener-
ality and inserts social bias to the inference. More-
over, the output is not fully correct (considering
gender-neutrality) and might offend the users who
expect fairer representations.

The aforementioned problem exists in Korean
as well (Figure 1). To look more into this, here
we investigate the issue with the gender-neutral



174

pronouns in Korean, using the lexicons regarding
sentiment and occupation. We provide the sen-
tences of the template “걔는 [xx]-해 (kyay-nun
[xx]-hay), S/he is [xx] ”, as in Prates et al. (2018),
to the translation system, and evaluate the bias ob-
served from the portion of pronouns being trans-
lated into female/male/neither. Here, [xx] denotes
either a sentiment word regarding one’s judgment
towards the third person (polar), or an occupa-
tion word (neutral). Since kyay in Korean, which
refers to s/he, is gender-neutral thus the transla-
tion output of the sentence becomes either “She
is [xx]”, “He is [xx]”, or “The person is [xx]”.
Although the expressions as used in the last out-
put are optimal, they are not frequently utilized
in conventional translation systems. Also, such re-
sult is difficult to be mechanically achieved since
transforming all the gender-related pronouns to the
neutral ones may cause loss of information, in the
circumstances where the context is given (e.g., To
tell you one thing about her, [she] is [xx]).

In this study, we collect a lexicon set of the size
of 1,059 for the construction of an equity evalua-
tion corpus (EEC) (Kiritchenko and Mohammad,
2018), specifically 324 sentiment-related phrases
and 735 occupation words. For each sentence of
the above template containing a lexicon, along
with an alternative pronoun (formal version) and
a politeness suffix (on/off), we eventually obtain
4,236 utterances to make up the EEC. We claim
the following as contributions of this paper:

• Construction of a corpus with template
sentences that can check the preservation
of gender-neutrality in KR-EN translation
(along with a detailed guideline)
• A measure to evaluate and compare the per-

formance of translation systems regarding the
preservation of gender neutrality of pronouns
• Rigorous contemplation on why the preserva-

tion of gender neutrality has to be guaranteed
in translation

In the following sections, after an introduction
to the literature, we describe how we made up the
corpus, and how it is utilized in evaluating the con-
ventional machine translation systems in service.

2 Related Work

It is essential to clarify the legitimate ground for
the necessity of mitigating gender bias in machine
learning models. For example, Binns (2017) sug-
gests that it should be considered as a problem of

individuality and context, rather than of statistics
and system. The paper poses a question on the fair-
ness of fairness utilized in fair machine learning,
and concludes that the fairness issue in algorithmic
decision-making should be treated in a contextu-
ally appropriate manner, along with the points that
may hinge on the factors which are not typically
present in the data available in situ. Although lit-
tle study has been undertaken in the field of ethics
in translation, we have plentiful research on the
call for mitigation of gender bias in NLP models.

One of them is image semantic role labeling, as
suggested in Zhao et al. (2017). It is claimed that
due to the bias in the image/caption pairs that as-
sociate specific verb/mood with a specific gender,
e.g., warm tone kitchen and cooking with women;
the trained model infers the wrong gender in the
captioning of some images. The primary reason is
assumed to be a lack of data with cooking males
in warm tone kitchen. However, since data aug-
mentation for all the imbalance is costly and not
promising, the paper proposes giving a constraint
in the training phase in the way of disassociating
verbs and gender information.

Other areas where gender bias is observed are
classification and recommendation, as represented
in a recent article2; in Amazon AI recruiting, the
system came out to recommend the applicants who
had sufficient work experience in the field, in most
cases male. This incident does not merely mean
that the data concerning female occupies much
smaller volume than male; it also conveys that
so-called “good” applicants were selected in per-
spective of choosing experienced and industrious
workers who might have been less forced to devote
their time to housework or childcare. However, it
is questionable that forcing the dataset to be bal-
anced by making the portion of female employ-
ment half is a sound solution. Instead, this is about
disentangling the factors that are less directly re-
lated to working ability.

Above kind of disentanglement is required as
well in the area of inference; for instance, a shared
task of GenderBiasNLP3. For such a task, re-
searchers find how contextual factors can be dis-
associated with gender information. In this pa-
per, a similar problem is discussed in cross-lingual
perspective. Along with the articles that pose the

2https://www.reuters.com/article/us-amazon-com-jobs-
automation-insight/amazon-scraps-secret-ai-recruiting-tool-
that-showed-bias-against-women-idUSKCN1MK08G

3https://www.kaggle.com/c/gendered-pronoun-resolution



175

problem4, some studies have been done in an
empirical viewpoint (as coreference resolution in
translation) including Kuczmarski and Johnson
(2018). In Prates et al. (2018) which is the clos-
est to this work, twelve languages are investi-
gated with about 1,000 occupations and 21 adjec-
tives, with a template sentence, demonstrating a
strong male dependency within Google translator.
However, albeit its syntax being similar to that of
Japanese, Korean was omitted due to some techni-
cal reasons. Here, we make the first known attempt
to create a concrete scheme for evaluating the gen-
der bias of KR-EN translation systems regarding
sentiment words and occupations, and propose a
measure for an inter-system comparison. Also, we
state that mitigated male dependency does not nec-
essarily mean that the system bias has reduced,
rather it can imply that another social bias has been
involved.

3 Proposed Method

In this section, we describe how the EEC is created
and how it is utilized in evaluating gender bias in
the MT models.

3.1 Corpus generation

The corpus generation scheme can be compared
with Burlot and Yvon (2017) in the sense that var-
ious morpho-syntactic/semantic features are taken
into account. However, here we focus more on
making the template sentences help discern the
gender bias regarding the translation of gender-
neutral pronouns.

3.1.1 Gender-neutral pronouns
The first thing to be clarified is the distinction of
gender-neutral words in Korean. Unlike some lan-
guages such as German, the Korean language does
not incorporate grammatical gender. However, for
the third person pronouns, there exist ‘그녀 (ku-
nye), she’ and ‘그 (ku), he’, which are clearly
gender-specific. Therefore, in some cases, to avoid
specifying a gender (e.g., in case the speaker asks
the addressee about a person whose gender is not
identified), the speakers use gender-neutral pro-
nouns such as ‘걔 (kyay), s/he’5, which is widely
used to indicate somebody that does not partici-

4https://qz.com/1141122/google-translates-gender-bias-
pairs-he-with-hardworking-and-she-with-lazy-and-other-
examples/

5An abbreviated form of ‘그애 (ku ay), the child’.

pate in the conversation (and who the speakers al-
together know).

Note that for a native speaker, kyay indicates
someone who is younger than or the same age as
the speaker, in an informal way. Thus, ‘그 사람
(ku salam), the person’ was adopted in this study
as well, as a variation of kyay to assign formal-
ity to the utterances. For both kyay and ku salam,
topic marker ‘은/는 (un/nun), is’ was agglutinated
to disambiguate the objectivity. In other words, all
the sentiment words or the occupations introduced
in the following paragraphs denote the property re-
garding the topic (the pronoun) of the sentence.

3.1.2 Sentiment words
Sentiment words in category of positive and neg-
ative polarity lexicons were collected from the
Korean Sentiment Word Dictionary published by
Kunsan National University6. The corpus is re-
ported to be constructed by majority voting of
at least three people. Among the total of 14,843
items including single words and phrases, we
only took roots into account, finally obtaining 124
and 200 items for positive and negative polarity
words. We selected not only single words such as
‘상냥한 (sangnyanghan), kind, positive’, but also
phrases such as ‘됨됨이가 뛰어난 (toymtoymika
ttwienan), be good in manner, positive’, some-
times including verb phrases such as ‘함부로말하
는 (hampwulo malhanun), bombard rough words,
negative’. Additional adverbs were not utilized in
the sentence generation.

In investigating the appropriateness of the sen-
timent words, two factors were considered: first,
does the sentiment word belong to the category of
the positive or negative lexicon? And second, does
it incorporate any prejudice if categorized into
positive or negative? For the first question, three
Korean native speakers examined the EEC and left
only the lexicons with the complete consensus.
For the second question, we removed the words
regarding appearance (e.g., pretty, tall), richness
(e.g., rich, poor), sexual orientation (e.g., ho-
mosexual), disability (e.g., challenged), academic
background (e.g., uneducated), occupation or sta-
tus (e.g., doctor, unemployed), etc. This was also
thoroughly checked.

3.1.3 Occupations
Occupation, which was not incorporated in the
previous section since assigning sentiment polar-

6http://dilab.kunsan.ac.kr/knusl.html



176

ity to it may not guarantee fairness, was taken into
account to form a separate corpus. We searched
for the official terms of each job and put them in
the template “S/he is [xx].”7. The occupation list
of size 735 was collected from an official govern-
ment web site for employment8 and was checked
for redundancy.

In choosing the occupations, gender-specificity
had to be concealed, which is disclosed in words
like “발레리노 (palleylino), ballerino” or “해녀
(haynye), woman diver”. Also, the words that con-
vey hate against specific groups of people were
omitted. By this, we made sure that the occupa-
tion words are free from sentiment polarity, even
though some may be listed in the sentiment word
dictionary.

3.1.4 Politeness suffix
Finally, the suffix “요 (yo)” was considered in as-
signing politeness to the sentences. It is usually at-
tached at the end of the sentence; if a straightfor-
ward attachment is not available, then the last part
of the sentence is transformed to guarantee the ut-
terance being polite. Overall, the criteria regard-
ing the construction scheme of the test set com-
prise three factors; formality, politeness, and po-
larity (occupation: neutral).

3.2 Measure
For any set of sentences S where each sentence
contains a pronoun of which the gender-neutrality
should be preserved in translation, let pw be the
portion of the sentences translated as female, pm
as male, and pn as gender-neutral9. Then we have
the following constraints:

pw + pm + pn = 1

0 ≤ pw, pm, pn ≤ 1
(1)

Consequently, by defining

Ps =
√
pwpm + pn (2)

we might be able to see how the translation is far
from guaranteeing gender neutrality. Note that the
measure is between 0 and 1, from constraint (1)10;

7Here, we notice that the Korean template differs regard-
ing the role of [xx]; if [xx] is noun phrase then the template
becomes “걔는 [xx]야 (kay-nun [xx]-ya)”, incorporating -ya
instead of -hay which fits with the modifiers.

8https://www.work.go.kr
9pw regards words such as she, her, woman, girl, and pm

regards he, him, man, guy, boy. Others including the person
were associated with pn.

10The proof is provided in the appendix A.

maximum when pn is 1 and minimum when ei-
ther pw or pm is 1. This condition matches with
the ideal goal of assigning gender-neutrality to
pronouns in context-free situations, and also with
the viewpoint that random guess of female/male
yields the optimum for a fixed pn.

For all the sentence sets namely S1 · · ·Sn and
the corresponding scores PS1 · · ·PSn , we define
the average value P = AV G(PSi) as a trans-
lation gender bias index (TGBI) of a translation
system, which yields 1 if all the predictions incor-
porate gender-neutral terms. Si can be associated
with whatever corpus that is utilized. Here, non-
weighted arithmetic average is used so that the
aspects investigated in each sentence set are not
overlooked for its small volume.

3.2.1 A remark on interpretation
At this point, we want to point out that two factors
should be considered in analyzing the result. The
first one is the bias caused by the volume of ap-
pearance in corpora (VBias), and the other is the
bias caused by the social prejudice which is pro-
jected in the lexicons utilized (SBias).

We assumed that VBias leans toward males and
that low pw might be obtained overall, which came
out to be generally correct. However, pw being rel-
atively high (among sentence sets) does not nec-
essarily mean that the bias is alleviated; rather
it can convey the existence of SBias, which as-
signs female-related translation for some senti-
ment words or occupations. In other words, we
cannot guarantee here that the translation sys-
tem that shows higher pw with a specific sentence
set substantiates their not being biased, consider-
ing both volume-related and social bias-related as-
pects.

3.2.2 Why the measure?
Despite the limitation of the proposed measure, as
explained above, we claim that using the measure
may be meaningful for some reasons. First, the
measure adopts square root function to reduce the
penalty of the result being gender-specific, taking
into account that many conventional translation
systems yield gender-specific pronouns as output.
Secondly, we evaluate the test result with the var-
ious sentence sets that comprise the corpus, not
just with a single set. This makes it possible for
the whole evaluation process to assess gender bias
regarding various topics. Finally, although it is un-
clear if the enhancement of PSi for some Si orig-



177

Sentence set [size] Google Translator (GT) Naver Papago (NP) Kakao Translator (KT)
(a) Informal [2,118] 0.4018 (0.2025, 0.0000) 0.3936 (0.1916, 0.0000) 0.1750 (0.0316, 0.0000)
(b) Formal[2,118] 0.0574 (0.0000, 0.0033) 0.0485 (0.0014, 0.0009) 0.0217 (0.0000, 0.0004)
(c) Impolite[2,118] 0.3115 (0.1062, 0.0023) 0.3582 (0.1506, 0.0004) 0.1257 (0.0155, 0.0004)

(d) Polite[2,118] 0.2964 (0.0963, 0.0009) 0.2724 (0.0807, 0.0000) 0.1256 (0.0160, 0.0000)
(e) Negative [800] 0.3477 (0.1362, 0.0037) 0.1870 (0.0350, 0.0012) 0.1311 (0.0175, 0.0000)
(f) Positive [496] 0.4281 (0.2358, 0.0040) 0.2691 (0.0786, 0.0000) 0.1259 (0.0161, 0.0000)

(g) Occupation [2,940] 0.2547 (0.0690, 0.0006) 0.2209 (0.0496, 0.0017) 0.1241 (0.0153, 0.0003)
Average 0.2992 0.2499 0.1184

Table 1: The overall evaluation result for three conventional KR-EN translation systems. Note that the values for
the sentence sets (a-g) denote Ps (pw, pn) for each sentence set S. The bold lines denote the sentence set with
which each translator shows the highest score.

inates in relieved VBias or inserted SBias, the av-
eraged value P is expected to be used as a repre-
sentative value for inter-system comparison, espe-
cially if the gap between the systems is noticeable.

4 Evaluation

For evaluation we investigate seven sentence sets
in total, namely (a) informal, (b) formal, (c) impo-
lite, (d) polite, (e) negative, (f) positive, and (g) oc-
cupation. (a-d) contains 2,118 sentences each and
(e-g) contains 800, 496, and 2,940 each. The va-
lidity of investigating multiple sentence subsets is
to be stated briefly in the appendix B.

In this study, we evaluate three conventional
translation systems in service, namely Google
translator (GT)11, Naver Papago12 (NP), and
Kakao translator (KT)13. Overall, GT scored the
highest and KT the lowest. We conduct additional
analysis to catch the meaning beyond the numbers.

4.1 Quantitative analysis

VBias is primarily assumed to be shown by pm
dominating the others (Table 1). However, in some
cases, VBias is attenuated if SBias is projected into
the translation output in the way of heightening
pw.

4.1.1 Content-related features
Considering the result with the sentence sets (e-g)
which are content-related, the tendency turned out
to be different by the systems; GT and NP show
relatively low score with positive sentiment words
and KT with negative sentiment words. We sus-
pected at first that the negative sentiment words
would be highly associated with translation into a

11https://translate.google.com/
12https://papago.naver.com/
13https://translate.kakao.com/

female, but the result proves otherwise. Instead, in
GT and NP, (f) the positive case shows relatively
more frequent inference as female compared with
(e) the negative case, although the absolute value
suggests that there exists VBias towards men.

For all the systems, intra-system analysis
demonstrates that the result regarding occupation
is more biased than the others. Except for NP, the
social bias inserted in the models seems to lower
the score regarding (g). This is to be investigated
more rigorously in the qualitative analysis.

4.1.2 Style-related features

The sentences in the set (b), with formal and
gender-neutral pronouns, turned out to be signif-
icantly biased to male compared with (a) the in-
formal cases, which was beyond our expectations.
From this, we could cautiously infer that corpora
incorporating relatively formal expressions (such
as news article, technical report, papers, etc.) gen-
erally associate the sentiment or occupation words
with males. With respect to politeness, the systems
did not show a significant difference between (c)
and (d). We infer that the politeness factor does
not affect the tendency of translation much since
it is largely related to the colloquial expressions
which might not have been employed in the train-
ing session.

The result regarding formality reminds us of the
phenomenon which has been discerned in the chal-
lenge on author profiling (Martinc et al., 2017),
that the formal style is known to be predictive for
identifying male authors. Undoubtedly, the iden-
tity of a writer is not direct evidence of s/he uti-
lizing the expressions biased to specific gender in
writing. However, a tendency has been reported
that the male writers frequently assume or refer to
male subject and topic, either unconsciously or to



178

follow the convention, in the formal writing (Arg-
amon et al., 2003). Also, it cannot be ignored that
the males are more engaged in formal writing in
the real world14, accounting for a large portion
of the corpora. Thus, it seems not unreasonable
to claim that, although controversial, the positive
correlation between the formal text style and the
male-related translation might have been affected
by the occupation gender ratio15.

The result regarding sentence style-related fea-
tures shows that in giving constraint to prevent the
association of gender-related features and contents
while training, at least in KR-EN translation, the
formality of expressions should not be neglected
since it is largely influenced by the context in the
corpora where the expressions and contents be-
long to, and even real world factors. Politeness
turned out not to be a severe influence, but the po-
liteness suffix can still reflect the relationship be-
tween the speakers, affecting the type of conversa-
tion that takes place.

4.2 Qualitative analysis
In brief, translation into male dominates due to
the bias in volume, and social bias is represented
mainly in the way of informal pronouns being
translated into a female with relative frequency,
although the content-related features do not neces-
sarily prove it to be so. However, qualitative evalu-
ation is indispensable for a comprehensive under-
standing of the bias since the quantitative result
only informs us of the number, not the semantics.

The most significant result was that of occupa-
tions. GT reflects the bias that is usually intimated
by people (e.g., experts such as engineers, techni-
cians, professors are usually men, and art/beauty-
related positions such as fashion designer, hair-
dresser are mainly held by women), and KT shows
the volume dominance of male in the corpus (over-
all score lower than GT and NP in Table 1), with
rare female cases related to design or nursing.
As stated in Section 4.1, we interpreted the re-
sult regarding GT as permeated SBias attenuating
VBias, and KT as VBias not attenuated.

In analyzing the result for NP, we observed
some unexpected inferences such as researchers
and engineers significantly being translated into
female pronoun and cuisine-related occupations
into male, which is different from social prejudice

14E.g., journalists, engineers, researchers; considering the
gender ratio statistics.

15As in https://www.bls.gov/cps/cpsaat11.htm

posed by GT or KT. We assume this phenomenon
as a result of technical modification performed by
NP team to reduce the gender bias in translat-
ing pronouns regarding occupations. The modifi-
cation seems to mitigate both VBias and SBias in
a positive way, although the final goal should be
a guaranteed utilization of gender-neutral expres-
sions rather than a half-half guess.

4.3 Comparison with other schemes

Although the scope and aim do not precisely over-
lap, we find it beneficial for our argument to com-
pare the previous studies with ours. In Kuczmarski
and Johnson (2018), the paper mainly aims to
perform post-processing that yields gender non-
biased result in pronoun translation for Turkish,
but no specific description of the evaluation was
accompanied. In Lu et al. (2018), a score function
on evaluating bias was suggested as a portion of
matched pair among masked occupations. How-
ever, the task was not on translation, and we es-
chew using the suggested type of linear computa-
tion so as to avoid the arithmetic average (TGBI)
not revealing the different performance on various
sentence sets. Most recently, Prates et al. (2018)
utilized a heat map regarding occupation and ad-
jectives in 12 languages to evaluate Google trans-
lator. They computed the p-values relative to the
null hypothesis that the number of translated male
pronouns is not significantly higher than that of fe-
male pronouns, with a significance level of α = .05.
They obtained the outliers in Finnish, Hungarian,
and Basque notably, but the study on Korean was
omitted, and the work only incorporates a single
sentence style, probably for simplicity.

Note that the aforementioned approaches are
not aimed to evaluate multiple translation systems
quantitatively, and omit the portion of gender-
neutral pronouns in translation output; which are
the strong points of utilizing the proposed EEC
and measure for the evaluation of translation gen-
der bias. Also, we take into account both VBias
and SBias in the analysis, of which neither side
should be underestimated. For example, someone
might assume that occupation gender bias is more
severe in NP than GT since the absolute numerics
say so (regarding (g) in Table 1). However, such
a conclusion should be hesitantly claimed since it
is highly probable that GT inherits another kind of
bias (from the corpora) that attenuates the VBias
on males as demonstrated in Section 4.2. Our ap-



179

proach aims to make it possible for the evalua-
tors (of the MT systems) to comprehend how the
bias is distributed and to perform an inter- or intra-
system comparison, by providing the various sen-
tence sets of which the corresponding scores rep-
resent content- and style-related aspects of trans-
lation gender bias.

4.4 Discussion

We do not claim here that the model which yields
the translation of test utterances being biased to
one gender is a biased translator, nor that the dis-
tribution of gender-related content in the corpora
should be half-half. However, since we decided to
investigate only the gender non-specific pronouns,
sentiment words, and occupations, so that the gen-
erated sentences hardly incorporate any context
that determines the pronouns to be one specific
gender, we claim that the translation is recom-
mended to contain each gender as equally as pos-
sible for the sentence sets that are constructed, or
use neutral pronouns if available. This is not about
making up a mechanical equality, but about avoid-
ing a hasty guess if the inference is not involved
with a circumstance that requires the resolution of
coreference.

For the user’s sake, Google translator recently
added the service on providing the result contain-
ing both genders as answer if the gender ambigu-
ity is detected16 (Kuczmarski and Johnson, 2018).
This is the first known approach in service that
mitigates gender bias in translation. We are en-
couraged to face this kind of change, although it
is tentative. In the long term, we hope the trans-
lators print random or gender-neutral answers for
the argued type of (or possibly various other kinds
of) sentences.

Another important point is that the systems also
have to distinguish the circumstances that require
a random guess from the ones that gender should
be specified. For example, with a sentence “걔
는 생리중이야 (kyay-nun sayngli-cwung-iya)17”,
GT yields “He’s on a period.”, which is physio-
logically unreasonable. Moreover, the resolution
of coreferences in long-term dependency with the
specified gender is required for a correct transla-
tion of the sentences with context.

16https://blog.google/products/translate/reducing-gender-
bias-google-translate/

17생리중 (saynglicwung) denotes to be in one’s menstrual
period, which matches only if kyay is translated into a female-
related term.

In response to concern on this study being
language-specific, we want to note that the pro-
posed methodology can be applied to other lan-
guages with gender-neutral pronouns, especially
with a high similarity if the source language con-
tains both a formality and politeness-related lex-
icons (e.g., Japanese). The extensibility regard-
ing the source language has recently been dis-
played in Prates et al. (2018), and in this paper,
a further and detailed experiment was conducted
with a language that had not been investigated.
For the cases of the target being non-English, we
assume that the tendency depends on the pres-
ence of gender-neutral pronouns in the target lan-
guage; in our pilot study utilizing Japanese as a
target, the gender-neutrality of the Korean pro-
nouns were preserved mostly in the translation.
However, even for the cases where the target lan-
guage incorporates gender-neutral pronouns, the
proposed scheme is useful since the measure re-
flects the preservation of the gender-neutrality. De-
spite the difficulty of a typological approach re-
garding generalization, our study is relevant for a
broader audience if the source language being an-
alyzed fits the condition above.

5 Conclusion

In this paper, we introduced a test corpus and
measure for the evaluation of multiple KR-EN
translation systems. A criteria set for choosing
the pronouns, lexicons, and markers was stated
in detail, making up a corpus of size 4,236 and
seven sentence subsets regarding (in)formality,
(im)politeness, sentiment polarity, and occupation.
The measurement was performed by averaging
PS for each sentence subsets where PS denotes√
pwpm + pn for pw, pm and pn each the por-

tion of the sentences with pronouns translated into
female/male/gender-neutral terms respectively.

Among the three candidates, Google Transla-
tor scored the highest overall, albeit the qualita-
tive analysis says that an algorithmic modification
seems to be implemented in Naver Papago consid-
ering the result regarding occupations. Although
Kakao Translator scored the lowest, the low score
here does not necessarily mean that the translator
malfunctions. In some sense, a well-biased trans-
lator is a well-performing translator that reflects
the inter-cultural difference. However, we believe
that the bias regarding gender should be reduced
as much as possible in the circumstances where



180

the gender specification is not required.
Our future work includes making up a post-

processing system that detects the presence of con-
text and assigning gender specificity/neutrality to
the pronouns in the translation. Though we hesi-
tate to claim that it is the best solution, such an
approach can be another step to alleviating the am-
plification of gender bias in cross-lingual tasks. Si-
multaneously, we aim to have an in-depth analy-
sis in the architecture or model behavior regarding
training datasets, with an extended test set that en-
compasses contextual inference, to find out how
each MT system performs better than others in
some aspects.

Acknowledgement

This research was supported by Projects for Re-
search and Development of Police science and
Technology under Center for Research and De-
velopment of Police science and Technology and
Korean National Police Agency funded by the
Ministry of Science, ICT and Future Planning
(PA-J000001-2017-101). Also, this work was sup-
ported by the Technology Innovation Program
(10076583, Development of free-running speech
recognition technologies for embedded robot sys-
tem) funded by the Ministry of Trade, Industry
& Energy (MOTIE, Korea). The authors appre-
ciate helpful comments from Ye Seul Jung and
Jeonghwa Cho. After all, the authors send great
thanks to Seong-hun Kim for providing a rigorous
proof for the boundedness of the proposed mea-
sure.

References
Shlomo Argamon, Moshe Koppel, Jonathan Fine, and

Anat Rachel Shimoni. 2003. Gender, genre, and
writing style in formal written texts. Text-The Hague
Then Amsterdam Then Berlin-, 23(3):321–346.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Reuben Binns. 2017. Fairness in machine learning:
Lessons from political philosophy. arXiv preprint
arXiv:1712.03586.

Franck Burlot and François Yvon. 2017. Evaluating
the morphological competence of machine transla-
tion systems. In Proceedings of the Second Confer-
ence on Machine Translation, pages 43–55.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Svetlana Kiritchenko and Saif M Mohammad. 2018.
Examining gender and race bias in two hun-
dred sentiment analysis systems. arXiv preprint
arXiv:1805.04508.

James Kuczmarski and Melvin Johnson. 2018.
Gender-aware natural language translation.

Kaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Aman-
charla, and Anupam Datta. 2018. Gender bias in
neural natural language processing. arXiv preprint
arXiv:1807.11714.

Matej Martinc, Iza Skrjanec, Katja Zupan, and Senja
Pollak. 2017. Pan 2017: Author profiling-gender
and language variety prediction. In CLEF (Working
Notes).

Marcelo OR Prates, Pedro HC Avelar, and Luis Lamb.
2018. Assessing gender bias in machine translation–
a case study with google translate. arXiv preprint
arXiv:1809.02208.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Kellie Webster, Marta Recasens, Vera Axelrod, and Ja-
son Baldridge. 2018. Mind the gap: A balanced
corpus of gendered ambiguous pronouns. Transac-
tions of the Association for Computational Linguis-
tics, 6:605–617.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. 2017. Men also
like shopping: Reducing gender bias amplifica-
tion using corpus-level constraints. arXiv preprint
arXiv:1707.09457.

Appendix

A Proof on the boundedness of the
measure

Let W0, x, y, z each denote Ps, pw, pm, pn. Then,
from eqn.s (1-2) in the paper, we have

x+ y + z = 1

0 ≤ x, y, z ≤ 1
(3)



181

and
W0 =

√
xy + z (4)

Here, note that we have to show the bound for the
following:

W (x, y, z) = xy + z (5)

Let
D = {(x, y, z)|x+ y + z = 1,

0 ≤ x, y, z ≤ 1}
(6)

which is a compact, convex set, and let a La-
grangian L of W be

L = xy + z + λ(x+ y + z − 1)
−µxx− µyy − µzz

(7)

Then, the KKT conditions for optimizing L are
given by

∂L

∂x
= y + λ± µx = 0

∂L

∂y
= x+ λ± µy = 0

∂L

∂z
= 1 + λ± µz = 0

(8)

where µx, µy, µz ≥ 0 and µxx∗ = µyy∗ =
µzz∗ = 0 for an optimal point (x∗, y∗, z∗).

If the optimal point lies in the interior ofD, then
µx = µy = µz = 0. Thus, in the optimal point, to
make ∂L∂z = 0, we have λ = −1. Thereby, to make
∂L
∂x =

∂L
∂y = 0, we have x = y = 1 which makes

z = 1 that contradicts eqn. (3).
Consequently, the optimal points lie on the

boundary of D which can be decomposed into the
following three independent segments:

(a){x+ y = 1, z = 0}
(b){y + z = 1, x = 0}
(c){z + x = 1, y = 0}

(9)

At most two of (9) can be satisfied.
For (a), optimizing L1 = xy subject to x+ y =

1 and x, y ≥ 0 yields

min = 0,max =
1

4
(10)

For (b) (and possibly (c)), optimizing L2 = z sub-
ject to y + z = 1 and y, z ≥ 0 yields

min = 0,max = 1 (11)

From eqn.s (9,10), we have 0 ≤ W ≤ 1 which
yields the boundedness of the proposed measure
W0. Moreover, we obtain that W0 is maximized if
pn = 1 and minimized if either pw or pm = 1.

Figure 2: A brief illustration on why equal distribution
is difficult to obtain for various subset pairs in deter-
ministic system. Best viewed in color.

B A brief demonstration on the utility of
adopting multiple sentence subsets

We want to recall that the conventional translation
services provide a determined answer to an input
sentence. This can happen to prevent the systems
from achieving a high score with the proposed
measure and EEC.

Let the emerald (top) and magenta (bottom)
discs in Figure 2 denote the gender-neutral pro-
nouns translated into female and male, respec-
tively. Note that for Si and Sj that comprise the
whole corpus, PSi and PSj are both high, whereas
for another sentence subset pair Sk and Sl, there
is high chance of PSk and PSl being lower than
the former ones. Thus, in conventional evaluating
schemes as in Lu et al. (2018), arithmetic averag-
ing may not be effective for displaying the amount
of bias.

This property could have deterred adopting the
proposed measure to multiple sentence subset
pairs (or triplets) since a similar mean value is ex-
pected to be obtained if the number of pairs in-
creases. However, since the utilization of pn and
square root function in the measure prevents the
average from being converged into a specific value
in the systems, we keep using all the sentence sets
that comprise the EEC so that we can observe
the tendency regarding various aspects of sociolin-
guistics.


