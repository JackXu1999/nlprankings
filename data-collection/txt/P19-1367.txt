



















































Vocabulary Pyramid Network: Multi-Pass Encoding and Decoding with Multi-Level Vocabularies for Response Generation


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3774–3783
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3774

Vocabulary Pyramid Network: Multi-Pass Encoding and Decoding with
Multi-Level Vocabularies for Response Generation

Cao Liu1,2, Shizhu He1, Kang Liu1,2, Jun Zhao1,2
1 National Laboratory of Pattern Recognition, Institute of Automation,

Chinese Academy of Sciences, Beijing, 100190, China
2 University of Chinese Academy of Sciences, Beijing, 100049, China

{cao.liu, shizhu.he, kliu, jzhao}@nlpr.ia.ac.cn

Abstract

We study the task of response generation.
Conventional methods employ a fixed vocab-
ulary and one-pass decoding, which not only
make them prone to safe and general respons-
es but also lack further refining to the first gen-
erated raw sequence. To tackle the above t-
wo problems, we present a Vocabulary Pyra-
mid Network (VPN) which is able to incor-
porate multi-pass encoding and decoding with
multi-level vocabularies into response genera-
tion. Specifically, the dialogue input and out-
put are represented by multi-level vocabular-
ies which are obtained from hierarchical clus-
tering of raw words. Then, multi-pass encod-
ing and decoding are conducted on the multi-
level vocabularies. Since VPN is able to lever-
age rich encoding and decoding information
with multi-level vocabularies, it has the po-
tential to generate better responses. Experi-
ments on English Twitter and Chinese Wei-
bo datasets demonstrate that VPN remarkably
outperforms strong baselines.

1 Introduction

As one of the long-term goals in AI and NLP, au-
tomatic conversation devotes to constructing auto-
matic dialogue systems to communicate with hu-
mans (Turing, 1950). Benefited from large-scale
human-human conversation data available on the
Internet, data-driven dialog systems have attracted
increasing attention of both academia and industry
(Ritter et al., 2011; Shang et al., 2015a; Vinyals
and Le, 2015; Li et al., 2016a,c, 2017).

Recently, a popular approach to build dialog
engines is to learn a response generation mod-
el within an encoder-decoder framework such
as sequence-to-sequence (Seq2Seq) model (Cho
et al., 2014a). In such a framework, an encoder
transforms the source sequence into hidden vec-
tors, and a decoder generates the targeted se-
quence based on the encoded vectors and previ-

Encoders

Decoders

Concepts

Meanings

Words

Distilled

Concepts

Coarse

Meanings

Grounding 

Words

Distilled

Concepts

Coarse

Meanings

Grounding 

Words

Multi-Pass Encoders

Multi-Pass
Decoders

Raw Words

High-Level

Encoder

Low-Level

Encoder

Raw-Word

Encoder

High-Level

Decoder

Low-Level

Decoder

Raw-Word

Decoder

Multi-Level
Vocabularies

High-Level

Clusters

Low-Level
Clusters

Figure 1: Vocabulary pyramid networks for response
generation. The dialogue input (context) and output
(response) are represented by multi-level vocabularies
(e.g., raw words, low-level clusters and high-level clus-
ters) and then processed by multi-pass encoder and de-
coder.

ously generated words. In this process, the en-
coder and decoder share a vocabulary (word list)1,
and the targeted words are typically performed by
a softmax classifier over the vocabulary word-by-
word.

However, such typical Seq2Seq model is prone
to generate safe and repeated responses, such as
“Me too” and “I don’t know”. In addition to
the exposure bias issue2, the main reasons of
this problem include: 1) a fixed (single) vocabu-
lary (word list) in decoding, which usually cov-
ers high-frequency words, so it is easy to cap-
ture high-frequency patterns (e.g., “Me too”) and
lose a great deal of content information in middle
and low-frequency patterns; 2) one-pass decoding,

1Encoder and decoder may have different word lists. We
find it performs closely using same or different vocabularies.

2A model generates the next word given the previous gold
words in training while it is based on previously predicted
words in the test (Ranzato et al., 2016).



3775

where word-by-word generation from left to right
is prone to error accumulation since previously
generated erroneous words will greatly affect fu-
ture un-generated words. More importantly, it can
leverage only the previously generated words but
not the future un-generated words.

In fact, there are some researches in text gener-
ation tasks such as dialogue generation, machine
translation and text summarization, are dedicated
to solving the above issues. In order to alleviate
issues on the fixed vocabulary, Wu et al. (2018a)
incorporated dynamic vocabulary mechanism in-
to Seq2Seq models, which dynamically allocates
vocabularies for each input by a vocabulary pre-
diction model. Xing et al. (2017) presented topic
aware response generation by incorporating top-
ic words obtained from a pre-trained LDA model
(Blei et al., 2003). Besides, several works attempt-
ed to solve the dilemma of one-pass decoding. X-
ia et al. (2017) proposed deliberation network for
sequence generation, where the first-pass decoder
generates a rough sequence and then the second-
pass decoder refines the rough sequence.

However, so far there has been no unified frame-
work to solve both of the aforementioned prob-
lems. In this study, we present Vocabulary Pyra-
mid Networks (VPN) to tackle the issues of one
fixed vocabulary and one-pass decoding simulta-
neously. Specifically, VPN incorporates multi-
pass encoding and decoding with multi-level vo-
cabularies into response generation. As depict-
ed in Figure 1, the multi-level vocabularies con-
tain raw words, low-level clusters and high-level
clusters, where low-level and high-level clusters
are obtained from hierarchical clustering of raw
words. Afterward, the multi-pass encoder (raw-
word encoder, low-level encoder, and high-level
encoder) gradually works on diminishing vocabu-
laries from raw words to low-level clusters until to
high-level clusters, and it looks like a “pyramid”
concerning the vocabulary size. On the other side,
the multi-pass decoder gradually increases the size
of processed vocabularies from high-level clusters
to low-level clusters and finally to raw words.

From a theoretical point of view, people usually
associate raw input words with low-level or high-
level abstractions like semantic meanings and con-
cepts on human-human conversations. Based on
the abstractive cognition, people organize contents
and select the expressive words as the response
(Xing et al., 2017). From a practical perspective,

VPN is able to capture much more sequence infor-
mation with multi-level vocabularies. As a result,
VPN has the potential to generate better responses.

To verify the effectiveness of the proposed mod-
el, we conduct experiments on two public response
generation datasets: English Twitter and Chinese
Weibo. Both automatic and manual evaluations
demonstrate that the proposed VPN is remarkably
better than the state-of-the-art.

2 Background

2.1 Sequence-to-Sequence Model
In Seq2Seq models (Cho et al., 2014a), an encod-
ing RNN (recurrent neural network) transforms
the source sequence X = {x1, x2, ..., xLX} into
distributed representations H = {h1,h1, ...,hLX}
through a basic model: ht = f(xt,ht−1). Here, xt
is the word embedding for xt, f is a non-linear
transformation, where GRU (Cho et al., 2014b)
and LSTM (Hochreiter and Schmidhuber, 1997)
are widely used for capturing long-term dependen-
cies. Then a decoder generates the targeted se-
quence Y = {y1, y2, ..., yLY } as follows:

st = f([yt−1, c], st−1) (1)
p(yt|y<t, X) = g(yt−1, c, st) (2)

where c = hLX , st is the decoding state in time
step t, and g is a non-linear function. In the ba-
sic Seq2Seq models, each word is generated from
a same context vector c. In order to capture d-
ifferent contexts for each generated word, atten-
tion mechanism (Bahdanau et al., 2015) extract-
s dynamic context vector ct in different decoding
time steps. Formally, ct =

∑LX
j=1 αijhj , αij ∝

exp(η(si−1,hj)), where η is a non-linear function.

2.2 Deliberation Network
Conventional Seq2Seq models can leverage on-
ly the generated words but not the un-generated
words in decoding, so they lack global informa-
tion to refine and polish the raw generated se-
quence. The deliberation network (Xia et al.,
2017) is proposed to deal with this issue. A delib-
eration network has two decoders, where the first-
pass decoder generates a raw word sequence Y 1 =
{y11, y12, ..., y1LY 1} and the second-pass decoder
polishes the raw word sequence. In the second-
pass decoder, an extra attention model is leveraged
to selectively read the output vector sequence Y 1

from the first-pass decoder, and then generate the
refined output sequence Y 2 = {y21, y22, ..., y2LY 2}.



3776

Seq2Seq

Encoder DecoderModel

Dynamic
Vocab Seq2Seq

Topic-Aware
Seq2Seq

Deliberate
Network

1rt Pass Dec.

2nd Pass Dec.

Multi-Pass 
Encoder

Multi-Pass 
Decoder

Vocabulary Pyramid Network(VPN)

High-Level
Dec.

Low-Level
Dec.

Low-Level
Enc.

High-Level
Enc.

Raw Words

Common Words

Topic Words

Dynamic Words

Low-level Clusters

High-level Clusters

Legend

Raw-Word Enc. Raw-Word Dec.

Figure 2: Differences in our VPN with typical Seq2Seq model and its variations, where different rectangles denote
different vocabularies (details in “Legend”). Seq2Seq uses a vocabulary (word list) in decoding. Dynamic vo-
cabulary Seq2Seq integrates a common vocabulary and a dynamic vocabulary in decoding. Topic-Aware Seq2Seq
incorporates topic words for each input. Deliberate network exploits first-pass and two-pass decoder within the
same vocabulary list. VPN employs multi-pass encoder and multi-pass decoder with multi-level vocabularies (raw
words, low-level clusters and high-level cluster). Among these models, only VPN makes use of vocabularies be-
yond words. Therefore, VPN could capture rich encoding and decoding information with multi-level vocabularies.

3 Methodology

3.1 Model Overview

As shown in Figure 2, VPN consists of three sub-
modules: multi-level vocabularies (Section 3.2),
multi-pass encoder (Section 3.3) and multi-pass
decoder (Section 3.4). Specifically, multi-level vo-
cabularies contain raw words, low-level clusters
and high-level clusters (black, blue and red solid
rectangles in Figure 2). The multi-pass encoder s-
tarts from the raw words and then to the low-level
clusters finally to the high-level clusters. In con-
trast, the multi-pass decoder works from the high-
level clusters to the low-level clusters until to the
raw words. The details of each component are in
the following.

3.2 Multi-Level Vocabularies

juniorsophomorefreshman amazingly surprisingly black

Figure 3: Multi-level vocabularies via hierarchical
clustering.

As illustrated in Figure 3, multi-level vocab-
ularies contain three different vocabularies: raw
words, low-level clusters and high-level clusters.
Specifically, the raw words are the original words

in the training data, and they are denoted as Vr =
{wr1, wr2, ..., wrR}. The raw words are agglomerat-
ed into low-level clusters Vl = {wl1, wl2, ..., wlL}
and high-level clusters Vh = {wh1 , wh2 , ..., whH}
by “bottom-up” hierarchical clustering. In order
to decide which clusters could be agglomerated,
we utilize the implementation of hierarchical clus-
tering in Scipy3. Specifically, we pre-train raw-
word embeddings by the word2vec model4 as in-
puts, and then we leverage the Ward (Ward, 1963)
linkage and maxclust5 criterion to automatically
construct hierarchical clustering.

In this way, we could obtain three different vo-
cabularies: Vr, Vl and Vh, where their vocabu-
lary sizes are decreased: |Vr|>|Vl|>|Vh|, and it
looks like a “pyramid” concerning the vocabulary
size. It should be emphasized that an original in-
put sequence could be expanded into three input
sequences through the three vocabulary lists, and
it is the same for the output sequence.

3.3 Multi-Pass Encoder

The encoder aims to transform input sequences in-
to distributional representations. In order to cap-
ture much more information from different input
sequences, VPN employs a multi-pass encoder,
which contains three different encoders in order:
raw-word encoder, low-level encoder and high-
level encoder. As a result, the multi-pass encoder
is able to encode more and more abstractive infor-

3https://scipy.org/
4Implemented in https://radimrehurek.com/gensim/mode-

ls/word2vec.html
5https://docs.scipy.org/doc/scipy-0.14.0/reference/gener-

ated/scipy.cluster.hierarchy.fcluster.html



3777

mation from words to clusters. The details are in
the following.

Raw-Word Encoder
The raw-word encoder accepts an input sequence
of word ids from raw words Vr. A bi-directional
LSTM (Schuster and Paliwal, 1997) is leveraged
to capture the long-term dependency from forward
and backward directions. The concatenated rep-
resentation of bi-directional hidden states: hri =
[
−→
h ri ,
←−
h rLi−i+1], is regarded as the encoded vector

for each input word. Finally, the input sequence is
transformed into a hidden state sequence:

Hr = {hr1,hr2, ...,hrLi} (3)

Specifically, the initiated hidden state is a zero
vector, and the hidden state (hrLi) in the last word
could be used for initiating the next encoder (low-
level encoder).

Low-Level Encoder
Low-level encoder is similar to the raw-word en-
coder. However, low-level encoder takes a se-
quence of low-level cluster ids from Vl as inputs,
and the hidden state is initiated by the last hidden
state of the raw-word encoder (hrLi). Similarly, we
can obtain the hidden state sequence in the low-
level encoder:

Hl = {hl1,hl2, ...,hlLi} (4)

High-Level Encoder
The high-level encoder accepts a sequence of
high-level cluster ids from Vh, and the initiated
hidden state is the final hidden state hlLi in the low-
level encoder. Finally, the hidden state sequence in
the high-level encoder is denoted as follows:

Hh = {hh1 ,hh2 , ...,hhLi} (5)

3.4 Multi-Pass Decoder
The decoder is responsible for generating targeted
sequences. Inspired from the deliberation network
(Xia et al., 2017), we present a multi-pass decoder
which consists of three decoders in order: high-
level decoder, low-level decoder and raw-word de-
coder. The three decoders have their own targeted
sequences from different vocabulary lists, and the
multi-pass decoder first generates the abstractive
(high and low-level) clusters and then generates
the raw (specific) words. It is different from the
deliberation network where both the first-pass de-
coder and the second-pass decoder aim to generate

raw words in the same vocabulary. The details of
our multi-pass decoder are in the following.

High-Level Decoder
The high-level decoder generates a high-level
cluster sequence from Vh. Similar to human-
human conversations, where people usually asso-
ciate an input message with high-level abstraction-
s like concepts in their minds before speaking, the
high-level decoder generates the most abstractive
cluster sequence before selecting specific words as
responses.

The high-level decoder is based on another L-
STM, which is initiated with the last hidden state
hhLi in the high-level encoder. In order to decide
which parts of sources need more attention, an at-
tention mechanism (Bahdanau et al., 2015) is in-
troduced in the high-level decoder. Intuitively, the
encoded hidden state sequence Hh in the high-
level encoder contains the most relevant encod-
ed information for the high-level decoder because
they share the same vocabulary Vh. Nevertheless,
in order to capture much more encoded informa-
tion from the source sequences, the high-level de-
coder adopts three attention models to attentive-
ly read different encoded state sequences: Hr, Hl
and Hh (Equation 3-5), respectively. Take Hr
as an example, at each decoding time step j, the
high-level decoder dynamically chooses the con-
text vector chrj based on H

r = {hr1,hr2, ...,hrLi}
and the decoding state shj−1 as follows:

chrj =
∑Li

i=1
αjihri ; α

ji =
eρ(s

h
j−1,h

r
i )∑

i′ e
ρ(shj−1,h

r
i′ )

(6)

where ρ is a non-linear function to compute the
attentive strength. Similarly, the attentive context
vectors (chlj and chhj ) from the low-level and high-
level encoders could be calculated by the attention
models. Based on chrj , chlj and chhj , the decoding
state shj is updated as:

shj = f
h([yhj−1, c

hr
j , c

hl
j , c

hh
j ], s

h
j−1) (7)

where yhj−1 is the embedding vector of the previ-
ously decoded cluster at time step j − 1, and fh
is the decoding LSTM unit. Finally, the target-
ed cluster is typically obtained by a softmax clas-
sifier over Vh based on the embedding similarity.
In this way, the high-level decoder could generate
the output sequence yh = {yh1 , yh2 , ..., yhLo}, which
corresponds to the embedding sequence:

Yh = {yh1 , yh2 , ..., yhLo} (8)



3778

Low-Level Decoder
Once the high-level cluster sequence is generated
from the high-level decoder, it could be leveraged
to the low-level decoder for further decoding the
low-level cluster sequence. Based on the three en-
coded state sequences (Hr,Hl,Hh) and the output
embedding sequence Yh of the high-level decoder,
the low-level encoder generates another sequence
from the low-level clusters Vl.

The low-level decoder is similar to the high-
level decoder. However, there still are some differ-
ences between them: 1) The initiated hidden state
sl0 in the low-level decoder is performed as the fi-
nal decoding state shLo in the high-level decoder. 2)
The attentive context vectors (clrj , cllj and clhj ) from
encoded state sequences are calculated with differ-
ent parameters compared to ones in the high-level
decoder. 3) Inspired from deliberation network-
s, previously generated sequence Yh in the high-
level encoder is fed into the low-level decoder,
where high-level (global) information guides low-
level generations, and another attention model is
leveraged to capture such information, which is
similar to Equation 6 mathematically:

olhj =
∑Lo

i=1
βjiyhi (9)

where the attentive weight βji is calculated from
the low-level decoding states slj−1 and output em-
bedding sequence Yh (Equation 8) in the high-
level decoder. Thereafter, olhj is concatenated to
update the decoded hidden state as follows:

slj = f
l([ylj−1, c

lr
j , c

ll
j , c

lh
j , o

lh
j ], s

l
j−1) (10)

where f l is another LSTM unit. Finally, the out-
put ylj is generated by a softmax classifier from Vl
based on embedding similarity.

Raw-Word Decoder
After obtaining the high-level and low-level clus-
ter sequence, the next step is to produce the final
raw word sequence from Vr by the raw-word de-
coder. The hidden state of the raw-word decoder
sh0 is initiated with the final decoding state s lLo in
the low-level decoder. The decoding state in the
raw-word decoder is updated as follows:

srj = f
r([yrj−1, c

rr
j , c

rl
j , c

rh
j , o

rl
j , o

rh
j ], s

r
j−1) (11)

where crrj , crlj , crhj are attentive context vectors
from three encoded hidden state sequences. orhj

and orlj (similar to Equation 9) are the weight-
ed sums of output embedding sequences from the
high-level decoder and low-level decoder. Simi-
larly, the targeted word is typically predicted by
a softmax classifier over Vr based on the word
embedding similarity. Eventually, the raw-word
decoder iteratively generates a targeted word se-
quence yr = {yr1, yr2, ..., yrLo}.

3.5 Learning
Multi-level vocabularies of hierarchical clustering
are obtained in advance through an un-supervised
way, while the multi-level encoder and decoder
could be optimized with supervised learning. The
encoder and decoder are totally differential, so
they are able to be optimized in an end-to-end
manner by the back propagation. Giving a source
input and a targeted output, there are three input-
output pairs obtained from different vocabulary
lists: {xn, yn}n∈{r,l,h}. Each output sequence cor-
responds to a training loss, and the total losses per-
form as follows:

L = Lh + Ll + Lr

Lh = −1
Lo

∑Lo
j=1

log[p(yhj |yh<j , xr, xl, xh)]

Ll = −1
Lo

∑Lo
j=1

log[p(ylj |yl<j , xr, xl, xh,Yh)]

Lr = −1
Lo

∑Lo
j=1

log[p(yrj |yr<j , xr, xl, xh,Yh,Yl)]

(12)

where the three negative log-likelihoods (Lh, Ll
and Lr) are losses for different-level targeted out-
puts. Yh and Yl are output embedding sequences
in the high-level decoder and low-level decoder,
respectively. Finally, the sum of different losses in
three decoders is considered as the total losses L.

4 Experiment

4.1 Datasets
There are large-scale message-response pairs on
social websites, which consist of informational
text from different topics (Chen et al., 2017). Our
experimental data comes from two public corpus:
English “Twitter”6 and Chinese “Weibo” (Shang
et al., 2015b). In order to improve the quality of
datasets, some noisy message-response pairs are
filtered (e.g., containing too many punctuations or
emoticons), and the datasets are randomly split in-
to Train/Dev/Test by a proportion (9:0.5:0.5).

6https://github.com/Marsan-Ma-zz/chat corpus



3779

4.2 Implementation Details

In order to make our model comparable with
typical existing methods, we keep the same ex-
perimental parameters for VPN and comparative
methods. We set the vocabulary size of raw word-
s as 34000, and the word vector dimension is
300. Moreover, source inputs are encoded by 600-
dimensional vectors with bi-direction LSTMs, and
responses are also decoded by LSTM with 600
dimensions. The total losses are minimized by
an Adam optimizer (Kingma and Ba, 2015) with
0.0001 learning rate. Particularly, the size of low-
level clusters and high-level clusters are 3400 and
340, respectively, which are significantly smaller
than the size of raw words (34000), and these clus-
ters are also represented by 300-dimensional vec-
tors. Finally, we implemented all models with the
TensorFlow.

4.3 Evaluation Metrics

Evaluation for generative responses is a chal-
lenging and under-researching problem (Noviko-
va et al., 2017). Similar to (Li et al., 2016b; Gu
et al., 2016), we borrow two well-established auto-
matic evaluation metrics from machine translation
and text summarization: BLEU (Papineni et al.,
2002) and ROUGE (Lin, 2004)7, which could be
leveraged to analyze the co-occurrences of n-gram
between the generated responses and references.

In addition to automatic evaluations, we also
leverage manual evaluations to enhance the eval-
uations. Following previous studies (He et al.,
2017; Qian et al., 2018; Liu et al., 2018), we em-
ploy three metrics for manual evaluations as fol-
lows. 1) Fluency (Flu.): measuring the grammati-
cality and fluency of generated responses, where
too short responses are regarded as lack of flu-
ency. 2) Consistency (Con.): measuring whether
the generated responses are consistent with the
inputs or not. 3) Informativeness (Inf.): mea-
suring whether the response provides informative
(knowledgeable) contents or not.

4.4 Overall Comparisons

Comparison Settings. We compare VPN with the
following methods:

7Implemented in https://github.com/Maluuba-/nlg-eval.
Evaluations on Twitter are based on token level. In partic-
ular, the BLEU and ROUGE on Weibo dataset are based on
the Chinese character because Chinese characters are with se-
mantics.

Models
Twitter Weibo

BLEU ROUGE BLEU ROUGE
S2SA (2014) 6.12 6.42 8.95 9.06

S2STA (2017) 7.73 7.57 11.45 11.29
S2SDV (2018b) 5.91 5.87 9.05 8.71
DelNet (2017) 6.42 6.76 10.04 10.03

VPN (ours) 8.58 7.88 12.51 11.76

Table 1: Overall performance on Twitter and Weibo
datasets. Note that the first three lines are only one-
pass decoding, and the fourth line (DelNet) is beyond
one-pass decoding.

(1) S2SA: Sequence-to-Sequence (Sutskever
et al., 2014) with attention mechanisms (Bahdanau
et al., 2015).

(2) S2SDV: Seq2Seq with dynamic vocabulary,
the implementation is similar to Wu et al. (2018b).

(3) S2STA: Seq2Seq with topic aware networks,
the implementation is similar to Xing et al. (2017).
S2STA could be regarded as using dynamic vocab-
ulary because topic words are changed along with
the input.

(4) DelNet: deliberation networks, the imple-
mentation is similar to Xia et al. (2017). Different
from the above methods, deliberation networks are
beyond one-pass decoding.

Comparison Results. We first report overal-
l performances on Table 1. These results support
the following statements:

(1) Our VPN achieves the highest performances
on English Twitter and Chinese Weibo dataset in
all metrics, which demonstrates multi-pass encod-
ing and decoding with multi-level vocabularies are
able to deliver better responses than baselines.

(2) For the one-pass decoding (the first three
methods in Table 1), S2STA performs the best.
Pre-trained topic words for each input are able
to make the generation more target-focused in
S2STA. Nevertheless, it is still worse than VPN.

(3) As for models beyond one-pass decoding
(the last two lines in Table 1), VPN is remark-
ably better than the deliberation network (DelNet),
which indicates the effectiveness of multi-pass en-
coder and decoder with multi-level vocabularies.

4.5 The Effectiveness of Multi-Level
Vocabularies

Comparison Settings. To validate the effective-
ness of multi-level vocabularies obtained from hi-
erarchical clustering, we design experiments on
whether using Multi-level Vocabularies (MVs) or
not. The comparison setting is shown in the first



3780

Models
Twitter Weibo

BLEU ROUGE BLEU ROUGE
enc3-dec1 (SV) 6.27 6.29 6.61 7.08

enc3-dec1 (MVs) 7.16 8.01 9.15 10.63
enc1-dec3 (SV) 7.43 7.54 9.92 10.24

enc1-dec3 (MVs) 6.75 7.78 12.01 10.86
enc3-dec3 (SV) 7.44 7.56 9.95 9.70

enc3-dec3 (MVs) 8.58 7.88 12.51 11.76

Table 2: Performances on whether using multi-level
vocabularies or not, where “SV” represents single vo-
cabulary (from raw words), and “MVs” means multi-
level vocabularies obtained from hierarchical cluster-
ing. “enc” and “dec” denote encoder and decoder, re-
spectively, and numbers after them represent how many
passes. For example, “enc1-dec3” means a encoder a-
long with three passes of decoders.

column in Table 2, where numbers after “enc/dec”
represent the number of encoders/decoders. “SV”
denotes single vocabulary (from raw words) while
“MVs” means multi-level vocabularies obtained
from hierarchical clustering.

Comparison Results. Table 2 demonstrates
performances on whether using multi-level vocab-
ularies. We can observe that incorporating multi-
level vocabularies could improve performances on
almost all of the metrics. For example, “enc3-
dec3 (MVs)” improves relative performance up to
25.73% in BLEU score compared with “enc3-dec3
(SV)” on the Weibo dataset. Only on the Twitter
dataset, “enc1-dec3 (MVs)” is slightly worse than
“enc1-dec3 (SV)” in the BLEU score.

4.6 The Effectiveness of Multi-Pass Encoding
and Decoding

Models
Twitter Weibo

BLEU ROUGE BLEU ROUGE
VPN 8.58 7.88 12.51 11.76

w/o low-level ED 7.83 7.57 11.96 11.60
w/o high-level ED 6.84 7.81 10.06 10.70

w/o low&high-level ED 6.12 6.42 8.95 9.06

Table 3: Influences of multi-pass encoding and decod-
ing, where “w/o” indicates without, “ED” represents
encoder and decoder. For example, “w/o low-level ED”
means removing low-level encoder and low-level de-
coder.

Comparison Settings. In order to demon-
strate the effectiveness of multi-pass encoder and
multi-pass decoder, we design an ablation study
as follows. 1) w/o low-level ED: without low-
level encoder and low-level decoder; 2) w/o high-

level ED: without high-level encoder and high-
level decoder; 3) w/o low&high-level ED: with-
out low-level encoder/decoder and high-level en-
coder/decoder, which is the same as the Seq2Seq
model with attention mechanisms.

Comparison Results. Results of the ablation s-
tudy are shown in Table 3. We can clearly see that
removing any encoder and decoder causes obvious
performance degradation. Specifically, “w/o high-
level ED” obtains worse performances than “w/o
low-level ED”. We guess that the high-level en-
coder and decoder are well trained since they have
the smallest vocabulary (the size of high-level
clusters is only 340), so removing the well-trained
component (“w/o high-level ED”) performs poor-
ly (Details in Section 4.8). Furthermore, “w/o
low&high-level ED” performs the worst. This fur-
ther indicates that multi-pass encoder and decoder
contribute to generating better responses.

4.7 Manual Evaluations (MEs)

Datasets Models Flu. Con. Inf.

Twitter
VPN vs. S2STA 56.49 54.92 54.07
VPN vs. DelNet 57.89 60.40 57.50

Weibo
VPN vs. S2STA 52.31 52.99 53.54
VPN vs. DelNet 56.56 55.66 54.72

Table 4: Manual evaluations with fluency (Flu.), con-
sistency (Con.), and informativeness (Inf.). The score is
the percentage that VPN wins a baseline after removing
“tie” pairs. VPN is clearly better than all baselines on
the three metrics, and all results are at 99% confidence
intervals.

Comparison Settings. Similar to manual eval-
uations used in Zhou et al. (2018), we conduct a
pair-wise comparison between the response gener-
ated by VPN and the one for the same input by two
typical baselines: S2STA and DelNet. we sample
100 responses from each system, then two cura-
tors judge (win, tie and lose) between these two
methods.

Comparison Results. The results of manual e-
valuations are shown in Table 4, where the score
is the percentage that VPN wins a baseline after
removing “tie” pairs. The Cohen Kappa for inter-
annotator statistics is 61.2, 62.1 and 70.8 for fluen-
cy, consistency and informativeness, respectively.
We can see that our VPN is significantly (sign test,
p-value < 0.01) better than all baselines in terms
of the three metrics, which further demonstrates
that VPN is able to deliver fluent, consistent and



3781

informative responses.

4.8 Discussion

Decoders
Twitter Weibo

BLEU ROUGE BLEU ROUGE
High-Level Dec. 12.44 14.93 23.92 10.66
Low-Level Dec. 8.84 8.21 22.50 9.50
Raw-Word Dec. 8.58 7.88 13.02 5.39

Table 5: Performances on each decoder in VPN8.

The multi-pass decoder in VPN has three de-
coders. In order to investigate the reasons why
the multi-pass decoder works, we will see perfor-
mances on each decoder in Table 5. We can ob-
serve that the high-level decoder obtains the best
performances on all metrics, and the low-level de-
coder outperforms the raw-word decoder. It is
intuitive that the high-level decoder performs the
best since it has the smallest vocabulary (340),
while the raw-word decoder performs the worst
because it is equipped with the biggest vocabu-
lary (34000). From the point of performances on
each decoder, the effectiveness of multi-pass de-
coder could be explained from curriculum learn-
ing (Bengio et al., 2009). Curriculum learning is
a learning strategy in machine learning, where the
key idea is to start easier aspects of the targeted
task and then gradually increase the complexity. It
is difficult for response generation tasks to gener-
ate raw words directly. To alleviate this problem,
the multi-pass decoder first generates the easier
(high-level and low-level) clusters from the smal-
l vocabularies, and then generates the raw word-
s from the big vocabulary under the guide of the
well-generated clusters. Therefore, the multi-pass
decoder obtains significant performances.

5 Related Work

Researches have achieved remarkable improve-
ments on response generation for human-machine
conversations. Currently, encoder-decoder frame-
work, especially the Seq2Seq learning (Cho et al.,
2014a), is becoming a backbone of data-drive re-
sponse generation, and it has been widely applied
in response generation tasks. For example, Shang
et al. (2015a) presented neural recurrent encoder-
decoder frameworks for short-text response gener-

8In the Discussion Section, all evaluations are based on
tokens (IDs) for unifying, so the performances of raw-word
decoder on Chinese Weibo dataset are different from the ones
(character level) in Table 1.

ation with attention mechanisms (Bahdanau et al.,
2015). Li et al. (2016b) introduced persona-based
neural response generation to obtain consistent re-
sponses for similar inputs to a speaker. Shao et al.
(2017) added a self-attention to generate long and
diversified responses in Seq2Seq learning.

In this study, we focus on two important prob-
lems in response generation: one fixed vocabu-
lary and one-pass decoding. Our work is inspired
by following researches to alleviate issues on the
fixed vocabulary. Gu et al. (2016) proposed Copy-
Net, which is able to copy words from the source
message. External knowledge bases were also
leveraged to extend the vocabulary (Qian et al.,
2018; Zhou et al., 2018; Ghazvininejad et al.,
2018). Moreover, Xing et al. (2017) incorporat-
ed topic words into Seq2Seq frameworks, where
topic words are obtained from a pre-trained L-
DA model (Blei et al., 2003). Wu et al. (2018b)
changed the static vocabulary mechanism by a dy-
namic vocabulary, which jointly learns vocabulary
selection and response generation.

We also borrow the idea from studies beyond
one-pass decoding. Mou et al. (2016) designed
backward and forward sequence generators. Xi-
a et al. (2017) proposed deliberation networks on
sequence generation beyond one-pass decoding,
where the first-pass decoder generates a raw word
sequence, and then the second decoder delivers a
refined word sequence based on the raw word se-
quence. Furthermore, Su et al. (2018) presented
hierarchical decoding with linguistic patterns on
data-to-text tasks.

However, there has been no unified frameworks
to solve the issues of fixed vocabulary and one-
pass decoding. Differently, we propose multi-pass
encoding and decoding with multi-level vocabu-
laries to deal with the above two problems simul-
taneously.

6 Conclusion and Future Work

In this study, we tackle the issues of one fixed vo-
cabulary and one-pass decoding in response gen-
eration tasks. To this end, we have introduced vo-
cabulary pyramid networks, in which dialogue in-
put and output are represented by multi-level vo-
cabularies and then processed by multi-pass en-
coding and decoding, where the multi-level vocab-
ularies are obtained from hierarchical clustering of
raw words. We conduct experiments on English
Twitter and Chinese Weibo datasets. Experiments



3782

demonstrate that the proposed method is remark-
ably better than strong baselines on both automatic
and manual evaluations.

In the future, there are some promising ex-
plorations in vocabulary pyramid networks. 1)
we will further study how to obtain multi-level
vocabularies, such as employing other clustering
methods and incorporating semantic lexicons like
WordNet; 2) we also plan to design deep-pass en-
coding and decoding for VPN; 3) we will investi-
gate how to apply VPN to other natural language
generation tasks such as machine translation and
generative text summarization.

Acknowledgments

This work is supported by the Natural Sci-
ence Foundation of China (No.61533018),
the Natural Key R&D Program of China
(No.2017YFB1002101), the Natural Science
Foundation of China (No.61702512) and the inde-
pendent research project of National Laboratory
of Pattern Recognition. This work is also support-
ed by Alibaba Group through Alibaba Innovative
Research (AIR) Program, CCF-DiDi BigData
Joint Lab and CCF-Tencent Open Research Fund.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
ICLR.

Yoshua Bengio, Jérôme Louradour, Ronan Collobert,
and Jason Weston. 2009. Curriculum learning. In
Proceedings of ICML, pages 41–48. ACM.

David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet allocation.
Journal of Machine Learning Research, 3:2003.

Xinchi Chen, Zhan Shi, Xipeng Qiu, and Xuanjing
Huang. 2017. Adversarial multi-criteria learning for
chinese word segmentation. In Proceedings of ACL.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014a. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings
of EMNLP, pages 1724–1734.

Kyunghyun Cho, B van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014b. On the proper-
ties of neural machine translation: Encoder-decoder
approaches. In Proceedings of SSST-8.

Marjan Ghazvininejad, Chris Brockett, Ming-Wei
Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and
Michel Galley. 2018. A knowledge-grounded neural
conversation model. In Proceedings of AAAI.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
ACL, pages 1631–1640.

Shizhu He, Cao Liu, Kang Liu, and Jun Zhao.
2017. Generating natural answers by incorporating
copying and retrieving mechanisms in sequence-to-
sequence learning. In Proceedings of ACL, pages
199–208.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of ICLR.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016a. A diversity-promoting ob-
jective function for neural conversation models. In
Proceedings of NAACL, pages 110–119.

Jiwei Li, Michel Galley, Chris Brockett, Georgios Sp-
ithourakis, Jianfeng Gao, and Bill Dolan. 2016b. A
persona-based neural conversation model. In Pro-
ceedings of ACL, pages 994–1003.

Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,
Michel Galley, and Jianfeng Gao. 2016c. Deep rein-
forcement learning for dialogue generation. In Pro-
ceedings of EMNLP, pages 1192–1202.

Jiwei Li, Will Monroe, Tianlin Shi, Sébastien Jean,
Alan Ritter, and Dan Jurafsky. 2017. Adversarial
learning for neural dialogue generation. In Proceed-
ings of EMNLP, pages 2157–2169.

Chin-Yew Lin. 2004. Rouge: A package for automat-
ic evaluation of summaries. In Proceedings of ACL
workshop, page 10.

Cao Liu, Shizhu He, Kang Liu, and Jun Zhao. 2018.
Curriculum learning for natural answer generation.
In Proceedings of IJCAI, pages 4223–4229.

Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang,
and Zhi Jin. 2016. Sequence to backward and for-
ward sequences: A content-introducing approach to
generative short-text conversation. In Proceedings
of COLING, pages 3349–3358.

Jekaterina Novikova, Ondřej Dušek, Amanda Cer-
cas Curry, and Verena Rieser. 2017. Why we need
new evaluation metrics for nlg. In Proceedings of
EMNLP, pages 2241–2252.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic e-
valuation of machine translation. In Proceedings of
ACL, pages 311–318.



3783

Qiao Qian, Minlie Huang, Haizhou Zhao, Jingfang X-
u, and Xiaoyan Zhu. 2018. Assigning personali-
ty/profile to a chatting machine for coherent conver-
sation generation. In Proceedings of IJCAI, pages
4279–4285.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2016. Sequence level train-
ing with recurrent neural networks. In Proceedings
of ICLR.

Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of EMNLP, pages 583–593.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673–2681.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015a.
Neural responding machine for short-text conversa-
tion. In Proceedings of the ACL, pages 1577–1586.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015b.
Neural responding machine for short-text conversa-
tion. In Proceedings of ACL-IJCNLP, pages 1577–
1586. Association for Computational Linguistics.

Yuanlong Shao, Stephan Gouws, Denny Britz, An-
na Goldie, Brian Strope, and Ray Kurzweil. 2017.
Generating high-quality and informative conversa-
tion responses with sequence-to-sequence models.
In Proceedings of EMNLP, pages 2210–2219.

Shang-Yu Su, Kai-Ling Lo, Yi Ting Yeh, and Yun-
Nung Chen. 2018. Natural language generation by
hierarchical decoding with linguistic patterns. In
Proceedings of NAACL, pages 61–66.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural network-
s. In Proceedings of NIPS, pages 3104–3112.

Alan M Turing. 1950. Computing machinery and in-
telligence. In Parsing the Turing Test, pages 23–65.
Springer.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. Proceedings of ICML workshop.

Joe H. Ward. 1963. Hierarchical grouping to optimize
an objective function. Journal of the American Sta-
tistical Association, 58(301):236–244.

Yu Wu, Wei Wu, Can Xu, and Zhoujun Li. 2018a.
Knowledge enhanced hybrid neural network for text
matching. In Proceedings of AAAI.

Yu Wu, Wei Wu, Dejian Yang, Can Xu, and Zhoujun
Li. 2018b. Neural response generation with dynam-
ic vocabularies. In Proceedings of AAAI.

Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin,
Nenghai Yu, and Tie-Yan Liu. 2017. Deliberation
networks: Sequence generation beyond one-pass de-
coding. In Proceedings of NIPS, pages 1782–1792.

Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang,
Ming Zhou, and Wei-Ying Ma. 2017. Topic aware
neural response generation. In Proceedings of AAAI,
pages 3351–3357.

Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao,
Jingfang Xu, and Xiaoyan Zhu. 2018. Com-
monsense knowledge aware conversation generation
with graph attention. In Proceedings of IJCAI, pages
4623–4629.


