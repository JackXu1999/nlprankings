



















































Contrastive Language Adaptation for Cross-Lingual Stance Detection


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4442–4452,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

4442

Contrastive Language Adaptation for Cross-Lingual Stance Detection

Mitra Mohtarami1, James Glass1, Preslav Nakov2
1MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA, USA

2Qatar Computing Research Institute, HBKU, Doha, Qatar
{mitra,glass}@csail.mit.edu; pnakov@hbku.edu.qa

Abstract

We study cross-lingual stance detection, which
aims to leverage labeled data in one language
to identify the relative perspective (or stance)
of a given document with respect to a claim in
a different target language. In particular, we
introduce a novel contrastive language adap-
tation approach applied to memory networks,
which ensures accurate alignment of stances
in the source and target languages, and can ef-
fectively deal with the challenge of limited la-
beled data in the target language. The evalua-
tion results on public benchmark datasets and
comparison against current state-of-the-art ap-
proaches demonstrate the effectiveness of our
approach.

1 Introduction

The rise of social media has enabled the phe-
nomenon of “fake news,” which could target spe-
cific individuals and can be used for deceptive pur-
poses (Lazer et al., 2018; Vosoughi et al., 2018).
As manual fact-checking is a time-consuming and
tedious process, computational approaches have
been proposed as a possible alternative (Popat et al.,
2017; Wang, 2017; Mihaylova et al., 2018, 2019),
based on information sources such as social me-
dia (Ma et al., 2017), Wikipedia (Thorne et al.,
2018), and knowledge bases (Huynh and Papotti,
2018). Fact-checking is a multi-step process (Vla-
chos and Riedel, 2014): (i) checking the reliability
of media sources, (ii) retrieving potentially relevant
documents from reliable sources as evidence for
each target claim, (iii) predicting the stance of each
document with respect to the target claim, and fi-
nally (iv) making a decision based on the stances
from (iii) for all documents from (ii).

Here, we focus on stance detection which aims to
identify the relative perspective of a document with
respect to a claim, typically modeled using labels
such as agree, disagree, discuss, and unrelated.

Current approaches to stance detection (Bar-Haim
et al., 2017; Dungs et al., 2018; Kochkina et al.,
2018; Sobhani et al., 2017; Mohtarami et al., 2018)
are well-studied in mono-lingual settings, in partic-
ular for English, but less attention has been paid to
other languages and cross-lingual settings. This is
partially due to domain differences and to the lack
of training data in other languages.

We aim to bridge this gap by proposing a
cross-lingual model for stance detection. Our
model leverages resources of a source language
(e.g., English) to train a model for a target lan-
guage (e.g., Arabic). Furthermore, we propose
a novel contrastive language adaptation approach
that effectively aligns samples with similar or
dissimilar stances across source and target lan-
guages using task-specific loss functions. We ap-
ply our language adaptation approach to memory
networks (Sukhbaatar et al., 2015), which have
been found effective for mono-lingual stance detec-
tion (Mohtarami et al., 2018).

Our model can explain its predictions about
stances of documents against claims in a differ-
ent/target language by extracting relevant text snip-
pets from the documents of the target language as
evidence. We use evidence extraction as a measure
to evaluate the trasferability of our model. This
is because more accurate evidence extraction in-
dicates that the model can better learn semantic
relations between claims and pieces of evidence,
and consequently can better transfer knowledge to
the target language.

The contributions of this paper are summarized
as follows:

• We propose a novel language adaptation ap-
proach based on contrastive stance alignment
that aligns the class labels between source and
target languages for effective cross-lingual
stance detection.



4443

• Our model is able to extract accurate text snip-
pets as evidence to explain its predictions in
the target language (results are in Section 4.2).

• To the best of our knowledge, this is the first
work on cross-lingual stance detection.

We conducted our experiments on English (as
source language) and Arabic (as target language).
In particular, we used the Fake News Challenge
dataset (Hanselowski et al., 2018) as source data
and an Arabic benchmark dataset (Baly et al.,
2018) as target data. The evaluation results have
shown 2.7 and 4.0 absolute improvement in terms
of macro-F1 and weighted accuracy for stance
detection over the current state-of-the-art mono-
lingual baseline, and 11.4, 14.9, 16.1, 12.9, and
13.1 points of absolute improvement in terms of
precision at ranks 1–5 for extracting evidence snip-
pets respectively. Furthermore, a key finding in our
investigation is that, in contrast to other tasks (De-
vlin et al., 2019; Peters et al., 2018), pre-training
with large amounts of source data is less effective
for cross-lingual stance detection. We show that
this is because pre-training can considerably bias
the model toward the source language.

2 Method

Assume that we are given a training dataset for
a source language, Ds, which contains a set of
triplets as follows: Ds =

{(
(csi , d

s
i ), y

s
i

)}N
i=1

,
where N is the number of source samples, (csi , d

s
i )

is a pair of claim csi and document d
s
i , and y

s
i ∈ Y ,

Y = {agree, disagree, discuss, unrelated}, is the
corresponding label indicating the stance of the
document with respect to the claim. In addition,
we are given a very small training dataset for the
target language Dt =

{(
(cti, d

t
i), y

t
i

)}M
i=1

, where
M (M << N ) is the number of target samples,
and (cti, d

t
i) is a pair of claim and document in the

target language with stance label yti ∈ Y . In real-
ity, (i) the size of the target dataset is very small,
(ii) claims and documents in the source and target
languages are from different domains, and (iii) the
only commonality between the source and target
datasets is in their stance labels, i.e., ysi , y

t
i ∈ Y .

We develop a language adaptation approach to
effectively use the commonality between the source
and the target datasets in their label space and to
deal with the limited size of the target training data.
We apply our language adaptation approach to end-
to-end memory networks (Sukhbaatar et al., 2015)
for cross-lingual stance detection.

We use memory networks as they have achieved
state-of-the-art performance for mono-lingual
stance detection (Mohtarami et al., 2018). How-
ever, our language adaptation approach can be ap-
plied to any other type of neural network. The archi-
tecture of our cross-lingual stance detection model
is shown in Figure 1. It has two main components:
(i) Memory Networks indicated with two dashed
boxes for the source and the target languages, and
(ii) Contrastive Language Adaptation component.
In what follows, we first explain our memory net-
work model for cross-lingual stance detection (Sec-
tion 2.1) and then present our contrastive language
adaptation approach (Section 2.2).

2.1 Memory Networks

Memory networks are designed to remember past
information (Sukhbaatar et al., 2015) and have been
successfully applied to NLP tasks ranging from
dialog (Bordes et al., 2017) to question answer-
ing (Xiong et al., 2016) and mono-lingual stance
detection (Mohtarami et al., 2018). They include
components that can potentially use different learn-
ing models and inference strategies. Our source
and target memory networks follow the same archi-
tecture as depicted in Figure 1:

A memory network consists of six components.
The network takes as input a document d and a
claim c and encodes them into the input space I .
These representations are stored in the memory
component M for future processing. The relevant
parts of the input are identified in the inference
component F , and used by the generalization com-
ponent G to update the memory M . Finally, the
output component O generates an output from the
updated memory, and encodes it to a desired format
in the response component R using a prediction
function, e.g., softmax for classification tasks.
We elaborate on these components below.

Input representation component I: It encodes
documents and claims into corresponding represen-
tations. Each document d is divided into a sequence
of paragraphs X = (x1, . . . , xl), where each xj is
encoded as mj using an LSTM network, and as
nj using a CNN; these representations are stored
in the memory component M . Note that while
LSTMs are designed to capture and memorize their
inputs (Tan et al., 2016), CNNs emphasize the local
interaction between individual words in sequences,
which is important for obtaining good representa-
tion (Kim, 2014).



4444

Figure 1: The architecture of our cross-lingual memory network for stance detection.

Thus, our I component uses both LSTM and CNN
representations. It also uses separate LSTM and
CNN with their own parameters to represent each
input claim c as clstm and ccnn, respectively.

We consider each paragraph as a single piece of
evidence because a paragraph usually represents a
coherent argument, unified under one or more inter-
related topics. We thus use the terms paragraph and
evidence interchangeably.

Inference component F : Our inference compo-
nent computes LSTM- and CNN-based similarity
between each claim c and evidence xj as follows:

P lstmj = clstm
ᵀ ×M×mj ,∀j

P cnnj = ccnn
ᵀ ×M′ × nj , ∀j

where P lstm. and P
cnn
. indicate claim-evidence sim-

ilarity based on LSTM and CNN respectively,
clstm ∈ Rq and mj ∈ Rd are LSTM representations
of c and xj respectively, ccnn ∈ Rq

′
and nj ∈ Rd

′

are the corresponding CNN representations, and
M ∈ Rq×d and M′ ∈ Rq′×d′ are similarity ma-
trices trained to map claims and paragraphs into
the same space with respect to their LSTM and
CNN representations. The rationale behind using
these similarity matrices is that, in the memory net-
work, we seek a transformation of the input claim,
i.e., cᵀ×M, in order to obtain the closest evidence
to the claim.

Additionally, we compute another semantic sim-
ilarity vector, P tfidfj , by applying a cosine similarity
between the TF.IDF (Spärck Jones, 2004) represen-
tation of xj and c. This is particularly useful for
stance detection as it can help filtering out unrelated
pieces of evidence.

Memory M and Generalization G components:
Our memory component stores representations and
the generalization component improves their qual-
ity by filtering out unrelated evidence. For example,
the LSTM representations of paragraphs, mj , ∀j,
are updated using the claim-evidence similarity
P tfidfj as follows: mj = mj � P

tfidf
j , ∀j. This

transformation will help filter out unrelated evi-
dence with respect to claims. The updated mj
in conjunction with clstm are used by the infer-
ence component F to compute P lstmj ,∀j as we ex-
plained above. Then, P lstmj are in turn used to up-
date CNN representations in memory as follows:
nj = nj � P lstmj ,∀j. Finally, the updated nj and
ccnn are used to compute P cnnj .

Output representation component O: This
component computes the output of the memory
M by concatenating the average vector of the up-
dated nj with the maximum and average of claim-
evidence similarity vectors P tfidfj , P

lstm
j and P

cnn
j .

The maximum helps to identify parts of documents
that are most similar to claims, while the average
estimates the overall document-claim similarity.

Response generation component R: This com-
ponent computes the final stance of a document
with respect to a claim. For this, the output of com-
ponent O is concatenated with clstm and ccnn and
fed into a softmax to predict the stance of the
document with respect to the claim.

All the memory network parameters, including
those of CNN and LSTM in the I component, the
similarity matrices M and M′ in F , and the classi-
fier parameters in R, are jointly learned during the
training process with our language adaptation.



4445

Figure 2: Illustration of stance equal alignment (SEA),
stance separation alignment (SSA), and classification
alignment (CA) constraints. Different shapes indicate
different stance labels and colors specify source (blue)
and target (green) languages.

2.2 Contrastive Language Adaptation
Memory networks are effective for stance detection
in mono-lingual settings (Mohtarami et al., 2018)
when there is sufficient training data. However, we
show that these models have limited transferability
to target languages with limited data. This could
be due to discrepancy between the underlying data
distributions in the source and target languages. We
show that the performance of these networks can be
trivially increased when the model, pre-trained on
source data, is fine-tuned using small amounts of
labeled target data. We further develop contrastive
language adaptation that can exploit the labeled
source data to perform well on target data.
Our contrastive adaptation approach:
• encourages pairs (dsi , csi ) from the source lan-

guage and (dti, c
t
i) from the target language

with the same stance labels (i.e., ysi = y
t
i) to

be nearby in the embedding space. We call
this mapping Stance Equal Alignment (SEA),
illustrated with dotted lines in Figure 2. Note
that documents and claims in the two lan-
guages are often semantically different and are
not corresponding translations of each other.
• encourages pairs (dsi , csi ) from the source lan-

guage and (dti, c
t
i) from the target language

with different stance labels (i.e., ysi 6= yti) to
be far apart in the embedding space. We call
this mapping Stance Separation Alignment
(SSA), shown with dashed lines in Figure 2.
• encourages pairs (dsi , csi ) from the source lan-

guage and (dti, c
t
i) from the target language

to be correctly classified as (dsi , c
s
i ) → ysi

and (dti, c
t
i)→ yti . We call this Classification

Alignment (CA), solid lines in Figure 2.

Algorithm 1. Cross-Lingual Stance Detection Model
Inputs:
• {(dsi , csi ), ysi }l

s

1 : set of pairs of documents dsi and
claims csi in the source language, where y

s
i ∈ Y ,

Y = {agree, disagree, discuss, unrelated}
• {(dti, cti), yti}l

t

1 : small set of pairs of documents dti
and claims cti in the target language with y

t
i ∈ Y

Output:
• {(dti, cti) → yti}Mlt+1: assign stance labels y

t
i ∈ Y

to given unlabeled target pairs
Cross-Lingual model:
1 Create the sets {

(
(dsi , c

s
i ), (d

t
j , c

t
j)
)
, (ysi , y

′
i)} and

{
(
(dtj , c

t
j), (d

s
i , c

s
i )
)
, (ytj , y

′
j)}, where y′ = 1 if the

source and the target have the same label, and y′ = 0
otherwise.

2 Loop for e epochs:
3 pass (dsi , c

s
i ) to the source memory network to

create its representation rsi .
4 pass (dtj , c

t
j) to the target memory network to

create its representation rtj .
5 pass (rsi , y

s
i ) to the classification to compute its

classification loss LCAsi .
6 pass (rsi , r

t
i , y

′i) to the language adaptation to
compute the stance alignment loss LCSAi.

7 compute total lossLsi = (1−α)LCAsi+αLCSAi.
8 repeat steps 2-7 with a change in step 5 by passing

the target sample (rtj , y
t
j) to the classification instead

of the source sample, and compute its Ltj in step 7.
9 jointly optimize all parameters of the model using

the average loss L = mean({Lsi}+ {Ltj}).

Table 1: Cross-lingual stance detection model.

We make complete use of the stance labels in the
cross-lingual setting by parameterizing our model
according to the distance between the source and
the target samples in the embedding space.

For stance equal alignment (SEA) constraint, the
objective is to minimize the distance between pairs
of source and target data with the same stance la-
bels. We achieve this using the following loss:

LSEA =

{
D
(
g(dsi , c

s
i )− g(dti, cti)

)2
, ysi = y

t
i

0, otherwise,
(1)

where g maps its input pair to an embedding space
using our memory network or any mono-lingual
model, and D computes Euclidean distance.

For stance separation alignment (SSA), the goal
is to maximize the distance between pairs with
different stance labels. We use the following loss:

LSSA =


max

(
0,m−

D
(
g(dsi , c

s
i )− g(dti, cti)

)2)
, ysi 6= yti

0, otherwise,
(2)

where we maximize the distance between pairs
with different stance labels up to the margin m.



4446

The margin parameter m specifies the extent of
separability in the embedding space.

We can further use any classification loss to en-
force classification alignment (CA). We use cat-
egorical cross-entropy and call it Classification
Alignment loss LCA.

We develop our overall language adaptation loss,
named Contrastive Stance Alignment loss, LCSA,
by combining LSEA and LSSA as follows:

LCSA = LSEA + LSSA. (3)

Finally, the total loss of our cross-lingual stance
detection model is defined as follows:

L = (1− α)LCA + αLCSA, (4)

where the α parameter controls the balance be-
tween classification and language adaptation losses,
which we optimize on the validation dataset.

Information Flow: Our overall cross-lingual
model for stance detection is shown in Figure 1,
and a summary of the algorithm is presented in
Table 1. As Figure 1 shows, each source and tar-
get pairs are passed to the source and to the target
memory networks to obtain their corresponding
representations (Lines 3-4 in Table 1). The source
representation and its gold stance label are passed
to the classifier to compute the classification loss
(Line 5). In addition, the source and the target rep-
resentations in conjunction with a binary parameter
(y′, which is 1 if the source and the target have
the same stance label, and 0 otherwise) are passed
to the language adaptation component to compute
the contrastive stance alignment loss LCSA (Line
6). Finally, the total loss is computed based on
Equation (4) (Line 7).

The classifier also uses labeled target samples to
create a shared embedding space and to fine-tune
itself with respect to the target language. For this
purpose, we repeat the above steps by switching
the target and the source pipelines (Line 8). Finally,
we compute the average of all losses and we use it
to optimize the parameters of our model (Line 9).

Pre-training for Language Adaptation: Pre-
training has been found effective in many language
adaptation settings (Tzeng et al., 2017). To inves-
tigate the effect of pre-training, we first pre-train
the source memory network and the classifier using
Ds (only the top pipeline in Figure 1), and then we
apply language adaptation with the full model.

3 Experiments

Data and Settings. As source data, we use
the Fake News Challenge dataset1 which con-
tains 75.4K claim-document pairs in English with
{agree, disagree, discuss, unrelated} as stance la-
bels. As target dataset, we use 3K Arabic claim-
document pairs developed in (Baly et al., 2018).2

We perform 5-fold cross-validation on the Ara-
bic dataset, using each fold in turn for testing, and
keeping 80% of the remaining data for training and
20% for development. We use 300-dimensional
pretrained cross-lingual Wikipedia word embed-
dings from MUSE (Lample et al., 2018). We use
300-dimensional units for the LSTM and 100 fea-
ture maps with filter width of 5 for the CNN. We
consider the first 9 paragraphs per document, which
is the median number of paragraphs in source doc-
uments. We optimize all hyper-parameters on vali-
dation data using Adam (Kingma and Ba, 2014).

Evaluation Measures. We use the followings:
• Accuracy: The fraction of correctly classified

examples. For multi-class classification, accu-
racy is equivalent to micro-average F1 (Man-
ning et al., 2008).
• Macro-F1: The average of F1 scores that were

calculated for each class separately.
• Weighted Accuracy: This is a hierarchical met-

ric, which first awards 0.25 points if the model
correctly predicts a document-claim pair as
related3 or unrelated. If it is related, 0.75
additional points are assigned if the model
correctly predicts the pair as agree, disagree,
or discuss. The goal of this weighting schema
is to balance out the large number of unrelated
examples (Hanselowski et al., 2018).

Baselines. We consider the following baselines:
• Heuristic: Given the imbalanced nature of

our data, we use two heuristic baselines where
all test examples are labeled as unrelated or
agree. The former is a majority class baseline
favoring accuracy and macro-F1, while the
latter is better for weighted accuracy.
• Gradient Boosting (Baly et al., 2018): This

is a Gradient Boosting classifier with n-gram
features as well as indicators for refutation
and polarity.

1Available at www.fakenewschallenge.org
2Available at http://groups.csail.mit.edu/

sls/downloads/
3related = {agree, disagree, discuss}

www.fakenewschallenge.org
http://groups.csail.mit.edu/sls/downloads/
http://groups.csail.mit.edu/sls/downloads/


4447

Methods Weigh. Acc. Acc. Macro-F1 F1 (agree, disagree, discuss, unrelated)
1. All-unrelated 34.8 68.1 20.3 0 / 0 / 0 / 81.0
2. All-agree 40.2 15.6 6.7 27.0 / 0 / 0 / 0
3. Gradient Boosting (Baly et al., 2018) 55.6 72.4 41.0 60.4 / 9.0 / 10.4 / 84.0
4. TFMLP (Riedel et al., 2017) 49.3 66.0 37.1 47.0 / 7.8 / 13.4 / 80.0
5. EnrichedMLP (Hanselowski et al., 2018) 55.1 70.5 41.3 59.1 / 9.2 / 14.1 / 82.3
6. Ensemble (Baird et al., 2017) 53.6 71.6 37.2 57.5 / 2.1 / 6.2 / 83.2
7. MN target → target (Mohtarami et al., 2018) 55.3 70.9 41.7 60.0 / 15.0 / 08.5 / 83.1
8. MN source → target 53.2 64.2 36.0 40.4 / 02.0 / 19.1 / 82.4
9. MN (source, target) → target 57.3 65.0 42.5 58.0 / 12.2 / 20.9 / 79.0
10. ADMN (adversarial) 58.6 58.4 43.4 60.2 / 16.1 / 24.2 / 72.9
11. CLMN (contrastive) 61.3 71.6 45.2 65.1 / 11.6 / 20.5 / 83.7

Table 2: Evaluation results on the target Arabic test dataset.

• TFMLP (Riedel et al., 2017): This is an MLP
with normalized bag-of-words features en-
riched with a single TF.IDF-based similarity
feature for each claim-document pair.
• EnrichedMLP (Hanselowski et al., 2018):

This model combines five MLPs, each with
six hidden layers and advanced features from
topic models, latent semantic analysis, etc.
• Ensemble (Baird et al., 2017): It is an en-

semble based on weighted average of a deep
convolutional neural network and a gradient-
boosted decision tree - the best model at the
Fake News Challenge.
• Mono-lingual Memory Network (Mo-

htarami et al., 2018): This model is the
current state-of-the-art for stance detection
on our source dataset. It is an end-to-end
memory network which incorporates both
CNN and LSTM for prediction.
• Adversarial Memory Network: We use ad-

versarial domain adaptation (Ganin et al.,
2016) instead of contrastive language adap-
tation in our cross-lingual memory network.

Results. Table 2 shows the performance of all
models on the target Arabic test set. The All-
unrelated and All-agree baselines perform poorly
across evaluation measures; All-unrelated per-
forms better than All-agree because unrelated is
the dominant class (∼ 68% of examples).

Rows 3–6 show that Gradient Boosting and En-
richedMLP yield similar results, while TFMLP per-
forms the worst. We attribute this to the advanced
features used in the two former models. Gradient
Boosting has better accuracy due to its better perfor-
mance on the dominant class. Note that Ensemble
performs poorly because of the limited labeled data,
which is insufficient to train a good CNN model.

Rows 7–9 show the results for the mono-lingual
memory network (MN) from (Mohtarami et al.,
2018). The performance of this model when trained
on Arabic data only (row 7) is comparable to pre-
vious baselines (rows 3–6). But, it shows poor
performance if trained on source English data and
tested on Arabic test data (row 8). The model per-
forms best (in terms of weighted accuracy and F1)
if first pretrained on source data and then fine-tuned
on target training data (row 9).

Row 10 in Table 2 shows the results for adver-
sarial memory network (ADMN). It improves the
performance of mono-lingual MN on weighted ac-
curacy and F1, but its accuracy significantly drops.
This is because adversarial approaches give higher
weights to samples of the majority class (i.e., unre-
lated) which makes classification more challenging
for the discriminator (Montahaei et al., 2018).

Row 11 shows the results for our cross-lingual
memory network (CLMN) with (α = .7); α con-
trols the balance between classification and lan-
guage adaptation losses (tuned using validation
data). CLMN outperforms other baselines in terms
of weighted accuracy and F1 while showing com-
parable accuracy. We show that the improvement
is due to language adaptation being able to effec-
tively transfer knowledge from the source to target
language (see Section 4.2).

The last column in Table 2 shows that unrelated
examples are the easiest ones. Also, although the
agree and the discuss classes have roughly the same
size, i.e., 474 and 409 examples, respectively, the
results for agree are notably higher. This is mainly
because the documents that discuss a claim often
share the same topic with the claim, but they do not
take a stance. In addition, the disagree examples
are the most difficult ones; this class is by far the
smallest one, with only 87 examples.



4448

Figure 3: Impact of pretraining on the macro-F1 across
α values. The Y axes show average results on the vali-
dation datasets with 5-fold cross-validation.

Methods Weigh. Acc. Acc. Macro-F1
CLMN (with pretraining) 60.2 69.8 43.2
CLMN (without pretraining) 61.3 71.6 45.2

Table 3: CLMN results on the target test dataset.

4 Discussion

4.1 Effect of Pretraining
Table 3 shows CLMN without pretraining (α = .7)
performs better on target test data than CLMN
with pretraining (α = .3), recall that α controls
the balance between classification and language
adaptation losses. Our further analysis shows that
pretraining biases the model toward the source lan-
guage. Figure 3 shows the impact of pretraining on
macro-average F1 score for CLMN across different
values of α on validation data. While the model
without pretraining achieves its best performance
with a large α (α = 0.7), the model with pretrain-
ing performs well with a smaller α (α = 0.3). This
suggests that our model can capture the characteris-
tics of the source dataset via pretraining when using
small supervision from language adaptation (i.e.,
small α). However, pretraining introduces bias to
the source space and the performance drops when
larger weights are given to language adaptation;
see the results with pretraining in Figure 3.

4.2 Assessment of Model Transferability
The improvements of CLMN model over the mono-
lingual MN models that use the target only, the
source only, or both the target and the source (rows
7–9 in Table 2 respectively) indicate its transfer-
ability. We further estimate transferability by mea-
suring the accuracy of the models in extracting
evidence that support their predictions. A more
accurate model should better transfer knowledge
to the target language by accurately learning the
relations between claims and pieces of evidence.

Figure 4: Transferability of our cross-lingual model.

Our target data has annotations (in terms of binary
labels) for each piece of evidence (here paragraph)
that indicate whether it is a rationale for the agree
or for the disagree class. Moreover, our inference
component (I) has a claim-evidence similarity vec-
tor, P cnnj , which can be used to rank pieces of evi-
dence from the target document against the target
claim.

We use the gold data and the rankings produced
by our model in order to measure its precision in
extracting evidence that supports its predictions.
Figure 4 shows that our CLMN model achieves pre-
cision of 40.2, 55.9, 66.0, 72.7, and 79.2 at ranks
1–5 respectively, and outperforms mono-lingual
MN models. This indicates that CLMN can better
generalize and transfer knowledge to the target lan-
guage through learning relations between pieces of
evidence and claims.

4.3 Effect of Language Adaptation

Figures 5a and 5b show the classification (LCA)
versus contrastive stance (LCSA) losses obtained
from our best language adaptation model (i.e., with-
out pretraining) across training epochs and α val-
ues. The results are averaged on validation data
when performing 5-fold cross-validation. As Fig-
ure 5a shows, there is greater reduction in the clas-
sification loss for smaller values of α, i.e., when
classification loss contributes more to the overall
loss; see Equation (4). On the other hand, Figure 5b
shows that the CSA loss decreases with larger val-
ues of α as the model pays more attention to the
CSA loss; see the red and green lines in Figure 5b.
These results indicate that our language adaptation
model can find a good balance between the classi-
fication loss and the CSA loss, with the value of
α = .7 yielding the best performance.



4449

(a) Classification loss (b) CSA loss

Figure 5: Classification vs. contrastive stance alignment (CSA) losses across training epochs and α values.

(a) without pretraining (b) with pretraining

Figure 6: Classification loss vs. contrastive stance alignment loss (CSA) vs. total loss during training.

Figure 6 compares the classification LCA, con-
trastive stanceLCSA, and totalL losses obtained by
our CLMN model on the validation dataset across
training epochs when the loss weight parameter (α)
is set to its best value. Figures 6a and 6b show
the results without and with pretraining for α = .7
and α = .3 respectively. Without pretraining (Fig-
ure 6a), the classification (light-blue line) and CSA
(dark-blue line) losses both decrease up to epoch
20, after which the classification loss keeps de-
creasing, but the CSA loss starts increasing. With
pretraining (Figure 6b), the CSA loss rapidly de-
creases for the first 10 epochs (even though it has
a small effect as α = .3), and then continues with
a smooth trend. This is because, during the initial
training epochs, the model is biased to the source
embedding space due to pretraining, and therefore
the source and the target examples are far from each
other. Then, our language adaptation model aligns
the source and the target examples to form a much

better shared embedding space and this alignment
strategy yields a rapid decrease of the CSA loss in
the first few epochs. Yet, in contrast to the CSA
loss, the classification loss increases in the first few
epochs. This is because the model enforces align-
ment between the source and the target samples
due to the large distances. Finally, the total loss
(orange line) indicates a good balance between the
classification and the language adaptation losses,
and it consistently decreases during training.

5 Related Work

Domain Adaptation. Previous work has pre-
sented several domain adaptation techniques. Un-
supervised domain adaptation approaches (Ganin
and Lempitsky, 2015; Long et al., 2016; Muandet
et al., 2013; Gong et al., 2012) attempt to align
the distribution of features in the embedding space
mapped from the source and the target domains.



4450

A limitation of such approaches is that, even
with perfect alignment, there is no guarantee that
the same-label examples from different domains
would map nearby in the embedding space. Super-
vised domain adaptation (Daumé and Marcu, 2006;
Becker et al., 2013; Bergamo and Torresani, 2010)
attempts to encourage same-label examples from
different domains to map nearby in the embedding
space. While supervised approaches perform better
than unsupervised ones, recent work (Motiian et al.,
2017) has demonstrated superior performance by
additionally encouraging class separation, mean-
ing that examples from different domains and with
different labels should be projected as far apart as
possible in the embedding space. Here, we com-
bined both types of alignments for cross-lingual
stance detection.

Stance Detection. This is an important compo-
nent for automatic fact-checking systems and ve-
racity inference (Nadeem et al., 2019; Zhang et al.,
2019; Atanasova et al., 2019). There have been
some nuances in the way researchers have defined
the stance detection task. Mohammad et al. (2016)
and Zarrella and Marsh (2016) worked on stances
regarding target propositions, e.g., entities, con-
cepts or events, as in-favor, against, or neither.
Most commonly, stance detection has been defined
with respect to a claim as agree, disagree, discuss
or unrelated (Hanselowski et al., 2018; Xu et al.,
2018; Fang et al., 2019). Previous work mostly de-
veloped the models with rich hand-crafted features
such as words, word embeddings, and sentiment
lexicons (Riedel et al., 2017; Baird et al., 2017;
Hanselowski et al., 2018). More recently, Mo-
htarami et al. (2018) presented a mono-lingual and
feature-light memory network for stance detection.
In this paper, we built on this work to extend pre-
vious efforts in stance detection to a cross-lingual
setting, achieving the state-of-the-art result on the
target language.

6 Conclusion and Future Work

We proposed an effective language adaptation ap-
proach to align class labels in source and target
languages for accurate cross-lingual stance detec-
tion. Moreover, we investigated the behavior of our
model in details and we have shown that it offers
sizable performance gains over a number of com-
peting approaches. In future, we will extend our
language adaptation model to document retrieval
and check-worthy claim detection tasks.

Acknowledgments

We thank the anonymous reviewers for their in-
sightful comments. This research was supported
in part by the Qatar Computing Research Institute,
HBKU4 and DSTA of Singapore.

References
Pepa Atanasova, Preslav Nakov, Lluı́s Màrquez, Al-

berto Barrón-Cedeño, Georgi Karadzhov, Tsve-
tomila Mihaylova, Mitra Mohtarami, and James
Glass. 2019. Automatic fact-checking using context
and discourse information. ACM Journal of Data
and Information Quality, 11(3):12:1–12:27.

Sean Baird, Doug Sibley, and Yuxi Pan. 2017. Ta-
los targets disinformation with fake news challenge
victory. https://blog.talosintelligence.com/2017/06/
talos-fake-news-challenge.html.

Ramy Baly, Mitra Mohtarami, James Glass, Lluı́s
Màrquez, Alessandro Moschitti, and Preslav Nakov.
2018. Integrating stance detection and fact check-
ing in a unified corpus. In Proceedings of the Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT), Volume 2, pages
21–27, New Orleans, Louisiana, USA.

Roy Bar-Haim, Indrajit Bhattacharya, Francesco Din-
uzzo, Amrita Saha, and Noam Slonim. 2017. Stance
classification of context-dependent claims. In
Proceedings of the 15th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, EACL ’17, pages 251–261, Valencia,
Spain.

Carlos J Becker, Christos M Christoudias, and Pas-
cal Fua. 2013. Non-linear domain adaptation with
boosting. In Proceedings of the 26th Interna-
tional Conference on Neural Information Process-
ing Systems - Volume 1, pages 485–493. Lake Tahoe,
Nevada.

Alessandro Bergamo and Lorenzo Torresani. 2010. Ex-
ploiting weakly-labeled web images to improve ob-
ject classification: a domain adaptation approach. In
Proceedings of the 23rd International Conference
on Neural Information Processing Systems - Volume
1, pages 181–189. Vancouver, British Columbia,
Canada.

Antoine Bordes, Y-Lan Boureau, and Jason Weston.
2017. Learning end-to-end goal-oriented dialog.
In Proceedings of the 5th International Conference
on Learning Representations, ICLR ’17, Toulon,
France.
4This research is part of the Tanbih project (available at

http://tanbih.qcri.org/) which aims to limit the
effect of “fake news”, propaganda and media bias by making
users aware of what they are reading.

http://tanbih.qcri.org/


4451

Hal Daumé, III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101–126.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1, pages 4171–4186, Minneapolis,
Minnesota, USA.

Sebastian Dungs, Ahmet Aker, Norbert Fuhr, and
Kalina Bontcheva. 2018. Can rumour stance alone
predict veracity? In Proceedings of the 27th Inter-
national Conference on Computational Linguistics,
pages 3360–3370, Santa Fe, NM, USA.

Wei Fang, Moin Nadeem, Mitra Mohtarami, and James
Glass. 2019. Neural multi-task learning for stance
prediction. In Proceedings of the EMNLP Workshop
on Fact Extraction and Verification, Hong Kong,
China.

Yaroslav Ganin and Victor Lempitsky. 2015. Unsu-
pervised domain adaptation by backpropagation. In
Proceedings of the 32nd International Conference
on Machine Learning - Volume 37, ICML’15, pages
1180–1189.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, François Lavi-
olette, Mario Marchand, and Victor Lempitsky.
2016. Domain-adversarial training of neural net-
works. Journal of Machine Learning Research,
17(1):2096–2030.

Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grau-
man. 2012. Geodesic flow kernel for unsupervised
domain adaptation. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition
(CVPR), pages 2066–2073.

Andreas Hanselowski, Avinesh PVS, Benjamin
Schiller, Felix Caspelherr, Debanjan Chaudhuri,
Christian M. Meyer, and Iryna Gurevych. 2018. A
retrospective analysis of the fake news challenge
stance-detection task. In Proceedings of the Inter-
national Conference on Computational Linguistics,
COLING ’18, pages 1859–1874, Santa Fe, NM,
USA.

Viet-Phi Huynh and Paolo Papotti. 2018. Towards a
benchmark for fact checking with knowledge bases.
In Companion Proceedings of the The Web Con-
ference 2018, WWW ’18, pages 1595–1598, Lyon,
France.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’14, pages 1746–1751, Doha,
Qatar.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Elena Kochkina, Maria Liakata, and Arkaitz Zubi-
aga. 2018. All-in-one: Multi-task learning for ru-
mour verification. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics,
pages 3402–3413, Santa Fe, NM, USA.

Guillaume Lample, Alexis Conneau, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018.
Word translation without parallel data. In Proceed-
ings of the 6th International Conference on Learning
Representations, Vancouver, BC, Canada.

David M.J. Lazer, Matthew A. Baum, Yochai Ben-
kler, Adam J. Berinsky, Kelly M. Greenhill, Filippo
Menczer, Miriam J. Metzger, Brendan Nyhan, Gor-
don Pennycook, David Rothschild, Michael Schud-
son, Steven A. Sloman, Cass R. Sunstein, Emily A.
Thorson, Duncan J. Watts, and Jonathan L. Zit-
train. 2018. The science of fake news. Science,
359(6380):1094–1096.

Mingsheng Long, Han Zhu, Jianmin Wang, and
Michael I. Jordan. 2016. Unsupervised domain
adaptation with residual transfer networks. In Pro-
ceedings of the 30th International Conference on
Neural Information Processing Systems, pages 136–
144, Barcelona, Spain.

Jing Ma, Wei Gao, and Kam-Fai Wong. 2017. De-
tect rumors in microblog posts using propagation
structure via kernel learning. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics, ACL ’17, pages 708–717, Van-
couver, Canada.

Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schütze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.

Tsvetomila Mihaylova, Georgi Karadzhov, Pepa
Atanasova, Ramy Baly, Mitra Mohtarami, and
Preslav Nakov. 2019. SemEval-2019 task 8: Fact
checking in community question answering forums.
In Proceedings of the 13th International Workshop
on Semantic Evaluation, pages 860–869, Minneapo-
lis, Minnesota, USA.

Tsvetomila Mihaylova, Preslav Nakov, Lluı́s Màrquez,
Alberto Barrón-Cedeño, Mitra Mohtarami, Georgi
Karadzhov, and James Glass. 2018. Fact checking
in community forums. In Proceedings of the 32nd
AAAI Conference on Artificial Intelligence, New Or-
leans, LA, USA.

Saif Mohammad, Svetlana Kiritchenko, Parinaz Sob-
hani, Xiao-Dan Zhu, and Colin Cherry. 2016.
SemEval-2016 task 6: Detecting stance in tweets. In
Proceedings of Semantic Evaluation, SemEval ’16,
pages 31–41, Berlin, Germany.



4452

Mitra Mohtarami, Ramy Baly, James Glass, Preslav
Nakov, Lluı́s Màrquez, and Alessandro Moschitti.
2018. Automatic stance detection using end-to-end
memory networks. In Proceedings of the Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics (Long Papers),
pages 767–776, New Orleans, LA, USA.

Ehsan Montahaei, Mahsa Ghorbani, Mahdieh Soley-
mani Baghshah, and Hamid R. Rabiee. 2018. Ad-
versarial classifier for imbalanced problems. CoRR,
abs/1811.08812.

Saeid Motiian, Marco Piccirilli, Donald A. Adjeroh,
and Gianfranco Doretto. 2017. Unified deep super-
vised domain adaptation and generalization. In Pro-
ceedings of the IEEE International Conference on
Computer Vision, pages 5715–5725, Venice, Italy.

Krikamol Muandet, David Balduzzi, and Bernhard
Schölkopf. 2013. Domain generalization via invari-
ant feature representation. In Proceedings of the
30th International Conference on Machine Learning,
volume 28, pages 10–18, Atlanta, GA, USA.

Moin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami,
and James Glass. 2019. FAKTA: An automatic end-
to-end fact checking system. In Proceedings of the
Conference of the North American Chapter of the
Association for Computational Linguistics (Demon-
strations), pages 78–83, Minneapolis, MN, USA.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 2227–2237, New Orleans, LA, USA.

Kashyap Popat, Subhabrata Mukherjee, Jannik
Strötgen, and Gerhard Weikum. 2017. Where the
truth lies: Explaining the credibility of emerging
claims on the Web and social media. In Proceedings
of the Conference on World Wide Web, WWW ’17,
pages 1003–1012, Perth, Australia.

Benjamin Riedel, Isabelle Augenstein, Georgios P Sp-
ithourakis, and Sebastian Riedel. 2017. A simple but
tough-to-beat baseline for the Fake News Challenge
stance detection task. ArXiv:1707.03264.

Parinaz Sobhani, Diana Inkpen, and Xiaodan Zhu.
2017. A dataset for multi-target stance detection. In
Proceedings of the 15th Conference of the European
Chapter of the Association for Computational Lin-
guistics: Volume 2, pages 551–557, Valencia, Spain.

Karen Spärck Jones. 2004. Idf term weighting and
ir research lessons. Journal of documentation,
60(5):521–523.

Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
and Rob Fergus. 2015. End-to-end memory net-
works. In Proceedings of the 28th International
Conference on Neural Information Processing Sys-
tems, pages 2440–2448, Montreal, Canada.

Ming Tan, Cicero dos Santos, Bing Xiang, and Bowen
Zhou. 2016. Improved representation learning for
question answer matching. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
464–473, Berlin, Germany.

James Thorne, Andreas Vlachos, Christos
Christodoulopoulos, and Arpit Mittal. 2018.
FEVER: a large-scale dataset for fact extraction and
VERification. In Proceedings of the Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, NAACL-HLT ’18, pages 809–819,
New Orleans, LA, USA.

Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate
Saenko. 2017. Adversarial discriminative domain
adaptation. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition,
CVPR ’17, Honolulu, HI, USA.

Andreas Vlachos and Sebastian Riedel. 2014. Fact
checking: Task definition and dataset construc-
tion. In Proceedings of the ACL Workshop on Lan-
guage Technologies and Computational Social Sci-
ence, pages 18–22, Baltimore, MD, USA.

Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018.
The spread of true and false news online. Science,
359(6380):1146–1151.

William Yang Wang. 2017. “liar, liar pants on fire”:
A new benchmark dataset for fake news detection.
In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics, ACL ’17,
pages 422–426, Vancouver, Canada.

Caiming Xiong, Stephen Merity, and Richard Socher.
2016. Dynamic memory networks for visual and
textual question answering. In Proceedings of the
33rd International Conference on Machine Learn-
ing, ICML ’16, pages 2397–2406, New York, NY,
USA.

Brian Xu, Mitra Mohtarami, and James Glass. 2018.
Adversarial domain adaptation for stance detection.
In Proceedings of the NeurIPS Workshop on Contin-
ual Learning, Montreal, Canada.

Guido Zarrella and Amy Marsh. 2016. MITRE at
SemEval-2016 Task 6: Transfer learning for stance
detection. In Proceedings of the 10th International
Workshop on Semantic Evaluation, SemEval ’16,
pages 458–463, San Diego, CA, USA.

Yifan Zhang, Giovanni Da San Martino, Alberto
Barrón-Cedeño, Salvatore Romeo, Jisun An, Hae-
woon Kwak, Todor Staykovski, Israa Jaradat, Georgi
Karadzhov, Ramy Baly, Kareem Darwish, James
Glass, and Preslav Nakov. 2019. Tanbih: Get to
know what you are reading. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (Demonstrations), EMNLP ’19,
Hong Kong, China.


