



















































Applying HMEANT to English-Russian Translations


Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 43–50,
October 25, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

Applying HMEANT to English-Russian Translations

Alexander Chuchunkov Alexander Tarelkin
{madfriend,newtover,galinskaya}@yandex-team.ru

Yandex LLC
Leo Tolstoy st. 16, Moscow, Russia

Irina Galinskaya

Abstract

In this paper we report the results of
first experiments with HMEANT (a semi-
automatic evaluation metric that assesses
translation utility by matching semantic
role fillers) on the Russian language. We
developed a web-based annotation inter-
face and with its help evaluated practica-
bility of this metric in the MT research
and development process. We studied reli-
ability, language independence, labor cost
and discriminatory power of HMEANT
by evaluating English-Russian translation
of several MT systems. Role labeling
and alignment were done by two groups
of annotators - with linguistic background
and without it. Experimental results were
not univocal and changed from very high
inter-annotator agreement in role labeling
to much lower values at role alignment
stage, good correlation of HMEANT with
human ranking at the system level sig-
nificantly decreased at the sentence level.
Analysis of experimental results and anno-
tators’ feedback suggests that HMEANT
annotation guidelines need some adapta-
tion for Russian.

1 Introduction

Measuring translation quality is one of the most
important tasks in MT, its history began long ago
but most of the currently used approaches and
metrics have been developed during the last two
decades. BLEU (Papineni et al., 2002), NIST
(Doddington, 2002) and METEOR (Banerjee and
Lavie, 2005)metric require reference translation
to compare it with MT output in fully automatic
mode, which resulted in a dramatical speed-up for
MT research and development. These metrics cor-
relate with manual MT evaluation and provide re-

liable evaluation for many languages and for dif-
ferent types of MT systems.

However, the major problem of popular MT
evaluation metrics is that they aim to capture lexi-
cal similarity of MT output and reference transla-
tion (fluency), but fail to evaluate the semantics of
translation according to the semantics of reference
(adequacy) (Lo and Wu, 2011a). An alternative
approach that is worth mentioning is the one pro-
posed by Snover et al. (2006), known as HTER,
which measures the quality of machine translation
in terms of post-editing. This method was proved
to correlate well with human adequacy judgments,
though it was not designed for a task of gisting.
Moreover, HTER is not widely used in machine
translation evaluation because of its high labor in-
tensity.

A family of metrics called MEANT was pro-
posed in 2011 (Lo and Wu, 2011a), which ap-
proaches MT evaluation differently: it measures
how much of an event structure of reference does
machine translation preserve, utilizing shallow se-
mantic parsing (MEANT metric) or human anno-
tation (HMEANT) as a gold standard.

We applied HMEANT to a new language —
Russian — and evaluated the usefulness of met-
ric. The practicability for the Russian language
was studied with respect to the following criteria
provided by Birch et al. (2013):

Reliability – measured as inter-annotator agree-
ment for individual stages of evaluation task.

Discriminatory Power – the correlation of
rankings of four MT systems (by manual evalu-
ation, BLEU and HMEANT) measured on a sen-
tence and test set levels.

Language Independence – we collected the
problems with the original method and guidelines
and compared these problems to those reported by
Bojar and Wu (2012) and Birch et al. (2013).

Efficiency – we studied the labor cost of anno-
tation task, i. e. average time required to evaluate

43



translations with HMEANT. Besides, we tested
the statement that semantic role labeling (SRL)
does not require experienced annotators (in our
case, with linguistic background).

Although the problems of HMEANT were out-
lined before (by Bojar and Wu (2012) and Birch
et al. (2013)) and several improvements were pro-
posed, we decided to step back and conduct ex-
periments with HMEANT in its original form. No
changes to the metric, except for the annotation
interface enhancements, were made.

This paper has the following structure. Sec-
tion 2 reports the previous experiments with
HMEANT; section 3 summarizes the methods be-
hind HMEANT; section 4 – the settings for our
own experiments; sections 5 and 6 are dedicated
to results and discussion.

2 Related Work

Since the beginning of the machine translation era
the idea of semantics-driven approach for transla-
tion wandered around in the MT researchers com-
munity (Weaver, 1955). Recent works by Lo and
Wu (2011a) claim that this approach is still per-
spective. These works state that in order for ma-
chine translation to be useful, it should convey the
shallow semantic structure of the reference trans-
lation.

2.1 MEANT for Chinese-English
Translations

The original paper on MEANT (Lo and Wu,
2011a) proposes the semi-automatic metric, which
evaluates machine translations utilizing annotated
event structure of a sentence both in reference and
machine translation. The basic assumption be-
hind the metric can be stated as follows: trans-
lation shall be considered "good" if it preserves
shallow semantic (predicate-argument) structure
of reference. This structure is described in the pa-
per on shallow semantic parsing (Pradhan et al.,
2004): basically, we approach the evaluation by
asking simple questions about events in the sen-
tence: "Who did what to whom, when, where, why
and how?". These structures are annotated and
aligned between two translations. The authors of
MEANT reported results of several experiments,
which utilized both human annotation and seman-
tic role labeling (as a gold standard) and automatic
shallow semantic parsing. Experiments show that
HMEANT correlates with human adequacy judg-

ments (for three MT systems) at the value of 0.43
(Kendall tau, sentence level), which is very close
to the correlation of HTER (BLEU has only 0.20).
Also inter-annotator agreement was reported for
two stages of annotation: role identification (se-
lecting the word span) and role classification (la-
beling the word span with role). For the former,
IAA ranged from 0.72 to 0.93 (which can be in-
terpreted as a good agreement) and for the latter,
from 0.69 to 0.88 (still quite good, but should be
put in doubt). IAA for the alignment stage was not
reported.

2.2 HMEANT for Czech-English
Translations

MEANT and HMEANT metrics were adopted
for an experiment on evaluation of Czech-English
and English-Czech translations by Bojar and
Wu (2012). These experiments were based on
a human-evaluated set of 40 translations from
WMT121, which were submitted by 13 systems;
each system was evaluated by exactly one anno-
tator, plus an extra annotator for reference trans-
lations. This setting implied that inter-annotator
agreement could not be examined. HMEANT cor-
relation with human assessments was reported as
0.28, which is significantly lower than the value
obtained by Lo and Wu (2011a).

2.3 HMEANT for German-English
Translations

Birch et al. (2013) examined HMEANT thor-
oughly with respect to four criteria, which address
the usefulness of a task-based metric: reliability,
efficiency, discriminatory power and language in-
dependence. The authors conducted an experi-
ment to evaluate three MT systems: rule-based,
phrase-based and syntax-based on a set of 214 sen-
tences (142 German and 72 English). IAA was
broken down into the different stages of annotation
and alignment. The experimental results showed
that whilst the IAA for HMEANT is satisfying at
the first stages of the annotation, the compound-
ing effect of disagreement at each stage (up to
the alignment stage) greatly reduced the effective
overall IAA — to 0.44 on role alignment for Ger-
man, and, only slightly better, 0.59 for English.
HMEANT successfully distinguished three types
of systems, however, this result could not be con-
sidered reliable as IAA is not very high (and rank

1http://statmt.org/wmt12

44



correlation was not reported). The efficiency of
HMEANT was stated as reasonably good; how-
ever, it was not compared to the labor cost of (for
example) HTER. Finally, the language indepen-
dence of the metric was implied by the fact that
original guidelines can be applied both to English
and German translations.

3 Methods

3.1 Evaluation with HMEANT

The underlying annotation cycle of HMEANT
consists of two stages: semantic role labeling
(SRL) and alignment. During the SRL stage, each
annotator is asked to mark all the frames (a pred-
icate and associated roles) in reference translation
and hypothesis translation. To annotate a frame,
one has to mark the frame head – predicate (which
is a verb, but not a modal verb) and its argu-
ments, role fillers, which are linked to that pred-
icate. These role fillers are given a role from the
inventory of 11 roles (Lo and Wu, 2011a). The
role inventory is presented in Table 1, where each
role corresponds to a specific question about the
whole frame.

Who? What? Whom?
Agent Patient Benefactive
When? Where? Why?
Temporal Locative Purpose
How?
Manner, Degree, Negation, Modal, Other

Table 1. The role inventory.

On the second stage, the annotators are asked
to align the elements of frames from reference
and hypothesis translations. The annotators link
both actions and roles, and these alignments can
be matched as “Correct” or “Partially Correct” de-
pending on how well the meaning was preserved.
We have used the original minimalistic guidelines
for the SRL and alignment provided by Lo and Wu
(2011a) in English with a small set of Russian ex-
amples.

3.2 Calculating HMEANT

After the annotation, HMEANT score of the
hypothesis translation can be calculated as the
F-score from the counts of matches of predicates
and their role fillers (Lo and Wu, 2011a). Pred-
icates (and roles) without matches are not ac-

counted, but they result in the lower value overall.
We have used the uniform model of HMEANT,
which is defined as follows.
#Fi – number of correct role fillers for predicate
i in machine translation;
#Fi(partial) – number of partially correct role
fillers for predicate i in MT;
#MTi, #REFi – total number of role fillers in
MT or reference for predicate i;
Nmt, Nref – total number of predicates in MT or
reference;
w – weight of the partial match (0.5 in the uniform
model).

P =
∑

matched i

#Fi
#MTi

R =
∑

matched i

#Fi
#REFi

Ppart =
∑

matched i

#Fi(partial)
#MTi

Rpart =
∑

matched i

#Fi(partial)
#REFi

Ptotal =
P + w ∗ Ppart

Nmt
Rtotal =

R+ w ∗Rpart
Nref

HMEANT =
2 ∗ Ptotal ∗Rtotal
Ptotal +Rtotal

3.3 Inter-Annotator Agreement
Like Lo and Wu (2011a) and Birch et al. (2013)
we studied inter-annotator agreement (IAA). It is
defined as an F1-measure, for which we consider
one of the annotators as a gold standard:

IAA =
2 ∗ P ∗R
P +R

Where precision (P ) is the number of labels (roles,
predicates or alignments) that match between an-
notators divided by the total number of labels by
annotator 1; recall (R) is the number of matching
labels divided by the total number of labels by an-
notator 2. Following Birch et al. (2013), we con-
sider only exact word span matches. Also we have
adopted the individual stages of the annotation
procedure that are described in (Birch et al. 2013):
role identification (selecting the word span), role
classification (marking the word span with a role),
action identification (marking the word span as a
predicate), role alignment (linking roles between
translations) and action alignment (linking frame
heads). Calculating IAA for each stage separately

45



helped to isolate the disagreements and to see,
which stages resulted in a low agreement value
overall. To look at the most common role dis-
agreements we also created the pairwise agree-
ment matrix, every cell (i, j) of which is the num-
ber of times the role i was confused with the role
j by any pair of annotators.

3.4 Kendall’s Tau Rank Correlation With
Human Judgments

For the set of translations used in our experiments,
we had a number of relative human judgments (the
set was taken from WMT132). We used the rank
aggregation method described in (Callison-Burch
et al., 2012) to build up one ranking from these
judgments. This method is called Expected Win
Score (EWS) and for MT system Si from the set
{Sj} it is defined the following way:

score(Si) =
1

|{Sj}|
∑
j,j 6=i

win(Si, Sj)
win(Si, Sj) + win(Sj , Si)

Where win(Si, Sj) is the number of times system
i was given a rank higher than system j. This
method of aggregation was used to obtain the com-
parisons of systems, which outputs were never
presented together to assessors during the evalu-
ation procedure at WMT13.

After we had obtained the ranking of systems
by human judgments, we compared this ranking
to the ranking by HMEANT values of machine
translations. To do that, we used Kendall’s tau
(Kendall, 1938) rank correlation coefficient and
reported the results as Lo and Wu (2011a) and Bo-
jar (Bojar and Wu, 2012).

4 Experimental Setup

4.1 Test Set
For our experiments we used the set of translations
from WMT13. We tested HMEANT on a set of
four best MT systems (Bojar et al., 2013) for the
English-Russian language pair (Table 2).

From the set of direct English-Russian transla-
tions (500 sentences) we picked those which al-
lowed to build a ranking for the four systems (94
sentences); then out of these we randomly picked
50 and split them into 6 tasks of 25 so that each
of the 50 sentences was present in exactly three
tasks. Each task consisted of 25 reference transla-
tions and 100 hypothesis translations.

2http://statmt.org/wmt13

System EWS (WMT)
PROMT 0.4949
Online-G 0.475
Online-B 0.3898
CMU-Primary 0.3612

Table 2. The top four MT systems for the en-ru
translation task at WMT13. The scores were

calculated for the subset of translations which we
used in experiments.

4.2 Annotation Interface

As far as we know there is no publically available
interface for HMEANT annotation. Thus, first
of all, having the prototype (Lo and Wu, 2011b)
and taking into account comments and sugges-
tions of Bojar and Wu (2012) (e.g., ability to go
back within the phases of annotation), we created
a web-based interface for role labeling and align-
ment. This interface allows to annotate a set of
references with one machine translation at a time
(Figure 1) and to align actions and roles. We also
provided a timer which allowed to measure the
time required to label the predicates and roles.

4.3 Annotators

We asked to participate two groups of annota-
tors: 6 researchers with linguistic background (lin-
guists) and 4 developers without it. Every annota-
tor did exactly one task; each of the 50 sentences
was annotated by three linguists and at least two
developers.

5 Results

As a result of the experiment, 638 frames were
annotated in reference translations (overall) and
2 016 frames in machine translations. More de-
tailed annotation statistics are presented in Table
3. A closer look indicates that the ratio of aligned
frames and roles in references was larger than in
any of machine translations.

5.1 Manual Ranking

After the test set was annotated, we compared
manual ranking and ranking by HMEANT; on the
system level, these rankings were similar; how-
ever, on the sentence level, there was no correla-
tion between rankings at all. Thus we decided to
take a closer look at the manual assessments. For
the selected 4 systems most of the pairwise com-

46



Figure 1. The screenshot of SRL interface. The tables under the sentences contain the information
about frames (the active frame has a red border and is highlighted in the sentence, inactive frames (not

shown) are semi-transparent).

Source # Frames # Roles Aligned frames, % Aligned roles, %
Reference 638 1 671 86.21 % 74.15 %
PROMT 609 1 511 79.97 % 67.57 %
Online-G 499 1 318 77.96 % 66.46 %
Online-B 469 1 257 78.04 % 68.42 %
CMU-Primary 439 1 169 75.17 % 66.30 %

Table 3. Annotation statistics.

parisons were obtained in a transitive way, i. e.
using comparisons with other systems. Further-
more, we encountered a number of useless rank-
ings, where all the outputs were given the same
rank. After all, for many sentences the ranking
of systems was based on a few pairwise compar-
isons provided by one or two annotators. These
rankings seemed to be not very reliable, thus we
decided to rank four machine translations for each
of the 50 sentences manually to make sure that the
ranking has a strong ground. We asked 6 linguists
to do that task. The average pairwise rank correla-
tion (between assessors) reached 0.77, making the
overall ranking reliable; we aggregated 6 rankings
for each sentence using EWS.

5.2 Correlation with Manual Assessments

To look at HMEANT on a system level, we com-
pared rankings produced during manual assess-
ment and HMEANT annotation tasks. Those rank-
ings were then aggregated with EWS (Table 4).

It should be noticed that HMEANT allowed to
rank systems correctly. This fact indicates that
HMEANT has a good discriminatory power on the
level of systems, which is a decent argument for

System Manual HMEANT BLEU
PROMT 0.532 0.443 0.126
Online-G 0.395 0.390 0.146
Online-B 0.306 0.374 0.147
CMU-Primary 0.267 0.292 0.136

Table 4. EWS over manual assessments, EWS
over HMEANT and BLEU scores for MT

systems.

the usage of this metric. Also it is worth to note
that ranking by HMEANT matched the ranking by
the number of frames and roles (Table 3).

On a sentence level, we studied the rank corre-
lation of ranking by manual assessments and by
HMEANT values for each of the annotators. The
manual ranking was aggregated by EWS from the
manual evaluation task (see Section 5.1). Results
are reported in Table 5.

We see that resulting correlation values are sig-
nificantly lower than those reported by Lo and Wu
(2011a) – our rank correlation values did not reach
0.43 on average across all the annotators (and even
0.28 as reported by Bojar and Wu (2012)).

47



Annotator τ
Linguist 1 0.0973
Linguist 2 0.3845
Linguist 3 0.1157
Linguist 4 -0.0302
Linguist 5 0.1547
Linguist 6 0.1468
Developer 1 0.1794
Developer 2 0.2411
Developer 3 0.1279
Developer 4 0.1726

Table 5. The rank correlation coefficients for
HMEANT and human judgments. Reliable

results (with p-value >0.05) are in bold.

5.3 Inter-Annotator Agreement

Following Lo and Wu (2011a) and Birch et al.
(2013) we report the IAA for the individual stages
of annotation and alignment. These results are
shown in Table 6.

Stage Linguists DevelopersMax Avg Max Avg
REF, id 0.959 0.803 0.778 0.582
MT, id 0.956 0.795 0.667 0.501
REF, class 0.862 0.715 0.574 0.466
MT, class 0.881 0.721 0.525 0.434
REF, actions 0.979 0.821 0.917 0.650
MT, actions 0.971 0.839 0.700 0.577
Actions – align 0.908 0.737 0.429 0.332
Roles – align 0.709 0.523 0.378 0.266

Table 6. The inter-annotator agreement for the
individual stages of annotation and alignment

procedures. Id, class, align stand for
identification, classification and alignment

respectively.

The results are not very different from those re-
ported in the papers mentioned above, except for
even lower agreement for developers. The fact
that the results could be reproduced on a new lan-
guage seems very promising, however, the lack of
training for the annotators without linguistic back-
ground resulted in lower inter-annotator agree-
ment.

Also we studied the most common role dis-
agreements for each pair of annotators (either lin-
guists or developers). As it can be deduced from

the IAA values, the agreement on all roles is lower
for linguists, however, both groups of annotators
share the roles on which the agreement is best of
all: Predicate, Agent, Locative, Negation, Tempo-
ral. Most common disagreements are presented in
Table 7.

Role A Role B %, L %, D
Whom What 18.0 15.2
Whom Who 13.7 23.1
Why None 17.0 22.3
How (manner) What 10.5 -
How (manner) How (degree) - 19.0
How (modal) Action 18.1 16.3

Table 7. Most common role disagreements. Last
columns (L for linguists, D for developers) stand
for the ratio of times Role A was confused with

Role B across all the label types (roles, predicate,
none).

These disagreements can be explained by the
fact that some annotators looked “deeper” in the
sentence semantics, whereas other annotators only
tried to capture the shallow structure as fast as pos-
sible. This fact explains, for example, disagree-
ment on the Whom role – for some sentences, e. g.
“mogli by ubedit~ politiqeskih liderov”
(“could persuade the political leaders”) it requires
some time to correctly mark politiqeskih lid-
erov (political leaders) as an answer to Whom,
not What. The disagreement on the Purpose (a lot
of times it was annotated only by one expert) is ex-
plained by the fact that there were no clear instruc-
tions on how to mark clauses. As for the Action
and Modal, this disagreement is based on the re-
quirement that Action should consist of one word
only; this requirement raised questions about com-
plex verbs, e.g. “zakonqil delat~” (“stopped
doing”). It is ambiguous how to annotate these
verbs: some annotators decided to mark it as
Modal+Action, some – as Action+What. Proba-
bly, the correct way to mark it should be just as
Action.

5.4 Efficiency

Additionnaly, we conducted an efficiency experi-
ment in the group of linguists. We measured the
average time required to annotate a predicate (in
reference or machine translation) and a role. Re-
sults are presented in Table 8.

48



Annotator REF MTRole Action Role Action
Linguist 1 14 26 11 36
Linguist 2 10 12 8 12
Linguist 3 13 14 8 23
Linguist 4 16 15 9 15
Linguist 5 13 20 11 24
Linguist 6 17 35 9 32

Table 8. Average times (in seconds) required to
annotate actions and roles.

These results look very promising; using the
numbers in Table 3, we get the average time re-
quired to annotate a sentence: 1.5 – 2 minutes for a
reference (and even up to 4 minutes for slower lin-
guists) and 1.5 – 2.5 minutes for a machine trans-
lation. Also for a group of “slower” linguists (1, 5,
6) inter-annotator agreement was lower (-0.05 on
average) than between “faster” linguists (2, 3, 4)
for all stages of annotation and alignment. Aver-
age time to annotate an action is similar for the ref-
erence and MT outputs, but it takes more time to
annotate roles in references than in machine trans-
lations.

6 Discussion

6.1 Problems with HMEANT

As we can see, HMEANT is an acceptably reliable
and efficient metric. However, we have met some
obstacles and problems with original instructions
during the experiments with Russian translations.
We believe that these obstacles are the main causes
of low inter-annotator agreement at the last stages
of annotation procedure and low correlation of
rankings.

Frame head (predicate) is required. This re-
quirement does not allow frames without predicate
at all, e.g. “On mo� drug” (“He is my friend”) –
the Russian translation of “is” (present tense) is a
null verb.

One-word predicates. There are cases where
complex verbs (e.g., which consist of two verbs)
can be correctly translated as a one-word verb.
For example, “ostanovils�” (“stopped”) is
correctly rephrased as “perestal delat~”
(“ceased doing”).

Roles only of one type can be aligned. Some-
times one role can be correctly rephrased as an-
other role, but roles of different type can not be

aligned. For example, “On uehal iz goroda”
(“He went away from the town”) means the same
as “On pokinul gorod” (“He left the town”).
The former has a structure of Who + Action +
Where, the latter – Who + Action + What.

Should we annotate as much as possible? It
is not clear from the guideline whether we should
annotate almost everything that looks like a frame
or can be interpreted as a role. There are some
prepositional phrases which can not be easily clas-
sified as one role or another. Example: “Nam ne
stoit ob �tom volnovat~s�” (“We should
not worry about this”) – it is not clarified how to
deal with “ob �tom” (“about this”) prepositional
phrase.

7 Conclusion

In this paper we describe a preliminary series of
experiments with HMEANT, a new metric for se-
mantic role labeling. In order to conduct these ex-
periments we developed a special web-based an-
notation interface with a timing feature. A team
of 6 linguists and 4 developers annotated Russian
MT output of 4 systems. The test set of 50 En-
glish sentences along with reference translations
was taken from the WMT13 data. We measured
IAA for each stage of annotation process, com-
pared HMEANT ranking with manual assessment
and calculated the correlation between HMEANT
and manual evaluation. We also measured anno-
tation time and collected a feedback from anno-
tators, which helped us to locate the problems and
better understand the SRL process. Analysis of the
preliminary experimental results of Russian MT
output annotation led us to the following conclu-
sions about HMEANT as a metric.

Language Independence. For a relatively
small set of Russian sentences, we encountered
problems with the guidelines, but they were not
specific to the Russian language. This can be
naively interpreted as language independence of
the metric.

Reliability. Inter-annotator agreement is high
for the first stages of SRL, but we noted that it de-
creases on the last stages because of the compound
effect of disagreements on previous stages.

Efficiency. HMEANT proved to be really ef-
fective in terms of time required to annotate ref-
erences and MT outputs and can be used in pro-
duction environment, though the statement that
HMEANT annotation task does not require quali-

49



fied annotators was not confirmed.
Discriminatory Power. On the system level,

HMEANT allowed to correctly rank MT systems
(according to the results of manual assessment
task). On the sentence level, correlation with hu-
man rankings is low.

To sum up, first experience with HMEANT
was considered to be successful and allowed us
to make a positive decision about applicability
of the new metric to the evaluation of English-
Russian machine translations. We have to say that
HMEANT guidelines, annotation procedures and
the inventory of roles work in general, however,
low inter-annotator agreement at the last stages
of annotation task and low correlation with hu-
man judgments on the sentence level suggest us
to make respective adaptations and conduct new
series of experiments.

Acknowledgements

We would like to thank our annotators for their ef-
forts and constructive feedback. We also wish to
express our great appreciation to Alexey Baytin
and Maria Shmatova for valuable ideas and ad-
vice.

References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An

automatic metric for mt evaluation with improved
correlation with human judgments. In Proceed-
ings of the ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation
and/or Summarization, pages 65–72.

Alexandra Birch, Barry Haddow, Ulrich Germann,
Maria Nadejde, Christian Buck, and Philipp Koehn.
2013. The Feasibility of HMEANT as a Human
MT Evaluation Metric. In Proceedings of the Eighth
Workshop on Statistical Machine Translation, page
52–61, Sofia, Bulgaria, August. Association for
Computational Linguistics.

Ondrej Bojar and Dekai Wu. 2012. Towards a
Predicate-Argument Evaluation for MT. In Pro-
ceedings of the Sixth Workshop on Syntax, Semantics
and Structure in Statistical Translation, page 30–38,
Jeju, Republic of Korea, July. Association for Com-
putational Linguistics.

Ondřej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 workshop
on statistical machine translation. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, page 1–44, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.

Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, page
10–51, Montréal, Canada, June. Association for
Computational Linguistics.

George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology Research, HLT ’02, pages 138–145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.

Maurice G Kendall. 1938. A new measure of rank
correlation. Biometrika, pages 81–93.

Chi-kiu Lo and Dekai Wu. 2011a. MEANT: An
inexpensive, high-accuracy, semi-automatic metric
for evaluating translation utility based on semantic
roles. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, page 220–229, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.

Chi-kiu Lo and Dekai Wu. 2011b. A radically sim-
ple, effective annotation and alignment methodol-
ogy for semantic frame based smt and mt evaluation.
LIHMT 2011, page 58.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Sameer S Pradhan, Wayne Ward, Kadri Hacioglu,
James H Martin, and Daniel Jurafsky. 2004. Shal-
low semantic parsing using support vector machines.
In HLT-NAACL, pages 233–240.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas, pages 223–231.

Warren Weaver. 1955. Translation. Machine transla-
tion of languages, 14:15–23.

50


