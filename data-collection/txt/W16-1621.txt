



















































Pair Distance Distribution: A Model of Semantic Representation


Proceedings of the 1st Workshop on Representation Learning for NLP, pages 184–192,
Berlin, Germany, August 11th, 2016. c©2016 Association for Computational Linguistics

Pair Distance Distribution:
a Model of Semantic Representation

Yonatan Ramni Oded Maimon
Department of Industrial Engineering

Tel-Aviv University
Tel-Aviv, Israel

{yona5;maimon;xmel}@post.tau.ac.il

Evgeni Khmelnitsky

Abstract

We introduce PDD (Pair Distance Distri-
bution), a novel corpus-based model of se-
mantic representation. Most corpus-based
models are VSMs (Vector Space Models),
which while being successful, suffer from
both practical and theoretical shortcom-
ings. VSM models produce very large,
sparse matrices, and dimensionality reduc-
tion is usually performed, leading to high
computational complexity, and obscuring
the meaning of the dimensions. Similarity
in VSMs is constrained to be both sym-
metric and transitive, contrary to evidence
from human subject tests. PDD is feature-
based, created automatically from corpora
without producing large, sparse matrices.
The dimensions along which words are
compared are meaningful, enabling better
understanding of the model and providing
an explanation as to how any two words
are similar. Similarity is neither symmet-
ric nor transitive. The model achieved ac-
curacy of 97.6% on a published semantic
similarity test.

1 Introduction

Semantic representation models are described by
Mitchell and Lapata (2008) as belonging to one of
three families, semantic networks, feature-based
models and semantic spaces. Briefly, semantic
networks represent words as nodes in a graph and
the semantic relations between them as edges, and
similarity between words is represented by the
path length between them. Edges may represent a
variety of different relations. Feature-based mod-
els assign a list of discrete features to each word,
and similarity of words is obtained from the com-
monalities and differences of their feature sets. As

indicated by Mitchell and Lapata (2008), seman-
tic networks and feature-based models are often
manually created by modelers, so that an effort is
required to produce them, and the results are sub-
jective.

Semantic spaces, also named DSMs (distribu-
tional semantic models) by Baroni and Lenci
(2010), rely on the distributional hypothesis, that
words that occur in the same contexts tend to have
similar meanings (Harris, 1954). At their most ba-
sic form, word co-occurrences in various contexts
are used to form feature vectors of words. DSMs
are divided by Baroni and Lenci (2010) into un-
structured DSMs, where word co-occurrences are
counted without regard to the relation between the
words, and structured DSMs, where triples of two
words and particular syntactic or lexico-syntactic
relations between them are counted. Various fea-
ture weighting schemes are employed, and a VSM
(vector space model) is usually formed using the
feature vectors (topic modeling (Griffiths et al.,
2007) is a notable exception). Similarities be-
tween words are measured by distances between
vectors in this multi-dimensional space, usually
following a dimensionality reduction. VSMs have
been successful in a number of tasks, such as word
similarity and word-relation similarity tests. How-
ever, VSMs have several shortcomings. Placing all
words in a multi-dimensional space, with greater
distance between any two words signifying lower
similarity between them, implies:

• All words have some similarity with one an-
other.

• For any word, all other words can be ordered
by their similarity to the given word.

• All pairs of words can be ordered by their
similarity.

• Similarity is symmetric.

184



• Transitivity - if any two words are both very
similar to a third word, they cannot be very
dissimilar.

• All instances of a word, whether the word is
ambiguous, polysemous, or attains different
meanings in different contexts, are mapped
to the same position in space.

It is our view that for similarity to exist between
two concepts (represented in our case by words),
they must have something in common, such as a
common dimension along which they have (possi-
bly different) values. With nothing in common,
two concepts bear no similarity to one another,
which is not the same as having little similarity.
As with relatives, some are close relatives of a per-
son, others are more distant relations of his, and
yet others are not related to him at all. Further-
more, similarity is ordinal, with numerical values,
when given by human subjects, serving as an aid
in ranking similarity, as is done with feelings of
pain, or happiness. Let’s illustrate some limita-
tions of VSMs with examples.
Example 1: is ’bank’ more similar to ’embank-
ment’ or to ’stock exchange’? ’bank’ is ambigu-
ous, so a possible solution would be to map these
two different senses independently to different po-
sitions in multi-dimensional space, assuming one
could automatically disambiguate them.
Example 2: is ’break’ more similar to ’interrupt’,
’separate’, ’breach’, ’burst’, or ’violate’? ’break’
is polysemous, with WordNet1 listing 59 senses,
which are in various degrees related to one an-
other, just for the verb. In this case, it does not
seem right to map each sense independently, as
they share some meaning.
Example 3: is ’queen’ more similar to ’king’ or to
’woman’? The court advisers may have one opin-
ion, and the queen’s physician another. It depends
on context. Similarly for ’man’ vs. ’woman’ and
’boy’, or ’cat’ vs. ’stuffed cat’ and ’dog’. When
VSMs are formed from a corpus, context is given
by the corpus for all instances of all words as a
package deal, and vectors of words are based on
that context.
Example 4: How similar is ’cat’ to ’submarine’?
If we find nothing in common, there is no similar-
ity, and the question doesn’t seem to make sense.
As with partially ordered sets, some pairs of words
are related to one another, while others are not.

1http://wordnet.princeton.edu

Example 5: Is ’flat’ more similar to ’apartment’,
or ’chair’ to ’table’? ’dog’ to ’cat’ or ’cow’ to
’sheep’? ’fork’ to ’shirt’ or ’stone’ to ’computer’?
For some questions of this kind we may have a
firm opinion, for others we may not be so sure,
and some questions don’t really make sense.
However, a VSM will have definite answers to
all questions in the above examples, regardless of
sense or context. Moreover, the symmetry and
transitivity of similarity imposed by VSMs contra-
dict human similarity judgments (Tversky, 1977).
These constraints of VSMs are due to the sym-
metry and triangle inequality conditions that must
be satisfied by any distance function. In addition,
Tversky and Hutchinson (1986) show that geomet-
ric models impose an upper bound on the number
of points that can share the same nearest neighbor,
and that particularly for conceptual data (such as
categorical ratings or associations of words), val-
ues for these exceed those possible in geometric
models. It has been suggested by Tversky and
Gati (1982) that ”similarity may be better char-
acterized as a feature-matching process based on
the weighting of common and distinctive features
than as a metric-distance function”. The model
we propose makes use of word co-occurrence in a
corpus to build a feature-based model of semantic
representation. We use sentence limits as our con-
text window, and measure the distance (counted in
the number of intervening words) between pairs
of words that co-occur in sentences. It is found
that for a word, its mean pmf (probability mass
function) of distance with its pair-words (hence
termed PDD - pair distance distribution) character-
izes it across corpora, and that semantically sim-
ilar words have a similar mean PDD. Given a
word, its features in our model are its pair-words
(those that co-occur with it within sentences), to-
gether with the frequency of pair occurrence and
its PDD. Thus we take into account word order,
which is disregarded by ’bag-of-words’ models.
As no sparse matrices are created, no dimension-
ality reduction is required. This makes our model
scalable both in computation and in storage, but
more importantly, the ’dimensions’ along which
we compare words are the feature words, which
are clear and meaningful. This stands in contrast
to the dimensions obtained following a dimen-
sionality reduction, the meanings of which often
aren’t clear. As words are not mapped into high-
dimensional spaces, and consequently similarities

185



are not measured with distances, the shortcomings
of VSMs are avoided. The rest of this paper is
structured as follows: Section 2 gives details of the
semantic representation model. In section 3, an
algorithm for evaluating similarity, based on our
model, is presented. In section 4, experiments and
their results are presented. Section 5 discusses the
scalability of our method, and section 6 concludes
the paper.

2 Model Details

Let w1, w2 be two distinct word forms (hence re-
ferred to as words). Given a corpus of docu-
ments C, let S be the collection of all sentences
in C in which both w1, w2 appear at least once,
S = {s1, s2, . . . sN}, for a total of N such sen-
tences in S. For each sentence si ∈ S, let p1, p2 be
the positions in the sentence of the words w1, w2
respectively. Define the distance between the two
words w1, w2 in the sentence si:

d(si) = p2 − p1 (1)

For sentences of maximal length L,

|d(si)| ≤ L− 1, d(si) 6= 0 (2)

As an example, in the sentence ”The cat drank
some milk”, ’cat’ and ’milk’ are in positions 2 and
5 respectively, and the distance between ’cat’ and
’milk’ is 3. Define Sj as the collection of sen-
tences si in S in which d(si) = j, and |Sj | as the
number of sentences in Sj , then for corpus C and
word-pair 〈w1, w2〉, the probability that the dis-
tance between the words (given a sentence con-
taining both words) is j is:

pr(d(si) = j) =

 |Sj |N , |j| ≤ L− 1, j 6= 00, otherwise (3)
where N is the total number of sentences in S,

and L is the maximal sentence length. (if either
w1 or w2 appear more than once in si , only the
nearest pair is counted). In this manner, the Cor-
pus pmf, termed PDDC(w1, w2), of word-pair
〈w1, w2〉 distance in corpus C is obtained, for any
word-pair. Given S and the position p1 of word
w1 for each si ∈ S, it is also possible to calculate
the pmf of position p2 of w2 in each sentence si ,
assuming random distribution of p2. Denoting the
length of sentence si as li, the probability for the

position p2 of w2 in the sentence to be k is:

pr(p2 = k) =


1

li − 1 , 1 ≤ k ≤ li, k 6= p1
0, otherwise (4)

From this, it follows that the probability for any
distance j between the two words w1, w2 in the
sentence si (given p1 and assuming random distri-
bution of p2 ) is:

pr(d(si) = j) =
1/(li − 1), 1− p1 ≤ d(si) ≤ li − p1,

d(si) 6= 0
0, otherwise (5)

The probability for any particular distance j for
the word-pair 〈w1, w2〉 in any sentence in corpus
C, pr(d(S) = j), given that the pair occurs in
the sentence, the position p1 of w1 in the sentence
and assuming random distribution of p2 may be
obtained by averaging the probability for that dis-
tance over all sentences si ∈ S:

pr(d(S) = j) =
1
N

∑
i

pr(d(si) = j) (6)

Hence another pmf for word-pair 〈w1, w2〉 dis-
tance in corpus C is obtained. Denote this, the
Random PDD, as PDDR(w1, w2). Whereas the
Corpus pmf, PDDC(w1, w2), is based on the cor-
pus data, the Random pmf, PDDR(w1, w2), is
based on the position of w1 in sentences where the
word-pair occurs and on the sentences’ lengths,
and assumes random position of w2 in those sen-
tences. Given a corpus C and a word w1, denote
the set of all sentences in C in which w1 appears
as Sw1 . The company of w1, Co(w1) are defined
as those words which appear in a number of sen-
tences in Sw1 above some threshold. Calculating
both Corpus and Random pmfs (as outlined above)
for each word-pair 〈w1, wi〉 , wi ∈ Co(w1), it is
now possible to calculate the average Corpus and
Random pmfs for w1 with its companion words,
PDDC(w1) and PDDR(w1) respectively, by av-
eraging the distance probabilities for all compan-
ion words weighted by their frequency of occur-
rence in sentences of Sw1 ,

PDDC(w1) =
∑

i(PDDC(w1, wi)× n(i))∑
i n(i)

,

wi ∈ Co(w1) (7)

186



PDDR(w1) =
∑

i(PDDR(w1, wi)× n(i))∑
i n(i)

,

wi ∈ Co(w1) (8)
where n(i) is the number of sentences in which
the pair of words 〈w1, wi〉 appears. By us-
ing KLD (Kullback-Leibler Divergence) between
Corpus and Random pmfs, D(PDDC‖PDDR),
we can measure the amount of information that
is lost when the Random pmf of a word w1,
PDDR(w1), is used to approximate its Corpus
pmf, PDDC(w1), as in the case of rectangular
context windows of unstructured DSMs.

D (PDDC‖PDDR) =∑
j

(
log

(
PDDC(j)
PDDR(j)

)
PDDC(j)

)
,

1− L ≤ j ≤ L− 1, j 6= 0 (9)

where j is the distance between w1 and its com-
pany words (all PDDs in eq. 9 are of w1, which
has been omitted for clarity). This statistic is a
property of word w1 in corpus C, which indicates
the amount of information contained in the order
of the words that are in the context of w1, above
the information that is carried by their mere pres-
ence there. It is also possible to calculate the in-
formation in any specific position in the context,
by replacing the PDDR value for that position
with the PDDC value, multiplying the remain-
ing probabilities by a suitable factor to keep the
sum of probabilities one, and calculating KLD be-
tween PDDC and the amended PDDR. The dif-
ference between this KLD value and the former
KLD value indicates the information for that posi-
tion.
Most previous research on unstructured DSMs has
used, in any one study, the same context win-
dow for all words in the corpus, as regards win-
dow size, position and weights. Even when sev-
eral different window parameters have been com-
pared,(Bullinaria and Levy, 2007; Levy et al.,
1999; Lund and Burgess, 1996; Sahlgren, 2006),
each window configuration was used for the whole
corpus, and comparison made on basis of the final
results. We know of no attempt to test different
window configurations for different words. How-
ever, there is no evidence to suggest that the same
window configuration is the optimal one for all

words. We suggest a scheme that could be use-
ful in determining weighting due to word order.
PPMI (positive pointwise mutual information),
which compares context window co-occurrence
frequency with expected frequency, has been suc-
cessfully used to weight words found in rectangu-
lar context windows (Bullinaria and Levy, 2007;
Church and Hanks, 1990; Niwa and Nitta, 1994),
based on their occurrence regardless of order. In
our case, PDDR(k) represents the chance proba-
bility of observing one of a word’s company words
a distance of k words from it, and PDDC(k) rep-
resents the probability of this occurring in the cor-
pus (given they co-occur in a sentence). We pro-
pose the following additional weight, Wt(k), be
used for word w1, for position k words away from
it,

Wt(k) = log
PDDC(k)
PDDR(k)

, PDDC(k) > PDDR(k),

0, otherwise (10)

where PDDC is the Corpus pmf of word w1, and
PDDR is its Random pmf.

3 Similarity Algorithm

Though differences between word PDDs could be
used to measure similarity between words, this
would lead to some of difficulties associated with
VSMs, as in effect we would be measuring dis-
tances in a high-dimensional space. By treating a
word’s company as its features, we can compare
two words based on their common pair-words.
The algorithm we adopt is as follows:

• For every vocabulary word w1:
– For every other vocabulary word w2,:
∗ Determine words Co(w1, w2) that are in the

company of both w1 and w2,

Co(w1, w2) = Co(w1) ∩ Co(w2) (11)
∗ For every wf ∈ Co(w1, w2), calculate

PDDC(w1, wf ) and PDDC(w2, wf ) using
Eq. (3), and determine the difference between
w1 and w2, based on wf , as the cosine “dis-
tance” between them,

dwf (w1, w2) =

1− PDDC(w1, wf )PDDC(w2, wf )|PDDC(w1, wf )||PDDC(w2, wf )| (12)

187



∗ For all wf ∈ Co(w1, w2), sort dwf (w1, w2) in
ascending order,

Dw1,w2 ={
dwf1 (w1, w2), dwf2 (w1, w2) . . .

}
(13)

∗ Set the dissimilarity of w1 to w2 to be the
sum of the first n elements of Dw1,w2 , (n is an
experimentally determined parameter):

Diss(w1, w2) =
n∑

i=1

Dw1,w2(i) (14)

w1 is considered to have no similarity with
words w2 that do not have common pair-words
with it.

– Order all vocabulary words that have common
pair-words with w1, by increasing dissimilarity
of w1 to them.

We now have, for each vocabulary word w1, all
other vocabulary words that have common pair-
words with w1, sorted by the dissimilarity of w1
to them. We define the dissimilarity of w1 to
any other word wx in this list to be the rank of
wx in this sorted list, not the numerical value
Diss(w1, wx). The differences used above, in
Equation (12), do not take into account the cor-
pus frequency of the words and pairs of words,
and are termed unweighted differences. Another
possibility is to use weighting that expresses these
frequencies. The following weighting has been
shown to give good results (see Section 4):

Weighted Differencewf (w1, w2) =

dwf (w1, w2)× log
(

Cf
C1,f

)d
× log(C2) d2 (15)

where dwf (w1, w2) is the unweighted difference,
Cf is the corpus count of wf , C1,f is the corpus
count of co-occurrence of w1 and wf in sentences,
C2 is the corpus count of w2, and d is an experi-
mentally determined parameter. The weighted dif-
ference is then used in place of the unweighted
difference in the next stages. It will be noted

that log
(

Cf
C1,f

)
is non-negative, and increases as

the PMI (pointwise mutual information) between
w1 and wf decreases (the corpus count of w1 is
constant when ordering the similarity of w1 to all
other words, and is therefore omitted), so this term
penalizes pair-words with low PMI to w1. The
last term penalizes, in the ordering of all corpus

words by similarity of w1 to them, words w2 with
high corpus frequency. Note that the dissimilar-
ity Diss(w1, w2) based on weighted differences is
not symmetric with respect to w1 and w2. Dissim-
ilarities based on both weighted and unweighted
differences do not obey the triangle inequality, so
that w1 may be very similar to both w2 and w3,
without requiring any minimal similarity between
w2 and w3. Both dissimilarities also do not re-
strict the number of words that can share a nearest
neighbor - any number of words can have w1 as
the word they are most similar to.

4 Experimental Details

4.1 Computing PDDC

An initial experiment was carried out on 17,000
medical papers on diseases in eight different, but
related, domains. The papers were returned by
Google Scholar using search words relating to
each domain, downloaded in pdf format and con-
verted to text, to form eight corpora. The texts
were tokenized, and lower-cased (using raw sur-
face forms of words means different parts of
speech, as well as different senses, are conflated).
For vocabulary purpose, the text was filtered for
stop words, numbers and any word not beginning
with an alphanumeric character, and only words
appearing at least 100 times were used. Sentences
were delimited, and sentences with a length of
over 50 words were discarded. Word-pairs were
obtained from the eight corpora, for word-pair dis-
tance of up to 25 words, for word-pairs appearing
in at least 10 sentences. PDDC was calculated
for all pair words, which ranged in number from
6,300 to 13,700 words for each domain, and a total
number of 14,900 unique words, and 2.95 million
unique pairs, for all domains combined. Fig. 1
below shows PDDC for ’effect’, ’cell’, ’red’ and
’show’, respectively, for the eight domains super-
imposed. It may be seen that each word has a char-
acteristic PDDC across domains, and that differ-
ent words have a different PDDC .

4.2 Computing PDDC and PDDR

Google Scholar was again used to download arti-
cles from journals with the word ’science’ in their
title. Seven search words were used: ’physics’,
’chemistry’, ’biology’, ’engineering’, ’medicine’,
’information’ and ’environment’. Over 27,000
pdfs were downloaded, and processed as in the
previous experiment. Again, only words appear-

188



−20 −10 1 10 20
0

0.02

0.04

0.06

0.08
effect

−20 −10 1 10 20
0

0.02

0.04

0.06

0.08
cell

−20 −10 1 10 20
0

0.05

0.1

0.15

0.2
red

Distance in Words

P
ro

ba
bi

lit
y

−20 −10 1 10 20
0

0.02

0.04

0.06

0.08
show

Figure 1: PDDC for ’effect’, ’cell’, ’red’ and
’show’ across eight domains

ing at least 100 times were used (32,341 words),
pair distance of up to 25 words was considered,
and PDDC and PDDR for all pairs appearing at
least 10 times calculated. This resulted in 23,155
words and 3.9 million word pairs. Figure 2 be-
low shows PDDC and PDDR for four words,
’black’, ’show’, ’study’ and ’significantly’. The
PDDR of a word is determined by its position in
sentences and the length of these sentences. The
PDDC is determined by the usage of the word
with its company. It may be seen that as we move
in the sentence away from the word, its PDDC
eventually follows its PDDR from below. This is
expected, as a word’s company are more likely to
be near it (and hence less likely to be farther away)
than predicted by random chance, and because not
much information is expected to be found in the
order of a word’s company that is not near it. How-
ever, the more interesting part is the one near the
word, where the PDDC and PDDR differ con-
siderably. Each word has a distinctive pattern,
from which we may learn the amount of informa-
tion in the order of the words around it, as detailed
in Section 2. For ’black’ and ’significantly’, it is
the following word that holds the most informa-
tion, for ’show’ it is the second word following,
and for ’study’ it is the third word following, with
positions immediately around it held by company
words less often than by chance (presumably be-
cause they are held by function words). This be-
havior is probably affected by each word’s most
frequent part-of-speech usage in the corpus, for
example, ’black’ as an adjective is likely to have
a related content word following it.

−20 −10 1 10 20
0

0.04

0.08

0.12
black

−20 −10 1 10 20
0

0.02

0.04

0.06

show

−20 −10 1 10 20
0

0.01

0.03

0.05

Distance in Words

P
ro

ba
bi

lit
y

study

 

 

−20 −10 1 10 20
0

0.02

0.04

0.06

0.08
significantly

PDDc
PDDr

Figure 2: PDDC and PDDR for ’black’, ’show’,
’study’, and ’significantly’

Fig. 3 compares the PDDC of 10 adjectives, 5
colors (’black’, ’red’, ’blue’, ’white’, ’green’) and
5 size adjectives (’huge’, ’big’, ’great’, ’large’,
’enormous’). The top row shows the five colors
and five sizes PDDC . The bottom row shows on
the left the mean color and size PDDC , and on
the right all color and size PDDC . It may be seen
that though color and size PDDC are similar, they
differ, particularly in positions nearest the word.
Clustering the ten PDDC into two clusters, us-
ing kmeans clustering and cityblock distance, sep-
arates them correctly into colors and sizes. This
illustrates that (at least in this case) the PDDC is
related also to their semantic content, and not only
to their part-of-speech.

−20 −10 1 10 20
0

0.1

0.2
5 colors

−20 −10 1 10 20
0

0.1

0.2
5 sizes

−6 −4 −2 1 2 4 6
0

0.1

0.2
mean color vs. mean size

Distance in Words

P
ro

ba
bi

lit
y

 

 
mean size
mean color

−6 −4 −2 1 2 4 6
0

0.1

0.2
colors and sizes

 

 
size
color

Figure 3: PDDC Comparison for Color and Size
Adjectives

189



4.3 Computing Weights for Positions in the
Context Window

Using eq. 10, weights were calculated for posi-
tions in the context window of all words that have
pairs. These weights are meant to reflect the infor-
mation in the order of the words, given that they
occur in the window (a fact that by itself carries
information). It turns out that these weights differ
from one word to another. Fig. 4.3 shows on the
top row PDDC vs. PDDR for ’red’ and ’year’.
The bottom row shows the weights calculated for
their context windows, together with the weights
for another, semantically similar word (’black’
and ’day’ respectively). Weights not shown are
zero. The adjectives get the greatest weight for
the following word, and zero weight for the pre-
ceding word. The nouns ’year’ and ’day’ get zero
weight for both the preceding and the following
words, with ’year’ getting the greatest weights for
the fourth word preceding and the third word fol-
lowing, and ’day’ for the second word preceding
and the third word following. The nouns also
have weights for wider contexts than the adjec-
tives. This example shows that different words
have different optimal context windows, both in
width and in weight, as regards the information in
word order.

−20 −10 1 10 20
0

0.02

0.04

0.06

0.08

0.1
red PDD

P
ro

ba
bi

lit
y

 

 
PDDc
PDDr

−20 −10 1 10 20
0

0.01

0.02

0.03

0.04

0.05
year PDD

 

 
PDDc
PDDr

−6−5−4−3−2−1 1 2 3 4 5
0

0.5

1

1.5
Weights for ’red’ and ’black’

Distance in Words

W
ei

gh
t

 

 
red
black

−12−10−8 −6 −4 −2 2 4 6 8
0

0.1

0.2

0.3

0.4

Weights for ’year’ and ’day’

 

 
year
day

Figure 4: Context Window Weights

4.4 TOEFL Test

In order to increase vocabulary size, the ukWaC
corpus2 holding over a billion words in documents
crawled from the internet was used (Baroni et al.,

2http://wacky.sslmit.unibo.it

2009). The same processing as in the previous ex-
periments was applied. This resulted in a vocabu-
lary of 136,812 words with a frequency of at least
100 in the corpus. The TOEFL synonym dataset
(Landauer and Dumais, 1997) consists of 80 ques-
tion words, for each of which 4 answer words are
given, and the task is to select the answer word
most similar to the question word. The test con-
tains 391 unique words, 7 of which were miss-
ing in our vocabulary. Three questions had one
wrong answer word missing, and these were at-
tempted without the missing word. One question
had all but one wrong answer word missing, and
was marked as wrong. For each TOEFL word,
word-pairs that appear in at least 10 sentences in
the corpus were extracted. The method we used
to select the correct answer is by ordering the an-
swer words by decreasing similarity of the ques-
tion word to them as outlined in section 3, and
choosing as correct the top word. Cosine distance
was used, and values for n, the number of common
feature words, from 1 to 50 were evaluated. Both
unweighted and weighted differences were calcu-
lated. A grid search was performed for the best
combination of n and d, and the values of 5 and
3.5 respectively give a result of 86%. However
any combination of values for n in the range 3-9
and for d in the range 2-5, give a result of 80% and
above. Fig. 5 shows the results with both weighted
(d = 3.5) and unweighted differences, as a func-
tion of the number of feature words used. It will
be noticed that with the weighted differences, it
takes 3-5 feature words to get optimal results. Bet-
ter results on the TOEFL test have been achieved
by (Rapp, 2003; Han, 2014; Pilehvar et al., 2013;
Turney et al., 2003; Bullinaria and Levy, 2012),
ranging from 92.5 to 100%. Han (2014) and Tur-
ney et al. (2003) are hybrid approaches, combin-
ing the results of several methods. Pilehvar et al.
(2013) relies on WordNet3 for sense inventory of
words, and uses a substantially different version
of the test. Both Bullinaria and Levy (2012) and
Rapp (2003), after obtaining a vocabulary from a
corpus, artificially introduce into their data out-
of-vocabulary TOEFL words, which would not be
possible for open-ended questions.

4.5 Distance Test

This experiment uses the same corpus and the
same processing as the previous experiment. The

3http://wordnet.princeton.edu

190



0 10 20 30 40 50

75

80

85
P

er
ce

nt
 A

ns
w

er
s 

C
or

re
ct

n, Number of Feature Words
 used for Sum

 

 

Weighted (d = 3.5 )
Unweighted

Figure 5: TOEFL Test Results

distance comparison test (Bullinaria and Levy,
2007), for which the data has kindly been made
available by the authors on their website, is also
similar to the TOEFL test. This test consists of 200
pairs of semantically related words. For each pair,
one word is set as the question word. The other
pair word, the answer word, is included in a list
with 10 additional words, chosen at random from
the other pairs. The task is to sort this list in order
of decreasing similarity to the question word, and
points are awarded according to the position of the
answer word in this list (1 point for 1st position,
0.9 for 2nd, etc.). Using the same method as in the
previous experiment, results for unweighted and
weighted distance (d = 3) are shown in fig. 6 be-
low. A value of 97.6% is obtained for this weight,
d=3, and for n with a value of 7 or 8. However any
combination of values for d in the range 2-4.5 and
n in the range 4-11 yields a result of over 97%.

0 10 20 30 40 50

86

90

94

98

n, Number of Feature Words used for Sum

P
er

ce
nt

 A
ns

w
er

s 
C

or
re

ct

 

 

Weighted ( d = 3 )
Unweighted

Figure 6: Distance Test Results

5 Scalability

The model size is governed by the number P of
distinct word-pairs that occur in sentences of the
corpus, which is related to the vocabulary size, V,
which in turn depends on N, the number of tokens
in the corpus. For the ukWac corpus, V ∼ cN0.53,

and for pairs with a distance of up to 6 words, that
appear at least 5 times, P grows as N0.80. This
shows that the model size is scalable with corpus
size. With p as the maximal pair distance used, the
complexity of building the PDD model is bounded
by 2pN , and is therefore O(N), again scalable
with corpus size. In order to arrange all vocab-
ulary words V by similarity of a single word w
to them, it is necessary to find the best n features
each vocabulary word has in common with w, and
calculate the similarity based on the sum of differ-
ences for the n features. Doing this for all vocab-
ulary words (i.e. arranging all words in order of
similarity of every word to them) is governed by
C, the number of common features all vocabulary
words have with all other vocabulary words. For
the ukWac corpus, (again for pairs with a distance
of up to 6 words, that appear at least 5 times),
C grows as N1.21. While this grows faster than
corpus size, it is feasible to calculate this for the
ukWac corpus. For larger corpora it may be nec-
essary to limit, for each word, the calculation of its
similarity to words that have a number of common
pair-words with it above some threshold.

6 Discussion and Conclusions

We have presented a novel model of semantic rep-
resentation, that is scalable and does not suffer
from the shortcomings of VSMs. Two words that
have no common features are not considered sim-
ilar, and are not given a similarity value. Simi-
larity is not symmetric, in accordance with human
similarity judgments. Similarity is not transitive,
so that a given word may be similar to two other
words, to each with different senses of the given
word, or in different contexts, without necessitat-
ing any similarity between the two other words.
The features with which the similarity between a
pair of words is evaluated are clear and meaning-
ful - the common pair-words. The model makes
it possible to select which features of a word to
use when evaluating similarity, thus enabling one
to take into account different senses and different
contexts of a word. The model has been shown
to work well on word similarity tasks. Further
work could use the model for word disambigua-
tion tasks, as different senses of a word are ex-
pected to have different PDDs. The current work
has used pair distance distribution, and compared
words based on their common features. Future
work could use triplet distance distribution, and

191



take into account distinctive word features as well
as the common features.

References

Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673–721.

Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide
web: a collection of very large linguistically pro-
cessed web-crawled corpora. Language resources
and evaluation, 43(3):209–226.

John A Bullinaria and Joseph P Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior re-
search methods, 39(3):510–526.

John A Bullinaria and Joseph P Levy. 2012. Extracting
semantic representations from word co-occurrence
statistics: stop-lists, stemming, and svd. Behavior
research methods, 44(3):890–907.

Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22–29.

Thomas L Griffiths, Mark Steyvers, and Joshua B
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological review, 114(2):211–244.

Lushan Han. 2014. Schema free querying of semantic
data. Ph.D. thesis, University of Maryland.

Zellig S Harris. 1954. Distributional structure. Word,
10(2-3):146–162.

Thomas K Landauer and Susan T Dumais. 1997. A
solution to plato’s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
104(2):211–240.

Joseph P Levy, John A Bullinaria, and Malti Patel.
1999. Explorations in the derivation of word co-
occurrence statistics. South Pacific Journal of Psy-
chology, 10(01):99–111.

Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, & Computers, 28(2):203–208.

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In ACL-08: HLT,
pages 236–244.

Yoshiki Niwa and Yoshihiko Nitta. 1994. Co-
occurrence vectors from corpora vs. distance vec-
tors from dictionaries. In Proceedings of the 15th
conference on Computational linguistics-Volume 1,
pages 304–309. Association for Computational Lin-
guistics.

Mohammad Taher Pilehvar, David Jurgens, and
Roberto Navigli. 2013. Align, disambiguate and
walk: A unified approach for measuring semantic
similarity. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1341–1351.

Reinhard Rapp. 2003. Word sense discovery based on
sense descriptor dissimilarity. In Proceedings of the
ninth machine translation summit, pages 315–322.

Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.

Peter Turney, Michael L Littman, Jeffrey Bigham, and
Victor Shnayder. 2003. Combining independent
modules to solve multiple-choice synonym and anal-
ogy problems. In Proceedings of the International
Conference on Recent Advances in Natural Lan-
guage Processing (RANLP-03), pages 482–489.

Amos Tversky and Itamar Gati. 1982. Similarity, sep-
arability, and the triangle inequality. Psychological
review, 89(2):123–154.

Amos Tversky and J Hutchinson. 1986. Nearest neigh-
bor analysis of psychological spaces. Psychological
review, 93(1):3–22.

Amos Tversky. 1977. Features of similarity. Psycho-
logical review, 84(4):327–352.

192


