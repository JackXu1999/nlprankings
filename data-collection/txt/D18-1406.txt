



















































CaLcs: Continuously Approximating Longest Common Subsequence for Sequence Level Optimization


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3708–3718
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

3708

CALCS: Continuously Approximating Longest Common Subsequence for
Sequence Level Optimization

Semih Yavuz∗
University of California, Santa Barbara

syavuz@cs.ucsb.edu

Chung-Cheng Chiu
Google Brain

chungchengc@google.com

Patrick Nguyen
Google Brain

drpng@google.com

Yonghui Wu
Google Brain

yonghui@google.com

Abstract

Maximum-likelihood estimation (MLE) is one
of the most widely used approaches for train-
ing structured prediction models for text-
generation based natural language process-
ing applications. However, besides expo-
sure bias, models trained with MLE suffer
from wrong objective problem where they
are trained to maximize the word-level cor-
rect next step prediction, but are evaluated
with respect to sequence-level discrete met-
rics such as ROUGE and BLEU. Several
variants of policy-gradient methods address
some of these problems by optimizing for fi-
nal discrete evaluation metrics and showing
improvements over MLE training for down-
stream tasks like text summarization and ma-
chine translation. However, policy-gradient
methods suffers from high sample variance,
making the training process very difficult and
unstable. In this paper, we present an alterna-
tive direction towards mitigating this problem
by introducing a new objective (CALCS) based
on a differentiable surrogate of longest com-
mon subsequence (LCS) measure that captures
sequence-level structure similarity. Experi-
mental results on abstractive summarization
and machine translation validate the effective-
ness of the proposed approach.

1 Introduction

Recently, deep neural networks have achieved
state-of-the-art results in various tasks including
computer vision, natural language processing, and
speech processing. Specifically, neural text gen-
eration models, central focus of this work, have
led to great progress in central downstream NLP
tasks like text summarization, machine transla-
tion, and image captioning. For example, the
abstractive summarization task, which has previ-
ously not been the popular choice for text sum-

∗Work done while interning at Google Brain.

marization due to lack of appropriate text gener-
ation methods, has gained revived attention with
the success of neural sequence-to-sequence mod-
els (Sutskever et al., 2014; Bahdanau et al., 2015).
There has been several recent work with an im-
pressive progress on this task including (Rush
et al., 2015; Chopra et al., 2016; Nallapati et al.,
2016; Miao and Blunsom, 2016; See et al., 2017;
Tan et al., 2017; Zhou et al., 2017). Machine trans-
lation is another central field in NLP where the
emergence of neural sequence-to-sequence mod-
els has enabled viable alternative approaches (Lu-
ong et al., 2015; Bahdanau et al., 2015; Cho et al.,
2014; Sutskever et al., 2014) to challenge tradi-
tional phrase-based methods (Koehn et al., 2003).

Most of the recent existing works on neural text
generation are based on variants of sequence-to-
sequence models with attention (Bahdanau et al.,
2015) trained with Maximum-likelihood estima-
tion (MLE) with teacher forcing. As Ranzato et al.
(2016) points out in a previous work, these models
have two major drawbacks. First, they are trained
to maximize the probability of correct next word
given the entire sequence of previous ground truth
words. While, at test time, the models need to
generate the entire sequence by feeding its own
predictions at previous time steps. This discrep-
ancy is called exposure bias and hurts the perfor-
mance as the model is never exposed to its own
predictions during training. The second drawback,
called wrong objective, is due yet another discrep-
ancy between training and testing. It refers to the
critique (Ranzato et al., 2016) that MLE-trained
models tend to have suboptimal performance as
they are trained to maximize a convenient objec-
tive (i.e., maximum likelihood of word-level cor-
rect next step prediction) rather than a desirable
sequence-level objective that correlates better with
the common discrete evaluation metrics such as
ROUGE (Lin and Och, 2004) for summarization,



3709

BLEU (Papineni et al., 2002) for translation, and
word error rate for speech recognition, not log-
likelihood. On the other hand, training models
that directly optimize for such discrete metrics as
objective is hard due to non-differentiable nature
of the corresponding loss functions (Rosti et al.,
2011). To address these issues, Ranzato et al.
(2016) introduces an incremental learning recipe
that uses a hybrid loss function combining REIN-
FORCE (Williams, 1992) and cross-entropy. Re-
cently, Paulus et al. (2018) also explored com-
bining maximum-likelihood and policy gradient
training for text summarization.

Towards sequence level optimization, previous
works (Ranzato et al., 2016; Wu et al., 2016;
Paulus et al., 2018) employ reinforcement learn-
ing (RL) with a policy-gradient approach which
works around the difficulty of differentiating the
reward function by using it as a weight. How-
ever, REINFORCE is known to suffer from high
sample variance and credit assignment problems
which makes the training process difficult and un-
stable besides resulting in models that are hard to
reproduce (Henderson et al., 2018).

In this paper, we propose an alternative ap-
proach for sequence-level training with longest
common subsequence (LCS) metric that measures
the sequence-level structure similarity between
two sequences. We essentially introduce a con-
tinuous approximation to the discrete LCS met-
ric which can be directly optimized against using
standard gradient-based methods. Our proposed
approach has the advantage of being able to di-
rectly optimize for a surrogate reward as opposed
to using the exact reward only as a weight as in
RL-inspired works. Hence, it provides a viable
alternative perspective to policy-gradient methods
for side stepping the non-differentiability with re-
spect to the exact reward. In addition, it simultenu-
ously combats the exposure bias problem through
exposing the model to its own predictions while
computing our approximation to LCS metric.

To this end, we introduce a new learning recipe
that incorporates the aformentioned continuous
approximation to LCS metric (CALCS) as an ad-
ditional objective on top of maximum-likelihood
loss in existing neural text generation models.
We evaluate the proposed approach on abstrac-
tive text summarization and machine translation
tasks. To this end, we use recently introduced
pointer-generator network (See et al., 2017) and

transformer (Vaswani et al., 2017) as underlying
baselines for summarization and machine transla-
tion, respectively. More precisely, we start from
a pre-trained baseline model with cross-entropy
loss, and continue training the model to optimize
for the proposed differentiable objective based on
CALCS. Using this recipe, we conduct various
experiments on CNN/Daily Mail (Hermann et al.,
2015; Nallapati et al., 2016) summarization and
WMT 2014 English-to-German machine transla-
tion tasks. Experimental results validate the effec-
tiveness of the proposed approach on both tasks.

2 Continuously Approximating Longest
Common Subsequence Metric

In this work, we explore the potential use of
longest common subsequence (LCS) metric from
an algorithmic point of view to address the afore-
mentioned wrong objective and exposure bias
problems. LCS metric measures a sequence-level
structure similarity between discrete sequences by
identifying longest co-occurring in sequence n-
grams and it has been shown to correlate well with
human judgments for downstream text generation
tasks (Lin and Och, 2004). To this end, we pro-
pose a way to continuously approximate LCS met-
ric and use this differentiable approximation as
the objective to train text generation models rather
than the exact LCS measure, which is hard to op-
timize for due to non-differentiability of the cor-
responding loss function. Although such differ-
entiable approximation provides a unique advan-
tage from modeling and optimization perspective,
the difficulty of controlling its tightness might be a
potential drawback in terms of its applicability. In
this section, we will first introduce our proposed
approximation to LCS metric, and then provide a
natural way to control its tightness.

Consider a sequence generation problem condi-
tioned on an input sequence x = (x1, x2, . . . , xn)
and let y = (y1, y2, . . . , ym) denote its corre-
sponding ground-truth output sequence. Let

f(x,Θ) = z = (z1, z2, . . . , zk)

denote hypothesis sequence obtained by greedy
decoding from a generic encoder-decoder archi-
tecture for input sequence x, where Θ represents
model parameters. Also, let p1, p2, . . . , pk be the
probability distributions over vocabulary V at de-
coding time steps from which z1, z2, . . . , zk are
generated via argmax operator, respectively.



3710

2.1 CALCS

In this section, we define our approach to con-
tinuously approximate the longest common sub-
sequence measure (LCS), which is an unnormal-
ized version of ROUGE-L metric (Lin and Och,
2004) (See Appendix B) that is commonly used
for performance evaluation of text summarization
models. The main intuition behind our approach is
to relax the common necessity for hard inferences
while computing discrete metrics by instead com-
paring discrete tokens in a soft way. Towards this
end, we start by defining LCS metric.

Definition 1. Given two sequences y and z of to-
kens, longest common subsequence LCS(y, z) is
defined as the longest sequence of tokens that ap-
pear left-to-right (but not necessarily in a contigu-
ous block) in both sequences.

The most common and intuitive solution for
computing longest common subsequence is via
dynamic programming. We will briefly revisit this
here as it will be useful in terms of both recall and
notational convenience while describing our sur-
rogate LCS measure. Let ri,j denote the longest
common subsequence between prefix sequences
y[:i] = (y1, y2, . . . , yi) and z[:j] = (z1, z2, . . . , zj)
of y and z, respectively. A dynamic programming
solution is given by

ri,j =


0 if i = 0 or j = 0
ri−1,j−1 + 1 if yi = zj
max(ri−1,j , ri,j−1) o/w.

(1)

ri,j for all i = 1, 2, . . . ,m and j = 1, 2, . . . , k.
It can be computed in mk iterations using the for-
mula in Eqn 1. After computing 2D dynamic pro-
gramming matrix r, we obtain LCS(y, z) = rm,k.

Towards removing the dependence on hard in-
ference for computing LCS, we now define our
approximation to longest common subsequence,
which we call CALCS. At high-level, the idea
is to continuously relax the original LCS mea-
sure. To this end, we leverage output probabil-
ity distributions p1, p2, . . . , pk as soft predictions
to refine the dynamic programming formulation
for original LCS. More precisely, we recursively
define soft longest common subsequence si,j be-
tween prefixes y[:i] and z[:j] in analogous to ri,j as

follows:

si,j = p
(yi)
j (si−1,j−1 + 1) + (2)

(1− p(yi)j )max(si−1,j , si,j−1) (3)

for i, j > 0 and si,0 = s0,j = 0, where p
(yi)
j de-

note the probability of generating yi at j-th decod-
ing step. Intuitively, CALCS replaces the hard to-
ken comparison 1 [yi = zj ] in Eq. 1 with the prob-
ability p(yi)j as in Eq. 2. Interpreting the proba-

bility p(yi)j as a continuous relaxation of discrete
comparison operator 1 [yi = zj ], si,j establishes a
natural continuous approximation to ri,j . Similar
to LCS, after iteratively filling up si,j matrix, we
define

CALCS(y, z) = sm,k (4)

Although the proposed approximation is a natu-
ral way of relaxing/extending the hard binary com-
parison of discrete tokens, it is not clear how tight
the approximation is, which is established in the
next section.

2.2 On the Tightness of Approximation
In this section, we first discuss the tightness of the
proposed approximation, and then provide a natu-
ral way of controlling it.

2.2.1 Bounding the Approximation Error
We now present a bound on the approximation
error of the proposed CALCS compared to the
original LCS measure. Characterization of this
bound will enable us to theoretically argue about
the feasibilty of using the proposed surrogate re-
ward function for our objective as well as control-
ling its tightness.

LCS measure is intrinsically monotonic by defi-
nition. We start by a lemma that establishes a sim-
ilar monotonicity property for CALCS.
Lemma 1. [Monotonicity] The following two in-
equalities

si,j ≤ si,j+1 ≤ si,j + 1
si,j ≤ si+1,j ≤ si,j + 1

hold for all 0 ≤ i < m and 0 ≤ j < k.

Proof. See Appendix A for the proof.

Having established a certain monotonicity prop-
erty for CALCS, we will discuss its approximation
error to the original LCS measure. Let

δi,j = si,j − ri,j (5)



3711

denote the approximation error of CALCS to LCS
measure between generated prefix sequence y[:i]
and the ground-truth prefix z[:j].

Lemma 2. Let Pi,j =
{(0, 0), (i1, j1), . . . , (iq−1, jq−1), (iq, jq)} de-
note the path of dynamic programming algorithm
for LCS ending at (i, j) = (iq, jq) cell of m × k
grid. Then,

|δi,j | <
∣∣δiq−1,jq−1∣∣+ (1−max(pj)) (6)

where max(pj) = max{p(1)j , p
(2)
j , . . . , p

(|V |)
j }.

Proof. We will establish the proof by investigating
two cases and combining them.

CASE 1: zj = yi.
In this case, we have

ri,j = 1 + ri−1,j−1 (7)

and

(iq−1, jq−1) = (i− 1, j − 1) (8)

by 1. Using Eq. 7, we get

δi,j = si,j − ri,j

=
(
1− p(yi)j

)
max (si−1,j , si,j−1)

+ p
(yi)
j (si−1,j−1 + 1)− (1 + ri−1,j−1)

= (si−1,j−1 − ri−1,j−1)

+
(
1− p(yi)j

) [
max(si−1,j , si,j−1)

− (1 + si−1,j−1)
]

Using the definition of δ and triangle inequality,
we get

|δi,j | ≤ |δi−1,j−1|+
(
1− p(yi)j

) ∣∣∣ (1 + si−1,j−1)
− max (si−1,j , si,j−1)

∣∣∣
≤ |δi−1,j−1|+

(
1− p(yi)j

)
(9)

where inequality 9 follows from the monotonicity
established by Lemma 1.

Moreover, zj = yi implies p
(yi)
j = max(pj) be-

cause z is generated by greedy decoding. Plugging
this in Eq. 9 and using Eq. 8, we can immediately
conclude that

|δi,j | <
∣∣δiq−1,jq−1∣∣+ (1−max(pj)) (10)

CASE 2: zj 6= yi.
By definition 1, we have

ri,j = max (ri−1,j , ri,j−1) .

Using this identity, we obtain

δi,j = si,j − ri,j

=
(
1− p(yi)j

)
max (si−1,j , si,j−1)

+ p
(yi)
j (si−1,j−1 + 1)−max (ri−1,j , ri,j−1)

= p
(yi)
j [(1 + si−1,j−1)−max (si−1,j , si,j−1)]

+ [max (si−1,j , si,j−1)−max (ri−1,j , ri,j−1)]

Applying triangle inequality on the last equation
above, we get

|δi,j | ≤

p
(yi)
j |(1 + si−1,j−1)−max (si−1,j , si,j−1)|

+ |max (si−1,j , si,j−1)−max (ri−1,j , ri,j−1)|
≤

p
(yi)
j |(1 + si−1,j−1)−max (si−1,j , si,j−1)|

+max (|si−1,j − ri−1,j | , |si,j−1 − ri,j−1|) (11)
=

p
(yi)
j |(1 + si−1,j−1)−max (si−1,j , si,j−1)|

+max (|δi−1,j | , |δi,j−1|)

≤ p(yi)j +max (|δi−1,j | , |δi,j−1|) (12)

where inequality 12 follows from again the mono-
tonicity of s[·, ·], and inequality 11 follows from
the following identity that holds true for all real
numbers a, b, c, d ≥ 0

|max(a, b)−max(c, d)| ≤ max(|a− c| , |b− d|)

Moreover, since zj 6= yi, we know that p(yi)j 6=
max(pj), which implies

p
(yi)
j ≤ 1−max(pi). (13)

Combining 11 and 13 completes the proof for this
case. Finally, two cases investigated above to-
gether establish the proof of Lemma 2.

Lemma 2 leads to the following important
corollary.
Corollary 1. Let Pi,j =
{(0, 0), (i1, j1), . . . , (iq, jq)} be the path of
dynamic programming algorithm for LCS ending
at (i, j) = (iq, jq) cell of m× k grid. Then,

|δi,j | ≤
q∑

w=1

(1−max(pjw)) (14)



3712

Proof. Applying Lemma 2 iteratively and using
δ0,0 = 0, we get

|δi,j | ≤
∣∣δiq−1,jq−1∣∣+ (1−max(pjq))∣∣δiq−1,jq−1∣∣ ≤ ∣∣δiq−2,jq−2∣∣+ (1−max(pjq−1))∣∣δiq−2,jq−2∣∣ ≤ ∣∣δiq−3,jq−3∣∣+ (1−max(pjq−2))

...

|δi1,j1 | ≤ |δ0,0|+ (1−max(pj1))
|δ0,0| ≤ 0

Summing (q+1)-many inequalities above side by
side and cancelling out the same terms appearing
on both sides of the resulting inequality establishes
the proof of corollary.

2.2.2 Controlling the Tightness of
Approximation

Corollary 1 hints for a natural way of control-
ling the tightness of approximation CALCS by ex-
ploiting the peakedness of model’s softmax output
probability distributions. More precisely, upper
bound on the approximation error is represented
as a sum of 1−max(pj)’s, hence the more peaked
the model’s output probability distributions on av-
erage, the smaller the approximation error we are
guaranteed by the established bounds.

We exploit this property to control the tight-
ness of approximation by making a modification
to computation of the proposed CALCS measure.
Formally, let l1, l2, . . . , lk denote the unnormal-
ized logits of the model output before applying
softmax to obtain probabilities p1, p2, . . . , pk at
decoding time steps, respectively. Hence,

p
(i)
j =

exp(l
(i)
j )∑

i exp(l
(i)
j )

(15)

Recall that CALCS is computed using pj’s. Using
peaked softmax, we can obtain more peaked prob-
ability distributions without causing any change in
the actual generated sequence z via greedy decod-
ing. This is simply because the order of probabil-
ities for corresponding vocabulary words will not
change, only the probability disribution pj will get
more peaked. So, we define peaked softmax oper-
ator with hyperparameter α as

p
(i)
j (α) =

exp(l
(i)
j /α)∑

i exp(l
(i)
j /α)

(16)

By Corollary 1, |δi,j | → 0 as α → 0 for CALCS
measure computed with pj(α). One can further

attempt to use Corollary 1 as a guide to pinpoint a
range of α values to force the approximation error
within certain desired limits. We will use α as a
hyperparameter in this work.

Corollary 1 is also useful for alternative ways
of controlling the tightness of approximation such
as incurring penalty for high-entropy output prob-
ability distributions or simply penalizing the max-
imum output probability values less than a desired
threshold (that explicitly controls the tightness of
the approximation). We leave such options of con-
trolling the approximation error for future work.

With the guidance of Corollary 1 and peaked
softmax in Eq. 16, we conclude that CALCS es-
tablishes a promising approximation for LCS mea-
sure. In the next section, we introduce a new ob-
jective function using CALCS as a continuously
differentiable reward to be directly maximized.

2.3 Sequence Level Optimization via CALCS
In this section, we describe how to leverage
CALCS to define a loss function for sequence level
optimization. For notational consistency, we will
use f(x,Θ) to denote an encoder-decoder archi-
tecture that takes an input sequence x and out-
puts a sequence of tokens z = (z1, z2, . . . , zm)
via greedy decoding from corresponding probabil-
ity distributions p1, p2, . . . , pm at each step.

For a pair of input sequence x and its corre-
sponding ground-truth output sequence y, we de-
fine

JCALCS(x,y;Θ) = − log
(

CALCS(y, f(x,Θ))
|y|

)
(17)

as the loss function for a sample (x,y) based on
the CALCS, where |y| denote the length of se-
quence y. It is important to note here that while
computing probability distribution pt at decoding
step t, we feed model’s own prediction zt−1 at the
previous time step to fight exposure bias.

It is important to observe here that
JCALCS(x,y;Θ) is differentiable in p1, p2, . . . , pk
by definition and each pi is differentiable in model
parameters Θ. Hence, JCALCS(x,y;Θ) is differ-
entiable in model parameters Θ, which allows us
to directly optimize the network parameters with
respect to LCS metric. The bound we established
on the approximation error and our proposed
strategy to control it theoretically ensures the
feasibility of using the introduced loss function
JCALCS to optimize for LCS metric.



3713

3 Model

In this section, we first briefly revisit the pointer-
generator (See et al., 2017) and transformer
(Vaswani et al., 2017) networks that are used as
the underlying baselines in our experiments. Sub-
sequently, we describe how the proposed objective
function and its variants are used to train new sum-
marization and machine translation models.

3.1 Baseline Models
Pointer-Generator Network. We use pointer-
generator network (See et al., 2017) as our base-
line sequence-to-sequence model for text sum-
marization. It is essentially a hybrid between
sequence-to-sequence model with attention (Bah-
danau et al., 2015) and a pointer network (Vinyals
et al., 2015) that supports two decoding modes,
copying and generating, via a soft switch mecha-
nism. This enables the model to copy a word from
the input sequence based on the attention distribu-
tion. On each decoding time step t, the decoder
LSTM is fed the word embedding of the previous
word, and computes a decoder state st, an atten-
tion distribution at over the words of input article,
and a probability Pvocab(w) of generating word w
for summary from output vocabulary V , which is
then softly combined with the copy mode’s proba-
bility distribution Pcopy(w) via soft switch proba-
bility pgen ∈ [0, 1] by

p
(w)
t = pgenPvocab(w) + (1− pgen)Pcopy(w)

and

Pcopy(w) =
∑

{i:wi=w}

ati

where ati indicates the attention probability on i-
th word of the input article. The whole network
is then trained end-to-end with the negative log-
likelihood loss function of

JPG(x,y;Θ) = −
1

|y|

|y|∑
t=1

log(p
(yt)
t )

for a sample article-summary pair (x,y) where Θ
denote the learnable model parameters. It is im-
portant to note here that we do not use the cov-
erage mechanism introduced by the original work
(See et al., 2017) to prevent the potential repetition
problem in the summaries generated by the model.

Transformer Network. For machine translation,
we use the transformer network (Vaswani et al.,
2017), which is a recently published model that
achieved state-of-the-art results on WMT 2014
English-to-German MT task with less computa-
tional time owing to its highly parallelizable ar-
chitecture. The core idea behind this model is to
use stacked self-attention mechanisms along with
point-wise, fully connected layers for both en-
coder and decoder to represent its input and out-
put. For the sake of brevity, we refer the reader
to (Vaswani et al., 2017) for further details regard-
ing the architecture. Similar to previously defined
loss functions, let JTF(x,y;Θ) denote the per-
example loss function of transformer networks for
an input-output translation pair (x,y) where Θ is
again indicating the learnable model parameters.

3.2 Model Variants and Training
Let {(x(l),y(l))}Nl=1 denote the set of training ex-
amples, where x(l)’s are input sequences, and
y(l)’s are their corresponding ground-truth output
sequences. Before optimizing for the introduced
objective JCALCS, we first train the corresponding
baseline network by minimizing

J{PG,TF}(Θ) =
1

N

N∑
l=1

J{PG,TF}(x,y;Θ).

Unlike JCALCS, loss functions J{PG,TF} for base-
line models are computed by teacher forcing, feed-
ing the previous ground-truth word at each de-
coding step. We will denote the baseline models
by POINTGEN for pointer-generator network and
TRANSFORMER for transformer network.

To optimize for the proposed objective JCALCS,
we initialize the model parameters Θ from the pre-
trained baseline network and continue training the
model by minimizing the joint loss

J(Θ) = λJCALCS(Θ) + (1− λ)J{PG,TF}(Θ)

(18)

JCALCS(Θ) =
1

N

N∑
l=1

JCALCS(x,y;Θ) (19)

where λ is a hyperparameter controlling the bal-
ance between the two losses. During the training
with the joint loss, we compute JCALCS(x,y;Θ),
defined in Eq. 17, by performing |y|-many
decoding steps as a simple strategy to pre-
vent the model from gaming the training objec-



3714

Model ROUGE-1 ROUGE-L

(Nallapati et al., 2016) 35.46 32.65
w/o coverage (See et al., 2017) 36.44 33.42
w/ coverage (See et al., 2017) 39.53 36.38
LEAD-3 baseline (See et al., 2017) 40.34 36.57
RL (Paulus et al., 2018) 41.16 39.08
ML + RL (Paulus et al., 2018) 39.87 36.90
Our Models
POINTGEN* 39.11 26.97**
POINTGEN*+SS 39.33 26.94**
POINTGEN*+SS+CALCS 40.37 29.18**

Table 1: ROUGE F1 results on CNN/Daily Mail summa-
rization dataset. Our reimplementation of POINTGEN* cor-
responds to w/o coverage (See et al., 2017). ** sign near
ROUGE-L results reported for our models indicates a differ-
ence in our ROUGE-L evaluation as explained below.

tive by generating longer and longer hypothe-
ses instead of incurring an additional length
penalty. We will refer to the resulting model
trained with the loss function in Eq. 18 as
{POINTGEN, TRANSFORMER}+CALCS depend-
ing on the baseline model.

4 Experiments

We numerically evaluate the proposed method on
two sequence generation benchmarks: abstrac-
tive document-summarization and machine trans-
lation. We compare the results of the proposed
method against the recently proposed strong base-
line models (See et al., 2017) for summarization
and and (Vaswani et al., 2017) for machine trans-
lation tasks.

4.1 Abstractive Summarization

We use a modified version of the CNN/Daily Mail
dataset (Hermann et al., 2015) that is first used for
summarization by (Nallapati et al., 2016). How-
ever, we follow the processing script provided by
(See et al., 2017) to obtain non-anonymized ver-
sion of the data that contains 287,226 training
pairs, 13,368 validation pairs, and 11,490 test pairs
of news articles (781 tokens on average) and their
corresponding ground-truth summaries (56 tokens
on average). We refer the reader to (See et al.,
2017) for further details of the difference of their
version from (Nallapati et al., 2016).

For training our baseline model, we use single
layer LSTM encoder (bi-directional) and decoder
with hidden dimensions of 512 and 1024, respec-
tively. We use a vocabulary of 50k words for both
source and target. Following the original paper, we
also do not pre-train word embeddings, which are
learned with the rest of model parameters during

training. We use the Adam (Kingma and Ba, 2015)
optimizer with a learning rate of 0.00001 for train-
ing. We pre-train the baseline model for 20k steps
by applying greedy scheduled sampling (Bengio
et al., 2015) with fixed ground-truth feeding prob-
ability of 75%. Once the baseline model training
is complete, we start optimizing for CALCS objec-
tive as described in the previous section. Also, we
set λ = 1.0 and α = 1.0, which are tuned on the
development set.

In Table 1, we report our main results on the
summarization task. POINTGEN+SS refers to the
baseline model trained with scheduled sampling.
POINTGEN+SS+CALCS corresponds our model
trained with CALCS starting from POINTGEN+SS
model. Experimental results demonstrate that
training with our proposed objective provides an
improvement of 2.2 points in ROUGE-L score.
This also provides empirical evidence to justify
that our approximate CALCS effectively captures
what the original LCS metric is supposed to mea-
sure, recalling ROUGE-L is a normalized LCS.
The reason why ROUGE-L scores of our models
are lower than previously reported is that we eval-
uate ROUGE-L score by taking the entire sum-
mary as a single sequence instead of splitting it
into sentences, which is also the way we compute
CALCS objective during the model training pro-
cess. The main motivation behind this approach
is to encourage the model to preserve the sentence
order within a summary, and evaluate its perfor-
mance in the same way. We consider the capability
of preserving the order across produced sentences
as an important attribute a multi-sentence sum-
marization model should have in terms of read-
ability and fluency of its generated summaries as
a whole. When POINTGEN*+SS and POINT-
GEN*+SS+CALCS are evaluated by splitting the
generated summaries into sentences, their cor-
responding ROUGE-L scores become 35.38 and
35.12, respectively. We also observe a nice side-
improvement of 1.0 point in ROUGE-1 score over
the baseline, which achieves a comparable per-
formance with the long-overdue LEAD-3 base-
line score. It might also be comparable to the
recently reported state-of-the-art ROUGE-1 re-
sult on CNN/DailyMail dataset by Paulus et al.
(2018) as they used a different dataset processing
pipeline, which makes it difficult to directly com-
pare with ours.



3715

Model BLEU

GNMT (Wu et al., 2016) 24.61
GNMT+RL (Wu et al., 2016) 24.60
TRANSFORMER (Vaswani et al., 2017) 27.3
WEIGHTED TRANSFORMER (Ahmed et al., 2018) 28.4
TRANSFORMER* 27.6
TRANSFORMER*+CALCS 27.8

Table 2: Machine translation results on WMT 2014 English-
to-German task. TRANSFORMER* corresponds to our train-
ing of the original model in (Vaswani et al., 2017).

4.2 Machine Translation

We also evaluate our sequence-level training ap-
proach on the WMT 2014 English-to-German ma-
chine translation task, which contains 4.5M pairs
of sentences.

To train our baseline transformer model, we
closely follow the small model in the original
transformer paper (Vaswani et al., 2017). We use a
vocabulary of size 32k. Our encoder and decoder
consist of N = 6 identical layers each. Following
the notation in the original paper, we set the other
parameters as dmodel = 512, dff = 2048, h = 8,
Pdrop = 0.1. We set λ = 0.3 and α = 1.0, which
are tuned on the development set.

In Table 2, we show our empirical results on
machine translation task. Our first observation
is that our trained baseline transformer network
achieves a better performance than the one re-
ported in the original paper (Vaswani et al., 2017)
by 0.3 BLEU score, which might be solely due
to hyperparameter tuning. More importantly, we
observe that training with our proposed CALCS
objective leads to noticeable 0.2 BLEU point im-
provements over the baseline, which further rein-
forces our confidence in effectiveness of our pro-
posed sequence-level training approach and its ap-
plicability to other sequence prediction tasks. It
is also interesting to note that optimizing for LCS
metric via its continuous approximation leads to
improvements in evaluation with another discrete
metric BLEU. On the other had, optimizing for
the exact discrete metric BLEU via reinforcement
learning strategy may not improve the evaluation
performance in BLEU as reported by (Wu et al.,
2016). As a final remark, we would like to note
that our proposed approach is orthogonal to ad-
vancements in more expressive and powerful ar-
chitecture designs. Hence it has the potential to
provide further improvements over the recently
proposed models such as WEIGHTED TRANS-
FORMER (Ahmed et al., 2018).

5 Related Work

Text Summarization. Before the successful ap-
plication of neural generative models, most of the
existing works on text summarization (Dorr et al.,
2003; Durrett et al., 2016) have focused on ex-
tractive methods. While some of the early ap-
proaches have used a rich set of heuristic rules or
sparse features to select textual units to include in
the summary, more recent works (Cheng and La-
pata, 2016; Nallapati et al., 2017) leverage neu-
ral models to select words and sentences from the
original text. With the emergence of sequence-
to-sequence models (Sutskever et al., 2014) and
large-scale datasets like CNN/Daily Mail (Her-
mann et al., 2015; Nallapati et al., 2016) and
NYT (Paulus et al., 2018), abstractive summariza-
tion of longer text have become a more feasi-
ble and popular task. Several recent approaches
have been proposed to tackle abstractive summa-
rization problem, where Nallapati et al. (2016)
exploits hierarchical encoders, See et al. (2017)
proposes pointer-generator network and cover-
age mechanism to overcome OOV and repetition
problems, Tan et al. (2017) introduces a graph-
based attention mechanism and hierarchical beam
search strategy, and (Paulus et al., 2018) pro-
poses to optimize for ROUGE metric via rein-
forcement learning. Although impressive progress
has been achieved for sentence-level summariza-
tion, attempts on abstractive document summa-
rization task are still in early stages where the sim-
ple LEAD-3 baseline performance is only very re-
cently matched (Paulus et al., 2018).
Neural Machine Translation. With the re-
cent success of encoder-decoder architectures
(Sutskever et al., 2014; Bahdanau et al., 2015),
neural machine translation systems has gained a
a lot of attention both from academia (Cho et al.,
2014; Luong et al., 2015; Luong and Manning,
2016) and industry (Wu et al., 2016; Vaswani
et al., 2017; Ahmed et al., 2018) over statistical
machine translation, which has been the domi-
nating translation paradigm for years. Most of
these works has focused more on enhancing the
architecture design aspect to tackle with various
challenges such as different attention mechanisms
(Bahdanau et al., 2015; Luong et al., 2015), a
character-level decoder (Chung et al., 2016), a
translation coverage mechanism (Tu et al., 2016),
and so on. However, only very recently, a few
works (Wu et al., 2016; Ranzato et al., 2016;



3716

Norouzi et al., 2016; Shen et al., 2016; Bahdanau
et al., 2017; Zhukov and Kretov, 2017; Casas
et al., 2018) have investigated sequence-level op-
timization by training to maximize BLEU score.
Neural Sequence Generation with RL. Most
neural sequence generation models are trained
with the objective of maximizing the probability
of the next correct word. However, this results
in a major discrepancy between training and test
settings of these models because they are trained
with cross-entropy loss at word-level, but evalu-
ated based on sequence-level discrete metrics such
as ROUGE (Lin and Och, 2004) or BLEU (Pap-
ineni et al., 2002). On the other hand, directly op-
timizing for such evaluation metrics is hard due
to non-differentiable nature of the exact objec-
tive (Rosti et al., 2011). Recent works (Ranzato
et al., 2016; Wu et al., 2016; Bahdanau et al.,
2017; Paulus et al., 2018) address the difficulty
of differentiating with respect to rewards based on
such discrete metrics using variants of reinforce-
ment learning. These methods essentially pro-
pose to mitigate the problem by optimizing the
reward weighted log-likelihood of the hypothesis
sequences generated by the model distribution. In
this paper, we propose an alternative solution to
tackle this problem by introducing a differentiable
approximation to exact LCS metric that can be di-
rectly optimized by standard gradient-based meth-
ods without RL, while still addressing the expo-
sure bias problem.

6 Conclusion and Future Work

In this work we explored an alternative approach
for training text generation models with sequence-
level optimization to combat wrong objective and
exposure bias problems. We introduced a new ob-
jective function based on a continuous approxima-
tion of LCS metric that measures sequence-level
structure similarity between sentences. We ap-
plied our proposed approach to CNN/Daily Mail
dataset for long document summarization and
WMT 2014 English-to-German machine transla-
tion task. By extending the objectives of strong
neural baseline models with our proposed objec-
tive, we empirically demonstrated its effective-
ness on these two tasks. Our proposed approach
suggests a promising alternative to policy-gradient
methods to side step the difficulty of differentiat-
ing w.r.t reward function while directly optimizing
for sequence-level discrete metrics.

References
Karim Ahmed, Nitish Shirish Keskar, and Richard

Socher. 2018. Weighted transformer network for
machine translation. In International Conference on
Learning Representations (ICLR).

Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,
Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C.
Courville, and Yoshua Bengi. 2017. An Actor-Critic
algorithm for sequence prediction. In International
Conference on Learning Representations (ICLR).

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations (ICLR).

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015. Scheduled sampling for se-
quence prediction with recurrent neural networks.
In Advances in Neural Information Processing Sys-
tems (NIPS).

N. Casas, M R. Costa-juss, and J.A. R. Fonollosa.
2018. A differentiable bleu loss. analysis and first
results. Workshop of the 6th International Confer-
ence on Learning Representations.

Jianpeng Cheng and Mirella Lapata. 2016. Neural
summarization by extracting sentences and words.
In Annual Meeting of the Association for Computa-
tional Linguistics (ACL).

Kyunghyun Cho, Bart Van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengi. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Sumit Chopra, Michael Auli, and M. Alexander Rush.
2016. Abstractive sentence summarization with at-
tentive recurrent neural networks. In The North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).

J. Chung, K. Cho, , and Yoshua Bengio. 2016. A
character-level decoder without explicit segmenta-
tion for neural machine translation. arXiv preprint
arXiv:1603.06147.

Bonnie Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: A parse-and-trim approach to head-
line generation. In Proceedings of the HLT-NAACL
03 on Text Summarization Workshop - Volume 5.

Greg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein.
2016. Learning-based single-document summariza-
tion with compression and anaphoricity constraints.
In Annual Meeting of the Association for Computa-
tional Linguistics (ACL).

P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Pre-
cup, and D. Meger. 2018. Deep reinforcement learn-
ing that matters. In AAAI Conference on Artificial
Intelligence (AAAI).



3717

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems (NIPS).

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In International
Conference on Learning Representations (ICLR).

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In An-
nual Meeting of the Association for Computational
Linguistics (ACL).

C.Y. Lin and Franz Josef Och. 2004. Automatic eval-
uation of machine translation quality using longest
common subsequence and skip-bigram statistics. In
Annual Meeting of the Association for Computa-
tional Linguistics (ACL).

Minh-Thang Luong and Christopher D. Manning.
2016. A hybrid Word-Character approach to open
vocabulary neural machine translation. In Annual
Meeting of the Association for Computational Lin-
guistics (ACL).

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Empir-
ical Methods on Natural Language Processing
(EMNLP).

Yishu Miao and Phil Blunsom. 2016. Discrete gen-
erative models for sentence compression. In Em-
pirical Methods on Natural Language Processing
(EMNLP).

Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.
Summarunner: A recurrent neural network based se-
quence model for extractive summarization of docu-
ments. In AAAI Conference on Artificial Intelligence
(AAAI).

Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
Caglar Gulcehre, and Bing Xiang. 2016. Abstrac-
tive text summarization using sequence-to-sequence
rnns and beyond. In Computational Natural Lan-
guage Learning (CoNLL).

Mohammad Norouzi, Samy Bengio, Zhifeng Chen,
Navdeep Jaitly, Mike Schuster, Yonghui Wu, and
Dale Schuurmans. 2016. Reward augmented max-
imum likelihood for neural structured prediction. In
Advances in Neural Information Processing Systems
(NIPS).

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL).

Romain Paulus, Caiming Xiong, and Richard Socher.
2018. A deep reinforced model for abstractive sum-
marization. In International Conference on Learn-
ing Representations (ICLR).

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2016. Sequence level train-
ing with recurrent neural networks. In International
Conference on Learning Representations (ICLR).

Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2011. Expected bleu training
for graphs: Bbn system description for wmt11 sys-
tem combination task. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.

M. Alexander Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Empirical Methods on Nat-
ural Language Processing (EMNLP).

Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Minimum
risk training for neural machine translation. In An-
nual Meeting of the Association for Computational
Linguistics (ACL).

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems (NIPS).

Jiwei Tan, Xiaojun Wan, and Jianguo Xiao. 2017.
Abstractive document summarization with a graph-
based attentional neural model. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL).

Z. Tu, Z. Lu, Y. Liu, X. Liu, and H Li. 2016. Coverage-
based neural machine translation. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL).

Ashish Vaswani, Shazeer Noam, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems (NIPS).

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural In-
formation Processing Systems (NIPS).

R. J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine Learning.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Norouzim Mohammad, Wolfgang Macherey,
Maxim Krikun, Cao Yuan, Qin Gao, and et al.
Macherey, Klaus. 2016. Googles neural machine
translation system: Bridging the gap between hu-
man and machine translation. arXiv preprint
arXiv:1609.08144.



3718

Qingyu Zhou, Na Yang, Fur Wei, and Ming Zhou.
2017. Selective encoding for abstractive sentence
summarization. In Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).

Vlad Zhukov and Maksim Kretov. 2017. Differentiable
lower bound for expected BLEU score. In NIPS
Workshop on Conversational AI.

A Proof of Lemma 1

[Monotonicity] The following two inequalities

si,j ≤ si,j+1 ≤ si,j + 1
si,j ≤ si+1,j ≤ si,j + 1

hold for all 0 ≤ i < m and 0 ≤ j < k.

Proof. We will prove this lemma by induction on
i+ j.

Base Case: i + j = 0. In this case, we have
i = j = 0. Since s0,0 = s1,0 = s0,1 = 0
by definition, both s0,0 ≤ s0,1 ≤ s0,0 + 1 and
s0,0 ≤ s1,0 ≤ s0,0 + 1 hold.

Inductive Step: Assume for i+ j = l that

si,j ≤ si,j+1 ≤ si,j + 1 (20)
si,j ≤ si+1,j ≤ si,j + 1 (21)

hold.
We will now prove that the inequalities of in-

ductive hypothesis hold for i+ j = l + 1.
We will start by showing si,j+1 ≥ si,j . By defi-

nition, we have

si,j+1 = p
(yi)
j+1(si−1,j + 1)+ (22)

(1− p(yi)j+1)max(si−1,j+1, si,j) (23)

≥ p(yi)j+1(si−1,j + 1) + (1− p
(yi)
j+1)si,j

(24)

≥ p(yi)j+1si,j + (1− pj+1,yi)si,j (25)
≥ si,j (26)

where inequality 24 follows from the definition of
max operator, and inequality 25 follows from in-
duction assumption 20 because (i − 1) + j = l.
Hence, final inequality 26 establishes the proof of
si,j+1 ≥ si,j .

Now, we will show that si,j+1 ≤ si,j + 1 holds.

Again by definition, we have

si,j+1 = p
(yi)
j+1(si−1,j + 1)+ (27)

(1− p(yi)j+1)max(si−1,j+1, si,j) (28)

≤ p(yi)j+1(si−1,j + 1)+ (29)

(1− p(yi)j+1)(si−1,j + 1) (30)
≤ si−1,j + 1 (31)
≤ si,j + 1 (32)

where inequalities 30 and 32 follow from inequal-
ities 20 and 21 of inductive step as (i−1)+ j = l.

Note that 26 and 32 completes the proof of
si,j ≤ si,j+1 ≤ si,j + 1 for i + j = l + 1. Fol-
lowing similar arguments, one can easily establish
the correctness of si,j ≤ si+1,j ≤ si,j + 1 for
i+j = l+1, which completes the proof of Lemma
by induction.

B Definition of Rouge-L

Definition 2. ROUGE-L is a discrete similarity
metric that takes into account sentence level struc-
ture similarity by identifying longest co-occurring
in-sequence n-grams automatically via longest
common subsequence measure. Formally, given
two sequences y and z of tokens, we define
ROUGE-L(y, z) as the harmonic mean of preci-
sion LCS(y,z)k and recall

LCS(y,z)
m based on LCS

measure, where k = |z| and m = |y|.


