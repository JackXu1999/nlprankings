



















































Towards Robust and Privacy-preserving Text Representations


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 25–30
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

25

Towards Robust and Privacy-preserving Text Representations

Yitong Li Timothy Baldwin Trevor Cohn
School of Computing and Information Systems

The University of Melbourne, Australia
yitongl4@student.unimelb.edu.au
{tbaldwin,tcohn}@unimelb.edu.au

Abstract

Written text often provides sufficient clues
to identify the author, their gender, age,
and other important attributes. Conse-
quently, the authorship of training and
evaluation corpora can have unforeseen
impacts, including differing model perfor-
mance for different user groups, as well
as privacy implications. In this paper, we
propose an approach to explicitly obscure
important author characteristics at train-
ing time, such that representations learned
are invariant to these attributes. Evaluat-
ing on two tasks, we show that this leads
to increased privacy in the learned repre-
sentations, as well as more robust models
to varying evaluation conditions, includ-
ing out-of-domain corpora.

1 Introduction

Language is highly diverse, and differs accord-
ing to author, their background, and personal at-
tributes such as gender, age, education and na-
tionality. This variation can have a substantial
effect on NLP models learned from text (Hovy
et al., 2015), leading to significant variation in in-
ferences across different types of corpora, such
as the author’s native language, gender and age.
Training corpora are never truly representative,
and therefore models fit to these datasets are bi-
ased in the sense that they are much more effec-
tive for texts from certain groups of user, e.g.,
middle-aged white men, and considerably poorer
for other parts of the population (Hovy, 2015).
Moreover, models fit to language corpora often
fixate on author attributes which correlate with the
target variable, e.g., gender correlating with class
skews (Zhao et al., 2017), or translation choices
(Rabinovich et al., 2017). This signal, however,

is rarely fundamental to the task of modelling lan-
guage, and is better considered as a confounding
influence. These auxiliary learning signals can
mean the models do not adequately capture the
core linguistic problem. In such situations, remov-
ing these confounds should give better generali-
sation, especially for out-of-domain evaluation, a
similar motivation to research in domain adapta-
tion based on selection biases over text domains
(Blitzer et al., 2007; Daumé III, 2007).

Another related problem is privacy: texts con-
vey information about their author, often inadver-
tently, and many individuals may wish to keep
this information private. Consider the case of the
AOL search data leak, in which AOL released de-
tailed search logs of many of their users in Au-
gust 2006 (Pass et al., 2006). Although they de-
identified users in the data, the log itself contained
sufficient personally identifiable information that
allowed many of these individuals to be identi-
fed (Jones et al., 2007). Other sources of user
text, such as emails, SMS messages and social me-
dia posts, would likely pose similar privacy issues.
This raises the question of how the corpora, or
models built thereupon, can be distributed without
exposing this sensitive data. This is the problem
of differential privacy, which is more typically ap-
plied to structured data, and often involves data
masking, addition or noise, or other forms of cor-
ruption, such that formal bounds can be stated in
terms of the likelihood of reconstructing the pro-
tected components of the dataset (Dwork, 2008).
This often comes at the cost of an accuracy re-
duction for models trained on the corrupted data
(Shokri and Shmatikov, 2015; Abadi et al., 2016).

Another related setting is where latent repre-
sentations of the data are shared, rather than the
text itself, which might occur when sending data
from a phone to the cloud for processing, or trust-
ing a third party with sensitive emails for NLP



26

processing, such as grammar correction or trans-
lation. The transfered representations may still
contain sensitive information, however, especially
if an adversary has preliminary knowledge of the
training model, in which case they can readily re-
verse engineer the input, for example, by a GAN
attack algorithm (Hitaj et al., 2017). This is true
even when differential privacy mechanisms have
been applied.

Inspired by the above works, and recent suc-
cesses of adversarial learning (Goodfellow et al.,
2014; Ganin et al., 2016), we propose a novel ap-
proach for privacy-preserving learning of unbiased
representations.1 Specially, we employ Ganin
et al.’s approach to training deep models with ad-
versarial learning, to explicitly obscure individu-
als’ private information. Thereby the learned (hid-
den) representations of the data can be transferred
without compromising the authors’ privacy, while
still supporting high-quality NLP inference. We
evaluate on the tasks of POS-tagging and senti-
ment analysis, protecting several demographic at-
tributes — gender, age, and location — and show
empirically that doing so does not hurt accuracy,
but instead can lead to substantial gains, most no-
tably in out-of-domain evaluation. Compared to
differential privacy, we report gains rather than
loss in performance, but note that we provide only
empirical improvements in privacy, without any
formal guarantees.

2 Methodology

We consider a standard supervised learning situ-
ation, in which inputs x are used to compute a
representation h, which then forms the parameter-
isation of a generalised linear model, used to pre-
dict the target y. Training proceeds by minimising
a differentiable loss, e.g., cross entropy, between
predictions and the ground truth, in order to learn
an estimate of the model parameters, denoted θM .

Overfitting is a common problem, particular in
deep learning models with large numbers of pa-
rameters, whereby h learns to capture specifics
of the training instances which do not generalise
to unseen data. Some types of overfitting are in-
sidious, and cannot be adequately addressed with
standard techniques like dropout or regularisation.
Consider, for example, the authorship of each sen-

1Implementation available at https://github.
com/lrank/Robust_and_Privacy_preserving_
Text_Representations.

xi
Model(θ)

h yi
θc

Di(θ
d
i ) bi

Dj(θ
d
j ) bj

Figure 1: Proposed model architectures, showing
a single training instance (xi, yi) with two pro-
tected attributes, bi and bj . D indicates a discrim-
inator, and the red dashed and blue lines denote
adversarial and standard loss, respectively.

tence in the training set in a sentiment prediction
task. Knowing the author, and their general dispo-
sition, will likely provide strong clues about their
sentiment wrt any sentence. Moreover, given the
ease of authorship attribution, a powerful learning
model might learn to detect the author from their
text, and use this to predict the sentiment, rather
than basing the decision on the semantics of each
sentence. This might be the most efficient use of
model capacity if there are many sentences by this
individual in the training dataset, yet will lead to
poor generalisation to test data authored by unseen
individuals.

Moreover, this raises privacy issues when h
is known by an attacker or malicious adversary.
Traditional privacy-preserving methods, such as
added noise or masking, applied to the representa-
tion will often incur a cost in terms of a reduction
in task performance. Differential privacy methods
are unable to protect the user privacy of h under
adversarial attacks, as described in Section 1.

Therefore, we consider how to learn an un-
biased representations of the data with respect to
specific attributes which we expect to behave as
confounds in a generalisation setting. To do so, we
take inspiration from adversarial learning (Good-
fellow et al., 2014; Ganin et al., 2016). The archi-
tecture is illustrated in Figure 1.

2.1 Adversarial Learning

A key idea of adversarial learning, following
Ganin et al. (2016), is to learn a discriminator
model D jointly with learning the standard super-
vised model. Using gender as an example, a dis-
criminator will attempt to predict the gender, b, of
each instance from h, such that training involves
joint learning of both the model parameters, θM ,
and the discriminator parameters θD. However,

https://github.com/lrank/Robust_and_Privacy_preserving_Text_Representations
https://github.com/lrank/Robust_and_Privacy_preserving_Text_Representations
https://github.com/lrank/Robust_and_Privacy_preserving_Text_Representations


27

the aim of learning for these components are in
opposition – we seek a h which leads to a good
predictor of the target y, while being a poor rep-
resentation for prediction of gender. This leads to
the objective (illustrated for a single training in-
stance),

θ̂ = min
θM

max
θD
X (ŷ(x; θM ),y)

− λ · X (b̂(x; θD), b) ,
(1)

where X denotes the cross entropy function. The
negative sign of the second term, referred to as the
adversarial loss, can be implemented by a gradi-
ent reversal layer during backpropagation (Ganin
et al., 2016). To elaborate, training is based on
standard gradient backpropagation for learning the
main task, but for the auxiliary task, we start with
standard loss backpropagation, however gradients
are reversed in sign when they reach h. Conse-
quently the linear output components are trained
to be good predictors, but h is trained to be maxi-
mally good for the main task and maximally poor
for the auxiliary task.

Furthermore, Equation 1 can be expanded to
scenarios with several (N ) protected attributes,

θ̂ = min
θM

max
{θDi}

N
i=1

X (ŷ(x; θM ),y) (2)

−
N∑
i=1

(
λi · X (b̂(x; θDi), bi)

)
.

3 Experiments

In this section, we report experimental results
for our methods with two very different language
tasks.

3.1 POS-tagging

This first task is part-of-speech (POS) tagging,
framed as a sequence tagging problem. Recent
demographic studies have found that the author’s
gender, age and race can influence tagger perfor-
mance (Hovy and Søgaard, 2015; Jørgensen et al.,
2016). Therefore, we use the POS tagging to
demonstrate that our model is capable of elimi-
nating this type of bias, thereby leading to more
robust models of the problem.

Model Our model is a bi-directional LSTM for
POS tag prediction (Hochreiter and Schmidhuber,

1997), formulated as:

hi = LSTM(xi,hi−1; θh)

h′i = LSTM(xi,h
′
i+1; θ

′
h)

yi ∼ Categorical(φ(
[
hi;h

′
i

]
); θo) ,

for input sequence xi|ni=1 with terminal hidden
states h0 and h′n+1 set to zero, where φ is a linear
transformation, and [·; ·] denotes vector concatena-
tion.

For the adversarial learning, we use the train-
ing objective from Equation 2 to protect gender
and age, both of which are treated as binary val-
ues. The adversarial component is parameterised
by 1-hidden feedforward nets, applied to the final
hidden representation [hn;h′0]. For hyperparame-
ters, we fix the size of the word embeddings and
h to 300, and set all λ values to 10−3. A dropout
rate of 0.5 is applied to all hidden layers during
training.

Data We use the TrustPilot English POS tagged
dataset (Hovy and Søgaard, 2015), which consists
of 600 sentences, each labelled with both the sex
and age of the author, and manually POS tagged
based on the Google Universal POS tagset (Petrov
et al., 2012). For the purposes of this paper, we
follow Hovy and Søgaard’s setup, categorising
SEX into female (F) and male (M), and AGE into
over-45 (O45) and under-35 (U35). We train the
taggers both with and without the adversarial loss,
denoted ADV and BASELINE, respectively.

For evaluation, we perform a 10-fold cross val-
idation, with a train:dev:test split using ratios of
8:1:1. We also follow the evaluation method in
Hovy and Søgaard (2015), by reporting the tag-
ging accuracy for sentences over different slices of
the data based on SEX and AGE, and the absolute
difference between the two settings.

Considering the tiny quantity of text in the
TrustPilot corpus, we use the Web English Tree-
bank (WebEng: Bies et al. (2012)), as a means
of pre-training the tagging model. WebEng was
chosen to be as similar as possible in domain to
the TrustPilot data, in that the corpus includes
unedited user generated internet content.

As a second evaluation set, we use a corpus
of African-American Vernacular English (AAVE)
from Jørgensen et al. (2016), which is used purely
for held-out evaluation. AAVE consists of three
very heterogeneous domains: LYRICS, SUBTI-
TLES and TWEETS. Considering the substantial



28

SEX AGE

F M ∆ O45 U35 ∆

BASELINE 90.9 91.1 0.2 91.4 89.9 1.5
ADV 92.2 92.1 0.1 92.3 92.0 0.3

Table 1: POS prediction accuracy [%] using the
Trustpilot test set, stratified by SEX and AGE
(higher is better), and the absolute difference (∆)
within each bias group (smaller is better). The best
result is indicated in bold.

difference between this corpus and WebEng or
TrustPilot, and the lack of any domain adaptation,
we expect a substantial drop in performance when
transferring models, but also expect a larger im-
pact from bias removal using ADV training.

Results and analysis Table 1 shows the results
for the TrustPilot dataset. Observe that the dispar-
ity for the BASELINE tagger accuracy (the ∆ col-
umn), for AGE is larger than for SEX, consistent
with the results of Hovy and Søgaard (2015). Our
ADV method leads to a sizeable reduction in the
difference in accuracy across both SEX and AGE,
showing our model is capturing the bias signal less
and more robust to the tagging task. Moreover, our
method leads to a substantial improvement in ac-
curacy across all the test cases. We speculate that
this is a consequence of the regularising effect of
the adversarial loss, leading to a better characteri-
sation of the tagging problem.

Table 2 shows the results for the AAVE held-
out domain. Note that we do not have annotations
for SEX or AGE, and thus we only report the over-
all accuracy on this dataset. Note that ADV also
significantly outperforms the BASELINE across the
three heldout domains.

Combined, these results demonstrate that our
model can learn relatively gender and age de-
biased representations, while simultaneously im-
proving the predictive performance, both for in-
domain and out-of-domain evaluation scenarios.

3.2 Sentiment Analysis

The second task we use is sentiment analysis,
which also has broad applications to the online
community, as well as privacy implications for the
authors whose text is used to train our models.
Many user attributes have been shown to be eas-
ily detectable from online review data, as used ex-
tensively in sentiment analysis results (Hovy et al.,
2015; Potthast et al., 2017). In this paper, we fo-

LYRICS SUBTITLES TWEETS Average

BASELINE 73.7 81.4 59.9 71.7
ADV 80.5 85.8 65.4 77.0

Table 2: POS predictive accuracy [%] over the
AAVE dataset, stratified over the three domains,
alongside the macro-average accuracy. The best
result is indicated in bold.

cus on three demographic variables of gender, age,
and location.

Model Sentiment is framed as a 5-class text
classification problem, which we model using
Kim (2014)’s convolutional neural net (CNN) ar-
chitecture, in which the hidden representation is
generated by a series of convolutional filters fol-
lowed a maxpooling step, simply denote as h =
CNN(x; θM ). We follow the hyper-parameter set-
tings of Kim (2014), and initialise the model with
word2vec embeddings (Mikolov et al., 2013). We
set the λ values to 10−3 and apply a dropout rate
of 0.5 to h.

As the discriminator, we also use a feed-forward
model with one hidden layer, to predict the pri-
vate attribute(s). We compare models trained with
zero, one, or all three private attributes, denoted
BASELINE, ADV-*, and ADV-all, respectively.

Data We again use the TrustPilot dataset de-
rived from Hovy et al. (2015), however now we
consider the RATING score as the target variable,
not POS-tag. Each review is associated with three
further attributes: gender (SEX), age (AGE), and
location (LOC). To ensure that LOC cannot be triv-
ially predicted based on the script, we discard all
non-English reviews based on LANGID.PY (Lui
and Baldwin, 2012), by retaining only reviews
classified as English with a confidence greater than
0.9. We then subsample 10k reviews for each lo-
cation to balance the five location classes (US,
UK, Germany, Denmark, and France), which were
highly skewed in the original dataset. We use the
same binary representation of SEX and AGE as
the POS task, following the setup in Hovy et al.
(2015).

To evaluate the different models, we perform
10-fold cross validation and report test perfor-
mance in terms of the F1 score for the RATING
task, and the accuracy of each discriminator. Note
that the discriminator can be applied to test data,
where it plays the role of an adversarial attacker,
by trying to determine the private attributes of



29

F1 Discrim. [%]

dev test AGE SEX LOC

Majority class 57.8 62.3 20.0

BASELINE 41.9 40.1 65.3 66.9 53.4

ADV-AGE 42.7 40.1 61.1 65.6 41.0
ADV-SEX 42.4 39.9 61.8 62.9 42.7
ADV-LOC 42.0 40.2 62.2 66.8 22.1
ADV-all 42.0 40.2 61.8 62.5 28.1

Table 3: Sentiment F1-score [%] over the RAT-
ING task, and accuracy [%] of all the discriminator
across three private attributes. The best score is in-
dicated in bold. The majority class with respect to
each private attribute is also reported.

users based on their hidden representation. That
is, lower discriminator performance indicates that
the representation conveys better privacy for indi-
viduals, and vice versa.

Results Table 3 shows the results of the differ-
ent models. Note that all the privacy attributes can
be easily detected in BASELINE, with results that
are substantially higher than the majority class, al-
though AGE and SEX are less well captured than
LOC. The ADV trained models all maintain the
task performance of the BASELINE method, how-
ever they clearly have a substantial effect on the
discrimination accuracy. The privacy of SEX and
LOC is substantially improved, leading to dis-
criminators with performance close to that of the
majority class (conveys little information). AGE
proves harder, although our technique leads to pri-
vacy improvements. Note that AGE appears to be
related to the other private attributes, in that pri-
vacy is improved when optimising an adversarial
loss for the other attributes (SEX and LOC).

Overall, these results show that our approach
learns hidden representations that hide much of
the personal information of users, without affect-
ing the sentiment task performance. This is a sur-
prising finding, which augurs well for the use of
deep learning as a privacy preserving mechanism
when handling text corpora.

4 Conclusion

We proposed a novel method for removing model
biases by explicitly protecting private author at-
tributes as part of model training, which we formu-
late as deep learning with adversarial learning. We
evaluate our methods with POS tagging and senti-

ment classification, demonstrating our method re-
sults in increased privacy, while also maintaining,
or even improving, task performance, through in-
creased model robustness.

Acknowledgements

We thank Benjamin Rubinstein and the anony-
mous reviewers for their helpful feedback and sug-
gestions, and the National Computational Infras-
tructure Australia for computation resources. We
also thank Dirk Hovy for providing the Trustpi-
lot dataset. This work was supported by the Aus-
tralian Research Council (FT130101105).

References
Martin Abadi, Andy Chu, Ian Goodfellow, H Bren-

dan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. 2016. Deep learning with differential pri-
vacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Se-
curity, pages 308–318.

Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. Linguistic Data Con-
sortium, Philadelphia, USA.

John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 440–447.

Hal Daumé III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256–263.

Cynthia Dwork. 2008. Differential privacy: A survey
of results. In International Conference on Theory
and Applications of Models of Computation, pages
1–19. Springer.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, François Lavi-
olette, Mario Marchand, and Victor Lempitsky.
2016. Domain-adversarial training of neural net-
works. Journal of Machine Learning Research,
17:59:1–59:35.

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. 2014. Gen-
erative adversarial nets. In Advances in Neural In-
formation Processing Systems 27, pages 2672–2680.

Briland Hitaj, Giuseppe Ateniese, and Fernando Pérez-
Cruz. 2017. Deep models under the gan: informa-
tion leakage from collaborative deep learning. In
Proceedings of the 2017 ACM SIGSAC Conference



30

on Computer and Communications Security, pages
603–618. ACM.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Dirk Hovy. 2015. Demographic factors improve clas-
sification performance. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), volume 1, pages 752–762.

Dirk Hovy, Anders Johannsen, and Anders Søgaard.
2015. User review sites as a resource for large-scale
sociolinguistic studies. In Proceedings of the 24th
International Conference on World Wide Web, pages
452–461.

Dirk Hovy and Anders Søgaard. 2015. Tagging perfor-
mance correlates with author age. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 2: Short Papers), volume 2, pages 483–488.

Rosie Jones, Ravi Kumar, Bo Pang, and Andrew
Tomkins. 2007. I know what you did last summer:
query logs and user privacy. In Proceedings of the
Sixteenth ACM Conference on Conference on Infor-
mation and Knowledge Management (CIKM 2007),
pages 909–914.

Anna Jørgensen, Dirk Hovy, and Anders Søgaard.
2016. Learning a POS tagger for AAVE-like lan-
guage. In Proceedings of the 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1115–1120.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1746–1751.

Marco Lui and Timothy Baldwin. 2012. langid.py:
An off-the-shelf language identification tool. In
Proceedings of ACL 2012 System Demonstrations,
pages 25–30.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111–3119.

Greg Pass, Abdur Chowdhury, and Cayley Torgeson.
2006. A picture of search. In The First Interna-
tional Conference on Scalable Information Systems,
volume 152, page 1.

Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings
of the Eighth International Conference on Language
Resources and Evaluation.

Martin Potthast, Francisco Rangel, Michael Tschug-
gnall, Efstathios Stamatatos, Paolo Rosso, and
Benno Stein. 2017. Overview of PAN17: Author
identification, author profiling, and author obfusca-
tion. In 8th International Conference of the CLEF
on Experimental IR Meets Multilinguality, Multi-
modality, and Visualization.

Ella Rabinovich, Raj Nath Patel, Shachar Mirkin, Lu-
cia Specia, and Shuly Wintner. 2017. Personal-
ized machine translation: Preserving original author
traits. In Proceedings of the 15th Conference of the
European Chapter of the Association for Computa-
tional Linguistics: Volume 1, Long Papers, pages
1074–1084.

Reza Shokri and Vitaly Shmatikov. 2015. Privacy-
preserving deep learning. In Proceedings of the
22nd ACM SIGSAC Conference on Computer and
Communications Security, pages 1310–1321.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. 2017. Men also like
shopping: Reducing gender bias amplification us-
ing corpus-level constraints. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing, pages 2979–2989.


