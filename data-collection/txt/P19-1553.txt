



















































Better Exploiting Latent Variables in Text Modeling


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5527–5532
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

5527

Better Exploiting Latent Variables in Text Modeling

Canasai Kruengkrai
Yahoo Japan Corporation

ckruengk@yahoo-corp.jp

Abstract

We show that sampling latent variables multi-
ple times at a gradient step helps in improving
a variational autoencoder and propose a simple
and effective method to better exploit these la-
tent variables through hidden state averaging.
Consistent gains in performance on two differ-
ent datasets, Penn Treebank and Yahoo, indi-
cate the generalizability of our method.1

1 Introduction

Introducing latent variables to neural language
models would help in generating plausible sen-
tences that reflect sentential semantics (Bowman
et al., 2016). The success of learning latent vari-
ables is also beneficial to various natural lan-
guage processing (NLP) tasks such as sentence
compression (Miao and Blunsom, 2016) and text
style transfer (Shen et al., 2017). One of the
widely-used latent variable models is the varia-
tional autoencoder (VAE) (Kingma and Welling,
2014; Rezende et al., 2014). When applying the
VAE to text data, recurrent neural networks are
typically utilized for both the encoder and the de-
coder. Training the VAE with a high-capacity de-
coder such as a long short-term memory (LSTM)
network (Hochreiter and Schmidhuber, 1997) can
be challenging. The LSTM is powerful enough to
model the underlying data distribution without the
use of latent variables.

In this paper, we take a closer look at one of the
components in an LSTM-VAE model, namely the
latent variable sampling scheme. Fig. 1 illustrates
our baseline LSTM-VAE model built upon Bow-
man et al. (2016)’s model. At each gradient step
(i.e., a minibatch run), most previous work pairs
an input sentence with a single latent variable de-
noted by z. This would be sufficient in some

1The code for reproducibility is available at https://
research-lab.yahoo.co.jp/en/software.

Word embedding
Encoding LSTM cell
Decoding LSTM cell

Description
Linear
Linear+Softmax
Dropout

new hopea !"# a new hope

a new hope $"#

%∼'(0,1)

()
*
⨁
⨀

Figure 1: Baseline LSTM-VAE model.

tasks but not necessarily effective in text model-
ing. At the beginning of training, the latent vari-
able z contains a small amount of information
about the input sentence. Many latent units of z
are pulled towards the prior early to optimize an
objective function before they capture useful in-
formation (Hoffman et al., 2013; Sønderby et al.,
2016). Without a cost annealing strategy or a con-
straint on the decoder (Bowman et al., 2016; Chen
et al., 2017; Yang et al., 2017), z would be en-
tirely ignored for the remaining training steps. In
our work, we aim at developing a simple variant
of the LSTM-VAE model to address this common
training issue. We observe that pairing the input
sentence with multiple latent variables improves
latent variable usage. In addition, we present
a method that leverages multiple latent variables
to further boost the performance of the baseline
LSTM-VAE model.

Our contributions are as follows: We suggest
sampling the latent variables multiple times at
each gradient step. We propose a simple method to
better exploit these latent variables through hidden
state averaging. We evaluate the proposed method
on two different datasets, Penn Treebank and Ya-
hoo, and compare to the best results published in
the literature. Our empirical results show that our
method can effectively make use of the latent vari-
ables, leading to the state-of-the-art performance.

https://research-lab.yahoo.co.jp/en/software
https://research-lab.yahoo.co.jp/en/software


5528

2 Related work

Bowman et al. (2016) first proposed an LSTM-
VAE model for text. They observed the posterior-
collapse problem in which the approximate pos-
terior collapses to the prior, and the model ig-
nores the latent variable. They suggested two
techniques to alleviate this issue: cost annealing
(called warm-up in (Sønderby et al., 2016)) and
word dropout. Weakening the decoder with word
dropout forces the latent variable to encode more
information, but their LSTM-VAE model still un-
derperforms against the standard LSTM language
model. Yang et al. (2017) proposed to replace the
LSTM decoder with a dilated convolutional neural
network (CNN) (van den Oord et al., 2016) to con-
trol the contextual capacity. However, their posi-
tive results also came from initializing the encoder
with a pre-trained LSTM language model. Guu
et al. (2018) first proposed using the von Mises–
Fisher (vMF) distribution to model the VAE in-
stead of using the Gaussian distribution. However,
the vMF distribution presupposes that all data are
directional unit vectors. Other applications of
the vMF distribution can be found in (Davidson
et al., 2018; Xu and Durrett, 2018). Kim et al.
(2018) presented a semi-amortized (SA) approach
to training the VAE, while He et al. (2019) pro-
posed an aggressive inference network training.
However, their training algorithms are computa-
tionally expensive since they require backpropa-
gating through the decoder or the encoder multi-
ple times. Our method is simpler and easy to im-
plement. In practice, we just place a loop before
reparameterization and do averaging.

3 Background

Let x = [w1, w2, . . . , wT ] be a sentence represen-
tation, where wt is the t-th word. Assume that
x is generated from a continuous latent variable
z using a random process x ∼ pθ(x|z) parame-
terized by θ. By applying the standard language
model (Bengio et al., 2003), we get:

pθ(x|z) =
T∏
t=1

pθ(wt|w1:t−1, z). (1)

Given a dataset X = {x(1), . . . ,x(N)}, we typi-
cally fit the model by maximizing the average log-
marginal likelihood 1N

∑N
1 log pθ(x

(i)). We can
express an individual log-marginal likelihood by
log pθ(x) = log

∫
z pθ(x|z)p(z)dz, where p(z) is

the prior on z. Unfortunately, the integral over
z is intractable (Hoffman et al., 2013). Alterna-
tively, we would sample z directly from the poste-
rior distribution pθ(z|x). However, pθ(z|x) is also
intractable since pθ(z|x) = pθ(x|z)p(z)/pθ(x).

Variational inference approximates the poste-
rior distribution pθ(z|x) with a variational family
of distributions qφ(z|x) parameterized by φ. We
wish that qφ(z|x) is close to pθ(z|x). We measure
this closeness by the Kullback–Leibler (KL) diver-
gence: KL(qφ(z|x)||pθ(z|x)). Instead of maxi-
mizing the true log-marginal likelihood, we maxi-
mize its lower bound:

log pθ(x) ≥ log pθ(x)−KL(qφ(z|x)||pθ(z|x))
= Eqφ(z|x)[log pθ(x|z)]
−KL(qφ(z|x)||p(z)). (2)

The above equation is typically referred to as
the evidence lower bound (ELBO) (Hoffman and
Johnson, 2016). The ELBO consists of two terms:
the expected reconstruction term and the KL-
divergence term. We can solve the KL-divergence
term analytically given that both the prior p(z)
and the variational posterior qφ(z|x) are Gaussian
(see Kingma and Welling (2014)’s Appendix B).
We then need to rewrite the expected reconstruc-
tion term into some closed-form expression (de-
tailed in §4) so that we can maximize it by apply-
ing stochastic optimization methods.

Optimizing the ELBO forms the VAE archi-
tecture in which qφ(z|x) encodes x into a latent
variable z, and pθ(x|z) decodes z to reconstruct
x. The gradient of the ELBO w.r.t. φ can have
low variance by applying the reparameterization
trick (Kingma and Welling, 2014) that estimates
z ∼ qφ(z|x) using z = µ+ σ � �, where mean µ
and variance σ2 are outputs of some neural net-
works, and � ∼ N (0,1).

4 Proposed method

Having covered the technical background, we now
describe our two extensions to improve the base-
line LSTM-VAE model in Fig. 1. The baseline
model approximates the expected reconstruction
term by sampling one latent variable z ∼ qφ(z|x)
at each gradient step (Bowman et al., 2016). Thus,
Eqφ(z|x)[log pθ(x|z)] ≈ log pθ(x|z).

Our first extension is to improve the sampling
by using a Monte Carlo estimate of the expected



5529

!(1)"
#

LSTM
encoder

$(1)
⨀
⨁

$(2)∼((0,1)

⨁
⨀
!(2)

Embedding layer

LSTM
decoder

Embedding layer

Linear+Softmax layer

Figure 2: Example of sampling two latent variables us-
ing different random noise vectors drawn from the stan-
dard Gaussian distribution.

Embedding layer

!(1)
!(2)

a

avg

new

avg

hope

avg

EOS

avg

Figure 3: Example of averaging two hidden states at
each decoding step.

reconstruction term (Kingma and Welling, 2014):

Eqφ(z|x)[log pθ(x|z)]≈
1

L

L∑
l=1

log pθ(x|z(l)), (3)

where z(l) = µ + σ � �(l) and �(l) ∼ N (0,1).
Sampling latent variables multiple times at each
gradient step should result in a better approxima-
tion of the expected reconstruction term. Fig. 2
shows an example of sampling two latent vari-
ables. Note that we use the same µ and σ for
both latent variables. By using the language model
from Eq. (1), we can decompose the reconstruc-
tion term as:

log pθ(x|z(l)) =
T∑
t=1

log pθ(wt|w1:t−1, z(l)). (4)

Let V be a fixed size vocabulary of words in
a dataset. Given the entire history of previous
words w1:t = [w1, . . . , wt] and the latent variable
z(l), we compute the distribution over the possi-
ble corresponding values of wt+1 by applying a
linear transformation to the decoder hidden state
followed by a softmax:

pθ(wt+1|w1:t, z(l)) = softmax(h
(l)
t M1),

h
(l)
t = dec(h

(l)
t−1,wt),

h
(l)
0 = M2z

(l),

(5)

where M1 ∈ Rm×|V| and M2 ∈ Rm×n are the
trainable weight matrices, h(l)t ∈ Rm is the de-

coder hidden state, z(l) ∈ Rn is the latent variable
at each sampling step l, and wt ∈ Rd is the embed-
ding vector of the word wt. We compute µ and σ2

used in the reparameterization trick by:

µ = M3sT ,

logσ2 = M4sT ,

st = enc(st−1,wt), t = 1, . . . , T

s0 = 0,

(6)

where M3,M4 ∈ Rn×m are the trainable weight
matrices and sT ∈ Rm is the last encoder hidden
state.

Our second extension is to exploit multiple la-
tent variables to directly improve the expressive-
ness of the decoder. Instead of computing the sep-
arate reconstruction terms and taking the average
of them as in Eq. (3), we combine the decoder hid-
den states at each time step t:

h̃t =
1

L

L∑
l=1

h
(l)
t , (7)

where each hidden state is initialized with a differ-
ent latent variable z(l). Fig. 3 shows an example of
averaging two hidden states at each decoding step.
Thus our distribution of wt+1 becomes:

pθ(wt+1|w1:t, z) = softmax(h̃tM1). (8)

Here we drop the superscript (l) since all hidden
states h(l)t are averaged into h̃t.

5 Experiments

5.1 Datasets and training details
We experiment on two datasets: Penn Treebank
(PTB) (Marcus et al., 1993) and Yahoo (Zhang
et al., 2015). Training/validation/test sets are iden-
tical to (Bowman et al., 2016; Xu and Durrett,
2018) for PTB and (Yang et al., 2017; Kim et al.,
2018) for Yahoo. We use single-layer unidirec-
tional LSTMs as an encoder and a decoder. Con-
figurations of our baseline model (LSTM-VAE,
Fig. 1) are identical to (Xu and Durrett, 2018) for
PTB and (Yang et al., 2017; Kim et al., 2018) for
Yahoo. When the LSTM encoder is not applied,
our model falls back to a vanilla language model
(LSTM-LM). Table 1 summarizes data statistics
and our model configurations.

We use the last hidden state (not the cell state) of
the LSTM encoder and feed it through linear trans-
formations to get the mean µ and the variance σ2.



5530

PTB Yahoo

Training 42068 100000
Validation 3370 10000
Test 3761 10000
|V| 10000 20000

d 100 512
m 400 1024
n 32 32

Table 1: Data statistics and model configurations.
|V| = vocabulary size; d = dimensionality of word
embeddings; m = number of LSTM hidden units; n =
dimensionality of latent variables.

We sample z using the reparameterization trick
and feed it through a linear transformation to get
the initial hidden state of the LSTM decoder while
setting the initial cell state to zero. We concate-
nate z with the word embedding at each decoding
step. We use dropout (Hinton et al., 2012) with
probability 0.5 on the input-to-hidden layers and
the hidden-to-softmax layers.

We initialize all model parameters and word
embeddings by sampling from U(−0.1, 0.1). We
train all models using stochastic gradient descent
(SGD) with the batch size of 32, the learning rate
of 1.0, and the gradient clipping at 5. The learning
rate decays by halves if the validation perplexity
does not improve. We train for 30 epochs or un-
til the validation perplexity has not improved for
3 times. All models are trained on NVIDIA Tesla
P40 GPUs.

Following previous work (Bowman et al., 2016;
Sønderby et al., 2016), we apply KL cost anneal-
ing to all LSTM-VAE models. The multiplier on
the KL term is increased linearly from 0 to 1 dur-
ing the first 10 epochs of training.

We also try word dropout (Bowman et al., 2016)
during development but find that it is not effective
when combined with standard dropout. Our find-
ing conforms to (Kim et al., 2018). So we do not
apply word dropout to our models.

5.2 Main results
We report the upper bounds (i.e., the negative
ELBO in Eq. (2)) on NLL/PPL. We vary the num-
ber of latent variables L in the variational mod-
els to assess their impact on performance. LSTM-
VAE-AVG indicates the averaging of hidden states
at each decoding step in Eq. (8). We also report
the results of the inputless setting (Bowman et al.,
2016), which corresponds to dropping all ground
truth words during decoding.

Table 2 shows the results of various mod-
els. The LSTM-VAE-AVG models with multi-
ple latent variables provide the best improvements
in terms of NLL/PPL. The LSTM-VAE models
trained with more latent variables offer slight im-
provements over the baseline version (i.e., using
one latent variable) for the standard setting.

The baseline LSTM-VAE models have low KL
values and underperform against LSTM-LM for
the standard setting. Incorporating multiple latent
variables consistently helps in increasing the KL
values. Note that a high KL term does not neces-
sarily imply a better upper bound. Generally, we
do not expect the KL term to approach zero. When
KL(qφ(z|x)||p(z)) = 0, it indicates that z and x
are independent (i.e., qφ(z|x) = qφ(z) = p(z)).
In other words, z learns nothing from x.

The LSTM-VAE-AVG models have relatively
high KL values (except the inputless setting on Ya-
hoo), while still maintaining better upper bounds
on NLL/PPL. These results suggest that our mod-
els with expressive decoders can effectively make
use of the latent variables.

5.3 Discussion

On PTB, LSTM-VAE-AVG (L = 10) achieves
the best results compared to previous work (Bow-
man et al., 2016; Xu and Durrett, 2018). On Ya-
hoo, LSTM-VAE-AVG (L = 5) slightly outper-
forms Kim et al. (2018)’s SA-VAE. Our model can
provide similar improvements while being sim-
pler. We also observe that our vanilla LSTM-
LM model and that of Kim et al. (2018) have bet-
ter results than Yang et al. (2017)’s models. One
plausible explanation is that Yang et al. (2017)
trained their models with Adam (Kingma and Ba,
2015), while we used SGD. For text modeling,
researchers have shown that SGD performs bet-
ter than other adaptive optimization methods such
as Adam (Wilson et al., 2017; Keskar and Socher,
2017).

The ELBO has been commonly used to evalu-
ate the variational models (Bowman et al., 2016;
Yang et al., 2017; Xu and Durrett, 2018; Kim
et al., 2018). There also exists a line of work that
uses importance sampling to estimate the true log-
marginal likelihood (Rezende et al., 2014; Burda
et al., 2016; Tomczak and Welling, 2018; He et al.,
2019). We further conduct experiments by com-
puting the importance sampling estimates with
500 samples and comparing to He et al. (2019)’s



5531

Model Standard Inputless
NLL KL PPL NLL KL PPL

Bowman et al. (2016) LSTM-LM 100 – 116 135 – 600
LSTM-VAE 101 2 119 125 15 380

Xu and Durrett (2018) LSTM-LM 100 – 114 134 – 596
LSTM-VAE 99 4.4 109 125 6.3 379
LSTM-vMF-VAE 96 5.7 98 117 18.6 262

This work LSTM-LM 100.8±0.2 – 99.4±0.7 139.9±0.0 – 592.3±0.5
LSTM-VAE 102.5±0.2 1.5±0.3 107.5±1.0 134.8±0.4 3.8±0.5 469.3±7.6
LSTM-VAE (L = 5) 100.7±0.3 2.1±0.4 98.8±1.2 134.7±0.9 3.9±0.9 468.1±19.4
LSTM-VAE (L = 10) 100.4±0.2 2.2±0.4 97.7±0.9 134.8±0.8 3.5±1.0 468.2±16.2
LSTM-VAE-AVG (L = 5) 97.3±0.6 7.6±0.9 84.6±2.4 118.8±0.5 10.6±0.3 225.8±5.6
LSTM-VAE-AVG (L = 10) 94.3±0.4 8.1±0.2 73.8±1.5 113.8±1.0 9.6±0.5 179.7±8.3

(a) PTB

Model Standard Inputless
NLL KL PPL NLL KL PPL

Yang et al. (2017) CNN-LM 335.4 – 66.6 – – –
CNN-VAE + init 332.1 10.0 63.9 – – –

Kim et al. (2018) LSTM-LM 329.1 – 61.6 – – –
SA-VAE 327.5 7.2 60.4 – – –

This work LSTM-LM 328.4±0.2 – 61.1±0.2 507.4±0.0 – 574.0±0.0
LSTM-VAE 330.4±0.4 1.5±0.5 62.6±0.3 467.5±0.3 18.5±0.4 348.5±1.5
LSTM-VAE (L = 5) 328.8±0.1 2.6±0.5 61.4±0.1 464.3±1.2 22.2±1.7 334.8±4.9
LSTM-VAE (L = 10) 329.1±0.1 2.8±0.7 61.6±0.1 464.3±1.3 22.8±1.7 334.8±5.5
LSTM-VAE-AVG (L = 5) 327.3±0.5 12.2±0.4 60.3±0.4 446.4±0.1 19.7±0.2 267.5±0.4
LSTM-VAE-AVG (L = 10) 328.5±1.3 10.8±1.0 61.2±1.0 441.4±0.5 16.8±0.2 251.2±1.5

(b) Yahoo

Table 2: Results on (a) PTB and (b) Yahoo test sets. For LSTM-LM, we show the exact negative log likelihood
(NLL) and perplexity (PPL). For the variational models, we show the upper bounds (i.e., the negative ELBO) on
NLL/PPL. The KL portion of the ELBO is given in the column alongside NLL. NLL/KL values are averaged
across examples. L indicates the number of latent variables at each gradient step. We report mean and standard
deviation computed across five training/test runs from different random initial starting points.

PTB Yahoo
NLL-ELBO NLLIW NLL-ELBO NLLIW

He et al. (2019) LSTM-VAE-AIN + anneal – – 328.4±0.2 326.7±0.1
This work LSTM-VAE 102.5±0.2 102.1±0.2 330.4±0.4 329.6±0.2

LSTM-VAE-AVG (L = 5) 97.3±0.6 95.1±0.8 327.3±0.5 324.0±0.5
LSTM-VAE-AVG (L = 10) 94.3±0.4 91.7±0.5 328.5±1.3 324.9±1.3

Table 3: Comparison of different NLL estimates on PTB and Yahoo test sets. NLL-ELBO = the upper bounds
taken from Table 2; NLLIW = the importance sampling estimates of NLL with 500 samples. We report mean and
standard deviation computed across five training/test runs from different random initial starting points.

aggressive inference network (AIN) training. Ta-
ble 3 shows a comparison of different NLL esti-
mates. Our results are consistent with those of (He
et al., 2019) in which the importance sampling
yields the tighter bounds than the ELBO.

6 Conclusion

We have shown that using multiple latent variables
at each gradient step can improve the performance
of the baseline LSTM-VAE model. The empirical

results indicate that our models combined with ex-
pressive decoders can successfully make use of the
latent variables, resulting in higher KL values and
better NLL/PPL results. Our proposed method is
simple and can serve as a strong baseline for latent
variable text modeling.

Acknowledgments

We thank the anonymous reviewers for their in-
sightful comments.



5532

References
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and

Christian Janvin. 2003. A neural probabilistic lan-
guage model. J. Mach. Learn. Res., 3.

Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, An-
drew M. Dai, Rafal Józefowicz, and Samy Ben-
gio. 2016. Generating sentences from a continuous
space. In Proceedings of CoNLL.

Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdi-
nov. 2016. Importance weighted autoencoders. In
Proceedings of ICLR.

Xi Chen, Diederik P. Kingma, Tim Salimans, Yan
Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. 2017. Variational
lossy autoencoder. In Proceedings of ICLR.

Tim R. Davidson, Luca Falorsi, Nicola De Cao,
Thomas Kipf, and Jakub M. Tomczak. 2018. Hyper-
spherical variational auto-encoders. In Proceedings
of UAI.

Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren,
and Percy Liang. 2018. Generating sentences by
editing prototypes. Transactions of the Association
for Computational Linguistics (TACL).

Junxian He, Daniel Spokoyny, Graham Neubig, and
Taylor Berg-Kirkpatrick. 2019. Lagging inference
networks and posterior collapse in variational au-
toencoders. In Proceedings of ICLR.

Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
dinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors.
CoRR, abs/1207.0580.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8).

Matthew D. Hoffman, David M. Blei, Chong Wang,
and John W. Paisley. 2013. Stochastic variational
inference. Journal of Machine Learning Research,
14(1).

Matthew D. Hoffman and Matthew J. Johnson. 2016.
Elbo surgery: yet another way to carve up the vari-
ational evidence lower bound. In Proceedings of
NIPS 2016 Workshop on Advances in Approximate
Bayesian Inference.

Nitish Shirish Keskar and Richard Socher. 2017. Im-
proving generalization performance by switching
from adam to SGD. CoRR, abs/1712.07628.

Yoon Kim, Sam Wiseman, Andrew Miller, David Son-
tag, and Alexander Rush. 2018. Semi-amortized
variational autoencoders. In Proceedings of ICML.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of ICLR.

Diederik P. Kingma and Max Welling. 2014. Auto-
encoding variational bayes. In Proceedings of ICLR.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Comput. Lin-
guist., 19(2).

Yishu Miao and Phil Blunsom. 2016. Language as a
latent variable: Discrete generative models for sen-
tence compression. In Proceedings of EMNLP.

Aäron van den Oord, Sander Dieleman, Heiga Zen,
Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew W. Senior, and Koray
Kavukcuoglu. 2016. Wavenet: A generative model
for raw audio. CoRR, abs/1609.03499.

Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Wierstra. 2014. Stochastic backpropagation and ap-
proximate inference in deep generative models. In
Proceedings of ICML.

Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2017. Style transfer from non-parallel text
by cross-alignment. In Proceedings of NIPS.

Casper Kaae Sønderby, Tapani Raiko, Lars Maalø e,
Søren Kaae Sø nderby, and Ole Winther. 2016. Lad-
der variational autoencoders. In Proceedings of
NIPS.

Jakub M. Tomczak and Max Welling. 2018. VAE with
a VampPrior. In Proceedings of AISTATS.

Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern,
Nati Srebro, and Benjamin Recht. 2017. The
marginal value of adaptive gradient methods in ma-
chine learning. In Proceedings of NIPS.

Jiacheng Xu and Greg Durrett. 2018. Spherical latent
spaces for stable variational autoencoders. In Pro-
ceedings of EMNLP.

Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and
Taylor Berg-Kirkpatrick. 2017. Improved varia-
tional autoencoders for text modeling using dilated
convolutions. In Proceedings of ICML.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Proceedings of NIPS.


