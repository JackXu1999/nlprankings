










































Turning	NMT	research	
into	commercial	
products	

Dragos	Munteanu	and	Adrià	de	Gispert	

Proceedings of AMTA 2018, vol. 2: MT Users' Track Boston, March 17 - 21, 2018   |   Page 166



•  Founded	in	1992	
•  3800+	Employees	
•  56	Offices	
•  38	Countries	
•  400	Partners	
•  1500	Enterprise	
customers	

helping	big	brands	go	global	

marketing	campaigns	 eCommerce	 documentation	 web,	social	media	 analytics	

78	of	the	top	
100	global	
companies	
work	with	
SDL	

+10	BILLION	
words	
translated	
monthly	

Proceedings of AMTA 2018, vol. 2: MT Users' Track Boston, March 17 - 21, 2018   |   Page 167



SDL	Research	–	a	long	history	in	MT	

•  Research	labs	in	Los	Angeles	(USA)	and	Cambridge	(UK)	
•  Team	members	have	published	+100	on	SMT	and	related	tech	

–  Bill	Byrne,	Abdessamad	Echihabi,	Dragos	Munteanu,	Gonzalo	Iglesias,	Eva	
Hasler,	Adrià	de	Gispert,	Steve	DeNeefe,	Jonathan	Graehl,	Wes	Feely,	Ling	
Tsou...	

•  Formerly	Language	Weaver	
–  15	years	of	leading	expertise	in	SMT	
–  major	contributions	(papers/patents)	in	phrase-based	and	string-to-tree	MT,	

automata-based	hierarchical	MT,	quality	estimation,	tuning,	evaluation...		

•  Strong	links	with	academia		(University	of	Cambridge)	
•  Summer	internships,	industrial	post-docs	

Proceedings of AMTA 2018, vol. 2: MT Users' Track Boston, March 17 - 21, 2018   |   Page 168



Our	mission:	Bring	MT	research	results	to	products	

Approaches	that	
work	for	many	
language	pairs	

Connectors,	
plug-ins…	

Translation	speed	

Privacy!	Top-quality	MT	
on	premise	and	in	
private	cloud	

High	translation	
quality	

Customization	/	
Personalization	

Respect	file	
formats	and	tags	

Controllable	
memory	and	disk	
footprint	

Robustness	to	
mis-spellings	

○  We	strive	to	provide	our	customers:		

Ability	to	learn	
over	time	
(AdaptiveMT)	

Terminology	and	
dictionaries	 Consistency	

Proceedings of AMTA 2018, vol. 2: MT Users' Track Boston, March 17 - 21, 2018   |   Page 169



SDL	Secure	Enterprise	Translation	Server	

ü 45	NMT	engines	currently	available	

Data	Security	
-  On	premises/private	cloud	
-  Used	by	gov’t	for	15	years	

Quality	/	Customization	
-  Neural	MT	
-  Custom	MT	out-of-the-box	

Cost-effective	scalability	
-  Elastic,	optimized	footprint	
-  Commodity	hardware	

Ease	of	Use	/	Integration	
-  deploys	In	hours	
-  MS	plug-in	&	REST	API	

Proceedings of AMTA 2018, vol. 2: MT Users' Track Boston, March 17 - 21, 2018   |   Page 170



Neural Machine Translation 

Proceedings of AMTA 2018, vol. 2: MT Users' Track Boston, March 17 - 21, 2018   |   Page 171



SMT	
•  Symbolic	models	
•  Independence	assumption	

(separate	sub-problems)	
•  Maximum-likelihood	

estimation	
•  CPU-oriented	training	
•  Source-side-guided	decoding	
•  Large	databases	

Neural	MT	
•  Continuous-space	models	
•  Single	end-to-end	model	
•  Discriminative	training	
•  Reliance	on	GPUs	
•  Target-side-guided	decoding	
•  Smaller	compact	models	

A	paradigm	shift	

Proceedings of AMTA 2018, vol. 2: MT Users' Track Boston, March 17 - 21, 2018   |   Page 172



Better	translation	models	

[Zhou	et	al.’16]	

[Gehring	et	al.’17]	
[Vaswani	et	al.’17]	

[Sutskever	et	al.’14]	[Bahdanau	et	al.’15]	

Proceedings of AMTA 2018, vol. 2: MT Users' Track Boston, March 17 - 21, 2018   |   Page 173



Better	BLEU	scores	

Rules-Based 
1970 

Statistical MT 

2002 

Neural MT 

2016 

WAT	 Jpn-Eng	 Eng-Jpn	

2014	 23.8	 35.0	

2015	 25.4	 35.8	

2016	 27.6	 36.2	

2017	 28.4	 41.5	

+4.4 !! +6.5 !! 



COMPARING	OUTPUTS 

©	2017	SDL	Plc	

SMT	

NMT 



Observable	quality	improvement	

国連難民高等弁務官事務所（UNHCR）は、内戦状態にあるシリアから逃れ
た難民の数が5百万人を超えたと発表した。	

Office of the United Nations High Commissioner for Refugees 
(UNHCR) is in a state of civil war when the number of refugees who 
have escaped from Syria have exceeded 5 million people.	

The United Nations High Commissioner for Refugees (UNHCR) 
announced that the number of refugees escaped from Syria in the 
civil war was over five million people.	

ü  30%	improvement	over	SMT	across	all	our	productized	engines	



But…	is	it	ALL	that	good?	

There	are	situations	in	which	NMT	fails	
	

•  When	it	fails,	it	fails	spectacularly 
–  unrelated	fluent	text	
–  repetitions,	neurobabble…	

•  MT	user/customer	expectations	
–  “MT	is	not	supposed	to	do	this”	!!!?!	
–  “Can	it	support	the	features	I	need”	???	



Over-generation	and	‘neurobabble’	

There was no clear correlation between the measured mass density 
and the measured mass density, and neither experiment A or B. 	

The company will pay approximately EUR 600 million in fines, and 
the U.S. Department of Justice (SEC) to pay for approximately EUR 
600 million, and the U.S. Department of Justice and the Justice 
Department of Justice (SEC) to reduce the amount of internal control 
of the board of directors of the board of directors of the board of 
directors… 



Over-generation	and	‘neurobabble’	

There was no clear correlation between the measured mass density 
and the measured mass density, and neither experiment A or B. 	

The company will pay approximately EUR 600 million in fines, and 
the U.S. Department of Justice (SEC) to pay for approximately EUR 
600 million, and the U.S. Department of Justice and the Justice 
Department of Justice (SEC) to reduce the amount of internal control 
of the board of directors of the board of directors of the board of 
directors… 



Data	is	EVEN	MORE	important	

•  New	NMT	models	are	better	learners	
– A	better	fit	to	the	training	data	
– Relevant	training	data	is	key		
– Avoid	babble	and	get	huge	gains!	

•  Domain	adaptation/data	selection	
[Freitag	and	Al-Onaizan’16]	[Chen	et	al.’17]	[Britz	et	al.’17]	
[Farajian	et	al.’17]	[Van	der	Wees	et	al.’17]	[Wang	et	al.’17]	
…	



Adapting	neural	models	

Major	improvements!	
	
Challenge:	
§  Adapt	to	customer	

domain/data	with	
minimal	re-training	

§  Maintain	high	quality	
across	domains	

Jpn-Eng	corpus	 #	words	

Generic	 >	300M	

Automotive	 <	1M	



Lexical	selection	

•  NMT	models	have	freedom	to	
produce	any	target	word	
–  Guided	by	source,	not	constrained	

•  SMT	engines	were	good	at	lexical	
selection	–	can	we	leverage?	
–  T-table,	n-gram	and	phrase	
probabilities,	memory-augmented	
models/search	

[Arthur	et	al.	EMNLP’16]	[Stahlberg	et	al.	EACL’17]	[Wang	et	al.;	Dahlmann	et	al.;						
Feng	et	al.	EMNLP’17]	[Zhang	et	al.	IJCNLP’17]	…	

[Arthur	et	al.	EMNLP’16]	

[Wang	et	al.	EMNLP’17]	



NMT	can	use	N-gram	posterior	probabilities	

Stahlberg	et	al.	(EACL’17):	“Neural	Machine	Translation	by	Minimising	the	Bayes-risk	with	Respect	
to	Syntactic	Translation	Lattices”	



But…	are	there	guarantees?	

•  Control	is	a	must	for	commercial	success	
•  One	very	bad	sentence	can	put	off	a	customer	

–  Back-off	if	needed	

•  Customers/Users	expect	certain	‘features’	
–  Decoding	speed,	dictionary	support,	formatting	
constraints,	Adaptive	MT,	…	



Dictionary	support	

•  Translation	output	must	translate	dictionary	
matches	exactly	–	constrained	search	

•  Easy	for	SMT	decoders	
•  NMT	beam	decoder	does	not	keep	an	alignment	between	

source	and	target	words	

“Zimra Games continues to innovate with 
the release next month of Coke Assault 3, 
which will satisfy the most demanding 
gamers.”	

English	 German	

Zimra	Games	 Zimra	Games	GmbH	

Coke	Assault	3	 Coke	Assault	III	

…	 …	

[Anderson	et	al.	EMNLP’17]	
[Hokamp	&	Liu	ACL’17]		
[Chatterjee	et	al.	WMT’17]	



Dictionary	support	

Constrained	search	
•  Build	a	finite-state	acceptor	with	

the	target-side	constraints	
•  Keep	one	separate	stack	per	

each	acceptor	state	
•  Output	only	hypotheses	from	

the	final	acceptor	state	
Ø Constraints	can	be	words	or	

phrases	
[Anderson	et	al.	EMNLP’17]	



Dictionary	support	

Challenges	
•  Computational	complexity	

grows	exponentially	with	the	
number	of	constraints	
–  order	is	unknown		

•  Nothing	prevents	repeated	
decoding:	

“Zimra Games GmbH setzt mit dem Veröffentlichung 
auf Coke Assault III im nächsten Monat der Angriff …
	



Entity	constraints	

•  Decoder	must	also	respect	meta-tags	
–  Key	to	support	file	formats	used	by	MT	users	

•  NMT	model	should	not	break	sequential	history		
•  Solutions	require	model	specialization	and/or	
decoding	restrictions	

“<B>Zimra Games</B> continues to innovate with the release <I>next 
month</I> of <B>Coke Assault <c=red>3</c></B>, which will satisfy the 
most demanding gamers.”	



Decoding	speed	

•  MT	users	are	expected	to	certain	translation	speeds	
–  Target	speed	varies,	but	well	above	research	engines	

•  Goal	is	to	provide	best	quality	at	desired	speed	
–  Speed	vs	quality	trade-off	

•  NMT	deployment	scenarios	
–  CPU	only	–	hand-held	devices,	…		
–  GPU	

•  NMT	training	speed	also	relevant	



Decoding	speed	vs	quality	trade-off	(1)	

•  Model	architecture	
–  recurrent,	
convolutional,	
attentional…	

–  number	of	
parameters,	layer	
precomputations…	

–  Unfolding	and	
shrinking	ensembles	

[Stahlberg	and	Byrne,	EMNLP’17]	

Stahlberg	and	Byrne	(EMNLP’17):	“Unfolding	and	Shrinking	Neural	Machine	Translation	Ensembles”	



Decoding	speed	vs	quality	trade-off	(2)	

•  Hardware	and	Linear	Algebra	
library	
–  Type	of	GPU	card	
–  CPU-GPU	communication	
–  GPU	usage	

•  Batching	
–  standard	in	training	



Decoding	speed	vs	quality	trade-off	(3)	

•  Decoding	parameters	
–  beam	size,	early	stopping…	

•  Reduced	vocabulary	softmax	(CPU)	
•  Weight	clipping	in	training	

–  Low-precision	inference	
[Wu	et	al.’16]		[Devlin,	EMNLP’17]	…	



Copyright	©	2008-2017	SDL	plc.	All	rights	reserved.	All	company	names,	brand	names,	
trademarks,	service	marks,	images	and	logos	are	the	property	of	their	respective	owners.	
	
This	presentation	and	its	content	are	SDL	confidential	unless	otherwise	specified,	and	may	
not	be	copied,	used	or	distributed	except	as	authorised	by	SDL.	

Software	and	Services	for	Human	Understanding	




