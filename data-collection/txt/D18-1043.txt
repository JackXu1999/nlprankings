



















































Non-Adversarial Unsupervised Word Translation


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 469–478
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

469

Non-Adversarial Unsupervised Word Translation

Yedid Hoshen
Facebook AI Research

Lior Wolf
Facebook AI Research and Tel Aviv University

Abstract

Unsupervised word translation from non-
parallel inter-lingual corpora has attracted
much research interest. Very recently, neu-
ral network methods trained with adversarial
loss functions achieved high accuracy on this
task. Despite the impressive success of the re-
cent techniques, they suffer from the typical
drawbacks of generative adversarial models:
sensitivity to hyper-parameters, long training
time and lack of interpretability. In this paper,
we make the observation that two sufficiently
similar distributions can be aligned correctly
with iterative matching methods. We present a
novel method that first aligns the second mo-
ment of the word distributions of the two lan-
guages and then iteratively refines the align-
ment. Extensive experiments on word trans-
lation of European and Non-European lan-
guages show that our method achieves better
performance than recent state-of-the-art deep
adversarial approaches and is competitive with
the supervised baseline. It is also efficient,
easy to parallelize on CPU and interpretable.

1 Introduction

Inferring word translations between languages is a
long-standing research task. Earliest efforts con-
centrated on finding parallel corpora in a pair
of languages and inferring a dictionary by force
alignment of words between the two languages.
An early example of this approach is the transla-
tion achieved using the Rosetta stone.

However, if most languages share the same ex-
pressive power and are used to describe similar
human experiences across cultures, they should
share similar statistical properties. Exploiting sta-
tistical properties of letters has been successfully
employed by substitution crypto-analysis since at
least the 9th century. It seems likely that one can
learn to map between languages statistically, by

considering the word distributions. As one spe-
cific example, it is likely that the set of elements
described by the most common words in one lan-
guage would greatly overlap with those described
in a second language.

Another support for the plausibility of unsuper-
vised word translation came with the realization
that when words are represented as vectors that
encode co-occurrences, the mapping between two
languages is well captured by an affine transfor-
mation (Mikolov et al., 2013b). In other words,
not only that one can expect the most frequent
words to be shared, one can also expect the rep-
resentations of these words to be similar up to a
linear transformation.

A major recent trend in unsupervised learning
is the use of Generative Adversarial Networks
(GANs) presented by Goodfellow et al. (2014), in
which two networks provide mutual training sig-
nals to each other: the generator and the discrimi-
nator. The discriminator plays an adversarial role
to a generative model and is trained to distinguish
between two distributions. Typically, these dis-
tributions are labeled as “real” and “fake”, where
“fake” denotes the generated samples.

In the context of unsupervised translation (Con-
neau et al., 2017; Zhang et al., 2017a,b), when
learning from a source language to a target lan-
guage, the “real” distribution is the distribution of
the target language and the “fake” one is the map-
ping of the source distribution using the learned
mapping. Such approaches have been shown re-
cently to be very effective when employed on top
of modern vector representations of words.

In this work, we ask whether GANs are nec-
essary for achieving the level of success recently
demonstrated for unsupervised word translation.
Given that the learned mapping is simple and that
the concepts described by the two languages are
similar, we suggest to directly map every word



470

in one language to the closest word in the other.
While one cannot expect that all words would
match correctly for a random initialization, some
would match and may help refine the affine trans-
formation. Once an improved affine transforma-
tion is recovered, the matching process can repeat.

Naturally, such an iterative approach relies on
a good initialization. For this purpose we employ
two methods. First, an initial mapping is obtained
by matching the means and covariances of the two
distributions. Second, multiple solutions, which
are obtained stochastically, are employed.

Using multiple stochastic solutions is crucial
for languages that are more distant, e.g., more
stochastic solutions are required for learning to
translate between English and Arabic, in compari-
son to English and French. Evaluating multiple so-
lutions relies on the ability to automatically iden-
tify the true matching without supervision and we
present an unsupervised reconstruction-based cri-
terion for determining the best stochastic solution.

Our presented approach is simple, has very few
hyper-parameters, and is trivial to parallelize. It
is also easily interpretable, since every step of the
method has a clear goal and a clear success metric,
which can also be evaluated without the ground
truth bilingual lexicon. An extensive set of exper-
iments shows that our much simpler and more ef-
ficient method is more effective than the state-of-
the-art GAN based method.

2 Related Work

The earlier contributions in the field of word trans-
lation without parallel corpora were limited to
finding matches between a small set of carefully
selected words and translations, and relied on co-
occurrence statistics (Rapp, 1995) or on similar-
ity in the variability of the context before and af-
ter the word (Fung, 1995). Finding translations of
larger sets of words was made possible in follow-
up work by incorporating a seed set of matching
words that is either given explicitly or inferred
based on words that appear in both languages or
are similar in edit distance due to a shared etymol-
ogy (Fung and Yee, 1998; Rapp, 1999; Schafer
and Yarowsky, 2002; Koehn and Knight, 2002;
Haghighi et al., 2008; Irvine and Callison-Burch,
2013; Xia et al., 2016; Artetxe et al., 2017).

For example, Koehn and Knight (2002)
matched English with German. Multiple heuris-
tics were suggested based on hand crafted rules,

including similarity in spelling and word fre-
quency. A weighted linear combination is em-
ployed to combine the heuristics and the matching
words are identified in a greedy manner. Haghighi
et al. (2008) modeled the problem of matching
words across independent corpora as a genera-
tive model, in which cross-lingual links are rep-
resented by latent variables, and employed an iter-
ative EM method.

Another example that employs iterations was
presented by Artetxe et al. (2017). Similarly to our
method, this method relies on word vector embed-
dings, in their case the word2vec method (Mikolov
et al., 2013a). Unlike our method, their method is
initialized using seed matches.

Our core method incorporates a circularity term,
which is also used in (Xia et al., 2016) for the
task of NMT and later on in multiple contributions
in the field of image synthesis (Kim et al., 2017;
Zhu et al., 2017). This term is employed when
learning bidirectional transformations to encour-
ages samples from either domain to be mapped
back to exactly the same sample when translated
to the other domain and back. Since our transfor-
mations are linear, this is highly related to employ-
ing orthogonality as done in (Xing et al., 2015;
Smith et al., 2017; Conneau et al., 2017) for the
task of weakly or unsupervised word vector space
alignment. Conneau et al. (2017) also employ a
circularity term, but unlike our use of it as part of
the optimization’s energy term, there it is used for
validating the solution and selecting hyperparam-
eters.

Very recently, Zhang et al. (2017a,b); Conneau
et al. (2017) have proposed completely unsuper-
vised solutions. All three solutions are based on
GANs. The methods differ in the details of the ad-
versarial training, in the way that model selection
is employed to select the best configuration and in
the way in which matching is done after the distri-
butions are aligned by the learned transformation.

Due to the min-max property of GANs, meth-
ods which rely on GANs are harder to interpret,
since, for example, the discriminator D could fo-
cus on a combination of local differences between
the distributions. The reliance on a discriminator
also means that complex weight dependent met-
rics are implicitly used, and that these metrics
evolve dynamically during training.

Our method does not employ GANs. Alter-
natives to GANs are also emerging in other do-



471

mains. For example, generative methods were
trained by iteratively fitting random (“noise”) vec-
tors by Bojanowski et al. (2017); In the recent im-
age translation work of Chen and Koltun (2017),
distinguishability between distribution of images
was measured using activations of pretrained net-
works, a practice that is referred to as the “percep-
tual loss” (Johnson et al., 2016).

3 Non-Adversarial Word Translation

We present an approach for unsupervised word
translation consisting of multiple parts: (i) Trans-
forming the word vectors into a space in which the
two languages are more closely aligned, (ii) Mini-
Batch Cycle iterative alignment. There is an op-
tional final stage of batch-based finetuning.

Let us define two languages X and Y , each con-
taining a set of NX and NY words represented by
the feature vectors x1..xNX and y1..yNY respec-
tively. Our objective is to find the correspondence
function f(n) such that for every xn, f(n) yields
the index of the Y word that corresponds to the
word xn. If a set of possible correspondences is
available for a given word, our objective is to pre-
dict one member of this set. In this unsupervised
setting, no training examples of f(n) are given.

3.1 Approximate Alignment with PCA

Each language consists of a set of words each pa-
rameterized by a word vector. A popular exam-
ple of a word embedding method is FastText (Bo-
janowski et al., 2016), which uses the internal
word co-occurrence statistics for each language.
These word vectors are typically not expected to
be aligned between languages and since the align-
ment method we employ is iterative, a good ini-
tialization is key.

Let us motivate our approach by a method com-
monly used in 3D point cloud matching. Let A be
a set of 3D points and TA be the same set of points
with a rotated coordinate system. Assuming non-
isotropic distributions of points, transforming each
set of points to its principle axes of variations (us-
ing PCA) will align the two point clouds. As noted
by Daras et al. (2012), PCA-based alignment is
common in the literature of point cloud matching.

Word distributions are quite different from 3D
point clouds: They are much higher dimensional,
and it is not obvious a priori that different lan-
guages present different “views” of the same “ob-
ject” and share exactly the same axes of variation.

The success of previous results, e.g. (Conneau
et al., 2017), to align word vectors between lan-
guages using an orthonormal transformation does
give credence to this approach. Our method re-
lies on the assumption that many language pairs
share some principle axes of variation. The em-
pirical success of PCA initialization in this work
supports this assumption.

For each language [X , Y], we first select the N
most frequent word vectors. In our implementa-
tion, we use N = 5000 and employ FastText vec-
tors of dimension D = 300. We project the word
vectors, after centering, to the top p principle com-
ponents (we use p = 50).

3.2 Mini-Batch Cycle Iterative Closest Point

Although projecting to the top principle axes of
variation would align a rotated non-isotropic point
cloud, it does not do so in the general case. This
is due to languages having different word distribu-
tions and components of variation.

We therefore attempt to find a transformation T
that will align every word xi from language X to
a word yf(i) in language Y . The objective is there-
fore to minimize:

argmin
T

∑
i

min
f(i)
|yi − Txf(i)| (1)

Eq. 1 is difficult to optimize directly and var-
ious techniques have been proposed for its opti-
mization. One popular method used in 3D point
cloud alignment is Iterative Closest Point (ICP).
ICP solves Eq. 1 iteratively in two steps.

1. For each yj , find the nearest Txi. We denote
its index by fy(j) = i

2. Optimize for T in
∑

j ‖yj − Txfy(j)‖

In this work, we use a modified version of ICP
which we call Mini-Batch Cycle ICP (MBC-ICP).
MBC-ICP learns transformations Txy for X → Y
and Tyx for Y → X . We include cycle-constraints
ensuring that a word x transformed to the Y do-
main and back is unchanged (and similarly for ev-
ery Y → X → Y transformation). The strength
of the cycle constraints is parameterized by λ (we
have λ = 0.1). We compute the nearest neigh-
bor matches at the beginning of each epoch, and
then optimize transformations Tyx and Txy using
mini-batch SGD with mini-batch size 128. Mini-
batch rather than full-batch optimization greatly



472

increases the success of the method. Experimen-
tal comparisons can be seen in the results section.
Note we only compute the nearest neighbors at
the beginning of each epoch, rather than for each
mini-batch due to the computational expense.

Every iteration of the final MBC-ICP algorithm
therefore becomes:

1. For each yj , find the nearest Txyxi. We de-
note its index by fy(j)

2. For each xi, find the nearest Tyxyj . We de-
note its index by fx(i)

3. Optimize Txy and Tyx using mini-batch SGD
for a single epoch of {xi} and {yj} on:∑

j ‖yj − Txyxfy(j)‖+
∑

i ‖xi − Tyxyfx(i)‖
+ λ

∑
i ‖xi − TyxTxyxi‖ + λ

∑
j ‖yj −

TxyTyxyj‖

A good initialization is important for ICP-type
methods. We therefore begin with the projected
data in which the transformation is assumed to be
relatively small and initialize transformations Txy
and Tyx with the identity matrix. We denote this
step PCA-MBC-ICP.

Once PCA-MBC-ICP has generated the corre-
spondences functions fx(i) and fy(j), we run a
MBC-ICP on the original 300D word vectors (no
PCA). We denote this step: RAW-MBC-ICP. We
initialize the optimization using fx(i) and fy(j)
learned before, and proceed with MBC-ICP. At
the end of this stage, we recover transformations
T̄xy and T̄yx that transform the 300D word vec-
tors from X → Y and Y → X respectively.

Reciprocal pairs: After several iterations of
MBC-ICP, the estimated transformations become
quite reliable. We can therefore use this transfor-
mation to identify the pairs that are likely to be
correct matches. We use the reciprocity heuristic:
For every word y ∈ Y we find the nearest trans-
formed word from the set {Txyx|x ∈ X}. We also
find the nearest neighbors for the Y → X trans-
formation. If a pair of words is matched in both
X → Y and Y → X directions, the pair is de-
noted reciprocal. During RAW-MBC-ICP, we use
only reciprocal pairs, after the 50th epoch (this pa-
rameter is not sensitive).

In summary: we run PCA-MBC-ICP on the 5k
most frequent words after transformation to prin-
ciple components. The resulting correspondences
fx(i) and fy(j) are used to initialize a RAW-
MBC-ICP on the original 300D data (rather than

PCA), using reciprocal pairs. The output of the
method are transformation matrices Txy and Tyx.

3.3 Fine-tuning

MBC-ICP is able to achieve very competitive per-
formance without any further finetuning or use
of large corpora. GAN-based methods on the
other hand require iterative finetuning (Conneau
et al., 2017; Hoshen and Wolf, 2018) to achieve
competitive performance. To facilitate compari-
son with such methods, we also add a variant of
our method with identical finetuning to (Conneau
et al., 2017). As we show in the results section,
fine-tuning European languages typically results
in small improvements in accuracy (1-2%) for our
method, in comparison to 10-15% for the previ-
ous work. Following (Xing et al., 2015; Conneau
et al., 2017), fine-tuning is performed by running
the Procrustes method iteratively on the full vo-
cabulary of 200k words, initialized with the final
transformation matrix from MBC-ICP. The Pro-
crustes method uses SVD to find the optimal or-
thonormal matrix between X and Y given approx-
imate matches. The new transformation is used to
finetune the approximate matches. We run 5 iter-
ations of successive transformation and matching
estimation steps.

3.4 Matching Metrics

Although we optimize the nearest neighbor metric,
we found that in accordance with (Conneau et al.,
2017), neighborhood retrieval methods such as In-
verted Soft-Max (ISF) (Smith et al., 2017) and
Cross-domain Similarity Local Scaling (CSLS)
improve final retrieval performance. We there-
fore evaluate using CSLS. The similarity between
a word x ∈ X and a word y ∈ Y is computed as
2 cos(Txyx, y)−r(Txyx)−r(y), where r(.) is the
average cosine similarity between the word and its
10-NN in the other domain.

4 Multiple Stochastic Solutions

Our approach utilizes multiple stochastic solu-
tions, to provide a good initialization for the MBC-
ICP algorithm. There are two sources of stochas-
ticity in our system: (i) The randomized nature
of the PCA algorithm (it uses random matrices
(Liberty et al., 2007)) (ii) The order of the train-
ing samples (the mini-batches) when training the
transformations.



473

(a) (b) (c)

Figure 1: (a) Evolution of reconstruction loss as a function of epoch number for successful (Blue) and unsuccessful
runs (Red). (b) The final reconstruction loss distribution for En-Fr. (c) A similar histogram for En-Ar.

The main issue faced by unsupervised learn-
ing in the case of multiple solutions, is either (i)
choosing the best solution in case of a fixed par-
allel run budget, or (ii) finding a good stopping
criterion if attempting to minimize the number of
runs serially.

We use the reconstruction cost as an unsuper-
vised metrics for measuring convergence of MBC-
ICP. Specifically, we measure how closely every
x ∈ X and y ∈ Y is reconstructed by a trans-
formed word from the other domain.∑

j

‖yj −Txyxfy(j)‖+
∑
i

‖xi−Tyxyfx(i)‖ (2)

Although for isotropic distributions this has many
degenerate solutions, empirically we find that val-
ues that are significantly lower than the median al-
most always correspond to a good solution.

The optimization profile of MBC-ICP is pre-
dictable and easily lends itself for choosing effec-
tive termination criteria. The optimization profile
of a successfully converged and non-converging
runs are presented in Fig. 1(a). The reconstruction
loss clearly distinguish between the converged and
non-converging runs. Fig. 1(b,c) presents the dis-
tribution of final reconstruction costs for 500 dif-
ferent runs for En-Fr and En-Ar.

5 Experiments

We evaluated our method extensively to ensure
that it is indeed able to effectively and efficiently
perform unsupervised word translation. As a
strong baseline, we used the code and datasets
from the MUSE repository by Conneau et al.
(2017)1. Care was taken in order to make sure that
we report these results as fairly as possible: (1) the
results from the previous work were copied as is,

1https://github.com/facebookresearch/MUSE

except for En-It, where our runs indicated better
baseline results. (2) For languages not reported,
we ran the code with multiple options and report
the best results obtained. One crucial option for
GAN was whether to center the data or not. From
communication with the authors we learned that,
in nearly all non-European languages, centering
the data is crucial. For European languages, not
centering gave better results. For Arabic, center-
ing helps in one direction but is very detrimental
in the other. In all such cases, we report the best
baseline result per direction. (3) For the super-
vised baseline, we report both the results from the
original paper (in Tab. 1) and the results post Pro-
crustes finetuning, which are better (Tab. 2). (4)
Esperanto is not available in the MUSE repository
at this time. We asked the authors for the data and
will update the paper once available. Currently we
are able to say (without the supervision data) that
our method converges on En-Eo and Eo-En.

The evaluation concentrated on two aspects of
the translation: (i) Word Translation Accuracy
measured by the fraction of words translated to
a correct meaning in the target language, and (ii)
Runtime of the method.

We evaluated our method against the best meth-
ods from (Conneau et al., 2017). The supervised
baseline method learns an alignment from 5k su-
pervised matches using the Procrustes method.
The mapping is then refined using the Procrustes
method and CSLS matching on 200k unsupervised
word vectors in the source and target languages.
The unsupervised method proposed by (Conneau
et al., 2017), uses generative adversarial domain
mapping between the word vectors of the 50k most
common words in each language. The mapping is
then refined using the same procedure that is used
in the supervised baseline.

A comparison of the word translation accura-



474

cies before finetuning can be seen in Tab. 1. Our
method significantly outperforms the method of
(Conneau et al., 2017) on all of the evaluated Eu-
ropean language pairs. Additionally, for these lan-
guages, our method performs comparably to the
supervised baseline on all pairs except En-Ru for
which supervision seems particularly useful. The
same trends are apparent for simple nearest neigh-
bors and CSLS although CSLS always performs
better. For non-European languages, none of the
unsupervised methods succeeds on all languages.
We found that the GAN baseline fails on Farsi,
Hindu, Bengali, Vietnamese and one direction of
Japanese and Indonesian while our method does
not succeed on Chinese, Japanese and Vietnamese.
We conclude that the methods have complemen-
tary strengths, our method doing better on more
languages. On languages where both methods suc-
ceed, MBC-ICP tends to do much better.

We present a comparison between the meth-
ods after finetuning and using the CSLS metric in
Tab. 2. All methods underwent the same finetun-
ing procedure. We can see that our method still
outperforms the GAN method and is comparable
to the supervised baseline on European languages.
Another observation is that on most European lan-
guage pairs, finetuning only makes a small differ-
ence for our method (1-2%). An unaligned vocab-
ulary of 7.5k is sufficient to achieve most of the ac-
curacy. This is in contrast with the GAN, that ben-
efits greatly from finetuning on 200k words. Non-
European language and English pairs are typically
more challenging, finetuning helps much more for
all unsupervised methods.

It is interesting to examine the languages on
which each method could not converge. They typ-
ically fall into geographical and semantic clusters.
The GAN method failed on Arabic and Hebrew,
Hindu, Farsi and Bengali. Whereas our method
failed on Japanese and Chinese. We suspect that
different statistical properties favor each method.

We also compare the different methods in terms
of training time required by the method. We em-
phasize that our method is trivially parallelizable,
simply by splitting the random initializations be-
tween workers. The run time of each solution of
MBC-ICP is 47 seconds on CPU. The run time of
all solutions can therefore be as low as a single
run, at linear increase in compute resources. As
it runs on CPU, parallelization is not expensive.
The average number of runs required for conver-

Table 1: Comparison of word translation accuracy (%)
- without finetuning. Bold: best unsupervised method.

Pair Supervised Unsupervised

Baseline GAN Ours

nn csls nn csls nn csls

European Languages

En-Es 77.4 81.4 69.8 75.7 75.9 81.1
Es-En 77.3 82.9 71.3 79.7 76.0 82.1

En-Fr 74.9 81.1 70.4 77.8 74.8 81.5
Fr-En 76.1 82.4 61.9 71.2 75.0 81.3

En-De 68.4 73.5 63.1 70.1 66.9 73.7
De-En 67.7 72.4 59.6 66.4 67.1 72.7

En-Ru 47.0 51.7 29.1 37.2 36.8 44.4
Ru-En 58.2 63.7 41.5 48.1 48.4 55.6

En-It 75.7 77.3 54.3 65.1 71.1 77.0
It-En 73.9 76.9 55.0 64.0 70.4 76.6

Non-European Languages

En-Fa 25.7 33.1 * * 19.6 29.0
Fa-En 33.5 38.6 * * 28.3 28.3

En-Hi 23.8 33.3 * * 19.4 30.3
Hi-En 34.6 42.8 * * 30.5 38.9

En-Bn 10.3 15.8 * * 9.7 13.5
Bn-En 21.5 24.6 * * 7.1 14.5

En-Ar 31.3 36.5 18.9 23.5 26.9 33.3
Ar-En 45.0 49.5 28.6 31 39.8 45.5

En-He 10.3 15.8 17.9 22.7 31.3 38.9
He-En 21.5 24.6 37.3 39.1 43.4 50.8

En-Zh 40.6 42.7 12.7 16.0 * *
Zh-En 30.2 36.7 18.7 25.1 * *

En-Ja 2.4 1.7 * * * *
Ja-En 0.0 0.0 3.1 3.6 * *

En-Vi 25.0 41.3 * * * *
Vi-En 40.6 55.3 * * * *

En-Id 55.3 65.6 18.9 23.5 39.4 57.1
Id-En 58.3 65.0 * * 37.1 58.1

*Failed to converge

gence depends on the language pair (see below,
Fig. 2). We note that our method learns transla-
tions for both languages at the same time.

The current state-of-the-art baseline by (Con-
neau et al., 2017) requires around 3000 seconds
on the GPU. It is not obvious how to parallelize
such a method efficiently. It requires about 30
times longer to train than our method (with par-
allelization) and is not practical on a multi-CPU
platform. The optional refinement step requires
about 10 minutes. The performance increase of re-
finement for our method are typically quite modest
and can be be skipped at the cost of 1-2% in accu-
racy, the GAN however requires finetuning to ob-
tain competitive results. Another obvious advan-



475

Table 2: Word translation accuracy (%) - after finetun-
ing and using CSLS. Bold: best unsupervised methods.

Pair Supervised Unsupervised

Baseline GAN Ours

European Languages

En-Es 82.4 81.7 82.1
Es-En 83.9 83.3 84.1

En-Fr 82.3 82.3 82.3
Fr-En 83.2 82.1 82.9

En-De 75.3 74.0 74.7
De-En 72.7 72.2 73.0

En-Ru 50.7 44.0 47.5
Ru-En 63.5 59.1 61.8

En-It 78.1 76.9 77.9
It-En 78.1 76.7 77.5

Non-European Languages

En-Fa 32.6 * 34.6
Fa-En 40.2 * 41.5

En-Hi 34.5 * 34.6
Hi-En 44.8 * 44.5

En-Bn 16.6 * 14.7
Bn-En 24.1 * 21.9

En-Ar 34.5 35.3 35.1
Ar-En 49.7 49.7 50.6

En-He 41.1 41.6 40.5
He-En 54.9 52.6 52.9

En-Zh 42.7 32.5 *
Zh-En 36.7 31.4 *

En-Ja 1.7 * *
Ja-En 0.0 4.2 *

En-Vi 44.6 * *
Vi-En 56.9 * *

En-Id 68.0 67.8 68.0
Id-En 68.0 66.6 68.0

*Failed to converge

tage is that our method does not require a GPU.
Implementation: We used 100 iterations for the

PCA-MBC-ICP stage on 50 PCA word vectors.
This was run in parallel over 500 stochastic so-
lutions. We selected the solution with the small-
est unsupervised reconstitution criterion. This
solution was used to initialize RAW-MBC-PCA,
which we run for 100 iterations on the raw word
vectors. The latter 50 iterations of RAW-MBC-
ICP were carried out with only reciprocal pairs
contributing to the optimization. Results were typ-
ically not sensitive to hyper-parameter choice, al-
though longer optimization generally resulted in
better performance.

Ablation Analyses There are three important
steps for the convergence of the ICP step: (i)

Table 3: En-Es accuracy with and without PCA

Method En-Es Es-En

No PCA 0.0% 0.0%
With 300 PCs 0.0% 0.0%
With 50 PCs 82.2% 83.8%

Table 4: Fraction of converging runs per stochasticity.

Method En-Es En-Ar

No randomization 0.0% 0.0%
Randomized Ordering 0.0% 0.0%
Randomized PCA 9.8% 0.0%
Randomized Ordering + PCA 16.8% 1.2%

PCA, (ii) Dimensionality reduction, (iii) Multiple
stochastic solutions. In Tab. 3 we present the ab-
lation results on the En-Es pair with PCA and no
dimensionality reduction, with only the top 50 PCs
and without PCA at all (best run out of 500 cho-
sen using the unsupervised reconstruction loss).
We can observe that the convergence rate with-
out PCA and with PCA but without dimension-
ality reduction is much lower than with PCA, the
best run without PCA has not succeeded in obtain-
ing a good translation. This provides evidence that
both PCA and dimensionality reduction are essen-
tial for the success of the method.

We experimented with the different factors
of randomness between runs, to understand the
causes of diversity allowing for convergence in the
more challenging language pairs (such as En-Ar).
We performed the following four experiments: i.
Fixing PCA and Batch Ordering. ii. Fixing all
data batches to have the same ordering in all runs,
iii. Fix the PCA bases of all runs, iv. Let both PCA
and batch ordering vary between runs.

Tab. 4 compares the results on En-Es and En-
Ar for the experiments described above. It can
be seen that using both sources of stochasticity is
usually better. Although there is some probability
the PCA will result in aligned principle compo-
nents between the two languages, this usually does
not happen and therefore using stochastic PCA is
highly beneficial.

Convergence Statistics In Fig. 2 we present the
statistics for all language pairs with Procrustes-
ICP (P-ICP) vs MBC-ICP. In P-ICP, we first cal-
culate the matches for the vocabulary, and then
perform a batch estimate of the transformation us-
ing the P-ICP method (starting from PCA word



476

Es Fr De

Ru It Ar

Figure 2: Histograms of the Reconstruction metric across 500 ICP runs for MBC-ICP (Red) and P-ICP (Blue).
The comparison is shown for En-Es, En-Fr, En-De, En-Ru, En-It, En-Ar. On average MBC-ICP converges to much
better minima. We can observe that MBC-ICP has many more converging runs than P-ICP. In fact for En-It and
En-Ar, P-ICP does not converge even once in 500 runs.

vectors and Txy initialized at identity). The only
source of stochasticity in P-ICP is the PCA where
in MBC-ICP the order of mini-batches provides
further stochasticity. Adding random noise to the
mapping initialization was not found to help. Each
plot shows the histogram in log space for the num-
ber of runs that achieved unsupervised reconstruc-
tion loss within the range of the bin. The con-
verged runs with lower reconstruction values typ-
ically form a peak which is quite distinct from the
non-converged runs allowing for easy detection of
converged runs. The rate of convergence gener-
ally correlates with our intuition for distance be-
tween languages (En-Ar much lower than En-Fr),
although there are exceptions.

MBC-ICP converges much better than P-ICP:
For the language pairs with a wide convergence
range (En-Es, En-Fr, En-Ru) we can see that
MBC-ICP converged on many more runs than P-
ICP. For the languages with a narrow convergence
range (En-Ar, En-It), P-ICP was not able to con-
verge at all. We therefore conclude that the mini-
batch update and batch-ordering stochasticity in-
crease the convergence range and is important for
effective unsupervised matching.

6 Conclusions

We have presented an effective technique for un-
supervised word-to-word translation. Our method
is simple and non-adversarial. We showed empir-
ically that our method outperforms current state-
of-the-art GAN methods in terms of pre and post
finetuning word translation accuracy. Our method
runs on CPU and is much faster than current meth-
ods when using parallelization. This will enable
researchers from labs that do not possess graphi-
cal computing resources to participate in this ex-
citing field. The proposed method is interpretable,
i.e. every stage has an intuitive loss function with
an easy to understand objective.

It is interesting to consider the relative perfor-
mance between language pairs. Typically more
related languages yielded better performance than
more distant languages (but note that Indonesian
performed better than Russian when translated to
English). Even more interesting is contrasting the
better performance of our method on West and
South Asian languages, and GAN’s better perfor-
mance on Chinese.

Overall, our work highlights the potential bene-
fits of considering alternatives to adversarial meth-
ods in unsupervised learning.



477

References
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.

Learning bilingual word embeddings with (almost)
no bilingual data. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). volume 1, pages
451–462.

Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-
tors with subword information. arXiv preprint
arXiv:1607.04606 .

Piotr Bojanowski, Armand Joulin, David Lopez-Paz,
and Arthur Szlam. 2017. Optimizing the la-
tent space of generative networks. arXiv preprint
arXiv:1707.05776 .

Qifeng Chen and Vladlen Koltun. 2017. Photographic
image synthesis with cascaded refinement networks.
In ICCV .

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2017.
Word translation without parallel data. arXiv
preprint arXiv:1710.04087 .

Petros Daras, Apostolos Axenopoulos, and Georgios
Litos. 2012. Investigating the effects of multiple
factors towards more accurate 3-d object retrieval.
IEEE Transactions on Multimedia 14(2):374–388.

Pascale Fung. 1995. Compiling bilingual lexicon en-
tries from a non-parallel english-chinese corpus.
In Proceedings of the Third Workshop on Very
Large Corpora. Massachusetts Institute of Technol-
ogy Cambridge, pages 173–183.

Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 17th international
conference on Computational linguistics-Volume 1.
Association for Computational Linguistics, pages
414–420.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In Advances in neural information
processing systems. pages 2672–2680.

Aria Haghighi, Percy Liang, Taylor Berg-kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
HLT .

Yedid Hoshen and Lior Wolf. 2018. Identifying analo-
gies across domains. In International Conference on
Learning Representations.

Ann Irvine and Chris Callison-Burch. 2013. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In HLT-NAACL. pages 518–
523.

Justin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016.
Perceptual losses for real-time style transfer and
super-resolution. In European Conference on Com-
puter Vision. Springer, pages 694–711.

Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jungkwon
Lee, and Jiwon Kim. 2017. Learning to discover
cross-domain relations with generative adversarial
networks. arXiv preprint arXiv:1703.05192 .

Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 workshop on Unsuper-
vised lexical acquisition-Volume 9. Association for
Computational Linguistics, pages 9–16.

Edo Liberty, Franco Woolfe, Per-Gunnar Martinsson,
Vladimir Rokhlin, and Mark Tygert. 2007. Ran-
domized algorithms for the low-rank approximation
of matrices. PNAS .

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .

Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013b.
Exploiting similarities among languages for ma-
chine translation. arXiv preprint arXiv:1309.4168
.

Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics. Association for Computational Linguistics,
pages 320–322.

Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics. Association for Compu-
tational Linguistics, pages 519–526.

Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In proceedings of the 6th con-
ference on Natural language learning-Volume 20.
Association for Computational Linguistics, pages 1–
7.

Samuel L Smith, David HP Turban, Steven Hamblin,
and Nils Y Hammerla. 2017. Offline bilingual word
vectors, orthogonal transformations and the inverted
softmax. arXiv preprint arXiv:1702.03859 .

Yingce Xia, Di He, Tao Qin, Liwei Wang, Nenghai
Yu, Tie-Yan Liu, and Wei-Ying Ma. 2016. Dual
learning for machine translation. arXiv preprint
arXiv:1611.00179 .

Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015.
Normalized word embedding and orthogonal trans-
form for bilingual word translation. In HLT-NAACL.
pages 1006–1011.



478

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017a. Adversarial training for unsupervised
bilingual lexicon induction. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers). vol-
ume 1, pages 1959–1970.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017b. Earth mover’s distance minimization
for unsupervised bilingual lexicon induction. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing. pages
1934–1945.

Jun-Yan Zhu, Taesung Park, Phillip Isola, and
Alexei A. Efros. 2017. Unpaired image-to-image
translation using cycle-consistent adversarial net-
works. In The IEEE International Conference on
Computer Vision (ICCV).


