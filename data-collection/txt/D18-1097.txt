



















































Utilizing Character and Word Embeddings for Text Normalization with Sequence-to-Sequence Models


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 837–843
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

837

Utilizing Character and Word Embeddings for Text Normalization
with Sequence-to-Sequence Models

Daniel Watson, Nasser Zalmout and Nizar Habash
Computational Approaches to Modeling Language Lab

New York University Abu Dhabi
{daniel.watson,nasser.zalmout,nizar.habash}@nyu.edu

Abstract

Text normalization is an important enabling
technology for several NLP tasks. Recently,
neural-network-based approaches have out-
performed well-established models in this
task. However, in languages other than En-
glish, there has been little exploration in this
direction. Both the scarcity of annotated data
and the complexity of the language increase
the difficulty of the problem. To address these
challenges, we use a sequence-to-sequence
model with character-based attention, which
in addition to its self-learned character embed-
dings, uses word embeddings pre-trained with
an approach that also models subword infor-
mation. This provides the neural model with
access to more linguistic information espe-
cially suitable for text normalization, without
large parallel corpora. We show that providing
the model with word-level features bridges the
gap for the neural network approach to achieve
a state-of-the-art F1 score on a standard Ara-
bic language correction shared task dataset.

1 Introduction

Text normalization systems have many potential
applications – from assisting native speakers and
language learners with their writing, to supporting
NLP applications with sparsity reduction by clean-
ing large textual corpora. This can help improve
benchmarks across many NLP tasks.

In recent years, neural encoder-decoder models
have shown promising results in language tasks
like translation, part-of-speech tagging, and text
normalization, especially with the use of an atten-
tion mechanism. In text normalization, however,
previous state-of-the-art results rely on developing
many other pipelines on top of the neural model.
Furthermore, such neural approaches have barely
been explored for this task in Arabic, where pre-
vious state-of-the-art systems rely on combining
various statistical and rule-based approaches.

We experiment with both character embeddings
and pre-trained word embeddings, using several
embedding models, and we achieve a state-of-the-
art F1 score on an Arabic spelling correction task.

2 Related Work

The encoder-decoder neural architecture
(Sutskever et al., 2014; Cho et al., 2014) has
shown promising results in text normalization
tasks, particularly in character-level models (Xie
et al., 2016; Ikeda et al., 2016). More recently,
augmenting this neural architecture with the
attention mechanism (Bahdanau et al., 2014;
Luong et al., 2015) has dramatically increased
the quality of results across most NLP tasks.
However, in text normalization, state-of-the-art
results involving attention (e.g., Xie et al. 2016)
also rely on several other models during inference,
such as language models and classifiers to filter
suggested edits. Neural architectures at the word
level inherently rely on multiple models to align
and separately handle out-of-vocabulary (OOV)
words (Yuan and Briscoe, 2016).

In the context of Arabic, we are only aware of
one attempt to use a neural model for end-to-end
text normalization (Ahmadi, 2017), but it fails to
beat all baselines reported later in this paper. Ara-
bic diacritization, which can be considered forms
of text normalization, has received a number of
neural efforts (Belinkov and Glass, 2015; Abandah
et al., 2015). However, state-of-the-art approaches
for end-to-end text normalization rely on several
additional models and rule-based approaches as
hybrid models (Pasha et al., 2014; Rozovskaya
et al., 2014; Nawar, 2015; Zalmout and Habash,
2017), which introduce direct human knowledge
into the system, but are limited to correcting spe-
cific mistakes and rely on expert knowledge to be
developed.



838

3 Approach

Many common mistakes addressed by text nor-
malization occur fundamentally at the character
level. Moreover, the input data tends to be too
noisy for a word-level neural model to be an end-
to-end solution due to the high number of OOV
words. In Arabic, particularly, mistakes may range
from simple orthographic errors (e.g., positioning
of Hamzas) and keyboard errors to dialectal code
switching and spelling variations, making the task
more challenging than a generic language correc-
tion task. We opt for a character-level neural ap-
proach to capture these highly diverse mistakes.
While this method is less parallelizable due to the
long sequence lengths, it is still more efficient due
to the small vocabulary size, making inference and
beam search computationally feasible.

3.1 Neural Network Architecture

Given an input sentence x and its corrected ver-
sion y, the objective is to model P (y|x). The
vocabulary can consist of any number of unique
tokens, as long as the following are included: a
padding token to make input batches have equal
length, the two canonical start-of-sentence and
end-of-sentence tokens of the encoder-decoder ar-
chitecture, and an OOV token to replace any char-
acter outside the training data during inference.
Each character xi in the source sentence x is
mapped to the corresponding dce-dimensional row
vector ci of a learnable dvoc× dce embedding ma-
trix, initialized with a random uniform distribution
with mean 0 and variance 1. For the encoder, we
learn d-dimensional representations for the sen-
tence with two gated recurrent unit (GRU) lay-
ers (Cho et al., 2014), making only the first layer
bidirectional following Wu et al. (2016). Like
long short-term memory (Hochreiter and Schmid-
huber, 1997), GRU layers are well-known to im-
prove the performance of recurrent neural net-
works (RNN), but are slightly more computation-
ally efficient than the former.

For the decoder, we use two GRU layers along
with the attention mechanism proposed by Luong
et al. (2015) over the encoder outputs hi. The
initial states for the decoder layers are learned
with a fully-connected tanh layer in a similar
fashion to Cho et al. (2014), but we do so from
the first encoder output. During training, we use
scheduled sampling (Bengio et al., 2015) and feed
the dce-dimensional character embeddings at ev-

⊕

h1 h2 h3

f1 
b1

f2 
b2

f3 
b3

rtrt-1

hT

fT 
bT

…

……

……

stst-1 ……

Figure 1: Illustration of the encoder and decoder recur-
rent layers.

ery time step, but using a constant sampling prob-
ability. While tuning scheduled sampling, we
found that introducing a sampling probability pro-
vided better results than relying on the ground
truth, i.e., teacher forcing (Williams and Zipser,
1989). However, introducing a schedule did not
yield any improvement as opposed to keeping the
sampling probability constant and unnecessarily
complicates hyperparameter search. For both the
encoder and decoder RNNs, we also use dropout
(Srivastava et al., 2014) on the non-recurrent con-
nections of both the encoder and decoder layers
during training.

The decoder outputs are fed to a final softmax
layer that reshapes the vectors to dimension dvoc
to yield an output sequence y. The loss function
is the canonical cross-entropy loss per time step
averaged over the yi.

3.2 Word Embeddings

To address the challenge posed by the small
amount of training data, we propose adding pre-
trained word-level information to each charac-
ter embedding. To learn these word representa-
tions, we use FastText (Bojanowski et al., 2016),
which extends Word2Vec (Mikolov et al., 2013)
by adding subword information to the word vec-
tor. This is very suitable for this task, not only be-
cause many mistakes occur at the character level,



839

I r

cI c_ cr

wI w_ wrun

u n

cu cn

wrun wrun

Figure 2: Illustration showing how the character em-
beddings are enriched with word-level features.

but also because FastText handles almost all OOVs
by omitting the Word2Vec representation and sim-
ply using the subword-based representation. It is
possible, yet extremely rare that FastText cannot
handle a word– this can occur if the word contains
an OOV n-gram or character that did not appear in
the data used to train the embeddings. It should
also be noted that these features are only fed to the
encoder layer; the decoder layer only receives dce-
dimensional character embeddings as inputs, and
the softmax layer has a dvoc-dimensional output.

Each character embedding ci is replaced by
the concatenation [ci;wj] before being fed to the
encoder-decoder model, where wj is the dwe-
dimensional word embedding for the word in
which ci appears in. This effectively handles al-
most all cases except white spaces, in which we
just always append a dwe-dimensional vector w_
initialized with a random uniform distribution of
mean 0 and variance 1. For OOVs, we just append
the whitespace embedding w_ to the word’s char-
acters.

3.3 Inference

During inference, the decoder layer uses a beam
search, keeping a fixed number (i.e., beam width)
of prediction candidates with the highest log-
likelihood at each step. Whenever an "end-of-
sentence" token is produced in a beam, the decoder
stops predicting further tokens for it. We then
pick the individual beam with the highest overall
log-likelihood as our prediction. As a final step,
we reduce text sequences that are repeated six or
more times to a threshold of 5 repetitions (e.g.,
"abababababab"→ "ababababab"). This
helps address rare cases where the decoder mis-
behaves and produces non-stop repetitions of text,

Baseline P R F1
MLE 77.08 41.56 54.00
MADAMIRA 77.47 32.10 45.39
MLE then MADAMIRA 64.38 38.42 48.12
MADAMIRA then MLE 73.55 44.61 55.54

Table 1: Baselines scores on the QALB 2014 shared
task development set.

and also helps avoid extreme running times for
the NUS MaxMatch scorer (Dahlmeier and Ng,
2012), which we use for evaluation and compar-
ison purposes.

4 Evaluation

4.1 Data

We tested the proposed approach on the QALB
dataset, a corpus for Arabic language correction
and subject of two shared tasks (Zaghouani et al.,
2014; Mohit et al., 2014; Rozovskaya et al., 2015).
Following the guidelines of both shared tasks, we
only used the training data of the QALB 2014
shared task corpus (19,411 sentences). Similarly,
the validation dataset used is only that of the
QALB 2014 shared task, consisting of 1,017 sen-
tences. We use two blind tests, one from each year.
During training, we only kept sentences of up to
400 characters in length, resulting in the loss of
172 sentences.

4.2 Metric

Like in the QALB shared tasks, we use the Max-
Match scorer to compute the optimal word-level
edits that map each source sentence to its respec-
tive corrected sentence. We report the F1 score
of these edits against those provided in the gold
data by the same tool. We compare against the
best reported system in the QALB 2014 shared
task test set (CLMB) (Rozovskaya et al., 2014),
as well as the best in the QALB 2014 shared task
development and the QALB 2015 shared task test
sets (CUFE) (Nawar, 2015).

4.3 Baselines

We consider two different baselines. The first is
the output from MADAMIRA (Pasha et al., 2014),
a tool for morphological analysis and disambigua-
tion of Arabic. The second is using maximum
likelihood estimation (MLE) based on the counts
of the MaxMatch gold edits from the training data;
that is, each word or phrase gets either replaced



840

Model P R F1
Wide embeds 80.80 59.80 68.73

+ preprocessing 79.63 58.81 67.57
Narrow embeds 80.00 62.46 70.15

+ preprocessing 80.25 57.80 67.20
Concat embeds 80.74 61.10 69.56

+ preprocessing 79.81 58.28 67.37
CUFE (Nawar, 2015) – – 68.72

Table 2: System scores on the QALB 2014 shared task
development dataset for the different FastText embed-
dings.

or kept as is, depending on the most common
action in the training data. We found that, un-
like Eskander et al. (2013) suggested, first using
MADAMIRA and then MLE yields better results
than composing these in the reverse order. The re-
sults are presented in Table 1.

4.4 Model Settings

In all experiments, we used batch and character
embedding sizes of b = dce = 128, hidden layer
size of d = 256, dropout probability of 0.1, de-
coder sampling probability of 0.35, and gradient
clipping with a maximum norm of 10. When run-
ning all the trained models during inference, we
used a beam width of 5. We used the Adam opti-
mizer (Kingma and Ba, 2014) with a learning rate
of 0.0005, �=1·10−8, β1=0.9, and β2=0.999, and
trained the model for 30 epochs. We report three
different setups with FastText word embeddings:
narrow, wide, and the concatenation of both. For
each of these, we report results on two separately
trained models: one without preprocessing, and
one with MADAMIRA and then MLE preprocess-
ing to the inputs. We also report an ablation study
where we choose the best of these six trained mod-
els and compare against two separately trained
models with identical setups, but using Word2Vec
and no word-level features, respectively.

All the word embeddings used are of dimen-
sion dwe=300, and were trained with a single
epoch over the Arabic Gigaword corpus (Parker
et al., 2011). In the experiments including prepro-
cessing, the respective word vectors were obtained
from Gigaword preprocessed with MADAMIRA.
The narrow and wide word embeddings were
trained with context windows of sizes 2 and 5, re-
spectively. All other hyperparameters were kept to
the default FastText values, except the minimum

Model P R F1
No word embeds 81.55 56.13 66.49
Word2Vec 82.16 51.53 63.33
FastText 80.00 62.46 70.15

Table 3: Ablation tests on the QALB 2014 shared task
development dataset. All settings used no preprocess-
ing and narrow word embeddings.

n-gram size, which was reduced from 3 to 2 to
compensate for single-character prefixes and suf-
fixes that appear in Arabic when omitting the short
vowels (Erdmann et al., 2018).

4.5 Results

Development set results are shown in Tables 2
and 3, test set results in Table 4. In all mod-
els, training without preprocessing consistently
yielded better results than their analogues with the
inputs pre-fed to MADAMIRA and then MLE. All
the FastText embeddings setups with no prepro-
cessing outperformed the previous state-of-the-art
results in the development dataset. We hypothe-
size that this is occurs because the model has ac-
cess to more examples of errors to normalize dur-
ing training, thereby increasing performance. The
best performing model was that with the narrow
word embeddings; consistent with the results of
Zalmout et al. (2018) showing the superior perfor-
mance of narrow word embeddings over both wide
embeddings and the concatenation of both. This
is justified by Goldberg (2015) and Trask et al.
(2015), who illustrate that wider word embeddings
tend to capture more semantic information, while
narrower word embeddings model more syntactic
phenomena.

In our ablation study, we compared the perfor-
mance of the narrow FastText embeddings against
narrow Word2Vec embeddings trained over the
same Arabic Gigaword corpus with the same hy-
perparameters, as well as to no word-level embed-
dings at all. The results, displayed in Table 3,
show that using only Word2Vec slightly increases
precision but significantly hurts recall. This high-
lights the effectiveness of using FastText for text
normalization, as well as the need to handle OOVs
in a noisy context for word-level representations to
help in this particular problem. Despite that hav-
ing OOV cases can help the model by indicating
that a word is likely erroneous, this does not pro-
vide linguistic information the way FastText does.
The narrow FastText embeddings with no prepro-



841

(2014) (2015)
Model F1 F1
MADAMIRA then MLE 55.56 60.98
CLMB (Rozovskaya et al., 2014) 67.91 –
CUFE (Nawar, 2015) 66.68 72.87
Narrow embeds 70.39 73.19

Table 4: System score on the QALB 2014 and QALB
2015 shared task test datasets.

cessing setup achieved state-of-the-art results in
all three datasets, beating all systems in both the
2014 and 2015 QALB shared tasks in F1 score.

4.6 Error Analysis

We conducted a detailed error analysis of 101 sen-
tences from the development set (6,370 words).
The sample contained 1,594 erroneous words
(25%). The errors were manually classified in a
number of categories, which are presented in Ta-
ble 5. The Table indicates the percentage of the
error type in the whole set of errors as well as the
error-specific F1 and an example. Some common
problems, Hamza (glottal stop) and Ta Marbuta
(feminine ending), are well handled in our best
system. This is due to their commonality in the
training data. Other types are less common –
dialectal constructions, consonantal switches and
Mood/Case. Punctuation is very common, how-
ever it is also very idiosyncratic. We also note
the presence of a small percentage (0.5%) of gold
errors. For more information on Arabic lan-
guage orthography issues from a computational
perspective, see (Buckwalter, 2007; Habash, 2010;
Habash et al., 2012).

5 Conclusion and Future Work

We propose a novel approach to text normal-
ization by enhancing character embeddings with
word-level features that model subword informa-
tion and model syntactic phenomena. This signifi-
cantly improves the neural model’s recall, allow-
ing the correction of more complex and diverse
errors. Our approach achieves state-of-the-art re-
sults in the QALB dataset, despite it being small
and seemingly unsuited for a neural model. Mo-
roever, our neural model is sophisticated enough
to not benefit from preprocessing techniques that
reduce the number of errors in the data. Our ap-
proach is general enough to be implemented for
any other text normalization task and does not rely

Gold% Error Type F1 Example
4.8 Ta Marbuta 95.4 éK. A

�
J»→ �éK. A

�
J»

29.8 Hamza 92.8 YJ
K
A
�
K→ YJ
K



A
�
K

10.5 Space 87.5 I. �. AÓ→ I. �.  AÓ
0.8 Alif Maqsura 83.3 ú �æË@→ ú




�
æË@

0.7 Repeated Letter 81.8 È@@ @ @ Ag. QË @→ ÈAg. QË @
0.6 Wa of Plurality 66.7 ñËA�¯→ @ñËA�¯
39.3 Punctuation 56.4 NIL→ .
2.2 Multiple Types 43.1 éÓAJ


�
®Ë


@→ �éÓAJ


�
®Ë @

1.7 Consonant Switch 41.0 m�
�
→ 	m��

1.6 Other Types 38.3 É�®�JË @→ É�J�®Ë@
2.3 Mood & Case 33.3 	àñK
QåÓ→

	á�
K
QåÓ

2.8 Dialect 32.8 I.
�
JºJ
ë→ I.

�
JºJ


1.3 Deleted Letter n/a Qå�J�
→ Qå
�
J
	
J�


1.1 Grammar n/a 	PðAj.
�
JK
→

	
àð 	PðAj.

�
JK


0.5 Gold Error n/a ú �æÊË @→ ú �æË@

Table 5: Error analysis on a sample from the QALB
2014 shared task development set, ordered by F1 score.

on domain-specific knowledge to develop.
Future directions include expanding the num-

ber of training pairs via synthetic data gener-
ation, where generative models can potentially
add human-like errors to a large, unannotated
corpus. Different sequence-to-sequence architec-
tures, such as the Transformer module (Vaswani
et al., 2017), could also be explored and re-
searched more exhaustively. The word-level fea-
tures provided by FastText could also be replaced
by separately trained neural approaches that gen-
erate word embeddings from a word’s characters
(e.g., ELMo embeddings, Peters et al. 2018), and
could also be fine-tuned towards specific applica-
tions. Another interesting direction includes hy-
brid word-character architectures, where the en-
coder receives word-level features, while the de-
coder operates at the character level. We are also
interested in applying our approach to other lan-
guages and dialects.

Acknowledgment

The second author was supported by the New York
University Abu Dhabi Global PhD Student Fel-
lowship program. The support and resources from
the High Performance Computing Center at New
York University Abu Dhabi are also gratefully ac-
knowledged.



842

References

Gheith A. Abandah, Alex Graves, Balkees Al-Shagoor,
Alaa Arabiyat, Fuad Jamour, and Majid Al-Taee.
2015. Automatic diacritization of Arabic text us-
ing recurrent neural networks. International Journal
on Document Analysis and Recognition (IJDAR),
18(2):183–197.

Sina Ahmadi. 2017. Attention-based encoder-decoder
networks for spelling and grammatical error correc-
tion. Master’s thesis, Paris Descartes University, 9.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.

Yonatan Belinkov and James Glass. 2015. Arabic di-
acritization with recurrent neural networks. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 2281–
2285.

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015. Scheduled sampling for se-
quence prediction with recurrent neural networks.
CoRR, abs/1506.03099.

Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-
tors with subword information. arXiv preprint
arXiv:1607.04606.

Tim Buckwalter. 2007. Issues in Arabic Morphological
Analysis. In A. van den Bosch and A. Soudi, editors,
Arabic Computational Morphology: Knowledge-
based and Empirical Methods. Springer.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734, Doha, Qatar. Association for Computational
Linguistics.

Daniel Dahlmeier and Hwee Tou Ng. 2012. A beam-
search decoder for grammatical error correction. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
568–578. Association for Computational Linguis-
tics.

Alexander Erdmann, Nasser Zalmout, and Nizar
Habash. 2018. Addressing Noise in Multidialectal
Word Embeddings. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics.

Ramy Eskander, Nizar Habash, Owen Rambow, and
Nadi Tomeh. 2013. Processing Spontaneous Or-
thography. In Proceedings of the 2013 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT), Atlanta, GA.

Yoav Goldberg. 2015. A primer on neural network
models for natural language processing. CoRR,
abs/1510.00726.

Nizar Habash, Mona Diab, and Owen Rambow. 2012.
Conventional Orthography for Dialectal Arabic. In
Proceedings of the Eighth International Conference
on Language Resources and Evaluation (LREC-
2012), pages 711–718, Istanbul, Turkey.

Nizar Y Habash. 2010. Introduction to Arabic natural
language processing, volume 3. Morgan & Clay-
pool Publishers.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780.

Taishi Ikeda, Hiroyuki Shindo, and Yuji Matsumoto.
2016. Japanese text normalization with encoder-
decoder model. In Proceedings of the 2nd Workshop
on Noisy User-generated Text (WNUT), pages 129–
137.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Minh-Thang Luong, Hieu Pham, and Christo-
pher D. Manning. 2015. Effective approaches to
attention-based neural machine translation. CoRR,
abs/1508.04025.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wa-
jdi Zaghouani, and Ossama Obeid. 2014. The
first QALB shared task on automatic text correc-
tion for Arabic. In Proceedings of the EMNLP 2014
Workshop on Arabic Natural Language Processing
(ANLP), pages 39–47.

Michael Nawar. 2015. CUFE@QALB-2015 shared
task: Arabic error correction system. In Proceed-
ings of the Second Workshop on Arabic Natural Lan-
guage Processing, pages 133–137, Beijing, China.

Robert Parker, David Graff, Ke Chen, Junbo Kong, and
Kazuaki Maeda. 2011. Arabic Gigaword Fifth Edi-
tion. LDC catalog number No. LDC2011T11, ISBN
1-58563-595-2.

Arfath Pasha, Mohamed Al-Badrashiny, Ahmed
El Kholy, Ramy Eskander, Mona Diab, Nizar
Habash, Manoj Pooleery, Owen Rambow, and Ryan
Roth. 2014. MADAMIRA: A fast, comprehensive
tool for morphological analysis and disambiguation
of Arabic.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Alla Rozovskaya, Houda Bouamor, Nizar Habash, Wa-
jdi Zaghouani, Ossama Obeid, and Behrang Mohit.
2015. The second QALB shared task on automatic
text correction for arabic. In Proceedings of the



843

Second Workshop on Arabic Natural Language Pro-
cessing, pages 26–35.

Alla Rozovskaya, Nizar Habash, Ramy Eskander,
Noura Farra, and Wael Salloum. 2014. The
Columbia system in the QALB-2014 shared task on
Arabic error correction. In ANLP@EMNLP.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. CoRR, abs/1409.3215.

Andrew Trask, David Gilmore, and Matthew Russell.
2015. Modeling order in neural word embeddings at
scale. arXiv preprint arXiv:1506.02338.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Ronald J Williams and David Zipser. 1989. A learn-
ing algorithm for continually running fully recurrent
neural networks. Neural computation, 1(2):270–
280.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin
Johnson, Xiaobing Liu, ÅĄukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant
Patil, Wei Wang, Cliff Young, Jason Smith, Jason
Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2016. Google’s
neural machine translation system: Bridging the gap
between human and machine translation. CoRR,
abs/1609.08144.

Ziang Xie, Anand Avati, Naveen Arivazhagan, Daniel
Jurafsky, and Andrew Y. Ng. 2016. Neural language
correction with character-based attention. CoRR,
abs/1603.09727.

Zheng Yuan and Ted Briscoe. 2016. Grammatical error
correction using neural machine translation. In Pro-
ceedings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
380–386.

Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-
sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura
Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014.
Large scale Arabic error annotation: Guidelines and
framework. In International Conference on Lan-
guage Resources and Evaluation (LREC 2014).

Nasser Zalmout, Alexander Erdmann, and Nizar
Habash. 2018. Noise-Robust Morphological Dis-
ambiguation for Dialectal Arabic. In Proceedings of

the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT).

Nasser Zalmout and Nizar Habash. 2017. Don’t throw
those morphological analyzers away just yet: Neural
morphological disambiguation for Arabic. In Pro-
ceedings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing, pages 704–
713, Copenhagen, Denmark. Association for Com-
putational Linguistics.


