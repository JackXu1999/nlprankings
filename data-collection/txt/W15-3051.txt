



















































Predicting Machine Translation Adequacy with Document Embeddings


Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 402–410,
Lisboa, Portugal, 17-18 September 2015. c©2015 Association for Computational Linguistics.

Predicting Machine Translation Adequacy
with Document Embeddings

Mihaela Vela
Saarland University

Saarbrücken, Germany
m.vela@mx.uni-saarland.de

Liling Tan
Saarland University

Saarbrücken, Germany
liling.tan@uni-saarland.de

Abstract

This paper describes USAAR’s submis-
sion to the the metrics shared task of the
Workshop on Statistical Machine Transla-
tion (WMT) in 2015. The goal of our sub-
mission is to take advantage of the seman-
tic overlap between hypothesis and ref-
erence translation for predicting MT out-
put adequacy using language independent
document embeddings. The approach pre-
sented here is learning a Bayesian Ridge
Regressor using document skip-gram em-
beddings in order to automatically evalu-
ate Machine Translation (MT) output by
predicting semantic adequacy scores. The
evaluation of our submission – measured
by the correlation with human judgements
– shows promising results on system-level
scores.

1 Introduction

Translation is becoming an utility in everyday life.
The increased availability of real-time machine
translation services relying on Statistical Machine
Translation (SMT) allows users who do not un-
derstand the language of the source text to quickly
gist the text and understand its general meaning.
For these users, accurate meaning of translated
words is more important than the fluency of the
translated sentence.

However, SMT suffers from poor lexical
choices. Fluent but inadequate translations are
commonly produced due to the strong bias to-
wards the language model component that prefers
consecutive words based on the data that the sys-
tem is trained on.

Current state of art MT evaluation metrics are
generally able to identify problems with grammat-
icality of the translation but less evidently accu-
racy of translated semantics, e.g. incorrect trans-
lation of ambiguous words or wrong assignment

of semantic roles. In the example below, the
ideal Machine Translation (MT) evaluation metric
should appropriately penalise poor lexical choice,
such as braked, and reward or at least allow lee-
way for semantically similar translations, such as
external trade.

Source (DE):
Auch der Auenhandel bremste die Konjunktur.

Phrase-based MT:
The foreign trade braked the economy.

Neural MT:
External trade also slowed the economy.

Reference (EN):
Foreign goods trade had slowed, too.

The German word bremste is commonly used
as braked in the context of driving, but the appro-
priate translation should have been slowed in the
example mentioned above. Although the phrase
external trade differs from foreign goods trade in
the reference sentence, it should be considered as
an acceptable translation.

We propose a semantically grounded, language
independent approach using Semantic Textual
Similarity (STS) to evaluate the adequacy of the
machine translation outputs with respect to their
reference translations.

The remainder of this paper is structured as fol-
lows. Section 2 gives an overview of the related
work in the field of MT evaluation. Section 3
presents the approach behind the USAAR submis-
sion to the metrics shared task. In Section 4 we
present the data and experiments for this submis-
sion. Section 5 covers the evaluation of our metric
by the WMT2015 metrics task organisers and in
Section 6 we conclude on our WMT2015 metrics
task submission.

402



2 Related Work

Researchers in the field of MT evaluation have
proposed a large variety of methods for assess-
ing the quality of automatically produced trans-
lations. Approaches range from fully automatic
quality scoring to efforts aimed at the development
of ”human” evaluation scores that try to exploit the
(often tacit) linguistic knowledge of human evalu-
ators.

2.1 Automatic Evaluation of MT
MT output is usually evaluated by automatic
language-independent metrics that can be applied
to MT output, independent of the target language.
Automatic metrics typically compute the close-
ness (adequacy) of a hypothesis to a reference
translation and differ from each other by how this
closeness is measured. The most popular MT eval-
uation metrics are IBM BLEU (Papineni et al.,
2002) and NIST (Doddington, 2002), used not
only for tuning MT systems, but also as evalua-
tion metrics for translation shared tasks, such as
the Workshop on Statistical Machine Translation
(WMT).

IBM BLEU uses n-gram precision by match-
ing machine translation output against one or more
reference translations. It accounts for adequacy
and fluency by calculating word precision, i.e. the
n-gram precision.

BLEU = BP · exp
(

N∑
n=1

wnlogpn

)
(1)

BP =

{
1 if c > r
−e(1−r/c) if c ≤ r (2)

In order to deal with the over generation of com-
mon words, precision counts are clipped, mean-
ing that a reference word is exhausted after it is
matched against the same word in the hypothesis.
This is then called the modified n-gram precision.
For BLEU, the modified n-gram precision is calcu-
lated with N=4, the results being combined by us-
ing the geometric mean. Instead of recall, BLEU
computes the Brevity Penalty (BP) (see formula
in 2), thus penalising candidate translations which
are shorter than the reference translations.

The NIST metric is derived from IBM BLEU.
The NIST score is the arithmetic mean of modified
n-gram precision for N=5 scaled by the BP. Addi-
tionally, NIST also considers the information gain

of each n-gram, giving more weight to more infor-
mative (less frequent) n-grams and less weight to
less informative (more frequent) n-grams.

Another often used machine translation evalu-
ation metric is METEOR (Denkowski and Lavie,
2014). Unlike IBM BLEU and NIST, METEOR
evaluates a candidate translation by calculating
precision and recall on the unigram level and com-
bining them into a parametrised harmonic mean.
The result from the harmonic mean is then scaled
by a fragmentation penalty which penalizes gaps
and differences in word order. METEOR is de-
scribed in detail in Section 3.1.

Besides these evaluation metrics, several other
metrics are used for the evaluation of MT out-
put. Some of these are the WER (word error-rate)
metric based on the Levensthein distance (Leven-
shtein, 1966), the position-independent error rate
metric PER (Tillmann et al., 1997) and the trans-
lation edit rate metric TER (Snover et al., 2006)
with its newer version TERp (Snover et al., 2009).

The semantics of both hypotheses and reference
translations is considered by MEANT (Lo et al.,
2012). MEANT, based on HMEANT (Lo and Wu,
2011a; Lo and Wu, 2011b; Lo and Wu, 2011c), is
a fully automatic semantic MT evaluation metric,
measuring semantic fidelity by determining the
degree of parallelism of verb frames and seman-
tic roles between hypothesis and reference trans-
lations. Some approaches aim at combining sev-
eral linguistic and semantic aspects. Gonzàlez et
al. (2014) as well as Comelles and Atserias (2014)
introduce their fully automatic approaches to ma-
chine translation evaluation using lexical, syntac-
tic and semantic information when comparing the
machine translation output with reference transla-
tions.

2.2 Human Evaluation of MT

Human MT evaluation approaches employ the
knowledge of human annotators to assess the qual-
ity of automatically produced translations along
the two axes of target language correctness and se-
mantic fidelity. The Linguistics Data Consortium
(LDC) introduced a MT evaluation task that elicits
quality judgement of MT output from human an-
notators using a numerical scale (Linguistics Data
Consortium, 2005). These judgements were split
into two categories: adequacy, the degree of mean-
ing preservation, and fluency, target language cor-
rectness.

403



Adequacy judgements require annotators to rate
the amount of meaning expressed in the reference
translation that is also present in the translation hy-
pothesis. Fluency judgements require annotators
to rate how well the translation hypothesis in the
target language is formed, disregarding the sen-
tence meaning. Although evaluators are asked to
assess the fluency and adequacy of a hypothesis
translation on a Likert scale separately, Callison-
Burch et al. (2007) reported high correlation be-
tween annotators’ adequacy and fluency scores.

MT output is also evaluated by measuring hu-
man post-editing time for productivity (Guerberof,
2009; Zampieri and Vela, 2014), or by asking eval-
uators to rank MT system outputs (by ordering
a set of translation hypotheses according to their
quality). Vela and van Genabith (2015) show that
this task is very easy to accomplish for evaluators,
since it does not imply specific skills, a homoge-
neous group being enough to perform this task.
This is also the method applied during the last
years WMTs, where humans are asked to rank ma-
chine translation output by using APPRAISE (Fe-
dermann, 2012), a software tool that integrates fa-
cilities for such a ranking task.

An indirect human evaluation method, that is
also employed for error analysis, are reading com-
prehension tests (e.g. Maney et al. (2012), Weiss
and Ahrenberg (2012)). Other evaluation metrics
try to measure the effort that is necessary for “re-
pairing” MT output, that is, for transforming it
into a linguistically correct and faithful transla-
tion. One such metric is HTER (Snover et al.,
2006), which uses human annotators to generate
targeted reference translations by means of post-
editing, the rationale being that by this the shortest
path between a hypothesis and its correct version
can be found.

2.3 Semantic Textual Similarity

Given two snippets of text, the Semantic Textual
Similarity (STS) task attempts to measure their se-
mantic equivalence on a scale of 1 to 5 (Agirre et
al., 2014). The STS task is organized annually dur-
ing the SemEval workshop and systems are evalu-
ated based on their Pearson correlation coefficient
with the human annotations.

The STS is similar to the task of determining
the adequacy of a translation hypothesis with re-
spect to a reference translation. The STS task is
usually treated as a regression task where systems

are trained using features such as:

(i) linguistics annotation overlaps between the
two text snippets, e.g. syntactic dependency,
lexical paraphrases, part of speech (Šarić et
al., 2012; Han et al., 2012; Pilehvar et al.,
2013)

(ii) machine translation metrics as features in
training a supervised regressor (Rios et al.,
2012; Barrón-Cedeño et al., 2013; Huang and
Chang, 2014; Tan et al., 2015b)

(iii) word/document embeddings similarity (Sul-
tan et al., 2015; Arora et al., 2015).

Linguistic annotations are restricted by the
availability of the annotation tools, that are often
language dependent. Machine translation evalua-
tion metrics generally provide a shallow compari-
son between hypotheses and reference translations
focusing on capturing the grammatical similari-
ties between the texts, whereas the use of docu-
ment embeddings focuses on capturing the seman-
tic similarity between texts. Word embeddings
dates back to the traditional Latent Semantic Anal-
ysis (LSA) vector spaces used for information re-
trieval (Landauer and Dutnais, 1997) to the current
trend of using neural nets for NLP/MT tasks (Bor-
des et al., 2011; Huang et al., 2012; Bordes et al.,
2012; Chen and Manning, 2014; Bowman et al.,
2015).

3 Our Approach

Although consensus exists that lexical-based met-
rics cannot cover the entire range of linguistic phe-
nomena (Vela et al., 2014a; Vela et al., 2014b),
the goal in the MT community remains to have
a language independent metric that takes into ac-
count for lexical, syntactic and semantic infor-
mation when mapping the MT output against the
reference translation. The questions that have to
be accounted for in such a language-independent
metric are:

(i) Is there a lexical overlap between reference
and hypothesis translation?

(ii) Is there a syntactic overlap between reference
and hypothesis translation?

(iii) Is there a semantic overlap between reference
and hypothesis translation?

404



In the ideal situation one would also take into
account lexical, syntactic and semantic informa-
tion from the source text. Specific information (on
lexical, syntactic, semantic level) from the source
text could help improving not only the translation
process, but also the evaluation.

As pointed out in Section 2, there are several
approaches which tend to cover the entire range
of linguistic phenomena in the evaluation process.
The approach presented in this paper is leaned on
the STS approach, mentioned in Section 2.3, aim-
ing to provide a language independent adequacy
score using document embedding similarity as op-
posed to the traditional synonyms and paraphrase
overlap approach used in METEOR. The match-
ing of synonyms in METEOR relies on Word-
Net (Miller, 1995), which is a limited resource,
making it impossible to use the synonymy mod-
ule from METEOR for other languages than En-
glish. The provided or self-extracted paraphrase
tables for METEOR are available only for lan-
guages for which big corpora are available, mak-
ing it difficult to provide paraphrases for under-
resourced languages. Since METEOR relies on
the WordNet synonymy and language dependent
paraphrase tables for its semantic component, our
goal is to substitute this components with a lan-
guage independent component.

Different from the STS task, the WMT metrics
task provides the ranks of the systems’ hypothe-
ses instead of absolute human evaluation scores of
the translation hypotheses. To generate the abso-
lute scores, we use the METEOR scores between
the translation hypotheses and the reference trans-
lations.

To induce the word embeddings, we
trained a skip-gram model phrasal word2vec
neural net (Mikolov et al., 2013) using
gensim (Řehůřek and Sojka, 2010). The
neural nets were trained to produce 400 dense
features for 100 epochs with a window size of 5
for all words from the WMT metrics task data.

v(doc) =
∑n

i v(wi)
n

(3)

doc = {w1, ..., wn}

To generate the document embeddings, v(doc),
we sum the word embeddings from the docu-
ment and normalised it by the number of words.
The setup for the skip-gram model and the docu-

ment vector is similar the techniques uses in STS
tasks (Sultan et al., 2015; Tan et al., 2015a).

sim(hyp, ref) = v(hyp) · v(ref) (4)
The document embedding similarity is achieved

by the dot product between the translation hypoth-
esis (hyp) and the reference translation (ref ). Ge-
ometrically, the dot product between the hypothe-
sis and the reference translation yields the cosine
similarity between two vectors. Alternatively, one
could also calculate the cosine similarity by sum-
ming the square of the word vector of the intersect-
ing word embeddings and normalise the document
by the root of the sum square for all words in the
documents (Tan, 2013)1.

Using the similarity scores between the hy-
pothesis and reference embeddings, we train a
Bayesian Ridge Regressor targeting the METEOR
scores as the desired output.

3.1 METEOR

METEOR (Metric for Evaluation of Translation
with Explicit ORdering) (Denkowski and Lavie,
2014) is an MT evaluation metric which tries to
consider both grammatical and semantic knowl-
edge. The metric is based on the alignment be-
tween a hypothesis translation and a reference
translation containing four modules. The num-
ber of modules to be used depends on the avail-
ability of resources for a specific language. The
first module generates the alignments based on the
surface forms of the words in the hypothesis and
reference translation. The next module performs
the alignment on word stems, followed by the
alignment of words listed as synonyms in Word-
Net (Miller, 1995). The last module is responsible
for the paraphrase matching between the hypoth-
esis and reference translation, based on the pro-
vided or the self-extracted paraphrase tables. For
the final score calculation all matches are gener-
alised to phrase/chunk matches with a start posi-
tion and phrase length in each sentence.

Different from other evaluation metrics, ME-
TEOR makes the distinction between content
words and function words in the hypothesis
(hc, hf ) and reference (rc, rf ) translation. This
distinction is made by a provided function words
list.

1An implementation of the alternative cosine can be fount
at http://tinyurl.com/pywsd-cosine. The origi-
nal implementation is reported in (Tan and Bond, 2013)

405



From the final alignment between hypothesis
and reference translation, precision (P ) and re-
call (R) is calculated by weighting content words
and function words differently. This is described
by Denkowski and Lavie (2014) as follows. For
each of the matchers (mi) count the number of
content and function words covered by matches
of this type in the hypothesis (mi(hc),mi(hr))
and reference (mi(rc),mi(rr)) translation. The
weighted precision (P ) and recall (R) is com-
puted by using the matcher weights wi...wn and
the function word weight γ as shown in 5 and 6.

P =
∑

iwi · (δ ·mi(hc) + (1− δ) ·mi(hc))
γ· | hc | +(1− γ)· | hf |

(5)

R =
∑

iwi · (δ ·mi(rc) + (1− δ) ·mi(rc))
γ· | rc | +(1− γ)· | rf |

(6)
The harmonic mean is calculated by the formula

in equation 7.

Fmean =
P ·R

α · P + (1− α) · P (7)

METEOR also accounts for word order differ-
ences and gaps by scaling Fmean by the fragmen-
tation penalty (Pen). The fragmentation penalty
(Pen) in equation 8 is computed by using the total
number of matched words (m) and the number of
chunks (ch).

Pen = γ ·
(
ch

m

)β
(8)

The final score is then:

Score = (1− Pen) · Fmean (9)
The parameters α, β, γ, δ and wi...wn are pa-

rameters that can be used for tuning METEOR for
a given task.

3.2 Cosine Similarity
Cosine similarity is a similarity measure that can
handle the fact that very similar documents (in our
case sentences) may have different lengths. The
cosine similarity of two documents is calculated
by deriving a vector (~V ) for each sentence or doc-
ument d, denoted as ~V (d)2. The set of documents

2The normalization of the terms in the vector is computed
by using using TF ∗ IDF

in a collection is viewed as a set of vectors in a
vector space, each term (meaning a word) having
its own axis. By this kind of representation the ini-
tial ordering of terms in the document is lost, since
cosine similarity does not incorporate context.

The cosine of two vectors can be derived by us-
ing the Euclidean dot product formula:

a · b =| a || b | cos θ (10)
Derived from the formula in (10) the similarity

between two documents d1 and d2 can be com-
puted by the cosine similarity of their vector rep-
resentations ~V (d1) and ~V (d2).

cos(θ) =
~V (d1) · ~V (d2)
| ~V (d1) || ~V (d2) |

(11)

The numerator in (11) represents the dot prod-
uct of the vectors ~V (d1) and ~V (d2) and is defined
as shown in equation (12).

~V (d1) · ~V (d2) =
n∑
i=1

~Vi(d1)× ~Vi(d2) (12)

The denominator corresponds to the product of
the Euclidean length of the vectors ~Vi(d1) and
~Vi(d1).

| ~V (d1) |=
√√√√ n∑

i=1

~Vi(d1) (13)

The vectors are length normalised by the formu-
las in (13) and (14).

| ~V (d2) |=
√√√√ n∑

i=1

~Vi(d2) (14)

3.3 ZWICKEL: A Regression-based Metric
Similar to the Semantic Textual Similarity (STS)
and MT Quality Estimation approaches (Scarton
et al., 2015), we treat the MT metric task as a re-
gression task with the aim of learning a Bayesian
Ridge function that maps the cosine similarity fea-
ture to the target METEOR score.

A Bayesian Regressor finds a maximum a pos-
teriori solution under a Gaussian prior N over the
parameters w with the precision of λ−1. The α
and λ parameters are treated as random variables
estimated from the data.

p(y|X,w, α) == N(y|X,w, α) (15)

406



The Bayesian Ridge estimates a probabilistic
regression model with a zero-mean prior for the
parameter w, given by a spherical Gaussian:

p(w|λ) = N(w|0, λ−1Ip) (16)
Without the caveats of mathematical argot, we

refer to the cosine similarities as X, and to the
METEOR scores as Y. We aim to learn a regres-
sor that outputs the paraphrase and synonym ME-
TEOR scores using the cosine similarities, without
the paraphrase/synonym tables. Essentially, this
leads to a language independent METEOR mea-
sure based on cosine similarity between transla-
tion and reference vectors.

3.4 COMET: A Combination of METEOR and
ZWICKEL

We noticed that the outputs of the basic ZWICKEL
score is conservative and does not allow an ex-
treme 0.0 or 1.0 score unlike the METEOR score.
Thus, we created a ”switch-like metric”, COMET,
that treat the METEOR scores as oracle when ME-
TEOR reports 0.0 or 1.0 scores, otherwise it falls
back to ZWICKEL.

4 Experiments

This year’s USAAR submission to the WMT met-
rics shared task concentrated on evaluating trans-
lations into German and into English, assigning a
score both at sentence and system level.

4.1 Training Data
For training our system we used the available data
from the previous WMT shared tasks by conflat-
ing them into a single data set3. The into German
set consisted of 359545 sentence pairs and the into
English set consisted of 1194017 sentence pairs.

4.2 Test Data
The test data for our evaluation metrics consist of
all system outputs from this year’s translation task
performed on the newstest2015 data set. Depend-
ing on the source language the data sets consist of
a different number of sentences. Into English we
evaluated MT systems having the following source
languages:

• Czech with 10 system submissions and 2655
translated sentences per system

3We have compiled the WMT08-15 metrics task data sets
into a single python-readable library that is easily accessible
at https://github.com/alvations/warmth.

• German with 13 system submissions and
2168 translated sentences per system

• Finnish with 14 system submissions and
1369 translated sentences per system

• Russian with 13 system submissions and
2817 translated sentences per system

Into German we evaluated 16 systems with 2168
translated sentences per system.

Based on the sentence scores we provided also
a system score for each language pair. The sys-
tem score was calculated by using different means
(median, arithmetic mean, arithmetic geometric
mean, harmonic mean and root squared mean) for
each proposed metric.

4.3 USAAR’s Submission to the WMT2015
Metrics Shared Task

In order to evaluate the efficacy of our method we
contributed with three systems to the metrics task:

• COSINE: the raw document embedding sim-
ilarity, i.e. sim(hyp, ref)

• ZWICKEL: the cosine-based metric outputs
from the regressor described above

• COMET: the combination of ZWICKEL out-
puts from the regressor and METEOR

5 Evaluation

All submissions to the metrics task were evalu-
ated4 at system level by computing their Pearson
correlation coefficient with human judgements.
For the evaluation of translations into English our
best submission is COMET, achieving on average
a correlation coefficient of 0.788±0.026. For the
evaluation of translations from English into Ger-
man, COMET is again our best submission with a
correlation coefficient of 0.448±0.40.

Table 5 shows the system-level Pearson cor-
relation coefficient for COSINE, ZWICKEL and
COMET5 for each language pair into English and
for the language pair English-German.

Spearman’s correlation coefficient was also
computed, but just the average over all language

4The numbers reported in this section are provided by the
organisers of the WMT2015 metrics shared task

5For the translations into English the system-level score
is the root mean square of the sentence-level scores. For the
translations from English into German the best system-level
scores are achieved by the arithmetic geometric mean of the
sentence-level scores.

407



Pearson Correlation Coefficient
Language pair COSINE ZWICKEL COMET
Finnish-English NaN -0.093±0.043 0.834±0.023
German-English 0.008±0.052 0.286±0.052 0.847±0.027
Czech-English 0.912±0.013 0.406±0.031 0.896±0.014
Russian-English NaN 0.264±0.052 0.603±0.041
English-German NaN -0.232±0.044 0.448±0.040

Table 1: Pearson correlation coefficient for COSINE, ZWICKEL and COMET.

Spearman’s Correlation Coefficient
Average COSINE ZWICKEL COMET
into English 0.122±0.079 0.066±0.087 0.665±0.069
into German 0.084±0.084 -0.235±0.069 0.588±0.072

Table 2: System-level Spearman’s correlation coefficient for COSINE, ZWICKEL and COMET.

pairs into English and into German. From the
results in Table 5 we notice that COMET was
the metric performing best for both translations
into English and German, achieving a coefficient
of 0.665±0.069 for translations into English and
0.588±0.072 for translations from German into
English.

6 Conclusion

This paper presents USAAR’s submission to the
WMT2015 metrics shared task. Our aim of our
submission was a language independent method
for predicting MT adequacy based on the semantic
similarity between hypothesis and reference trans-
lation by using document embeddings. We con-
tributed with three evaluation metrics, COMET, a
combination of a cosine-based metric and ME-
TEOR, being the one correlating best with the hu-
man evaluators.

Previous studies have shown that METEOR
systematically underestimate the quality of the
translations (Vela et al., 2014b). Future work on
our approach using document embeddings and co-
sine similarities could be used to also predict dif-
ferent scores (i.e. other than METEOR). Addi-
tionally, further experiments on document/word
embeddings would be beneficial to find the best-
fit solution for the cosine similarity calculation
between a machine translation and its reference
translation.

Acknowledgements

The research leading to these results has received
funding from the People Programme (Marie Curie

Actions) of the European Union’s Seventh Frame-
work Programme FP7/2007-2013/ under REA
grant agreement no. 317471.

References
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel

Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. SemEval-2014 Task 10: Multilingual
Semantic Textual Similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval 2014), Dublin, Ireland, August.

Piyush Arora, Chris Hokamp, Jennifer Foster, and
Gareth Jones. 2015. DCU: Using Distributional
Semantics and Domain Adaptation for the Seman-
tic Textual Similarity SemEval-2015 Task 2. In Pro-
ceedings of the 9th International Workshop on Se-
mantic Evaluation (SemEval 2015), pages 143–147,
Denver, Colorado, June.

Alberto Barrón-Cedeño, Lluı́s Màrquez, Maria
Fuentes, Horacio Rodriguez, and Jordi Turmo.
2013. UPC-CORE: What Can Machine Trans-
lation Evaluation Metrics and Wikipedia Do for
Estimating Semantic Textual Similarity? In 2nd
Joint Conference on Lexical and Computational
Semantics (SEM), pages 143–147, Atlanta, Georgia,
USA, June.

Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning Structured Embed-
dings of Knowledge Bases. In Proceedings of the
Association for the Advancement of Artificial Intel-
ligence (AAAI).

Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words
and meaning representations for open-text semantic
parsing. In Proceedings of the 18th International
Conference on Artificial Intelligence and Statistics,
pages 127–135, La Palma, Canary Islands, April.

408



Samuel R. Bowman, Christopher Potts, and Christo-
pher D. Manning. 2015. Learning Distributed Word
Representations for Natural Logic Reasoning. In
Proceedings of the Association for the Advancement
of Artificial Intelligence Spring Symposium (AAAI),
pages 10–13, March.

Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) Evaluation of Machine Translation. In Pro-
ceedings of the 2nd Workshop on Statistical Machine
Translation (WMT), pages 136–158.

Danqi Chen and Christopher Manning. 2014. A Fast
and Accurate Dependency Parser using Neural Net-
works. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 740–750, Doha, Qatar, October.

Elisabet Comelles and Jordi Atserias. 2014. VERTa
Participation in the WMT14 Metrics Task. In Pro-
ceedings of the 9th Workshop on Statistical Ma-
chine Translation (WMT), pages 368–375, Balti-
more, Maryland, USA, June. Association for Com-
putational Linguistics.

Michael Denkowski and Alon Lavie. 2014. Me-
teor Universal: Language Specific Translation Eval-
uation for Any Target Language. In Proceedings
of the Workshop on Statistical Machine Translation
(WMT).

George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd In-
ternational Conference on Human Language Tech-
nologies (HLT), pages 138–145.

Christian Federmann. 2012. Appraise: An Open-
Source Toolkit for Manual Evaluation of Machine
Translation Output. PBML, 98:25–35, 9.

Meritxell Gonzàlez, Alberto Barrón-Cedeño, and Lluı́s
Màrquez. 2014. IPA and STOUT: Leverag-
ing Linguistic and Source-based Features for Ma-
chine Translation Evaluation. In Proceedings of
the 9th Workshop on Statistical Machine Translation
(WMT), pages 394–401, Baltimore, Maryland, USA,
June. Association for Computational Linguistics.

Ana Guerberof. 2009. Productivity and Quality in the
Post-editing of Outputs from Translation Memories
and Machine Translation. International Journal of
Localization, 7(1).

Aaron L. F. Han, Derek F. Wong, and Lidia S. Chao.
2012. LEPOR: A Robust Evaluation Metric for Ma-
chine Translation with Augmented Factors. In Pro-
ceedings of COLING 2012: Posters, pages 441–450,
Mumbai, India, December.

Pingping Huang and Baobao Chang. 2014. SSMT:A
Machine Translation Evaluation View To Paragraph-
to-Sentence Semantic Similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval 2014), pages 585–589, Dublin, Ire-
land, August.

Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 873–882.

Thomas K. Landauer and Susan T. Dutnais. 1997. A
Solution to Platos Problem: The Latent Semantic
Analysis Theory of Acquisition, Induction, and Rep-
resentation of Knowledge. PSYCHOLOGICAL RE-
VIEW, 104(2):211–240.

Vladimir Iosifovich Levenshtein. 1966. Binary Codes
Capable of Correcting Deletions, Insertions and Re-
versals. Soviet Physics Doklady, 10(8):707–710.

Linguistics Data Consortium. 2005. Linguistic Data
Annotation Specification: Assessment of Fluency
and Adequacy in Translations.

Chi-Kiu Lo and Dekai Wu. 2011a. MEANT: An Inex-
pensive, High-accuracy, Semi-automatic Metric for
Evaluating Translation Utility Based on Semantic
Roles. In Proceedings of the 49th Annual Meet-
ing of the Association of Computational Linguistics:
Human Language Technologies (ACL-HLT), pages
220–229.

Chi-Kiu Lo and Dekai Wu. 2011b. SMT vs. AI re-
dux: How Semantic Frames Evaluate MT More Ac-
curately. In Proceedings of the 22nd International
Joint Conference on Artificial Intelligence (IJCAI).

Chi-Kiu Lo and Dekai Wu. 2011c. Structured vs. Flat
Semantic Role Representations for Machine Trans-
lation Evaluation. In Proceedings of the 5th Work-
shop on Syntax and Structure in Statistical Transla-
tion (SSST).

Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
2012. Fully Automatic Semantic MT Evaluation. In
Proceedings of the 7th Workshop on Statistical Ma-
chine Translation (WMT), pages 243–252, Montréal,
Canada, June. Association for Computational Lin-
guistics.

Tucker Maney, Linda Sibert, Dennis Perzanowski,
Kalyan Gupta, and Astrid Schmidt-Nielsen. 2012.
Toward Determining the Comprehensibility of Ma-
chine Translations. In Proceedings of the 1st PITR,
pages 1–7.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Repre-
sentations in Vector Space. arXiv, 1301.3781.

George A. Miller. 1995. WordNet: A Lexical
Database for English. Communications of the ACM,
38(11):39–41, November.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311–318.

409



Mohammad Taher Pilehvar, David Jurgens, and
Roberto Navigli. 2013. Align, Disambiguate and
Walk: A Unified Approach for Measuring Seman-
tic Similarity. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, pages 1341–1351, Sofia, Bulgaria, August.

Miguel Rios, Wilker Aziz, and Lucia Specia. 2012.
UOW: Semantically Informed Text Similarity. In
The 1st Joint Conference on Lexical and Computa-
tional Semantics (SEM), pages 673–678, Montréal,
Canada, June.

Carolina Scarton, Marcos Zampieri, Mihaela Vela,
Josef van Genabith, and Lucia Specia. 2015.
Searching for Context: a Study on Document-Level
Labels for Translation Quality Estimation. In Pro-
ceedings of the 18th Annual Conference of the Euro-
pean Association for Machine Translation (EAMT),
pages 121–128, Antalya, Turkey, May.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of the Association for Ma-
chine Translation in the Americas (AMTA), pages
223–231.

Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy,
or HTER? Exploring Different Human Judgments
with a Tunable MT Metric. In Proceedings of the
4th Workshop on Statistical Machine Translation
(WMT), pages 259–268.

Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2015. DLS@CU: Sentence Similarity from
Word Alignment and Semantic Vector Composition.
In Proceedings of the 9th International Workshop on
Semantic Evaluation (SemEval 2015), pages 148–
153, Denver, Colorado, June.

Liling Tan and Francis Bond. 2013. Xling: Matching
query sentences to a parallel corpus using topic mod-
els for wsd. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013),
pages 167–170, Atlanta, Georgia, USA, June.

Liling Tan, Rohit Gupta, and Josef van Genabith.
2015a. Usaar-wlv: Hypernym generation with deep
neural nets. In Proceedings of the 9th International
Workshop on Semantic Evaluation (SemEval 2015),
pages 932–937.

Liling Tan, Carolina Scarton, Lucia Specia, and Josef
van Genabith. 2015b. USAAR-SHEFFIELD: Se-
mantic Textual Similarity with Deep Regression and
Machine Translation Evaluation Metrics. In Pro-
ceedings of the 9th International Workshop on Se-
mantic Evaluation (SemEval 2015), pages 85–89,
Denver, Colorado, June.

Liling Tan. 2013. Examining Crosslingual Word Sense
Disambiguation. Master’s thesis, Nanyang Techno-
logical University.

Christoph Tillmann, Stephan Vogel, Hermann Ney,
Alexander Zubiaga, and Hassan Sawaf. 1997. Ac-
celerated DP Based Search For Statistical Transla-
tion. In Proceedings of the EUROSPEECH, pages
2667–2670.

Mihaela Vela and Josef van Genabith. 2015. Re-
assessing the WMT2013 Human Evaluation with
Professional Translators Trainees. In Proceedings of
the 18th Annual Conference of the European Associ-
ation for Machine Translation (EAMT), pages 161–
168, May.

Mihaela Vela, Anne-Kathrin Schumann, and Andrea
Wurm. 2014a. Beyond Linguistic Equivalence.
An Empirical Study of Translation Evaluation in a
Translation Learner Corpus. In Proceedings of the
EACL Workshop on Humans and Computer-assisted
Translation (HaCat), pages 47–56, April.

Mihaela Vela, Anne-Kathrin Schumann, and Andrea
Wurm. 2014b. Human Translation Evaluation and
its Coverage by Automatic Scores. In Proceedings
of the Language Resources and Evaluation Con-
ference Workshop on Automatic and Manual Met-
rics for Operational Translation Evaluation (MTE),
pages 20–30, May.

Radim Řehůřek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In
Proceedings of Language Resources and Evaluation
Conference, pages 46–50, Valletta, Malta.

Frane Šarić, Goran Glavaš, Mladen Karan, Jan Šnajder,
and Bojana Dalbelo Bašić. 2012. TAKELAP: Sys-
tems for Measuring Semantic Text Similarity. In
Proceedings of the 1st Joint Conference on Lexical
and Computational Semantics (SEM), pages 441–
448.

Sandra Weiss and Lars Ahrenberg. 2012. Error Pro-
filing for Evaluation of Machine-translated Text: a
Polish-English Case Study. In Proceedings of the
8th Language Resources and Evaluation Conference
(LREC), pages 1764–1770.

Marcos Zampieri and Mihaela Vela. 2014. Quantify-
ing the Influence of MT Output in the Translators
Performance: A Case Study in Technical Transla-
tion. In Proceedings of the EACL Workshop on Hu-
mans and Computer-assisted Translation (HaCat),
pages 93–98, April.

410


