



















































Efficient Computation of Implicational Universals in Constraint-Based Phonology Through the Hyperplane Separation Theorem


Proceedings of the 15th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 1–10
Brussels, Belgium, October 31, 2018. c©2018 The Special Interest Group on Computational Morphology and Phonology

https://doi.org/10.18653/v1/P17

1

Efficient Computation of Implicational Universals in Constraint-Based
Phonology Through the Hyperplane Separation Theorem

Giorgio Magri
CNRS, SFL, UPL

magrigrg@gmail.com

Abstract

This paper focuses on the most basic im-
plicational universals in phonological the-
ory, called T-orders after Anttila and Andrus
(2006). It develops necessary and sufficient
constraint characterizations of T-orders within
Harmonic Grammar and Optimality Theory.
These conditions rest on the rich convex ge-
ometry underlying these frameworks. They
are phonologically intuitive and have signifi-
cant algorithmic implications.

1 Introduction

A typology T is a collection of grammars
G1, G2, . . . For instance, T could be the set of
syntactic grammars corresponding to all possi-
ble combinations of values of a set of parame-
ters (Chomsky, 1981). Or the set of phonological
grammars corresponding to all possible orderings
of an underlying set of phonological rules (Chom-
sky and Halle 1968). Or the set of grammars cor-
responding to all rankings of an underlying con-
straint set (Prince and Smolensky, 2004).

The structure induced by a typology T can be
investigated though its implicational universals of
the form (1). This implication holds provided ev-
ery grammar in the typology T that satisfies the
antecedent property P also satisfies the conse-
quent property P̂ (Greenberg 1963).

P
T−→ P̂ (1)

To illustrate, suppose that T is the typology of syn-
tactic grammars. Consider the antecedent property
P of having VSO as the basic word order. And
the consequent property P̂ of having prepositions
(as opposed to postpositions). In this case, (1) is
Greenberg’s implicational universal #3.

In this paper, we are interested in typologies
of phonological grammars. We assume a rep-

resentational framework which distinguishes be-
tween two representational levels: underlying rep-
resentations (URs), denoted as x, x̂, . . . ; and sur-
face representations (SRs), denoted as y, ŷ, . . . or
z, ẑ, . . . . A phonological grammar G is a func-
tion which takes a UR x and returns a SR y. For
instance, the phonology of German maps the UR
x = /bE:d/ to the SR y = [bE:t] (‘bath’). A phono-
logical typology T is a collection of phonological
grammars G1, G2, . . . that we assume are all de-
fined over the same set of URs (Richness of the
Base assumption; Prince and Smolensky 2004).

Since phonological grammars are functions
from URs to SRs, the most basic or atomic an-
tecedent property P of an implicational universal
(1) is the property of mapping a certain UR x to
a certain SR y. Analogously, the most basic con-
sequent property P̂ is the property of mapping a
certain UR x̂ to a certain SR ŷ. We thus focus
on implicational universals of the form (2). This
implication holds provided every grammar in the
typology T that succeeds on the antecedent map-
ping (i.e., it maps the antecedent UR x to the an-
tecedent SR y), also succeeds on the consequent
mapping (i.e., it also maps the consequent UR x̂ to
the consequent SR ŷ). This definition makes sense
because every grammar in the typology T is de-
fined on every UR, so that every grammar can be
applied to the two URs x and x̂.

(x, y)
T−→ (x̂, ŷ) (2)

The relation T→ thus defined over mappings turns
out to be a partial order (under mild additional as-
sumptions). It is called the T-order induced by the
typology T (Anttila and Andrus, 2006).

A familiar example concerns coda cluster sim-
plification in English. Suppose that a coda t/d
deletes before vowels in a certain dialect, so that
the UR /cost us/ is realized as the SR [cos’ us].



2

Then the coda also deletes before consonants in
that same dialect, so that the UR /cost me/ is re-
alized as the SR [cos’ me] (Guy, 1991; Kiparsky,
1993; Coetzee, 2004). In other words, the impli-
cation (/tV/, [V]) T→ (/tC/, [C]) holds relative to the
typology T of English dialects.

Two important phonological frameworks ex-
plored in the literature are Harmonic Grammar
(HG; Legendre et al., 1990; Smolensky and Leg-
endre, 2006; Potts et al., 2010) and Optimality
Theory (OT; Prince and Smolensky, 2004). The
crucial idea shared by HG and OT is that the rel-
evant properties of phonological mappings are ex-
tracted by a set of n phonological constraints that
effectively represent discrete phonological map-
pings as points of Rn. The goal of this paper is to
express an implication (x, y) → (x̂, ŷ) in HG and
OT in terms of the constraint violations of the two
mappings (x, y) and (x̂, ŷ) and their competitors.

Section 2 presents the constraint condition for
HG T-orders. It rests on the rich geometry un-
derlying HG, as it follows from a classical re-
sult of convex geometry (the Hyperplane Separa-
tion Theorem), as detailed in section 3. Section 4
presents the constraint condition for OT T-orders.
It rests on an equivalence between OT and HG T-
orders established in section 5.

These constraint conditions admit a straightfor-
ward interpretation and thus help us better under-
stand the phonological import of T-orders. Fur-
thermore, they allow us to compute T-orders effi-
ciently, circumventing the laborious computation
of the entire HG or OT typology (as it is currently
done in the literature; see for instance the OT T-
order Generator by Anttila and Andrus, 2006).

2 Constraint Conditions for HG T-orders

HG assumes a relation Gen which pairs each
UR x with a set Gen(x) of candidate SRs. It
also assumes a set of n phonological constraints
C1, . . . , Cn. Each constraint Ck takes a phono-
logical mapping (x, y) of a UR x and a candidate
SR y in Gen(x) and returns the corresponding
number of violations Ck(x, y) ∈ N, a nonnega-
tive integer which quantifies the “badness” of that
mapping (x, y) from the phonological perspective
encoded by that constraint Ck. A weight vector
w = (w1, . . . , wn) ∈ Rn+ assigns a nonnegative
weight wk ≥ 0 to each constraint Ck.

The w-harmony of a mapping (x, y) is the
weighted sum of the constraint violations multi-

plied by −1, namely −
∑n

k=1wkCk(x, y). Be-
cause of the minus sign, mappings with a large
harmony have few constraint violations. The HG
grammar corresponding to a weight vector w
maps a UR x to the candidate SR y in Gen(x)
such that the mapping (x, y) has a larger w-
harmony than the mapping (x, z) corresponding to
any other candidate z in Gen(x) (Legendre et al.,
1990; Smolensky and Legendre, 2006; Potts et al.,
2010). The HG typology (relative to a candidate
relation and a constraint set) consists of the HG
grammars corresponding to all weight vectors.

We denote by (x, y) HG−→ (x̂, ŷ) the implication
between an antecedent mapping (x, y) and a con-
sequent mapping (x̂, ŷ) relative to the HG typol-
ogy. We assume that the antecedent UR x comes
with only a finite number m of antecedent loser
candidates z1, . . . , zm besides the antecedent win-
ner candidate y. Analogously, we assume that the
consequent UR x̂ comes with only a finite num-
ber m̂ of consequent loser candidates ẑ1, . . . , ẑm̂
besides the consequent winner candidate ŷ. This
assumption is nonrestrictive. In fact, a UR ad-
mits only a finite number of HG optimal candi-
dates (Magri, 2018). Candidate sets can thus be
assumed to be finite without loss of generality.

For each antecedent loser zi, we define the an-
tecedent difference vector C(x, y, zi) as in (3). It
has a component for each constraint Ck defined
as the violation difference Ck(x, y, zi) between the
number Ck(x, zi) of violations assigned by Ck
to the loser mapping (x, zi) minus the number
Ck(x, y) of violations assigned to the antecedent
winner mapping (x, y).

C(x, y, zi) =


C1(x, zi)− C1(x, y)

...
Ck(x, zi)− Ck(x, y)...
Cn(x, zi)− Cn(x, y)

 (3)

The consequent difference vector C(x̂, ŷ, ẑj) is de-
fined analogously, as pitting the consequent win-
ner mapping (x̂, ŷ) against one of its losers (x̂, ẑj).

The definition of the HG implication (x, y) HG→
(x̂, ŷ) requires every HG grammar which succeeds
on the antecedent mapping to also succeed on the
consequent mapping. This condition is trivially
satisfied if no HG grammar succeeds on the an-
tecedent mapping, namely the mapping (x, y) is
HG unfeasible. Thus, let’s suppose that is not the
case. The following proposition then provides a



3

•
• •

•

a.

•
• •

•

◦b.
Figure 1: Geometric representation of condition (4).

complete (both necessary and sufficient) charac-
terization of the HG implication (x, y) HG→ (x̂, ŷ) in
terms of condition (4) stated entirely in terms of
antecedent and consequent difference vectors.

Proposition 1 If the antecedent mapping (x, y) is
HG feasible, the HG implication (x, y) HG→ (x̂, ŷ)
holds if and only if for every consequent loser can-
didate ẑj with j = 1, . . . , m̂, there exist m non-
negative coefficients λ1, . . . , λm ≥ 0 (one for each
antecedent loser candidate z1, . . . , zm) such that

C(x̂, ŷ, ẑj) ≥
m∑
i=1

λiC(x, y, zi) (4)

and furthermore at least one of these coefficients
λ1, . . . , λm is different from zero. 2

Proposition 1 admits the following phonologi-
cal interpretation. Condition (4) says that each
consequent loser ẑj violates the constraints at least
as much as (some conic combination of) the an-
tecedent losers z1, . . . , zm. In other words, the
consequent losers are “worse” than the antecedent
losers. The consequent winner ŷ thus has an “eas-
ier” time beating its losers than the antecedent
winner y, as required by the definition of T-order.

Proposition 1 has important algorithmic impli-
cations. In fact, checking the definition of T-
order (in general, of any implicational universal)
directly is costly, because it requires computing
the entire typology, which can be large. But propo-
sition 1 says that, in the case of HG, T-orders can
be determined locally, by only looking at the an-
tecedent and consequent mappings together with
their losers. Indeed, this proposition effectively
reduces the problem of computing HG T-orders to
the problem of finding coefficients λi which sat-
isfy the inequality (4). The latter is a polyhedral
feasibility problem that can be solved efficiently
with standard linear programming technology. A
Python package to compute HG T-orders using
condition (4) will be released shortly.

ON
SE

T

NO
CO

DA

MA
X

DE
PV

DE
PC

/CC/ [null] 0 0 2 0 0
[CV.CV] 0 0 0 2 0
[CVC] 0 1 0 1 0

/CCC/ [null] 0 0 3 0 0
[CV.CV.CV] 0 0 0 3 0
[CV.CVC] 0 1 0 2 0
[CVC] 0 1 1 1 0

Table 1: Violation profiles for the mappings of the URs /CC/
and /CCC/ to their non-harmonically bounded candidates.

Proposition 1 admits the following geometric
interpretation. Suppose there are only n = 2 con-
straints and m = 4 antecedent difference vectors
C(x, y, zi), represented as the black dots in fig-
ure 1a. The region {

∑m
i=1 λiC(x, y, zi) |λi ≥ 0}

is the convex cone generated by these antecedent
difference vectors, depicted in dark gray in figure
1a. The region in light gray singles out the points
which are at least as large (component by compo-
nent) as some point in this cone. Condition (4)
thus says that each consequent difference vector
C(x̂, ŷ, ẑj) must belong to this light gray region.

Indeed, suppose that some consequent differ-
ence vector does not belong to this light gray re-
gion, as represented by the white dot in figure 1b.
The dashed line leaves the antecedent difference
vectors (black dots) and the consequent difference
vector (white dot) on two different sides. This
means that the HG grammar corresponding to a
nonnegative weight vector orthogonal to this line
succeeds on the antecedent mapping (x, y) but it
fails on the consequent mapping (x̂, ŷ), defying the
implication (x, y) HG→ (x̂, ŷ).

The existence of (a weight vector corresponding
to) a dashed line such as the one depicted in figure
1b is geometrically obvious in the case with only
n = 2 constraints. For an arbitrary number n of
constraints, a fundamental result of convex geom-
etry, the Hyperplane Separation Theorem (HST;
Rockafellar, 1970, §11; Boyd and Vandenberghe,
2004, §2.5), indeed guarantees the existence of a
weight vector which separates the cone generated
by the antecedent difference vectors from the out-
lier consequent difference vector. This is the core
of the proof of proposition 1 provided in section 3.

Let’s finally look at a couple of examples (based
on Bane and Riggle 2009). We assume n = 5 con-



4

ONSET

NOCODA

MAX

DEPV

DEPC


0
0
3
−3
0

≥1.5


0
0
2
−2
0

+0


0
1
0
−1
0

,


0
1
0
−1
0

≥0


0
0
2
−2
0

+1


0
1
0
−1
0

,


0
1
1
−2
0

≥0.5


0
0
2
−2
0

+1


0
1
0
−1
0


Table 2: Verifying that condition (4) holds for the HG implication (CC, CV.CV) → (CCC, CV.CV.CV).

straints: ONSET, which penalizes surface syllables
starting with a vowel (V); NOCODA, which pe-
nalizes surface syllables ending with a consonant
(C); MAX, which penalizes deletion of underlying
segments; and DEPV and DEPC, which penalize
epenthetic vowels and consonants, respectively.
We focus on the two URs /CC/ and /CCC/. We only
consider their non-harmonically bounded candi-
dates, listed in table 1 with their constraint vio-
lations (the candidate [CVC.CV] is omitted because
indistinguishable by the constraints from [CV.CVC]).

We focus on the implication (CC,CV.CV) →
(CCC,CV.CV.CV). The antecedent UR x = /CC/
comes with the winner candidate y = [CV.CV]
and the m = 2 loser candidates z1 = [null] and
z2 = [CVC]. There are therefore two antecedent
difference vectors C(x, y, zi), repeated on the right
hand side of each of the three inequalities in table
2. The consequent UR x̂ = /CCC/ comes with the
winner candidate ŷ = [CV.CV.CV] and the m̂ = 3
loser candidates ẑ1 = [null], ẑ2 = [CV.CVC], and
ẑ3 = [CVC]. There are therefore three consequent
difference vectors C(x̂, ŷ, ẑj), which appear on the
left hand side of the three inequalities in table 2.
Condition (4) holds: each consequent difference
vector is at least as large as a conic combination
of the antecedent difference vectors, as shown in
table 2. Proposition 1 thus establishes the HG im-
plication (CC,CV.CV) HG→ (CCC,CV.CV.CV).

Proposition 1 can also be used to show that an
implication fails in HG. To illustrate, we focus
on the implication (CC,CVC) → (CCC,CV.CVC).
We consider the consequent difference vector
C(/CCC/, [CV.CVC], [null]), which appears on the
left hand side of (5). There are two an-
tecedent difference vectors C(/CC/, [CVC], [null])
and C(/CC/, [CVC], [CV.CV]), which appear on the
right hand side of (5).

ONSET

NOCODA

MAX

DEPV

DEPC


0
−1
3
−2
0

 6≥ λ1


0
−1
2
−1
0

 + λ2


0
−1
0
1
0

 (5)

Condition (4) fails: the consequent difference vec-
tor is not larger than any conic combination of
the two antecedent difference vectors, no mat-
ter the choice of the coefficients λ1, λ2 ≥ 0.
In fact, the inequality (5) for DEPV requires
λ1 ≥ 2, whereby the inequality fails for MAX.
Proposition 1 thus establishes that the implication
(CC,CVC) 6HG→ (CCC,CV.CVC) fails in HG.

3 Proof of Proposition 1

The HST has a number of algebraic consequences
known as theorems of the alternatives.1 One of
these theorems is the Motzkin Transposition The-
orem (MTT; Bertsekas, 2009, proposition 5.6.2),
which is particularly suited to our needs. It states
that conditions (C1) and (C2) below are mutually
exclusive (one and only one of them holds) for any
two matrices A ∈ Rp×n and B ∈ Rq×n.

(C1) There exists a vector w ∈ Rn such that
Aw < 0 and Bw ≤ 0.

(C2) There exist two nonnegative vectors ξ ∈
Rq+ and µ ∈ R

p
+ with µ 6= 0 such that

ATµ + BTξ = 0.

It is useful to specialize the MTT as follows.
Consider some vectors a1, . . . ,am,b ∈ Rn.
Let A be the matrix whose p = m rows are
−aT1 , . . . ,−aTm. Let B be the matrix whose q =
n+1 rows are−eT1 , . . . ,−eTn ,bT (where ei ∈ Rn
has all components equal to 0 but for the ith com-
ponent which is equal to 1). The two conditions
(C1) and (C2) thus become (C1′) and (C2′).

(C1′) There exists a nonnegative vector w ∈ Rn+
such that aT1w > 0, . . . ,a

T
mw > 0 but

bTw ≤ 0.

(C2′) There exist some nonnegative coefficients
µ1, . . . , µm, ξ ≥ 0 with at least one of
the coefficients µ1, . . . , µm different from 0
such that ξb ≥

∑m
i=1 µiai.

1 Throughout this section, all vectors are column vectors;
T stands for matrix transposition. Vector inequalities must
hold component-wise.



5

With these preliminaries in place, we now con-
sider the HG implication (x, y) HG→ (x̂, ŷ). Suppose
that the HG grammar corresponding to some non-
negative weight vector w ∈ Rn+ succeeds on the
antecedent mapping (x, y). This means that the w-
harmony of this mapping (x, y) is larger than that
of every antecedent loser mapping (x, zi). This
condition can be stated in terms of the antecedent
difference vectors as in (6), taking advantage of
the linearity of the HG harmony.

C(x, y, zi)
Tw > 0, i = 1, . . . ,m (6)

The implication(x, y) HG→ (x̂, ŷ) then requires the
HG grammar corresponding to that weight vec-
tor w to also succeed on the consequent mapping
(x̂, ŷ). This means that the w-harmony of this
mapping (x̂, ŷ) is larger than that of every conse-
quent loser mapping (x̂, ẑj). This condition can be
stated in terms of the consequent difference vec-
tors as in (7).

C(x̂, ŷ, ẑj)
Tw > 0, j = 1, . . . , m̂ (7)

In other words, the HG implication (x, y) HG→
(x̂, ŷ) holds if and only if every nonnegative
weight vector w which satisfies (6) also satisfies
(7). Equivalently, the HG T-order holds if and only
if for every j = 1, . . . , m̂, it is false that there ex-
ists a nonnegative weight vector w ∈ Rn+ such
that C(x, y, zi)Tw > 0 for every i = 1, . . . ,m
but C(x̂, ŷ, ẑj)Tw ≤ 0. In other words, for
every j = 1, . . . , m̂, condition (C1′) is false,
with the positions ai = C(x, y, zi) and b =
C(x̂, ŷ, ẑj). By the MTT, condition (C2′) must
therefore be true for every j = 1, . . . , m̂. This
means that there exist some non-negative coeffi-
cients µ1, . . . , µm, ξ ≥ 0 such that at least one of
the coefficients µ1, . . . , µm is strictly positive and
furthermore the inequality (8) holds.

ξC(x̂, ŷ, ẑj) ≥
m∑
i=1

µiC(x, y, zi) (8)

The coefficient ξ in (8) must be strictly positive. In
fact, suppose by contradiction that ξ = 0, whereby
inequality (8) becomes (9).

0 ≥
m∑
i=1

µiC(x, y, zi) (9)

Consider a weight vector w whose correspond-
ing HG grammar maps the antecedent UR x to

the antecedent winner y, which exists by hypoth-
esis. This weight vector w thus satisfies condi-
tion (6). Since w is non-negative, the scalar prod-
uct of both sides of (9) with w preserves the in-
equality, yielding (10). But the latter inequality
requires µ1 = · · · = µm = 0, contradicting the
assumption that at least one of the nonnegative co-
efficients µ1, . . . , µm ≥ 0 is strictly positive.

0 ≥
m∑
i=1

µiC(x, y, zi)
Tw︸ ︷︷ ︸

>0

(10)

Since the coefficient ξ is strictly positive, both
sides of (8) can be divided by ξ, yielding the in-
equality (4) with the position λi = µi/ξ.

4 Constraint Conditions for OT T-orders

This section extends the convex geometric analy-
sis of T-orders developed in the preceding sections
from HG to OT. We start by recalling that in OT a
constraint Ck is said to prefer a mapping (x, y) to
another mapping (x, z) provided Ck assigns less
violations to the former than to the latter, namely
Ck(x, y) < Ck(x, z). A constraint ranking is an
arbitrary linear order � over the constraint set.
A constraint ranking � prefers a mapping (x, y)
to another mapping (x, z) provided the highest�-
ranked constraint which distinguishes between the
two mappings (x, y) and (x, z) prefers (x, y). The
fact that the highest�-ranked relevant constraint
defines the preference of the entire ranking, irre-
spectively of the preferences of lower �-ranked
constraints, is captured by saying that the former
constraint strictly dominates the latter constraints.
The OT grammar corresponding to a ranking �
maps a UR x to that SR y such that� prefers the
mapping (x, y) to the mapping (x, z) correspond-
ing to any other candidate z inGen(x) (Prince and
Smolensky, 2004). The OT typology (for a given
candidate relation and constraint set) consists of
the OT grammars corresponding to all rankings.

We denote by (x, y) OT→ (x̂, ŷ) the implication
between an antecedent mapping (x, y) and a con-
sequent mapping (x̂, ŷ) relative to the OT typol-
ogy. By definition, this implication holds pro-
vided every constraint ranking that succeeds on the
antecedent mapping also succeeds on the conse-
quent mapping. Thus, a natural strategy to check
the OT implication (x, y) OT→ (x̂, ŷ) would be to
use Recursive Constraint Demotion (RCD; Tesar
and Smolensky, 1998) to check that for every j =



6

1, . . . , m̂, no ranking is consistent simultaneously
with the two mappings (x, y) and (x̂, ẑj). In this
section, we develop instead an alternative strategy
which uses the HG-to-OT-portability result of Ma-
gri (2013) to extend to OT the convex geometric
characterization of HG T-orders developed in sec-
tions 2-3.

To start, we recall that an OT grammar can be
construed as an HG grammar (as long as the con-
straint violations are bounded, which is the case
when the set of URs and the candidate sets are
finite). In fact, OT’s strict domination can be
mimicked through HG weights which decrease ex-
ponentially. Indeed, if a weight is much larger
than every smaller weight, the preferences of the
constraint with the larger weight cannot be over-
come by the preferences of the constraints with
smaller weights (Prince and Smolensky, 2004;
Keller, 2006). Since the OT typology is a sub-
set of the HG typology, whenever an implica-
tion (x, y) HG→ (x̂, ŷ) holds in HG, the implication
(x, y)

OT→ (x̂, ŷ) holds in OT.
Lemma 1 slightly strengthens this conclusion.

In fact, OT only cares about constraints’ prefer-
ences. Equivalently, about the sign of the violation
differences. Thus, the HG implication (x, y) HG→
(x̂, ŷ) entails not only the corresponding OT im-
plication (x, y) OT→ (x̂, ŷ) but also any other OT
implication (x∗, y∗) OT→ (x̂∗, ŷ∗) whose antecedent
and consequent mappings (x∗, y∗) and (x̂∗, ŷ∗)
yield violation differences with the same sign as
the original antecedent and consequent mappings
(x, y) and (x̂, ŷ). The proof of this lemma simply
uses the observation that exponentially decaying
HG weights mimic OT strict domination and it is
therefore omitted.

Lemma 1 Given an antecedent mapping (x, y)
with itsm antecedent loser candidates z1, . . . , zm,
consider another mapping (x∗, y∗) with the same
numberm of loser candidates z∗1, . . . , z

∗
m such that

the m corresponding violation differences have
the same sign, in the sense that condition (11)
holds for k = 1, . . . , n and i = 1, . . . ,m.

Ck(x, y, zi) T 0 ⇐⇒ Ck(x∗, y∗, z∗i ) T 0 (11)

Analogously, given the consequent mapping
(x̂, ŷ) with its m̂ consequent loser candidates
ẑ1, . . . , ẑm̂, consider another mapping (x̂∗, ŷ∗)
with the same number m̂ of loser candidates
ẑ∗1, . . . , ẑ

∗
m̂ such that the m̂ corresponding viola-

tion differences have the same sign, in the sense

that condition (12) holds for k = 1, . . . , n and
j = 1, . . . , m̂.

Ck(x̂, ŷ, ẑj) T 0 ⇐⇒ Ck(x̂∗, ŷ∗, ẑ∗j ) T 0 (12)

The HG implication (x, y) HG−→ (x̂, ŷ) then entails
the OT implication (x∗, y∗) OT−→ (x̂∗, ŷ∗).

The preceding lemma establishes an entailment
from HG to OT implications. We now want to in-
vestigate the reverse entailment from OT to HG
implications. Thus, we suppose that an implica-
tion (x, y) OT→ (x̂, ŷ) holds in OT. Of course, that
does not entail that the implication (x, y) HG→ (x̂, ŷ)
between the same two mappings also holds in
HG. That is because the HG typology is usually a
proper superset of the OT typology. And a larger
typology yields sparser T-orders. Thus, it makes
no sense to try to establish that the OT implica-
tion (x, y) OT→ (x̂, ŷ) entails the HG implication
(x, y)

HG→ (x̂, ŷ) between the same two mappings.
We will try to establish something weaker in-

stead: the OT implication (x, y) OT→ (x̂, ŷ) entails
an HG implication (xdif, ydif) HG→ (x̂easy, ŷeasy) be-
tween an antecedent mapping (xdif, ydif) different
from (x, y) and a consequent mapping (x̂easy, ŷeasy)
different from (x̂, ŷ). And we will choose this new
antecedent mapping (xdif, ydif) and this new conse-
quent mapping (x̂easy, ŷeasy) in such a way that the
new HG implication (xdif, ydif) HG→ (x̂easy, ŷeasy) is
“more likely to hold” than the original implication
(x, y)

HG→ (x̂, ŷ) and thus validates the entailment
from OT to HG implications.

What does it mean that an implication is “more
likely to hold”? Intuitively, an implication from
an antecedent to a consequent mapping is “likely
to hold” when the antecedent mapping is “diffi-
cult” to obtain, namely it is consistent with very
few grammars. In the limit, the implication holds
trivially when the antecedent mapping is consis-
tent with no grammars at all. Thus, we want to
define the new antecedent mapping (xdif, ydif) in
such a way that it is “more difficult” to obtain in
HG than the original antecedent mapping (x, y),
whereby the superscript “diff”. Analogously, an
implication from an antecedent to a consequent
mapping is intuitively “likely to hold” when the
consequent mapping is “easy” to obtain, namely
it is consistent with very many grammars. In the
limit, the implication holds trivially when the con-
sequent mapping is consistent with every gram-
mar. Thus, we want to define the new consequent



7

mapping (x̂easy, ŷeasy) in such a way that it is “eas-
ier” to obtain in HG than the original consequent
mapping (x̂, ŷ), whereby the superscript “easy”.

Let us now turn to the details. As discussed
above around (6), it suffices to define the differ-
ence vectors corresponding to the new difficult
mapping antecedent (xdif, ydif). Given the original
antecedent mapping (x, y) with its m loser can-
didates z1, . . . , zm, we assume that the new an-
tecedent mapping (xdif, ydif) comes with the same
number m of loser candidates zdif1 , . . . , z

dif
m whose

violation differences are defined as in (13). Here,
Ωi is the total number of constraints Ck such that
Ck prefers the original antecedent winner map-
ping (x, y) to the original antecedent loser map-
ping (x, zi), in the sense that Ck(x, y, zi) > 0.

Ck(x
dif, ydif, zdifi ) =

=


1 if Ck(x, y, zi) > 0

0 if Ck(x, y, zi) = 0

−Ωi − 1 if Ck(x, y, zi) < 0

(13)

The intuition behind this definition (13) is as
follows. OT only cares about the sign of the vi-
olation differences. Thus, the new violation dif-
ference Ck(xdif, ydif, zdifi ) is defined in such a way
that it has the same sign as the original violation
difference Ck(x, y, zi): one is positive or nega-
tive if and only if the other is as well. HG also
cares about the size of the violation differences,
not only about their sign. In order for the map-
ping (xdif, ydif) to be “difficult” in HG, we want
its positive violation differences to be as small as
possible. For this reason, the positive violation dif-
ferences in (13) have been set equal to 1, which is
the smallest positive integer. Analogously, in or-
der for the mapping (xdif, ydif) to be “difficult” in
HG, we want its negative violation differences to
be large (in absolute value) relative to the strength
of the positive violation differences they have to
“fight off”. Since the positive entries are all equal
to 1 in (13), the “strength” of the positive entries
only depends on their number Ωi. For this rea-
son, the absolute value of the negative violation
differences in (13) has been set equal to Ωi + 1.
In conclusion, this definition (13) ensures that the
mapping (xdif, ydif) is “difficult” in HG, because
the positive violation differences are small and the
negative ones are large (in absolute value).

We now turn to the consequents. Given the orig-
inal consequent mapping (x̂, ŷ) with its m̂ loser
candidates ẑ1, . . . , ẑm̂, we assume that the new

consequent mapping (x̂easy, ŷeasy) comes with the
same number m̂ of loser candidates ẑeasy1 , . . . , ẑ

easy
m̂

whose violation differences are defined as in (14).
Here Λ̂j is the total number of constraints Ck
such that Ck prefers the original consequent loser
mapping (x̂, ẑj) to the original consequent winner
mapping (x̂, ŷ), in the sense that Ck(x̂, ŷ, ẑj) < 0.

Ck(x̂
easy, ŷeasy, ẑeasyj ) =

=


Λ̂j + 1 if Ck(x̂, ŷ, ẑj) > 0

0 if Ck(x̂, ŷ, ẑj) = 0

−1 if Ck(x̂, ŷ, ẑj) < 0

(14)

The intuition behind this definition (14) is as
follows. Whenever the original violation differ-
ence Ck(x̂, ŷ, ẑj) is positive or negative, the new
violation difference Ck(x̂easy, ŷeasy, ẑ

easy
j ) is posi-

tive or negative as well, so that the original and
the new violation differences have the same sign.
The size of the new violation differences has been
chosen as follows. In order for the mapping
(x̂easy, ŷeasy) to be “easy” in HG, we want its nega-
tive violation differences to be as small as possible
(in absolute value). For this reason, the negative
violation differences in (14) have been set equal to
−1, which is the negative integer smallest in ab-
solute value. Analogously, in order for the map-
ping (x̂easy, ŷeasy) to be “easy” in HG, we want its
positive violation differences to be large relative to
the strength of the negative violation differences
they have to “fight off”. Since the negative entries
are all equal to −1 in (14), the “strength” of the
negative entries only depends on their number Λ̂j .
For this reason, the positive violation differences
in (14) have been set equal to Λ̂j + 1. In conclu-
sion, this definition (14) ensures that the mapping
(x̂easy, ŷeasy) is “easy” in HG, because the positive
violation differences are large and the negative vi-
olation differences are small (in absolute value).

We are now ready to put the pieces together. As
anticipated, the OT implication (x, y) OT→ (x̂, ŷ)
might not entail the HG implication (x, y) HG→
(x̂, ŷ) with the same antecedent and consequent
mappings. Nonetheless, the following lemma 2
ensures that the OT implication (x, y) OT→ (x̂, ŷ)
does entail the HG implication (xdif, ydif) HG→
(x̂easy, ŷeasy). The intuition is that the lat-
ter is less demanding than the HG implication
(x, y)

HG→ (x̂, ŷ), because its antecedent is “diffi-
cult” (namely, consistent with few HG grammars)
and its consequent is “easy” (namely, consistent



8

with many HG grammars). The proof of this
lemma is provided in section 5, mimicking a rea-
soning in Magri (2013).

Lemma 2 The OT implication (x, y) OT→ (x̂, ŷ) en-
tails the HG implication (xdif, ydif) HG→ (x̂easy, ŷeasy)
between the antecedent mapping (xdif, ydif) and the
consequent mapping (x̂easy, ŷeasy) whose violation
differences are defined in (13) and (14). 2

As remarked explicitly above, (13) ensures
that the original antecedent violation differences
Ck(x, y, zi) and the new antecedent violation dif-
ferences Ck(xdif, ydif, zdifi ) have the same sign. In
other words, condition (11) holds with the posi-
tions x∗ = xdif, y∗ = ydif, and z∗i = z

dif
i . Anal-

ogously, (14) ensures that the original consequent
violation differences Ck(x̂, ŷ, ẑj) and the new con-
sequent violation differences Ck(x̂easy, ŷeasy, ẑ

easy
i )

have the same sign. In other words, condition (12)
holds with the positions x̂∗ = x̂easy, ŷ∗ = ŷeasy, and
ẑ∗i = ẑ

easy
i . The two lemmas 1 and 2 can therefore

be combined into the following conclusion: the
OT implication (x, y) OT→ (x̂, ŷ) holds if and only
the HG implication (xdif, ydif) HG→ (x̂easy, ŷeasy)
holds. We can thus extend to OT the characteri-
zation of HG T-orders provided by the HG propo-
sition 1 above, obtaining the following:

Proposition 2 If the antecedent mapping (x, y) is
OT feasible, the OT implication (x, y) OT→ (x̂, ŷ)
holds iff for every j=1, . . . , m̂, there exist m non-
negative coefficients λ1, . . . , λm ≥ 0 such that

C(x̂easy, ŷeasy, ẑeasyj ) ≥
m∑
i=1

λiC(x
dif, ydif, zdifi )

(15)
and furthermore at least one of these coefficients
λ1, . . . , λm is different from zero. 2

To illustrate, we have seen at the end of sec-
tion 2 that the HG implication (CC, CVC) HG→
(CCC, CV.CVC) fails in HG because condition (4)
fails, as shown in (5). But this entailment
(CC, CVC)

OT→ (CCC, CV.CVC) does hold in OT. In
fact, the three “easy” consequent difference vec-
tors C(x̂easy, ŷeasy, ẑeasyj ) in this case are listed on
the left hand side of the three inequalities in ta-
ble 3. The two “difficult” antecedent difference
vectors C(xdif, ydif, zdifi ) are repeated on the right
hand side of the three inequalities. The table thus
shows that condition (15) holds.

5 Proof of Lemma 2

We assume that the OT implication (x, y) OT→
(x̂, ŷ) holds. We consider an arbitrary nonnegative
weight vector w = (w1, . . . , wn) which succeeds
on the “difficult” antecedent mapping (xdif, ydif)
and we prove that it is also succeeds on the “easy”
consequent mapping (x̂easy, ŷeasy), thus securing
the HG implication (xdif, ydif) HG−→ (x̂easy, ŷeasy).

The assumption that the weight vec-
tor w succeeds on the “difficult” an-
tecedent mapping (xdif, ydif) means that∑n

k=1wkCk(x
dif, ydif, zdifi ) > 0 for every

i = 1, . . . ,m. The latter inequality can be
unpacked as in (16). In step (16a), we have
used the definition (13). Here W (x, y, zi) and
L(x, y, zi) are the sets of winner-preferring and
loser-preferring constraints relative to the winner
(x, y) and the loser (x, zi). In step (16b), we have
upper bounded the sum

∑
h∈W (x,y,zi)wh with its

largest term maxh∈W (x,y,zi)wh times the number
Ωi of its addenda. In step (16c), we have lower
bounded the sum

∑
k∈L(x,y,zi)wk with one of its

terms, as the addenda are all non-negative.
n∑

k=1

wkCk(x
dif, ydif, zdifi ) > 0

(a)⇐⇒
∑

h∈W (x,y,zi)

wh > (Ωi + 1)
∑

k∈L(x,y,zi)

wk

(b)
=⇒ Ωi max

h∈W (x,y,zi)
wh > (Ωi + 1)

∑
k∈L(x,y,zi)

wk

(c)
=⇒ Ωi max

h∈W (x,y,zi)
wh > (Ωi + 1)wk

for every k ∈ L(x, y, zi)
=⇒ max

h∈W (x,y,zi)
wh > wk

(16)
We now show that the conclusion reached in the

last line of (16) entails that the strict inequality
(17) holds for every j = 1, . . . , m̂.

max
h∈W (x̂,ŷ,̂zj)

wh > max
k∈L(x̂,ŷ,̂zj)

wk (17)

In fact, suppose by contradiction that (17) fails
for some j = 1, . . . , m̂. Consider a ranking �
which respects the relative size of the weights, in
the sense that conditions [A] and [B] hold for any
two constraints Cs, Ct with weights ws, wt.

[A] If ws > wt, then Cs is�-ranked above Ck.
[B] If ws = wt and Cs ∈ L(x̂, ŷ, ẑj) and Ct ∈

W (x̂, ŷ, ẑj), then Cs is�-ranked above Ck.



9

ONSET

NOCODA

MAX

DEPV

DEPC


0
−1
3
−1
0

≥0.5


0
−2
1
−2
0

+0


0
−2
0
1
0




0
−1
0
2
0

≥0


0
−2
1
−2
0

+0.5


0
−2
0
1
0




0
0
2
−1
0

≥0.5


0
−2
1
−2
0

+0


0
−2
0
1
0


Table 3: Verifying that condition (15) holds for the OT implication (CC, CVC) → (CCC, CV.CVC).

The ranking � succeeds on the antecedent
mapping (x, y). In fact, the condition obtained
in the last line of (16) says that there exists a
constraint which prefers the winner (x, y) to the
loser (x, zi) whose weight is strictly larger than the
weight of every constraint which instead prefers
the loser (x, zi) to the winner (x, y). By [A],
this means that a constraint which prefers the
winner (x, y) is �-ranked above every constraint
that instead prefers the loser (x, zi). The rank-
ing � therefore prefers the winner (x, y) to the
loser (x, zi). Since this conclusion holds for ev-
ery i = 1, . . . ,m, the ranking� succeeds on the
antecedent mapping (x, y).

On the other hand, the ranking � fails on
the consequent mapping (x̂, ŷ). In fact, the con-
tradictory assumption that (17) fails means that
maxh∈W (x̂,ŷ,̂zj)wh ≤ maxk∈L(x̂,ŷ,̂zj)wk. In other
words, there exists a constraint which prefers the
loser (x̂, ẑj) to the winner (x̂, ŷ) whose weight is
strictly larger than or equal to the weights of the
constraints which instead prefer the winner (x̂, ŷ)
to the loser (x̂, ẑj). By [A] and [B], the ranking�
cannot prefer (x̂, ŷ) to (x̂, ẑj).

The conclusion that � succeeds on the an-
tecedent (x, y) but fails on the consequent (x̂, ŷ)
contradicts the assumption that the implication
(x, y)

OT→ (x̂, ŷ) holds in OT, thus establishing
the inequality (17). This inequality can in turn
be unpacked as in (18). In step (18a), we have
lower bounded Λ̂j maxk∈L(x̂,ŷ,̂zj)wk with the sum∑

k∈L(x̂,ŷ,̂zj)wk, because Λ̂j is the number of ad-
denda in the sum. In step (18b), we have upper
bounded the maximum maxh∈W (x̂,ŷ,̂zj)wh with
the sum

∑
h∈W (x̂,ŷ,̂zj)wh, because the weights be-

ing summed over are all non-negative. In step
(18c), we have used the definition (14) of the con-

straint differences Ck(x̂easy, ŷeasy, ẑ
easy
j ).

max
h∈W (x̂,ŷ,̂zj)

wh > max
k∈L(x̂,ŷ,̂zj)

wk =⇒

=⇒ (Λ̂j + 1) max
h∈W (x̂,ŷ,̂zj)

wh > Λ̂j max
k∈L(x̂,ŷ,̂zj)

wk

(a)
=⇒ (Λ̂j + 1) max

h∈W (x̂,ŷ,̂zj)
wh >

∑
k∈L(x̂,ŷ,̂zj)

wk

(b)
=⇒ (Λ̂j + 1)

∑
h∈W (x̂,ŷ,̂zj)

wh >
∑

k∈L(x̂,ŷ,̂zj)

wk

(c)
=⇒

n∑
k=1

wkCk(x̂
easy, ŷeasy, ẑeasyj ) > 0

(18)
The inequality in the last line of (18) holds for ev-
ery j = 1, . . . , m̂, ensuring that the weights w
succeed on the consequent mapping (x̂easy, ŷeasy).

6 Conclusions

A central task of linguistic theory is to character-
ize the typological structure predicted by a gram-
matical formalism in order to match it to linguistic
data. A classical strategy to characterize typologi-
cal structure is to chart the implicational universals
predicted by the formalism. In this paper, we have
focused on the two constraint-based phonological
formalisms of HG and OT. And we have consid-
ered the simplest type of implicational universals,
namely T-orders. The main result of this paper
has been a complete constraint characterization of
T-orders in HG and OT. These constraint condi-
tions rely on an elegant underlying convex geom-
etry. These conditions are phonologically intuitive
and have important algorithmic implications.

Acknowledgments

The research reported in this paper has been
funded by the Agence National de la Recherche
(project title: ‘The mathematics of segmental
phonotactics’). This paper is part of a larger
project on T-orders, developed in collaboration
with Arto Anttila. His comments on this paper are
gratefully acknowledged.



10

References
Arto Anttila and Curtis Andrus. 2006. T-orders.

Manuscript and software (Stanford).

Maximilian Bane and Jason Riggle. 2009. Evaluating
Strict Domination: The typological consequences of
weighted constraints. In Proceedings of the 45th
annual meeting of the Chicago Linguistics Society,
pages 13–27.

Dimitri P. Bertsekas. 2009. Convex Optimization The-
ory. Athena Scientific, Belmont, MA, USA.

Stephen Boyd and Lieven Vandenberghe. 2004. Con-
vex Optimization. Cambridge University Press.

Noam Chomsky. 1981. Lectures on Government and
Binding. Mouton de Gruyter.

Noam Chomsky and Morris Halle. 1968. The Sound
Pattern of English. Harper and Row, New York.

Andries W. Coetzee. 2004. What it Means to be a
Loser: Non-Optimal Candidates in Optimality The-
ory. Ph.D. thesis, University of Massachusetts,
Amherst.

Joseph H. Greenberg. 1963. Some universals of gram-
mar with particular reference to the order of mean-
ingful elements. In Joseph H. Greenberg, editor,
Universals of Language, pages 73–113. MIT Press,
Cambridge, MA.

G. Guy. 1991. Explanation in variable phonology.
Language Variation and Change, 3:1–22.

Frank Keller. 2006. Linear Optimality Theory as
a model of gradience in grammar. In Gisbert
Fanselow, Caroline Féry, Ralph Vogel, and Matthias
Schlesewsky, editors, Gradience in Grammar: Gen-
erative Perspectives, pages 270–287. Oxford Uni-
versity Press, Oxford.

Paul Kiparsky. 1993. An OT perspective on phonolog-
ical variation. Handout (Stanford).

Géraldine Legendre, Yoshiro Miyata, and Paul
Smolensky. 1990. Harmonic Grammar: A for-
mal multi-level connectionist theory of linguistic
well-formedness: Theoretical foundations. In An-
nual conference of the Cognitive Science Society 12,
pages 388–395, Mahwah, NJ. Lawrence Erlbaum.

Giorgio Magri. 2013. HG has no computational ad-
vantages over OT: towards a new toolkit for compu-
tational OT. Linguistic Inquiry, 44.4:569–609.

Giorgio Magri. 2018. Finiteness of optima in
constraint-based phonology. Manuscript, CNRS.

Christopher Potts, Joe Pater, Karen Jesney, Rajesh
Bhatt, and Michael Becker. 2010. Harmonic Gram-
mar with Linear Programming: From linear systems
to linguistic typology. Phonology, 27(1):1–41.

Alan Prince and Paul Smolensky. 2004. Optimality
Theory: Constraint Interaction in generative gram-
mar. Blackwell, Oxford. Original version, Techni-
cal Report CU-CS-696-93, Department of Computer
Science, University of Colorado at Boulder, and
Technical Report TR-2, Rutgers Center for Cogni-
tive Science, Rutgers University, April 1993. Avail-
able from the Rutgers Optimality Archive as ROA
537.

R. Tyrrell Rockafellar. 1970. Convex Analysis. Prince-
ton Landmarks in Mathematics; Princeton Univer-
sity Press.

Paul Smolensky and Géraldine Legendre. 2006. The
Harmonic Mind. MIT Press, Cambridge, MA.

Bruce Tesar and Paul Smolensky. 1998. Learnability in
Optimality Theory. Linguistic Inquiry, 29:229–268.


