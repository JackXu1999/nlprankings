



















































The Instantiation Discourse Relation: A Corpus Analysis of Its Properties and Improved Detection


Proceedings of NAACL-HLT 2016, pages 1181–1186,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

The Instantiation Discourse Relation:
A Corpus Analysis of Its Properties and Improved Detection

Junyi Jessy Li and Ani Nenkova
University of Pennsylvania

{ljunyi,nenkova}@seas.upenn.edu

Abstract

INSTANTIATION is a fairly common discourse
relation and past work has suggested that it
plays special roles in local coherence, in sen-
timent expression and in content selection in
summarization. In this paper we provide the
first systematic corpus analysis of the rela-
tion and show that relation-specific features
can improve considerably the detection of the
relation. We show that sentences involved
in INSTANTIATION are set apart from other
sentences by the use of gradable (subjec-
tive) adjectives, the occurrence of rare words
and by different patterns in part-of-speech us-
age. Words across arguments of INSTANTI-
ATION are connected through hypernym and
meronym relations significantly more often
than in other sentences and that they stand
out in context by being significantly less sim-
ilar to each other than other adjacent sentence
pairs. These factors provide substantial pre-
dictive power that improves the identification
of implicit INSTANTIATION relation by more
than 5% F-measure.

1 Introduction

In an INSTANTIATION relation, one text span ex-
plains in further detail the events, reasons, behaviors
and attitudes mentioned in the other (Miltsakaki et
al., 2008), as illustrated by the segments below:
[a] Other fundamental “reforms” of the 1986 act have been
threatened as well.

[b] The House seriously considered raising the top tax rate paid
by individuals with the highest incomes.

Sentence [a] mentions “other reforms” and a threat
to them, but leaves unspecified what are the reforms

or how they are threatened. Sentence [b] provides
sufficient detail for the reader to infer more con-
cretely what has happened.

The INSTANTIATION relation has some special
properties. A study of discourse relations as in-
dicators for content selection in single document
summarization revealed that the first sentences from
INSTANTIATION pairs are included in human sum-
maries significantly more often than other sentences
(Louis et al., 2010) and that being a first sentence
in an INSTANTIATION relation is the most powerful
indicator for content selection related to discourse
relation sense. The sentences between which the
relation holds also contain more sentiment expres-
sions than other sentences (Trnavac and Taboada,
2013), making it a special target for sentiment anal-
ysis applications. Moreover, INSTANTIATION rela-
tions appear to play a special role in local coherence
(Louis and Nenkova, 2010), as the flow between IN-
STANTIATION sentences is not explained by the ma-
jor coherence theories (Kehler, 2004; Grosz et al.,
1995). Many of the sentences in INSTANTIATION
relation contain entity instantiations (complex ex-
amples of set-instance anaphora), such as “several
EU countries”—“the UK”, “footballers”—“Wayne
Rooney” and “most cosmetic purchase”—“lipstick”
(McKinlay and Markert, 2011), raising further ques-
tions about the relationship between INSTANTIA-
TIONS and key discourse phenomena.

Detecting an INSTANTIATION, however, is hard.
In the Penn Discourse Treebank (PDTB) (Prasad et
al., 2008), INSTANTIATION is one of the few re-
lations that are more often implicit, i.e., expressed
without a discourse marker such as “for exam-

1181



ple”. Identifying implicit discourse relation is an ac-
knowledged difficult task (Braud and Denis, 2015;
Ji and Eisenstein, 2015; Rutherford and Xue, 2014;
Biran and McKeown, 2013; Park and Cardie, 2012;
Lin et al., 2009; Pitler et al., 2009), but the chal-
lenge is exacerbated due to the lack of explicit IN-
STANTIATIONs: explicit relations are shown to im-
prove their implicit counterparts using data source
expansion (Rutherford and Xue, 2015). Moreover,
detecting INSTANTIATION also involves the skewed
class distribution problem (Li and Nenkova, 2014a)
because although it is one of the largest class of im-
plicit relations, it constitutes less than 10% of all the
implicit relations annotated in the PDTB.

In this work, we identify a rich set of factors
that sets apart each sentence in an implicit INSTAN-
TIATION and the pair as a whole. We show that
these factors improve the identification of implicit
INSTANTIATION by at least 5% in F-measure and
8% in balanced accuracy compared to prior systems.

2 Presence of INSTANTIATION

We use the Penn Discourse Tree Bank (PDTB)
(Prasad et al., 2008) for the analysis and experiments
presented in this paper. There are a total of 1,747 IN-
STANTIATION relations in the PDTB, of which 83%
are implicit. INSTANTIATIONs make up 8.7% of all
implicit relations and is the 5th largest among the 16
second-level relations in the PDTB.

3 Characteristics of INSTANTIATION

We identify significant factors1 that characterize: (i)
s1 and s2: the first and second sentence in an IN-
STANTIATION pair vs. all other sentences; (ii) s1 vs.
s2: adjacent sentence pairs in INSTANTIATION rela-
tion vs. all other adjacent sentence pairs.

Our analysis is conducted on the PDTB except
section 23, which is reserved for testing as in prior
work (Lin et al., 2014; Biran and McKeown, 2015).
In total, there are 1,337 INSTANTIATION sentence
pairs and 43,934 non-INSTANTIATION sentences for
the corpus analysis.

1p < 0.05 according to paired Wilcoxon signed rank test for
real valued comparison between the two sentences in a relation,
non-paired Wilcoxon rank sum for real valued factors in dif-
ferent types of sentences, and Kruskal-Wallis for binary valued
features across different types of sentences.

s1 s2 ¬ Inst.
#words/sent 18.4∗ 26.8∗ 23.9

Table 1: Average # words. [∗]: significant (p < 0.05) com-
pared to non-instantiation sentences.

s1 s2 ¬ Inst.
%oov/sent 0.68∗ 1.54 1.46

Table 2: Average % of rare words per sentence. [∗]: significant
(p < 0.05) compared to non-instantiation sentences.

Sentence length. Intuitively, longer sentences are
more likely to involve details. Table 1 demonstrates
that there is an average of 8.4-word difference in
length between the two sentences in an INSTAN-
TIATION relation; moreover, s1s are significantly
shorter (more than 5 words on average) than other
sentences, and s2s are significantly longer.

Rare words. For each sentence, we compute the
percentage of words that are not present in the
400K vocabulary of the Glove vector representa-
tions (Pennington et al., 2014). Table 2 shows
that s1 of INSTANTIATIONs contain significantly
fewer out-of-vocabulary words compared to either
s2 and non-INSTANTIATIONs. We also compare
the difference in unigram probability2 of content
word pairs across sentence pairs, i.e., (wi, wj), wi ∈
s1, wj ∈ s2. Compared to non-INSTANTIATION,
words across INSTANTIATION arguments show sig-
nificantly larger average unigram log probability dif-
ference (1.24 vs. 1.22). These numbers show that
the first sentences of INSTANTIATION do not involve
many unfamiliar words — an indication of higher
readability (Pitler and Nenkova, 2008).

Gradable adjectives. The use of gradable ad-
jectives (Frazier et al., 2008; de Marneffe et al.,
2010)—popular, high, likely— may require further
explanation to justify the appropriateness of their
use. Here we compute the average percentage of
gradable adjectives in a sentence. The list of ad-
jectives is from Hatzivassiloglou and Wiebe (2000)
and the respective percentages are shown in Table 3.
Compared to other sentences, s1 of INSTANTIATION
involves significantly more gradable adjectives.

2We use a unigram language model on year 2006 of the New
York Times Annotated Corpus (Sandhaus, 2008).

1182



s1 s2 ¬ Inst.
%gradable adj 2.96∗ 2.22 2.22

Table 3: Average % of gradable adjectives per sentence. [∗]:
significant (p < 0.05) compared to non-instantiation sentences.

s1 > s2 CC EX JJR JJS NNS PDT RB† RBR VBG
VBN VBP VBZ†

s1 < s2 NN NNP† PRP TO VBD WRB

s1 vs ¬Inst. CD− JJ− MD− NN− NNP− NNS+ PRP−
RB+ TO− VB− VBD− VBG+ VBP+

VBZ+ WDT−

s2 vs ¬Inst. CD+ DT+ MD− NNP+ NNS+ PRP+ RB−
VB− VBN−

Table 4: POS tags significantly different in percentage com-
pared to non-instantiation. [†]: significance in non-instantiation
pairs in the other direction. [+/−]: used more/less often than
non-instantiation.

Parts of speech. We study word categories that
are heavily or rarely used with INSTANTIATION
by inspecting the percentage of part-of-speech tags
found in each sentence. In Table 4, we show
POS tags whose presence is significantly differ-
ent across arguments in INSTANTIATION but not
so across non-INSTANTIATION, with significance
in non-INSTANTIATION in the reverse direction de-
noted in †. Four cases of POS occurrences are in-
spected:

• more often in s1 compared to s2,
• more often in s2 compared to s1,
• more (+) or less (-) in s1 compared to non-

INSTANTIATION,
• more (+) or less (-) in s2 compared to non-

INSTANTIATION.

We see that s1 of INSTANTIATION contains more
characteristic POS usage than s2. There are more
comparative adjectives and adverbs as well as fewer
nouns in s1 compared to s2 in INSTANTIATION
pairs. The usage of verbs is also different between
two arguments and s1 contains more conjunctions
and existential there. On the other hand, s2 con-
tains more nouns, numbers, determiners, personal
pronouns and proper nouns, intuitively associated
with the presence of detailed information.

Wordnet relations. Here we consider word-level
relationships across arguments using Wordnet (Fell-
baum, 1998). For each noun, verb, adjective and

Relation (pos) Inst. ¬ Inst.
hypernym (n) 18.01∗ 21.63
meronym (n) 18.66∗ 15.48
holonym (n) 17.14 15.07

indirect hypernym (n) 19.17 21.03
hyponym (n) 20.33 21.75

group (v) 72.5∗ 68.7
indirect hypernym (v) 38.5∗ 41.7

hypernym (v) 76.25 74.79
hyponym (v) 80.76 78.44
entailment (v) 4.94 4.18

cause (v) 17.65 17.00

similar to (adj) 3.78 2.77
also see (adj) 6.25 5.78

Table 5: Percentage of sentence pairs with Wordnet relation-
ships. [∗]: significant (p < 0.05) compared to non-instantiation
sentence pairs.

s1 s2 ∆sim

Inst. 0.282∗ 0.275∗ 0.007
¬ Inst. 0.390 0.358 0.042∗

Table 6: Average Jaccard similarity of an adjacent sentence pair
s1, s2 with immediate context. [∗]: significant (p < 0.05)
compared to non-instantiation sentence pairs.

adverb content word pairs across arguments, we cal-
culate the percentage of sentences with each type
of Wordnet relation. Shown in Table 5, among IN-
STANTIATION sentence pairs there are significantly
more noun-noun pairs with hypernym or meronym
relationships and verbs with indirect hypernym rela-
tionship. We also observe significantly more seman-
tically similar verbs (group (v)).

Lexical similarity. Finally, we inspect the similar-
ity between sentences in each pair as well as be-
tween each sentence in a pair and their immediate
prior context; specifically:

• Between s1 and s2;
• Between s1 and C and between s2 and C,

where C denotes up to two sentences immedi-
ately before s1.

We compute the Jaccard similarity between sen-
tences using their nouns, verbs, adjectives and ad-
verbs. INSTANTIATION arguments are significantly
less similar than other adjacent sentence pairs (0.335
vs. 0.505), indicating higher difference in content.

1183



System P R F BA

Inst. specific 0.3072 0.6986 0.4268 0.7862
Vote (L&N) 0.3052 0.6438 0.4141 0.7632

L&N 0.3028 0.4521 0.3626 0.6843
B&M 0.2542 0.2055 0.2273 0.5786

Lin et al. 0.5500 0.1507 0.2366 0.5704
Brown-concat 0.1333 0.3836 0.1979 0.5919

Table 7: Precision, recall, F-measure and balanced accuracy of
identifying INSTANTIATION.

Shown in Table 6, both arguments of INSTANTI-
ATION are less similar to the immediate context.
While other sentence pairs follow the pattern that s2
is much less similar to s1’s immediate context, this
phenomenon is not significant for INSTANTIATION.

4 Experiments

In this section, we demonstrate the benefit of ex-
ploiting INSTANTIATION characteristics in the iden-
tification of the relation.

Settings. Following prior work that identifies
the more detailed (second-level) relations in the
PDTB (Biran and McKeown, 2015; Lin et al., 2014),
we use sections 2-21 as training, section 23 as test-
ing. The rest of the corpus is used for development.
The task is to predict if an implicit INSTANTIATION
relation holds between pairs of adjacent sentences in
the same paragraph. Sentence pairs with INSTANTI-
ATION relation constitute the positive class; all other
non-explicit relations3 constitute the negative class.
We use Logistic Regression with class weights in-
versely proportional to the size of each class.

Features. The factors discussed in § 3 are adopted
as the only features in the classifier. We use the av-
erage values of s1 and s2 and their difference for:
the number of words, difference in number of words
compared to the sentence before s1, the percentage
of OOVs, gradable adjectives, POS tags and Jaccard
similarity to immediate context. We use the min-
imum, maximum and average differences in word-
pair unigram log probability, and average Jaccard
similarity across sentence pairs. For Wordnet rela-
tions, we use binary features indicating the presence
of a relation.

3including AltLex, EntRel and NoRel

Results. To compare with our INSTANTIATION-
specific classifier (Inst. specific), we show results
from two state-of-the-art PDTB discourse parsers
that identify second-level relations: Biran and McK-
eown (2015) (B&M) and Lin et al. (2014). We
also compare the results with the classifier from our
prior work (Li and Nenkova, 2014b) (L&N). In that
work we introduce syntactic production-stick fea-
tures, which minimize the occurrence of zero val-
ues in instance representation. Furthermore, we re-
implemented Brown-cluster features (concatenation
of clusters in each sentence) that have been shown to
perform well in identifying INSTANTIATION’s par-
ent class EXPANSION (Braud and Denis, 2015).4

Table 7 shows the precision, recall, F-measure
and balanced accuracy (average of the accuracies
for the positive and negative class respectively) for
each system. We show balanced accuracy rather
than overall accuracy due to the highly skewed class
distribution. For Inst. specific, we use a threshold
of 0.65 for positive labels5. We also use it along
with L&N for a soft voting classifier where the la-
bel is assigned to the class with larger weighted pos-
terior probability sum from each classifier6. Both
classifiers achieved at least 5% improvement of F-
measure and 8%-10% improvement of balanced ac-
curacy compared to other systems. These improve-
ments mostly come from a dramatic improvement
in recall. The improvement achieved by the vot-
ing classifier also indicate that Inst. specific pro-
vide complementary signals to syntactic production
rules. Note that compared to Lin et al., Inst. spe-
cific behaves very differently in precision and recall,
indicating potential for further system combination.

Finally, we analyze the confusion matrix induced
by false positives and false negatives across Lin
et al., B&M, Inst. specific and soft vote7. In Ta-
ble 8, we list relations contributing at least 10%
to false positives for at least one system. Remark-
ably, INSTANTIATION is consistently confused with

4The dimension of clusters are tuned on the development
set. As in prior work, we use clusters in Turian et al. (2010).

5Tuned on development set.
6The weights are: 0.9 for L&N and 1.0 for Inst. specific,

tuned on development set. We also tried voting with Brown-
concat but it did not outperform combining with L&N.

7For other systems, we did not perform full implicit dis-
course parsing.

1184



Relation Lin et al. B&M Inst. Vote

Restatement 33.3 34.1 28.7 30.8
Cause 22.2 25.0 33.9 33.6

Contrast 22.2 6.8 9.6 10.3
Conjunction 11.1 4.5 6.1 2.8

EntRel 11.1 18.2 17.4 19.6
Table 8: Relations involved in false positives, ≥ 10% for least
one system.

a constant set of relations: RESTATEMENT, CAUSE
and EntRel. Different from other systems, the
Inst. specific classifier demonstrates more confu-
sion towards CAUSE than RESTATEMENT. On the
false negative side, all 62 mistakes are not anno-
tated (i.e., NoRel/EntRel) in Lin et al. For the 58
false negatives in B&M, we observe above 10%:
NoRel/EntRel (56.9%), CAUSE (19.0%), RESTATE-
MENT (13.8%), consistent with relations relations
involved in false positives.

5 Conclusion

We have characterized the implicit INSTANTIATION
relation by studying significant factors that discrim-
inate individual arguments and the sentence pairs
connected by the relation. We show distinctive pat-
terns in sentence length, word usage, semantic rela-
tionships between words as well as cross-argument
and contextual similarity associated with INSTANTI-
ATION. Using these factors as features we demon-
strate significant improvement on the detection of
implicit INSTANTIATION relation.

References
Or Biran and Kathleen McKeown. 2013. Aggregated

word pair features for implicit discourse relation dis-
ambiguation. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics:
Short Papers, pages 69–73.

Or Biran and Kathleen McKeown. 2015. PDTB dis-
course parsing as a tagging task: The two taggers ap-
proach. In Proceedings of the 16th Annual Meeting of
the Special Interest Group on Discourse and Dialogue,
pages 96–104.

Chloé Braud and Pascal Denis. 2015. Comparing word
representations for implicit discourse relation classi-
fication. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing,
pages 2201–2211.

Marie-Catherine de Marneffe, Christopher D. Manning,
and Christopher Potts. 2010. “Was it good? It was
provocative.” Learning the meaning of scalar adjec-
tives. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
167–176.

Christiane Fellbaum. 1998. WordNet. Wiley Online Li-
brary.

Lyn Frazier, Charles Clifton Jr., and Britta Stolterfoht.
2008. Scale structure: Processing minimum standard
and maximum standard scalar adjectives. Cognition,
106(1):299 – 324.

Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2).

Vasileios Hatzivassiloglou and Janyce M. Wiebe. 2000.
Effects of adjective orientation and gradability on sen-
tence subjectivity. In Proceedings of the 18th Confer-
ence on Computational Linguistics - Volume 1, pages
299–305.

Yangfeng Ji and Jacob Eisenstein. 2015. One vector is
not enough: Entity-augmented distributed semantics
for discourse relations. Transactions of the Associa-
tion for Computational Linguistics, 3:329–344.

Andrew Kehler. 2004. Discourse coherence. The hand-
book of pragmatics, pages 241–265.

Junyi Jessy Li and Ani Nenkova. 2014a. Addressing
class imbalance for improved recognition of implicit
discourse relations. In Proceedings of the 15th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 142–150.

Junyi Jessy Li and Ani Nenkova. 2014b. Reducing spar-
sity improves the recognition of implicit discourse re-
lations. In Proceedings of the 15th Annual Meeting of
the Special Interest Group on Discourse and Dialogue,
pages 199–207.

Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 343–351.

Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014.
A PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20:151–184.

Annie Louis and Ani Nenkova. 2010. Creating local
coherence: An empirical assessment. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 313–316.

Annie Louis, Aravind Joshi, and Ani Nenkova. 2010.
Discourse indicators for content selection in summa-
rization. In Proceedings of the SIGDIAL 2010 Confer-
ence, pages 147–156.

1185



Andrew McKinlay and Katja Markert. 2011. Modelling
entity instantiations. In Proceedings of the Interna-
tional Conference Recent Advances in Natural Lan-
guage Processing 2011, pages 268–274.

Eleni Miltsakaki, Livio Robaldo, Alan Lee, and Aravind
Joshi. 2008. Sense annotation in the Penn Discourse
Treebank. In Computational Linguistics and Intelli-
gent Text Processing, volume 4919 of Lecture Notes
in Computer Science, pages 275–286. Springer Berlin
Heidelberg.

Joonsuk Park and Claire Cardie. 2012. Improving im-
plicit discourse relation recognition through feature set
optimization. In Proceedings of the 13th Annual Meet-
ing of the Special Interest Group on Discourse and Di-
alogue, pages 108–112.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word rep-
resentation. Proceedings of the Empirical Methods in
Natural Language Processing, 12:1532–1543.

Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 186–
195.

Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Au-
tomatic sense prediction for implicit discourse rela-
tions in text. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 683–691.

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0. In
Proceedings of the International Conference on Lan-
guage Resources and Evaluation.

Attapol Rutherford and Nianwen Xue. 2014. Discov-
ering implicit discourse relations through brown clus-
ter pair representation and coreference patterns. In
Proceedings of the 14th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 645–654.

Attapol Rutherford and Nianwen Xue. 2015. Improving
the inference of implicit discourse relations via classi-
fying explicit discourse connectives. In Proceedings
of the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 799–808.

Evan Sandhaus. 2008. The New York Times Annotated
Corpus LDC2008T19. Linguistic Data Consortium,
Philadelphia.

Radoslava Trnavac and Maite Taboada. 2013. Discourse
relations and affective content in the expression of sen-
timent in texts. In 11th ICGL Conference–Workshop
on The semantic field of emotions: Interdisci.

Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 384–394.

1186


