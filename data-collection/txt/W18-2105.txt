






































Are we experiencing the
Golden Age of Automatic Post-Editing?

Marcin Junczys-Dowmunt
Microsoft AI and Research

Translation Quality Estimation and Automatic Post-Editing
AMTA 2018

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 144



Why automatic post-editing?

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 145



Why automatic post-editing?

Can’t we just retrain the original system?

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 146



Why automatic post-editing?

Can’t we just retrain the original system?

Not always:

� black-box scenario

� specialized system make better use of PE data (?)

� synergy effects (RB-MT + SMT, SMT + NMT)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 147



Popular metrics:
TER (Translation Error Rate) and BLEU

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 148



Historic APE systems:

Simard et. al (2007). Statistical Phrase-based
Post-editing. NAACL.

� Automatic Post-editing of a rule-based system with a
phrase-based SMT system;

� About 30,000 paragraphs of triples per language pair
(En-Fr/Fr-En);

� Train PB-SMT system on RB-MT output and PE data;

� Chain systems together;

� Impressive gains over the baselines.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 149



Historic APE systems:

Bechara et. al (2011). Statistical Post-Editing for a
Statistical MT System. MT-Summit.

� Automatic Post-editing of a phrase-based SMT with another
phrase-based SMT system.

� Barely any gains over the baselines.

� But interesting idea: Contextual Statistical APE

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 150



Contextual Statistical APE

le#the

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 151



Contextual Statistical APE

le#the programme#programme

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 152



Contextual Statistical APE

le#the programme#programme a#has

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 153



Contextual Statistical APE

le#the programme#programme a#has été#been

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 154



Contextual Statistical APE

le#the programme#programme a#has été#been
mis#implemented

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 155



Contextual Statistical APE

le#the programme#programme a#has été#been
mis#implemented en#implemented

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 156



Contextual Statistical APE

le#the programme#programme a#has été#been
mis#implemented en#implemented application#implemented

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 157



Contextual Statistical APE

le#the programme#programme a#has été#been
mis#implemented en#implemented application#implemented

Problems?

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 158



WMT 2015 Shared Task on Automatic post-editing
(The Stone Age of Automatic post-editing)

ID Avg. TER

Baseline 22.91
FBK Primary 23.23
LIMSI Primary 23.33
USAAR-SAPE 23.43
LIMSI Contrastive 23.57
Abu-MaTran Primary 23.64
FBK Contrastive 23.65
(Simard et al., 2007) 23.84
Abu-MaTran Contrastive 24.72

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 159



WMT 2015 Shared Task on Automatic post-editing
(The Stone Age of Automatic post-editing)

ID Avg. TER

Baseline 22.91
FBK Primary 23.23
LIMSI Primary 23.33
USAAR-SAPE 23.43
LIMSI Contrastive 23.57
Abu-MaTran Primary 23.64
FBK Contrastive 23.65
(Simard et al., 2007) 23.84
Abu-MaTran Contrastive 24.72

WMT2016-best 23.29
WMT2017-best ??

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 160



WMT 2016 Shared Task on Automatic post-editing

Create an APE system that returns automatic post-edition of an
English-German black-box MT system. 10,000 training triplets of
the following form were provided:

SRC These files are encoded as UTF-8 or ASCII , which is a subset of
UTF-8 .

MT Diese Dateien werden als UTF-8 oder ASCII , bei der es sich um eine
Untergruppe von UTF-8 kodiert .

PE Diese Dateien werden als UTF-8 oder ASCII , eine Teilmenge von
UTF-8 , kodiert .

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 161



Problem: very little publicly available PE data

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 162



Source: Koehn and Knowles (2017). Six Challenges for Neural Machine
Translation. 1st Neural Machine Translation Workshop. Vancouver.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 163



Solution: create your own PE data using:

� Official APE training and develsopment data sets.

� EN-DE bilingual data from the WMT-16 shared tasks on IT
and news translation.

� German monolingual Common Crawl (CC) corpus.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 164



Round-trip translation

gibt die Prozesskennung des aktuellen Prozesses zurück . (= PE)

the process ID of the current process . (= SRC)

die Prozess-ID des aktuellen Prozesses . (= MT)

DE-EN Moses

EN-DE Moses

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 165



Selecting in-domain data

� Cross-entropy filtering of German CC corpus based on
in-domain post-editing and IT-domain data.

� We keep 10M sentences with the best cross-entropy scores.

Filtering for TER statistics:

Data set Sent. NumWd WdSh NumEr TER

training set 12K 17.89 0.72 4.69 26.22
development set 1K 19.76 0.71 4.90 24.81

round-trip.full 9,960K 13.50 0.58 5,72 42.02
round-trip.n10 4,335K 15.86 0.66 5.93 36.63
round-trip.n1 531K 20.92 0.55 5.20 25.28

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 166



Experiments with neural models
� Attentional encoded-decoder models trained with Nematus:

https://github.com/rsennrich/nematus

� C++/CUDA AmuNMT decoder:
https://github.com/emjotde/amunmt

MT-PE and SRC-PE systems
� Trained on round-trip.n10 data (4M triplets).

� Fine-tuned on round-trip.n1 and 20x oversampled official
training data (700K triplets).

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 167



0 5 10 15 20 25 30 35 40

20.0

30.0

40.0

50.0

60.0

70.0

n × 10000 iterations

mt-pe
src-pe

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 168



Log-linear combination

� Log-linear combination of two models with different input
languages.

� Weights determined by MERT for two models: ca. 0.8 for
mt-pe and 0.2 for src-pe model.

� Post-Editing Penalty (PEP) to control the faithfulness of the
APE results.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 169



Progress on the dev set

System TER BLEU

Baseline (mt) 25.14 62.92

mt→pe 23.37 66.71
mt→pe×4 23.23 66.88
src→pe 32.31 53.89
src→pe×4 31.42 55.41
mt→pe×4 / src→pe×4 22.38 68.07
mt→pe×4 / src→pe×4 / pep 21.46 68.94

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 170



Automatic evaluation on unseen test set

� AMU (primary) = mt→pe×4 / src→pe×4 / pe
� AMU (contrastive) = mt→pe×4

System TER BLEU

AMU (primary) 21.52 67.65
AMU (contrastive) 23.06 66.09
FBK 23.92 64.75
USAAR 24.14 64.10
CUNI 24.31 63.32
Baseline (Moses) 24.64 63.47
Baseline (mt) 24.76 62.11
DCU 26.79 58.60
JUSAAR 26.92 59.44

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 171



Results of human evaluation

# Score Range System

1 1.967 1 AMU (primary)

2 0.033 2 FBK

3 -0.108 3-4 CUNI
-0.191 3-5 USSAR
-0.211 3-5 Baseline (mt)

4 -0.712 6-7 JUSAAR
-0.778 6-7 DCU

Table: With post-edited sentence shown as reference

Source: WMT2016 overview paper.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 172



Results of human evaluation

# Score Range System

1 2.058 1 Human

2 0.867 2 AMU (primary)

3 -0.213 3-4 CUNI
-0.348 3-6 FBK
-0.374 3-6 USSAR
-0.499 5-7 Baseline (mt)
-0.675 6-8 JUSAAR
-0.816 7-8 DCU

Table: With post-edited sentence included as system

Source: WMT2016 overview paper.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 173



Source: WMT 2016 overview paper

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 174



Some conclusions

� One of the first successful applications of NMT models to
APE

� Artificial APE triplets allow training of NMT models with
little original training data and help against overfitting.

� Positive effects of log-linear combinations of NMT models
with multiple input languages.

� Tuning with MERT to assign model component weights

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 175



WMT 2017 Shared Task on Automatic post-editing

� The same setting;

� Additional 12,500 sentences of PE data;

� Still no post-editing of NMT system

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 176



Our submission to the WMT 2017 Shared Task on
Automatic Post-editing

� We explore the interaction of hard-attention and
multi-encoder models.

� All models trained and available in Marian
(http://marian-nmt.github.io)

� We use the same data as last year.

� This time proper regularization and no need for fine-tuning.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 177



Soft vs. hard monotonic attention

W
äh

len
Si

e
ein

en
Ta

st
at

ur
-

be
-
fe

hl
-

ss
at

z
im M

en
ü

fe
st

leg
en

. EO
S

Wählen
Sie

einen
Tastatur-

be-
fehl-
ssatz

im
Menü
aus

.

EOS

cgru

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 178



Soft vs. hard monotonic attention

W
äh

len
Si

e
ein

en
Ta

st
at

ur
-

be
-
fe

hl
-

ss
at

z
im M

en
ü

fe
st

leg
en

. EO
S

Wählen
Sie

einen
Tastatur-

be-
fehl-
ssatz

im
Menü
aus

.

EOS

gru-hard

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 179



Soft vs. hard monotonic attention

W
äh

len
Si

e
ein

en
Ta

st
at

ur
-

be
-
fe

hl
-

ss
at

z
im M

en
ü

fe
st

leg
en

. EO
S

Wählen
Sie

einen
Tastatur-

be-
fehl-
ssatz

im
Menü
aus

.

EOS

cgru-hard

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 180



Reminder: Gated Recurrent Unit

GRU (s, x) =(1− z)� s+ z� s, (1)
s = tanh (Wx+ r �Us) ,
r = σ (Wrx+Urs) ,

z = σ (Wzx+Uzs) ,

where x is the cell input; s is the previous recurrent state; W, U,
Wr , Ur , Wz , Uz are trained model parameters1; σ is the logistic
sigmoid activation function.

1Biases have been omitted.
Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 181



Conditional GRU (cgru)

C = {h1, . . . ,hn}

sj = cGRUatt (sj−1,E[yj−1],C) (2)

sj ′ =GRU1 (sj−1,E[yj−1])
cj =ATT

(
C, s′j

)
sj =GRU2

(
s′j , cj

)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 182



Hard monotonic attention (gru-hard)

� Aharoni and Goldberg (2016) introduce a simple model for
monolingual morphological re-inflection with hard monotonic
attention.

� The target word vocabulary Vy is extended with a special step
symbol 〈step〉

� Whenever 〈step〉 is predicted as the output symbol, the hard
attention is moved to the next encoder state.

� We calculate the hard attention indices as follows:

a1 = 1,

aj =

{
aj−1 + 1 if yj−1 = 〈step〉
aj−1 otherwise.

sj = GRU
(
sj−1,

[
E[yj−1];haj

])
, (3)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 183



Mixing hard and soft attention (cgru-hard)

sj = cGRUatt
(
sj−1,

[
E[yj−1];haj

]
,C

)
(4)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 184



Example sentence and corrections

mt Wählen Sie einen Tastaturbefehlssatz im Menü festlegen .
src Select a shortcut set in the Set menu .

cgru Wählen Sie einen Tastaturbefehlssatz im Menü aus .
gru-hard Wählen Sie einen Tastaturbefehlssatz im Menü aus .
cgru-hard Wählen Sie einen Tastaturbefehlssatz im Menü aus .
m-cgru Wählen Sie einen Tastaturbefehlssatz im Menü ” Satz ” aus .
m-cgru-hard Wählen Sie einen Tastaturbefehlssatz im Menü ” Satz . ”

pe Wählen Sie einen Tastaturbefehlssatz im Menü ” Satz . ”

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 185



Dual attention

W
äh

len
Si

e
ein

en
Ta

st
at

ur
-

be
-
fe

hl
-

ss
at

z
im M

en
ü

fe
st

leg
en

. EO
S

Wählen
Sie

einen
Tastatur-

be-
fehl-
ssatz

im
Menü

“
Satz

”
aus

.

EOS

mt

Se
lec

t
a sh

or
tc

ut
se

t
in th

e
Se

t
m

en
u

. EO
S

Wählen
Sie
einen
Tastatur-
be-
fehl-
ssatz

im
Menü
“
Satz
”
aus
.

EOS

src

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 186



Dual attention

W
äh

len
Si

e
ein

en
Ta

st
at

ur
-

be
-
fe

hl
-

ss
at

z
im M

en
ü

fe
st

leg
en

. EO
S

Wählen
Sie

einen
Tastatur-

be-
fehl-
ssatz

im
Menü

“
Satz

”
aus

.

EOS

mt

Se
lec

t
a sh

or
tc

ut
se

t
in th

e
Se

t
m

en
u

. EO
S

Wählen
Sie
einen
Tastatur-
be-
fehl-
ssatz

im
Menü
“
Satz
”
aus
.

EOS

src

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 187



Dual soft attention (m-cgru)

Cmt = {hmt1 , . . . ,hmtTmt}
C src = {hsrc1 , . . . ,hsrcTsrc}

s0 = tanh

(
Winit

[∑Tmt
i=1 h

mt
i

Tmt
;

∑Tsrc
i=1 h

src
i

Tsrc

])
.

sj = cGRU2-att
(
sj−1,E[yj−1],Cmt ,Csrc

)
. (5)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 188



Dual soft attention (m-cgru)

sj = cGRU2-att
(
sj−1,E[yj−1],Cmt ,Csrc

)
. (6)

s′j =GRU1 (sj−1,E[yj−1]) ,

cmtj =ATT
(
Cmt , s′j

)
,

csrcj =ATT
(
Csrc , s′j

)
,

cj =
[
cmtj ; c

src
j

]
,

sj =GRU2
(
s′j , cj

)
.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 189



Dual soft attention with hard attention (m-cgru-hard)

sj = cGRU2-att
(
sj−1,

[
E[yj−1];hmtaj

]
,Cmt ,Csrc

)
.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 190



Results

21.52WMT2016 best

15 25
← TER (WMT16)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 191



Results

21.52WMT2016 best

22.27cgru

15 25
← TER (WMT16)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 192



Results

21.52WMT2016 best

22.27cgru

22.72gru-hard

15 25
← TER (WMT16)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 193



Results

21.52WMT2016 best

22.27cgru

22.72gru-hard

22.10cgru-hard

15 25
← TER (WMT16)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 194



Results

21.52WMT2016 best

22.27cgru

22.72gru-hard

22.10cgru-hard

20.69m-cgru

15 25
← TER (WMT16)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 195



Results

21.52WMT2016 best

22.27cgru

22.72gru-hard

22.10cgru-hard

20.69m-cgru

20.87m-cgru-hard

15 25
← TER (WMT16)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 196



Results

21.52WMT2016 best

22.27cgru

22.72gru-hard

22.10cgru-hard

20.69m-cgru

19.92m-cgru ×4
20.87m-cgru-hard

20.34m-cgru-hard ×4

15 25
← TER (WMT16)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 197



Faithfulness

22.27cgru

12.01cgru

22.72gru-hard

9.48gru-hard

22.10cgru-hard

11.57cgru-hard

5 25
← TER (WMT16)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 198



Faithfulness

22.27cgru

12.01cgru

22.72gru-hard

9.48gru-hard

22.10cgru-hard

11.57cgru-hard

20.69m-cgru

15.98m-cgru

20.87m-cgru-hard

13.62m-cgru-hard

5 25
← TER (WMT16)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 199



Results for the WMT2017 shared task on APE

Systems TER BLEU

FBK EnsembleRerank Primary 19.60 70.07
AMU.multi-transducer-composed PRIMARY 19.77 69.50
DCU FRANKENAPE-TUNED PRIMARY 20.11 69.19
USAAR NMT-OSM PRIMARY 23.05 65.01
LIG chained syn PRIMARY 23.22 65.12
JXNU JXNU EDITFreq PRIMARY 23.31 65.66
CUNI char conv rnn beam PRIMARY 24.03 64.28
Official Baseline (MT) 24.48 62.49
Baseline 2 (Statistical phrase-based APE) 24.69 62.97

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 200



Results for the WMT2017 shared task on APE

# Ave % Ave z System

– 84.8 0.520 Human post edit

1 78.2 0.261 AMU
77.9 0.261 FBK
76.8 0.221 DCU

4 73.8 0.115 JXNU

5 71.9 0.038 USAAR
71.1 0.014 CUNI
70.2 -0.020 LIG

– 68.6 -0.083 No post edit

Source: WMT 2017 overview paper

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 201



Results for the WMT2017 shared task on APE

Systems Modified Improved Deteriorated

FBK Primary 1,607 1,035 334
AMU Primary 1,583 1,040 322
DCU Primary 1,592 1,014 361
...

Source: WMT 2017 overview paper

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 202



WMT 2018 Shared Task on Automatic post-editing

� First shared tasks to post-edit NMT output (exciting!)

� Still en-de and IT (not ideal!)

� Domain mis-match between artifical data and NMT (bad!)

� More artifical data (good!)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 203



More guesses

General MT is eating your lunch!

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 204



Justification

� General QE and APE will be gone before translators even
need start worrying;

� QE and APE are bug-fixes that operate within very narrow
error margin (too bad to exploit full error margin);

� This error margin might already be gone in many real-word
applications.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 205



But ... but... it works, right?

Maybe, maybe not. I think we are mostly seeing:

� Favorably chosen test sets, domains and language pairs;

� Synergy effects (different approaches): SMT+NMT

� System combination effects (similar approches);

� Two-pass decoding effects (see MS results);

� Domain-adaptation or style-transfer effects (the last hope!)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 206


	AMTA_2018_Workshop_Proceedings_QEAPE
	Wks3_Front_Material

	AMTA_2018_Workshop_Proceedings_QEAPE_3
	405_update
	JoaoGraca_qeape2018_footer
	MaximKhalilov_qeape2018_footer
	MarcinJunczys-Dowmunt_qeape2018_enlarge_footer
	MarcelloFederico_qeape2018_footer
	406_footer
	403_footer



