










































An Unsupervised Parameter Estimation Algorithm for a Generative Dependency N-gram Language Model


International Joint Conference on Natural Language Processing, pages 516–524,
Nagoya, Japan, 14-18 October 2013.

An Unsupervised Parameter Estimation Algorithm
for a Generative Dependency N-gram Language Model

Chenchen Ding and Mikio Yamamoto
Department of Computer Science

University of Tsukuba
1-1-1 Tennodai, Tsukuba, 305-8573, Japan

{tei@mibel.,myama@}cs.tsukuba.ac.jp

Abstract

We design a language model based on a
generative dependency structure for sen-
tences. The parameter of the model is
the probability of a dependency N-gram,
which is composed of lexical words with
four kinds of extra tags used to model the
dependency relation and valence. We fur-
ther propose an unsupervised expectation-
maximization algorithm for parameter es-
timation, in which all possible dependency
structures of a sentence are considered. As
the algorithm is language-independent, it
can be used on a raw corpus from any lan-
guage, without any part-of-speech annota-
tion, tree-bank or trained parser. We con-
ducted experiments using four languages:
English, German, Spanish and Japanese.
The results illustrate the applicability and
the properties of the proposed approach.

1 Introduction

Statistical language models are a fundamental
component of speech recognition systems, ma-
chine translation systems, and so forth. Presently,
the N-gram language model is the most widely
used approach. This model focuses on sequences
of neighboring lexical words (Fig. 1), and uses the
probabilities of these sequences as model parame-
ters. Due to the full lexicalization of the N-gram
language model, local features of word sequences
can be well modeled. However, an N-gram lan-
guage model cannot capture relatively long-range
features, because it regards a sentence as a flat
string and ignores its structure.

Figure 1: The N-gram language model treats the
English sentence “all things pass” composed of
(all, things) and (things, pass), for N = 2.

Figure 2: The constituency-based parsing (A)
and the dependency-based parsing (B)1 for the
English sentence “all things pass”.

On the other hand, revealing the structure of a
sentence is the task of parsing, which is based on
linguistically oriented formulations and focuses on
generating the likeliest structure for a given sen-
tence. For this purpose, there are constituency-
based and dependency-based formulations (Fig.
2). The former organizes continuous word se-
quences in a hierarchy of small range to large
range groups with linguistically oriented labels;
the latter links words with dependency relations2.

Figure 3: All possible dependency structures for
the English sentence “all things pass”. (I) is the
linguistically correct structure while the original
N-gram language model handles the sentence as
if it has the structure labeled (II). We consider all
these structures in our unsupervised estimation
algorithm.

In this paper, we focus on introducing sentence
structure into language modeling. We propose
a generative dependency N-gram language model
that integrates a generative dependency structure
of a sentence into the original N-gram language
model. We prefer the dependency-based formula-

1For the illustrations in this paper, we use the follow-
ing representation to show the dependency structure. If two
aligned words are on different levels, the upper one is the
head of the lower one; if they are on the same level, they are
siblings.

2In general, the dependency relations can be further clas-
sified using linguistically oriented labels. However, they are
not indispensable and we do not use them in our approach.

516



tion because it can directly model the relations be-
tween words. In the proposed model, the parame-
ter is the probability of the dependency N-gram,
which is a sequence of words along the depen-
dency structure rather than along a flat left-to-right
string. The proposed model is thus as fully lexical
as the original N-gram language model. We fur-
ther propose an expectation-maximization (EM)
algorithm for estimating the probability of arbi-
trary order3 dependency N-grams, by considering
all possible dependency structures4 of a sentence
(Fig. 3). The proposed algorithm is unsupervised,
language-independent and needs no linguistic in-
formation.

2 Related Work

The technical report by Chen and Goodman
(1998) has compared various approaches to the N-
gram language model and the modified Kneser-
Ney discounting proposed in it is still the state-
of-the-art approach. Since the N-gram language
model only captures local lexical features, there
have been proposals to generalize the lexical N-
gram by word-class (Brown et al., 1992) or to
model long-range word co-occurrences by word
triggers (Tillmann and Ney, 1997). However,
these models are unaware of the sentence structure
and basically take a sentence as a flat string.

Many approaches have been proposed for
constituency-based parsing (Collins, 1998; Klein
and Manning, 2003; Klein and Manning, 2004)
and for dependency-based parsing (Eisner, 1996;
Lee and Choi, 1997; Kudo and Matsumoto,
2002; Klein and Manning, 2004; Nivre, 2008).
Presently, discriminative approaches (Kudo and
Matsumoto, 2002; Nivre, 2008) are used more
than generative ones for dependency-based pars-
ing, because a generative model is usually re-
stricted to being bi-lexical (i.e., the components
are bi-grams of head-modifier pairs) and it is hard
to handle more lexical information.

There have been some attempts to inte-
grate sentence structure into language model-
ing. Chelba and Jelinek (2000) have pro-
posed a constituency-based approach, but the use
of language-dependent non-terminals cannot be
avoided. There are also dependency-based ap-
proaches (Stolcke et al., 1997; Gao and Suzuki,
2003; Graham and van Genabith, 2010). However,

3“Order” here means the number of lexical words (N ).
4Only projective dependency structures are considered.

these approaches need a trained dependency parser
because they construct a language model based on
the decisive best structure produced by the parser.

In our approach, we utilize a generative depen-
dency model to guarantee the constituency of a
language model5, but our model and algorithm can
handle arbitrary numbers of lexical words. Fur-
thermore, our approach needs no extra parser to
generate the best structure of a sentence but, in-
stead, takes all possible dependency structures into
consideration.

3 Generative Dependency Model

We model the marginal probability of a sentence
S over the set D of all possible dependency struc-
tures of S, P (S) =

∑
d∈D P (S, d). As de-

scribed in Klein and Manning (2004), if we sep-
arate the dependency structure and lexicalization,
then

∑
d∈D P (S, d) =

∑
d∈D P (d)P (S|d). The

term P (S|d) is given by a model of fully lexical
word sequences with dependency relations. How-
ever, the term P (d) is difficult to model and is usu-
ally taken to be a constant, as in Paskin (2002). To
deal with this problem, the dependency model with
valence (DMV)6 proposed by Klein and Manning
(2004) introduces a special mark STOP. However,
it is necessary to distinguish two kinds of param-
eters, PSTOP and PCHOOSE in the bi-gram esti-
mation, which makes it difficult to extend the ap-
proach to higher orders.

In a similar approach to that used in the DMV,
we introduce four kinds of tags to normalize the
distribution of modifier numbers (the valence) of
a head word. In this paper, we use ⟨L⟩, ⟨/L⟩, ⟨R⟩
and ⟨/R⟩ to show the start and end of the left and
right modifier word sequences of a head word, re-
spectively. The dependency structure can thus be
organized as nested word sequences. Specifically,
a modifier word sequences of a head word is in
a form of M = mϕ+10 ≡ m0,m1, · · · ,mϕ+1,
where m0 ≡ ⟨O⟩, mϕ+1 ≡ ⟨/O⟩ (O ∈ {L,R})
and mϕ1 is a lexical ϕ-word sequence. We show
an example of the dependency structure in Fig. 4.
On the other hand, in contrast to the DMV, we
treat the tags as ordinary words in the parameter
estimation. So the parameters of our model have
a uniform representation, by which our approach
can be easily extended to arbitrary high orders.

5∑
S∈L P (S) = 1 for the set L composed of all the sen-

tences S in a language.
6A generative model.

517



Figure 4: A dependency structure for the English sentence “i get a book from him .”, with ⟨L⟩, ⟨R⟩,
⟨/L⟩, ⟨/R⟩ tags. The root of the sentence is marked as ⟨/s⟩ and for a word without modifiers, its
modifier word sequences are ⟨O⟩⟨/O⟩ where O ∈ {L,R}.

Because our model is essentially equivalent to
the generative Model C in Eisner (1996), the con-
sistence of the language model can be guaran-
teed. That is, ⟨O⟩mϕ1 ⟨/O⟩ (O ∈ {L,R}) is gen-
erated as a Markov sequence to serve as the mod-
ifier word sequences (left/right separately) of the
head word. The “start tag” ⟨O⟩ always satisfies
P (m0 = ⟨O⟩) ≡ 1 to represent the nested struc-
ture. The “end tag” ⟨/O⟩ terminates the genera-
tion process, so: the larger P (mϕ+1 = ⟨/O⟩) is,
the smaller ϕ, which is the number of generated
words, becomes, and vice versa.

Without loss of generality, the probability of
mκ+1 (0 ≤ κ ≤ ϕ) in M = mϕ+10 can be rep-
resented by P (mκ+1|mκ0 ,H), where H is the his-
tory of M along the generated path7. We use the
independent assumption that the probability of a
word in the generation process depends on only its
direct ancestors and the orientation between them.
So, the general probability can be simplified to:

P (h0|o1, h1, . . . , on−1, hn−1) (1)

where hk is a lexical word, hk+1 is the head word
of hk, and ok ∈ {⟨L⟩, ⟨R⟩} is retained in the his-
tory to show the dependency orientation. ⟨/L⟩ and
⟨/R⟩ tags can and only can8 take the place of h0.

The sequence (h0, o1, h1, . . . , on−1, hn−1) in
Exp. (1) is referred as a dependency N-gram in
this paper. For example, a dependency N-gram is
(⟨/L⟩, ⟨L⟩, him, ⟨R⟩, from, ⟨R⟩, get, ⟨L⟩, ., ⟨/s⟩)
in the dependency structure illustrated in Fig. 4.
Exp. (1) is the probability of the dependency N-
gram and thus the parameter of our model, where
the dependency relation and valence are modeled
uniformly for arbitrary order parameters.

7The generation process can be realized in a depth-first or
a breadth-first way but the distinction is unessential.

8Because they cannot have further modifiers.

4 Parameter Estimation

4.1 Notation

For a sentence S = wl+10 ≡ w0, w1, · · · , wl+1,
where w0 ≡ ⟨s⟩ and wl+1 ≡ ⟨/s⟩, a dependency
N-gram (h0, o1, h1, . . . , on−1, hn−1) can be de-
noted by d = (d0, d1, . . . , dn−1) where hk = wdk .
That is, a dependency N-gram can be denoted by
an N-tuple of the absolute positions of words in a
given sentence. As the magnitudes of dk and dk+1
show the orientation, ok+1 can be omitted.9

Lee and Choi (1997) propose the complete-link
set and complete-sequence set for head-modifier
pair (i.e., a dependency bi-gram in our model) to
handle all possible projective dependency struc-
tures of a sentence in a recursive manner. We fol-
low the terms they use and extend their definitions
to adapt them to our dependency N-gram model.
We use Link(d) to denote the complete-link set
of an N-tuple d, and Seq(d) for the complete-
sequence set.

In Lee and Choi (1997), the complete-link set
of a span [i, j] in a sentence is composed of all
possible dependency structures within the span,
with the directional dependency link of the two
words wi and wj . The complete-sequence set of
a span [i, j] is defined as the set of all possible se-
quences with any number (including zero) of ad-
jacent complete-link sets having the same direc-
tion within the span. By our notation, i.e. the
word at d1 is the direct head of the word at d0
for Link(d0, d1); but the word at d1 is an ances-
tor (not only a direct head) of the word at d0 for
Seq(d0, d1). The two kinds of sets can be defined
recursively and the set of all possible dependency

9If h0 is ⟨/L⟩ or ⟨/R⟩, we retain them in d. The orienta-
tions can also be unambiguously omitted for these two tags.

518



Figure 5: Link(d = (i, j)). In Lee and Choi
(1997), for a span [i, j], Link(i, j) is composed
of the dependency link of wi and wj , and all pos-
sible pairs of complete-sequence sets Seq(x, i)
and Seq(x + 1, j).

Figure 6: Seq(d = (i, j)). In Lee and Choi
(1997), for a span [i, j], Seq(i, j) is composed
of all possible pairs of complete-sequence set
Seq(i, x) and complete-link set Link(x, j).

structures of a sentence S = wl+10 is the complete-
sequence set over the span [1, l + 1]. We illustrate
these recursive relations in Fig. 5 and Fig. 6.

Because more than two words are involved in
the proposed dependency N-gram, we generalize
the two kinds of sets for the N-tuples d rather
than just spans. The generalization still retains the
properties of d0 and d1 in Link(d) and Seq(d), as
well as the recursive properties of the two kinds of
sets. We show examples of a dependency tri-gram
in Fig, 7 and Fig. 8.

4.2 Recursive Definition

Here, we give the formulation of the recursive
definition of the complete-link set and complete-
sequence set for an arbitrary order dependency N-
gram. First, due to the properties of the projective
dependency structure, any dk (k ∈ [1, n−1]) in the
N-tuple d = (d0, d1, . . . , dn−1) needs to satisfy
the following constraint to guarantee that a head
word is outside of the range covered by a chain of
its descendants.

dk > max(d0, · · · , dk−1), or
dk < min(d0, · · · , dk−1)

(2)

Trivially, we take ⟨/s⟩ as the root mark of a sen-
tence S = wl+10 , and the ⟨s⟩ as the head of itself
or the ⟨/s⟩. So, we have the following constraints.

dk+1 = 0, if dk = l + 1, or dk = 0 (3)

For convenience, we introduce three kinds of
operations, Push, Cover, and Insert over an in-

Figure 7: Link(d = (i, j, k)). In our model, an
extended high-order (3 is shown here) complete-
link set Link(i, j, k) is composed of the N-tuple
d, and all possible pairs of complete-sequence
sets Seq(x, i, j) and Seq(x + 1, j, k).

Figure 8: Seq(d = (i, j, k)). In our model, an
extended high-order (3 is shown here) complete-
sequence set Seq(i, j, k) is composed of all pos-
sible pairs of complete-sequence set Seq(i, x, j)
and complete-link set Link(x, j, k).

dex x (absolute word position) and an N-tuple
d = (d0, d1, . . . , dn−1):

Push(x,d) = (x, d0, d1, . . . , dn−2) (4)

Cover(x,d) = (x, d1, d2, . . . , dn−1) (5)

Insert(x,d) = (d0, x, d2, . . . , dn−1) (6)

Then, the Link(d) and Seq(d) can be defined by
Exp. (7) and Exp. (9) below, where “×” indicates
the direct product of sets.

Link(d) =
∪

if d1=l+1, then i=d1−1;
else i∈[min(d0,d1), max(d0,d1)−1]

{Seq(Left(i,d))×
Seq(Right(i + 1,d)) × d}

(7)

where

(Left ,Right) ={
(Push,Cover), if d0 < d1

(Cover ,Push), if d0 > d1

(8)

Seq(d) =
∪

i∈[min(d0,d1), max(d0,d1)]
and i̸=d1

{Seq(Insert(i,d))×
Link(Cover(i,d))}

(9)

519



Exp. (7) shows that a complete-link set is re-
cursively composed of the direct product of all
possible complete-sequence set pairs, with the N-
tuple d itself.10 Exp. (9) shows that a complete-
sequence set is recursively composed of the direct
product of all possible pairs of a complete-link set
and a smaller complete-sequence set.

To start the recursive definition, we replace d0
by ⟨/L⟩ and ⟨/R⟩ for all Seq(d) with d0 = d111.

Left(x,d) = Left(⟨/R⟩,d),
if x = min(d0, d1) in Exp. (7)

(10)

Right(x,d) = Right (⟨/L⟩,d),
if x = max(d0, d1) in Exp. (7)

(11)

Insert(x,d) = Push(⟨/L⟩,d),
if x = d0, and d0 < d1 in Exp. (9)

(12)

Insert(x,d) = Push(⟨/R⟩,d),
if x = d0, and d0 > d1 in Exp. (9)

(13)

4.3 Estimation

According to the recursive definition, it is natu-
ral to derive an inside-outside algorithm (Lari and
Young, 1990), which is an adaption of the EM al-
gorithm (Dempster et al., 1977) to tree structures,
to conduct parameter re-estimation by calculating
the inside and outside probabilities of all complete
sets in sentences.

We generalize the expressions in Exp. (7) and
Exp. (9) to Exp. (14) and Exp. (15) respectively,
where the notation ⟨·, ·⟩ stands for an unordered
2-tuple of a complete-set pair.

Link(d) =
∪

∀⟨Sub1,Sub2⟩

{Sub1 × Sub2 × d} (14)

Seq(d) =
∪

∀⟨Sub1,Sub2⟩

{Sub1 × Sub2} (15)

We further define RLink(Link(d), ⟨Sub1,Sub2⟩)
as a relation for Link(d), ⟨Sub1,Sub2⟩ satisfying
Exp. (14). Similarly, RSeq(Seq(d), ⟨Sub1,Sub2⟩)
is a relation for Seq(d), ⟨Sub1,Sub2⟩ satisfying
Exp. (15). Then, the inside probability β and out-
side probability α of the two kinds of complete
sets can be calculated by Exp. (16) to Exp. (19),

10We further restrict the root mark ⟨/s⟩ to take only one
modifier (the situation when d1 = l + 1 in Exp. (7)), accord-
ing to the general restrictions of the dependency grammar.

11From the restriction in Exp. (2), d0 should not be equal
to d1. This is only possible for those Seq(d) at the start of the
recursive definition, where the word at d0 is actually a ⟨/L⟩
tag or a ⟨/R⟩ tag, which does not have an absolute position
in a sentence.

where p(d) is the probability of the lexical depen-
dency N-gram represented by d in a sentence.

β(Link(d)) =∑
⟨Sub1 ,Sub2⟩, s.t.

RLink(Link(d),⟨Sub1,Sub2⟩)

β(Sub1)β(Sub2)p(d) (16)

β(Seq(d)) =∑
⟨Sub1,Sub2⟩, s.t.

RSeq(Seq(d),⟨Sub1,Sub2⟩)

β(Sub1)β(Sub2) (17)

α(Link(d)) =∑
⟨Sup,Con⟩, s.t.

RSeq(Sup,⟨Link(d),Con⟩)

α(Sup)β(Con) (18)

α(Seq(d)) =∑
⟨Sup,Con⟩, s.t.

RLink(Sup,⟨Seq(d),Con⟩)

α(Sup)β(Con)p(d′)

+
∑

⟨Sup,Con⟩, s.t.
RSeq(Sup,⟨Seq(d),Con⟩)

α(Sup)β(Con)

(where d′ is the N-tuple of Sup)

(19)

Specifically, Exp. (16) and Exp. (17) can be di-
rectly derived from the definitions of Exp. (7) and
Exp. (9), respectively. Further, a complete-link set
can only be a component of a complete-sequence
set from Exp. (9), while a complete-sequence set
can be both a component of a complete-link set
from Exp. (7), and a component of a complete-
sequence set from Exp. (9). As a result, Exp. (18)
and Exp. (19) can be derived respectively.

For all Seq(d) with ⟨/L⟩ or ⟨/R⟩, we use:

β(Seq(d)) = p(d) (20)

as the start of the calculation. At the end of the
calculation, the probability of the whole sentence
S = wl+10 can be obtained as:

P (S) = β(Seq(d = (1, l + 1, 0, · · · , 0))) (21)

For the re-estimation, we can get the probabilis-
tic counts12 of a dependency N-gram represented
by d in a sentence using:

β(Link(d)) · α(Link(d)) · P (S)−1 13 (22)

according to the inside-outside algorithm. Finally,
all the counts of a dependency N-gram in the train-
ing corpus are added and normalized using Exp.
(1), to update the model parameters.

12They are no longer integers.
13For the situation in Exp. (20), we use β(Seq(d))·α(Seq(d))

P (S)
.

520



5 Experiments

5.1 Experiment Setting

Corpus

As the proposed dependency N-gram model and
estimation algorithm are language-independent,
we conduct experiments using four different lan-
guages: English, German, Spanish and Japanese.
The corpora we use for English, German, and
Spanish are the sets of sentences with 5 – 15 words
from the corresponding single-language corpora
of Europarl14 (Koehn, 2005). The corpus for
Japanese is the set of sentences with 5 – 20 words
from the Japanese side of the NTCIR-8 corpus
(Fujii et al., 2010). We take one two-hundredth
of the sentences from a corpus to form each of the
development and test sets used in experiments, and
the remaining sentences are used for training. The
details of training, development and test sets are
shown in Tables 1 and 2.

language sentences types tokens
English 400, 100 40, 913 4, 355, 333
German 422, 951 105, 303 4, 545, 263
Spanish 370, 791 58, 314 4, 007, 816
Japanese 477, 118 47, 930 7, 758, 437

Table 1: The training sets.

language development set test set
English 2, 020 2, 021
German 2, 136 2, 136
Spanish 1, 872 1, 873
Japanese 2, 409 2, 410

Table 2: The numbers of sentences in develop-
ment and test sets.

Parameter Collection and Initialization

In order to investigate the fundamental properties
of the model and algorithm, we do not use any
pruning or approximating methods in the param-
eter estimation. Specifically, we collect from the
raw corpora all possible lexical dependency N-
grams15 without any cut-off thresholds for models
of every order. Before estimation, we use relative
frequency to initialize the probabilities.

14http://www.statmt.org/europarl/
15As Japanese is a typical head-final language, that is, the

head word always comes after its modifiers, we only take the
left-oriented (from head to modifier) dependency links into
account. For the other three languages, dependency links of
both two orientations are considered. The parameter collec-
tion and initialization do not take the structure into account.

5.2 Results

Algorithm Convergence

Figure 9: The English training set perplexities
before each iteration. (The y-axis is logarith-
mic.)

Figure 9 shows the change of English training
set perplexities before each iteration by the pro-
posed estimation algorithm, for 2 (bi-) and 3 (tri-)
order dependency N-gram models. The conver-
gence trend along with the iteration times can be
observed. For the dependency bi-gram, the train-
ing set perplexity becomes nearly stable after 5 it-
erations. However, for the dependency tri-gram,
the first iteration already reaches a very low train-
ing set perplexity and it does not change much in
further iterations. This phenomenon suggests that
the non-pruned dependency tri-gram model may
already be too complex a model with too many pa-
rameters, so the features of the training set are rep-
resented well, resulting in a low perplexity. This
suggests the model is over-fitting the data. We dis-
cuss this in Sec. 5.3.

Test Set Perplexity

As well as the training set perplexity, the perplex-
ity of a test set which has not been used in parame-
ter estimation should be investigated in evaluation.
Because different order dependency N-gram mod-
els are trained separately, we use linear interpola-
tion in calculating the test set perplexity. Specifi-
cally, we use the hand-out development set to tune
the interpolation coefficients (weights) and to se-
lect the iteration times of different order models
to minimize the development set perplexity. Then
we use the tuned weights to combine the iteration-
time-selected models in the test set perplexity cal-
culation. The reason for using simple and straight-
forward linear interpolation is also that we want
to discover the essential aspects of the proposed
model and algorithm, so we use no further smooth-
ing approaches. As the lowest order of a depen-
dency N-gram is two, we use a uni-gram model
with modified Kneser-Ney discounting to handle
the unknown words. The uni-gram model is inter-
polated with the dependency bi-gram model. Fur-

521



language dev-ppl (bi / tri) test-ppl (bi / tri) iter bi iter tri λuni λbi λtri
English 145 / 143 159 / 156 6 1 0.93 0.99 0.13
German 268 / 256 265 / 261 12 1 0.88 0.98 0.04
Spanish 165 / 164 159 / 158 7 1 0.92 0.99 0.04
Japanese (left-only) 88 / 67 88 / 67 4 1 0.86 0.99 0.70

Table 3: The development set perplexities (dev-ppl) and test set perplexities (test-ppl) of dependency
N-gram models (N = 2 (bi), 3 (tri)). The iteration times in dependency bi- and tri-gram model training
are iterbi and iter tri , respectively. The weights of uni-gram, dependency bi- and tri-gram models are
λuni , λbi and λtri . (1 − λbi) and (1 − λtri) are assigned to the interpolated lower order models and
(1− λuni) is assigned to the ⟨/L⟩ and ⟨/R⟩ tags.

thermore, as the ⟨/L⟩ tag and ⟨/R⟩ tag are taken
as general words but they never really appear in a
training set, we treat them separately, and interpo-
late them with the uni-gram model.

language MLE (bi / tri) MKN (bi / tri)
English 162 / 457 157 / 86
German 396 / 1371 252 / 139
Spanish 176 / 499 161 / 86
Japanese 62 / 87 91 / 39

Table 4: The test set perplexities of the original
N-gram models. MLE is the maximum likeli-
hood estimation realized by setting the adding
delta to 0 in adding smoothing. MKN is the in-
terpolated modified Kneser-Ney discounting.

In Table 3, we show the development and test
set perplexities of the linear-interpolated depen-
dency bi- and tri-gram models. For comparison,
we used SRILM16 (Stolcke, 2002) to build two
original N-gram language models on the same
training sets: one is constructed by maximum
likelihood estimation without any smoothing, the
other one is constructed by state-of-the-art inter-
polated modified Kneser-Ney discounting. We
calculate the test set perplexities of the two N-
gram language models on the same test sets. The
results are listed in Table 4. In both Table 3 and
Table 4, the perplexities are calculated according
to the number of lexical words, and the tags used
for normalization are not counted17. We discuss
these results in Sec. 5.3.

16http://www.speech.sri.com/projects/
srilm/

17That is, we do not count the ⟨/s⟩ tag in the original N-
gram language models, or ⟨/L⟩ and ⟨/R⟩ in our models. If
they are included, the perplexities decrease. In the original N-
gram model, this is because a ⟨/s⟩ tag nearly always appears
after the period mark. The effect is even more dramatic in
our model, as each word in a sentence has a ⟨/L⟩ and a ⟨/R⟩
tag to normalize its modifier numbers, so the token number
in a sentence is multiplied by. Therefore, we only count the
lexical words in perplexity calculation for fairness.

5.3 Discussion

Parameter Number

For a sentence with l words, the number of de-
pendency N-gram that can be collected increases
exponentially as O(lN ) if we consider all possible
combinations. Although for a given N , the pro-
posed algorithm takes a time which is polynomial
in the sentence length l, a large N will be practi-
cally intractable, especially for long sentences. In
Fig. 10, we show the numbers of complete sets
of different order dependency N-gram models for
different sentence lengths.

Figure 10: The numbers of complete sets. (The
y-axis is logarithmic.)

This behavior is also related to the over-fitting
problem because our algorithm is essentially an it-
erative maximum likelihood estimation. A model
that is too complex will be too specific to the train-
ing set. From Table 3, we see that the perfor-
mance of a dependency tri-gram model will sat-
urate after only one iteration, which is also indi-
cated in Fig. 9, and does little to improve the
test set perplexities. The exception is Japanese,
where the dependency tri-gram does improve the
performance. The linguistic reason for this is that
Japanese is a head-final language with a simpler
syntactic structure, so we restrict the dependency
link in Japanese to “left only”, which leads to a
model with fewer parameters. Consequently, the
high order model performs better. From the ex-
perimental results, we can see that the proposed
algorithm has the usual strengths and weaknesses
of an EM algorithm.

522



Figure 11: The best dependency structure of the
English sentence “i would , however , add one
important caveat .”

Figure 12: The best dependency structure of the
German sentence “trotzdem möchte ich der kom-
mission einige fragen stellen .”

Test Set Perplexity

Comparing the test set perplexities in Tables 3
and 4, we can see the dependency bi-gram model
achieves the same, or sometimes better perfor-
mance of the original N-gram language models.
However, when we look at the tri-grams, the in-
terpolated modified Kneser-Ney (KN) discount-
ing method, which is state-of-the-art, shows its
strength and our dependency model does not pro-
duce much improvement for the reasons we de-
scribed above18. As the modified KN method uses
an efficient discounting to avoid the over-fitting
problem, and our model has no smoothing, the dif-
ference in performance is reasonable for complex
models. On the other hand, the generally com-
petitive results of our bi-gram model and its per-
formance on Japanese show that our model is a
promising one, particularly if the number of pa-
rameters can be reduced.

Model Preference

In Fig. 11 to Fig. 14, we present examples of the
best dependency structures generated by our ap-
proach of sentences in test sets. We used the set-
tings in Table 3 and generated them by the Viterbi
algorithm (Viterbi, 1967). It can be seen the pro-
posed approach can reveal features of specific lan-
guages even though it is unsupervised: e.g. the
final-position verb “stellen” and its relation with
the second-position auxiliary verb “möchte” in the
German sentence. The results also show a prefer-
ence for associating semantic relations and mak-
ing the function words19 of a language the mod-
ifiers of the content words. For example, in the
Spanish sentence, syntactically the preposition “a”

18For Japanese, the result is improved by the dependency
tri-gram model, but the original tri-gram model with interpo-
lated modified KN discounting method performs much better.

19Articles, prepositions, etc.

Figure 13: The best dependency structure of the
Spanish sentence “la comisión está haciendo
muchas cosas a este respecto .”

Figure 14: The best dependency structure of the
Japanese sentence “図 3 は、 その 実際 の 配
置例である。”

is the head of the noun “respecto”, but in unsu-
pervised training, our model prefers to assign “a”
to be the modifier of “respecto” and directly link
two content words: “respecto” and the verb “ha-
ciendo”. We think this is because the probabili-
ties of ⟨/L⟩ and ⟨/R⟩ tags have large estimates,
especially when they appear after function words,
which prevents them from having modifiers20.

6 Conclusion and Future Work

In this paper, we proposed a generative depen-
dency N-gram language model and the definition
of the complete sets for arbitrary order, by which
an unsupervised parameter estimation algorithm is
facilitated. The experimental results demonstrate
the applicability and the properties of the proposed
approach. In future work, we will develop meth-
ods of parameter pruning and discounting to han-
dle the over-fitting problem. As the the proposed
dependency language model is intrinsically com-
plex, we also plan to do more fundamental sim-
plifications. On the other hand, although our pro-
posed algorithm is unsupervised, the output of a
trained parser, which can provide clear and lexical
heuristics, can be integrated in it. We will investi-
gate this possibility and evaluate the performance
by linguistically motivated criteria.

Acknowledgment

We would like to thank anonymous reviewers for
their valuable comments and suggestions. This
work was supported by JSPS KAKENHI Grant
Number 24650063.

20This tendency, however, is correct for articles, such as
the “der” in German and “la” in Spanish.

523



References

Peter F. Brown, Peter V. deSouza, Robert L. Mercer,
Vicent J. Della Pietra, and Jenifer C. Lai. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467–479.

Ciprian Chelba and Frederick Jelinek. 2000. Struc-
tured language modeling. Computer Speech and
Language, 14(4):283–332.

Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report, TR-10-98, Computer
Science Group, Harvard Univ.

Michael Collins. 1998. Three generative, lexicalised
models for statistical parsing. In Proc. of ACL 1998,
pages 16–23.

Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin . 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), 39(1):1–
38.

Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: an exploration. In Proc.
of COLING 1996, pages 340–345.

Atsushi Fujii, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, Terumasa Ehara, Hiroshi Echizen-
ya, and Sayori Shimohata. 2010. Overview of the
patent translation task at the NTCIR-8 workshop. In
Proc. of NTCIR-8, pages 371–376.

Jianfeng Gao and Hisami Suzuki. 2003. Unsupervised
learning of dependency structure for language mod-
eling. In Proc. of ACL 2003, pages 521–580.

Yvette Graham and Josef van Genabith. 2010. Deep
syntax language models and statistical machine
translation. In Proc. of SSST at COLING 2010,
pages 118–126.

Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proc. of ACL 2003,
pages 423–430.

Dan Klein and Chirstopher D. Manning. 2004.
Corpus-based induction of syntactic structure: mod-
els of dependency and constituency. In Proc. of ACL
2004, pages 478–485.

Philipp Koehn. 2005. Europarl: a parallel corpus for
statistical machine translation. In Proc. of MT sum-
mit 2005, pages 79–86.

Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc. of COLING 2002, pages 1–7.

K. Lari and S. J. Young. 1990. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
4(1):35–56.

Seungmi Lee and Key-Sun Choi. 1997. Reestimation
and best-first parsing algorithm for probabilistic de-
pendency grammars. In Proc. of WVLC 1997, pages
41–55.

Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513–553.

Mark A. Paskin. 2002. Grammatical digrams. Ad-
vances In Neural Information Processing Systems
14, 1:91–97.

Andreas Stolcke, Ciprian Chelba, David Engle, Victor
Jimenez, Lidia Mangu, Harry Printz, Eric Ristad,
Roni Rosenfeld, Dekai Wu, Frederick Jelinek, and
Sanjeev Khudanpur. 1997. Dependency language
modeling.

Andreas Stolcke. 2002. SRILM–an extensible lan-
guage modeling toolkit. In Proc. of ICSLP 2002,
pages 901–904.

Christoph Tillmann and Hermann Ney. 1997. Word
triggers and the EM algorithm. In Proc. of CoNNL
1997, pages 117–124.

Andrew Viterbi. 1967. Error bounds for convolutional
codes and an asymptotically optimum decoding al-
gorithm. Information Theory, IEEE Transactions on
Information Theory, 13(2):260–269.

524


