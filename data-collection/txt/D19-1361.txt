



















































Fine-tune BERT with Sparse Self-Attention Mechanism


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3548–3553,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

3548

Fine-tune BERT with Sparse Self-Attention Mechanism

Baiyun Cui, Yingming Li∗, Ming Chen, and Zhongfei Zhang
College of Information Science and Electronic Engineering,

Zhejiang University, China
baiyunc@yahoo.com,{yingming,funkyblack,zhongfei}@zju.edu.cn

Abstract

In this paper, we develop a novel Sparse
Self-Attention Fine-tuning model (referred as
SSAF) which integrates sparsity into self-
attention mechanism to enhance the fine-
tuning performance of BERT. In particular,
sparsity is introduced into the self-attention
by replacing softmax function with a control-
lable sparse transformation when fine-tuning
with BERT. It enables us to learn a structurally
sparse attention distribution, which leads to
a more interpretable representation for the
whole input. The proposed model is evalu-
ated on sentiment analysis, question answer-
ing, and natural language inference tasks. The
extensive experimental results across multiple
datasets demonstrate its effectiveness and su-
periority to the baseline methods.

1 Introduction

Recently, the pre-trained language models obtain
new state-of-the-art results on a broad range of
tasks, for example ULMFiT (Howard and Ruder,
2018), ELMo (Peters et al., 2018), and Ope-
nAI GPT (Radford et al., 2018). Devlin et al.
(2018) proposed BERT, a deep bidirectional lan-
guage representation model which substantially
outperforms the previous methods and has at-
tracted wide attention in natural language process-
ing. Fine-tuning on the pre-trained BERT has
shown to be beneficial to improve many down-
stream tasks such as document classification (Ad-
hikari et al., 2019), extractive summarization (Liu,
2019), question answering (Yang et al., 2019), nat-
ural language inference (Liu et al., 2019), and
reading comprehension (Xu et al., 2019).

To further understand the impact of BERT on
the fine-tuning performance, we exploit the atten-
tion behavior of the model when fine-tuning BERT

∗Corresponding author

Figure 1: The explorations of attention behavior for
four models: No pre-training, Fine-tuning BERT,
SSAF(No pre-training), and SSAF. All the attention
matrices are extracted from the eighth attention layer of
the model for the sentence in Sentube-A dataset. Words
on the y-axis are attending to the words on the x-axis.
The attention distribution in SSAF presents more struc-
turally sparse patterns compared to the others.

in sentiment analysis task. In particular, the at-
tention matrices of models with different setups
are visualized in Figure 1. As shown in Figure
1(b), the attention distribution of fine-tuning with
BERT is close to being a band matrix, with interest
concentrated on small regions near the diagonal.
Compared to the model trained from scratch with-
out pre-training (Figure 1(a)), more interpretabil-
ity would be provided by fine-tuning with BERT
due to its structural attention distribution. This dif-
ference in the distributions might be caused by the
masked language model task of BERT, where the
model is required to predict the original vocabu-



3549

lary id of the masked token based on its context.
Consequently, the local interactions among differ-
ent words are strengthened and play more impor-
tant roles in predicting the masked token. Such
modeling makes the learned representation more
expressive for a diversity of tasks. However, the
current fine-tuning models usually employ the pre-
trained BERT as initialization of the networks and
do not pay enough attention on how to dynami-
cally control this attention distribution to be struc-
turally sparse during fine-tuning process, which
might further help improve the interpretability of
the framework.

To overcome the above limitation, in this work,
we develop a novel Sparse Self-Attention Fine-
tuning model (referred as SSAF) by integrat-
ing sparsity into self-attention to refine the at-
tention distribution when fine-tuning the pre-
trained BERT. Specifically, we propose sparse
self-attention mechanism (SSAM) to induce spar-
sity by replacing the traditional softmax func-
tion with a sparse transformation in self-attention
networks. Instead of covering all the connec-
tions built between different words as the origi-
nal fine-tuning model, the sparsity is designed to
promote the most essential relationships among
them with higher attention weights, and mean-
while eliminate the influence from meaningless re-
lations by truncating their probabilities to exactly
0. As presented in Figure 1(d), more sparse atten-
tion structure are obtained by SSAF through fine-
tuning BERT with sparsity constraint. Even with-
out the pre-training, the sparse self-attention net-
works (SSAF No pre-training) also behaves better
than the traditional self attention version in respect
to sparsity, which is shown in Figure 1(c). As a
generic module, SSAM is flexible to be applied to
neural network models in a wide variety of tasks.

Extensive experiments are conducted on three
NLP tasks to investigate the performances of
SSAF, which include sentiment analysis, question
answering, and natural language inference. Evalu-
ation results on seven public datasets show that the
proposed approach achieves remarkable improve-
ments over other competing models.

2 Fine-tuning with Sparse Self-Attention

In this section, we first clarify the traditional
self-attention model, then present a sparse self-
attention mechanism, which extends the self-
attention by replacing the standard softmax func-

tion with a sparse transformation, and finally de-
velop a sparse self-attention fine-tuning model.

2.1 Self-attention mechanism

We start by introducing self-attention mechanism,
which is the foundation of Transformer encoder
(Vaswani et al., 2017) as well as BERT. Self-
attention networks is capable of directly relating
tokens at different positions from the sequence by
computing the attention score (relevance) between
each pair of tokens.

Formally, given an input sequence x =
(x1, · · · xL), the output representation y =
(y1, · · · yL) is constructed by applying weighted
sum of transformations of the input elements
x based on the relevance, where the elements
{xi, yi} ∈ Rd. The i-th output yi is computed as:

yi =
∑L

j=1 αi j(xjWv) (1)

ei j =
(xiWq )(x jWk )T√

d
, αi j = ρ(ei j) (2)

where Wq ∈ Rd×d, Wk ∈ Rd×d, and Wv ∈ Rd×d
denote the trainable parameter matrices. ρ is a
probability mapping function. The resulting atten-
tion weight αi j represents the relevance between
the i-th and j-th input element.

The classical choice for ρ is the softmax trans-
formation, which is calculated as:

ρ(ei j) = softmax(ei j) =
exp(ei j)∑L
t=1 exp(eit )

(3)

However, since softmax function is strictly pos-
itive, it produces attention distribution with full
support. This results in the dense dependencies
between each pair of words and fails to assign ex-
actly zero probability to less meaningful relation-
ships. Further, putting nonzero weight on every
relationship would also degrade the interpretabil-
ity. With such softmax transformation, the self-
attention networks does not pay more attention to
those important connections while also being eas-
ily disturbed by many unrelated words.

2.2 Sparse Self-Attention Mechanism

To address this problem, we employ sparsegen-
lin transformation (Laha et al., 2018) to replace
softmax in Equation 3, which not only leads to
a sparse probability distribution but also offers an
explicit control over the degree of sparsity.

Sparsegen-lin formulation projects the attention
scores ei = (ei1, ei2, · · · eiL) onto the probability



3550

simplex pi and introduces coefficient λ < 1 to in-
fluence the regularization strength:

ρ(ei) = argmin
pi ∈∆L−1

{
| |pi − ei | |2 − λ | |pi | |2

}
(4)

where ∆L−1 :=
{
pi ∈ RL |

∑L
j=1 pi j = 1, pi ≥ 0

}
.

The sparse attention distribution is then computed
as follows:

ρ(ei j) = pi j = max
{
0,

ei j − τ(ei)
1 − λ

}
(5)

where j ∈ {1, · · · L} and τ : RL → R is the thresh-
old function.

More specifically, let ei(1) ≥ ei(2) ≥ · · · ≥ ei(L)
be the sorted attention scores of ei and k(ei) :=
max

{
k ∈ {1, · · · , L} |1 − λ + kei(k) >

∑
j≤k ei(j)

}
.

The threshold τ(ei) is obtained as:

τ(ei) =
(∑j≤k(ei ) ei(j)) − 1 + λ

k(ei)

=
(∑j∈S(ei ) ei j) − 1 + λ

|S(ei)|

(6)

where S(ei) is the support of ρ(ei), i.e., a set of
the indices of nonzero coordinates. As in Equa-
tion 5, all the coordinates in the S(ei) will be
modified, and the others will be truncated to zero,
thus providing a sparse solution. The choice of
λ helps control the cardinality of the support set
S(ei)which influences the sparsity of attention dis-
tribution.

By introducing sparsity to refine the atten-
tion weight, our sparse self-attention mechanism
(SSAM) strengthens the most important relations
among different words such as local interactions,
and assigns zero probability to those meaningless
connections. This enables us to achieve a more
expressive representation for the whole input.

2.3 Sparse Self-Attention Fine-tuning model
In this part, we propose a sparse self-attention
fine-tuning model (SSAF). In particular, this fine-
tuning model with BERT is composed of N sparse
self-attention layers, where each layer learns a rep-
resentation by taking the output from the previous
layer:

h̃n = LN(hn−1 + SSAM(hn−1)) (7)
hn = LN(h̃n + FFN(h̃n)) (8)

where SSAM is adopted to replace the traditional
self-attention mechanism, h0 = embed(x) denotes
the representation for the input sequence x which
is the sum of token embeddings and the position

embeddings, and LN is the layer normalization
operation.

Relationships with existing methods Al-
though several sparse formulations have been
developed in the literature such as sparsemax
(Martins and Astudillo, 2016), fusedmax (Niculae
and Blondel, 2017), and constrained sparsemax
(Malaviya et al., 2018), they are mostly applied in
the classification layer or in the attention-based
encoder-decoder architecture. Instead, in this
work, we introduce sparsity into self-attention
based transformer encoder. Other than concen-
trating on words as the existing approaches do,
we take the advantage of sparsity to identify
the most essential relationships among words to
capture a better sequence representation. Further,
more interpretability would be obtained by our
method due to the structurally sparse attention
distribution.

3 Experiments

In the following sections, we empirically evaluate
the effectiveness of SSAF for three NLP tasks on
seven public datasets.

3.1 Datasets

Sentiment Analysis (SA) The goal of sentiment
analysis is to determine the sentiment classifica-
tion of a piece of text. Accuracy is used as the
evaluation metric. We evaluate our model on five
datasets in this task, which are listed as follows. 1)
SST-1: The Stanford Sentiment Treebank (Socher
et al., 2013) consists of sentences extracted from
movie reviews with five classes. We follow the
previous works (Kim, 2014; Gong et al., 2018)
to train the model on both phrase and sentence
level; 2) SST-2: This dataset is constructed from
the same data with SST-1 but without the neutral
reviews. We adopt the dataset version provided by
GLUE (Wang et al., 2018); 3) SemEval: The Se-
mEval 2013 Twitter dataset (Nakov et al., 2013)
contains tweets with three classes: positive, neg-
ative, and neutral; 4) Sentube-A, Sentube-T: The
SenTube datasets (Uryupina et al., 2014) are texts
obtained from YouTube comments with two senti-
ment classes: positive and negative.
Question Answering (QA) This task is to predict
the answer text span in the paragraph according to
the question. We adopt the SQuAD v1.1 dataset
(Rajpurkar et al., 2016). Since BERTBASE is re-



3551

Corpus Task Train Dev. Test Class
SST-1 SA 8.5k 1.1k 2.2k 5
SST-2 SA 67.3k 0.8k 1.8k 2
Sentube-A SA 3.3k 0.2k 0.9k 2
Sentube-T SA 4.9k 0.3k 1.3k 2
SemEval SA 6.0k 0.8k 2.3k 3
SQuAD QA 87.5k 10.1k 10.1k -
SciTail NLI 23.5k 1.3k 2.1k 2

Table 1: Summary of seven datasets used in our exper-
iments. Train, Dev., and Test: The size of train, devel-
opment, and test set respectively.

ported on the development set of SQuAD in De-
vlin et al. (2018), we follow them to evaluate our
model on the same set. The Exact Match (EM)
and F1 are two evaluation metrics.
Natural Language Inference (NLI) The task in-
volves assessing if two sentences entail or contra-
dict each other. We use SciTail dataset (Khot et al.,
2018) which is derived from a science question an-
swering (SciQ) dataset. The evaluation metric is
accuracy.

Further statistics about datasets are illustrated in
Table 1.

3.2 Implementation details

We adopt the pre-trained BERTBASE 1 as the ba-
sis for our experiments. We choose BERTBASE
in our work rather than another larger pre-trained
model BERTLARGE due to the resource limitations
and computation cost. The coefficient λ which
controls the sparisty in Equation 4 is set to -3 in
SST-1 and SemEval, -4 in SST-2 and SenTube-T,
-6 in SenTube-A and SciTail, and -7 in SQuAD.
We investigate the influence of different λ settings
in the experiment analysis part. We adopt Adam
as our optimizer with a learning rate of 2e-5 and
batch size is 16. The maximum number of training
epoch is 2 for SQuAD, 3 for SciTail, 4 for SST-1
and SST-2, 5 for SenTube-A, 6 for SenTube-T, and
8 for SemEval.

3.3 Baselines

To demonstrate that SSAF truly improves the fine-
tuning performance, we compare SSAF against
BERTBASE from (Devlin et al., 2018) on all the
tasks. In particular, for sentiment analysis, we
follow (Ambartsoumian and Popowich, 2018) to
make comparisons with the following representa-

1https://github.com/google-research/bert

Models SST-1 SST-2 SenTube-A SenTube-T SemEval

Ave 42.3 81.1 61.5 64.3 63.6

LSTM 46.5 82.9 57.4 63.6 67.6

BiLSTM 47.1 83.7 59.3 66.2 65.1

CNN 41.9 81.8 57.3 62.1 63.5

SSAN 48.6 85.3 62.5 68.4 72.2

Np 50.1 85.2 66.8 69.6 70.0

SSAF(Np) 50.7 86.4 68.1 70.3 71.2

BERTBASE 55.2 93.5 70.3 73.3 76.2

SSAF 56.2 94.7 72.4 75.0 77.3

Table 2: Experimental results of classification accuracy
for different methods with five datasets on sentiment
analysis task.

tive methods: Ave, LSTM, BiLSTM, and CNN
from (Barnes et al., 2017); SSAN from (Am-
bartsoumian and Popowich, 2018). We report
the results of these five models with Google
300-dimensional word2vec embeddings 2 on all
datasets. For SST-1 and SST-2, we reproduce
the compared methods according to the different
dataset version.

For thorough comparison, besides the ap-
proaches proposed in the existing literature, we
further implement another two models to verify
the ability of sparse self-attention mechanism:
No pre-training (Np): It has the same archi-
tecture as BERTBASE but without pre-training.
SSAF(Np): We train SSAF from scratch only with
the sparse self-attention module.

3.4 Results

The experimental results on sentiment analysis
are reported in Table 2, and the performances on
question answering and natural language inference
are summarized in Table 3. Results show that
SSAF achieves the best performance across all
three tasks with a significant improvement over the
previous approaches.

Compared with other competing methods in the
experiments, BERTBASE provides a strong base-
line owing to the pre-training procedure. How-
ever, this model still suffers from its ineffi-
ciency by taking all the relationships between each
pair of words into consideration when construct-
ing the representation. Our model outperforms
BERTBASE by a stable margin especially in SST-
2, SenTube-A, SenTube-T, and SciTail, with the
improvements of 1.2%, 2.1%, 1.7%, and 1.5%,

2https://code.google.com/archive/p/word2vec/



3552

Models SQuAD SciTail
EM F1 Accuracy

Np 65.3 74.1 78.6
SSAF(Np) 66.2 74.6 78.9
BERTBASE 80.8 88.5 92.0
SSAF 81.6 88.8 93.5

Table 3: Experimental results on question answering
and natural language inference tasks.

Figure 2: Experimental results on varying the coeffi-
cient λ for SSAF in SciTail and SenTube-T datasets.
Grey dash line denotes the result of BERTBASE.

respectively. Meanwhile, even without the pre-
training, SSAF(No pre-training) still surpasses No
pre-training across the board which clearly proves
the effectiveness and universality of incorporating
sparsity into self-attention model. Combining the
strength of sparsity and the pre-trained language
model, SSAF stands out in performance with other
baselines.

Moreover, we study how the value of coefficient
λ in SSAF affects the fine-tuning performance.
We explore different settings from -8 to 0. Fig-
ure 2 shows the comparison results on the test
set of SciTail and SenTube-T. Results summarize
that λ = −6 is superior to other settings for Sc-
iTail and the performance on SenTube-T reaches
the best when λ is set to -4. According to the
experimental results above, we could found that
on both of the datasets, the values from -7 to -
3 can reach comparable performance while both
larger and smaller values are more likely to de-
crease the accuracy to some extent though they
still outperform previous work by a large margin.
To understand why different values of λ have such
different effects, we visualize the attention matri-
ces extracted from SSAF with a series of λ and
also visualize the one from BERTBASE for com-
parison. As shown in Figure 3, the different struc-
tural patterns among these five sub-figures lie in
the different strength of the sparsity constraint. It
shows that too large coefficient λ makes the atten-

Figure 3: Visualization of attention matrices for
BERTBASE and SSAF with different values of λ on
SenTube-T dataset. With higher λ, the attention dis-
tribution in SSAF becomes sparser.

tion distribution of SSAF extremely sparse, which
may even overlooks some important interactions
between different words. With the decrease of λ ,
the influence from other unnecessary connections
increases, which reduces the interpretability of the
model. Thus, it is important to have appropriate
parameter to encourage a desirably sparse atten-
tion distribution while achieving competing accu-
racy.

4 Conclusion

In this paper, we develop a novel Sparse Self-
Attention Fine-tuning model (referred as SSAF)
which integrates sparsity into self-attention mech-
anism to enhance the fine-tuning performance of
BERT. We conduct extensive experiments on sen-
timent analysis, question answering, and natu-
ral language inference tasks with seven public
datasets. The proposed approach substantially im-
proves the performance over the strong baseline
methods, demonstrating its effectiveness and uni-
versality while achieving higher interpretability
for the framework.

Acknowledgments

This work was supported in part by National Nat-
ural Science Foundation of China (No. 61702448,
61672456), Zhejiang Lab (2018EC0ZX01-2),
the Fundamental Research Funds for the Cen-
tral Universities in China (No. 2017FZA5007,
2019FZA5005), the Key Program of Zhejiang
Province, China (No. 2015C01027), Artificial In-
telligence Research Foundation of Baidu Inc., the
funding from HIKVision and Horizon Robotics,
and ZJU Converging Media Computing Lab. We
thank all reviewers for their valuable comments.

References
Ashutosh Adhikari, Achyudh Ram, Raphael Tang, and

Jimmy Lin. 2019. Docbert: Bert for document clas-
sification. arXiv preprint arXiv:1904.08398.



3553

Artaches Ambartsoumian and Fred Popowich. 2018.
Self-attention: A better building block for sentiment
analysis neural network classifiers. arXiv preprint
arXiv:1812.07860.

Jeremy Barnes, Roman Klinger, and Sabine Schulte im
Walde. 2017. Assessing state-of-the-art sentiment
models on state-of-the-art sentiment datasets. arXiv
preprint arXiv:1709.04219.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Jingjing Gong, Xipeng Qiu, Shaojing Wang, and Xu-
anjing Huang. 2018. Information aggregation via
dynamic routing for sequence encoding. arXiv
preprint arXiv:1806.01501.

Jeremy Howard and Sebastian Ruder. 2018. Universal
language model fine-tuning for text classification. In
ACL, pages 328–339.

Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018.
Scitail: A textual entailment dataset from science
question answering. In AAAI.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882.

Anirban Laha, Saneem Ahmed Chemmengath,
Priyanka Agrawal, Mitesh Khapra, Karthik
Sankaranarayanan, and Harish G Ramaswamy.
2018. On controllable sparse alternatives to
softmax. In NeurIPS, pages 6422–6432.

Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-
feng Gao. 2019. Multi-task deep neural networks
for natural language understanding. arXiv preprint
arXiv:1901.11504.

Yang Liu. 2019. Fine-tune BERT for extractive sum-
marization. arXiv preprint arXiv:1903.10318.

Chaitanya Malaviya, Pedro Ferreira, and André F. T.
Martins. 2018. Sparse and constrained attention for
neural machine translation. In ACL, pages 370–376.

Andre Martins and Ramon Astudillo. 2016. From soft-
max to sparsemax: A sparse model of attention and
multi-label classification. In ICML, pages 1614–
1623.

Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wil-
son. 2013. Semeval-2013 task 2: Sentiment
analysis in twitter. In Proceedings of the 7th
International Workshop on Semantic Evaluation,
SemEval@NAACL-HLT, pages 312–320.

Vlad Niculae and Mathieu Blondel. 2017. A regular-
ized framework for sparse and structured neural at-
tention. In NeurIPS, pages 3338–3348.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In NAACL, pages 2227–2237.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training. URL https://s3-
us-west-2. amazonaws. com/openai-assets/research-
covers/languageunsupervised/language under-
standing paper. pdf.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In EMNLP, pages
2383–2392.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In EMNLP, pages 1631–1642.

Olga Uryupina, Barbara Plank, Aliaksei Severyn,
Agata Rotondi, and Alessandro Moschitti. 2014.
Sentube: A corpus for sentiment analysis on youtube
social media. In LREC, pages 4244–4249.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NeurIPS, pages 5998–6008.

Alex Wang, Amapreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: A multi-task benchmark and analysis platform
for natural language understanding. arXiv preprint
arXiv:1804.07461.

Hu Xu, Bing Liu, Lei Shu, and Philip S Yu. 2019.
Bert post-training for review reading comprehension
and aspect-based sentiment analysis. arXiv preprint
arXiv:1904.02232.

Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen
Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.
End-to-end open-domain question answering with
bertserini. arXiv preprint arXiv:1902.01718.

http://aclweb.org/anthology/S/S13/S13-2052.pdf
http://aclweb.org/anthology/S/S13/S13-2052.pdf

