



















































Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1118–1127
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

1118

Multi-Turn Response Selection for Chatbots with Deep Attention
Matching Network

Xiangyang Zhou∗, Lu Li∗, Daxiang Dong, Yi Liu, Ying Chen,
Wayne Xin Zhao†, Dianhai Yu and Hua Wu

Baidu Inc., Beijing, China{
zhouxiangyang, lilu12, dongdaxiang, liuyi05,

chenying04, v zhaoxin, yudianhai, wu hua

}
@baidu.com

Abstract

Human generates responses relying on se-
mantic and functional dependencies, in-
cluding coreference relation, among dia-
logue elements and their context. In this
paper, we investigate matching a response
with its multi-turn context using depen-
dency information based entirely on atten-
tion. Our solution is inspired by the re-
cently proposed Transformer in machine
translation (Vaswani et al., 2017) and we
extend the attention mechanism in two
ways. First, we construct representations
of text segments at different granularities
solely with stacked self-attention. Second,
we try to extract the truly matched seg-
ment pairs with attention across the con-
text and response. We jointly introduce
those two kinds of attention in one uni-
form neural network. Experiments on two
large-scale multi-turn response selection
tasks show that our proposed model sig-
nificantly outperforms the state-of-the-art
models.

1 Introduction

Building a chatbot that can naturally and con-
sistently converse with human-beings on open-
domain topics draws increasing research interests
in past years. One important task in chatbots is
response selection, which aims to select the best-
matched response from a set of candidates given
the context of a conversation. Besides playing a
critical role in retrieval-based chatbots (Ji et al.,
2014), response selection models have been used
in automatic evaluation of dialogue generation

∗Equally contributed.
†Work done as a visiting scholar at Baidu. Wayne Xin

Zhao is an associate professor of Renmin University of China
and can be reached at batmanfly@ruc.edu.cn.

(Lowe et al., 2017) and the discriminator of GAN-
based (Generative Adversarial Networks) neural
dialogue generation (Li et al., 2017).

Conversation Context

Speaker A: Hi I am looking to see what packages are installed on my system,  
  I don’t see a path, is the list being held somewhere else? 
Speaker B: Try dpkg - get-selections 
Speaker A: What is that like? A database for packages instead of  a flat file  
structure? 
Speaker B: dpkg is the debian package manager - get-selections simply shows  
you what packages are handed by it 

Response of Speaker A: No clue what do you need it for, its just reassurance 
as I don’t know the debian package manager

Figure 1: Example of human conversation on Ubuntu sys-
tem troubleshooting. Speaker A is seeking for a solution of
package management in his/her system and speaker B recom-
mend using, the debian package manager, dpkg. But speaker
A does not know dpkg, and asks a backchannel-question
(Stolcke et al., 2000), i.e., “no clue what do you need it for?”,
aiming to double-check if dpkg could solve his/her problem.
Text segments with underlines in the same color across con-
text and response can be seen as matched pairs.

Early studies on response selection only use
the last utterance in context for matching a reply,
which is referred to as single-turn response selec-
tion (Wang et al., 2013). Recent works show that
the consideration of a multi-turn context can fa-
cilitate selecting the next utterance (Zhou et al.,
2016; Wu et al., 2017). The reason why richer
contextual information works is that human gen-
erated responses are heavily dependent on the pre-
vious dialogue segments at different granularities
(words, phrases, sentences, etc), both semanti-
cally and functionally, over multiple turns rather
than one turn (Lee et al., 2006; Traum and Hee-
man, 1996). Figure 1 illustrates semantic con-
nectivities between segment pairs across context
and response. As demonstrated, generally there
are two kinds of matched segment pairs at dif-
ferent granularities across context and response:
(1) surface text relevance, for example the lexi-
cal overlap of words “packages”-“package” and
phrases “debian package manager”-“debian pack-



1119

age manager”. (2) latent dependencies upon which
segments are semantically/functionally related to
each other. Such as the word “it” in the response,
which refers to “dpkg” in the context, as well as
the phrase “its just reassurance” in the response,
which latently points to “what packages are in-
stalled on my system”, the question that speaker
A wants to double-check.

Previous studies show that capturing those
matched segment pairs at different granularities
across context and response is the key to multi-
turn response selection (Wu et al., 2017). How-
ever, existing models only consider the textual
relevance, which suffers from matching response
that latently depends on previous turns. More-
over, Recurrent Neural Networks (RNN) are con-
veniently used for encoding texts, which is too
costly to use for capturing multi-grained seman-
tic representations (Lowe et al., 2015; Zhou et al.,
2016; Wu et al., 2017). As an alternative, we
propose to match a response with multi-turn con-
text using dependency information based entirely
on attention mechanism. Our solution is inspired
by the recently proposed Transformer in machine
translation (Vaswani et al., 2017), which addresses
the issue of sequence-to-sequence generation only
using attention, and we extend the key attention
mechanism of Transformer in two ways:

self-attention By making a sentence attend to it-
self, we can capture its intra word-level de-
pendencies. Phrases, such as “debian pack-
age manager”, can be modeled with word-
level self-attention over word-embeddings,
and sentence-level representations can be
constructed in a similar way with phrase-
level self-attention. By hierarchically stack-
ing self-attention from word embeddings, we
can gradually construct semantic representa-
tions at different granularities.

cross-attention By making context and response
attend to each other, we can generally capture
dependencies between those latently matched
segment pairs, which is able to provide com-
plementary information to textual relevance
for matching response with multi-turn con-
text.

We jointly introduce self-attention and cross-
attention in one uniform neural matching network,
namely the Deep Attention Matching Network

(DAM), for multi-turn response selection. In prac-
tice, DAM takes each single word of an utter-
ance in context or response as the centric-meaning
of an abstractive semantic segment, and hierar-
chically enriches its representation with stacked
self-attention, gradually producing more and more
sophisticated segment representations surround-
ing the centric-word. Each utterance in context
and response are matched based on segment pairs
at different granularities, considering both textual
relevance and dependency information. In this
way, DAM generally captures matching informa-
tion between the context and the response from
word-level to sentence-level, important matching
features are then distilled with convolution & max-
pooling operations, and finally fused into one sin-
gle matching score via a single-layer perceptron.

We test DAM on two large-scale public multi-
turn response selection datasets, the Ubuntu Cor-
pus v1 and Douban Conversation Corpus. Exper-
imental results show that our model significantly
outperforms the state-of-the-art models, and the
improvement to the best baseline model on R10@1
is over 4%. What is more, DAM is expected
to be convenient to deploy in practice because
most attention computation can be fully paral-
lelized (Vaswani et al., 2017). Our contributions
are two-folds: (1) we propose a new matching
model for multi-turn response selection with self-
attention and cross-attention. (2) empirical results
show that our proposed model significantly out-
performs the state-of-the-art baselines on public
datasets, demonstrating the effectiveness of self-
attention and cross-attention.

2 Related Work

2.1 Conversational System

To build an automatic conversational agent is a
long cherished goal in Artificial Intelligence (AI)
(Turing, 1950). Previous researches include task-
oriented dialogue system, which focuses on com-
pleting tasks in vertical domain, and chatbots,
which aims to consistently and naturally converse
with human-beings on open-domain topics. Most
modern chatbots are data-driven, either in a fash-
ion of information-retrieval (Ji et al., 2014; Banchs
and Li, 2012; Nio et al., 2014; Ameixa et al.,
2014) or sequence-generation (Ritter et al., 2011).
The retrieval-based systems enjoy the advantage
of informative and fluent responses because it
searches a large dialogue repository and selects



1120

Input Representation Matching Aggregation

Word
Embedding Representation Module

!
Word-word Matching 
with Cross-Attention

"#

"$

"%

Matching
Score

g(c,r)

3D Matching
Image Q

Ui

R
Multi-grained

Representations

Mui,rself M
ui,r
cross

Figure 2: Overview of Deep Attention Matching Network.

candidate that best matches the current context.
The generation-based models, on the other hand,
learn patterns of responding from dialogues and
can directly generalize new responses.

2.2 Response Selection

Researches on response selection can be generally
categorized into single-turn and multi-turn. Most
early studies are single-turn that only consider the
last utterance for matching response (Wang et al.,
2013, 2015). Recent works extend it to multi-
turn conversation scenario, Lowe et al.,(2015) and
Zhou et al.,(2016) use RNN to read context and
response, use the last hidden states to represent
context and response as two semantic vectors, and
measure their relevance. Instead of only consider-
ing the last states of RNN, Wu et al.,(2017) take
hidden state at each time step as a text segment
representation, and measure the distance between
context and response via segment-segment match-
ing matrixes. Nevertheless, matching with depen-
dency information is generally ignored in previous
works.

2.3 Attention

Attention has been proven to be very effective in
Natural Language Processing (NLP) (Bahdanau
et al., 2015; Yin et al., 2016; Lin et al., 2017) and
other research areas (Xu et al., 2015). Recently,
Vaswani et al.,(2017) propose a novel sequence-
to-sequence generation network, the Transformer,

which is entirely based on attention. Not only
Transformer can achieve better translation results
than convenient RNN-based models, but also it is
very fast in training/predicting as the computation
of attention can be fully parallelized. Previous
works on attention mechanism show the superior
ability of attention to capture semantic dependen-
cies, which inspires us to improve multi-turn re-
sponse selection with attention mechanism.

3 Deep Attention Matching Network

3.1 Problem Formalization
Given a dialogue data set D = {(c, r, y)Z}NZ=1,
where c = {u0, ..., un−1} represents a conversa-
tion context with {ui}n−1i=0 as utterances and r as
a response candidate. y ∈ {0, 1} is a binary la-
bel, indicating whether r is a proper response for
c. Our goal is to learn a matching model g(c, r)
with D, which can measure the relevance between
any context c and candidate response r.

3.2 Model Overview
Figure 2 gives an overview of DAM, which
generally follows the representation-matching-
aggregation framework to match response with
multi-turn context. For each utterance ui =
[wui,k]

nui−1
k=0 in a context and its response candi-

date r = [wr,t]nr−1t=0 , where nui and nr stand for
the numbers of words, DAM first looks up a shared
word embedding table and represents ui and r as
sequences of word embeddings, namely U0i =



1121

[e0ui,0, ..., e
0
ui,nui−1

] and R0 = [e0r,0, ..., e0r,nr−1]
respectively, where e ∈ Rd denotes a d-dimension
word embedding.

A representation module then starts to construct
semantic representations at different granularities
for ui and r. Practically, L identical layers of
self-attention are hierarchically stacked, each lth

self-attention layer takes the output of the l − 1th
layer as its input, and composites the input se-
mantic vectors into more sophisticated represen-
tations based on self-attention. In this way, multi-
grained representations of ui and r are gradually
constructed, denoted as [Uli]Ll=0 and [R

l]Ll=0 re-
spectively.

Given [U0i , ...,ULi ] and [R0, ...,RL], utterance
ui and response r are then matched with each
other in a manner of segment-segment similar-
ity matrix. Practically, for each granularity l ∈
[0...L], two kinds of matching matrixes are con-
structed, i.e., the self-attention-match Mui,r,lself and
cross-attention-match Mui,r,lcross , measuring the rele-
vance between utterance and response with textual
information and dependency information respec-
tively.

Those matching scores are finally merged into
a 3D matching image Q1. Each dimension of Q
represents each utterance in context, each word
in utterance and each word in response respec-
tively. Important matching information between
segment pairs across multi-turn context and can-
didate response is then extracted via convolution
with max-pooling operations, and further fused
into one matching score via a single-layer percep-
tron, representing the matching degree between
the response candidate and the whole context.

Specifically, we use a shared component,
the Attentive Module, to implement both self-
attention in representation and cross-attention in
matching. We will discuss in detail the implemen-
tation of Attentive Module and how we used it to
implement both self-attention and cross-attention
in following sections.

3.3 Attentive Module
Figure 3 shows the structure of Attentive Mod-
ule, which is similar to that used in Transformer
(Vaswani et al., 2017). Attentive Module has
three input sentences: the query sentence, the key
sentence and the value sentence, namely Q =
[ei]

nQ−1
i=0 ,K = [ei]

nK−1
i=0 ,V = [ei]

nV−1
i=0 respec-

1We refer to it as Q because it is like a cube.

query

Attention Weighted Sum

key value

Sum & Norm

Feed-Forward

Sum & Norm

Figure 3: Attentive Module.

tively, where nQ, nK and nV denote the number
of words in each sentence and ei stands for a d-
dimension embedding, nK is equal to nV . The At-
tentive Module first takes each word in the query
sentence to attend to words in the key sentence
via Scaled Dot-Product Attention (Vaswani et al.,
2017), then applies those attention results upon the
value sentence, which is defined as:

Att(Q,K) =
[
softmax(

Q[i] · KT√
d

)
]nQ−1
i=0

(1)

Vatt = Att(Q,K) · V ∈ RnQ×d (2)

where Q[i] is the ith embedding in the query sen-
tence Q. Each row of Vatt, denoted as Vatt[i],
stores the fused semantic information of words in
the value sentence that possibly have dependen-
cies to the ith word in query sentence. For each i,
Vatt[i] and Q[i] are then added up together, com-
positing them into a new representation that con-
tains their joint meanings. A layer normalization
operation (Ba et al., 2016) is then applied, which
prevents vanishing or exploding of gradients. A
feed-forward network FFN with RELU (LeCun
et al., 2015) activation is then applied upon the
normalization result, in order to further process the
fused embeddings, defined as:

FFN(x) = max(0, xW1 + b1)W2 + b2 (3)

where, x is a 2D-tensor in the same shape of query
sentence Q and W1, b1,W2, b2 are learnt parame-
ters. This kind of activation is empirically useful
in other works, and we also adapt it in our model.
The result FFN(x) is a 2D-tensor that has the same
shape as x, FFN(x) is then residually added (He
et al., 2016) to x, and the fusion result is then nor-
malized as the final outputs. We refer to the whole
Attentive Module as:

AttentiveModule(Q,K,V) (4)



1122

As described, Attentive Module can capture de-
pendencies across query sentence and key sen-
tence, and further use the dependency information
to composite elements in the query sentence and
the value sentence into compositional representa-
tions. We exploit this property of the Attentive
Module to construct multi-grained semantic rep-
resentations as well as match with dependency in-
formation.

3.4 Representation
Given U0i or R0, the word-level embedding rep-
resentations for utterance ui or response r, DAM
takes U0i ro R0 as inputs and hierarchically stacks
the Attentive Module to construct multi-grained
representations of ui and r, which is formulated
as:

Ul+1i = AttentiveModule(U
l
i,U

l
i,U

l
i) (5)

Rl+1 = AttentiveModule(Rl,Rl,Rl) (6)

where l ranges from 0 to L − 1, denoting the dif-
ferent levels of granularity. By this means, words
in each utterance or response repeatedly function
together to composite more and more holistic rep-
resentations, we refer to those multi-grained rep-
resentations as [U0i , ...,ULi ] and [R0, ...,RL] here-
after.

3.5 Utterance-Response Matching
Given [Uli]Ll=0 and [R

l]Ll=0, two kinds of segment-
segment matching matrixes are constructed at each
level of granularity l, i.e., the self-attention-match
Mui,r,lself and cross-attention-match M

ui,r,l
cross . M

ui,r,l
self

is defined as:

Mui,r,lself = {U
l
i[k]

T · Rl[t]}nui×nr (7)

in which, each element in the matrix is the dot-
product of Uli[k] and Rl[t], the kth embedding in
Uli and the tth embedding in Rl, reflecting the tex-
tual relevance between the kth segment in ui and
tth segment in r at the lth granularity. The cross-
attention-match matrix is based on cross-attention,
which is defined as:

Ũ
l

i = AttentiveModule(U
l
i,R

l,Rl) (8)

R̃
l
= AttentiveModule(Rl,Uli,U

l
i) (9)

Mui,r,lcross = {Ũ
l

i[k]
T
· R̃l[t]}nui×nr (10)

where we use Attentive Module to make Uli and
Rl crossly attend to each other, constructing two

new representations for both of them, written as
Ũ
l

i and R̃
l

respectively. Both Ũ
l

i and R̃
l

implicitly
capture semantic structures that cross the utterance
and response. In this way, those inter-dependent
segment pairs are close to each other in represen-
tations, and dot-products between those latently
inter-dependent pairs could get increased, provid-
ing dependency-aware matching information.

3.6 Aggregation

DAM finally aggregates all the segmental match-
ing degrees across each utterance and response
into a 3D matching image Q, which is defined as:

Q = {Qi,k,t}n×nui×nr (11)

where each pixel Qi,k,t is formulated as:

Qi,k,t =

[Mui,r,lself [k, t]]
L
l=0 ⊕ [Mui,r,lcross [k, t]]Ll=0

(12)

⊕ is concatenation operation, and each pixel has
2(L + 1) channels, storing the matching degrees
between one certain segment pair at different lev-
els of granularity. DAM then leverages two-
layered 3D convolution with max-pooling opera-
tions to distill important matching features from
the whole image. The operation of 3D convo-
lution with max-pooling is the extension of typi-
cal 2D convolution, whose filters and strides are
3D cubes2. We finally compute matching score
g(c, r) based on the extracted matching features
fmatch(c, r) via a single-layer perceptron, which
is formulated as:

g(c, r) = σ(W3fmatch(c, r) + b3) (13)

where W3 and b3 are learnt parameters, and σ is
sigmoid function that gives the probability if r is a
proper candidate to c. The loss function of DAM
is the negative log likelihood, defined as:

p(y|c, r) = g(c, r)y + (1− g(c, r))(1− y) (14)
L(·) = −

∑
(c,r,y)∈D

log(p(y|c, r)) (15)

4 Experiment

2https://www.tensorflow.org/api docs/python/tf/nn/conv3d



1123

Ubuntu Corpus Douban Conversation Corpus
R2@1 R10@1 R10@2 R10@5 MAP MRR P@1 R10@1 R10@2 R10@5

DualEncoderlstm 0.901 0.638 0.784 0.949 0.485 0.527 0.320 0.187 0.343 0.720
DualEncoderbilstm 0.895 0.630 0.780 0.944 0.479 0.514 0.313 0.184 0.330 0.716
MV-LSTM 0.906 0.653 0.804 0.946 0.498 0.538 0.348 0.202 0.351 0.710
Match-LSTM 0.904 0.653 0.799 0.944 0.500 0.537 0.345 0.202 0.348 0.720
Multiview 0.908 0.662 0.801 0.951 0.505 0.543 0.342 0.202 0.350 0.729
DL2R 0.899 0.626 0.783 0.944 0.488 0.527 0.330 0.193 0.342 0.705
SMNdynamic 0.926 0.726 0.847 0.961 0.529 0.569 0.397 0.233 0.396 0.724
DAM 0.938 0.767 0.874 0.969 0.550 0.601 0.427 0.254 0.410 0.757
DAMfirst 0.927 0.736 0.854 0.962 0.528 0.579 0.400 0.229 0.396 0.741
DAMlast 0.932 0.752 0.861 0.965 0.539 0.583 0.408 0.242 0.407 0.748
DAMself 0.931 0.741 0.859 0.964 0.527 0.574 0.382 0.221 0.403 0.750
DAMcross 0.932 0.749 0.863 0.966 0.535 0.585 0.400 0.234 0.411 0.733

Table 1: Experimental results of DAM and other comparison approaches on Ubuntu Corpus V1 and
Douban Conversation Corpus.

4.1 Dataset

We test DAM on two public multi-turn response
selection datasets, the Ubuntu Corpus V1 (Lowe
et al., 2015) and the Douban Conversation Corpus
(Wu et al., 2017). The former one contains multi-
turn dialogues about Ubuntu system troubleshoot-
ing in English and the later one is crawled from a
Chinese social networking on open-domain topics.
The Ubuntu training set contains 0.5 million multi-
turn contexts, and each context has one positive re-
sponse that generated by human and one negative
response which is randomly sampled. Both vali-
dation and testing sets of Ubuntu Corpus have 50k
contexts, where each context is provided with one
positive response and nine negative replies. The
Douban corpus is constructed in a similar way to
the Ubuntu Corpus, except that its validation set
contains 50k instances with 1:1 positive-negative
ratios and the testing set of Douban corpus is con-
sisted of 10k instances, where each context has 10
candidate responses, collected via a tiny inverted-
index system (Lucene3), and labels are manually
annotated.

4.2 Evaluation Metric

We use the same evaluation metrics as in pre-
vious works (Wu et al., 2017). Each compari-
son model is asked to select k best-matched re-
sponse from n available candidates for the given
conversation context c, and we calculate the re-
call of the true positive replies among the k se-
lected ones as the main evaluation metric, denoted
as Rn@k =

∑k
i=1 yi∑n
i=1 yi

, where yi is the binary la-
bel for each candidate. In addition to Rn@k,
we use MAP (Mean Average Precision) (Baeza-

3https://lucenent.apache.org/

Yates et al., 1999), MRR (Mean Reciprocal Rank)
(Voorhees et al., 1999), and Precision-at-one P@1
especially for Douban corpus, following the set-
ting of previous works (Wu et al., 2017).

4.3 Comparison Methods
RNN-based models : Previous best performing

models are based on RNNs, we choose
representative models as baselines, includ-
ing SMNdynamic(Wu et al., 2017), Multi-
view(Zhou et al., 2016), DualEncoderlstm
and DualEncoderbilstm (Lowe et al., 2015),
DL2R (Yan et al., 2016), Match-LSTM
(Wang and Jiang, 2017) and MV-LSTM
(Pang et al., 2016), where SMNdynamic
achieves the best scores against all the other
published works, and we take it as our state-
of-the-art baseline.

Ablation : To verify the effects of multi-grained
representation, we setup two comparison
models, i.e., DAMfirst and DAMlast, which
dispense with the multi-grained representa-
tions in DAM, and use representation re-
sults from the 0th layer and Lth layer of
self-attention instead. Moreover, we setup
DAMself and DAMcross, which only use
self-attention-match or cross-attention-match
respectively, in order to examine the ef-
fectiveness of both self-attention-match and
cross-attention-match.

4.4 Model Training
We copy the reported evaluation results of all base-
lines for comparison. DAM is implemented in
tensorflow4, and the used vocabularies, word em-

4https://www.tensorflow.org. Our code and data will be
available at https://github.com/baidu/Dialogue/DAM



1124

bedding sizes for Ubuntu corpus and Douban cor-
pus are all set as same as the SMN (Wu et al.,
2017). We consider at most 9 turns and 50 words
for each utterance (response) in our experiments,
word embeddings are pre-trained using training
sets via word2vec (Mikolov et al., 2013), simi-
lar to previous works. We use zero-pad to han-
dle the variable-sized input and parameters in FFN
are set to 200, same as word-embedding size. We
test stacking 1-7 self-attention layers, and reported
our results with 5 stacks of self-attention because
it gains the best scores on validation set. The
1st convolution layer has 32 [3,3,3] filters with
[1,1,1] stride, and its max-pooling size is [3,3,3]
with [3,3,3] stride. The 2nd convolution layer has
16 [3,3,3] filters with [1,1,1] stride, and its max-
pooling size is also [3,3,3] with [3,3,3] stride. We
tune DAM and the other ablation models with
adam optimizer (Le et al., 2011) to minimize loss
function defined in Eq 15. Learning rate is initial-
ized as 1e-3 and gradually decreased during train-
ing, and the batch-size is 256. We use validation
sets to select the best models and report their per-
formances on test sets.

4.5 Experiment Result

Table 1 shows the evaluation results of DAM as
well as all comparison models. As demonstrated,
DAM significantly outperforms other competitors
on both Ubuntu Corpus and Douban Conversa-
tion Corpus, including SMNdynamic, which is the
state-of-the-art baseline, demonstrating the supe-
rior power of attention mechanism in matching re-
sponse with multi-turn context. Besides, both the
performances of DAMfirst and DAMself decrease
a lot compared with DAM, which shows the effec-
tiveness of self-attention and cross-attention. Both
DAMfirst and DAMlast underperform DAM,
which demonstrates the benefits of using multi-
grained representations. Also the absence of
self-attention-match brings down the precision, as
shown in DAMcross, exhibiting the necessity of
jointly considering textual relevance and depen-
dency information in response selection.

One notable point is that, while DAMfirst is
able to achieve close performance to SMNdynamic,
it is about 2.3 times faster than SMNdynamic in our
implementation as it is very simple in computa-
tion. We believe that DAMfirst is more suitable
to the scenario that has limitations in computation
time or memories but requires high precise, such

as industry application or working as an compo-
nent in other neural networks like GANs.

5 Analysis

We use the Ubuntu Corpus for analyzing how self-
attention and cross-attention work in DAM from
both quantity analysis as well as visualization.

5.1 Quantity Analysis

We first study how DAM performs in different ut-
terance number of context. The left part in Fig-
ure 4 shows the changes of R10@1 on Ubuntu
Corpus across contexts with different number of
utterance. As demonstrated, while being good at
matching response with long context that has more
than 4 utterances, DAM can still stably deal with
short context that only has 2 turns.

av
er

ag
e 

nu
m

be
r o

f w
or

ds
 in

 e
ac

h 
tu

rn

number of turns in context

Figure 4: DAM’s performance on Ubuntu Corpus across
different contexts. The left part shows the performance in
different utterance number of context. The right part shows
performance in different average utterance text length of con-
text as well as self-attention stack depth.

Moreover, the right part of Figure 4 gives the
comparison of performance across different con-
texts with different average utterance text length
and self-attention stack depth. As demonstrated,
stacking self-attention can consistently improve
matching performance for contexts having differ-
ent average utterance text length, implying the
stability advantage of using multi-grained seman-
tic representations. The performance of matching
short utterances, that have less than 10 words, is
obviously lower than the other longer ones. This
is because the shorter the utterance text is, the
fewer information it contains, and the more dif-
ficult for selecting the next utterance, while stack-
ing self-attention can still help in this case. How-
ever for long utterances like containing more than
30 words, stacking self-attention can significantly
improve the matching performance, which means
that the more information an utterance contains,
the more stacked self-attention it needs to capture
its intra semantic structures.



1125

no
clue
what

do
you

need
it

for.
its

just
reassurance

as
i

dont
know

the
debain

package
manager

hi i am
lo

ok
in

g
to se

e
wh

at
pa

ck
ag

es
ar

e
in

st
al

le
d

on m
y

sy
st

em i.1 do
nt

se
e.

1
a pa
th is th
e

lis
t

be
in

g
he

ld
so

m
ew

he
re

el
se

turn 0

re
sp

on
se

self−attention−match in stack 0

no
clue
what

do
you

need
it

for.
its

just
reassurance

as
i

dont
know

the
debain

package
manager

hi i am
lo

ok
in

g
to se

e
wh

at
pa

ck
ag

es
ar

e
in

st
al

le
d

on m
y

sy
st

em i.1 do
nt

se
e.

1
a pa
th is th
e

lis
t

be
in

g
he

ld
so

m
ew

he
re

el
se

turn 0

re
sp

on
se

self−attention−match in stack 2

no
clue
what

do
you

need
it

for.
its

just
reassurance

as
i

dont
know

the
debain

package
manager

hi i am
lo

ok
in

g
to se

e
wh

at
pa

ck
ag

es
ar

e
in

st
al

le
d

on m
y

sy
st

em i.1 do
nt

se
e.

1
a pa
th is th
e

lis
t

be
in

g
he

ld
so

m
ew

he
re

el
se

turn 0

re
sp

on
se

self−attention−match in stack 4

no
clue
what

do
you

need
it

for.
its

just
reassurance

as
i

dont
know

the
debain

package
manager

hi i am
lo

ok
in

g
to se

e
wh

at
pa

ck
ag

es
ar

e
in

st
al

le
d

on m
y

sy
st

em i.1 do
nt

se
e.

1
a pa
th is th
e

lis
t

be
in

g
he

ld
so

m
ew

he
re

el
se

turn 0

re
sp

on
se

cross−attention−match in stack 4

no
clue
what

do
you

need
it

for
its

just
reassurance

as
i

dont
know

the
debain

package
manager

no clu
e

wh
at

do yo
u

ne
ed it fo
r. its ju
st

re
as

su
ra

nc
e

as i do
nt

kn
ow th
e

de
ba

in
pa

ck
ag

e
m

an
ag

er

response

re
sp

on
se

self−attention of response in stack 3

hi
i

am
looking

to
see

what
packages

are
installed

on
my

system
i.1

dont
see.1

a
path

is
the
list

being
held

somewhere
else

hi i am
lo

ok
in

g
to se

e
wh

at
pa

ck
ag

es
ar

e
in

st
al

le
d

on m
y

sy
st

em i.1 do
nt

se
e.

1
a pa
th is th
e

lis
t

be
in

g
he

ld
so

m
ew

he
re

el
se

.
turn 0

tu
rn

 0
self−attention of turn 0 in stack 3

no
clue
what

do
you

need
it

for
its

just
reassurance

as
i

dont
know

the
debain

package
manager

hi i am
lo

ok
in

g
to se

e
wh

at
pa

ck
ag

es
ar

e
in

st
al

le
d

on m
y

sy
st

em i.1 do
nt

se
e.

1
a pa
th is th
e

lis
t

be
in

g
he

ld
so

m
ew

he
re

el
se

.

turn 0
re

sp
on

se

attention of response over turn 0 in stack 4

hi
i

am
looking

to
see

what
packages

are
installed

on
my

system
i.1

dont
see.1

a
path

is
the
list

being
held

somewhere
else

no clu
e

wh
at

do yo
u

ne
ed it fo
r. its ju
st

re
as

su
ra

nc
e

as i do
nt

kn
ow th
e

de
ba

in
pa

ck
ag

e
m

an
ag

er

response

tu
rn

 0

attention of turn 0 over response in stack 4

self-attention cross-attention

no
clue
what

do
you

need
it

for.
its

just
reassurance

as
i

dont
know

the
debain

package
manager

hi i am
lo

ok
in

g
to se

e
w

ha
t

pa
ck

ag
es

ar
e

in
st

al
le

d
on m

y
sy

st
em

do
nt a pa
th is th
e

lis
t

be
in

g
he

ld
so

m
ew

he
re

el
se

turn 0

re
sp

on
se

prior−match in stack 0

no
clue
what

do
you

need
it

for.
its

just
reassurance

as
i

dont
know

the
debain

package
manager

hi i am
lo

ok
in

g
to se

e
w

ha
t

pa
ck

ag
es

ar
e

in
st

al
le

d
on m

y
sy

st
em

do
nt a pa
th is th
e

lis
t

be
in

g
he

ld
so

m
ew

he
re

el
se

turn 0

re
sp

on
se

prior−match in stack 2

no
clue
what

do
you

need
it

for.
its

just
reassurance

as
i

dont
know

the
debain

package
manager

hi i am
lo

ok
in

g
to se

e
w

ha
t

pa
ck

ag
es

ar
e

in
st

al
le

d
on m

y
sy

st
em

do
nt a pa
th is th
e

lis
t

be
in

g
he

ld
so

m
ew

he
re

el
se

turn 0

re
sp

on
se

prior−match in stack 4

no
clue
what

do
you

need
it

for.
its

just
reassurance

as
i

dont
know

the
debain

package
manager

hi i am
lo

ok
in

g
to se

e
w

ha
t

pa
ck

ag
es

ar
e

in
st

al
le

d
on m

y
sy

st
em

do
nt a pa
th is th
e

lis
t

be
in

g
he

ld
so

m
ew

he
re

el
se

turn 0

re
sp

on
se

posterior−match in stack 4

no
clue
what

do
you

need
it

for
its

just
reassurance

as
i

dont
know

the
debain

package
manager

no cl
ue

w
ha

t
do yo

u
ne

ed it fo
r. its ju
st

re
as

su
ra

nc
e

as i do
nt

kn
ow th
e

de
ba

in
pa

ck
ag

e
m

an
ag

er

response

re
sp

on
se

self−attention of response in stack 3

hi
i

am
looking

to
see

what
packages

are
installed

on
my

system
dont

a
path

is
the
list

being
held

somewhere
else

hi i am
lo

ok
in

g
to se

e
w

ha
t

pa
ck

ag
es

ar
e

in
st

al
le

d
on m

y
sy

st
em

i.1 do
nt

se
e.

1
a pa
th is th
e

lis
t

be
in

g
he

ld
so

m
ew

he
re

el
se

.

turn 0

tu
rn

 0

self−attention of turn 0 in stack 3

no
clue
what

do
you

need
it

for
its

just
reassurance

as
i

dont
know

the
debain

package
manager

hi i am
lo

ok
in

g
to se

e
w

ha
t

pa
ck

ag
es

ar
e

in
st

al
le

d
on m

y
sy

st
em

i.1 do
nt

se
e.

1
a pa
th is th
e

lis
t

be
in

g
he

ld
so

m
ew

he
re

el
se

.

turn 0

re
sp

on
se

attention of response over turn 0 in stack 4

hi
i

am
looking

to
see

what
packages

are
installed

on
my

system
dont

a
path

is
the
list

being
held

somewhere
else

no cl
ue

w
ha

t
do yo

u
ne

ed it fo
r. its ju
st

re
as

su
ra

nc
e

as i do
nt

kn
ow th
e

de
ba

in
pa

ck
ag

e
m

an
ag

er

response

tu
rn

 0

attention of turn 0 over response in stack 4

prior-match posterior-match

self-attention cross-attention

no
clue
what

do
you

need
it

for.
its

just
reassurance

as
i

dont
know

the
debain

package
manager

hi i am
lo

ok
in

g
to se

e
w

ha
t

pa
ck

ag
es

ar
e

in
st

al
le

d
on m

y
sy

st
em

do
nt a pa
th is th
e

lis
t

be
in

g
he

ld
so

m
ew

he
re

el
se

turn 0

re
sp

on
se

prior−match in stack 0

no
clue
what

do
you

need
it

for.
its

just
reassurance

as
i

dont
know

the
debain

package
manager

hi i am
lo

ok
in

g
to se

e
w

ha
t

pa
ck

ag
es

ar
e

in
st

al
le

d
on m

y
sy

st
em

do
nt a pa
th is th
e

lis
t

be
in

g
he

ld
so

m
ew

he
re

el
se

turn 0

re
sp

on
se

prior−match in stack 2

no
clue
what

do
you

need
it

for.
its

just
reassurance

as
i

dont
know

the
debain

package
manager

hi i am
lo

ok
in

g
to se

e
w

ha
t

pa
ck

ag
es

ar
e

in
st

al
le

d
on m

y
sy

st
em

do
nt a pa
th is th
e

lis
t

be
in

g
he

ld
so

m
ew

he
re

el
se

turn 0

re
sp

on
se

prior−match in stack 4

no
clue
what

do
you

need
it

for.
its

just
reassurance

as
i

dont
know

the
debain

package
manager

hi i am
lo

ok
in

g
to se

e
w

ha
t

pa
ck

ag
es

ar
e

in
st

al
le

d
on m

y
sy

st
em

do
nt a pa
th is th
e

lis
t

be
in

g
he

ld
so

m
ew

he
re

el
se

turn 0

re
sp

on
se

posterior−match in stack 4

no
clue
what

do
you

need
it

for
its

just
reassurance

as
i

dont
know

the
debain

package
manager

no cl
ue

w
ha

t
do yo

u
ne

ed it fo
r. its ju
st

re
as

su
ra

nc
e

as i do
nt

kn
ow th
e

de
ba

in
pa

ck
ag

e
m

an
ag

er

response

re
sp

on
se

self−attention of response in stack 3

hi
i

am
looking

to
see

what
packages

are
installed

on
my

system
dont

a
path

is
the
list

being
held

somewhere
else

hi i am
lo

ok
in

g
to se

e
w

ha
t

pa
ck

ag
es

ar
e

in
st

al
le

d
on m

y
sy

st
em

i.1 do
nt

se
e.

1
a pa
th is th
e

lis
t

be
in

g
he

ld
so

m
ew

he
re

el
se

.

turn 0

tu
rn

 0

self−attention of turn 0 in stack 3

no
clue
what

do
you

need
it

for
its

just
reassurance

as
i

dont
know

the
debain

package
manager

hi i am
lo

ok
in

g
to se

e
w

ha
t

pa
ck

ag
es

ar
e

in
st

al
le

d
on m

y
sy

st
em

i.1 do
nt

se
e.

1
a pa
th is th
e

lis
t

be
in

g
he

ld
so

m
ew

he
re

el
se

.

turn 0

re
sp

on
se

attention of response over turn 0 in stack 4

hi
i

am
looking

to
see

what
packages

are
installed

on
my

system
dont

a
path

is
the
list

being
held

somewhere
else

no cl
ue

w
ha

t
do yo

u
ne

ed it fo
r. its ju
st

re
as

su
ra

nc
e

as i do
nt

kn
ow th
e

de
ba

in
pa

ck
ag

e
m

an
ag

er

response

tu
rn

 0

attention of turn 0 over response in stack 4

prior-match posterior-match

self-attention cross-attention

self-attention-match in stack 0 self-attention-match in stack 2 self-attention-match in stack 4 cross-attention-match in stack 4
self-attention-match cross-attention-match

Figure 5: Visualization of self-attention-match, cross-attention-match as well as the distribution of self-attention and cross-
attention in matching response with the first utterance in Figure 1. Each colored grid represents the matching degree or attention
score between two words. The deeper the color is, the more important this grid is.

5.2 Visualization

We study the case in Figure 1 for analyzing in de-
tail how self-attention and cross-attention work.
Practically, we apply a softmax operation over
self-attention-match and cross-attention-match, to
examine the variance of dominating matching
pairs during stacking self-attention or applying
cross-attention. Figure 5 gives the visualization
results of the 0th, 2nd and 4th self-attention-match
matrixes, the 4th cross-attention-match matrix, as
well as the distribution of self-attention and cross-
attention in the 4th layer in matching response with
the first utterance (turn 0) due to space limitation.
As demonstrated, important matching pairs in self-
attention-match in stack 0 are nouns, verbs, like
“package” and “packages”, those are similar in
topics. However matching scores between prepo-
sitions or pronouns pairs, such as “do” and “what”,
become more important in self-attention-match in
stack 4. The visualization results of self-attention
show the reason why matching between preposi-
tions or pronouns matters, as demonstrated, self-
attention generally capture the semantic structure
of “no clue what do you need package manager”
for “do” in response and “what packages are in-
stalled” for “what” in utterance, making segments
surrounding “do” and “what” close to each other
in representations, thus increases their dot-product
results.

Also as shown in Figure 5, self-attention-
match and cross-attention-match capture com-
plementary information in matching utterance
with response. Words like “reassurance” and
“its” in response significantly get larger match-
ing scores in cross-attention-match compared with
self-attention-match. According to the visual-
ization of cross-attention, “reassurance” generally
depends on “system” “don’t” and “held” in utter-
ance, which makes it close to words like “list”,
“installed” or “held” of utterance. Scores of cross-
attention-match trend to centralize on several seg-
ments, which probably means that those segments
in response generally capture structure-semantic
information across utterance and response, ampli-
fying their matching scores against the others.

5.3 Error Analysis

To understand the limitations of DAM and where
the future improvements might lie, we analyze 100
strong bad cases from test-set that fail in R10@5.
We find two major kinds of bad cases: (1) fuzzy-
candidate, where response candidates are basi-
cally proper for the conversation context, except
for a few improper details. (2) logical-error,
where response candidates are wrong due to logi-
cal mismatch, for example, given a conversation
context A: “I just want to stay at home tomor-
row.”, B: “Why not go hiking? I can go with



1126

you.”, response candidate like “Sure, I was plan-
ning to go out tomorrow.” is logically wrong be-
cause it is contradictory to the first utterance of
speaker A. We believe generating adversarial ex-
amples, rather than randomly sampling, during
training procedure may be a good idea for address-
ing both fuzzy-candidate and logical-error, and
to capture logic-level information hidden behind
conversation text is also worthy to be studied in
the future.

6 Conclusion

In this paper, we investigate matching a response
with its multi-turn context using dependency in-
formation based entirely on attention. Our solu-
tion extends the attention mechanism of Trans-
former in two ways: (1) using stacked self-
attention to harvest multi-grained semantic repre-
sentations. (2) utilizing cross-attention to match
with dependency information. Empirical results
on two large-scale datasets demonstrate the ef-
fectiveness of self-attention and cross-attention in
multi-turn response selection. We believe that
both self-attention and cross-attention could bene-
fit other research area, including spoken language
understanding, dialogue state tracking or seq2seq
dialogue generation. We would like to explore
in depth how attention can help improve neu-
ral dialogue modeling for both chatbots and task-
oriented dialogue systems in our future work.

Acknowledgement

We gratefully thank the anonymous reviewers for
their insightful comments. This work is supported
by the National Basic Research Program of China
(973 program, No. 2014CB340505).

References
David Ameixa, Luisa Coheur, Pedro Fialho, and Paulo

Quaresma. 2014. Luke, i am your father: dealing
with out-of-domain requests by using movies subti-
tles. In International Conference on Intelligent Vir-
tual Agents, pages 13–21. Springer.

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. 2016. Layer normalization. arXiv preprint
arXiv:1607.06450.

Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et al.
1999. Modern information retrieval, volume 463.
ACM press New York.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly

learning to align and translate. international con-
ference on learning representations.

Rafael E Banchs and Haizhou Li. 2012. Iris: a chat-
oriented dialogue system based on the vector space
model. In Proceedings of the ACL 2012 System
Demonstrations, pages 37–42. Association for Com-
putational Linguistics.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770–
778.

Zongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An
information retrieval approach to short text conver-
sation. arXiv preprint arXiv:1408.6988.

Quoc V Le, Jiquan Ngiam, Adam Coates, Abhik
Lahiri, Bobby Prochnow, and Andrew Y Ng. 2011.
On optimization methods for deep learning. In Pro-
ceedings of the 28th International Conference on In-
ternational Conference on Machine Learning, pages
265–272.

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
2015. Deep learning. Nature, 521(7553):436–444.

Alan Lee, Rashmi Prasad, Aravind Joshi, Nikhil Di-
nesh, and Bonnie Webber. 2006. Complexity of de-
pendencies in discourse: Are dependencies in dis-
course more complex than in syntax. In Proceed-
ings of the 5th International Workshop on Treebanks
and Linguistic Theories, Prague, Czech Republic,
page 12.

Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, and
Dan Jurafsky. 2017. Adversarial learning for neural
dialogue generation. In EMNLP, pages 372–381.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. international conference on learning
representations.

Ryan Lowe, Michael Noseworthy, Iulian V Serban,
Nicolas Angelard-Gontier, Yoshua Bengio, and
Joelle Pineau. 2017. Towards an automatic turing
test: Learning to evaluate dialogue responses. In
ACL, pages 372–381.

Ryan Lowe, Nissan Pow, Iulian Vlad Serban, and
Joelle Pineau. 2015. The ubuntu dialogue corpus:
A large dataset for research in unstructured multi-
turn dialogue systems. annual meeting of the spe-
cial interest group on discourse and dialogue, pages
285–294.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.



1127

Lasguido Nio, Sakriani Sakti, Graham Neubig, Tomoki
Toda, Mirna Adriani, and Satoshi Nakamura. 2014.
Developing non-goal dialog system based on exam-
ples of drama television. In Natural Interaction with
Robots, Knowbots and Smartphones, pages 355–
361. Springer.

Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu,
Shengxian Wan, and Xueqi Cheng. 2016. Text
matching as image recognition. In AAAI, pages
2793–2799.

Alan Ritter, Colin Cherry, and William B Dolan. 2011.
Data-driven response generation in social media. In
In Proc. EMNLP, pages 583–593. Association for
Computational Linguistics.

Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. Computational linguistics, 26(3):339–373.

David R Traum and Peter A Heeman. 1996. Utter-
ance units in spoken dialogue. In Workshop on
Dialogue Processing in Spoken Language Systems,
pages 125–140.

Alan M Turing. 1950. Computing machinery and in-
telligence. Mind, 59(236):433–460.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 6000–6010.

Ellen M Voorhees et al. 1999. The trec-8 question an-
swering track report. In Trec, pages 77–82.

Hao Wang, Zhengdong Lu, Hang Li, and Enhong
Chen. 2013. A dataset for research on short-text
conversations. In EMNLP, pages 935–945.

Mingxuan Wang, Zhengdong Lu, Hang Li, and Qun
Liu. 2015. Syntax-based deep matching of short
texts. International Joint Conferences on Artificial
Intelligence.

Shuohang Wang and Jing Jiang. 2017. Machine com-
prehension using match-lstm and answer pointer. in-
ternational conference on learning representations.

Yu Wu, Wei Wu, Ming Zhou, and Zhoujun Li. 2017.
Sequential match network: A new architecture for
multi-turn response selection in retrieval-based chat-
bots. In ACL, pages 372–381.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:
Neural image caption generation with visual at-
tention. In International Conference on Machine
Learning, pages 2048–2057.

Rui Yan, Yiping Song, and Hua Wu. 2016. Learning
to respond with deep neural networks for retrieval-
based human-computer conversation system. In
Proceedings of the 39th International ACM SIGIR
conference on Research and Development in Infor-
mation Retrieval, pages 55–64.

Wenpeng Yin, Hinrich Schütze, Bing Xiang, and
Bowen Zhou. 2016. Abcnn: Attention-based convo-
lutional neural network for modeling sentence pairs.
Transactions of the Association of Computational
Linguistics, 4(1):259–272.

Xiangyang Zhou, Daxiang Dong, Hua Wu, Shiqi Zhao,
Dianhai Yu, Hao Tian, Xuan Liu, and Rui Yan. 2016.
Multi-view response selection for human-computer
conversation. In EMNLP, pages 372–381.


