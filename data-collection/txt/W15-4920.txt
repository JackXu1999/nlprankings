
























Assessing linguistically aware fuzzy matching in translation memories

Tom Vanallemeersch, Vincent Vandeghinste
Centre for Computational Linguistics, University of Leuven

Blijde Inkomststraat 13
B-3000 Leuven, Belgium

{tom,vincent}@ccl.kuleuven.be

Abstract

The concept of fuzzy matching in trans-
lation memories can take place using lin-
guistically aware or unaware methods, or a
combination of both.

We designed a flexible and time-efficient
framework which applies and combines
linguistically unaware or aware metrics in
the source and target language.

We measure the correlation of fuzzy
matching metric scores with the evaluation
score of the suggested translation to find
out how well the usefulness of a sugges-
tion can be predicted, and we measure the
difference in recall between fuzzy match-
ing metrics by looking at the improve-
ments in mean TER as the match score de-
creases. We found that combinations of
fuzzy matching metrics outperform single
metrics and that the best-scoring combina-
tion is a non-linear combination of the dif-
ferent metrics we have tested.

1 Introduction

Computer-aided translation (CAT) has become an
essential aspect of translators’ working environ-
ments. CAT tools speed up translation work, create
more consistent translations, and reduce repetitive-
ness of the translation work. One of the core com-
ponents of a CAT tool is the translation memory
system (TMS). It contains a database of already
translated fragments, called the translation mem-
ory (TM), which consists of translation units: seg-
ments of a text together with their translation.

c� 2015 The authors. This article is licensed under a Creative
Commons 3.0 licence, no derivative works, attribution, CC-
BY-ND.

Given a sentence to be translated, the traditional
TMS looks for source language sentences in a TM
which are identical (exact matches) or highly sim-
ilar (fuzzy matches), and, upon success, suggests
the translation of the matching sentence to the
translator (Sikes, 2007).

Formally, a TM consists of a set of source sen-
tences S1, ..., Sn and target sentences T1, ..., Tn,
where (Si, Ti) form a translation unit. Let us call
the sentence that we want to translate Q (the query
sentence).

The TMS checks whether Q already occurs in
the TM, i.e. whether ∃Si ∈ S1, ..., Sn : Q = Si.
If this is the case, Q needs no new translation and
the translation Ti can be retrieved and used as the
translation of Q. This is an exact match. If the
TMS cannot find a perfect match, fuzzy matching
is applied using some function Sim, which calcu-
lates the best match Sb of Q in the TM, i.e. the
most similar match, as in (1):

Sb = max
Si

Sim(Q,Si) (1)

If Sim(Q,Sb) >= θ (a predefined minimal
threshold, which is typically 0.7 in CAT tools)1),
Tb, the translation of Sb, is retrieved from the TM
and provided as a suggestion for translating Q. If
the threshold is not reached, the TMS assumes that
Tb is of no value for the translator and does not
provide it as a translation suggestion.

Similarity calculation can be done in many
ways. In current TMS systems, fuzzy matching
techniques mainly consider sentences as simple se-
quences of words and contain very limited linguis-
tic knowledge. The latter is for instance present in
1In CAT tool interfaces, this is usually expressed as a percent-
age, 70%, which may be modified by the user. Developers
may determine the threshold empirically, but their use of a
70% threshold may also be a matter of convention.

153



the form of stop word lists. Few tools use more
elaborate linguistic knowledge.2

2 Related work

There is a large variety of methods that can be
used for comparing sentences to each other. They
are designed for comparing any pair of sequences
or trees (not necessarily sentences and their parse
trees), for fuzzy matching in a TM, or for com-
paring machine translation (MT) output to a ref-
erence translation. As pointed out by Simard and
Fujita (2012), the third type, MT automatic eval-
uation metrics, can also be used in the context
of TMs, both as fuzzy matching metric and as
metric for comparing the translation of a fuzzy
match with the desired translation. Some match-
ing methods specifically support the integration of
fuzzy matches within an MT system (example-
based or statistical MT); see for instance Aramaki
et al. (2005), Smith and Clark (2009), Zhechev and
van Genabith (2010), Ma et al. (2011).

Some matching methods are linguistically un-
aware. Levenshtein distance (Levenshtein, 1966),
which calculates the effort needed to convert one
sequence into another using the operations inser-
tion, deletion and substitution, is the most com-
monly used fuzzy matching method (Bloodgood
and Strauss, 2014). Tree edit distance (Klein,
1998) applies this principle to trees; another tree
comparison method is tree alignment (Jiang et al.,
1995).3

To allow using string-based matching methods
on trees, there are several ways of converting trees
into strings without information loss, as described
in Li et al. (2008), who applies a method de-
signed by Prüfer (1918) and based on post-order
tree traversal.

Examples of matching methods specifically de-
signed for fuzzy matching are percent match and
ngram precision (Bloodgood and Strauss, 2014),
which act on unigrams and longer ngrams. Bald-
win (2010) compares bag-of-words fuzzy match-
ing metrics with order-sensitive metrics, and word-
based with character-based metrics. Examples of
well-known MT evaluation metrics are BLEU (Pa-

2One example of such a tool is Similis (http://www.similis.
org), which determines constituents in sentences and allows
to retrieve (Si, Ti) when Si shares constituents with Q.
3We implemented the Tree Edit Distance algorithm of Klein
from its description in Bille (2005), as well as the Tree Align-
ment Distance algorithm. However, both were too slow to be
useful for parse trees unless severe optimization takes place.

pineni et al., 2002) and TER, i.e. Translation Error
Rate4 (Snover et al., 2006).

Linguistically aware matching methods make
use of several layers of information. The ”subtree
metric” of Liu and Gildea (2005) compares sub-
trees of phrase structure trees. We devised a sim-
ilar method, shared partial subtree matching, de-
scribed in Section 3.1.2. Matching can also involve
dependency structures, as in the approach of Smith
and Clark (2009), head word chains (Liu and
Gildea, 2005), semantic roles, as in the HMEANT
metric (Lo and Wu, 2011), and semantically simi-
lar words or paraphrases, as in the MT evaluation
metric Meteor (Denkowski and Lavie, 2014). The
latter aligns MT output to one or more reference
translations, not only by comparing word forms,
but also through shallow linguistic knowledge, i.e.
by calculating the stem of words (in some cases
using language-specific rules), and by using lists
with function words, synonyms and paraphrases.

Some MT evaluation metrics, such as
VERTa (Comelles et al., 2014) and LAY-
ERED (Gautam and Bhattacharyya, 2014), and
some fuzzy matching methods, like the one
of Gupta et al. (2014), are based on multiple
linguistic layers. The layers are assigned weights
or combined using a support vector machine.

Different types of metrics can be combined in
order to join their strengths. For instance, the
Asiya toolkit (Giménez and Márquez, 2010) con-
tains a large number of matching metrics of dif-
ferent origins and applies them for MT evaluation.
An optimal metric set is determined by progres-
sively adding metrics to the set if that increases the
quality of the translation.

3 Experimental setup

3.1 Independent variables
In Sections 3.1.1, 3.1.2, and 3.1.3, we describe the
independent variables of our experiment.

3.1.1 Linguistically unaware metrics
Levenshtein (baseline) Given the Levenshtein
distance ΔLEV (S, Ti), we define Levenshtein
score (i.e. similarity) as in (2):

SimLEV (Q,Si) = 1−
ΔLEV (Q,Si)
max(|Q|, |Si|)

(2)

4While the developers of TER call it Translation Edit Rate,
the name Translation Error Rate is often used, through the
influence of the metric name Word Error Rate, which is used
in automatic speech recognition.

154



Levenshtein distance, which is based on three
operation types (insertion, deletion and substitu-
tion), and its variants, assign a specific cost to each
type of operation. Typically, each type has a cost
of 1. Certain costs may be changed in order to
obtain a specific behaviour. For instance, the cost
of a substitution may depend on the similarity of
words.

Translation Error Rate Given a sentence Q
output by an MT system and a reference trans-
lation R, TER keeps on applying shifts to Q as
long as ΔLEV (Q,R) keeps decreasing.5 The TER
distance ΔTER(Q,R), which equals ΔLEV (Q,R)
plus the cost of the shifts, is normalized as in (3):

ScoreTER(Q,R) =
ΔTER(Q,R)

|R| (3)

We convert ScoreTER into a similarity score
between 0 and 1 as in (4). This formula assumes a
very high upper bound for ScoreTER.6

SimTER(Q,R) = 1−
log(1 + ScoreTER(Q,R))

3
(4)

Percent match calculates the percent of uni-
grams in Q that are found in Si, as in (5):7

SimPM (Q,Si) =
|Q1grams ∩ Si,1grams|

|Q1grams|
(5)

Ngram precision compares ngrams, i.e. subse-
quences of one or more elements, of length 1 up
till N , as in (6), where the precision for ngrams of
length n is calculated as in (7).

SimNGP (Q,Si) =
N�

n=1

1
N

pn (6)

pn =
|Qngrams ∩ Si,ngrams|

Z ∗ |Qngrams| + (1− Z) ∗ |Si,ngrams|
(7)

5An implementation of TER can be found here: http://www.
cs.umd.edu/˜snover/tercom. We used version 0.7.25 for our
experiment.
6Setting the denominator to 3 ensures that SimTER is a non-
negative number unless ScoreTER exceeds the upper bound
of 19. We chose this arbitrary bound in order to have an inte-
ger as denominator in the formula.
7This metric is similar to the metric PER, position-
independent word error rate (Tillmann et al., 1997). The dif-
ference between both metrics lies in the fact that PER takes
account of multiple occurrences of a token in a sentence, does
not calculate a normalized value between 0 and 1, and does
not ignore words which are present in Si but not in Q.

Qngrams is the set of ngrams in Q. Si,ngrams
is the set of ngrams in Si, and Z is a parameter to
control normalization. Setting Z to a high value
prefers longer translations.8

Bloodgood and Strauss propose weighted vari-
ants for the SimPM and SimNGP metrics, us-
ing IDF weights, which reflect the relevance of the
matching words. We will refer to one of these vari-
ants later on, calling it SimPMIDF .

3.1.2 Linguistically aware metrics
Adaptations of linguistically unaware metrics
We investigated Levenshtein, percent match, and
TER not only on sequences of word forms, but
also on sequences of lemmas. We will refer
to these lemma-based metrics as SimLEV LEM ,
SimPMLEM and SimTERLEM .

Shared partial subtree matching We devised a
method which aims specifically at comparing two
parse trees. In order to perform this comparison in
an efficient way, we apply the following steps: (1)
check whether pairs of subtrees in the two parses
share a partial subtree; (2) determine the scores of
the shared partial subtrees, based on lexical and
non-lexical similarity of the nodes, on the rele-
vance of the words, and on the number of nodes
in the shared partial subtree; (3) perform a greedy
search for the best combination of shared partial
subtrees.

Based on the scores of the partial subtrees in
the final combination, we determine the shared
partial subtree similarity, as in (8). In this equa-
tion, ScoreSPS(Q,Si) stands for the sum of the
scores of the partial subtrees in the combination
and MaxScoreSPS(Q,Si) stands for the score we
obtain if Q and Si are equal.

SimSPS(Q,Si) =
ScoreSPS(Q,Si)

MaxScoreSPS(Q,Si)
(8)

Levenshtein for Prüfer sequences Extracting
information from a tree and gathering it into a
sequence allows us to apply string-based meth-
ods, which are less time-costly than tree-based me-
thods, and which come in a great variety.

When comparing the structures in two Prüfer se-
quences, we may use either a cost of 0 (identity of
structures) or 1. However, some structures which

8For our experiment, we set N to 4 and Z to 0.5. Setting its
value experimentally, however, would be more appropriate.

155



are not identical may have some degree of similar-
ity (for instance, a terminal node with equal part-
of-speech but different lemmas). Therefore, we as-
sign costs between 0 and 1 when calculating the
Levenshtein distance. We refer to Levenshtein cal-
culation on Prüfer sequences as SimLEV PRFC .

Ngram precision for head word chains Head
word chains can be considered as ngrams. There-
fore, we apply a variant of ngram precision to them
which we call SimNGPHWC .

Meteor For brevity’s sake we do not provide the
formulas on which SimMETEOR is based. We use
the standard settings including shallow linguistic
knowledge and paraphrases.9

3.1.3 Combinations of metrics
Could a combination of matching metrics per-

form better than the metrics on their own? We
checked this by creating regression trees.10 The
training examples provided for building the tree
are the matches of sentences to translate, the fea-
tures (independent variables) are matching met-
rics, and their values are the matching score. The
regression trees model decisions for predicting the
evaluation score of the translation of the match (the
dependent variable) in a non-linear way. We con-
sider the predicted evaluation score as a new fuzzy
match score.

3.2 Dependent variable

The dependent variable of our experiment is the
evaluation score of the translation suggestion. We
use SimTER as evaluation metric. It reflects the
effort required to change a translation suggestion
into the desired translation. It should be noted that
the usefulness of a translation suggestion should
ultimately be determined by a translator working
with a CAT tool. However, human evaluation is
time-consuming. We therefore use an automatic
evaluation metric as a proxy for human evaluation,
similarly to the modus operandi in the develop-
ment of MT systems.

In order to assess the usefulness of an indvidual
or combined fuzzy matching metric, we apply a
leave-one-out test to a set of parallel sentences and
investigate how well each metric correlates with

9We use version 1.5 of Meteor. See http://www.cs.cmu.edu/
˜alavie/METEOR.
10We used complexity parameter 0.001, retained 500 competi-
tor splits in the output and applied 100 cross-validations.

the evaluation score. For each Qi ∈ Q1, . . . , Qn,
we select the best match produced by the metric,
which we call Sb,i. We call its match score Mb,i
and its translation in the TM Tb,i. We call Qi’s
translation Ri. The evaluation score of the transla-
tion is Eb,i = SimTER(Tb,i, Ri). We compute the
Pearson correlation coefficient between M and E.
A higher coefficient indicates a more useful fuzzy
matching metric.

A second way for assessing the usefulness of
metrics is considering their mean evaluation score
and investigating the significance of the differ-
ence between metrics through bootstrap resam-
pling. This approach consists of taking a large
number of subsets of test sentences and compar-
ing the mean evaluation score of their best matches
across metrics. For instance, if one metric has a
higher mean in at least 95% of the subsets than
another one, the first metric is significantly better
than the second one at confidence level 0.05.

A third way we study the usefulness of metrics
is by investigating the degree to which the mean
evaluation score decreases as we keep adding sen-
tences with diminishing match score. If the de-
crease in mean evaluation score is slower in one
metric than in another, the first metric has a higher
recall than the second metric, as we need to put
less effort in editing the translation suggestions to
reach the desired translation.

3.3 Speed of retrieval

We developed a filter called approximate query
coverage (AQC). Its purpose is to select candi-
date sentences in the TM which are likely to reach
a minimal matching threshold when submitting
them to a fuzzy matching metric, in order to in-
crease the speed of matching. A candidate sen-
tence is a sentence which shares one or more
ngrams of a minimal length N with Q, and which
shares enough ngrams with Q so as to cover the
latter sufficiently.

The implementation of the filter uses a suffix ar-
ray (Manber and Myers, 1993), which allows for a
very efficient search for sentences sharing ngrams
with Q.11 This approach is similar to the one used
in the context of fuzzy matching by Koehn and
Senellart (2010).

In order to measure the usefulness of the AQC
filter, we measured the tradeoff between the gain

11We used the SALM toolkit (Zhang and Vogel, 2006) for
building and consulting suffix arrays in our experiment.

156



in speed and the loss of potentially useful matches.
We used a sample of about 30,000 English-Dutch
sentence pairs selected from Europarl (Koehn,
2005), and a threshold of 0.2. After applying
a leave-one-out test, which consists of consider-
ing each Si in the sample as a Q and comparing
it to all the other Si in the sample, it appeared
that the AQC filter selected about 9 candidate sen-
tences per Q. The gain in speed is very signifi-
cant: after filtering, a fuzzy matching metric like
SimLEV only needs to be applied to 0.03% of the
sentences in the sample. As for the loss of po-
tentially useful matches, we considered each Si
for which SimLEV (Q,Si) >= 0.3 to be such a
match. It appears that most of these Si are still
available after filtering: 93% of all pairs (Q,Si)
with a SimLEV value between 0.3 and 0.4 have an
AQC score >= 0.2. For pairs between 0.4 and 0.6,
this is 98%, and for pairs above 0.6 100%. Hence,
there is a very good tradeoff between gain in speed
and loss of potentially useful matches.

3.4 Preprocessing data

We use the Stanford parser (Klein and Manning,
2003) to parse English sentences. We divide a
sample of sentences into two equally sized sets: a
training set, from which regression trees are built,
and a test set, to which individual metrics and com-
bined metrics derived from regression trees are ap-
plied. We derive IDF weights from the full sample.

4 Results

We tested the setup described in the previous sec-
tion on a sample of 30,000 English-Dutch sentence
pairs from Europarl. We built regression trees for
different combinations of metrics. The combined
metrics either involve the baseline and an individ-
ual metric or a larger set of metrics. The results are
shown in Table 1. The leftmost column shows the
metric used:

• Individual metrics: LEV (Levenshtein), TER,
METEOR, PM (percent match), PMIDF (per-
cent match with weights), NGP (ngram preci-
sion), NGPHWC (head word chains), LEVLEM
(lemma-based Levenshtein), PMLEM, TER-
LEM, SPS (shared partial subtree matching)

• Combination of baseline and individual met-
ric: TER+LEV, SPS+LEV, . . .

• Combination of all linguistically aware met-
rics: LING

Table 1: Comparison of metrics with baseline
Sim Corr(Mb,i, Eb,i) ScoreTER

Baseline
LEV 0.278 1.007

Linguistically aware metrics
LEVLEM 0.279 1.009
LEVPRFC 0.283 0.983
METEOR 0.058 1.066
NGPHWC 0.291 1.028
PMLEM 0.420 0.927∗

SPS 0.275 0.987
TERLEM 0.500 0.926∗

Linguistically unaware metrics
NGP 0.222 1.035
PM 0.424 0.926∗
PMIDF 0.335 0.963∗

TER 0.502 0.926∗
Metrics combined using regression tree

LEVLEM+LEV 0.362 0.869∗
LEVPRFC+LEV 0.386 0.905∗

METEOR+LEV 0.391 0.910∗

NGPHWC+LEV 0.347 0.869∗
PMLEM+LEV 0.478 0.916∗

SPS+LEV 0.363 0.908∗

TERLEM+LEV 0.562 0.894∗

NGP+LEV 0.376 0.903∗

PM+LEV 0.455 0.906∗

PMIDF+LEV 0.405 0.906∗

TER+LEV 0.561 0.894∗

LING 0.564 0.899∗

NONLING 0.571 0.889∗
ALL 0.563 0.899∗

∗p < 0.05

• Combination of all linguistically unaware
metrics (except for the baseline): NONLING

• Combination of all metrics (including the
baseline): ALL

The middle column of Table 1 shows the Pear-
son correlation coefficient between the match
score and the evaluation score (SimTER). The
rightmost column shows the means of ScoreTER
values (which reflect the estimated editing effort)
instead of the means of the SimTER values. We
used the latter primarily to facilitate the calculation
of certain statistics regarding TER, such as corre-
lations.

Let us first have a look at the individual metrics
in Table 1. The SimPM and SimTER metrics,
and their lemma-based variants, have the highest
correlation with the evaluation score; their corre-
lation is markedly higher than that of the baseline.
Interestingly, IDF weights do not seem to help per-
cent match, on the contrary. The correlation of
most other individual metrics is close to that of
the baseline. Looking at the worst-performing two
metrics, SimNGP and SimMETEOR, it is strik-
ing that the latter has an extremely low correla-

157



tion compared to the baseline. This needs fur-
ther investigation. The high score of SimTER
and SimTERLEM raises the question whether an
evaluation metric favors a fuzzy matching metric
which is identical or similar to it.

The means of the ScoreTER values for individ-
ual metrics more or less confirm the differences
observed for correlation. SimPM , SimTER and
their lemma-based variants have the lowest mean.
As shown by the asterisks in the table, the differ-
ence in mean with the baseline is significant at the
0.05 level for about half of the individual metrics.

Looking at the combined metrics in Table 1, we
see that all of them have a higher correlation with
the evaluation score than the baseline, and a lower
ScoreTER mean; the difference in mean with the
baseline is always significant. Of all two-metric
combinations, the ones involving SimTER and its
lemma-based variant perform the best. The com-
binations SimLING and SimNONLING perform
slightly better than the best two-metric combina-
tions. SimALL, which includes the baseline itself,
comes close to SimLING and SimNONLING but
does not exceed their performance. From the re-
gression tree involving the combination of all met-
rics, it appears that it uses 9 of the 12 individ-
ual metrics, including the baseline, to predict the
evaluation score. There is no clearcut association
between the correlation values of the combined
metrics and their ScoreTER mean. For instance,
SimNGPHWC has the lowest correlation but also
the lowest ScoreTER mean.

Figure 1 shows the mean ScoreTER increase
(i.e. increase in editing effort) that we obtain when
adding baseline matches with decreasing match
score. When we order all test sentences according
to the baseline score of their best match, the mean
ScoreTER of the first 1000 sentences (the 1000 top
sentences) is 0.74. When we order the test sen-
tences according to SimALL, the mean ScoreTER
of the 1000 top sentences is 0.67. As we add more
sentences to the top list, the ScoreTER mean for
the baseline increases more strongly than that of
SimALL. The recall of SimALL increases, as we
need to put less effort in editing the translation
suggestions of the top list. For instance, the re-
call for 1000 sentences is 10% lower for the base-
line (0.74/0.67=1.10). For 2000 sentences, the dif-
ference increases to 11%, and for 3000 to 13%.
The oracle line in Figure 1 indicates the mean
ScoreTER increase in case we know the evalua-

tion score of the best match beforehand; this is the
upper bound for a matching metric.

From the results in Table 1, we can conclude
that, though linguistically unaware metrics help a
long way in improving on the baseline, linguistic
metrics clearly have added value. A question that
arises here, and to which we already pointed pre-
viously, is whether the use of an identical metric
for fuzzy matching and for evaluation favors that
fuzzy matching metric with respect to others. If
that is the case, it may be better to optimize fuzzy
matching methods towards a combination of eval-
uation metrics rather than a single metric. Ideally,
human judgment of translation should also be in-
volved in evaluation.

5 Conclusion and future

Our comparison of the baseline matching metric,
Levenshtein distance, with linguistically aware
and unaware matching metrics, has shown that the
use of linguistic knowledge in the matching pro-
cess provides clear added value. This is especially
the case when several metrics are combined into a
new metric using a regression tree. The correla-
tion of combined metrics with the evaluation score
is much stronger than the correlation of the base-
line. Moreover, significant improvement is ob-
served in terms of mean evaluation score, and the
difference in recall with the baseline increases as
match scores decrease.

Considering the fact that there is added value in
linguistic information, we may further improve the
performance of matching metrics by testing more
metric configurations, by using additional metrics
or metric combinations built for MT evaluation,
and by building regression trees using larger train-
ing set sizes. Testing on an additional language,
for instance a highly inflected one, may also shed
light on the value of fuzzy metrics.

Our experiments were performed using a sin-
gle evaluation metric, TER. We may also use other
metrics for evaluation, such as percent match, Me-
teor or shared partial subtree matching, in order to
assess to which degree the use of an identical met-
ric for fuzzy matching and for evaluation affects
results. In this respect, we will also investigate the
low correlation between Meteor as a fuzzy match-
ing metric and TER as an evaluation score, and
select a new metric which we use for evaluation
only and which applies matching techniques ab-
sent from the other metrics. An example of such a

158



Figure 1: Mean ScoreTER increase

metric is the recently developed BEER (Stanojević
and Sima’an, 2014), which is based on permuta-
tion of tree nodes. Human judgment of translation
suggestions will also be taken into account.

Last but not least, we would like to point out
that we have created an innovative fuzzy match-
ing framework with powerful features: integration
of matching metrics with different origins and lev-
els of linguistic information, support for different
types of structures (sequences, trees, trees con-
verted into sequences), combination of metrics us-
ing regression trees, use of any metric in the source
or target language (fuzzy matching metric or eval-
uation metric), and fast filtering through a suffix
array.

6 Acknowledgements

This research is funded by the Flemish govern-
ment agency IWT (project 130041, SCATE). See
http://www.ccl.kuleuven.be/scate.

References
Aramaki, Eiji, Sadao Kurohashi, Hideki Kashioka

and Naoto Kato. 2005. Probabilistic Model for
Example-based Machine Translation. Proceedings
of the 10th Machine Translation Summit, Phuket,
Thailand. pp. 219–226.

Baldwin, Timothy. 2010. The Hare and the Tortoise:
Speed and Accuracy in Translation Retrieval. Ma-
chine Translation, 23(4):195–240.

Bille, Philip. 2005. A Survey on Tree Edit Distance

and Related Problems. Theoretical Computer Sci-
ence, 337(1-3):217–239.

Bloodgood, Michael and Benjamin Strauss. 2014.
Translation Memory Retrieval Methods. Procee-
dings of the 14th Conference of the European Asso-
ciation for Computational Linguistics, Gothenburg,
Sweden. pp. 202–210.

Comelles, Elisabet, Jordi Atserias, Victoria Arranz,
Irene Castellón, and Jordi Sesé. 2014. VERTa: Fa-
cing a Multilingual Experience of a Linguistically-
based MT Evaluation. Proceedings of the 9th In-
ternational Conference on Language Resources and
Evaluation, Reykjavik, Iceland. pp. 2701–2707.

Denkowski, Michael and Alon Lavie. 2014. Meteor
Universal: Language Specific Translation Evalua-
tion for Any Target Language. Proceedings of the
9th Workshop on Statistical Machine Translation,
Baltimore, Maryland, USA. pp. 376–380.

Gautam, Shubham and Pushpak Bhattacharyya. 2014.
LAYERED: Metric for Machine Translation Eva-
luation. Proceedings of the 9th Workshop on Sta-
tistical Machine Translation, Baltimore, Maryland,
USA. pp. 387-393.

Giménez, Jesús and Lluı́s Márquez. 2010. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, 94:77–86.

Gupta, Rohit, Hanna Bechara and Constantin Orasan.
2014. Intelligent Translation Memory Matching and
Retrieval Metric Exploiting Linguistic Technology.
Proceedings of Translating and the Computer 36,
London, UK. pp. 86–89.

Jiang, Tao, Lushen Wang, and Kaizhong Zhang. 1995.
Alignment of Trees – An Alternative to Tree Edit.
Theoretical Computer Science, 143(1):137-148.

159



Klein, Dan and Christopher Manning. 2003. Fast Ex-
act Inference with a Factored Model for Natural Lan-
guage Parsing. Advances in Neural Information Pro-
cessing Systems 15 (NIPS), MIT Press. pp. 3–10.

Klein, Philip. 1998. Computing the Edit Distance
between Unrooted Ordered Trees. Proceedings of
the 6th Annual European Symposium on Algorithms,
Venice, Italy. pp. 91–102.

Koehn, Philipp. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. Proceedings of the
10th Machine Translation Summit, Phuket, Thailand.
pp. 79–86.

Koehn, Philipp and Jean Senellart. 2010. Fast Ap-
proximate String Matching with Suffix Arrays and
A*Parsing. Proceedings of the 9th Conference
of the Association for Machine Translation in the
Americas, Denver, Colorado. 9 pp. [http://www.mt-
archive.info/AMTA-2010-Koehn.pdf]

Levenshtein, Vladimir I. 1966. Binary Codes Capable
of Correcting Deletions, Insertions, and Reversals.
Soviet Physics Doklady, 10(8):707–710.

Li, Guoliang, Xuhui Liu, Jianhua Feng, and Lizhu
Zhou. 2008. Efficient Similarity Search for Tree-
Structured Data. Proceedings of the 20th Inter-
national Conference on Scientific and Statistical
Database Management, Hong Kong, China. pp.
131–149.

Liu, Ding and Daniel Gildea. 2005. Syntactic Fea-
tures for Evaluation of Machine Translation. Pro-
ceedings of ACL 2005 Workshop on Intrinsic and
Extrinsic Evaluation Measures for Machine Trans-
lation and/or Summarization, Ann Arbor, Michigan,
USA. pp. 25–32.

Lo, Chi-kiu and Dekai Wu. 2011. MEANT: An Inex-
pensive, High-accuracy, Semi-automatic Metric for
Evaluating Translation Utility via Semantic Frames.
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies – Volume 1, Portland, Oregon,
USA. pp. 220–229.

Ma, Yanjun, Yifan He, Andy Way, and Josef van Gen-
abith. 2011. Consistent Translation using Discrimi-
native Learning: a Translation Memory-inspired Ap-
proach. Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies – Volume 1, Portland,
Oregon. pp. 1239–1248.

Manber, Udi and Gene Myers. 1993. Suffix Arrays:
A New Method for On-line String Searches. SIAM
Journal on Computing, 22:935–948.

Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, Philadelphia, Pennsylvania,
USA. pp. 311–318.

Prüfer, Heinz. 1918. Neuer Beweis eines Satzes über
Permutationen. Archiv der Mathematik und Physik,
27:742–744.

Sikes, Richard. 2007. Fuzzy Matching in Theory and
Practice. Multilingual, 18(6):39–43.

Simard, Michel and Atsushi Fujita. 2012. A Poor
Man’s Translation Memory Using Machine Trans-
lation Evaluation Metrics. Proceedings of the
10th Conference of the Association for Machine
Translation in the Americas, San Diego, California,
USA. 10 pp. [http://www.mt-archive.info/AMTA-
2012-Simard.pdf]

Smith, James and Stephen Clark. 2009. EBMT for
SMT: a new EBMT-SMT hybrid. Proceedings of the
3rd International Workshop on Example-Based Ma-
chine Translation, Dublin, Ireland. pp. 3–10.

Snover, Matthew, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
Cambridge, Massachusetts, USA. pp. 223–231.

Stanojević, Miloš and Khalil Sima’an. 2014. BEER:
BEtter Evaluation as Ranking. Proceedings of the
9th Workshop on Statistical Machine Translation,
Baltimore, Maryland, USA. pp. 414–419.

Tillmann, Christoph, Stephan Vogel, Hermann Ney,
Alex Zubiaga, and Hassan Sawaf. 1997. Ac-
celerated Dp Based Search For Statistical Transla-
tion. Proceedings of the 5th European Conference
on Speech Communication and Technology, Rhodes,
Greece. pp. 2667–2670.

Zhang, Ying and Stephan Vogel. 2006. Suffix Ar-
ray and its Applications in Empirical Natural Lan-
guage Processing. Technical Report CMU-LTI-06-
010, Language Technologies Institute, School of
Computer Science, Carnegie Mellon University.

Zhechev, Ventsislav and Josef van Genabith. 2010.
Maximising TM Performance through Sub-Tree
Alignment and SMT. Proceedings of the 9th
conference of the Association for Machine Trans-
lation in the Americas, Denver, Colorado, USA.
10 pp. [http://www.mt-archive.info/AMTA-2010-
Zhechev.pdf]

160


