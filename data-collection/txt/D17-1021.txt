



















































A Mention-Ranking Model for Abstract Anaphora Resolution


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 221–232
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

A Mention-Ranking Model for Abstract Anaphora Resolution

Ana Marasović, Leo Born†, Juri Opitz†, and Anette Frank†
Research Training Group AIPHES

Department of Computational Linguistics
Heidelberg University

69120 Heidelberg, Germany
{marasovic,born,opitz,frank}@cl.uni-heidelberg.de

Abstract

Resolving abstract anaphora is an impor-
tant, but difficult task for text understand-
ing. Yet, with recent advances in represen-
tation learning this task becomes a more
tangible aim. A central property of ab-
stract anaphora is that it establishes a re-
lation between the anaphor embedded in
the anaphoric sentence and its (typical-
ly non-nominal) antecedent. We propose
a mention-ranking model that learns how
abstract anaphors relate to their antece-
dents with an LSTM-Siamese Net. We
overcome the lack of training data by
generating artificial anaphoric sentence–
antecedent pairs. Our model outperforms
state-of-the-art results on shell noun re-
solution. We also report first benchmark
results on an abstract anaphora subset of
the ARRAU corpus. This corpus presents
a greater challenge due to a mixture of
nominal and pronominal anaphors and a
greater range of confounders. We found
model variants that outperform the base-
lines for nominal anaphors, without train-
ing on individual anaphor data, but still
lag behind for pronominal anaphors. Our
model selects syntactically plausible can-
didates and – if disregarding syntax – dis-
criminates candidates using deeper fea-
tures.

1 Introduction

Current research in anaphora (or coreference) res-
olution is focused on resolving noun phrases re-
ferring to concrete objects or entities in the real

†Leo Born, Juri Opitz and Anette Frank contributed
equally to this work.

world, which is arguably the most frequently oc-
curring type. Distinct from these are diverse types
of abstract anaphora (AA) (Asher, 1993) where
reference is made to propositions, facts, events or
properties. An example is given in (1) below.1

While recent approaches address the resolution
of selected abstract shell nouns (Kolhatkar and
Hirst, 2014), we aim to resolve a wide range of
abstract anaphors, such as the NP this trend in (1),
as well as pronominal anaphors (this, that, or it).

Henceforth, we refer to a sentence that contains
an abstract anaphor as the anaphoric sentence
(AnaphS), and to a constituent that the anaphor
refers to as the antecedent (Antec) (cf. (1)).

(1) Ever-more powerful desktop computers, designed with
one or more microprocessors as their ”brains”, are ex-
pected to increasingly take on functions carried out
by more expensive minicomputers and mainframes.
”[Antec The guys that make traditional hardware are
really being obsoleted by microprocessor-based ma-
chines]”, said Mr. Benton. [AnaphS As a result of this
trendAA, longtime powerhouses HP, IBM and Digital
Equipment Corp. are scrambling to counterattack with
microprocessor-based systems of their own.]

A major obstacle for solving this task is the lack
of sufficient amounts of annotated training data.
We propose a method to generate large amounts
of training instances covering a wide range of ab-
stract anaphor types. This enables us to use neu-
ral methods which have shown great success in
related tasks: coreference resolution (Clark and
Manning, 2016a), textual entailment (Bowman
et al., 2016), learning textual similarity (Mueller
and Thyagarajan, 2016), and discourse relation
sense classification (Rutherford et al., 2017).

Our model is inspired by the mention-ranking
model for coreference resolution (Wiseman et al.,
2015; Clark and Manning, 2015, 2016a,b) and
combines it with a Siamese Net (Mueller and
Thyagarajan, 2016), (Neculoiu et al., 2016) for

1Example drawn from ARRAU (Uryupina et al., 2016).

221



learning similarity between sentences. Given an
anaphoric sentence (AntecS in (1)) and a candi-
date antecedent (any constituent in a given context,
e.g. being obsoleted by microprocessor-based ma-
chines in (1)), the LSTM-Siamese Net learns rep-
resentations for the candidate and the anaphoric
sentence in a shared space. These representations
are combined into a joint representation used to
calculate a score that characterizes the relation be-
tween them. The learned score is used to select
the highest-scoring antecedent candidate for the
given anaphoric sentence and hence its anaphor.
We consider one anaphor at a time and provide the
embedding of the context of the anaphor and the
embedding of the head of the anaphoric phrase to
the input to characterize each individual anaphor –
similar to the encoding proposed by Zhou and Xu
(2015) for individuating multiply occurring predi-
cates in SRL. With deeper inspection we show that
the model learns a relation between the anaphor in
the anaphoric sentence and its antecedent. Fig. 1
displays our architecture.

In contrast to other work, our method for gener-
ating training data is not confined to specific types
of anaphora such as shell nouns (Kolhatkar and
Hirst, 2014) or anaphoric connectives (Stede and
Grishina, 2016). It produces large amounts of in-
stances and is easily adaptable to other languages.
This enables us to build a robust, knowledge-lean
model for abstract anaphora resolution that easily
extends to multiple languages.

We evaluate our model on the shell noun reso-
lution dataset of Kolhatkar et al. (2013b) and show
that it outperforms their state-of-the-art results.
Moreover, we report results of the model (trained
on our newly constructed dataset) on unrestricted
abstract anaphora instances from the ARRAU cor-
pus (Poesio and Artstein, 2008; Uryupina et al.,
2016). To our knowledge this provides the first
state-of-the-art benchmark on this data subset.

Our TensorFlow2 implementation of the model
and scripts for data extraction are available
at: https://github.com/amarasovic/
neural-abstract-anaphora.

2 Related and prior work

Abstract anaphora has been extensively stud-
ied in linguistics and shown to exhibit specific
properties in terms of semantic antecedent types,
their degrees of abstractness, and general dis-

2Abadi et al. (2015)

course properties (Asher, 1993; Webber, 1991). In
contrast to nominal anaphora, abstract anaphora is
difficult to resolve, given that agreement and lexi-
cal match features are not applicable. Annotation
of abstract anaphora is also difficult for humans
(Dipper and Zinsmeister, 2012), and thus, only
few smaller-scale corpora have been constructed.
We evaluate our models on a subset of the AR-
RAU corpus (Uryupina et al., 2016) that contains
abstract anaphors and the shell noun corpus used
in Kolhatkar et al. (2013b).3 We are not aware of
other freely available abstract anaphora datasets.

Little work exists for the automatic resolu-
tion of abstract anaphora. Early work (Eckert
and Strube, 2000; Strube and Müller, 2003; By-
ron, 2004; Müller, 2008) has focused on spoken
language, which exhibits specific properties. Re-
cently, event coreference has been addressed us-
ing feature-based classifiers (Jauhar et al., 2015;
Lu and Ng, 2016). Event coreference is re-
stricted to a subclass of events, and usually fo-
cuses on coreference between verb (phrase) and
noun (phrase) mentions of similar abstractness
levels (e.g. purchase – acquire) with no spe-
cial focus on (pro)nominal anaphora. Abstract
anaphora typically involves a full-fledged clausal
antecedent that is referred to by a highly abstract
(pro)nominal anaphor, as in (1).

Rajagopal et al. (2016) proposed a model for
resolution of events in biomedical text that refer
to a single or multiple clauses. However, instead
of selecting the correct antecedent clause(s) (our
task) for a given event, their model is restricted to
classifying the event into six abstract categories:
this these changes, responses, analysis, context,
finding, observation, based on its surrounding con-
text. While related, their task is not comparable to
the full-fledged abstract anaphora resolution task,
since the events to be classified are known to be
coreferent and chosen from a set of restricted ab-
stract types.

More related to our work is Anand and Hardt
(2016) who present an antecedent ranking ac-
count for sluicing using classical machine learn-
ing based on a small training dataset. They em-
ploy features modeling distance, containment, dis-
course structure, and – less effectively – content
and lexical correlates.4

Closest to our work is Kolhatkar et al. (2013b)

3We thank the authors for making their data available.
4Their data set was not publicized.

222



(KZH13) and Kolhatkar and Hirst (2014) (KH14)
on shell noun resolution, using classical ma-
chine learning techniques. Shell nouns are abstract
nouns, such as fact, possibility, or issue, which can
only be interpreted jointly with their shell content
(their embedded clause as in (2) or antecedent as
in (3)). KZH13 refer to shell nouns whose an-
tecedent occurs in the prior discourse as anaphoric
shell nouns (ASNs) (cf. (3)), and cataphoric shell
nouns (CSNs) otherwise (cf. (2)).5

(2) Congress has focused almost solely on the fact that
[special education is expensive - and that it takes away
money from regular education.]

(3) Environmental Defense [...] notes that [Antec Mowing
the lawn with a gas mower produces as much pollution
[...] as driving a car 172 miles.] [AnaphS This fact
may [...] explain the recent surge in the sales of [...]
old-fashioned push mowers [...]].

KZH13 presented an approach for resolving six
typical shell nouns following the observation that
CSNs are easy to resolve based on their syn-
tactic structure alone, and the assumption that
ASNs share linguistic properties with their em-
bedded (CSN) counterparts. They manually de-
veloped rules to identify the embedded clause
(i.e. cataphoric antecedent) of CSNs and trained
SVMrank (Joachims, 2002) on such instances.
The trained SVMrank model is then used to re-
solve ASNs. KH14 generalized their method to
be able to create training data for any given shell
noun, however, their method heavily exploits the
specific properties of shell nouns and does not ap-
ply to other types of abstract anaphora.

Stede and Grishina (2016) study a related phe-
nomenon for German. They examine inherently
anaphoric connectives (such as demzufolge – ac-
cording to which) that could be used to access their
abstract antecedent in the immediate context. Yet,
such connectives are restricted in type, and the
study shows that such connectives are often am-
biguous with nominal anaphors and require sense
disambiguation. We conclude that they cannot be
easily used to acquire antecedents automatically.

In our work, we explore a different direction:
we construct artificial training data using a gen-
eral pattern that identifies embedded sentence con-
stituents, which allows us to extract relatively se-
cure training data for abstract anaphora that cap-
tures a wide range of anaphora-antecedent rela-

5We follow this terminology for their approach and data
representation.

tions, and apply this data to train a model for the
resolution of unconstrained abstract anaphora.

Recent work in entity coreference resolu-
tion has proposed powerful neural network-based
models that we will adapt to the task of abstract
anaphora resolution. Most relevant for our task is
the mention-ranking neural coreference model
proposed in Clark and Manning (2015), and their
improved model in Clark and Manning (2016a),
which integrates a loss function (Wiseman et al.,
2015) which learns distinct feature representations
for anaphoricity detection and antecedent ranking.

Siamese Nets distinguish between similar and
dissimilar pairs of samples by optimizing a loss
over the metric induced by the representations. It
is widely used in vision (Chopra et al., 2005), and
in NLP for semantic similarity, entailment, query
normalization and QA (Mueller and Thyagarajan,
2016; Neculoiu et al., 2016; Das et al., 2016).

3 Mention-Ranking Model

Given an anaphoric sentence s with a marked
anaphor (mention) and a candidate antecedent c,
the mention-ranking (MR) model assigns the pair
(c, s) a score, using representations produced by
an LSTM-Siamese Net. The highest-scoring can-
didate is assigned to the marked anaphor in the
anaphoric sentence. Fig. 1 displays the model.

We learn representations of an anaphoric sen-
tence s and a candidate antecedent c using a bi-
directional Long Short-Term Memory (Hochre-
iter and Schmidhuber, 1997; Graves and Schmid-
huber, 2005). One bi-LSTM is applied to the
anaphoric sentence s and a candidate antecedent c,
hence the term siamese. Each word is represented
with a vector wi constructed by concatenating em-
beddings of the word, of the context of the anaphor
(average of embeddings of the anaphoric phrase,
the previous and the next word), of the head of
the anaphoric phrase6, and, finally, an embedding
of the constituent tag of the candidate, or the S
constituent tag if the word is in the anaphoric sen-
tence. For each sequence s or c, the word vectors
wi are sequentially fed into the bi-LSTM, which
produces outputs from the forward pass,

−→
hi, and

outputs
←−
hi from the backward pass. The final out-

put of the i-th word is defined as hi = [
←−
hi ;
−→
hi ].

To get a representation of the full sequence, hs or
hc, all outputs are averaged, except for those that
correspond to padding tokens.

6Henceforth we refer to it as embedding of the anaphor.

223



Figure 1: Mention-ranking architecture for ab-
stract anaphora resolution (MR-LSTM).

To prevent forgetting the constituent tag of the
sequence, we concatenate the corresponding tag
embedding with hs or hc (we call this a short-
cut for the tag information). The resulting vector
is fed into a feed-forward layer of exponential lin-
ear units (ELUs) (Clevert et al., 2016) to produce
the final representation h̃s or h̃c of the sequence.

From h̃c and h̃s we compute a vector hc,s =
[|h̃c − h̃s|; h̃c � h̃s] (Tai et al., 2015), where |–|
denotes the absolute values of the element-wise
subtraction, and � the element-wise multiplica-
tion. Then hc,s is fed into a feed-forward layer of
ELUs to obtain the final joint representation, h̃c,s,
of the pair (c, s). Finally, we compute the score for
the pair (c, s) that represents relatedness between
them, by applying a single fully connected linear
layer to the joint representation:

score(c, s) = W h̃c,s + b ∈ R, (1)
where W is a 1 × d weight matrix, and d the di-
mension of the vector h̃c,s.

We train the described mention-ranking model
with the max-margin training objective from
Wiseman et al. (2015), used for the antecedent
ranking subtask. Suppose that the training set
D = {(ai, si, T (ai),N (ai)}ni=1, where ai is
the i-th abstract anaphor, si the corresponding
anaphoric sentence, T (ai) the set of antecedents
of ai and N (ai) the set of candidates that are
not antecedents (negative candidates). Let t̃i =
arg maxt∈T (ai) score(ti, si) be the highest scor-

VP

v S’

x S

Figure 2: A general pattern for artificially creating
anaphoric sentence–antecedent pairs.

ing antecedent of ai. Then the loss is given by

n∑
i=1

max(0, max
c∈N (ai)

{1+score(c, si)−score(t̃i, si)}).

4 Training data construction

We create large-scale training data for abstract
anaphora resolution by exploiting a common con-
struction, consisting of a verb with an embed-
ded sentence (complement or adverbial) (cf. Fig.
2). We detect this pattern in a parsed corpus,
’cut off’ the S′ constituent and replace it with a
suitable anaphor to create the anaphoric sentence
(AnaphS), while S yields the antecedent (Antec).
This method covers a wide range of anaphora-
antecedent constellations, due to diverse semantic
or discourse relations that hold between the clause
hosting the verb and the embedded sentence.

First, the pattern applies to verbs that embed
sentential arguments. In (4), the verb doubt estab-
lishes a specific semantic relation between the em-
bedding sentence and its sentential complement.

(4) He doubts [S′ [S a Bismarckian super state will emerge
that would dominate Europe], but warns of ”a risk of
profound change in the [..] European Community from
a Germany that is too strong, even if democratic”].

From this we extract the artificial antecedent A
Bismarckian super state will emerge that would
dominate Europe, and its corresponding anaphoric
sentence He doubts this, but warns of ”a risk of
profound change ... even if democratic”, which
we construct by randomly choosing one of a pre-
defined set of appropriate anaphors (here: this,
that, it), cf. Table 1. The second row in Table 1
is used when the head of S′ is filled by an overt
complementizer (doubts that), as opposed to (4).
The remaining rows in Table 1 apply to adverbial
clauses of different types.

Adverbial clauses encode specific discourse re-
lations with their embedding sentences, often in-
dicated by their conjunctions. In (5), for example,
the causal conjunction as relates a cause (embed-
ded sentence) and its effect (embedding sentence):

224



type head of S
′

possible anaphoric phrase

empty ∅ this, that
general that, this that, this
causal because, as therefore, because of this/that,
temporal while, since, etc. during this/that
conditional if, whether if this/that is true

Table 1: S
′
-heads and the anaphoric types and

phrases they induce (most frequent interpretation).

(5) There is speculation that property casualty firms will
sell even more munis [S′ as [S they scramble to raise
cash to pay claims related to Hurricane Hugo [..] ]].

We randomly replace causal conjunctions be-
cause, as with appropriately adjusted anaphors,
e.g. because of that, due to this or therefore that
make the causal relation explicit in the anaphor.7

Compared to the shell noun corpus of KZH13,
who made use of a carefully constructed set of
extraction patterns, a downside of our method is
that our artificially created antecedents are uni-
formly of type S. However, the majority of ab-
stract anaphora antecedents found in the existing
datasets are of type S. Also, our models are in-
tended to induce semantic representations, and so
we expect syntactic form to be less critical, com-
pared to a feature-based model.8 Finally, the gen-
eral extraction pattern in Fig. 2, covers a much
wider range of anaphoric types.

Using this method we generated a dataset of ar-
tificial anaphoric sentence–antecedent pairs from
the WSJ part of the PTB Corpus (Marcus et al.,
1993), automatically parsed using the Stanford
Parser (Klein and Manning, 2003).

5 Experimental setup

5.1 Datasets

We evaluate our model on two types of anaphora:
(a) shell noun anaphora and (b) (pro)nominal ab-
stract anaphors extracted from ARRAU.

a. Shell noun resolution dataset. For comparabi-
lity we train and evaluate our model for shell noun
resolution, using the original training (CSN) and
test (ASN) corpus of Kolhatkar et al. (2013a,b).9

7In case of ambiguous conjunctions (e.g. as interpreted
as causal or temporal), we generally choose the most frequent
interpretation.

8This also alleviates problems with languages like Ger-
man, where (non-)embedded sentences differ in surface posi-
tion of the finite verb. We can either adapt the order or ignore
it, when producing anaphoric sentence – antecedent pairs.

9We thank the authors for providing the available data.

We follow the data preparation and evaluation
protocol of Kolhatkar et al. (2013b) (KZH13).

The CSN corpus was constructed from the
NYT corpus using manually developed patterns to
identify the antecedent of cataphoric shell nouns
(CSNs). In KZH13, all syntactic constituents of
the sentence that contains both the CSN and its an-
tecedent were considered as candidates for train-
ing a ranking model. Candidates that differ from
the antecedent in only one word or one word
and punctuation were as well considered as an-
tecedents10. To all other candidates we refer to as
negative candidates. For every shell noun, KZH13
used the corresponding part of the CSN data to
train SVMrank.

The ASN corpus serves as the test corpus. It
was also constructed from the NYT corpus, by se-
lecting anaphoric instances with the pattern ”this
〈shell noun〉” for all covered shell nouns. For val-
idation, Kolhatkar et al. (2013a) crowdsourced an-
notations for the sentence which contains the an-
tecedent, which KZH13 refer to as a broad re-
gion. Candidates for the antecedent were obtained
by using all syntactic constituents of the broad
region as candidates and ranking them using the
SVMrank model trained on the CSN corpus. The
top 10 ranked candidates were presented to the
crowd workers and they chose the best answer that
represents the ASN antecedent. The workers were
encouraged to select None when they did not agree
with any of the displayed answers and could pro-
vide information about how satisfied they were
with the displayed candidates. We consider this
dataset as gold, as do KZH13, although it may be
biased towards the offered candidates.11

b. Abstract anaphora resolution data set. We
use the automatically constructed data from the
WSJ corpus (Section 4) for training.12 Our test
data for unrestricted abstract anaphora resolution
is obtained from the ARRAU corpus (Uryupina
et al., 2016). We extracted all abstract anaphoric
instances from the WSJ part of ARRAU that are
marked with the category abstract or plan,13 and
call the subcorpus ARRAU-AA.

10We obtained this information from the authors directly.
11The authors provided us with the workers’ annotations

of the broad region, antecedents chosen by the workers and
links to the NYT corpus. The extraction of the anaphoric
sentence and the candidates had to be redone.

12We excluded any documents that are part of ARRAU.
13ARRAU distinguishes abstract anaphors and (mostly)

pronominal anaphors referring to an action or plan, as plan.

225



shell noun abstract anaphora

CSN
train

ASN
test

artifical
train

ARRAU-AA
test

# shell nouns / anaphors 114492 2303 8527 600

median
# of tokens

Antec 12.75 13.87 11 20.5

AnaphS 11.5 24 19 28

median
#

Antec 2 4.5 2 1

negatives 44.5 39 15 48

#
nominal 114492 2303 0 397

pronominal 0 0 8527 203

Table 2: Data statistics. For the ASN and CSN we
report statistics over all shell nouns, but classifiers
are trained independently.

Candidates extraction. Following KZH13, for
every anaphor we create a list of candidates by ex-
tracting all syntactic constituents from sentences
which contain antecedents. Candidates that differ
from antecedents in only one word, or one word
and punctuation, were as well considered as an-
tecedents. Constituents that are not antecedents
are considered as negative candidates.

Data statistics. Table 2 gives statistics of the
datasets: the number of anaphors (row 1), the me-
dian length (in tokens) of antecedents (row 2), the
median length (in tokens) for all anaphoric sen-
tences (row 3), the median of the number of an-
tecedents and candidates that are not antecedents
(negatives) (rows 4–5), the number of pronomi-
nal and nominal anaphors (rows 6–7). Both train-
ing sets, artificial and CSN, have only one possi-
ble antecedent for which we accept two minimal
variants differing in only one word or one word
and punctuation. On the contrary, both test sets
by design allow annotation of more than one an-
tecedent that differ in more than one word. Every
anaphor in the artificial training dataset is pronom-
inal, whereas anaphors in CSN and ASN are nom-
inal only. ARRAU-AA has a mixture of nominal
and pronominal anaphors.

Data pre-processing. Other details can be
found in Supplementary Materials.

5.2 Baselines and evaluation metrics
Following KZH13, we report success@n (s@n),
which measures whether the antecedent, or a can-
didate that differs in one word14, is in the first
n ranked candidates, for n ∈ {1, 2, 3, 4}. Addi-
tionally, we report the preceding sentence baseline

14We obtained this information in personal communica-
tion with one of the authors.

(PSBL) that chooses the previous sentence for the
antecedent and TAGbaseline (TAGBL) that ran-
domly chooses a candidate with the constituent
tag label in {S, VP, ROOT, SBAR}. For TAGBL
we report the average of 10 runs with 10 fixed
seeds. PSBL always performs worse than the
KZH13 model on the ASN, so we report it only
for ARRAU-AA.

5.3 Training details for our models

Hyperparameters tuning. We recorded perfor-
mance with manually chosen HPs and then tuned
HPs with Tree-structured Parzen Estimators (TPE)
(Bergstra et al., 2011)15. TPE chooses HPs for the
next (out of 10) trails on the basis of the s@1 score
on the devset. As devsets we employ the ARRAU-
AA corpus for shell noun resolution and the ASN
corpus for unrestricted abstract anaphora resolu-
tion. For each trial we record performance on the
test set. We report the best test s@1 score in 10 tri-
als if it is better than the scores from default HPs.
The default HPs and prior distributions for HPs
used by TPE are given below. The (exact) HPs we
used can be found in Supplementary Materials.

Input representation. To construct word vec-
tors wi as defined in Section 3, we used 100-dim.
GloVe word embeddings pre-trained on the Gi-
gaword and Wikipedia (Pennington et al., 2014),
and did not fine-tune them. Vocabulary was built
from the words in the training data with frequency
in {3, U(1, 10)}, and OOV words were replaced
with an UNK token. Embeddings for tags are ini-
tialized with values drawn from the uniform distri-
bution U(− 1√

d+t
, 1√

d+t

)
, where t is the number of

tags16 and d ∈ {50, qlog-U(30, 100)} the size of
the tag embeddings.17 We experimented with re-
moving embeddings for tag, anaphor and context.

Weights initialization. The size of the LSTMs
hidden states was set to {100, qlog-U(30, 150)}.
We initialized the weight matrices of the LSTMs
with random orthogonal matrices (Henaff et al.,
2016), all other weight matrices with the ini-
tialization proposed in He et al. (2015). The
first feed-forward layer size is set to a value in
{400, qlog-U(200, 800)}, the second to a value in
{1024, qlog-U(400, 2000)}. Forget biases in the
LSTM were initialized with 1s (Józefowicz et al.,
2015), all other biases with 0s.

15https://github.com/hyperopt/hyperopt.
16We used a list of tags obtained from the Stanford Parser.
17qlog-U is the so-called qlog-uniform distribution.

226



s @ 1 s @ 2 s @ 3 s @ 4

fact
(train: 43809,

test: 472)

MR-LSTM 83.47 85.38 86.44 87.08
KZH13 70.00 86.00 92.00 95.00
TAGBL 46.99 - - -

reason
(train: 4529,

test: 442)

MR-LSTM 71.27 77.38 80.09 80.54
+ tuning 87.78 91.63 93.44 93.89
KZH13 72.00 86.90 90.00 94.00
TAGBL 42.40 - - -

issue
(train: 2664,

test: 303)

MR-LSTM 88.12 91.09 93.07 93.40
KZH13 47.00 61.00 72.00 81.00
TAGBL 44.92 - - -

decision
(train: 42289,

test: 389)

MR-LSTM 76.09 85.86 91.00 93.06
KZH13 35.00 53.00 67.00 76.00
TAGBL 45.55 - - -

question
(train: 9327,

test: 440)

MR-LSTM 89.77 94.09 95.00 95.68
KZH13 70.00 83.00 88.00 91.00
TAGBL 42.02 - - -

possibility
(train: 11874,

test: 277)

MR-LSTM 93.14 94.58 95.31 95.67
KZH13 56.00 76.00 87.00 92.00
TAGBL 48.66 - - -

Table 3: Shell noun resolution results.

Optimization. We trained our model in mini-
batches using Adam (Kingma and Ba, 2015) with
the learning rate of 10−4 and maximal batch
size 64. We clip gradients by global norm
(Pascanu et al., 2013), with a clipping value in
{1.0, U(1, 100)}. We train for 10 epochs and
choose the model that performs best on the devset.

Regularization. We used the l2-regularization
with λ ∈ {10−5, log-U(10−7, 10−2)}. Dropout
(Srivastava et al., 2014) with a keep probability
kp ∈ {0.8, U(0.5, 1.0)} was applied to the out-
puts of the LSTMs, both feed-forward layers and
optionally to the input with kp ∈ U(0.8, 1.0).

6 Results and analysis

6.1 Results on shell noun resolution dataset

Table 3 provides the results of the mention-
ranking model (MR-LSTM) on the ASN corpus
using default HPs. Column 2 states which model
produced the results: KZH13 refers to the best
reported results in Kolhatkar et al. (2013b) and
TAGBL is the baseline described in Section 5.2.

In terms of s@1 score, MR-LSTM outperforms
both KZH13’s results and TAGBL without even
necessitating HP tuning. For the outlier reason
we tuned HPs (on ARRAU-AA) for different vari-
ants of the architecture: the full architecture, with-
out embedding of the context of the anaphor (ctx),
of the anaphor (aa), of both constituent tag em-

reason
ctx aa tag cut ffl1 ffl2 s@1 s@2 s@ 3 s@ 4

3 3 3 3 3 3 87.78 91.63 93.44 93.89
7 3 3 3 3 3 85.97 87.56 89.14 89.82
3 7 3 3 3 3 86.65 88.91 91.18 91.40
3 3 7 7 3 3 68.10 80.32 85.29 89.37
3 3 3 7 3 3 85.52 88.24 89.59 90.05
7 7 7 7 3 3 66.97 80.54 85.75 88.24
3 3 3 3 7 3 87.56 91.63 92.76 94.12
3 3 3 3 3 7 85.97 88.69 89.14 90.05

Table 4: Architecture ablation for reason.

bedding and shortcut (tag,cut), dropping only the
shortcut (cut), using only word embeddings as in-
put (ctx,aa,tag,cut), without the first (ffl1) and sec-
ond (ffl2) layer. From Table 4 we observe: (1) with
HPs tuned on ARRAU-AA, we obtain results well
beyond KZH13, (2) all ablated model variants per-
form worse than the full model, (3) a large perfor-
mance drop when omitting syntactic information
(tag,cut) suggests that the model makes good use
of it. However, this could also be due to a bias in
the tag distribution, given that all candidates stem
from the single sentence that contains antecedents.
The median occurrence of the S tag among both
antecedents and negative candidates is 1, thus the
model could achieve 50.00 s@1 by picking S-type
constituents, just as TAGBL achieves 42.02 for
reason and 48.66 for possibility.

Tuning of HPs gives us insight into how differ-
ent model variants cope with the task. For exam-
ple, without tuning the model with and without
syntactic information achieves 71.27 and 19.68
(not shown in table) s@1 score, respectively, and
with tuning: 87.78 and 68.10. Performance of
68.10 s@1 score indicates that the model is able
to learn without syntactic guidance, contrary to the
19.68 s@1 score before tuning.

6.2 Results on the ARRAU corpus

Table 5 shows the performance of different vari-
ants of the MR-LSTM with HPs tuned on the ASN
corpus (always better than the default HPs), when
evaluated on 3 different subparts of the ARRAU-
AA: all 600 abstract anaphors, 397 nominal and
203 pronominal ones. HPs were tuned on the ASN
corpus for every variant separately, without shuf-
fling of the training data. For the best performing
variant, without syntactic information (tag,cut),
we report the results with HPs that yielded the
best s@1 test score for all anaphors (row 4), when
training with those HPs on shuffled training data
(row 5), and with HPs that yielded the best s@1

227



all (600) nominal (397) pronominal (203)
ctx aa tag cut ffl1 ffl2 s@1 s@2 s@ 3 s@ 4 s@1 s@2 s@ 3 s@ 4 s@1 s@2 s@ 3 s@ 4

3 3 3 3 3 3 24.17 43.67 54.50 63.00 29.47 50.63 62.47 72.04 13.79 30.05 38.92 45.32
7 3 3 3 3 3 29.67 52.50 66.00 75.00 33.50 58.19 72.04 80.86 22.17 41.38 54.19 63.55
3 7 3 3 3 3 22.83 39.00 52.00 61.33 22.42 41.31 54.66 64.48 23.65 34.48 46.80 55.17
3 3 7 7 3 3 38.33 54.83 63.17 69.33 46.60 64.48 72.54 79.09 22.17 35.96 44.83 50.25
3 3 7 7 3 3 43.83 56.33 66.33 73.00 51.89 64.48 73.55 79.85 28.08 40.39 52.22 59.61
3 3 7 7 3 3 38.17 52.50 61.33 68.67 43.07 57.43 65.49 72.04 28.57 42.86 53.20 62.07
3 3 3 7 3 3 30.17 48.00 57.83 67.33 30.73 50.88 61.21 71.54 29.06 42.36 51.23 59.11
7 7 7 7 3 3 26.33 40.50 50.67 58.67 28.46 41.81 52.14 59.70 22.17 37.93 47.78 56.65
3 3 3 3 7 3 21.33 41.17 53.17 60.33 23.43 47.36 60.45 69.52 17.24 29.06 38.92 42.36
3 3 3 3 3 7 12.00 24.67 33.50 41.50 13.35 27.20 37.28 45.84 9.36 19.70 26.11 33.00

PSBL 27.67 - - - 30.48 - - - 22.17 - - -
TAGBL 38.43 - - - 40.10 - - - 35.17 - - -

Table 5: Results table for the ARRAU-AA test set. Refer to text for explanation of duplicated rows.

score for pronominal anaphors (row 6).
The MR-LSTM is more successful in resolv-

ing nominal than pronominal anaphors, although
the training data provides only pronominal ones.
This indicates that resolving pronominal abstract
anaphora is harder compared to nominal abstract
anaphora, such as shell nouns. Moreover, for
shell noun resolution in KZH13’s dataset, the
MR-LSTM achieved s@1 scores in the range
76.09–93.14, while the best variant of the model
achieves 51.89 s@1 score for nominal anaphors
in ARRAU-AA. Although lower performance is
expected, since we do not have specific training
data for individual nominals in ARRAU-AA, we
suspect that the reason for better performance for
shell noun resolution in KZH13 is due to a larger
number of positive candidates in ASN (cf. Table 2,
rows: antecedents/negatives).

We also note that HPs that yield good perfor-
mance for resolving nominal anaphors are not nec-
essarily good for pronominal ones (cf. rows 4–6 in
Table 5). Since the TPE tuner was tuned on the
nominal-only ASN data, this suggest that it would
be better to tune HPs for pronominal anaphors on
a different dataset or stripping the nouns in ASN.

Contrary to shell noun resolution, omitting syn-
tactic information boosts performance in ARRAU-
AA. We conclude that when the model is provided
with syntactic information, it learns to pick S-type
candidates, but does not continue to learn deeper
features to further distinguish them or needs more
data to do so. Thus, the model is not able to point
to exactly one antecedent, resulting in a lower s@1
score, but does well in picking a few good candi-
dates, which yields good s@2-4 scores. This is
what we can observe from row 2 vs. row 6 in Ta-
ble 5: the MR-LSTM without context embedding

(ctx) achieves a comparable s@2 score with the
variant that omits syntactic information, but better
s@3-4 scores. Further, median occurrence of tags
not in {S, VP, ROOT, SBAR} among top-4 ranked
candidates is 0 for the full architecture, and 1 when
syntactic information is omitted. The need for dis-
criminating capacity of the model is more empha-
sized in ARRAU-AA, given that the median oc-
currence of S-type candidates among negatives is
2 for nominal and even 3 for pronominal anaphors,
whereas it is 1 for ASN. This is in line with the
lower TAGBL in ARRAU-AA.

Finally, not all parts of the architecture con-
tribute to system performance, contrary to what is
observed for reason. For nominal anaphors, the
anaphor (aa) and feed-forward layers (ffl1, ffl2)
are beneficial, for pronominals only the second ffl.

6.3 Exploring the model

We finally analyze deeper aspects of the model:
(1) whether a learned representation between the
anaphoric sentence and an antecedent establishes a
relation between a specific anaphor we want to re-
solve and the antecedent and (2) whether the max-
margin objective enforces a separation of the joint
representations in the shared space.

(1) We claim that by providing embeddings of
both the anaphor and the sentence containing the
anaphor we ensure that the learned relation be-
tween antecedent and anaphoric sentence is de-
pendent on the anaphor under consideration. Fig.
3 illustrates the heatmap for an anaphoric sen-
tence with two anaphors. The i-th column of
the heatmap corresponds to absolute differences
between the output of the bi-LSTM for the i-th
word in the anaphoric sentence when the first vs.
second anaphor is resolved. Stronger color indi-

228



Figure 3: Visualizing the differences between out-
puts of the bi-LSTM over time for an anaphoric
sentence containing two anaphors.

cates larger difference, the blue rectangle repre-
sents the column for the head of the first anaphor,
the dashed blue rectangle the column for the head
of the second anaphor. Clearly, the representa-
tions differ when the first vs. second anaphor is
being resolved and consequently, joint representa-
tions with an antecedent will differ too.

(2) It is known that the max-margin objective
separates the best-scoring positive candidate from
the best-scoring negative candidate. To investi-
gate what the objective accomplishes in the MR-
LSTM model, we analyze the joint representations
of candidates and the anaphoric sentence (i.e., out-
puts of ffl2) after training. For a randomly cho-
sen instance from ARRAU-AA, we plotted out-
puts of ffl2 with the tSNE algorithm (v.d. Maaten
and Hinton, 2008). Fig. 4 illustrates that the joint
representation of the first ranked candidate and the
anaphoric sentence is clearly separated from other
joint representations. This shows that the max-
margin objective separates the best scoring posi-
tive candidate from the best scoring negative can-
didate by separating their respective joint repre-
sentations with the anaphoric sentence.

7 Conclusions

We presented a neural mention-ranking model for
the resolution of unconstrained abstract anaphora,
and applied it to two datasets with different types
of abstract anaphora: the shell noun dataset and
a subpart of ARRAU with (pro)nominal abstract
anaphora of any type. To our knowledge this
work is the first to address the unrestricted ab-
stract anaphora resolution task with a neural net-
work. Our model also outperforms state-of-the-art
results on the shell noun dataset.

In this work we explored the use of purely artifi-
cially created training data and how far it can bring

Figure 4: tSNE projection of outputs of ffl2. La-
bels are the predicted ranks and the constituent tag.

us. In future work, we plan to investigate mixtures
of (more) artificial and natural data from different
sources (e.g. ASN, CSN).

On the more challenging ARRAU-AA, we
found model variants that surpass the baselines for
the entire and the nominal part of ARRAU-AA, al-
though we do not train models on individual (nom-
inal) anaphor training data like the related work
for shell noun resolution. However, our model still
lags behind for pronominal anaphors. Our results
suggest that models for nominal and pronominal
anaphors should be learned independently, start-
ing with tuning of HPs on a more suitable devset
for pronominal anaphors.

We show that the model can exploit syntactic
information to select plausible candidates, but that
when it does so, it does not learn how to distin-
guish candidates of equal syntactic type. By con-
trast, if the model is not provided with syntactic
information, it learns deeper features that enable
it to pick the correct antecedent without narrow-
ing down the choice of candidates. Thus, in or-
der to improve performance, the model should be
enforced to first select reasonable candidates and
then continue to learn features to distinguish them,
using a larger training set that is easy to provide.

In future work we will design such a model, and
offer it candidates chosen not only from sentences
containing the antecedent, but the larger context.

Acknowledgments

This work has been supported by the German Re-
search Foundation as part of the Research Training
Group Adaptive Preparation of Information from
Heterogeneous Sources (AIPHES) under grant
No. GRK 1994/1. We would like to thank anony-
mous reviewers for useful comments and espe-
cially thank Todor Mihaylov for the model imple-
mentations advices and everyone in the Computa-
tional Linguistics Group for helpful discussion.

229



References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene

Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Mané, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Vasude-
van, Fernanda Viégas, Oriol Vinyals, Pete Warden,
Martin Wattenberg, Martin Wicke, Yuan Yu, and Xi-
aoqiang Zheng. 2015. TensorFlow: Large-scale ma-
chine learning on heterogeneous systems. Software
available from tensorflow.org.

Pranav Anand and Daniel Hardt. 2016. Antecedent se-
lection for sluicing: Structure and content. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1234–
1243, Austin, Texas.

Nicholas Asher. 1993. Reference to Abstract Objects
in Discourse. Kluwer Academic Publishers.

James Bergstra, Rémi Bardenet, Yoshua Bengio, and
Balázs Kégl. 2011. Algorithms for hyper-parameter
optimization. In Proceedings of the 35th Annual
Conference on Neural Information Processing Sys-
tems (NIPS), Granada, Spain.

Samuel R. Bowman, Jon Gauthier, Abhinav Ras-
togi, Raghav Gupta, Christopher D. Manning, and
Christopher Potts. 2016. A fast unified model for
parsing and sentence understanding. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1466–1477, Berlin, Germany.

Donna K. Byron. 2004. Resolving pronominal refer-
ence to abstract entities. Ph.D. thesis, University of
Rochester, Rochester, New York.

Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005.
Learning a similarity metric discriminatively, with
application to face verification. In Computer Vision
and Pattern Recognition, 2005. CVPR 2005. IEEE
Computer Society Conference on, volume 1, pages
539–546. IEEE.

Kevin Clark and Christopher D. Manning. 2015.
Entity-centric coreference resolution with model
stacking. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (ACL), Beijing, China.

Kevin Clark and Christopher D. Manning. 2016a.
Deep reinforcement learning for mention-ranking
coreference models. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), Austin, Texas.

Kevin Clark and Christopher D. Manning. 2016b. Im-
proving coreference resolution by learning entity-
level distributed representations. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (ACL), Berling, Germany.

Djork-Arné Clevert, Thomas Unterthiner, and Sepp
Hochreiter. 2016. Fast and accurate deep network
learning by exponential linear units (elus). In Pro-
ceedings of the 4th International Conference on
Learning Representations (ICLR), San Juan, Puerto
Rico.

Arpita Das, Harish Yenala, Manoj Kumar Chinnakotla,
and Manish Shrivastava. 2016. Together we stand:
Siamese networks for similar question retrieval. In
Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (ACL),
Berling, Germany.

Stefanie Dipper and Heike Zinsmeister. 2012. Anno-
tating Abstract Anaphora. Language Resources and
Evaluation, 46(1):37–52.

Miriam Eckert and Michael Strube. 2000. Dialogue
acts, synchronising units and anaphora resolution.

Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise Phoneme Classification With Bidirectional
LSTM And Other Neural Network Architectures.
Neural Networks, 18:602–610.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Delving deep into rectifiers: Surpass-
ing human-level performance on imagenet classifi-
cation. In 2015 IEEE International Conference on
Computer Vision (ICCV).

Mikael Henaff, Arthur Szlam, and Yann LeCun. 2016.
Recurrent orthogonal networks and long-memory
tasks. In Proceedings of the the 33rd Interna-
tional Conference on Machine Learning (ICML),
New York City, USA.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Sujay Kumar Jauhar, Raul Guerra, Edgar Gonzàlez Pel-
licer, and Marta Recasens. 2015. Resolving
discourse-deictic pronouns: A two-stage approach
to do it. In Proceedings of the Fourth Joint Con-
ference on Lexical and Computational Semantics,
pages 299–308, Denver, Colorado.

Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the 8th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 133–142.

Rafal Józefowicz, Wojciech Zaremba, and Ilya
Sutskever. 2015. An empirical exploration of recur-
rent network architectures. In Proceedings of the
32nd International Conference on Machine Learn-
ing (ICML), Lille, France.

230



Diederik Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceed-
ings of the 3rd International Conference on Learn-
ing Representations (ICLR), San Diego, USA.

Dan Klein and Christopher D Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430. Asso-
ciation for Computational Linguistics.

Varada Kolhatkar and Graeme Hirst. 2014. Resolv-
ing shell nouns. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 499–510, Doha, Qatar.

Varada Kolhatkar, Heike Zinsmeister, and Graeme
Hirst. 2013a. Annotating anaphoric shell nouns with
their antecedents. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse, pages 112–121, Sofia, Bulgaria.

Varada Kolhatkar, Heike Zinsmeister, and Graeme
Hirst. 2013b. Interpreting anaphoric shell nouns us-
ing antecedents of cataphoric shell nouns as train-
ing data. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 300–310, Seattle, Washington,
USA.

Jing Lu and Vincent Ng. 2016. Event Coreference Res-
olution with Multi-Pass Sieves. In Proceedings of
the Tenth International Conference on Language Re-
sources and Evaluation (LREC), pages 3996–4003,
Portoroz.

Laurens v.d. Maaten and Geoffrey Hinton. 2008. Visu-
alizing data using t-sne. Journal of Machine Learn-
ing Research, 9(2579-2605):85.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics, 19(2):313–330.

Jonas Mueller and Aditya Thyagarajan. 2016. Siamese
recurrent architectures for learning sentence similar-
ity. In Proceedings of the 13th Conference on Artifi-
cial Intelligence (AAAI), pages 2786–2792, Phoenix,
Arizona.

Christoph Müller. 2008. Fully Automatic Resolution of
It, This and That in Unrestricted Multi-Party Dialog.
Ph.D. thesis, Universität Tübingen, Tübingen.

Paul Neculoiu, Maarten Versteegh, and Mihai Rotaru.
2016. Learning Text Similarity with Siamese Recur-
rent Networks. In Proceedings of the 1st Workshop
on Representation Learning for NLP, pages 148–
157, Berlin, Germany.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neural
networks. In Proceedings of the 30th International
Conference on Machine Learning (ICML), Atlanta,
USA.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Massimo Poesio and Ron Artstein. 2008. Anaphoric
Annotation in the ARRAU Corpus. In Proceedings
of the Sixth International Conference on Language
Resources and Evaluation (LREC’08), Marrakech,
Morocco.

Dheeraj Rajagopal, Eduard Hovy, and Teruko Mita-
mura. 2016. Unsupervised event coreference for
abstract words. In Proceedings of EMNLP 2016
Workshop on Uphill Battles in Language Process-
ing: Scaling Early Achievements to Robust Methods,
pages 22–26, Austin, Texas.

Attapol T. Rutherford, Vera Demberg, and Nianwen
Xue. 2017. A Systematic Study of Neural Discourse
Models for Implicit Discourse Relation. In Proceed-
ings of the 15th Conference of the European Chapter
of the Association for Computational Linguistics.

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: a simple way to prevent neural
networks from overfitting. Journal of Machine
Learning Research, 15:1929–1958.

Manfred Stede and Yulia Grishina. 2016. Anaphoric-
ity in Connectives: A Case Study on German. In
Proceedings of the Coreference Resolution Beyond
OntoNotes (CORBON) Workshop, San Diego, Cali-
fornia.

Michael Strube and Christoph Müller. 2003. A ma-
chine learning approach to pronoun resolution in
spoken dialogue. In Proceedings of the 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 168–175, Sapporo, Japan.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (ACL), Beijing, China.

Olga Uryupina, Ron Artstein, Antonella Bristot, Fed-
erica Cavicchio, Kepa J Rodriguez, and Massimo
Poesio. 2016. ARRAU: Linguistically-Motivated
Annotation of Anaphoric Descriptions. In Pro-
ceedings of the Tenth International Conference on
Language Resources and Evaluation (LREC), pages
2058–2062, Portoroz.

Bonnie Lynn Webber. 1991. Structure and ostension in
the interpretation of discourse deixis. Language and
Cognitive processes, 6(2):107–135.

Sam Joshua Wiseman, Alexander Matthew Rush, Stu-
art Merrill Shieber, and Jason Weston. 2015. Learn-
ing anaphoricity and antecedent ranking features for

231



coreference resolution. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (ACL),
Beijing, China.

Jie Zhou and Wei Xu. 2015. End-to-end learning of
semantic role labeling using recurrent neural net-
works. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1127–1137, Beijing, China.

232


