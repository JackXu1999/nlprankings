



















































Neural Sequence-to-sequence Learning of Internal Word Structure


Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 184–194,
Vancouver, Canada, August 3 - August 4, 2017. c©2017 Association for Computational Linguistics

Neural Sequence-to-sequence Learning of Internal Word Structure

Tatyana Ruzsics Tanja Samardžić
CorpusLab, URPP Language and Space,

University of Zurich, Switzerland
{tatiana.ruzsics, tanja.samardzic}@uzh.ch

Abstract

Learning internal word structure has re-
cently been recognized as an important
step in various multilingual processing
tasks and in theoretical language com-
parison. In this paper, we present a
neural encoder-decoder model for learn-
ing canonical morphological segmenta-
tion. Our model combines character-level
sequence-to-sequence transformation with
a language model over canonical seg-
ments. We obtain up to 4% improvement
over a strong character-level encoder-
decoder baseline for three languages. Our
model outperforms the previous state-of-
the-art for two languages, while eliminat-
ing the need for external resources such
as large dictionaries. Finally, by compar-
ing the performance of encoder-decoder
and classical statistical machine transla-
tion systems trained with and without cor-
pus counts, we show that including corpus
counts is beneficial to both approaches.

1 Introduction

One of the most obvious structural differences be-
tween languages is the variation in the complex-
ity of internal word structure. In some languages,
such as English, words are relatively short and
morphologically less complex. In other languages,
such as Chintang in Example 11, words tend to
be long and encapsulate rather rich structure. The
Chintang verb thaptakha in Example 1 consists of
a number of morphemes expressing the impera-
tive mode, aspect and deixis. The information ex-
pressed by a single Chintang verb requires several

1The example is adapted from (Stoll et al., In press)

words in English, as it can be seen in the glosses.2

and in the translation.

Example 1
a. cuwa thaptakha
b. cuwa thapt -a -khag -a
c. water move -IMP -see -IMP

across [2sS]
d. Bring some water over here!

The variation in word structure is observed even
in common categories such as plural, which is typ-
ically part of a word, but expressed using different
structures. The items in Example 23 show three
different structural types associated with express-
ing plural across languages.

Example 2
Type Sg. Pl.
Suffix Turkish ev ev-ler

‘house’ ‘houses’
Prefix Swahili m-toto wa-toto

‘child’ ‘children’
Redupli- Malay anak anak-anak
cation ‘child’ ‘children’

With the spread of natural language process-
ing to a wider range of languages, learning in-
ternal word structure becomes increasingly impor-
tant for developing practical applications. Analy-
sis of internal word structure, usually termed mor-
phological segmentation, has been shown helpful
in tasks such as machine translation (Dyer et al.,
2008; Narasimhan et al., 2014), speech process-
ing (Creutz et al., 2007) and parsing (Seeker and
Çetinoglu, 2015). Additionally, there is a grow-
ing interest in automatic learning of morphologi-
cal segmentation for the purpose of theoretical lan-

2Cf. Leipzig Glossing Rules at https://www.eva.
mpg.de/lingua/resources/glossing-rules.
php

3The examples are adapted from (Eifring and Theil, 2005)

184



guage comparison (Bentz et al., 2017). In this con-
text, it becomes particularly important to be able
to process a wide variety of languages, for which
the available data sets consist of small, annotated
corpora.

In the present work, we cast the task of
morphological segmentation as supervised neu-
ral sequence-to-sequence learning over characters.
Our goal is to define a method for automatic seg-
mentation that can be easily ported across lan-
guages, taking advantage of the relatively small,
manually analyzed corpora increasingly available
in the linguistic community. Our approach there-
fore needs to rely only on the data available in an
annotated corpus.

We follow the line of research based on the soft-
attention encoder-decoder paradigm (Bahdanau
et al., 2014). This paradigm was recently applied
to the task of canonical segmentation by Kann
et al. (2016). It was shown to achieve the state-of-
the-art performance when combined with a neural
re-ranker method that employs additional external
dictionary information (Kann et al., 2016). Our
approach improves the results achieved by Kann
et al. (2016) in case of Indonesian and German
languages eliminating at the same time the need
for resources outside of annotated corpora.

2 Related Work

The task of the morphological segmentation can
be defined in two ways, illustrated with the Chin-
tang verb from Example 1:

a. Surface segmentation:
thaptakha→ thapt-a-kh-a

b. Canonical segmentation:
thaptakha→ thapt-a-khag-a

The term surface or allomorph segmentation is
used by Creutz and Linden (2004) to refer to the
analysis where the input word is segmented into
substrings without any further string transforma-
tion. This definition is most widely applied in
computational processing; it is, however, too sim-
plistic for the majority of languages. It does not
allow, for instance, to identify -es in bus-es and -s
in car-s as two variants of the same English plural
marker.

More recently, the term canonical segmentation
was used by Cotterell et al. (2016) to refer to the
same definition that was termed morpheme seg-
mentation by Creutz and Linden (2004). In this

case, a more abstract internal word structure is
learned by transforming the resulting substrings
into their canonical forms.

While a great variety of methods has been pro-
posed for surface segmentation, canonical seg-
mentation, which is address in our work, has
started being addressed only recently.

The task of surface morphological segmentation
is traditionally approached using finite-state tech-
nology, such as OpenFst library (Allauzen and Ri-
ley, 2012) and OpenGrm Thrax Grammar Com-
piler library (Roark et al., 2012), with sequence
modeling used to disambiguate the finite-state out-
put (Heintz, 2008).

Another line of research has addressed sur-
face segmentation with unsupervised algorithms
(MORFESSOR (Creutz and Lagus, 2002), MOR-
FESSOR CAT-MAP (Creutz and Lagus, 2005)),
and, more recently, with semi-supervised ap-
proaches (Poon et al., 2009; Kohonen et al.,
2010)). Narasimhan et al. (2015) include seman-
tic information in the unsupervised learning, stay-
ing at the level of surface segmentation. Their
approach is extended by Bergmanis and Goldwa-
ter (2017), who make a step towards canonical
segmentation, proposing a method to generalize
over spelling variants of functionally similar mor-
phemes.

Supervised approaches to learning morpholog-
ical structure are rather rare. Ruokolainen et al.
(2013) apply conditional random fields algorithm,
used for different sequence classification tasks,
to the task of surface segmentation. Their CRF-
MORPH system tags each character of a morpho-
logically complex word with one of the tags ‘B’
for the beginning, ‘M’ middle, and ‘E’ end of a
segment, and ‘S’ for a single character segment.
The CHIPMUNK model of Cotterell et al. (2015)
based on a semi-Markov model extends the CRF-
MORPH approach by adding features from stand-
alone dictionaries and affix lists.

Canonical segmentation is tackled by Cotterell
et al. (2016), who develop a log-linear model on
conjunction of a finite-state transduction model
for modeling orthographic changes and a semi-
Markov segmentation model.

Recently, neural network models achieved
state-of-the-art results for both types of segmen-
tation tasks. Wang et al. (2016) applied window
LSTM model for surface segmentation. Kann
et al. (2016) improved the results by Cotterell

185



et al. (2016) on canonical segmentation by apply-
ing the encoder-decoder RNN framework. Kann
et al. (2016) achieve the current state-of-the-art for
canonical segmentation by re-ranking the output
of the encoder-decoder system. The re-ranking
component is a multilayer perception run on the
morphemes embeddings (Kann et al., 2016). The
morphemes embedding used for this re-ranking
model are calculated using additional information
from the Aspell dictionaries. We follow Kann
et al. (2016) in using the encoder-decoder RNN
framework, but we do not use any external re-
sources. Instead of that, we extract and exploit
more information from the training corpus.

Our approach is in the spirit of the “shallow fu-
sion” approach to machine translation of Gulcehre
et al. (2017). Like Gulcehre et al. (2017), we in-
tegrate a language model into an encoder-decoder
framework. There are, however, several important
differences.

First of all, the role of the language model is dif-
ferent. Integrating a language model allows Gul-
cehre et al. (2017) to augment the parallel train-
ing data with additional monolingual corpora on
the target side. In this way, they add new infor-
mation about sequencing, not captured in training
on parallel data alone. Both components of their
system are trained on the same kind of units —
characters. As opposed to this, we use a language
model to extract more information from the paral-
lel data. We add new information by training the
system at two levels: the basic encoder-decoder
component is trained on character sequences and
the language model component is trained on the
sequences of morphemes. In the case of Gulcehre
et al. (2017), the use of a language model is moti-
vated by the fact that external monolingual target-
side data is almost always universally available.
The situation is reversed for the task of morpho-
logical segmentation: morphologically segmented
corpora are produced manually by experts in the
process of linguistic analysis and they tend to be
small and expensive. Our approach is motivated
by the need to extract as much information as pos-
sible from relatively small target-side data sets.

Second, we add a third component to our model
which controls for the difference in characters
length between the input word string and the out-
put segmentation string. This helps overcome lan-
guage model preference for a short output.

Last, while Gulcehre et al. (2017) use a lan-

guage model implemented with recurrent neural
networks, we employ a statistical language model,
which is better adapted to our settings with small
data sets.

Finally, we note that corpus frequencies are not
used in previous supervised approaches, but that
systems are trained on word types (training data
consist of a list of word types and a segmentation
for each type). In the present work, we study token
versus types training set up and how this difference
affects the performance of statistical models.

3 Model Description

Given an input sequence, such as the Chintang
sentence in Example 1 (line a.), we produce a
canonical segmentation (line b.), where we recog-
nize that the sequence kh in the surface form is an
instance of the light verb khag.

We follow the notations of Kann et al. (2016)
in the formalization of the task of canonical seg-
mentation. First, we define two discrete alphabets,
Σ of the surface symbols and Σcan of the canon-
ical symbols. For many languages these two al-
phabets coincide, for example in the case of En-
glish they consist of 26 letters of the Latin alpha-
bet. In the case of Chintang, these alphabets are
different: the surface symbols express more spe-
cific pronunciation features. Our task is to learn
a mapping from a surface word form w ∈ Σ∗
(e.g., w=‘thaptakha’), to its canonical segmenta-
tion c ∈ Ω∗ (e.g., c=thapt|-a|-khag|-a). We define
Ω = Σcan ∪ {|}, where the symbol ‘|’ marks seg-
mentation boundaries.4

To learn the mappings, we combine the general
sequence transformation framework — known as
the encoder-decoder RNN — trained on a charac-
ter level with a language model trained on mor-
phemes. Note that a kind of a language model
over characters is implicitly included as a part
of the decoder in the character-based encoder-
decoder RNN. The language model in our ap-
proach is trained over higher level units, provid-
ing additional information about the sequences.
This, however, poses a challenge for its integration
in the general framework. We tackle this prob-
lem with our “synchronization” method applied at
the decoding stage: the segmentation hypotheses

4Furthermore, specifically to the Chintang corpus, the
canonical symbols additionally use a dash element ‘-’ to dis-
tinguish between suffixes, prefixes and roots. We do not ex-
clude this symbol in our experiments.

186



are expanded and scored using a log-linear com-
bination of (a) scores from a lower-level encoder-
decoder model and (b) higher-level scores of the
language model. The fusion of the scores is trig-
gered only at the segmentation boundaries.

In this section, we first review the encoder-
decoder RNN framework. Then, we present our
fusion approach for integrating a language model
into the encoder-decoder system.

3.1 Background: Standard Encoder-Decoder
Set-up (cED)

The canonical segmentation problem fits the
general sequence-to-sequence framework, that is
mapping a variable-length sequence to another
variable-length sequence. In machine translation,
a relatively standard way to perform this task is
using encoder-decoder RNN (Cho et al., 2014;
Sutskever et al., 2014), extended with a bidirec-
tional encoder and attention mechanism of Bah-
danau et al. (2014). For the task of canonical seg-
mentation, the input and the output to the encoder-
decoder model is represented as a sequence of
characters separated by spaces. Formally, we use
the following set up.

The encoder RNN processes the input sequence
of vectors, X = (x1, . . . , xnx), into a sequence of
vectors representing hidden states:

ht = f(ht−1, xt), t = 1, . . . , nx (1)

where f stands for gated recurrent units (Cho
et al., 2014). The decoder RNN is conditioned
on the information produced by the encoder to
generate the output sequence Y = (y1, . . . , yny).
Specifically, the decoder RNN models a condi-
tional probability at each step as a function of pre-
vious output, current decoder hidden state and cur-
rent context vector:

p(yt|y1, . . . , yt−1, X) = g(yt−1, st, ct), (2)

st = f(st−1, yt−1, ct), t = 1, . . . , ny (3)

The context vector ct is computed at each step as
a weighted sum of the hidden states:

ct =
nx∑

k=1

αtkhk, (4)

where the weights are calculated by an alignment
model which scores how well the inputs around

the position k and the output at the position t
match. Intuitively, the decoder produces an output
element one at a time, each time focusing (putting
attention) on a different part of the input sequence
in order to gather the details that are required to
produce the next output element.

We use the bidirectional setting of the encoder
model (Bahdanau et al., 2014): the hidden state
ht for each time step is obtained by concatenat-
ing a forward and backward state ht = [

−→
ht ;
←−
ht ].

This means that the hidden state contains the sum-
maries of both the preceding elements and the fol-
lowing elements. We refer to this standard frame-
work as cED.

3.2 Integrating a Morpheme Language
Model (LM) into cED

Before the integration, we assume that cED sys-
tem and language model LM have been trained
separately. The cED system is trained on charac-
ter sequences in a parallel corpus where the source
side consists of unsegmented words and the target
side consist of the aligned canonically segmented
words. During the training the cED model learns
conditional probability distribution over the char-
acter sequences (2). The LM is trained over mor-
pheme sequences on the target side of the corpus
and scores how likely a given sequence is in a
given language.

We can find the most probable segmentation us-
ing a beam search algorithm guided by “synchro-
nized” character-level cED and morpheme-level
LM scores. At each time step t, the cED system
computes a score for each possible next charac-
ter yt in the vocabulary Ω as a continuation of the
segmentation hypothesis from the previous step
{(y1y2 . . . yt−1)i}, i = 1, . . . ,K where K is the
beam size (how many best scored segmentation
hypothesis we keep from each step). This score is
a logarithm of the probability (2). Then, each pos-
sible continuation {(y1y2 . . . yt−1)iyt}, yt ∈ Ω,
i = 1, . . . ,K gets a score which is a sum of cED
scores for each character, that is, a sum of the
scores for a hypothesis from a previous time step
(y1y2 . . . yt−1)i and a score for the next character
yt. Thus we get a set of |Ω|×K new hypothesis of
length |t| together with their respective scores. All
these new hypotheses at the step t are then sorted
according to their respective scores, and the top K
ones are selected as candidates for the expansion
at the next time step.

187



In order to guide the described beam decod-
ing with the LM scores we perform a “synchro-
nization”. Specifically, we continue the beam
search based on the character cED scores till the
step s1 where all the segmentation hypothesis
{(y1y2 . . . ys1)i} i = 1, . . . ,K end with a bound-
ary symbol. The boundary symbol can be either
end of word symbol ‘< /w >’ or a segmentation
boundary symbol ‘|’. At this step, we re-score the
segmentation hypotheses with a weighted sum of
the cED score and the LM score:

log p(ys1|y1, . . . , ys1−1, X)
= log pcED(ys1|y1, . . . , ys1−1, X)

+ αLM log pLM (y1, . . . , ys1−1) (5)

At this step, y1, . . . , ys1 is considered a se-
quence of s1 characters by the cED system and
y1, . . . , ys1−1 (without the last boundary symbol)
is considered one morpheme by the LM.

From this synchronization point s1 we
continue to expand the re-scored hypothesis
{(y1y2 . . . ys1)i} i = 1, . . . ,K at the next
time step using again only cED scores. We
continue this process until we get to the next
synchronization point s2 where all the hypoth-
esis {(y1y2 . . . ys2)i} i = 1, . . . ,K end with a
boundary symbol. After rescoring them with a
weighted sum of cED and LM we continue this
process again till the next synchronization point.
The decoding process ends at a synchronization
point where the last symbol of the best scored
hypothesis (using the combined cED and LM
score) is an end-of-word symbol.

The described decoding process therefore
scores the segmentation hypotheses at two levels:
normally working at the character level with cED
scores and adding the LM scores only when it hits
a boundary symbol. In this way, the LM score
helps to evaluate how probable the last generated
morpheme is based on the morpheme history, that
is the sequence of morphemes generated at the
previous synchronization time steps.

3.3 The Length Constraint
It is well known that language models give higher
preference to shorter sequences. This becomes
an issue in the proposed fused model described
above: at the synchronization points high LM
scores tend to stop further hypothesis expansion.
For example, only the first segment can be gen-
erated as a model output if it happens to be a fre-

quent standalone word. This leads to favoring seg-
mentation predictions where the output is shorter
than the input, which is rarely plausible in segmen-
tation. Our early experiments confirmed this intu-
ition, therefore we consider the length constraint
component to be an integral part of the language
model inclusion and we do not report experiments
without this component.

To deal with the length issue, we add a “length
constraint” component LC. The LC score is based
on the difference in character length between the
input word and its segmentation hypothesis. To
synchronize the LC score with LM scoring process
described before we assign it only at the synchro-
nization time steps and attach it to the boundary
symbol. Therefore, the LC score, combined with
the LM score, helps to evaluate how probable is
the last generated morpheme given the sequence
of morphemes generated at the previous steps.

Assume that the input word is X = x1 . . . xn
and the produced segmentation hypothesis at the
first synchronization step s1 is y1 . . . ys1 where
ys1 is a boundary symbol. Then the LC score as-
signed to the morpheme y1 . . . ys1−1 and attached
to the boundary symbol ys1 is calculated as the
negative value of the absolute difference between
the morpheme length and input word length di-
vided by the the input length: LC(y1 . . . ys1−1) =
−(|y1 . . . ys1−1| − |X|)/|X| = −|s1 − 1 −
n|/n. At the next synchronization point s2 the
LC score is calculated using the length of the
next produced segment: LC(ys1+1 . . . ys2−1) =
−(|ys1+1 . . . ys2−1|−|X|)/|X|. In a general case,
the LC score for the last generated segment σi can
be expressed as

LC(σi) = −(|σi| − |X|)/|X| (6)

Note that boundary symbols are excluded for the
segments length calculation.

The intuition behind the LC score is that it gives
a contribution to the total score of a segmenta-
tion hypothesis showing how different the length
of the so far produced hypothesis is compared to
the length of the input word. The characters in
the canonical segments tend to be either inserted
or deleted compared to their surface form equiv-
alents, therefore we measure LC score using an
absolute difference in the length. The higher the
absolute value of the difference between the input
and the hypothesis, the higher the penalty.

With the inclusion of the LC score for the length

188



control the total score of our fusion model be-
comes:

log p(ys1|y1, . . . , ys1−1, X)
= log pcED(ys1|y1, . . . , ys1−1, X)
+ αLM log pLM (y1, . . . , ys1−1)

+ αLCLC(y1, . . . , ys1−1) (7)

where the weights αLM and αLC are optimized on
a development set.

4 Data and Experiments

In this section, we first give a description of the
corpora that we employ for the experiments. Then,
we discuss the experimental setup for our model.
Finally, we discuss the different configurations of
the corpus we employ to explore encoder-decoder
model behavior with and without corpus frequen-
cies.

4.1 Corpora

We run our experiments on the datasets for En-
glish, German and Indonesian released by Cot-
terell et al. (2016).5 The corpus for each language
is constructed based on the 10,000 forms selected
at random from a uniform distribution over types.
This data is further used to sample 5 splits into
8000 training forms, 1000 development forms and
1000 test forms. Following Cotterell et al. (2016)
and Kann et al. (2016), we report the results on
each of the 5 splits for all three languages.

In addition to these sets, we use a manually seg-
mented and glossed corpus of Chintang (Bickel
et al., 2004-2015), a language that features a
high degree of synthesis and free prefix ordering
(Bickel et al., 2007). The total corpus size of
955,025 word tokens makes the Chintang corpus
an exceptional resource for the task of morpholog-
ical segmentation. As discussed in Section 2, the
target segmented data is not easily available and
corpora of this size are not likely to be developed
for many langauges. We are interested in exper-
imenting with a realistic setup, therefore we use
around 150,000 tokens out of the total corpus size.
This set is divided into the training set (around
100,000 word tokens) and development and test
set (around 25,000 word tokens each).

5ryancotterell.github.io/
canonical-segmentation

The data set taken from Cotterell et al. (2016)
allows us a comparison of our system with the pre-
vious state-of-the-art. The Chintang set allows us
to run the segmentation models in different train-
ing regimes, with and without corpus frequencies,
and therefore to assess the influence of the corpus
counts on the performance.

4.2 Tools

We combine the three different components, cED,
LM and LC, into a single fused model using
the SGNMT (syntactically guided neural machine
translation) framework of Stahlberg et al. (2016).6

This framework provides an elegant solution to de-
composing an encoder-decoder system into three
components: training, decoding and scoring.

The training module implements the encoder-
decoder model with attention mechanism using
the Blocks framework built on top of Theano.7.
We employ this implementation for the cED
model.

The scoring component of SGNMT consists of
predictor modules, which define scores over the
target vocabulary given the current internal pre-
dictor state, the history, the source sentence, and
external side information. Predictors can be com-
bined with other predictors to form complex de-
coding tasks. In the case of our model, we use
three predictors: cED, LM and LC.

The decoding component is represented by the
decoder modules which are search strategies that
traverse the space spanned by the predictors. We
use a beam search module.

We train the language model LM over mor-
pheme sequences using SRILM toolkit.8. The
model is trained on the target side of the parallel
corpus, i.e. the canonical segmentations.

The weights of the predictors are optimized us-
ing MERT (Och, 2003). This is a standard opti-
mization routine in statistical machine translation
which searches for the weights of the model com-
ponents by directly maximizing the performance
of the system on a development set. We use the
Z-MERT tool9 in our implementation. The code is
available on our GitHub account.10

6http://ucam-smt.github.io/sgnmt
7https://github.com/mila-udem/blocks
8http://www.speech.sri.com/projects/

srilm/
9http://www.cs.jhu.edu/˜ozaidan/zmert/

10https://github.com/tatyana-ruzsics/
uzh-corpuslab-morphological-segmentation

189



4.3 Experimental Setup

Baseline and comparison As a baseline, we
use the basic component of our model (cED), an
ensemble of 5 character-level attention encoder-
decoder models with the hyperparameters de-
scribed below.

We also compare the encoder-decoder model
to the character-level statistical machine transla-
tion (cSMT). This approach is a natural choice in
machine translation with small training sets, but
no results have been reported so far for the task
of canonical segmentation. We used the Moses
toolkit with the following settings: distortion is
disallowed and build-in MERT optimization is
used to optimize the translation model and lan-
guage model.

As a reference, we compare our results to the
state-of-the-art neural re-ranker model of Kann
et al. (2016) Note, however, that the results can-
not be directly compared since Kann et al. (2016)
use extra training material in the form of external
dictionaries.

Evaluation Since our method is intended to be
used for processing corpora, the evaluation is per-
formed at the level of word tokens using accuracy
of the full segmentation. In addition, we evaluate
the performance on subsets of test words. Besides
the seen words, we distinguish between two kinds
of test words that are not seen in the training cor-
pus: a) new combinations of morphemes already
seen during training and b) words that contain un-
seen morphemes.

Hyperparameters The bidirectional encoder
consists of a forward and a backward recurrent
RNN each having 100 hidden units. The decoder
also has 100 hidden units. The dimensionality of
the character embedding is 300. We use a mini-
batch stochastic gradient descent (SGD) algorithm
together with Adadelta to train each model. Each
update direction is computed using a minibatch of
20 training examples. At decoding, we use a beam
search with a beam size of 12 to find the segmenta-
tion that approximately maximizes the conditional
probability. All the described hyperparameters are
the same as in the work of Kann et al. (2016).

Initialization of all weights (encoder, decoder,
embeddings) to the identity matrix and the biases
to zero (Le et al., 2015) results in a very fast
convergence rate compared to other initializations.
We train a single model for 20 epochs with an

early stopping based on the development set per-
formance. We also shuffle the training data be-
tween the epochs.

Finally, we use an ensemble of five encoder-
decoder models with different random initializa-
tions. We shuffle the training data for each of the
model using different seed value. The ensemble
model is based on a combined score from all 5
models and is used to guide the decoding process.

Following earlier experiments, we use mor-
pheme 3-gram language model and apply Kneser-
Ney smoothing.

As the objective for the MERT weight optimiza-
tion we use accuracy on the development set.

Tokens vs Types The Chintang corpus allows
us to assess the influence of the corpus counts on
the performance of the models used for canonical
segmentation. In these experiments, we run only
the two baseline models, cSMT and cED (without
our language model component), in order to eval-
uate directly the relevance of such corpus signal to
these two training paradigms.11

We run each model, cSMT and cED, in two
regimes. In the first, type regime, we train the
models using a parallel corpus which consists of
word types, i.e. unique pairs of surface form and
its canonical segmentation. The size of such type
corpus is around 21,000 word forms. In the second
regime, we train the models using a parallel corpus
where each pair of surface form and its canonical
segmentation appears as many time as the corre-
sponding word appears in the corpus. The size of
the token-based corpus is then 100,000 tokens.

In the type regime, the amount of training ex-
amples is substantially smaller than in the token
regime. In order to make the comparison between
the token regime and the type regime more fair in
terms of the amount of training and testing data,
we add one more experiment. Specifically, we
train the cED model in the type regime using the
same number of iterations as in the token regime:
100,000 iterations. In this way, each word type is
seen multiple times both in the type and the to-
ken regime. The difference is that all the types
are equally frequent in the type regime, while we
observe their natural text frequency in the token

11One way to include language model into SMT would be
the n-best list reranking which is not exactly the same as our
synchronization approach that guides the decoding process.
Integrating our fusion approach for a higher-level language
model into MOSES is not trivial, and the systems are not
comparable without that.

190



regime.

5 Results and Discussion

The performance of our fused model cED+LM to-
gether with the two baseline models is reported
for English, German, Indonesian in Table 1.12 We
show the average results over five splits for each
language along with the standard deviation (in
brackets). Additionally, we present the results on
the words not seen in the training which make up
for 99% of the test sets in this corpora.

Our fused approach gives an improvement from
1% to 4% over the stronger cED baseline. The
bigger improvement for Indonesian could be at-
tributed to the regular patterns of orthographic
changes which appear on the segmentation bound-
aries. In the category of unseen data, the LM com-
ponent helps to correct errors for the words con-
sisting of new combinations of seen morphemes.
In case of Indonesian, around 80% of new words
(an average over five splits) belongs to this cate-
gory, while its share is only around 25% for Ger-
man and English. The overall lower performance
for English and German thus might be due to the
less regular patterns and more unseen roots in the
training data.

We observe that out of the two baseline models,
cED and cSMT, cED performs on average better
although their behavior is very similar with a dif-
ference of only 1% in accuracy in the case of Ger-
man and Indonesian.

For reference, we also show the results of the
joint model of Cotterell et al. (2016) and the state-
of-the-art neural reranker model of Kann et al.
(2016) which are available for these data sets. We
can see that our approach (cED+LM) gives an im-
provement of 1% for German and 2% for Indone-
sian over the state-of-the-art performance while
we do not employ extra information from external
dictionaries. Note that the languages for which we
improve the state of the art are morphologically
richer than English.

Table 2 shows the cED and cSMT model on
Chintang in two training regimes, word types and
word tokens. We also report the results for com-
parative setting of the type-based regime.

12We obtained significantly better results for cED model
than those reported in Kann et al. (2016). We speculate that
the difference might be due to the shuffling of the data be-
tween the training epochs and early stopping based on the
validation set performance.

We observe that the corpus-wide training based
on word tokens increases the overall performance
for both, the cED and cSMT models. The ob-
served improvement can be partially explained by
the fact that our evaluation is token based: we
count the same result as many times as it appears
in the test set. Nevertheless, the score is infor-
mative because it shows the coverage over the
whole corpus. Additionally, our comparable set-
ting shows that seeing a segmentation for a word
type multiple times is not what helps learning.
What is beneficial is knowing the actual distribu-
tion in the corpus.

It can be seen in Table 2 that cSMT outper-
forms cED in the token regime. One possible ex-
planation for this outcome is that the inclusion of
the word counts helps to learn the character align-
ments better. This explanation would be in line
with the results of Aharoni and Goldberg (2017),
who showed that using pretrained character-level
statistical alignments to guide the encoder-decoder
network in training time can help to improve over
the end-to-end soft attention approach for morpho-
logical inflection generation task, which also falls
into the category of a more general sequence trans-
duction task.

Regarding the different subsets of the test data,
the highest improvement on unseen words with
new morphemes is achieved by the cED model,
while the cSMT model generalizes better in the
category of unseen words that consist of new com-
bination of seen morphemes. Both models be-
have similarly on seen words with the cSMT being
slightly better. This leads to an overall best perfor-
mance of the cSMT model, since the category of
seen words has the largest weight among all the
word tokens in the test data.

6 Conclusion and Future Work

We presented a neural model based on charac-
ter level encoder-decoder framework for morpho-
logical canonical segmentation in a low-resource
setting. The model is fused with a language
model over morpheme segments and length con-
trol model. Our approach gives higher results than
the state-of-the-art approach to canonical segmen-
tation for languages with more morphology, In-
donesian and German, while using only the infor-
mation contained in the training corpus.

Future work may include a development of a
single canonical segmentation model where the

191



Error Rate (%)
Types Regime

cED+LM cED cSMT Joint* cED+RR*
Baseline Baseline

English Total 0.21 (.01) 0.22 (.01) 0.27 (.02) 0.27 (.02) 0.19 (.01)
New comb. 0.15 (.03) 0.24 (.01) - - -

New morph. 0.23 (.01) 0.20 (.01) - - -
German Total 0.19 (.00) 0.23 (.02) 0.24 (.02) 0.41 (.03) 0.20 (.01)

New comb. 0.11 (.02) 0.33 (.03) - - -
New morph. 0.21 (.01) 0.20 (.01) - - -

Indonesian Total 0.03 (.02) 0.07 (.01) 0.06 (.01) 0.10 (.01) 0.05 (.01)
New comb. 0.02 (.03) 0.06 (.01) - - -

New morph. 0.09 (.03) 0.09 (.03) - - -

Table 1: Performance on the task of canonical segmentation for English, German and Indonesian. Type-
based regime. cED+LM - character based encoder-decoder model fused with morpheme based language
model. Baseline models: cED - character based encoder-decoder model, cSMT - character based statis-
tical machine translation model. For reference only: Joint* - model of Cotterell et al. (2016), cED+RR*
- model of Kann et al. (2016), not directly comparable since based on external dictionary information)

Correct predictions (%)
No. of Types Regime Tokens Regime

cED cSMT cED cED cSMT
Baseline Compar. Baseline

Total 24,606 0.19 0.18 0.23 0.16 0.14
Seen words 19,920 0.13 0.12 0.18 0.08 0.07
New comb. 3,959 0.44 0.41 0.42 0.47 0.41

New morph. 727 0.53 0.57 0.60 0.48 0.56

Table 2: Performance on the task of canonical segmentation for Chintang. Type-based vs token-based
training regime. cED - character based encoder-decoder model, cSMT - character based statistical ma-
chine translation model. Comparative setting for cED: training in types regime for the same number of
iterations as in the individual setting of token regime.

optimization of model components is performed
using neural approaches.

Another idea relevant to explore in future work
is to consider the networks that are designed to be
strong at character copying which is the most com-
mon operation in string transduction tasks such as
morphological segmentation, morphological rein-
flection and normalization (Gu et al., 2016; See
et al., 2017; Makarov et al., 2017).

We also analyzed the effect of incorporating
corpus counts for the purpose of training statis-
tical and neural models for canonical segmenta-
tion. The results show that incorporating counts as
they are seen in the corpora is beneficial for such
task for both types of models. Our findings sug-
gest that training sets including real word counts
should be further developed for this and similar

tasks and would be beneficial for development of
future models.

References
Roee Aharoni and Yoav Goldberg. 2017. Morphologi-

cal inflection generation with hard monotonic atten-
tion. In ACL.

Cyril Allauzen and Michael Riley. 2012. A pushdown
transducer extension for the openfst library. In Im-
plementation and Application of Automata - 17th In-
ternational Conference, CIAA 2012, Porto, Portu-
gal, July 17-20, 2012. Proceedings. pages 66–77.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR
abs/1409.0473.

Christian Bentz, Dimitrios Alikaniotis, Tanja Samardi,

192



and Paula Buttery. 2017. Variation in word fre-
quency distributions: Definitions, measures and im-
plications for a corpus-based language typology.
Journal of Quantitative Linguistics pages 128–162.
https://doi.org/10.1080/09296174.2016.1265792.

Toms Bergmanis and Sharon Goldwater. 2017. From
segmentation to analyses: a probabilistic model for
unsupervised morphology induction. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics.
Valencia, Spain.

Balthasar Bickel, Goma Banjade, Martin Gaen-
szle, Elena Lieven, Netra Prasad Paudyal,
Ichchha Purna Rai, Manoj Rai, Novel Kishore
Rai, and Sabine Stoll. 2007. Free prefix or-
dering in chintang. Language 83(1):43–73.
https://muse.jhu.edu/article/214599.

Balthasar Bickel, Sabine Stoll, Martin Gaenszle,
Novel Kishor Rai, Elena Lieven, Goma Banjade,
Toya Nath Bhatta, Netra Paudyal, Judith Pettigrew,
Ichchha P. Rai, and Manoj Rai. 2004-2015. Audio-
visual corpus of the chintang language, including a
longitudinal corpus of language acquisition by six
children. http://www.mpi. nl/DOBES.

Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. In Proceedings of SSST@EMNLP 2014,
Eighth Workshop on Syntax, Semantics and Struc-
ture in Statistical Translation, Doha, Qatar, 25 Oc-
tober 2014. pages 103–111.

Ryan Cotterell, Thomas Müller, Alexander Fraser, and
Hinrich Schütze. 2015. Labeled morphological seg-
mentation with semi-markov models. In Proceed-
ings of the Nineteenth Conference on Computational
Natural Language Learning. Association for Com-
putational Linguistics, pages 164–174.

Ryan Cotterell, Tim Vieira, and Hinrich Schütze. 2016.
A joint model of orthography and morphological
segmentation. In NAACL HLT 2016, The 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, San Diego California, USA,
June 12-17, 2016. pages 664–669.

Mathias Creutz, Teemu Hirsimäki, Mikko Kurimo,
Antti Puurula, Janne Pylkkönen, Vesa Siivola, Matti
Varjokallio, Ebru Arisoy, Murat Saraçlar, and An-
dreas Stolcke. 2007. Morph-based speech recog-
nition and modeling of out-of-vocabulary words
across languages. ACM Trans. Speech Lang. Pro-
cess. 5(1):3:1–3:29.

Mathias Creutz and Krista Lagus. 2002. Unsupervised
discovery of morphemes. In Proceedings of the
ACL-02 Workshop on Morphological and Phonolog-
ical Learning. Association for Computational Lin-
guistics, pages 21–30.

Mathias Creutz and Krista Lagus. 2005. Inducing the
morphological lexicon of a natural language from
unannotated text. In Proc. International and In-
terdisciplinary Conference on Adaptive Knowledge
Representation and Reasoning (AKRR-05). Espoo,
Finland, pages 106–113.

Mathias Creutz and Krister Linden. 2004. Morpheme
segmentation gold standards for finnish and english.

Christopher Dyer, A Muresan, and Philip Resnik. 2008.
Generalizing word lattice translation. In In ACL-
HLT .

H. Eifring and R. Theil. 2005. Linguistics for Students
of Asian and African Languages. [available at
http://www.uio.no/studier/emner/
hf/ikos/EXFAC03-AAS/h05/larestoff/
linguistics/], Universitetet i Oslo.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor
O. K. Li. 2016. Incorporating copying mech-
anism in sequence-to-sequence learning. CoRR
http://arxiv.org/abs/1603.06393.

Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun
Cho, and Yoshua Bengio. 2017. On integrat-
ing a language model into neural machine
translation. Computer Speech and Language
https://doi.org/http://doi.org/10.1016/j.csl.2017.01.014.

Ilana Heintz. 2008. Arabic language modeling with fi-
nite state transducers. In ACL 2008, Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics, June 15-20, 2008, Columbus,
Ohio, USA, Student Research Workshop. pages 37–
42.

Katharina Kann, Ryan Cotterell, and Hinrich Schütze.
2016. Neural morphological analysis: Encoding-
decoding canonical segments. In Proceedings of the
2016 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2016, Austin, Texas,
USA, November 1-4, 2016. pages 961–967.

Oskar Kohonen, Sami Virpioja, and Krista Lagus.
2010. Semi-supervised learning of concatenative
morphology. In Proceedings of the 11th Meeting of
the ACL Special Interest Group on Computational
Morphology and Phonology. Association for Com-
putational Linguistics, pages 78–86.

Quoc V. Le, Navdeep Jaitly, and Geoffrey E. Hinton.
2015. A simple way to initialize recurrent networks
of rectified linear units. CoRR abs/1504.00941.

Peter Makarov, Tatyana Ruzsics, and Simon
Clematide. 2017. Align and copy: Uzh at sig-
morphon 2017 shared task for morphological
reinflection. In Proceedings of the CoNLL-
SIGMORPHON 2017 Shared Task: Universal
Morphological Reinflection. Vancouver, Canada.

Karthik Narasimhan, Regina Barzilay, and Tommi
Jaakkola. 2015. An unsupervised method for uncov-
ering morphological chains. Transactions of the As-
sociation for Computational Linguistics 3:157–167.

193



Karthik Narasimhan, Damianos Karakos, Richard
Schwartz, Stavros Tsakalidis, and Regina Barzilay.
2014. Morphological segmentation for keyword
spotting. EMNLP .

Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, 7-12 July 2003, Sapporo Con-
vention Center, Sapporo, Japan.. pages 160–167.

Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics. Association for
Computational Linguistics, pages 209–217.

Brian Roark, Richard Sproat, Cyril Allauzen, Michael
Riley, Jeffrey Sorensen, and Terry Tai. 2012. The
opengrm open-source finite-state grammar software
libraries. In Proceedings of the ACL 2012 Sys-
tem Demonstrations. Association for Computational
Linguistics, ACL ’12, pages 61–66.

Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja,
and Mikko Kurimo. 2013. Supervised morphologi-
cal segmentation in a low-resource learning setting
using conditional random fields. In Proceedings of
the Seventeenth Conference on Computational Nat-
ural Language Learning. Association for Computa-
tional Linguistics, pages 29–37.

Abigail See, Peter J Liu, and Christopher D Man-
ning. 2017. Get To The Point: Summarization with
Pointer-Generator Networks. In ACL.

Wolfgang Seeker and Özlem Çetinoglu. 2015. A
graph-based lattice dependency parser for joint
morphological segmentation and syntactic analysis.
TACL 3:359–373.

Felix Stahlberg, Eva Hasler, Aurelien Waite, and Bill
Byrne. 2016. Syntactically guided neural machine
translation. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2016, August 7-12, 2016, Berlin, Ger-
many, Volume 2: Short Papers.

Sabine Stoll, Jekaterina Mazara, and Balthasar Bickel.
In press. The acquisition of polysynthetic verb
forms in chintang. In Michael Fortescue, Marianne
Mithun, and Nicholas Evans, editors, Handbook of
Polysynthesis, Oxford University Press, Oxford.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems 27: Annual Conference on Neural In-
formation Processing Systems 2014, December 8-
13 2014, Montreal, Quebec, Canada. pages 3104–
3112.

Linlin Wang, Zhu Cao, Yu Xia, and Gerard de Melo.
2016. Morphological segmentation with window
LSTM neural networks. In Proceedings of the
Thirtieth AAAI Conference on Artificial Intelligence,
February 12-17, 2016, Phoenix, Arizona, USA..
pages 2842–2848.

194


