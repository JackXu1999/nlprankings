



















































Ruminating Reader: Reasoning with Gated Multi-hop Attention


Proceedings of the Workshop on Machine Reading for Question Answering, pages 1–11
Melbourne, Australia, July 19, 2018. c©2018 Association for Computational Linguistics

1

Ruminating Reader: Reasoning with Gated Multi-Hop Attention

Yichen Gong and Samuel R. Bowman
New York University

New York, NY
{yichen.gong,bowman}@nyu.edu

Abstract

To answer the question in machine com-
prehension (MC) task, the models need
to establish the interaction between the
question and the context. To tackle the
problem that the single-pass model can-
not reflect on and correct its answer, we
present Ruminating Reader. Ruminating
Reader adds a second pass of attention and
a novel information fusion component to
the Bi-Directional Attention Flow model
(BIDAF). We propose novel layer struc-
tures that construct a query aware con-
text vector representation and fuse encod-
ing representation with intermediate rep-
resentation on top of BIDAF model. We
show that a multi-hop attention mecha-
nism can be applied to a bi-directional
attention structure. In experiments on
SQuAD, we find that the Reader outper-
forms the BIDAF baseline by 2.1 F1 score
and 2.7 EM score. Our analysis shows that
different hops of the attention have differ-
ent responsibilities in selecting answers.

1 Introduction

The majority of recorded human knowledge is cir-
culated in unstructured natural language. It is
tremendously valuable to allow machines to read
and comprehend the text knowledge. Machine
comprehension (MC)—especially in the form of
question answering (QA)—is therefore attracting
a significant amount of attention from the ma-
chine learning community. Recently introduced
large-scale datasets like CNN/Daily Mail (Her-
mann et al., 2015), the Stanford Question Answer-
ing Dataset (SQuAD; Rajpurkar et al., 2016) and
the Microsoft MAchine Reading COmprehension
Dataset (MS-MARCO; Nguyen et al., 2016) have

(a) The high-level structure of BIDAF.

(b) The high-level structure of Ruminating Reader.

Figure 1: The high-level comparison between
BIDAF and Ruminating Reader.

allow data-driven methods, including deep learn-
ing, to become viable.

Recent approaches toward solving machine
comprehension tasks using neural networks can
be viewed as falling into two broad categories:
single-pass reasoners and multiple-pass reasoners.
Single-pass models read a question and a source
text once and often adopt the differentiable atten-
tion mechanism that emphasizes important parts
of the context related to the question.

BIDAF (Seo et al., 2017) represents one of the



2

competitive single-pass models in Machine Com-
prehension. BIDAF uses a bi-directional attention
matrix which calculates the correlations between
each word pair in context and query to build query-
aware context representation. However, BIDAF
and some similar models miss some questions be-
cause they don’t have the capacity to reflect on
problematic candidate answers and revise their de-
cisions.

When humans are reading a text with the goal of
answering a question, they tend to read it multiple
times to get a better understanding of the context
and question, and to give a better response. With
this intuition, recent multi-pass models revisit the
question and the context passage (or ruminate) to
infer the relations between the context, the ques-
tion and the answer.

We propose an extension of BIDAF, called Ru-
minating Reader, which uses a second pass of
reading and reasoning to allow it to learn to avoid
mistakes and to ensure that it is able to effectively
use the full context when selecting an answer. In
addition to adding a second pass, we also intro-
duce two novel layer types, the ruminate layers,
which use gating mechanisms to fuse the obtained
from the first and second passes. We observe a
surprising phenomenon that when an LSTM layer
in the context ruminate layer takes same input in
each timestep, it can produce useful representation
for the gates. In addition, we introduce an answer–
question similarity loss to penalize overlap be-
tween question and predicted answer, a common
feature in the errors of our base model. This allows
us to achieve an F1 score of 79.5 and Exact Match
(EM) score of 70.6 on hidden test set,1 an im-
provement of 2.2 F1 score and 2.9 EM on BIDAF.
Figure 1 shows a high-level comparison between
BIDAF and Ruminating Reader. Our analysis
shows that the first hop attention is responsible for
identifying key informative word while the second
hop is responsible for finding candidate answers.
The gates are effective in selecting useful infor-
mation from different representations.

This paper is organized as follows: In Section
2 we define the problem to be solved and intro-
duce the SQuAD task. In Section 3 we introduce
Ruminating Reader, focusing on the information-
extracting and information-digesting components
and how they integrate. Section 4 discusses related

1The latest results are listed at https://rajpurkar.
github.io/SQuAD-explorer/

Figure 2: The model structure of our Ruminating
Reader.

work. Section 5 presents the experimental setting,
results and analysis. Section 6 concludes.

2 Question Answering

The task of the Ruminate Reader is to answer
a question by reading and understanding a para-
graph of text and selecting a span of words within
the context. Formally, the Training and develop-
ment data consist of tuples (Q, P, A), where Q =
(q1, ..., qi, ...q|Q|) is the question, a sequence of
words with length |Q|, C = (c1, ...cj , ..., c|C|) is the
context, a sequence of words with length |C|, and
A = (ab, ae) is the answer span marking the begin-
ning and end indices of the answer in the context
(1 <= ab <= ae <= |C|).

SQuAD The SQuAD corpus is built using
536 articles randomly selected from English
Wikipedia. Images, figures, tables are stripped
and any paragraphs shorter than 500 characters
are discarded. Unlike other datasets that such
as CNN/Daily Mail whose questions are synthe-
sized, Rajpurkar et al. (2016) uses a crowdsourc-
ing platform to generate realistic question and an-
swer pairs. SQuAD contains 107,785 question-

https://rajpurkar.github.io/SQuAD-explorer/
https://rajpurkar.github.io/SQuAD-explorer/


3

answer pairs. The typical context length spans
from 50 tokens to 250 tokens. The typical length
of a question is around 10 tokens. The answer
be any span of words from the context, result-
ing in O(|C|2) possible outputs. While model
pefromance on SQuAD is nearing human perfor-
mance, and systems trained on the task are known
to be brittle when presented with unexpected con-
texts (Jia and Liang, 2017), the dataset nonetheless
remains the standard evaluation task for reading
comprehension.

3 Related Work

Recently, both QA and Cloze-style machine com-
prehension tasks like CNN/Daily Mail have seen
fast progress. Much of this recent work has been
based on end-to-end trained neural network mod-
els, and within that, most have used recurrent neu-
ral networks with soft attention (Bahdanau et al.,
2015), which emphasizes one part of the data over
the others. These models can be coarsely divided
into two categories: single-pass and multi-pass
reasoners.

Most papers on single-pass reasoning systems
propose novel ways to use the attention mech-
anism: Wang and Jiang (2016) propose match-
LSTM to model the interaction between context
and query, as well as introducing the use of a
pointer network (Vinyals et al., 2015) to extract
the answer span from the context. Xiong et al.
(2017b) propose the Dynamic Coattention Net-
work, which uses co-dependent representations of
the question and the context, and iteratively up-
dates the start and end indices to recover from lo-
cal maxima and to find the optimal answer span.
Wang et al. (2016) propose the Multi-Perspective
Context Matching model that matches the encoded
context with query by combining various match-
ing strategies, aggregates matching vector with bi-
directional LSTM, and predict start and end po-
sitions. Chen et al. (2016) propose to use a bi-
linear term to calculate the attentional alignment
between context and query.

Among multi-hop reasoning systems: Hill et al.
(2015) apply attention on window-based mem-
ory, by extending multi-hop end-to-end mem-
ory network (Sukhbaatar et al., 2015). Dhin-
gra et al. (2016) extend attention-sum reader to
multi-turn reasoning with an added gating mech-
anism. The Iterative Alternative (IA) reader (Sor-
doni et al., 2016) produces query glimpse and

document glimpse in each iteration and uses both
glimpses to update recurrent state in each iteration.
Shen et al. (2017) propose a multi-hop attention
model that used reinforcement learning to dynam-
ically determine when to stop digesting intermedi-
ate information and produce an answer. The r-net
(Wang et al., 2017) adopts self-attention mecha-
nism to allow the representation to select the im-
portant information in both context and query. Yu
et al. (2018) design a new non-recurrent neural
network topology, that achieves 13x faster in train-
ing and 4x to 9x faster in inference without losing
accuracy with respect to its recurrent counterpart.

4 Our Model

4.1 Ruminating Reader

In this section, we review the BIDAF model (Seo
et al., 2017) and introduce our extension, the Ru-
minating Reader.

Our additions to the base model are motivated
by the intuition that adding an additional pass
of reading will allow the model to better inte-
grate information from the question and answer
and to better weigh possible answers, and that by
interpolating the results of the second pass with
those of the first pass through gating, we can pre-
vent the additional complexity that we add to the
model from substantially increasing the difficulty
of training. The structure of our model is shown in
Figure 2 and explained in the following sections.

Character Embedding Layer Just as in the
base BIDAF model, the character embedding
layer maps each word to a high dimensional vector
using character features. It does so using a con-
volutional neural network with max pooling over
learned character vectors (Lee et al., 2017; Kim
et al., 2016). Thus we have a context character
representation M ∈ Rf×C and a query repre-
sentation N ∈ Rf×Q, where C is the sequence
length of the context, Q is the sequence length of
the query and f is the number of 1D convolutional
neural network filters.

Word Embedding Layer Again as in the base
model, the word embedding layer uses pretrained
word vectors (the 6B GloVe vectors of Pennington
et al., 2014) to map the word into a high dimen-
sional vector space. We do not update the word
embeddings during training. The character em-
bedding and the word embedding are concatenated
and passed into a two-layer highway network (Sri-



4

vastava et al., 2015) to obtain a d dimensional vec-
tor representation of each single word. Hence, we
have a context representation H ∈ Rd×C and a
query representation U ∈ Rd×Q.

Sequence Encoding Layers As in BIDAF, we
use two LSTM RNNs (Hochreiter and Schmidhu-
ber, 1997) with d-dimensional outputs to encode
the context and query representations in both di-
rections. Therefore, we obtain a context encoding
matrix C ∈ R2d×C , and a query encoding matrix
Q ∈ R2d×Q.

Attention Flow Layer As in BIDAF, the atten-
tion flow layer constructs a query-aware context
representation G from inputs C and Q. This layer
takes two steps. In the first step, an interaction ma-
trix I ∈ RC×Q is computed, which indicates the
affinities between each context word encoding and
each query word encoding. Icq indicates the cor-
relation between the c-th word in context and q-th
word in query. The interaction matrix is computed
by

Icq = w
>
(I)[Cc;Qq;Cc ◦Qq] (1)

where wI ∈ R6d is a trainable parameter, Cc is
c-th column of context encoding and Qq is q-th
column of query encoding, ◦ is elementwise mul-
tiplication, and [; ] is vector concatenation.

Context-to-query Attention As in BIDAF, the
context-to-query attention component generates,
for each context word, an attention-weighted sum
of query word encodings. Let Q̃ ∈ R2d×C rep-
resent the context-to-query attention matrix. For
column c in Q̃ is defined by Q̃c =

∑
(acqQq),

where a is the attention weight. a is computed by
ac = softmax(Ic) ∈ RQ.

Query-to-context Attention Query-to-context
attention indicates the most relevant context words
to query. The most relevant word vector repre-
sentation is an attention-weighted sum defined by
c̃ =

∑
bcCc where b, is an attention weight which

is calculated by b = softmax(maxcol(I)) ∈ RC .
c̃ is replicated C times across the column, there-
fore giving C̃ ∈ R2d×C .

We then obtain the final query-aware context
representation by

Gc = [Cc; Q̃c;Cc ◦ Q̃c;Cc ◦ C̃c] (2)

where Gc ∈ R8d×C .

Summarization Layer We propose summariza-
tion layer which produces a vector representa-
tion that summarizes the information in the query-
aware context representation. The input to sum-
marization layer is G. We use one bi-directional
LSTM network to model the learned information.
We select the final states from both directions and
concatenate them together as s = [sf ; sb] . where
s ∈ R2d represents the representation summarized
from the reading of context and query, sf is the fi-
nal state of LSTM in forward direction, and sb is
the final state of LSTM in backward direction.

Query Ruminate Layer The query ruminate
layer fuses the summarization vector representa-
tion with the query encoding Q, helping reformu-
late the query representation in order to maximize
the chance of retrieving the correct answer. The in-
put to this layer is s tiled Q times (SQ ∈ R2d×Q).
A gating function then fuses this with the existing
query encoding:

zi = tanh(W
1>
Qz SQi +W

2>
Qz Qi + bQz) (3)

fi = σ(W
1>
Qf SQi +W

2>
QfQi + bQf ) (4)

Q̃i = fi ◦Qi + (1− fi) ◦ zi (5)

where W 1Qz,W
2
Qz,W

1
Qf ,W

2
Qf ∈ R2d×2d and

bQz, bQf ∈ R2d are trainable parameters, SQiis
the i-th column of the SQ, Qi is the i-th column
of Q.

Context Ruminate Layer Context ruminate
layer digests the summarization and integrates it
with the context encoding C to facilitate answer
extraction. In this layer, we tile s C times and
we have SC ∈ R2d×C . To incorporate the posi-
tional information into this relatively long tiled se-
quence, we feed it into an additional bidirectional
LSTM with output size d in each direction. This
approach, while somewhat inefficient, proves to be
a valuable addition to the model and allows it to
better track position information, loosely follow-
ing the positional encoding strategy of Sukhbaatar
et al. (2015). Hence we obtain S̃C ∈ R2d×C ,
which is fused with context encoding C via a gate:

zi = tanh(W
1>
Cz S̃Ci +W

2>
Cz Ci + bCz) (6)

fi = σ(W
1>
Cf S̃Ci +W

2>
Cf Ci + bCf ) (7)



5

C̃i = fi ◦Ci + (1− fi) ◦ zi (8)

where W 1Cz,W
2
Cz,W

1
Cf ,W

2
Cf ∈ R2d×2d and

bCz, bCf ∈ R2d are trainable parameters, S̃Ciis
the i-th column of the S̃C , Ci is the i-th column
of C.

Second Hop Attention Flow Layer We take
Q̃ ∈ R2d×Q and C̃ ∈ R2d×C as the input to an-
other attention flow layer with the same structure
as described above, yielding G(2) ∈ R8d×C .

Modeling Layer We use two layers of bi-
directional LSTM with output size d in each direc-
tion to aggregate the information in G(2), yielding
a pre-output matrix M s ∈ R2d×C .

Output Layer As in BIDAF, our output layer
independently models the probability of each
word being selected as the start or end of an an-
swer span. We calculate the probability distribu-
tion of the start index of the answer span by

ps = softmax(w>p1 [G;M
s]) (9)

where w(p1) ∈ R10d is a trainable parameter.
We pass the matrix M s to another bi-directional
LSTM with output size d in single direction yield-
ing M e. We obtain the probability distribution of
the end index of the answer span by

pe = softmax(w>(p2)[G;M
e]) (10)

Training Loss We define the training loss as the
sum of three components: negative log likelihood
loss, L2 regularization loss, and a novel answer–
question similarity loss.

Answer–Question Similarity Loss We observe
that a version of our model trained only on the two
standard loss terms often selects answers that over-
lap substantially in content with their correspond-
ing questions, and that this nearly always results
in an error. A sample error of this kind is shown in
Table 1. This motivates an additional loss term at
training time: We penalize the similarity between
the question and the selected answer. Formally,
the answer question similarity loss is defined as

s = Argmax(p1) (11)

e = Argmax(p2) (12)

Context: The Broncos took an early lead in Su-
per Bowl 50 and never trailed. Newton was
limited by Denver’s defense, which sacked him
seven times and forced him into three turnovers,
including a fumble which they recovered for a
touchdown. Denver linebacker Von Miller was
named Super Bowl MVP, recording five solo
tackles, 2 sacks, and two forced fumbles.

Question: Which Newton turnover resulted in
seven points for Denver?

Ground Truth: {a fumble, a fumble, Fumble}

Prediction: three turnovers

Table 1: An error of the type that motivated the
answer–question similarity loss.

~qBoW =
Sumrow(Q)

Q
(13)

AQSL(θ) = cos(Cs, ~qBoW ) + cos(Ce, ~qBoW )
(14)

where s refers to the start index of answer span,
e refers to the end index of the answer span, ~qBoW
is the bag of words representation of query encod-
ing, cos(a, b) is the cosine similarity between a
and b, Cs and Ce are the s-th and e-th vector rep-
resentation of context encoding.

Prediction During prediction, we use a local
search strategy that for token indices a and a′, we
maximize psa × pea′ , where 0 ≤ a′ − a ≤ 15 .
Dynamic programming is applied during search,
resulting in O(C) time complexity.

5 Evaluation

5.1 Implementation details
Our model configuration closely follows that of
Seo et al. (2017): In the character encoding layer,
we use 100 filters of width 5. In the remainder
of the model, we set the hidden layer dimension
(d) to 100. We use pretrained 100D GloVe vec-
tors (6B-token version) as word embeddings. Out-
of-vocobulary tokens are represented by an UNK
symbol in the word embedding layer, but treated
normally by the character embedding layer. The
BiLSTMs in context and query encoding layers
share same weights. We use the AdaDelta opti-
mizer (Zeiler, 2012) for optimization.



6

Model Test
F1 EM

Match-LSTMa 73.743 64.744
Bidirectional Attention Flowb 77.323 67.974
Multi-perspective Matchingc 77.771 68.877
FastQAExtd 78.857 70.849
Document Readere 79.353 70.733
ReasoNetf 79.364 70.555
DCN+g 83.081 75.087
Reinforced Mnemonic Readerh 86.654 79.545
r-neti 88.170 81.391
QANetj 88.608 82.209
Human Performance k 91.221 82.304

Ruminating Reader 79.456 70.639

Table 2: Selected results on SQuAD leader-
board.
a Wang and Jiang (2016); b Seo et al. (2017);
c Wang et al. (2016); d Weissenborn et al. (2017);
e Chen et al. (2017); f Shen et al. (2017);
g Xiong et al. (2017a); h Hu et al. (2018);
i Wang et al. (2017); j Yu et al. (2018);
k Rajpurkar et al. (2016)

We selected hyperparameter values through ran-
dom search (Bergstra and Bengio, 2012). Batch
size is 30. Learning rate starts at 0.5, and
decreases to 0.2 once the model stops improv-
ing. The L2-regularization weight is 1e-4, AQSL
weight is 1 and dropout with a drop rate of 0.2 is
applied to all forward connections in the CNN, the
LSTMs, and all feedforward layers.

A typical model run converges in about 40k
steps. This takes two days using Tensorflow
(Abadi et al., 2015) and a single NVIDIA K80
GPU.

5.2 Evaluation Method

Rajpurkar et al. (2016) provide an official evalua-
tion script that allows us to measure F1 score and
EM score by comparing the prediction and ground
truth answers. Three answers are provided for
each question. The prediction is compared to each
of the answer and best score is selected. F1 score
is defined by recall and precision of words and EM
score, as Exact Match score, is defined as the score
of 100% accuracy in prediction. We do not use
any kind of ensembling, and compare our results
primarily with other single-model (non-ensemble)
results. The test set performance is evaluated us-
ing the CodaLab platform by a task administrator.

Ablation Variant Dev
F1 EM

1. BIDAF 77.3 67.7
2. BIDAF w/ L2 Reg., AQSL, LS 77.7 68.6
3. RR w/o query ruminate layer 78.7 69.6
4. RR w/o context ruminate layer 78.9 70.0
5. RR w/ BiLSTM in QRL 79.4 70.4
6. RR w/o BiLSTM in CRL 74.0 64.2
7. RR w/o query input at s,f in QRL 78.8 70.1
8. RR w/o context input at s,f in CRL 78.9 70.3
9. RR w/o query input in QRL 63.3 54.1
10. RR w/o context input in CRL 27.0 9.4
11. RR w/o summ. input in QRL 79.2 70.1
12. RR w/o summ. input in CRL 79.2 70.3

Ruminating Reader 79.5 70.6

Table 3: Layer ablation results. The order of
the listing corresponds to the description in Ap-
pendix A.1. CRL refers to context ruminate layer
and QRL refers to query ruminate layer.

5.3 Results

The Ruminating Reader achieves an F1 score of
79.5 and EM score of 70.6. The SQuAD leader-
board with selected result is displayed in Table 2.
The leaderboard is listed in descending order of F1
score, but if an entry’s F1 score is better than the
adjacent entry’s, while its EM score is worse, then
these two entries are considered tied.

5.4 Analysis

Layer Ablation Analysis To analyze how each
component contribute to the model, we run a layer
ablation experiment. We present results for twelve
versions of the model on the development set, each
missing some or all of the major components of
the full Ruminating Reader. The precise definition
of each of the twelve ablated models can be found
in Appendix A.1.

The results of the ablation experiment are
shown in Table 3. The ablation experiments show
how each component contribute to the model. Ex-
periments 3 and 4 show that the two ruminate lay-
ers are both important and helpful in contributing
performance. It is worth noting that the BiLSTM
in the context ruminate layer contributes substan-
tially to model performance. We find this some-
what surprising, since it takes the same input in
each timestep, but it nonetheless successfully di-
gests the summarization information representa-
tion and produces a useful input for the gating
component. Experiments 7 and 8 show that the
modeled summarization vector representation can
provide information to gates reasonably well. The



7

Figure 3: The visualization of first hop (top) and second hop (bottom) attention interaction matrix. We
use coolwarm colormap, where red is close to 1 and blue is close to 0. In the question “What is the name
of the trophy given to anyone who plays on the winning team in a super Bowl?”, the key words name,
trophy, given, who are strongly attended to in the first hop.

Figure 4: Visualizations of gate values for the
query (top) and context (bottom) ruminate layers.
The order of words is the same as in Figure 3. We
use coolwarm colormap, where red means the gate
uses more information from intermediate repre-
sentation, and blue from encoding representation.

drop in performance in both experiments 9 and 10
shows that the key information for new query and
context representation is the first stage query and
context encodings. Experiments 11 and 12 shows
that the summarization vector representation does
help the later stage of reasoning.

Visualization Figure 3 provides a visualization
of the first hop and second hop attention interac-

tion matrix I . We also provide a sample of visual-
ization for the L2 sum of gate value in context and
query ruminate layers in Figure 4.

From Figure 3 we see that though the struc-
tures of two hops of attention flow layer are the
same, they function quite differently in typical
cases. The first hop attention appears to be primar-
ily concerned with identifying the key informative
word (or words, as here) in the query. Though in
Figure 3 four key words are signified, one or two
words are attended to in the first hop in the com-
mon case. The second hop is then responsible for
finding candidate answers that are relevant to those
key words and generating a query-aware context
representation. We observe the first hop attention
shows a consistent attention pattern across context
words, suggesting that there may be room to make
the first hop component more efficient in future
work.

From Figure 4, we see the gate value on both
query ruminate layer and context ruminate layer
shows that the gates are working to fuse informa-
tion to original query encoding and context encod-



8

Figure 5: An analysis according to answer length
of questions and question types. The top graph
shows the comparison of F1 score between Rumi-
nating Reader and the BIDAF model on the devel-
opment set by answer length. The blue line shows
the distribution of questions by answer length. The
bottom graph shows a corresponding comparison
BIDAF by question type (wh-word).

ing. We observe that in most of the case the gates
in ruminate layers uses more information from en-
coding than from summarization representation.
The observation matches our expectation that the
gates modify and improve on the encoding repre-
sentation.

We also provide a comparison of F1 score be-
tween BIDAF and Ruminating Reader on ques-
tion with different ground truth answer length and
different types of questions in Figure 5. Exact
match score is highly correlated with F1 score so
we omit it for clarity. We observe that the Rumi-
nating Reader outperforms BIDAF on most of the
questions with respect to different answer length.
On the question with long answer length, of 5, 8
and 9, Ruminating Reader outperforms BIDAF by
a great margin. Questions with longer reference
answers appear to be more difficult to answer. In
addition, the Ruminating Reader does better on
each type of question. Both models work best

for when questions—these question are answer-
able by temporal expressions, which are relatively
easy to recognize. The Why questions are hardest
to answer—they tend to have long answers with
no purely lexical cues marking their beginnings
or ends. Ruminating Reader outperforms BIDAF
model on why questions by a substantial margin.

Performance Breakdown Following Zhang
et al. (2017), we break down Ruminating Reader’s
79.5% F1 score on the development set into three
sub-scores, representing failures, partial suc-
cesses, and successes. On 13.5% of development
set examples, Ruminate Reader fails, yielding 0%
F1. On 70.6% of examples, Ruminate Reader
achieves a perfect F1 score. On the remaining
15.9%, Ruminate Reader got only partial matches
(i.e., answers that partially overlapped with
reference answers), with an average F1 score
of 56.0%. Comparing to the jNet (Zhang et al.,
2017) whose success answers occupy 69.1% of
all answers, failure score answers 14.9% and
partial success 16.01% with an average F1 score
of 58.0%, our model works better on increasing
successes and reducing failures.

6 Conclusion

We propose the Ruminating Reader, an exten-
sion to the BIDAF model with two-hop atten-
tion. The model surpasses the original BIDAF
model’s performance on Stanford Question An-
swering Dataset (SQuAD) by a large margin.
These results and our qualitative analysis both
suggest that the model successfully fuses the in-
formation from two passes of reading using gat-
ing and uses the result to identify appropriate an-
swers to Wikipedia questions. An ablation experi-
ment shows that each of components of this com-
plex model contribute substantially. The analysis
shows that the first hop attention is responsible for
identifying key informative word while the second
hop is responsible for finding candidate answers.
The gate is empirically proved to be effective in se-
lecting information from different representations.
In future work, we aim to find ways to simplify
this model without impacting performance, to ex-
plore the possibility of yet deeper models, and to
expand our study to machine comprehension tasks
more broadly.



9

Acknowledgments

We thank Pranav Rajpurkar for testing Ruminat-
ing Reader on SQuAD hidden test set. This
project has benefited from financial support to SB
by Google, Tencent Holdings, and Samsung Re-
search.

References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene

Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Mané, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Vasude-
van, Fernanda Viégas, Oriol Vinyals, Pete Warden,
Martin Wattenberg, Martin Wicke, Yuan Yu, and Xi-
aoqiang Zheng. 2015. TensorFlow: Large-scale ma-
chine learning on heterogeneous systems.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural Machine Translation by Jointly
Learning to Align and Translate. In Proc. ICLR.

James Bergstra and Yoshua Bengio. 2012. Random
Search for Hyper-Parameter Optimization. In Proc.
JMLR.

Danqi Chen, Jason Bolton, and Christopher D Man-
ning. 2016. A Thorough Examination of the
CNN/Daily Mail Reading Comprehension Task. In
Proc. ACL.

Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading Wikipedia to Answer Open-
Domain Questions. In Proc. ACL.

Bhuwan Dhingra, Hanxiao Liu, William W Cohen, and
Ruslan Salakhutdinov. 2016. Gated-Attention Read-
ers for Text Comprehension. In CoRR.

Karl Moritz Hermann, Tomás Kociský, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching Ma-
chines to Read and Comprehend. In Proc. NIPS.

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. 2015. The Goldilocks Principle: Reading
Children’s Books with Explicit Memory Represen-
tations. In Proc. ICLR.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
Short-Term Memory. volume 9, pages 1735–1780.

Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,
Furu Wei, and Ming Zhou. 2018. Reinforced
Mnemonic Reader for Machine Reading Compre-
hension. In Proc. IJCAI.

Robin Jia and Percy Liang. 2017. Adversarial Ex-
amples for Evaluating Reading Comprehension Sys-
tems. In Proc. EMNLP.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016. Character-Aware Neural Lan-
guage Models. In Proc. AAAI.

Jason Lee, Kyunghyun Cho, and Thomas Hofmann.
2017. Fully Character-Level Neural Machine Trans-
lation without Explicit Segmentation. In Proc. ACL.

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. MS MARCO: A Human Generated MA-
chine Reading COmprehension Dataset. In Proc.
CoCo@NIPS.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global Vectors for Word
Representation. In Proc. EMNLP.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD - 100, 000+ Ques-
tions for Machine Comprehension of Text. In Proc.
EMNLP.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional Attention
Flow for Machine Comprehension. In Proc. ICLR.

Yelong Shen, Po-Sen Huang, Jianfeng Gao, and
Weizhu Chen. 2017. ReasoNet: Learning to Stop
Reading in Machine Comprehension. In Proc.
NIPS.

Alessandro Sordoni, Phillip Bachman, and Yoshua
Bengio. 2016. Iterative Alternating Neural Atten-
tion for Machine Reading. In CoRR.

Rupesh Kumar Srivastava, Klaus Greff, and Jürgen
Schmidhuber. 2015. Highway Networks. In Proc.
ICML.

Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
and Rob Fergus. 2015. End-To-End Memory Net-
works. In Proc. NIPS.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer Networks. In Proc. NIPS.

Shuohang Wang and Jing Jiang. 2016. Machine
Comprehension Using Match-LSTM and Answer
Pointer. ArXiv:1608.07905.

Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang,
and Ming Zhou 0001. 2017. Gated Self-Matching
Networks for Reading Comprehension and Question
Answering. In Proc. ACL.

Zhiguo Wang, Haitao Mi, Wael Hamza, and Radu Flo-
rian. 2016. Multi-Perspective Context Matching for
Machine Comprehension. ArXiv:1612.04211.

Dirk Weissenborn, Georg Wiese, and Laura Seiffe.
2017. Making Neural QA as Simple as Possible but
not Simpler. ArXiv:1703.04816.



10

Caiming Xiong, Victor Zhong, and Richard Socher.
2017a. DCN+: Mixed Objective and Deep Resid-
ual Coattention for Question Answering. In Proc.
ICLR.

Caiming Xiong, Victor Zhong, and Richard Socher.
2017b. Dynamic Coattention Networks For Ques-
tion Answering. In Proc. ICLR.

Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui
Zhao, Kai Chen 0010, Mohammad Norouzi 0002,
and Quoc V Le. 2018. QANet - Combining Local
Convolution with Global Self-Attention for Reading
Comprehension. In Proc. ICLR.

Matthew D Zeiler. 2012. ADADELTA: An Adaptive
Learning Rate Method. ArXiv:1212.5701.

Junbei Zhang, Xiaodan Zhu, Qian Chen, Lirong Dai,
and Hui Jiang. 2017. Exploring Question Under-
standing and Adaptation in Neural-Network-Based
Question Answering. ArXiv:1703.04617.

A Appendix

A.1 Layer Ablation Experiments setup
In this section we show the setup of layer ablation
experiment details in Table 3.

1. Vanilla BIDAF

2. Ruminating Reader without context and
query ruminate layer. Therefore, the model
is equivalent to original BIDAF model with
L2-regurization, answer-question similarity
penalization and local search prediction fea-
ture.

3. Ruminating Reader without query ruminate
layer. The query encoding Q is directly fed
into the second hop attention flow layer.

4. Ruminating Reader without context ruminate
layer. The context encoding C is directly
connected to the second hop attention flow
layer without digesting newly acquired infor-
mation.

5. Ruminating Reader with BiLSTM modeling
in query ruminate layer. Formally, we have
S̃Q = BiLSTM(SQ) in query ruminate
layer. Therefore, the query ruminate layer is
defined by

zi = tanh(W
1>
Qz S̃Qi +W

2>
Qz Qi + bQz)

(15)

fi = σ(W
1>
Qf S̃Qi +W

2>
QfQi + bQf ) (16)

Q̃i = fi ◦Qi + (1− fi) ◦ zi (17)

6. Ruminating Reader without BiLSTM model-
ing in context ruminate layer. Formally, we
have S̃C = SC in query ruminate layer and
all other components remains the same.

7. Ruminating Reader without query input at z,
f in query ruminate layer. While all other
components remain the same as in Ruminat-
ing Reader, the gate in query ruminate layer
is defined by

zi = tanh(W
1>
Qz SQi + bQz) (18)

fi = σ(W
1>
Qf SQi + bQf ) (19)

Q̃i = fi ◦Qi + (1− fi) ◦ zi (20)

8. Ruminating Reader without context input at
z, f in context ruminate layer. While all
other components remain the same as in Ru-
minating Reader, the gate in context ruminate
layer is defined by

zi = tanh(W
1>
Cz S̃Ci + bCz) (21)

fi = σ(W
1>
f S̃Ci + bCf ) (22)

C̃i = fi ◦Ci + (1− fi) ◦ zi (23)

9. Ruminating Reader without query input in
query ruminate layer. In this version, we dis-
card query encoding input Q in the gate of
query ruminate layer. Formally, the gate in
Query Ruminate layer is

zi = tanh(W
1>
Qz SQi + bQz) (24)

fi = σ(W
1>
Qf SQi + bQf ) (25)

Q̃i = (1− fi) ◦ zi (26)

10. Ruminating Reader without context encoding
input in context ruminate layer. We ablate
the context encoding input C in the gate of
context ruminate layer. Therefore, the gate in
context ruminate layer is

zi = tanh(W
1>
Cz S̃Ci + bCz) (27)



11

fi = σ(W
1>
f S̃Ci + bCf ) (28)

C̃i = (1− fi) ◦ zi (29)

11. Ruminating Reader without summarization
information input in query ruminate layer.
In case that the summarization do not help
the encoding, while on the other hand, the
gate contributes to the learning, we design
the experiment that allows to eliminate the
influence of summarization. We discard the
summarization input in query ruminate layer.
Formally, the gate in query ruminate layer is
defined as

zi = tanh(W
2>
Qz Qi + bQz) (30)

fi = σ(W
2>
QfQi + bQf ) (31)

Q̃i = fi ◦Qi + (1− fi) ◦ zi (32)

12. Ruminating Reader without summarization
information input in context ruminate layer.
The summarization information is not in-
cluded in zi,fi

zi = tanh(W
2>
Cz Ci + bCz) (33)

fi = σ(W
2>
Cf Ci + bCf ) (34)

C̃i = fi ◦Ci + (1− fi) ◦ zi (35)


