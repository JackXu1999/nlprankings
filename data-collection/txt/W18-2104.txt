






































Maxim Khalilov, Ph.D.

Machine translation that makes sense: 
the Booking.com use case

Technical presentation March 6, 2018
Cambridge, UK

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 86



Booking.com.
The world’s #1 website for booking hotels and other accommodations.

• Founded in 1996 in Amsterdam
• Part of the Priceline Group (NASDAQ:

PCLN) since 2005
• 1,500,000+ properties in more than 220

countries and territories representing over
27M rooms

• Over 1,550,000 room nights every 24 hours
• Number of unique destinations worldwide:

120,000+
• Total number of guest reviews:

173,000,000+
• 43 languages
• 198 offices worldwide
• More than 15,500 employees



Use case of MT 
at Booking.com



Mission: Empower people to experience the world 
without any language barrier.

of daily bookings on Booking.com is made in a 
language other than English

… thus it is important to have locally relevant content at scale

How Locally Relevant?

Allow partners and guests to 
consume and produce content 
in their own language

▸ Hotel Descriptions
▸ Customer Reviews
▸ Customer Service

Support

Why At Scale?
● One Million+ properties and

growing very fast
● Frequent change requests to

update the content
● 43 languages and more
● New user-generated

customer reviews / tickets
every second



Limited 
domain One product

Language 
expertise

In-house 
evaluators for 
43 languages

Why MT?

Lots of in-
domain 

data

Av. 10M 
parallel sent. 

for big 
languages



Use Case #1: Hotel descriptions – currently translated by 
human in 43 languages based on visitor demand.

Hotel in 
Japan

Sees English 
description

Lost Business

Machine 
Translation

German 
Visitor

Drops
Off

Human
Translation

Pipeline



Use Case #2: Customer Reviews – currently not 
translated; available only if user leaves a review in that 
language.

Hotel in 
Japan

No German 
Reviews

Lost Business

Machine 
Translation

German 
Visitor

Drops
Off



Use Case #3: Partner support – Partner-facing 
localization and customer/partner support.



Use Case #4: Translation support – make 
translation cheaper by providing high-quality 
productivity tools.



And there is even more..
Messages. Room 

descriptions.

Attractions .



Why not general 
purpose MT engines?



3 
Reasons



1. Quality



Customized MT can do much better for our own 
content.

General-purpose

Customized MT 1

Customized MT 2

Customized MT 3

MT 
Quality

Domain



Hotel Description: Evaluation Results
English        German

General-
Booking

Human

General-purpose



Customer Review: Evaluation Results
English        German

General purpose



1. Quality
2. Risk



Can machine 
translation be

dangerous?

Look at me! 
I’m so 

innocent!



Yes!
The imperfection of MT might 
mislead users, have legal 
consequences for the company or 
damage brand's reputation and 
customer’s confidence of translated 
content.



Examples of business sensitive errors

Offering a restaurant with WiFi, Hodor 
Ecolodge is located in Winterfell. On-

site parking is free.

Die Hodor Ecolodge in Winterfell bietet

ein Restaurant mit WLAN. Parkplatz
vor Ort ist verfügbar.

The hotel offers 24-hour concierge 

service and free-use bicycles. Pets 
can be accommodated with 
advance reservation.

Der Conciergeservice steht rund um die
Uhr zu Ihrer Verfügung und die
Leihfahrräder nutzen Sie kostenfrei.



1. Quality
2. Risk
3. Cost





But why neural?



Adequacy / Fluency Scores for EN->DE 
hotel description translations

Our In-domain NMT system 
outperforms all other MT 
engines

Both Neural systems still 
consistently outperform 
their statistical counterparts

General Purpose NMT beats 
In-domain SMT

Particularly fluency score of 
our NMT engine is close to 
human level



The

Data



Hotel descriptions translated by human in 43 
languages resulting in lots of in-domain data for 
MT

* Approximate numbers based 
on average of some languages

50%
Translation 
Coverage

90%
Demand

Coverage

10M
Average 

Corpus Size



Monolingual reviews never translated in 43 
languages resulting in lots of out-of-domain data 
potentially useful for MT

173M
Total reviews

17
Languages 

>1M reviews

37%
Properties 

w/o reviews



Few specific challenges 
and proposed solutions



Our NMT Model Configuration Details



• Named entities
• Rare words

Our challenges

Real-world content

Customer facing output
• Human loop
• BLEU & human evaluation correlation
• Business sensitive issues

Lack of parallel training 
data

• Use and sources of data
• Domain adaptation



• Named entities
• Rare words

Our challenges

Real-world content

Customer facing output
• Human loop
• BLEU & human evaluation correlation
• Business sensitive issues

Lack of parallel training 
data

• Use and sources of data
• Domain adaptation



End-to-end approach insufficient to handle Named 
Entities, pre-processing improves performance

Raw source Winterfell Railway Station can be reached in a 55-minute car ride.

Pure NMT Translation Den Bahnhof Winterfell erreichen Sie nach einer 5-mintigen Autofahrt.

NMT with distance 
placeholders Den Bahnhof Winterfell erreichen Sie nach einer 55-mintigen Autofahrt.

Hotel Shirakawa is just a 5-minute walk from Fushimi Subway Station. Nagoya 
Castle is a 10-minute drive, and the Sakae shopping area is 500 m away. 

Landmark nameHotel name Time

DistanceTime Landmark name

Landmark name

Search for 
named entities 
in source

Substitute as 
per target 
format

Replace with 
placeholders Translate

- Regular expression for distance, date, time
- Hybrid dictionary, conditional random field NER for names

Problem

Approach

Results



BLEU 50K-Vocab 
baseline

Joint BPE Separate BPE

30K 50K 70K 90K 30K 50K 70K 90K

Epoch 5 39.54 43.75 43.46 43.40 41.23 42.81 42.35 39.73

N/A
Epoch 10 40.95 44.55 44.52 43.81 43.81 43.39 43.48 43.51

Epoch 15 42.01 45.08 45.91 46.14 45.75 43.58 43.23 45.17

Epoch 20 42.15 46.31 46.43 46.61 45.62 45.22 46.00 45.90

Better handling of rare words and 4 points BLEU 
score improvement with Byte Pair Encoding (BPE)

Raw source Offering a restaurant with WiFi, Hodor Ecolodge is located in Winterfell.

Tokenized source

Tokenized output

De-tokenized output Die Hodor Ecolodge in Winterfell bietet ein Restaurant mit WLAN.



Translation of informal language of customer 
reviews and partner-(company)-user comms

Correct typos 
which are easy 
to fix

IterateAdapt to the UGC domain Translate

Examples

Approach

Results

- The stuff
- The night guy aund the girl in the morning who looks like 
manage the hotel
- They keep your luggage for free if you for some days to Sapa
- And as well the offered us a breakfast in the morning asap
- Thans for the detail

Adequacy score Positive reviews Negative reviews

Baseline 80 % 27 %

+typos correction+DA 95 % 96 %



• Named entities
• Rare words

Our challenges

Real-world content

Customer facing output
• Human loop
• BLEU & human evaluation correlation
• Business sensitive issues

Lack of parallel training 
data

• Use and sources of data
• Domain adaptation



How can we control (M)T 
quality in eCommerce
environment?



Integrated approach to MT evaluation.

Entity analysis
BLEU

Adequacy/Fluency 
scoring

Business Sensitivity 
Analysis

A/B testing

Applicable to make sure 
there are no new bugs 
introduced as the result 

of the MT engine 
retraining and some 

experiments.

Scoring the quality of 
entity handling. 

Links MT quality with 
potential threats for the 

business

Rough assessment of the 
MT-ed content in terms of 

its publishability

Two-sample hypothesis 
testing where business 

metrics are to be 
optimized



Improvement with more data is better seen from 
human evaluation... 

...which doesn’t seem to be completely aligned 
with BLEU



Business Sensitivity Framework to detect if aspects and 
sub-aspects match between source & translated 
content

Sensitive Aspect 
Detection
Word2Vec

Sub-Aspect 
Classifier

TFIDF Based

Source

Sub-Aspect 
Classifier

TFIDF Based

TargetDoes the hotel description 
talk about parking?

Is parking free / not free / 
available as per source?

Is parking free / not free / 
available as per target?

Match 
Evaluation

Input 
Brochure

Error 
or Not



Business Sensitivity Framework: results

Correction 
mouleFREE/NOT FREE 

PARKING translation

source

free parking not free parking not about 
parking

free parking 99.4% 0.5% 0.1%

not free 
parking

5.1% 94.6% 0.3%

not about 
parking

<0.1% <0.1% 99.9%



• Named entities
• Rare words

Our challenges

Real-world content

Customer facing output
• Human loop
• BLEU & human evaluation correlation
• Business sensitive issues

Lack of parallel training 
data

• Use and sources of data
• Domain adaptation



- A few thousand of in domain 
sentences.

-In addition to the hotel descriptions data, 
available external open data is used 
including data from:

-Synthetic Data

-Gradual downsampling (Wees et al., 
2017)

Method. -Movie subtitles
-Wikipedia
-TED talks
-New commentary
-EuroParl



Data generation for customer reviews based on mono -
lingual  /  non-parallel bilingual data

External 
Corpus

Synthetic 
Data

In-domain 
Data

Use in-domain language model 
to select most relevant 
sentences from external corpus

Data Idea Methodology

Bilingual Cross Entropy Difference (Axelrod et al) 
- To select sentences that are most similar to in-
domain but different to out-of-domain. 

Use large amount of mono-
lingual data to create some 
synthetic in-domain data

Rico Sennrich et al. – Back translate target 
language in-domain data into source by reversing 
our MT model. 

Create a small amount of in-
domain corpus as well, to test 
for additional impact

Human Translation



Domain Adaptation using gradual downsampling to 
most relevant data selected by in-domain language 
model

Out-of-
domain 

Data

Synthetic 
Data

In-
domain 

External
Data 80%

80%
80%

100%

100%

Data Epoch 1 & 2
Epoch 
3 & 4

Epoch 
5 & 6

Epoch 
7 & 8

Least
Relevant

Most
Relevant



Gradual downsampling vs fine tuning

Faster iteration

Gradual downsampling Fine tuning

Takes time to get the General Model trained

Trained for specific use case from the 
beginning

Can be adapted to multiple use cases

Less accurate More accurate

Applicable without In-domain parallel data Needs In-domain parallel data 

No answer yet



Human Evaluation Results for Domain Adapted Model to 
translate customer reviews (gradual downsampling)

Adequacy Score for Positive Reviews Adequacy Score for Negative Reviews



Want to know
more?
Machine Translation at Booking.com: 
Journey and Lessons Learned
EAMT (User Track)
Prague, May 2017
Best Paper Award

Toward a full-scale neural machine 
translation in production: the 
Booking.com use case
MT Summit XVI (Commercial Track)
Nagoya, Sep 2017

https://arxiv.org/abs/1707.07911
https://arxiv.org/abs/1707.07911
https://arxiv.org/abs/1709.05820
https://arxiv.org/abs/1709.05820
https://arxiv.org/abs/1709.05820


TechnologyAutomatic post-editing and 
Quality Estimation



What is the business rationale?

• The Whys:

▸ Reduce monetary and legal risks
▸ Increase user trust
▸ Increase traction with partners and customers (B2B and B2C)
▸ As a part of the better integrated MT system, improve user 

experience



Complete MT-QE-APE architecture

Content
generation

Manual
Automatic

Machine 
translation QE

Scorer

Publishing
Good enough

None
Manual

Automatic

Post-editing

Not good 
enough

No change Exclude

Changes 
introduced

Sample-based

Scorer
Iterative improvements

Development stage



How can we validate?

Content
generation

Manual
Automatic

Machine 
translation Sample-based

BSF
# of OOV

NE analysis

Scorer
Publishing

Good 
enough

Not good 
enough

Exclude

APE

Machine translation

Sample-based

Scorer Iterative 
improvements

Development stage



How can we design an APE system, which would address 
the most important problems?

Machine 
translation

Negative=raw MT
Positive=PE

Sent Level APERaw MT 
output and 
post-edited 

data

Machine translation

MT 
input

MT 
output

Sentence level APE

Credit: MT research group at the University of Edinburgh



Negative and Positive training examples

Offering a restaurant with WiFi, Hodor Ecolodge is 
located in Winterfell. On-site parking is free.

Die Hodor Ecolodge in Winterfell bietet ein
Restaurant mit WLAN. Parkplatz vor Ort ist
verfügbar.

Die Hodor Ecolodge in Winterfell bietet ein
Restaurant mit WLAN. Parkplatz vor Ort ist
kostenlos.

Source

Raw MT

Post-edited MT

Negative 
example

Positive example



How can we design an APE system, which would 
address the most important problems?

Machine translation

Negative=contrastive
Positive=raw MT

Word Level APE
Contrastive 
references

Machine translation

MT 
input

MT 
output

Word level

Credit: MT research group at the University of Edinburgh



Contrastive references

On-site parking is free.

Parkplatz vor Ort ist verfügbar.

Parkplatz vor Ort ist nicht verfügbar
or

Parkplatz vor Ort ist kostenlos.

Source

Translation

Contrastive 

Positive example

Negative 
example



Future Directions (applied research and technology)

Explore alternative NMT technologies
- “Transformer” by (Vaswani et al., 2017)

Ensure high quality of translations
- Named Entities 
- NMT with reconstruction (Tu et al., 2017)
- Optimization for UGC
- Conditioning MT output on structured data

Reinforcement learning (Nguyen et al., 2017)



TAUS
MT Survey 

2018
http://info.taus.net/tau
s-mt-survey-2018

Deadline: Friday, April 14th



Thank You
Questions? Maxim Khalilov

maxim.khalilov@booking.com
www.linkedin.com/nl/maximkhalilov

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018  |  Page 143


	AMTA_2018_Workshop_Proceedings_QEAPE
	Wks3_Front_Material

	AMTA_2018_Workshop_Proceedings_QEAPE_3
	405_update
	JoaoGraca_qeape2018_footer
	MaximKhalilov_qeape2018_footer
	MarcinJunczys-Dowmunt_qeape2018_enlarge_footer
	MarcelloFederico_qeape2018_footer
	406_footer
	403_footer



