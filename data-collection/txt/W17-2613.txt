



















































Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context


Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 101–110,
Vancouver, Canada, August 3, 2017. c©2017 Association for Computational Linguistics

Beyond Bilingual: Multi-sense Word Embeddings using Multilingual
Context

Shyam Upadhyay1 Kai-Wei Chang2 Matt Taddy3 Adam Kalai3 James Zou4
1University of Illinois at Urbana-Champaign, Urbana, IL, USA

2University of Virginia, Charlottesville, VA, USA
3Microsoft Research, Cambridge, MA, USA

4Stanford University, Stanford, CA, USA
upadhya3@illinois.edu, kw@kwchang.net

{taddy,adum}@microsoft.com, jamesyzou@gmail.com

Abstract

Word embeddings, which represent a word
as a point in a vector space, have become
ubiquitous to several NLP tasks. A re-
cent line of work uses bilingual (two lan-
guages) corpora to learn a different vec-
tor for each sense of a word, by ex-
ploiting crosslingual signals to aid sense
identification. We present a multi-view
Bayesian non-parametric algorithm which
improves multi-sense word embeddings
by (a) using multilingual (i.e., more than
two languages) corpora to significantly
improve sense embeddings beyond what
one achieves with bilingual information,
and (b) uses a principled approach to learn
a variable number of senses per word,
in a data-driven manner. Ours is the
first approach with the ability to lever-
age multilingual corpora efficiently for
multi-sense representation learning. Ex-
periments show that multilingual training
significantly improves performance over
monolingual and bilingual training, by al-
lowing us to combine different parallel
corpora to leverage multilingual context.
Multilingual training yields comparable
performance to a state of the art mono-
lingual model trained on five times more
training data.

1 Introduction

Word embeddings (Turian et al., 2010; Mikolov
et al., 2013, inter alia) represent a word as a point
in a vector space. This space is able to capture se-
mantic relationships: vectors of words with sim-
ilar meanings have high cosine similarity (Tur-
ney, 2006; Turian et al., 2010). Use of embed-
dings as features has been shown to benefit sev-

eral NLP tasks and serve as good initializations for
deep architectures ranging from dependency pars-
ing (Bansal et al., 2014) to named entity recogni-
tion (Guo et al., 2014b).

Although these representations are now ubiqui-
tous in NLP, most algorithms for learning word-
embeddings do not allow a word to have differ-
ent meanings in different contexts, a phenomenon
known as polysemy. For example, the word
bank assumes different meanings in financial (eg.
“bank pays interest”) and geographical contexts
(eg. “river bank”) and which cannot be repre-
sented adequately with a single embedding vector.
Unfortunately, there are no large sense-tagged cor-
pora available and such polysemy must be inferred
from the data during the embedding process.

My	interest	lies	in	
History.	

[ ]
	

Mon	[intérêt]	
réside	dans	
l'Histoire.	

Je	suis	un	grand	
[intérêt]	sur	mes	
économies	de	la	
banque.	

I	got	high	interest	on	my	
savings	from	the	bank.	 [ ] 	

Figure 1: Benefit of Multilingual Information (beyond
bilingual): Two different senses of the word “interest” and
their translations to French and Chinese (word translation
shown in [bold]). While the surface form of both senses are
same in French, they are different in Chinese.

Several attempts (Reisinger and Mooney, 2010;
Neelakantan et al., 2014; Li and Jurafsky, 2015)
have been made to infer multi-sense word rep-
resentations by modeling the sense as a latent
variable in a Bayesian non-parametric framework.
These approaches rely on the ”one-sense per col-
location” heuristic (Yarowsky, 1995), which as-
sumes that presence of nearby words correlate
with the sense of the word of interest. This heuris-
tic provides only a weak signal for sense identifi-
cation, and such algorithms require large amount
of training data to achieve competitive perfor-

101



mance.
Recently, several approaches (Guo et al., 2014a;

Šuster et al., 2016) propose to learn multi-sense
embeddings by exploiting the fact that different
senses of the same word may be translated into
different words in a foreign language (Dagan and
Itai, 1994; Resnik and Yarowsky, 1999; Diab and
Resnik, 2002; Ng et al., 2003). For example, bank
in English may be translated to banc or banque in
French, depending on whether the sense is finan-
cial or geographical. Such bilingual distributional
information allows the model to identify which
sense of a word is being used during training.

However, bilingual distributional signals often
do not suffice. It is common that polysemy for a
word survives translation. Fig. 1 shows an illus-
trative example – both senses of interest get trans-
lated to intérêt in French. However, this becomes
much less likely as the number of languages under
consideration grows. By looking at Chinese trans-
lation in Fig. 1, we can observe that the senses
translate to different surface forms. Note that
the opposite can also happen (i.e. same surface
forms in Chinese, but different in French). Exist-
ing crosslingual approaches are inherently bilin-
gual and cannot naturally extend to include addi-
tional languages due to several limitations (details
in Section 4). Furthermore, works like (Šuster
et al., 2016) sets a fixed number of senses for each
word, leading to inefficient use of parameters, and
unnecessary model complexity.1

This paper addresses these limitations by
proposing a multi-view Bayesian non-parametric
word representation learning algorithm which
leverages multilingual distributional information.
Our representation learning framework is the first
multilingual (not bilingual) approach, allowing us
to utilize arbitrarily many languages to disam-
biguate words in English. To move to multilin-
gual system, it is necessary to ensure that the
embeddings of each foreign language are relat-
able to each other (i.e., they live in the same
space). We solve this by proposing an algorithm
in which word representations are learned jointly
across languages, using English as a bridge. While
large parallel corpora between two languages are
scarce, using our approach we can concatenate
multiple parallel corpora to obtain a large multi-
lingual corpus. The parameters are estimated in

1Most words in conventional English are monosemous,
i.e. single sense (eg. the word monosemous)

a Bayesian nonparametric framework that allows
our algorithm to only associate a word with a new
sense vector when evidence (from either same or
foreign language context) requires it. As a result,
the model infers different number of senses for
each word in a data-driven manner, avoiding wast-
ing parameters.

Together, these two ideas – multilingual dis-
tributional information and nonparametric sense
modeling – allow us to disambiguate multiple
senses using far less data than is necessary for pre-
vious methods. We experimentally demonstrate
that our algorithm can achieve competitive perfor-
mance after training on a small multilingual cor-
pus, comparable to a model trained monolingually
on a much larger corpus. We present an analy-
sis discussing the effect of various parameters –
choice of language family for deriving the multi-
lingual signal, crosslingual window size etc. and
also show qualitative improvement in the embed-
ding space.

2 Related Work

Work on inducing multi-sense embeddings can
be divided in two broad categories – two-staged
approaches and joint learning approaches. Two-
staged approaches (Reisinger and Mooney, 2010;
Huang et al., 2012) induce multi-sense embed-
dings by first clustering the contexts and then
using the clustering to obtain the sense vec-
tors. The contexts can be topics induced us-
ing latent topic models(Liu et al., 2015a,b), or
Wikipedia (Wu and Giles, 2015) or coarse part-
of-speech tags (Qiu et al., 2014). A more recent
line of work in the two-staged category is that
of retrofitting (Faruqui et al., 2015; Jauhar et al.,
2015), which aims to infuse semantic ontologies
from resources like WordNet (Miller, 1995) and
Framenet (Baker et al., 1998) into embeddings
during a post-processing step. Such resources list
(albeit not exhaustively) the senses of a word, and
by retro-fitting it is possible to tease apart the dif-
ferent senses of a word. While some resources like
WordNet (Miller, 1995) are available for many
languages, they are not exhaustive in listing all
possible senses. Indeed, the number senses of a
word is highly dependent on the task and can-
not be pre-determined using a lexicon (Kilgarriff,
1997). Ideally, the senses should be inferred in a
data-driven manner, so that new senses not listed
in such lexicons can be discovered. While re-

102



cent work has attempted to remedy this by using
parallel text for retrofitting sense-specific embed-
dings (Ettinger et al., 2016), their procedure re-
quires creation of sense graphs, which introduces
additional tuning parameters. On the other hand,
our approach only requires two tuning parameters
(prior α and maximum number of senses T ).

In contrast, joint learning approaches (Nee-
lakantan et al., 2014; Li and Jurafsky, 2015)
jointly learn the sense clusters and embeddings
by using non-parametrics. Our approach belongs
to this category. The closest non-parametric ap-
proach to ours is that of (Bartunov et al., 2016),
who proposed a multi-sense variant of the skip-
gram model which learns the different number of
sense vectors for all words from a large mono-
lingual corpus (eg. English Wikipedia). Our
work can be viewed as the multi-view extension of
their model which leverages both monolingual and
crosslingual distributional signals for learning the
embeddings. In our experiments, we compare our
model to monolingually trained version of their
model.

Incorporating crosslingual distributional infor-
mation is a popular technique for learning word
embeddings, and improves performance on sev-
eral downstream tasks (Faruqui and Dyer, 2014;
Guo et al., 2016; Upadhyay et al., 2016). However,
there has been little work on learning multi-sense
embeddings using crosslingual signals (Bansal
et al., 2012; Guo et al., 2014a; Šuster et al., 2016)
with only (Šuster et al., 2016) being a joint ap-
proach. (Kawakami and Dyer, 2015) also used
bilingual distributional signals in a deep neural ar-
chitecture to learn context dependent representa-
tions for words, though they do not learn separate
sense vectors.

3 Model Description

Let E = {xe1, .., xei , .., xeNe} denote the words of
the English side and F = {xf1 , .., xfi , .., xfNf } de-
note the words of the foreign side of the paral-
lel corpus. We assume that we have access to
word alignments Ae→f and Af→e mapping words
in English sentence to their translation in foreign
sentence (and vice-versa), so that xe and xf are
aligned if Ae→f (xe) = xf .

We define Nbr(x, L, d) as the neighborhood in
language L of size d (on either side) around word
x in its sentence. The English and foreign neigh-
boring words are denoted by ye and yf , respec-

tively. Note that ye and yf need not be translations
of each other. Each word xf in the foreign vocab-
ulary is associated with a dense vector xf in Rm,
and each word xe in English vocabulary admits at
most T sense vectors, with the kth sense vector
denoted as xek.

2 As our main goal is to model
multiple senses for words in English, we do not
model polysemy in the foreign language and use a
single vector to represent each word in the foreign
vocabulary.

We model the joint conditional distribution of
the context words ye, yf given an English word xe

and its corresponding translation xf on the parallel
corpus:

P (ye, yf | xe, xf ;α, θ), (1)
where θ are model parameters (i.e. all embed-
dings) and α governs the hyper-prior on latent
senses.

Assume xe has multiple senses, which are in-
dexed by the random variable z, Eq. (1) can be
rewritten,∫

β

∑
z
P (ye, yfz, β | xe, xf , α; θ)dβ

where β are the parameters determining the model
probability on each sense for xe (i.e., the weight
on each possible value for z). We place a Dirich-
let process (Ferguson, 1973) prior on sense assign-
ment for each word. Thus, adding the word-x sub-
script to emphasize that these are word-specific
senses,

P (zx = k | βx) = βxk
∏k−1

r=1
(1− βxr) (2)

βxk | α ind∼ Beta(βxk | 1, α), k = 1, . . . . (3)
That is, the potentially infinite number of senses
for each word x have probability determined
by the sequence of independent stick-breaking
weights, βxk, in the constructive definition of the
DP (Sethuraman, 1994). The hyper-prior concen-
tration α provides information on the number of
senses we expect to observe in our corpus.

After conditioning upon word sense, we decom-
pose the context probability,

P (ye, yf | z, xe, xf ; θ) =
P (ye | xe, xf , z; θ)P (yf | xe, xf , z; θ).

2We also maintain a context vector for each word in the
English and Foreign vocabularies. The context vector is used
as the representation of the word when it appears as the con-
text for another word.

103



Both the first and the second terms are sense-
dependent, and each factors as,

P (y |xe, xf , z=k; θ)∝Ψ(xe, z=k, y)Ψ(xf , y)
= exp(yTxek) exp(y

Txf ) = exp(yT (xek+x
f )),

where xek is the embedding corresponding to the
kth sense of the word xe, and y is either ye or yf .
The factor Ψ(xe, z = k, y) use the corresponding
sense vector in a skip-gram-like formulation. This
results in total of 4 factors,

P (ye, yf | z, xe, xf ; θ) ∝ Ψ(xe, z, ye)Ψ(xf , yf )
Ψ(xe, z, yf )Ψ(xf , ye)

(4)

See Figure 2 for illustration of each factor. This
modeling approach is reminiscent of (Luong et al.,
2015), who jointly learned embeddings for two
languages l1 and l2 by optimizing a joint objective
containing 4 skip-gram terms using the aligned
pair (xe,xf )– two predicting monolingual contexts
l1 → l1, l2 → l2 , and two predicting crosslingual
contexts l1 → l2, l2 → l1.
Learning. Learning involves maximizing the
log-likelihood,

P (ye, yf | xe, xf ;α, θ) =∫
β

∑
z
P (ye, yf , z, β | xe, xf , α; θ)dβ

for which we use variational approximation. Let
q(z, β) = q(z)q(β) where

q(z) =
∏
i q(zi) q(β) =

∏V
w=1

∏T
k=1 βwk

(5)
are the fully factorized variational approximation
of the true posterior P (z, β | ye, yf , xe, xf , α),
where V is the size of english vocabulary, and T is
the maximum number of senses for any word. The
optimization problem solves for θ,q(z) and q(β)
using the stochastic variational inference tech-
nique (Hoffman et al., 2013) similar to (Bartunov
et al., 2016) (refer for details).

The resulting learning algorithm is shown as Al-
gorithm 1. The first for-loop (line 1) updates the
English sense vectors using the crosslingual and
monolingual contexts. First, the expected sense
distribution for the current English wordw is com-
puted using the current estimate of q(β) (line 4).
The sense distribution is updated (line 7) using the
combined monolingual and crosslingual contexts

The	bank	paid	me	[interest]	on	my	savings.	

la	banque	m'a	payé	des	[intérêts]	sur	mes	économies.	

Ψ(interest,2,savings)	

Ψ(intérêts,	économies)	

Ψ(interest,2,banque)	 Ψ(intérêts,savings)	

Figure 2: The aligned pair (interest,intérêt) is used to predict
monolingual and crosslingual context in both languages (see
factors in eqn. (4)). We pick each sense (here 2nd) vector for
interest, to perform weighted update. We only model poly-
semy in English.

(line 5) and re-normalized (line 8). Using the up-
dated sense distribution q(β)’s sufficient statistics
is re-computed (line 9) and the global parameter θ
is updated (line 10) as follows,

θ ← θ + ρt∇θ
∑

k|zik>�

∑
y∈yc

zik log p(y|xi, k, θ)

(6)
Note that in the above sum, a sense participates in
a update only if its probability exceeds a thresh-
old � (= 0.001). The final model retains sense
vectors whose sense probability exceeds the same
threshold. The last for-loop (line 11) jointly opti-
mizes the foreign embeddings using English con-
text with the standard skip-gram updates.

Disambiguation. Similar to (Bartunov et al.,
2016), we can disambiguate the sense for the word
xe given a monolingual context ye as follows,

P (z | xe, ye) ∝
P (ye | xe, z; θ)

∑
β
P (z | xe, β)q(β) (7)

Although the model trains embeddings using both
monolingual and crosslingual context, we only use
monolingual context at test time. We found that
so long as the model has been trained with mul-
tilingual context, it performs well in sense dis-
ambiguation on new data even if it contains only
monolingual context. A similar observation was
made by (Šuster et al., 2016).

4 Multilingual Extension

Bilingual distributional signal alone may not be
sufficient as polysemy may survive translation in
the second language. Unlike existing approaches,
we can easily incorporate multilingual distribu-
tional signals in our model. For using languages
l1 and l2 to learn multi-sense embeddings for En-
glish, we train on a concatenation of En-l1 par-
allel corpus with an En-l2 parallel corpus. This
technique can easily be generalized to more than

104



Algorithm 1 Psuedocode of Learning Algorithm
Input: parallel corpus E = {xe1, .., xei , .., xeNe}

and F = {xf1 , .., xfi , .., xfNf } and alignments
Ae→f and Af→e, Hyper-parameters α and T ,
window sizes d, d′ .

Output: θ, q(β), q(z)
1: for i = 1 to Ne do . update english vectors
2: w← xei
3: for k = 1 to T do
4: zik ← Eq(βw)[log p(zi = k|, xei )]
5: yc←Nbr(xei ,E,d)∪Nbr(xfi ,F ,d′)∪ {xfi }

where xfi = Ae→f (x
e
i )

6: for y in yc do
7: SENSE-UPDATE(xei , y, zi)

8: Renormalize zi using softmax
9: Update suff. stats. for q(β) like (Bartunov

et al., 2016)
10: Update θ using eq. (6)
11: for i = 1 to Nf do . jointly update foreign

vectors
12: yc←Nbr(xfi ,F ,d) ∪Nbr(xei ,E,d′) ∪ {xei}

where xei = Af→e(x
f
i )

13: for y in yc do
14: SKIP-GRAM-UPDATE(xfi , y)

15: procedure SENSE-UPDATE(xi, y, zi)
16: zik ← zik + log p(y|xi, k, θ)
two foreign languages to obtain a large multilin-
gual corpus.

Value of Ψ(ye, xf ). The factor modeling the de-
pendence of the English context word ye on for-
eign word xf is crucial to performance when us-
ing multiple languages. Consider the case of using
French and Spanish contexts to disambiguate the
financial sense of the English word bank. In this
case, the (financial) sense vector of bank will be
used to predict vector of banco (Spanish context)
and banque (French context). If vectors for banco
and banque do not reside in the same space or are
not close, the model will incorrectly assume they
are different contexts to introduce a new sense for
bank. This is precisely why the bilingual mod-
els, like that of (Šuster et al., 2016), cannot be ex-
tended to multilingual setting, as they pre-train the
embeddings of second language before running
the multi-sense embedding process. As a result of
naive pre-training, the French and Spanish vectors
of semantically similar pairs like (banco,banque)
will lie in different spaces and need not be close.
A similar reason holds for (Guo et al., 2014a), as

Corpus Source Lines (M) EN-Words (M)

En-Fr EU proc. ≈ 10 250
En-Zh FBIS news ≈ 9.5 286
En-Es UN proc. ≈ 10 270
En-Fr UN proc. ≈ 10 260
En-Zh UN proc. ≈ 8 230
En-Ru UN proc. ≈ 10 270

Table 1: Corpus Statistics (in millions). Horizontal lines de-
marcate corpora from the same domain.

they use a two step approach instead of joint learn-
ing.

To avoid this, the vector for pairs like banco
and banque should lie in the same space and close
to each other and the sense vector for bank. The
Ψ(ye, xf ) term attempts to ensure this by using the
vector for banco and banque to predict the vector
of bank. This way, the model brings the embed-
ding space for Spanish and French closer by using
English as a bridge language during joint training.
A similar idea of using English as a bridging lan-
guage was used in the models proposed in (Her-
mann and Blunsom, 2014) and (Coulmance et al.,
2015). Beside the benefit in the multilingual case,
the Ψ(ye, xf ) term improves performance in the
bilingual case as well, as it forces the English and
second language embeddings to remain close in
space.

To show the value of Ψ(ye, xf ) factor in our ex-
periments, we ran a variant of Algorithm 1 with-
out the Ψ(ye, xf ) factor, by only using monolin-
gual neighborhood Nbr(xfi , F ) in line 12 of Al-
gorithm 1. We call this variant ONE-SIDED model
and the model in Algorithm 1 the FULL model.

5 Experimental Setup

We first describe the datasets and the preprocess-
ing methods used to prepare them. We also de-
scribe the Word Sense Induction task that we used
to compare and evaluate our method.

Parallel Corpora. We use parallel corpora in
English (En), French (Fr), Spanish (Es), Russian
(Ru) and Chinese (Zh) in our experiments. Corpus
statistics for all datasets used in our experiments
are shown in Table 1. For En-Zh, we use the FBIS
parallel corpus (LDC2003E14). For En-Fr, we use
the first 10M lines from the Giga-EnFr corpus re-
leased as part of the WMT shared task (Callison-
Burch et al., 2011). Note that the domain from
which parallel corpus has been derived can affect

105



the final result. To understand what choice of lan-
guages provide suitable disambiguation signal, it
is necessary to control for domain in all paral-
lel corpora. To this end, we also used the En-Fr,
En-Es, En-Zh and En-Ru sections of the MultiUN
parallel corpus (Eisele and Chen, 2010). Word
alignments were generated using fast_align
tool (Dyer et al., 2013) in the symmetric intersec-
tion mode. Tokenization and other preprocessing
were performed using cdec 3 toolkit. Stanford
Segmenter (Tseng et al., 2005) was used to pre-
process the Chinese corpora.

Word Sense Induction (WSI). We evaluate our
approach on word sense induction task. In this
task, we are given several sentences showing us-
ages of the same word, and are required to cluster
all sentences which use the same sense (Nasirud-
din, 2013). The predicted clustering is then com-
pared against a provided gold clustering. Note
that WSI is a harder task than Word Sense Disam-
biguation (WSD)(Navigli, 2009), as unlike WSD,
this task does not involve any supervision or ex-
plicit human knowledge about senses of words.
We use the disambiguation approach in eq. (7) to
predict the sense given the target word and four
context words.

To allow for fair comparison with earlier work,
we use the same benchmark datasets as (Bartunov
et al., 2016) – Semeval-2007, 2010 and Wikipedia
Word Sense Induction (WWSI). We report Ad-
justed Rand Index (ARI) (Hubert and Arabie,
1985) in the experiments, as ARI is a more strict
and precise metric than F-score and V-measure.

Parameter Tuning. For fairness, we used five
context words on either side to update each En-
glish word-vectors in all the experiments. In the
monolingual setting, all five words are English;
in the multilingual settings, we used four neigh-
boring English words plus the one foreign word
aligned to the word being updated (d = 4, d′ = 0
in Algorithm 1). We also analyze effect of varying
d′, the context window size in the foreign sentence
on the model performance.

We tune the parameters α and T by maximizing
the log-likelihood of a held out English text.4 The
parameters were chosen from the following values
α = {0.05, 0.1, .., 0.25}, T = {5, 10, .., 30}. All
models were trained for 10 iteration with a decay-

3github.com/redpony/cdec
4first 100k lines from the En-Fr Europarl (Koehn, 2005)

ing learning rate of 0.025, decayed to 0. Unless
otherwise stated, all embeddings are 100 dimen-
sional.

Under various choice of α and T , we identify
only about 10-20% polysemous words in the vo-
cabulary using monolingual training and 20-25%
polysemous using multilingual training. It is evi-
dent using the non-parametric prior has led to sub-
stantially more efficient representation compared
to previous methods with fixed number of senses
per word.

6 Experimental Results

Setting S-2007 S-2010 WWSI avg. ARI SCWS

En-Fr

MONO .044 .064 .112 .073 41.1
ONE-SIDED .054 .074 .116 .081 41.9
FULL .055 .086 .105 .082 41.8

En-Zh

MONO .054 .074 .073 .067 42.6
ONE-SIDED .059 .084 .078 .074 45.0
FULL .055 .090 .079 .075 41.7

En-FrZh

MONO .056 .086 .103 .082 47.3
ONE-SIDED .067 .085 .113 .088 44.6
FULL .065 .094 .120 .093 41.9

Table 2: Results on word sense induction (left four columns)
in ARI and contextual word similarity (last column) in per-
cent correlation. Language pairs are separated by horizontal
lines. Best results shown in bold.

We performed extensive experiments to evalu-
ate the benefit of leveraging bilingual and multi-
lingual information during training. We also ana-
lyze how the different choices of language family
(i.e. using more distant vs more similar languages)
affect performance of the embeddings.

6.1 Word Sense Induction Results.
The results for WSI are shown in Table 2. Recall
that the ONE-SIDED model is the variant of Algo-
rithm 1 without the Ψ(ye, xf ) factor. MONO refers
to the AdaGram model of (Bartunov et al., 2016)
trained on the English side of the parallel corpus.
In all cases, the MONO model is outperformed by
ONE-SIDED and FULL models, showing the ben-
efit of using crosslingual signal in training. Best
performance is attained by the multilingual model
(En-FrZh), showing value of multilingual signal.
The value of Ψ(ye, xf ) term is also verified by the
fact that the ONE-SIDED model performs worse
than the FULL model.

106



Train S-2007 S-2010 WWSI Avg. ARI
Setting En-FrEs En-RuZh En-FrEs En-FrEs En-FrEs En-RuZh En-FrEs En-RuZh

(1) MONO .035 .033 .046 .049 .054 .049 .045 .044
(2) ONE-SIDED .044 .044 .055 .063 .062 .057 .054 .055
(3) FULL .046 .040 .056 .070 .068 .069 .057 .059

(3) - (1) .011 .007 .010 .021 .014 .020 .012 .015

Table 3: Effect (in ARI) of language family distance on WSI task. Best results for each column is shown in bold. The
improvement from MONO to FULL is also shown as (3) - (1). Note that this is not comparable to results in Table 2, as we use a
different training corpus to control for the domain.

We can also compare (unfairly to our FULL
model) to the best results described in (Bartunov
et al., 2016), which achieved ARI scores of 0.069,
0.097 and 0.286 on the three datasets respectively
after training 300 dimensional embeddings on En-
glish Wikipedia (≈ 100M lines). Note that, as
WWSI was derived from Wikipedia, training on
Wikipedia gives AdaGram model an undue ad-
vantage, resulting in high ARI score on WWSI.
In comparison, our model did not train on En-
glish Wikipedia, and uses 100 dimensional em-
beddings. Nevertheless, even in the unfair com-
parison, it noteworthy that on S-2007 and S-2010,
we can achieve comparable performance (0.067
and 0.094) with multilingual training to a model
trained on almost 5 times more data using higher
(300) dimensional embeddings.

6.2 Contextual Word Similarity Results.

For completeness, we report correlation scores
on Stanford contextual word similarity dataset
(SCWS) (Huang et al., 2012) in Table 2. The
task requires computing similarity between two
words given their contexts. While the bilin-
gually trained model outperforms the monolin-
gually trained model, surprisingly the multilin-
gually trained model does not perform well on
SCWS. We believe this may be due to our param-
eter tuning strategy.5

6.3 Effect of Language Family Distance.

Intuitively, choice of language can affect the result
from crosslingual training as some languages may
provide better disambiguation signals than oth-
ers. We performed a systematic set of experiment
to evaluate whether we should choose languages
from a closer family (Indo-European languages)
or farther family (Non-Indo European Languages)

5Most works tune directly on the test dataset for Word
Similarity tasks (Faruqui et al., 2016)

as training data alongside English.6 To control for
domain here we use the MultiUN corpus. We use
En paired with Fr and Es as Indo-European lan-
guages, and English paired with Ru and Zh for
representing Non-Indo-European languages.

From Table 3, we see that using Non-Indo Eu-
ropean languages yield a slightly higher improve-
ment on an average than using Indo-European lan-
guages. This suggests that using languages from a
distance family aids better disambiguation. Our
findings echo those of (Resnik and Yarowsky,
1999), who found that the tendency to lexicalize
senses of an English word differently in a second
language, correlated with language distance.

6.4 Effect of Window Size.

Figure 3d shows the effect of increasing the
crosslingual window (d′) on the average ARI on
the WSI task for the En-Fr and En-Zh models.
While increasing the window size improves the
average score for En-Zh model, the score for the
En-Fr model goes down. This suggests that it
might be beneficial to have a separate window pa-
rameter per language. This also aligns with the
observation earlier that different language fami-
lies have different suitability (bigger crosslingual
context from a distant family helped) and require-
ments for optimal performance.

7 Qualitative Illustration

As an illustration for the effects of multilingual
training, Figure 3 shows PCA plots for 11 sense
vectors for 9 words using monolingual, bilin-
gual and multilingual models. From Fig 3a, we
note that with monolingual training the senses are
poorly separated. Although the model infers two
senses for bank, the two senses of bank are close
to financial terms, suggesting their distinction was
not recognized. The same observation can be

6 (Šuster et al., 2016) compared different languages but
did not control for domain.

107



(a) Monolingual (En side of En-Zh) (b) Bilingual (En-Zh)

(c) Multilingual (En-FrZh)

0 1 2 3 4

7

7.5

8

8.5
·10−2

window

av
g.

A
R

I

En-Fr
En-Zh

(d) Window size v.s. avg. ARI

Figure 3: Qualitative: PCA plots for the vectors for {apple, bank, interest, itunes, potato, west, monetary, desire} with
multiple sense vectors for apple,interest and bank obtained using monolingual (3a), bilingual (3b) and multilingual (3c) training.
Window Tuning: Figure 3d shows tuning window size for En-Zh and En-Fr.

made for the senses of apple. In Fig 3b, with bilin-
gual training, the model infers two senses of bank
correctly, and two sense of apple become more
distant. The model can still improve eg. pulling
interest towards the financial sense of bank, and
pulling itunes towards apple 2. Finally, in Fig 3c,
all senses of the words are more clearly clustered,
improving over the clustering of Fig 3b. The
senses of apple, interest, and bank are well sepa-
rated, and are close to sense-specific words, show-
ing the benefit of multilingual training.

8 Conclusion

We presented a multi-view, non-parametric word
representation learning algorithm which can
leverage multilingual distributional information.
Our approach effectively combines the bene-
fits of crosslingual training and Bayesian non-
parametrics. Ours is the first multi-sense repre-

sentation learning algorithm capable of using mul-
tilingual distributional information efficiently, by
combining several parallel corpora to obtained a
large multilingual corpus. Our experiments show
how this multi-view approach learns high-quality
embeddings using substantially less data and pa-
rameters than prior state-of-the-art. We also an-
alyzed the effect of various parameters such as
choice of language family and cross-lingual win-
dow size on the performance. While we focused
on improving the embedding of English words in
this work, the same algorithm could learn better
multi-sense embedding for other languages. Ex-
citing avenues for future research include extend-
ing our approach to model polysemy in foreign
language. The sense vectors can then be aligned
across languages, to generate a multilingual Word-
net like resource, in a completely unsupervised
manner thanks to our joint training paradigm.

108



References
Collin F Baker, Charles J Fillmore, and John B Lowe.

1998. The berkeley framenet project. In ACL.

Mohit Bansal, John DeNero, and Dekang Lin. 2012.
Unsupervised translation sense clustering. In
NAACL.

Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In ACL.

Sergey Bartunov, Dmitry Kondrashkin, Anton Osokin,
and Dmitry Vetrov. 2016. Breaking sticks and am-
biguities with adaptive skip-gram. AISTATS .

Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar F Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In
WMT Shared Task.

Jocelyn Coulmance, Jean-Marc Marty, Guillaume
Wenzek, and Amine Benhalloum. 2015. Trans-
gram, fast cross-lingual word-embeddings. In
EMNLP.

Ido Dagan and Alon Itai. 1994. Word sense disam-
biguation using a second language monolingual cor-
pus. Computational linguistics .

Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel cor-
pora. In ACL.

Chris Dyer, Victor Chahuneau, and Noah A Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In NAACL.

Andreas Eisele and Yu Chen. 2010. MultiUN: A mul-
tilingual corpus from united nation documents. In
LREC.

Allyson Ettinger, Philip Resnik, and Marine Carpuat.
2016. Retrofitting sense-specific word vectors using
parallel text. In NAACL.

Manaal Faruqui, Jesse Dodge, Sujay K. Jauhar, Chris
Dyer, Eduard Hovy, and Noah A. Smith. 2015.
Retrofitting word vectors to semantic lexicons. In
NAACL.

Manaal Faruqui and Chris Dyer. 2014. Improving vec-
tor space word representations using multilingual
correlation. In EACL.

Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi,
and Chris Dyer. 2016. Problems with evaluation of
word embeddings using word similarity tasks. In 1st
RepEval Workshop.

Thomas S Ferguson. 1973. A bayesian analysis of
some nonparametric problems. The annals of statis-
tics .

Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014a. Learning sense-specific word embed-
dings by exploiting bilingual resources. In COL-
ING.

Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014b. Revisiting embedding features for sim-
ple semi-supervised learning. In EMNLP.

Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2016. A representation learn-
ing framework for multi-source transfer parsing. In
AAAI.

Karl Moritz Hermann and Phil Blunsom. 2014. Mul-
tilingual Distributed Representations without Word
Alignment. In ICLR.

Matthew D Hoffman, David M Blei, Chong Wang, and
John William Paisley. 2013. Stochastic variational
inference. JMLR .

Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In ACL.

Lawrence Hubert and Phipps Arabie. 1985. Compar-
ing partitions. Journal of classification .

Sujay Kumar Jauhar, Chris Dyer, and Eduard Hovy.
2015. Ontologically grounded multi-sense represen-
tation learning for semantic vector space models. In
NAACL.

Kazuya Kawakami and Chris Dyer. 2015. Learning to
represent words in context with multilingual super-
vision. ICLR Workshop .

Adam Kilgarriff. 1997. I don’t believe in word senses.
Computers and the Humanities .

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit. vol-
ume 5, pages 79–86.

Jiwei Li and Dan Jurafsky. 2015. Do multi-sense em-
beddings improve natural language understanding?
EMNLP .

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2015a.
Learning context-sensitive word embeddings with
neural tensor skip-gram model. In IJCAI.

Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong
Sun. 2015b. Topical word embeddings. In AAAI.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Bilingual word representations with
monolingual quality in mind. In Workshop on Vector
Space Modeling for NLP.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In NAACL.

109



George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM .

Mohammad Nasiruddin. 2013. A state of the
art of word sense induction: A way towards
word sense disambiguation for under-resourced lan-
guages. arXiv preprint arXiv:1310.1425 .

Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys (CSUR) .

Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In EMNLP.

Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003.
Exploiting parallel texts for word sense disambigua-
tion: An empirical study. In ACL.

Lin Qiu, Yong Cao, Zaiqing Nie, Yong Yu, and Yong
Rui. 2014. Learning word representation consider-
ing proximity and ambiguity. In AAAI.

Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In NAACL.

Philip Resnik and David Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: New evalua-
tion methods for word sense disambiguation. NLE
.

Jayaram Sethuraman. 1994. A constructive definition
of dirichlet priors. Statistica sinica .

Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proc. of SIGHAN.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In ACL.

Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics .

Shyam Upadhyay, Manaal Faruqui, Chris Dyer, and
Dan Roth. 2016. Cross-lingual models of word em-
beddings: An empirical comparison. In ACL.

Simon Šuster, Ivan Titov, and Gertjan van Noord. 2016.
Bilingual learning of multi-sense embeddings with
discrete autoencoders. In NAACL.

Zhaohui Wu and C Lee Giles. 2015. Sense-aaware se-
mantic analysis: A multi-prototype word represen-
tation model using wikipedia. In AAAI.

David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In ACL.

110


