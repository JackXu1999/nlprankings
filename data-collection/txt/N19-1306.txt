



















































Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph Convolution Networks


Proceedings of NAACL-HLT 2019, pages 3016–3025
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

3016

Long-tail Relation Extraction via Knowledge Graph Embeddings and
Graph Convolution Networks

Ningyu Zhang1,2,3 Shumin Deng1,3 Zhanlin Sun1,3 Guanying Wang1,3
Xi Chen4 Wei Zhang2,3 Huajun Chen1,3∗

1. College of Computer Science and Technology, Zhejiang University
2. Alibaba Group

3. AZFT†Joint Lab for Knowledge Engine
4. Tencent

{3150105645,231sm,guanying wgy,huajunsir}@zju.edu.cn
jasonxchen@tencent.com,{ningyu.zny,lantu.zw}@alibaba-inc.com

Abstract

We propose a distance supervised relation ex-
traction approach for long-tailed, imbalanced
data which is prevalent in real-world settings.
Here, the challenge is to learn accurate ”few-
shot” models for classes existing at the tail of
the class distribution, for which little data is
available. Inspired by the rich semantic cor-
relations between classes at the long tail and
those at the head, we take advantage of the
knowledge from data-rich classes at the head
of the distribution to boost the performance of
the data-poor classes at the tail. First, we pro-
pose to leverage implicit relational knowledge
among class labels from knowledge graph em-
beddings and learn explicit relational knowl-
edge using graph convolution networks. Sec-
ond, we integrate that relational knowledge
into relation extraction model by coarse-to-
fine knowledge-aware attention mechanism.
We demonstrate our results for a large-scale
benchmark dataset which show that our ap-
proach significantly outperforms other base-
lines, especially for long-tail relations.

1 Introduction

Relation extraction (RE) is an important task in in-
formation extraction, aiming to extract the relation
between two given entities based on their related
context. Due to the capability of extracting tex-
tual information and benefiting many NLP appli-
cations (e.g., information retrieval, dialog genera-
tion, and question answering), RE appeals to many
researchers. Conventional supervised models have
been widely explored in this task (Zelenko et al.,
2003; Zeng et al., 2014); however, their perfor-
mance heavily depends on the scale and quality
of training data.

∗ Corresponding author.
†Alibaba-Zhejiang University Frontier Technology Re-

search Center

0 10 20 30 40 50
Sorted Relation ID

0

10000

20000

30000

40000

50000

60000

70000

Re
la

tio
n 

La
be

l F
re

qu
en

cy

~40 relation labels occur <1000 times

Figure 1: Label frequency distribution of classes with-
out NA in NYT dataset.

To construct large-scale data, (Mintz et al.,
2009) proposed a novel distant supervision (DS)
mechanism to automatically label training in-
stances by aligning existing knowledge graphs
(KGs) with text. DS enables RE models to work
on large-scale training corpora and has thus be-
come a primary approach for RE recently (Wu
et al., 2017; Feng et al., 2018). Although these
DS models achieve promising results on common
relations, their performance still degrades dramat-
ically when there are only a few training instances
for some relations. Empirically, DS can automat-
ically annotate adequate amounts of training data;
however, this data usually only covers a limited
part of the relations. Many relations are long-tail
and still suffer from data deficiency. Current DS
models ignore the problem of long-tail relations,
which makes it challenging to extract comprehen-
sive information from plain text.

Long-tail relations are important and cannot be
ignored. Nearly 70% of the relations are long-
tail in the widely used New York Times (NYT)
dataset1 (Riedel et al., 2010; Lei et al., 2018) as
shown in Figure 1. Therefore, it is crucial for mod-

1http://iesl.cs.umass.edu/riedel/ecml/



3017

els to be able to extract relations with limited num-
bers of training instances.

Dealing with long tails is very difficult as
few training examples are available. There-
fore, it is natural to transfer knowledge from
data-rich and semantically similar head classes
to data-poor tail classes (Wang et al., 2017).
For example, the long-tail relation /peo-
ple/deceased person/place of burial and head
relation /people/deceased person/place of death
are in the same branch /people/deceased person/*
as shown in Figure 2. They are semantically simi-
lar, and it is beneficial to leverage head relational
knowledge and transfer it to the long-tail relation,
thus enhancing general performance. In other
words, long-tail relations of one entity tuple can
have class ties with head relations, which can be
leveraged to enhance RE for narrowing potential
search spaces and reducing uncertainties between
relations when predicting unknown relations (Ye
et al., 2017). If one pair of entities contains
/people/deceased person/place of death, there
is a high probability that it will contain /peo-
ple/deceased person/place of burial. If we can
incorporate the relational knowledge between two
relations, extracting head relations will provide
evidence for the prediction of long-tail relations.

However, there exist two problems: (1) Learn-
ing relational knowledge: Semantically simi-
lar classes may contain more relational informa-
tion that will boost transfer, whereas irrelevant
classes (e.g., /location/location/contains and /peo-
ple/family/country) usually contain less relational
information that may result in negative transfer.
(2) Leveraging relational knowledge: Integrat-
ing relational knowledge to existing RE models is
challenging.

To address the problem of learning relational
knowledge, as shown in (Lin et al., 2016; Ye
et al., 2017), we use class embeddings to rep-
resent relation classes and utilize KG embed-
dings and graph convolution networks (GCNs)
to extract implicit and explicit relational knowl-
edge. Specifically, previous studies (Yang et al.,
2015) have shown that the embeddings of se-
mantically similar relations are located near
each other in the latent space. For instance,
the relation /people/person/place lived and /peo-
ple/person/nationality are more relevant, whereas
the relation /people/person/profession has less cor-
relation with the former two relations. Thus, it

[ismail_merchant], whose filmmaking collaboration with james ivory 
created a genre of films with visually sumptuous settings that told 
literate tales of individuals trying to adapt to shifting societal values , 
died yesterday in a  [London] hospital

[darren_mcgavin] , an actor with hundreds of television , movie and 
theatrical credits to his name , died on saturday in [los_angeles] .

the night the news hit that [hunter_s._Thompson] had committed 
suicide at his home in [woody_creek] , colo. , i drove to my office and 
read a few of the letters we had exchanged over the years .

noting that [charles_Darwin] is buried in [westminster_abbey ], dr. 
barrow said that in contrast with the so-called culture wars in america , 
science and religion had long coexisted peaceably in england . ''

/people/deceased_person/place_of_burial

/people/deceased_person/place_of_death

Long-tail relation (24 samples)

Head  relation (2541 samples)

/people/deceased_person/*

Knowledge Transfer

Figure 2: Head and long-tail relations.

is natural to leverage this knowledge from KGs.
However, because there are many one-to-multiple
relations in KGs, the relevant information for each
class may be scattered. In other words, there may
not be enough relational signal between classes.
Therefore, we utilize GCNs to learn explicit rela-
tional knowledge.

To address the problem of leveraging relational
knowledge, we first use convolution neural net-
works (Zeng et al., 2014, 2015) to encode sen-
tences; then introduce coarse-to-fine knowledge-
aware attention mechanism for combining rela-
tional knowledge with encoded sentences into bag
representation vectors. The relational knowledge
not only provides more information for relation
prediction but also provides a better reference
message for the attention module to raise the per-
formance of long-tail classes.

Our experimental results on the NYT dataset
show that: (1) our model is effective compared
to baselines especially for long-tail relations; (2)
leveraging relational knowledge enhances RE and
our model is efficient in learning relational knowl-
edge via GCNs.

2 Related Work

Relation Extraction. Supervised RE models (Ze-
lenko et al., 2003; GuoDong et al., 2005; Mooney
and Bunescu, 2006) require adequate amounts
of annotated data for training which is time-
consuming. Hence, (Mintz et al., 2009) proposd
DS to automatically label data. DS inevitably ac-
companies with the wrong labeling problem. To
alleviate the noise issue, (Riedel et al., 2010; Hoff-
mann et al., 2011) proposed multi-instance learn-
ing (MIL) mechanisms. Recently, neural mod-
els have been widely used for RE; those mod-
els can accurately capture textual relations with-
out explicit linguistic analysis (Zeng et al., 2015;



3018

Lin et al., 2016; Zhang et al., 2018a). To fur-
ther improve the performance, some studies incor-
porate external information (Zeng et al., 2017; Ji
et al., 2017; Han et al., 2018a) and advanced train-
ing strategies (Ye et al., 2017; Liu et al., 2017;
Huang and Wang, 2017; Feng et al., 2018; Zeng
et al., 2018; Wu et al., 2017; Qin et al., 2018).
These works mainly adopt DS to make large-scale
datasets and reduce the noise caused by DS, re-
gardless of the effect of long-tail relations.

There are only a few studies on long-tail for
RE (Gui et al., 2016; Lei et al., 2018; Han et al.,
2018b). Of these, (Gui et al., 2016) proposed an
explanation-based approach, whereas (Lei et al.,
2018) utilized external knowledge (logic rules).
These studies treat each relation in isolation, re-
gardless of the rich semantic correlations between
the relations. (Han et al., 2018b) proposed a
hierarchical attention scheme for RE, especially
for long-tail relations. Different from those ap-
proaches, we leverage implicit and explicit rela-
tional knowledge from KGs and GCNs rather than
data-driven learned parameter spaces where simi-
lar relations may have distinct parameters, hinder-
ing the generalization of long-tail classes.

Knowledge Graph Embedding. Recently,
several KG embedding models have been pro-
posed. These methods learn low-dimensional vec-
tor representations for entities and relations (Bor-
des et al., 2013; Wang et al., 2014; Lin et al.,
2015). TransE (Bordes et al., 2013) is one of
the most widely used models, which views rela-
tions as translations from a head entity to a tail en-
tity on the same low-dimensional hyperplane. In-
spired by the rich knowledge in KGs, recent works
(Han et al., 2018a; Wang et al., 2018; Lei et al.,
2018) extend DS models under the guidance of
KGs. However, these works neglect rich corre-
lations between relations. Relation structure (re-
lational knowledge) has been studied and is quite
effective for KG completion (Zhang et al., 2018b).
To the best of our knowledge, this is the first effort
to consider the relational knowledge of classes (re-
lations) using KGs for RE.

Graph Convolutional Networks. GCNs gen-
eralize CNNs beyond two-dimensional and one-
dimensional spaces. (Defferrard et al., 2016)
developed spectral methods to perform efficient
graph convolutions. (Kipf and Welling, 2016) as-
sumed the graph structure is known over input
instances and apply GCNs for semi-supervised

learning. GCNs were applied to relational data
(e.g., link prediction) by (Schlichtkrull et al.,
2018). GCNs have also had success in other NLP
tasks such as semantic role labeling (Marcheg-
giani and Titov, 2017), dependency parsing
(Strubell and McCallum, 2017), and machine
translation (Bastings et al., 2017).

Two GCNs studies share similarities with our
work. (1) (Chen et al., 2017) used GCNs on struc-
tured label spaces. However, their experiments
do not handle long-tail labels and do not incorpo-
rate attention but use an average of word vectors
to represent each document. (2) (Rios and Kavu-
luru, 2018) proposed a few-shot and zero-shot text
classification method by exploiting structured la-
bel spaces with GCNs. However, they used GCNs
in the label graph whereas we utilize GCNs in the
hierarchy graph of labels.

3 Methodology

In this section, we introduce the overall framework
of our approach for RE, starting with the notations.

3.1 Notations

We denote a KG as G = E ,R,F , where E , R and
F indicate the sets of entities, relations and facts
respectively. (h, r, t) ∈ F indicates that there is
a relation r ∈ R between h ∈ E and t ∈ E . We
follow the MIL setting and split all instances into
multiple entity-pair bags {Sh1,t1 ,Sh2,t2 , ...}. Each
bag Shi,ti contains multiple instances {s1, s2, ...}
mentioning both entities hi and ti. Each instance
s in these bags is denoted as a word sequence s =
{w1, w2, ...}.

3.2 Framework

Our model consists of three parts as shown in Fig-
ure 3:

Instance Encoder. Given an instance and its
mentioned entity pair, we employ neural networks
to encode the instance semantics into a vector.
In this study, we implement the instance encoder
with convolutional neural networks (CNNs) given
both model performance and time efficiency.

Relational Knowledge Learning. Given pre-
trained KG embeddings (e.g., TransE (Bordes
et al., 2013)) as implicit relational knowledge, we
employ GCNs to learn explicit relational knowl-
edge. By assimilating generic message-passing in-
ference algorithms with neural-network counter-
part, we can learn better embeddings for Knowl-



3019

 

!" !# !$

%" %# %$

-

Figure 3: Architecture of our proposed model.

edge Relation. We concatenate the outputs of
GCNs and the pretrained KG embeddings to form
the final class embeddings.

Knowledge-aware Attention. Under the guid-
ance of final class embeddings, knowledge-aware
attention is aimed to select the most informative
instance exactly matching relevant relation.

3.3 Instance Encoder

Given an instance s = {w1, ..., wn} mentioning
two entities, we encode the raw instance into a
continuous low-dimensional vector x, which con-
sists of an embedding layer and an encoding layer.

Embedding Layer. The embedding layer is
used to map discrete words in the instance into
continuous input embeddings. Given an instance
s, we map each word wi in the instance to a
real-valued pretrained Skip-Gram (Mikolov et al.,
2013) embedding wi ∈ Rdw . We adopt position
embeddings following (Zeng et al., 2014). For
each word wi, we embed its relative distances to
the two entities into two dp-dimensional vectors.
We then concatenate the word embeddings and po-
sition embeddings to achieve the final input em-
beddings for each word and gather all the input
embeddings in the instance. We thus obtain an em-
bedding sequence ready for the encoding layer.

Encoding Layer. The encoding layer aims
to compose the input embeddings of a given in-
stance into its corresponding instance embedding.
In this study, we choose two convolutional neu-
ral architectures, CNN (Zeng et al., 2014) and
PCNN (Zeng et al., 2015) to encode input em-

beddings into instance embeddings. Other neu-
ral architectures such as recurrent neural networks
(Zhang and Wang, 2015) can also be used as sen-
tence encoders. Because previous works show that
both convolutional and recurrent architectures can
achieve comparable state-of-the-art performance,
we select convolutional architectures in this study.
Note that, our model is independent of the encoder
choices, and can, therefore, be easily adapted to fit
other encoder architectures.

3.4 Relational Knowledge Learning through
KG Embeddings and GCNs.

Given pretrained KG embeddings and predefined
class (relation) hierarchies2, we first leverage the
implicit relational knowledge from KGs and ini-
tialize the hierarchy label graph; then we apply
two layer GCNs to learn explicit fine-grained re-
lational knowledge from the label space.

Hierarchy Label Graph Construction. Given
a relation set R of a KG G (e.g., Freebase),
which consists of base-level relations (e.g., /peo-
ple/person/ethnicity), we can generate the corre-
sponding higher-level relation set RH . The re-
lations in a high-level set (e.g., people) are more
general and common; they usually contain several
sub-relations in the base-level set. The relation
hierarchies are tree-structured, and the generation
process can be done recursively. We use a virtual
father node to construct the highest level associa-

2For datasets without predefined relation hierarchies, hi-
erarchy clustering (Johnson, 1967) or K-means can construct
relation hierarchies (Zhang et al., 2018b); details can be
found in supplementary materials.



3020

tions between relations as shown in Figure 3. In
practice, we start from R0 = R which is the set
of all relations we focus on for RE, and the gener-
ation process is performed L − 1 times to get the
hierarchical relation sets {R0,R1, ...,RL}, where
RL is the virtual father node. Each node has
a specific type t ∈ {0, 1, ..., L} to identify its
layer hierarchies. For example, as shown in Fig-
ure 3, node /people/person/ethnicity has a spe-
cific type 0 to indicate it is in the bottom layer of
the graph. The vectors of each node in the bot-
tom layer are initialized through pretrained TransE
(Bordes et al., 2013) KG embeddings. Other KG
embeddings such as TransR (Lin et al., 2015) can
also be adopted. Their parent nodes are initialized
by averaging all children vectors. For example,
the node vector of /people/person/ is initialized by
averaging all the nodes under the branch of /peo-
ple/person/* (all child nodes).

GCN Output Layer. Due to one-to-multiple
relations and incompleteness in KGs, the implicit
relevant information obtained by KG embeddings
for each label is not enough. Therefore, we ap-
ply GCNs to learn explicit relational knowledge
among labels. We take advantage of the structured
knowledge over our label space using a two-layer
GCNs. Starting with the pretrained relation em-
bedding vimpliciti ∈ Rd from KGs, we combine
the label vectors of the children and parents for
the i-th label to form,

v1i = f(W
1vi +

∑
j∈Np

W 1p vj

|Np|
+

∑
j∈Nc

W 1c vj
|Nc|

+ b1g)

(1)
where W 1 ∈ Rq∗d, W 1p ∈ Rq∗d, W 1c ∈ Rq∗d,

b1g ∈ Rq, f is the rectified linear unit (Nair and
Hinton, 2010) function, and Nc (Np) is the index
set of the i-th labels children (parents). We use
different parameters to distinguish each edge type
where parent edges represent all edges from high
level labels and child edges represent all edges
from low level labels. The second layer follows
the same formulation as the first layer and outputs
vexpliciti . Finally, we concatenate both pretrained
vimpliciti with GCNs node vector v

explicit
i to form

hierarchy class embeddings,

qr = v
implicit
i ||v

explicit
i (2)

where qr ∈ Rd+q.

3.5 Knowledge-aware Attention

Traditionally, the output layer of PCNN/CNN
would learn label specific parameters optimized
by a cross-entropy loss. However, the label spe-
cific parameters spaces are unique to each relation,
matrices associated with the long-tails can only be
exposed to very few facts during training, result-
ing in poor generalization. Instead, our method
attempts to match sentence vectors to their corre-
sponding class embeddings rather than learning la-
bel specific attention parameters. In essence, this
becomes a retrieval problem. Relevant informa-
tion from class embeddings contains useful rela-
tional knowledge for long-tails among labels.

Practically, given the entity pair (h, t) and its
bag of instances Sh,t = {s1, s2, ..., sm}, we
achieve the instance embeddings {s1, s2, ..., sm}
using the sentence encoder. We group the class
embeddings according to their type (i.e., accord-
ing to their layers in the hierarchy label graph),
e.g., qri , i ∈ {0, 1, ..., L}. We adopt qri , i 6= L
(layer L is the virtual father node) as layer-wise
attention query vector. Then, we apply coarse-to-
fine knowledge-aware attention to them to obtain
the textual relation representation rh,t. For a rela-
tion r, we construct its hierarchical chain of par-
ent relations (r0, ..., rL−1) using the hierarchy la-
bel graph, where ri−1 is the sub-relation of ri. We
propose the following formulas to compute the at-
tention weight (similarity or relatedness) between
each instances feature vector sk and qri ,

ek =Ws(tanh[sk; qri ]) + bs

αik =
exp(ek)∑m
j=1 exp(ej)

(3)

where [x1;x2] denotes the vertical concatenation
of x1 and x2, Ws is the weight matrix, and bs
is the bias. We compute attention operations on
each layer of hierarchy label graph to obtain cor-
responding textual relation representations,

rih,t = ATT (qri , {s1, s2, ..., sm}) (4)

Then we need to combine the relation represen-
tations on different layers. Direct concatenation of
all the representations is a straightforward choice.
However, different layers have different contribu-
tions for different tuples. For example, the relation
/location/br state/ has only one sub-relation /loca-
tion/br state/capital, which indicates that it is more
important. In other words, if the sentence has high



3021

attention weights on relation /location/br state/, it
has a very high probability to have relation /loca-
tion/br state/capital. Hence, we use an attention
mechanism to emphasize the layers,

gi =Wgtanh(rh,t)

βi =
exp(gi)∑L−1

j=0 exp(gj)

(5)

where Wg is a weight matrix, rh,t is referred to
as a query-based function that scores how well the
input textual relation representations and the pre-
dict relation r match. The textual relation repre-
sentations in each layer are computed as,

rih,t = βir
i
h,t (6)

We simply concatenate the textual relation repre-
sentations on different layers as the final represen-
tation,

rh,r = Concat(r
0
h,t, .., r

L−1
h,t ) (7)

The representation rh,t will be finally fed to com-
pute the conditional probability P(r|h, t,Sh,t),

P(r|h, t,Sh,t) =
exp(or)∑
r̃∈R exp(or̃)

(8)

where o is the score of all relations defined as,

o =Mrh,t (9)

where M is the representation matrix to calculate
the relation scores. Note that, attention weight qri
is obtained from the outputs of GCNs and pre-
trained KG embeddings, which can provide more
informative parameters than data-driven learned
parameters, especially for long-tails.

4 Experiments

4.1 Datasets and Evaluation
We evaluate our models on the NYT dataset de-
veloped by (Riedel et al., 2010), which has been
widely used in recent studies (Lin et al., 2016; Liu
et al., 2017; Wu et al., 2017; Feng et al., 2018).
The dataset has 53 relations including the NA
relation, which indicates that the relations of in-
stances are not available. The training set has
522611 sentences, 281270 entity pairs, and 18252
relational facts. In the test set, there are 172448
sentences, 96678 entity pairs, and 1950 relational
facts. In both training and test set, we truncate sen-
tences with more than 120 words into 120 words.

Figure 4: Precision-recall curves for the proposed
model and various baseline models.

Figure 5: Precision-recall curves for the proposed
model and various attention-based neural models.

We evaluate all models in the held-out evalua-
tion. It evaluates models by comparing the rela-
tional facts discovered from the test articles with
those in Freebase and provides an approximate
measure of precision without human evaluation.
For evaluation, we draw precision-recall curves
for all models. To further verify the effect of our
model for long-tails, we follow previous studies
(Han et al., 2018b) to report the Precision@N re-
sults. The dataset and baseline code can be found
on Github 3.

4.2 Parameter Settings4

To fairly compare the results of our models with
those baselines, we also set most of the experimen-
tal parameters by following (Lin et al., 2016). We
apply dropout on the output layers of our models
to prevent overfitting. We also pretrain the sen-
tence encoder of PCNN before training our model.

3https://github.com/thunlp/OpenNRE
4Details of hyper-parameters settings and evaluation of

different instances can be found in supplementary materials



3022

Training Instances
Hits@K (Macro)

<100 <200
10 15 20 10 15 20

CNN
+ATT <5.0 <5.0 18.5 <5.0 16.2 33.3

+HATT 5.6 31.5 57.4 22.7 43.9 65.1
+KATT 9.1 41.3 58.5 23.3 44.1 65.4

PCNN
+ATT <5.0 7.4 40.7 17.2 24.2 51.5

+HATT 29.6 51.9 61.1 41.4 60.6 68.2
+KATT 35.3 62.4 65.1 43.2 61.3 69.2

Table 1: Accuracy (%) of Hits@K on relations with
training instances fewer than 100/200.

4.3 Overall Evaluation Results

To evaluate the performance of our proposed
model, we compare the precision-recall curves
of our model with various previous RE models.
The evaluation results are shown in Figure 4 and
Figure 5. We report the results of neural ar-
chitectures including CNN and PCNN with var-
ious attention based methods: +KATT denotes
our approach, +HATT is the hierarchical atten-
tion method (Han et al., 2018b), +ATT is the
plain selective attention method over instances
(Lin et al., 2016), +ATT+ADV is the denoising
attention method by adding a small adversarial
perturbation to instance embeddings (Wu et al.,
2017), and +ATT+SL is the attention-based model
using soft-labeling method to mitigate the side
effect of the wrong labeling problem at entity-
pair level (Liu et al., 2017). We also compare
our method with feature-based models, including
Mintz (Mintz et al., 2009), MultiR (Hoffmann
et al., 2011) and MIML (Surdeanu et al., 2012).

As shown in both figures, our approach achieves
the best results among all attention-based mod-
els. Even when compared with PCNN+HATT,
PCNN+ATT+ADV, and PCNN+ATT+SL, which
adopt sophisticated denoising schemes and extra
information, our model is still more advantageous.
This indicates that our method can take advantage
of the rich correlations between relations through
KGs and GCNs, which improve the performance.
We believe the performance of our model can be
further improved by adopting additional mecha-
nisms like adversarial training, and reinforcement
learning, which will be part of our future work.

4.4 Evaluation Results for Long-tail
Relations

To further demonstrate the improvements in per-
formance for long-tail relations, following the
study by (Han et al., 2018b) we extract a subset
of the test dataset in which all the relations have

Training Instances
Hits@K (Macro)

<100 <200
10 15 20 10 15 20

+KATT 35.3 62.4 65.1 43.2 61.3 69.2
w/o hier 34.2 62.1 65.1 42.5 60.2 68.1

w/o GCNs 30.5 61.9 63.1 39.5 58.4 66.1
Word2vec 30.2 62.0 62.5 39.6 57.5 65.8
w/o KG 30.0 61.0 61.3 39.5 56.5 62.5

Table 2: Results of ablation study with PCNN.

fewer than 100/200 training instances. We employ
the Hits@K metric for evaluation. For each en-
tity pair, the evaluation requires its corresponding
golden relation in the first K candidate relations
recommended by the models. Because it is diffi-
cult for the existing models to extract long-tail re-
lations, we select K from {10,15,20}. We report
the macro average Hits@K accuracies for these
subsets because the micro-average score generally
overlooks the influences of long-tails. From the
results shown in Table 1, we observe that for both
CNN and PCNN models, our model outperforms
the plain attention model and the HATT model.
Although our KATT method has achieved better
results for long-tail relations as compared to both
plain ATT method and HATT method, the results
of all these methods are still far from satisfactory.
This indicates that distantly supervised RE mod-
els still suffer from the long-tail relation problem,
which may require additional schemes and extra
information to solve this problem in the future.

4.5 Ablation Study

To analyze the contributions and effects of differ-
ent technologies in our approach, we perform ab-
lation tests. +KATT is our method; w/o hier is
the method without coarse-to-fine attention (only
utilizes bottom node embeddings of the hierarchy
label graph), which implies no knowledge trans-
fer from its higher level classes; w/o GCN is the
method without GCNs, which implies no explicit
relational knowledge; Word2vec is the method in
which the node is initialized with pretrained Skip-
Gram (Mikolov et al., 2013) embeddings; and w/o
KG is the method in which the node is initial-
ized with random embeddings, which implies no
prior relational knowledge from KGs. From the
evaluation results in Table 2, we observe that the
performance slightly degraded without coarse-to-
fine attention, which proves that knowledge trans-
fer from the higher node is useful. We also no-
ticed that the performance slightly degraded with-
out KG or using word embeddings, and the perfor-



3023

(a) HATT (b) +KG (c) +GCNs (d) KATT

Figure 6: T-SNE visualizations of class embeddings. Cluster in the upper right is the relation /location/*/* and
cluster in the bottom left is the relation /people/*/* ). The square, triangle, and star refer to the high (/location)
middle (/location/location/) and base (/location/location/contains) level relations, respectively.

mance significantly degraded when we removed
GCNs. This is reasonable because GCNs can learn
more explicit correlations between relation labels,
which boost the performance for long-tail rela-
tions.

/people/deceased person/place of burial HATT KATT
richard wagner had his bayreuth, with
its festspielhaus specially designed to ac-
commodate his music dramas.

0.21 0.07

wotan and alberich are richard wagner;
and the rheingold and valhalla are wag-
ner’s real-life grail, the opera house in
bayreuth.

0.15 0.13

/location/br state/capital HATT KATT
there’s violence everywhere, said ms.
mesquita, who, like her friend, lives
in belo horizonte, the capital of mi-
nas gerais state

0.47 0.51

all the research indicates that we are cer-
tain to find more gas in th amazon, ed-
uardo braga, the governor of amazonas,
said in an interview in manaus

0.46 0.45

Table 3: Example sentences for case study.

4.6 Case Study

We give some examples to show how our
method affects the selection of sentences. In
Table 3, we display the sentence’s attention
score in the lowest level5. Both the relation
/people/deceased person/place of burial (24 in-
stances) and /location/br state/capital (4 instances)
are long-tail relations. On one hand, relation /peo-
ple/deceased person/place of burial has semanti-
cally similar data-rich relation such as /peo-
ple/deceased person/place of death. We observe
that HATT erroneously assigns high attention to
the incorrect sentence whereas KATT successfully
assigns the right attention weights, which demon-

5Both HATT and KATT methods can successfully select
the correct sentence at the higher-level; details can be found
in supplementary materials.

strates the efficacy of knowledge transfer from
semantically similar relations (Both HATT and
KATT methods can take advantage of knowledge
transfer of high-level relations.). On the other
hand, the relation /location/br state/capital does
not have semantically similar relations. However,
we notice that KATT still successfully assigns the
right attention weights, which demonstrates the ef-
ficacy of knowledge transfer from high-level rela-
tions using coarse-to-fine knowledge-aware atten-
tion.

4.7 Visualizations of Class Embeddings

We visualize the class embeddings via t-SNE
(Maaten and Hinton, 2008) to further show how
GCNs and KG embeddings can help RE for long-
tail relations. We observe that (1) Figure 6(a)
and 6(d) show that semantically similar class em-
beddings are closer with GCNs and pretrained
KG embeddings, which help select long-tail in-
stances. (2) Figure 6(b) and 6(c) show that
KG embeddings and GCNs have different con-
tributions for different relations to learn rela-
tional knowledge between classes. For example,
/location/location/contain has a sparse hierarchy
structure, which leads to inefficient learning for
GCNs; therefore, the relative distance changes
only slightly, which reveals the necessity of im-
plicit relational knowledge from KGs. (3) Figure
6(d) shows that there are still some semantically
similar class embeddings located far away, which
may degrade the performance for long-tails. This
may be caused by either sparsity in the hierarchy
label graph or equal treatment for nodes with the
same parent in GCNs, which is not a reasonable
hypothesis. We will address this by integrating
more information such as relation descriptions or
combing logic reasoning as a part of future work.



3024

5 Conclusion and Future Work

In this paper, we take advantage of the knowledge
from data-rich classes at the head of distribution
to boost the performance of the data-poor classes
at the tail. As compared to previous methods, our
approach provides fine-grained relational knowl-
edge among classes using KG and GCNs, which
is quite effective and encoder-agnostic.

In the future, we plan to explore the follow-
ing directions: (1) We may combine our method
with recent denoising methods to further improve
performance. (2) We may combine rule mining
and reasoning technologies to learn better class
embeddings to boost performance. (3) It will be
promising to apply our method to zero-shot RE
and further adapt to other NLP scenarios.

Acknowledgments

We want to express gratitude to the anony-
mous reviewers for their hard work and kind
comments, which will further improve our
work in the future. This work is funded
by NSFC91846204/61473260, national key re-
search program YS2018YFB140004, Alibaba
CangJingGe (Knowledge Engine) Research Plan
and Natural Science Foundation of Zhejiang
Province of China (LQ19F030001).

References
Joost Bastings, Ivan Titov, Wilker Aziz, Diego

Marcheggiani, and Khalil Sima’an. 2017. Graph
convolutional encoders for syntax-aware neural ma-
chine translation. arXiv preprint arXiv:1704.04675.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Proceedings of NIPS, pages
2787–2795.

Meihao Chen, Zhuoru Lin, and Kyunghyun Cho.
2017. Graph convolutional networks for classifica-
tion with a structured label space. arXiv preprint
arXiv:1710.04908.

Michaël Defferrard, Xavier Bresson, and Pierre Van-
dergheynst. 2016. Convolutional neural networks on
graphs with fast localized spectral filtering. In Ad-
vances in Neural Information Processing Systems,
pages 3844–3852.

Jun Feng, Minlie Huang, Li Zhao, Yang Yang, and Xi-
aoyan Zhu. 2018. Reinforcement learning for rela-
tion classification from noisy data. In Proceedings
of AAAI.

Yaocheng Gui, Qian Liu, Man Zhu, and Zhiqiang
Gao. 2016. Exploring long tail data in distantly su-
pervised relation extraction. In Natural Language
Understanding and Intelligent Applications, pages
514–522. Springer.

Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd annual meeting
on association for computational linguistics, pages
427–434. Association for Computational Linguis-
tics.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2018a. Neu-
ral knowledge acquisition via mutual attention be-
tween knowledge graph and text.

Xu Han, Pengfei Yu, Zhiyuan Liu, Maosong Sun, and
Peng Li. 2018b. Hierarchical relation extraction
with coarse-to-fine grained attention. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pages 2236–2245.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In Proceedings of ACL,
pages 541–550. Association for Computational Lin-
guistics.

Yi Yao Huang and William Yang Wang. 2017. Deep
residual learning for weakly-supervised relation ex-
traction. arXiv preprint arXiv:1707.08866.

Guoliang Ji, Kang Liu, Shizhu He, Jun Zhao, et al.
2017. Distant supervision for relation extraction
with sentence-level attention and entity descriptions.
In Proceedings of AAAI, pages 3060–3066.

Stephen C Johnson. 1967. Hierarchical clustering
schemes. Psychometrika, 32(3):241–254.

Thomas N Kipf and Max Welling. 2016. Semi-
supervised classification with graph convolutional
networks. arXiv preprint arXiv:1609.02907.

Kai Lei, Daoyuan Chen, Yaliang Li, Nan Du, Min
Yang, Wei Fan, and Ying Shen. 2018. Coopera-
tive denoising for distantly supervised relation ex-
traction. In Proceedings of Coling, pages 426–436.

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015. Learning entity and relation em-
beddings for knowledge graph completion. In AAAI,
volume 15, pages 2181–2187.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. In Proceed-
ings of ACL, volume 1, pages 2124–2133.

Tianyu Liu, Kexiang Wang, Baobao Chang, and Zhi-
fang Sui. 2017. A soft-label method for noise-
tolerant distantly supervised relation extraction. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
1790–1795.



3025

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of machine
learning research, 9(Nov):2579–2605.

Diego Marcheggiani and Ivan Titov. 2017. En-
coding sentences with graph convolutional net-
works for semantic role labeling. arXiv preprint
arXiv:1703.04826.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003–1011. Association for
Computational Linguistics.

Raymond J Mooney and Razvan C Bunescu. 2006.
Subsequence kernels for relation extraction. In Ad-
vances in neural information processing systems,
pages 171–178.

Vinod Nair and Geoffrey E Hinton. 2010. Rectified
linear units improve restricted boltzmann machines.
In Proceedings of ICML, pages 807–814.

Pengda Qin, Weiran Xu, and William Yang Wang.
2018. Dsgan: Generative adversarial training for
distant supervision relation extraction. In Proceed-
ings of ACL.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Joint European Conference
on Machine Learning and Knowledge Discovery in
Databases, pages 148–163. Springer.

Anthony Rios and Ramakanth Kavuluru. 2018. Few-
shot and zero-shot multi-label learning for struc-
tured label spaces. In Proceedings of EMNLP, pages
3132–3142.

Michael Schlichtkrull, Thomas N Kipf, Peter Bloem,
Rianne van den Berg, Ivan Titov, and Max Welling.
2018. Modeling relational data with graph convolu-
tional networks. In European Semantic Web Confer-
ence, pages 593–607. Springer.

Emma Strubell and Andrew McCallum. 2017. De-
pendency parsing with dilated iterated graph cnns.
arXiv preprint arXiv:1705.00403.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of EMNLP, pages 455–465. Association
for Computational Linguistics.

Guanying Wang, Wen Zhang, Ruoxu Wang, Yalin
Zhou, Xi Chen, Wei Zhang, Hai Zhu, and Huajun
Chen. 2018. Label-free distant supervision for rela-
tion extraction via knowledge graph embedding. In
Proceedings of EMNLP, pages 2246–2255.

Yu-Xiong Wang, Deva Ramanan, and Martial Hebert.
2017. Learning to model the tail. In Proceedings of
NIPS, pages 7029–7039.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph embedding by trans-
lating on hyperplanes. In AAAI, volume 14, pages
1112–1119.

Yi Wu, David Bamman, and Stuart Russell. 2017. Ad-
versarial training for relation extraction. In Proceed-
ings of EMNLP, pages 1778–1783.

Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. 2015. Embedding entities and
relations for learning and inference in knowledge
bases. Proceedings of ICLR.

Hai Ye, Wenhan Chao, Zhunchen Luo, and Zhoujun
Li. 2017. Jointly extracting relations with class ties
via effective deep ranking. In Proceedings of ACL,
volume 1, pages 1810–1820.

Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of machine learning research,
3(Feb):1083–1106.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
piecewise convolutional neural networks. In Pro-
ceedings of EMNLP, pages 1753–1762.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via con-
volutional deep neural network. In Proceedings of
COLING, pages 2335–2344.

Wenyuan Zeng, Yankai Lin, Zhiyuan Liu, and
Maosong Sun. 2017. Incorporating relation paths
in neural relation extraction. In Proceddings of
EMNLP.

Xiangrong Zeng, Shizhu He, Kang Liu, and Jun Zhao.
2018. Large scaled relation extraction with rein-
forcement learning. In Processings of AAAI, vol-
ume 2, page 3.

Dongxu Zhang and Dong Wang. 2015. Relation classi-
fication via recurrent neural network. arXiv preprint
arXiv:1508.01006.

Ningyu Zhang, Shumin Deng, Zhanling Sun, Xi Chen,
Wei Zhang, and Huajun Chen. 2018a. Attention-
based capsule networks with dynamic routing for re-
lation extraction. In Proceedings of EMNLP.

Zhao Zhang, Fuzhen Zhuang, Meng Qu, Fen Lin, and
Qing He. 2018b. Knowledge graph embedding with
hierarchical relation structure. In Proceedings of
EMNLP, pages 3198–3207.


