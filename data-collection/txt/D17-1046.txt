



















































Why ADAGRAD Fails for Online Topic Modeling


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 446–451
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Why ADAGRAD Fails for Online Topic Modeling

You Lu
Computer Science (CS)

University of Colorado Boulder
Boulder, CO

you.lu@colorado.edu

Jeffrey Lund
Computer Science (CS)

Brigham Young University
Provo, UT

jefflund@byu.edu

Jordan Boyd-Graber
CS, iSchool, LSC, and UMIACS

University of Maryland
College Park, MD

jbg@umiacs.umd.edu

Abstract

Online topic modeling, i.e., topic modeling
with stochastic variational inference, is a
powerful and efficient technique for ana-
lyzing large datasets, and ADAGRAD is a
widely-used technique for tuning learning
rates during online gradient optimization.
However, these two techniques do not work
well together. We show that this is because
ADAGRAD uses accumulation of previous
gradients as the learning rates’ denomina-
tors. For online topic modeling, the mag-
nitude of gradients is very large. It causes
learning rates to shrink very quickly, so the
parameters cannot fully converge until the
training ends.

Probabilistic topic models (Blei, 2012) are pop-
ular algorithms for uncovering hidden thematic
structure in text. They have been widely used
to help people understand and navigate document
collections (Blei et al., 2003), multilingual collec-
tions (Hu et al., 2014), images (Chong et al., 2009),
networks (Chang and Blei, 2009; Yang et al., 2016),
etc. Probabilistic topic modeling usually requires
computing a posterior distribution over thousands
or millions of latent variables, which is often in-
tractable. Variational inference (Blei et al., 2016,
VI) approximates posterior distributions. Stochas-
tic variational inference (Hoffman et al., 2013, SVI)
is its natural online extension and enables the anal-
ysis of large datasets.

Online topic models (Hoffman et al., 2010;
Bryant and Sudderth, 2012; Paisley et al., 2015)
optimize the global parameters of interest using
stochastic gradient ascent. At each iteration, they
sample data points to estimate the gradient. In
practice, the sample has only a small percentage
of the vocabulary. The resulting sparse gradients

hurt performance. ADAGRAD (Duchi et al., 2011)
is designed for high dimensional online optimiza-
tion problems and adjusts learning rates for each
dimension, favoring rare features. This makes
ADAGRAD well-suited for tasks with sparse gra-
dients such as distributed deep networks (Dean
et al., 2012), forward-backward splitting (Duchi
and Singer, 2009), and regularized dual averaging
methods (Xiao, 2010).

Thus, it may seem reasonable to apply ADA-
GRAD to optimize online topic models. However,
ADAGRAD is not suitable for online topic models
(Section 1). This is because to get a topic model,
the training algorithm must break the symmetry be-
tween parameters of words that are highly related
to the topic and words that are not related to the
topic. Before the algorithm converges, the magni-
tude of gradients of the parameters are very large.
Since ADAGRAD uses the accumulation of previous
gradients as learning rates’ denominators, the learn-
ing rates shrink very quickly. Thus, the algorithm
cannot break the symmetry quickly. We provide
solutions for this problem. Two alternative learning
rate methods, i.e., ADADELTA (Zeiler, 2012) and
ADAM (Kingma and Ba, 2014), can address this
incompatibility with online topic models. When
the dataset is small enough, e.g., a corpus with only
hundreds of documents, ADAGRAD can still work.

1 Buridan’s Optimizer

Latent Dirichlet allocation (Blei et al., 2003, LDA)
is perhaps the most well known topic model. In
this section, we analyze problems with ADAGRAD
for online LDA (Hoffman et al., 2010), and provide
some solutions. Our analysis is easy to generalize
to other online topic models, e.g., online Hierarchi-
cal Dirichlet Process (Wang et al., 2011, HDP).

446



Iteration 0

Iteration 5000

Iteration 10000

Iteration 20000

Adagrad Constant

λ of top wordsλ of bottom words

λ scaled up

λ scaled down

Figure 1: Illustration of ADAGRAD’s problem. Ini-
tially, the topic does not favor particular words
over others, so the training algorithm incorrectly
increases the parameters of bottom words. Then,
ADAGRAD learning rates decrease too quickly, leav-
ing the tie between top and bottom unbroken. Thus,
the algorithm fails to form appropriate topics. A
constant rate easily breaks the tie. When the tie is
broken, the algorithm decreases the parameters of
bottom words and increases the parameters of top
words until convergence.

1.1 Online LDA
To train LDA, we want to compute the posterior

p(β, θ,z |w,α, η) ∝
K∏

k=1

p(βk | η)·

D∏
d=1

p(θd |α)
Nd∏
n=1

p(zdn | θd)p(wdn |βzdn),

where βk is the topic-word distribution for the kth

of K topics, θd is the document-topic distribution
for the dth of D document, zdn is the topic assign-
ment for the nth ofNd words in in the dth document,
wdn is the word type of the nth word in the dth doc-
ument, with α and η the Dirichlet priors over the
document-topic and topic-word distributions.

However, this is intractable. Stochastic varia-
tional inference (SVI) is a popular approach for
approximation. It first posits a mean field varia-
tional distribution

q(β, θ, z |λ, γ, φ) =
K∏

k=1

q(βk |λk)·

D∏
d=1

q(θd | γd)
Nd∏
n=1

q(zdn |φdn),

where γ (Dirichlet) and φ (multinomial) are local
parameters and λ (Dirichlet) is a global parame-
ter. SVI then optimizes the variational parameters
to minimize the KL divergence between the varia-
tional distribution and the true posterior.

At iteration t, SVI samples a document d from
the corpus and updates the local parameters:

φdvk ∝ exp
{

Ψ (γdk) + Ψ
(
λ

(t)
kv

)
−Ψ

(∑
i
λ

(t)
ki

)}
,

(1)

γ
(t)
k = α+

∑
v

nvφ
d
vk, (2)

where nv is the number of words v in d, and Ψ (.) is
the digamma function. After finding φd and γd, SVI
optimizes the global parameters using stochastic
gradient ascent,

λ
(t+1)
kv = (1− ρ(t)kv )λ(t)kv + ρ(t)kv (η +Dφdvkndv)

= (1− ρ(t)kv )λ(t)kv + ρ(t)kv λ̂(t)kv
= λ(t)kv + ρ

(t)
kvg

(t)
kv , (3)

where ρ(t) is the learning rate, λ̂(t)kv = η+Dφ
d
vkndv

is the intermediate parameter and g(t)kv = −λ(t)kv +
λ̂

(t)
kv is the gradient.

1.2 ADAGRAD for Online LDA
In general, ρ(t)kv = κ

(t), for all v ∈ 1, .., V and
k ∈ 1, ...,K, where κ(t) can be a decreasing
rate (Hoffman et al., 2013), a small constant (Col-
lobert et al., 2011) or an adaptive rate (Ranganath
et al., 2013). These three methods are all global
learning rate methods, which cannot adaptively ad-
just learning rate for each dimension of the pa-
rameter, or address the problems caused by sparse
gradients.

ADAGRAD is a popular learning rate method de-
signed for online optimization problems with high
dimension and sparse gradients. Thus, it seems
reasonable to apply ADAGRAD to update learning
rates for online topic models. When using ADA-
GRAD (Duchi et al., 2011) with online LDA, the
update rule for the each learning rate is

ρ
(t)
kv =

ρ0√
�+

∑t
i=0

(
g
(i)
kv

)2 , (4)
where ρ0 is a constant, and a very small � guaran-
tees that the learning rates are non-zero.

447



1.3 ADAGRAD’s Indecision

A philosophical thought experiment provides us
with the story of Buridan’s ass (Bayle, 1826): sit-
uated between two piles of equally tasty hay, the
poor animal starved to death. ADAGRAD faces
a similar problem in breaking the symmetries of
common variational inference initializations. For
convenience, we unfold an example with a single
document at each iteration. Our analysis general-
izes to mini-batches.

Initially, the topics β1:K do not favor particular
words over others as inference cannot know a pri-
ori which words will have high probability in a
particular topic. The algorithm must break ties be-
tween parameters of the top and bottom words in a
topic. Unfortunately, the momentum of ADAGRAD
fails for topic models. We now explain why this is.

ADAGRAD looks to the gradient for clues about
what features will be important. This is because
before the equilibrium is broken, the values of dif-
ferent λkv are close, so Equation 1 will be approx-
imately seen as φdvk ∝ exp {Ψ (γdk)}, which im-
plicates that λ has very small influence on the op-
timization of φ. If some topics are prevalent in
the sampled document d, large probability will be
assigned to the corresponding φ.k, meaning that
all words in document d are treated as top words.
The initial clues are at best random and at words
counter productive.

However, ADAGRAD uses these cues to prefer
some dimensions over others. Let λ∗ be the opti-
mum; the topic ADAGRAD should find at conver-
gence: λ∗kv ≈ E

[
λ̂

(t)
kv

]
. By definition, once the

algorithm converges, λ∗kv for top words will have
very large values while λ∗kv for bottom words will
be small. After using noisy momentum terms, it
must overcome initial faulty signals.

We now show the lower and upper bounds of
E
[
λ̂

(t)
kv

]
to show how big of an uphill battle ADA-

GRAD faces. Expanding the update rule,

E
[
λ̂

(t)
kv

]
= E

[
η +Dφdvkndv

]
= η +Dn̄vE [φvk] ,

where n̄v =
∑D

i=1 niv/D, and φvk is the probabil-
ity that word v is assigned to topic k. For a bottom
word, φvk → 0. For a top word, φvk ≥ 1/K. After
convergence, for a bottom word E [φvk] ≈ η. For
a top word, 1/K ≤ E [φvk] ≤ 1. Thus, the lower

and upper bounds of E
[
λ̂

(t)
kv

]
are

η + (1/K)Dn̄v ≤ E
[
λ̂

(t)
kv

]
≤ η +Dn̄v.

For a large datasets, Dn̄v should be large. Thus for
top words, λ∗kv will converge to a large value: quite
a large hill to climb.

How quickly the algorithm climbs the hill is
inversely proportional to the gradient size. We next
show that the magnitude of gradients of top words
are very large before the algorithm converges. Let
g∗ be the gradient after convergence. We show the
bounds of |gkv|, where |.| is the absolute value, in
the following:

| g∗kv | = | − λ∗kv + η +Dφdvkndv |
≈ | − η −Dn̄vE[φvk] + η +Dφdvkndv |
≈ E [φvk] ∗D |ndv − n̄v | .

Thus,

(D/K) |ndv − n̄v | ≤ | g∗kv | ≤ D |ndv − n̄v | .
Only when ndv = n̄v, does | g(t)kv | = 0. Otherwise,
due to the large D, | g∗kv | will be large. However,
in practice, ndv varies largely from document to
document, which leads to large values of | g∗kv | .
Based on the gradient’s property, when λkv is far
away from the optimum, | g(t)kv | ≥ | g∗kv | . Thus,
the values of | g(t)kv | for the top words are very large
before convergence.

ADAGRAD uses the accumulations of previous
gradients as learning rates’ denominators. Because
of these large gradients in the first several iterations,
learning rates soon decrease to small values; even if
a topic has gathered a few words, ADAGRAD lacks
the momentum to move other words into the topic.
These small learning rates slows the updates of λ.

In sum, the initial gradient signals confuse the
algorithm, the gradients are large enough to impede
progress later, and large datasets imply a very large
hill the algorithm must climb. Since the update pro-
gresses slowly, online LDA needs more iterations
to break the equilibrium. Because the gradients
of all words are still very large, the learning rates
decrease quickly, which makes the update progress
slower. When the update progresses more slowly,
online LDA needs more iterations to break the tie.
This cycle repeats, until some learning rates de-
crease to zero and learning effectively stops. Thus,
the algorithm will never break the tie or infer good
topics. Figure 1 illustrates the problem of online
LDA with ADAGRAD.

448



1.4 Alternative Solutions
ADADELTA (Zeiler, 2012) and ADAM (Kingma and
Ba, 2014) are extensions to ADAGRAD. ADADELTA
does not have guaranteed convergence on con-
vex optimization problems. Even though ADAM
has a theoretical bound on its convergence rate,
it is controlled by and sensitive to several learn-
ing rate parameters. For good performance with
ADAM, manual adjustment is necessary. In addi-
tion, since ADADELTA computes the moving aver-
age of updates, and ADAM needs to compute the
bias-corrected gradient estimate, they require more
intricate implementations. Consequently, these two
methods are not as popular as ADAGRAD for begin-
ners. However, for SVI latent variable models, they
can address the problems with ADAGRAD.

ADADELTA updates the learning rates with the
following rule:

ρ
(t)
kv =

√
E
[
(λ(t)kv − λ(t−1)kv )

]
+ ε√

E
[
g
(t)
kv

]
+ ε

, (5)

where E
[
x(t)
]

= ρ0E
[
x(t−1)

]
+ (1− ρ0)(x(t))2,

ρ0 is a decay constant, and ε is for numerical sta-
bility.

ADAM’s update rule is determined based on esti-
mates of first and second moments of the gradients:

m
(t)
kv = bmm

(t−1)
kv + (1− bm)g(t)kv ,

u
(t)
kv = buu

(t−1)
kv + (1− bu)(g(t)kv )2,

m̂
(t)
kv =

m
(t)
kv

1− btm
, û

(t)
kv =

u
(t)
kv

1− btu
,

λ
(t+1)
kv = λ

(t)
kv + ρ0m̂

(t)
kv/(

√
û

(t)
kv + ε), (6)

where ρ0 is a constant, b controls the decay rate.
Both ADADELTA and ADAM use the moving av-

erage of gradients as the denominator of learn-
ing rates. The learning rates will not monotoni-
cally decrease, but vary in a certain range. This
property prevents online topic models from being
trapped and breaks the tie between top words and
bottom topic words. ADAM in particular uses bias-
corrected estimate of gradient m̂kv, rather than the
original stochastic gradient gkv to guide direction
for the optimization and therefore achieves better
results.

In addition, the magnitude of gradients is propor-
tional to the dataset’s size. Thus, when the dataset
is small enough, ADAGRAD will still work.

●
●

●

●
●

●

0.0

0.5

1.0

1.5

2.0

3E2 5E2 1E3 1E4 1E5 1E6

E
rr

o
r 

R
at

e

Adagrad Error Rate

● ● ● ● ● ●0.0

0.5

1.0

1.5

2.0

3E2 5E2 1E3 1E4 1E5 1E6

E
rr

o
r 

R
at

e

Constant Rate Error Rate

●

●

● ● ● ●0.0

0.5

1.0

1.5

2.0

3E2 5E2 1E3 1E4 1E5 1E6
Data Set Size

E
rr

o
r 

R
at

e

Adadelta Error Rate

● ● ● ● ● ●0.0

0.5

1.0

1.5

2.0

3E2 5E2 1E3 1E4 1E5 1E6
Data Set Size

E
rr

o
r 

R
at

e

Adam Error Rate

Vocabulary Size ● V=10 V=100 V=1000 V=2 V=5000

Figure 2: Experimental results on synthetic data
sets. We vary the vocabulary size V , and the num-
ber of documents D. ADADELTA, ADAM and con-
stant rate perform better with more data, while
ADAGRAD only does well with small values of D.

2 Empirical Study

We study three datasets: synthetic data,
Wikipedia and SMS spam corpus.1 We use
the generative process of LDA to generate syn-
thetic data. We vary the vocabulary size V ∈
{2, 10, 100, 1000, 5000}, and the number of doc-
uments D ∈ {300, 500, 103, 104, 105, 106}. The
Wikipedia dataset consists of 1M articles collected
from Wikipedia.2 The vocabulary is the same
as (Hoffman et al., 2010). The SMS corpus is a
small corpus containing 1084 documents.

2.1 Metrics and Settings

Error rate: For experiments on synthetic data
set, we use error rate

Error(β̂) =
1
K

∑K
k=1

mini||β̂i − βk||1 (7)

to measure the difference between the estimated β̂
and the known β. The min greedily matches each
β̂k to its best fit. While an uncommon metric for
unsupervised algorithms, on the synthetic data we
have the true β.

1http://www.esp.uem.es/jmgomez/smsspamcorpus/
2http://www.wikipedia.org

449



●

●
● ●

●

●

●
● ● ●

Online HDP Online LDA

W
ikip

ed
ia

0 2500 5000 7500 10000 0 2500 5000 7500 10000

−8.4

−8.0

−7.6

−7.2
P

re
d

ic
ti

ve
 L

ik
el

ih
o

o
d

●

● ● ● ●

●

● ● ● ●

Online HDP Online LDA

S
M

S

0 1000 2000 3000 4000 0 1000 2000 3000 4000

−7.5

−7.0

−6.5

−6.0

−5.5

Number of Iterations

P
re

d
ic

ti
ve

 L
ik

el
ih

o
o

d

LearningRate ● Adadelta Adagrad Adam Constant

Figure 3: Experimental results on real cor-
pora. Larger predictive likelihood is better. On
Wikipedia, ADAGRAD has does worse than other
methods. On SMS corpus, ADAGRAD is competi-
tive.

Predictive likelihood: For experiments on real
data sets, we use per-word likelihood (Hoffman
et al., 2013) to evaluate the model quality. We ran-
domly hold out 10K documents and 100 documents
on Wikipedia and SMS respectively.

Settings: In the experiments on synthetic data,
we use online LDA (Hoffman et al., 2010), since
the data is generated by LDA. In the experiments
on real datasets, we use online LDA and online
HDP (Wang et al., 2011). In the experiments on
Wikipedia, we set the number of topics K = 100
and the mini-batch size M = 100. In the experi-
ments on SMS corpus, we setK = 10 andM = 20.
For ADAM, we use the default setting of b, and set
ρ0 = 10 and � = 1000. For ADADELTA, we set
� = 1000. For ADAGRAD, we set ρ0 = � = 1.
These are best settings for these three methods.
The best constant rate is 10−3.

2.2 Experimental Results

Figure 2 illustrates the experimental results on syn-
thetic datasets. ADAGRAD only works well with
small datasets. When the number of documents
increases, ADAGRAD performance degrades. Con-
versely, other methods can handle more documents.

Figure 3 illustrates experimental results on real
corpora. ADAGRAD gets competitive results to the

other algorithms on the small SMS corpus. How-
ever on very large Wikipedia corpus, ADAGRAD
fails to infer good topics, and its predictive ability
is worse than the other methods. While ADADELTA
and ADAM work well on Wikipedia, ADAM is the
clear winner between the two.

3 Conclusion

ADAGRAD is a simple and popular technique for
online learning, but is not compatible with tradi-
tional initializations and objective functions for
online topic models. We show that practitioners are
best off using simpler online learning techniques or
ADADELTA and ADAM, which are two variants of
ADAGRAD, which use the moving average of gra-
dients as denominator. These two methods avoid
ADAGRAD’s problem. In particular, ADAM per-
forms much better for prediction.

We would like to build a deeper understanding of
which aspects of an unsupervised objective, near-
uniform initialization, and non-identifiability con-
tribute to these issues and to discover other learning
problems that may share these issues.

Acknowledgments

We thank the anonymous reviewers, Stephan
Mandt, Alp Kucukelbir, Bill Foland, Forough
Poursabzi-Sangdeh and Alvin Grissom II for their
insightful comments. Boyd-Graber and Lu’s contri-
bution is supported by NSF grants NCSE-1422492
and IIS-1409287, (UMD). Boyd-Graber is also sup-
ported by IIS-1564275 and IIS-1652666. Lund is
supported by collaborative NSF Grant IIS-1409739
(BYU). Any opinions, findings, results, or recom-
mendations expressed here are of the authors and
do not necessarily reflect the view of the sponsor.

450



References
Pierre Bayle. 1826. An historical and critical dictio-

nary, selected and abridged. Number 1 in An his-
torical and critical dictionary, selected and abridged.
https://books.google.com/books?id=cDsN3xOyO-
oC.

David M Blei. 2012. Probabilistic topic models. Com-
munications of the ACM 55(4):77–84.

David M Blei, Alp Kucukelbir, and Jon D McAuliffe.
2016. Variational inference: A review for statisti-
cians. arXiv preprint arXiv:1601.00670 .

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research 3:993–1022.

Michael Bryant and Erik B Sudderth. 2012. Truly non-
parametric online variational inference for hierarchi-
cal Dirichlet processes. In Proceedings of Advances
in Neural Information Processing Systems.

Jonathan Chang and David M Blei. 2009. Relational
topic models for document networks. In Proceed-
ings of Artificial Intelligence and Statistics. vol-
ume 9, pages 81–88.

Wang Chong, David Blei, and Fei-Fei Li. 2009. Si-
multaneous image classification and annotation. In
Computer Vision and Pattern Recognition. IEEE.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research 12:2493–
2537.

Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen,
Matthieu Devin, Mark Mao, Andrew Senior, Paul
Tucker, Ke Yang, Quoc V Le, et al. 2012. Large
scale distributed deep networks. In Proceedings
of Advances in Neural Information Processing Sys-
tems.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research 12:2121–2159.

John Duchi and Yoram Singer. 2009. Efficient online
and batch learning using forward backward splitting.
Journal of Machine Learning Research 10:2899–
2934.

Matthew Hoffman, Francis R Bach, and David M Blei.
2010. Online learning for latent Dirichlet allocation.
In Proceedings of Advances in Neural Information
Processing Systems.

Matthew D Hoffman, David M Blei, Chong Wang, and
John William Paisley. 2013. Stochastic variational
inference. Journal of Machine Learning Research
14:1303–1347.

Yuening Hu, Ke Zhai, Vlad Eidelman, and Jordan
Boyd-Graber. 2014. Polylingual tree-based topic
models for translation domain adaptation. In Pro-
ceedings of the Association for Computational Lin-
guistics.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

John Paisley, Chong Wang, David M Blei, and
Michael I Jordan. 2015. Nested hierarchical Dirich-
let processes. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence 37:256–270.

Rajesh Ranganath, Chong Wang, David M Blei, and
Eric P Xing. 2013. An adaptive learning rate for
stochastic variational inference. In Proceedings of
the International Conference of Machine Learning.

Chong Wang, John William Paisley, and David M Blei.
2011. Online variational inference for the hierarchi-
cal Dirichlet process. In Proceedings of Artificial
Intelligence and Statistics.

Lin Xiao. 2010. Dual averaging methods for regu-
larized stochastic learning and online optimization.
Journal of Machine Learning Research 11:2543–
2596.

Weiwei Yang, Jordan Boyd-Graber, and Philip Resnik.
2016. A discriminative topic model using docu-
ment network structure. In Association for Compu-
tational Linguistics.

Matthew D Zeiler. 2012. ADADELTA: an adap-
tive learning rate method. arXiv preprint arXiv:
1212.5701 .

451


