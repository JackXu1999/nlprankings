



















































Speculative Beam Search for Simultaneous Translation


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 1395–1402,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

1395

Speculative Beam Search for Simultaneous Translation

Renjie Zheng 2,∗ Mingbo Ma 1,∗ Baigong Zheng 1 Liang Huang 1,2
1Baidu Research, Sunnyvale, CA, USA

2Oregon State University, Corvallis, OR, USA
zrenj11@gmail.com

{mingboma, baigongzheng, lianghuang}@baidu.com

Abstract

Beam search is universally used in full-
sentence translation but its application to
simultaneous translation remains non-trivial,
where output words are committed on the fly.
In particular, the recently proposed wait-k pol-
icy (Ma et al., 2019a) is a simple and effec-
tive method that (after an initial wait) com-
mits one output word on receiving each input
word, making beam search seemingly impos-
sible. To address this challenge, we propose
a speculative beam search algorithm that hal-
lucinates several steps into the future in order
to reach a more accurate decision, implicitly
benefiting from a target language model. This
makes beam search applicable for the first time
to the generation of a single word in each step.
Experiments over diverse language pairs show
large improvements over previous work.

1 Introduction

Beam search has been widely used in neural text
generation such as machine translation (Sutskever
et al., 2014; Bahdanau et al., 2014), summariza-
tion (Rush et al., 2015; Ranzato et al., 2016), and
image captioning (Vinyals et al., 2015; Xu et al.,
2015). It often leads to substantial improvement
over greedy search and becomes an essential com-
ponent in almost all text generation systems.

However, beam search is easy for the above
tasks because they are all full-sequence problems,
where the whole input sequence is available at the
beginning and the output sequence only needs to
be revealed in full at the end. By contrast, in lan-
guage and speech processing, there are many in-
cremental processing tasks with simultaneity re-
quirements, where the output needs to be revealed
to the user incrementally without revision (word
by word, or in chunks) and the input is also being

∗These authors contributed equally.

received incrementally. Two most salient exam-
ples are streaming speech recognition (Chiu et al.,
2018), widely used in speech input and dialog sys-
tems (such as Siri), and simultaneous translation
(Bangalore et al., 2012; Oda et al., 2015; Gris-
som II et al., 2014; Jaitly et al., 2016), widely used
in international conferences and negotiations. In
these tasks, the use of full-sentence beam search
becomes seemingly impossible as output words
need to be committed on the fly.

How to adapt beam search for such incremen-
tal tasks in order to improve their generation qual-
ity? We propose a general technique of specula-
tive beam search (SBS), and apply it to simulta-
neous translation. At a very high level, to gener-
ate a single word, instead of simply choosing the
highest-scoring one (as in greedy search), we fur-
ther speculate w steps into the future, and use the
ranking at step w+1 to reach a more informed
decision for step 1 (the current step); this method
implicitly benefits from a target language model,
alleviating the label bias problem in neural genera-
tion (Murray and Chiang, 2018; Ma et al., 2019b).

We apply this algorithm to two representative
approaches to simultaneous translation: the fixed
policy method (Ma et al., 2019a) and the adaptive
policy method (Gu et al., 2017). In both cases, we
show that SBS improves translation quality while
maintaining latency (i.e., simultaneity).

2 Preliminaries

We first review standard full-sentence NMT and
beam search to set up the notations, and then re-
view different approaches to simultaneous MT.

2.1 Full Sentence NMT and Beam Search
The encoder processes the input sequence x =
(x1, ..., xn), where xi ∈ Rd represents an input
token as a d dimensional vector, and produces a
new list of hidden states h = f(x) = (h1, ..., hn)



1396

world bank 
 

Shìháng

<s> bank

world

the

bank

bank

of

will

is

of
1 (2) (3)

plan 
 

nǐ

world bank

 outlook

cup

to

plans

planning

to

plan

prepare
2 (3) (4)

…

1

exempt 
 

jǐanmiǎn

bank reduce

exempts

plans

to

or

from

exempts

reduce
some

3 (4) (5)

…

2

debt 
 

zhàiwù

plans

to

exempts

some

reduce

exempt

exempts

some

from

from

4 5 6

…

3
some

debt

7

debt
…

<eos>
<eos>

8

Figure 1: Wait-1 policy example to illustrate the procedure of SBS. The top Chinese words are the source side
inputs which are incrementally revealed to the encoder. Gloss is annotated above Chinese word and Pinyin is
underneath. There are two extra steps (speculative window) are taken (red part) beyond greedy. When source
reaches the last word “债务” (debt), the decoder gets into tail and performs conventional beam search (in green).

to represent x. The encoding function f can be
RNN, CNN or Transformer.

On the other hand, the (greedy) decoder selects
the highest-scoring word yt given source represen-
tation h and previously generated target tokens,
y<t = (y1, ..., yt−1). The greedy search contin-
ues until it emits <eos>, and the final hypothesis
y = (y1, ..., yt) with yt = <eos>

p(y | x) =
∏|y|

t=1 p(yt | x, y<t) (1)
As greedy search only explores one single

path among exponential many alternatives, beam
search is used to improve the search. At each
step t, it maintains a beam Bt of size b, which is an
ordered list of 〈hypothesis, probability〉 pairs; for
example B0 = [〈<s>, 1〉]. We then define one-step
transition from the previous beam to the next as

nextb1(B)=top
b{〈y◦ v, s·p(v|x,y)〉 | 〈y, s〉∈B}

where topb(·) returns the top-scoring b pairs, and
◦ is the string concatenation operator. Now Bt =
nextb1(Bt−1). As a shorthand, we also define the
multi-step beam search function recursively:

nextbi(B)=next
b
1(next

b
i−1(B)) (2)

Full-sentence beam search (over a maximum of T
steps) yields the best hypothesis y∗ with score s∗

(see Huang et al. (2017) for stopping criteria):

〈y∗, s∗〉 =top1(nextbT ([〈<s>, 1〉])) (3)
2.2 Simultaneous MT: Policies and Models
There are two main categories of policies in neural
simultaneous translation decoding (Tab. 1):

1. The first method is to use a fixed-latency pol-
icy, such as the wait-k policy (Ma et al.,
2019a). Such a method would, after an ini-
tial wait of k source words, commit one tar-
get word on receiving each new source word.
When the source sentence ends, the decoder
can do a tail beam search on the remaining
target words, but beam search is seemingly
impossible before the source sentence ends.

policy
model sequence-to-sequence prefix-to-prefix

(full-sentence model) (simultaneous model)
fixed-
latency

test-time wait-k (Dalvi
et al., 2018; Ma et al., 2019a)

wait-k (Ma et al., 2019a)

adaptive

RL MILk
(Gu et al., 2017) (Arivazhagan et al., 2019)

Supervised Learning Imitation Learning
(Zheng et al., 2019a) (Zheng et al., 2019b)

Table 1: Recent advances in simultaneous translation.
2. The second method learns an adaptive policy

which uses either supervised (Zheng et al.,
2019a) or reinforcement learning (Grissom II
et al., 2014; Gu et al., 2017) to decide
whether to READ (the next source word) or
WRITE (the next target word) . Here the de-
coder can commit a chunk of multiple words
for a series of consecutive WRITEs.

In terms of modeling (which is orthogonal to
decoding policies), we can also divide most simul-
taneous translatoin efforts into two camps:

1. Use the standard full-sentence translation
model trained by classical seq-to-seq (Dalvi
et al., 2018; Gu et al., 2017; Zheng et al.,
2019a). For example, the “test-time wait-
k” scheme (Ma et al., 2019a) uses the
full-sentence translation model and performs
wait-k decoding at test time. However,
the obvious training-testing mismatch in this
scheme usually leads to inferior quality.

2. Use a genuinely simultaneous model trained
by the recently proposed prefix-to-prefix
framework (Ma et al., 2019a; Arivazhagan
et al., 2019; Zheng et al., 2019b). There
is no training-testing mismatch in this new
scheme, with the cost of slower training.

3 Speculative Beam Search

We first present our speculative beam search on
the fixed-latency wait-k policy (generating a sin-
gle word per step), and then adapt it to the adaptive
policies (generating multiple words per step).



1397

y<t
<latexit sha1_base64="X/hD3qRrqyBGHyCwDXVE49Ad/0Y=">AAACAXicbVDLSsNAFJ3UV62vqks3g0VwVRIRdOGi6MZlBfuANJTJdNIOnUeYmQghZOU3uNW1O3Hrl7j0T5y0WdjWAxcO59zLvfeEMaPauO63U1lb39jcqm7Xdnb39g/qh0ddLROFSQdLJlU/RJowKkjHUMNIP1YE8ZCRXji9K/zeE1GaSvFo0pgEHI0FjShGxkr+IORZmg+zG5MP6w236c4AV4lXkgYo0R7WfwYjiRNOhMEMae17bmyCDClDMSN5bZBoEiM8RWPiWyoQJzrIZifn8MwqIxhJZUsYOFP/TmSIa53y0HZyZCZ62SvE/zw/MdF1kFERJ4YIPF8UJQwaCYv/4Ygqgg1LLUFYUXsrxBOkEDY2pYUtIS8y8ZYTWCXdi6bnNr2Hy0brtkynCk7AKTgHHrgCLXAP2qADMJDgBbyCN+fZeXc+nM95a8UpZ47BApyvX6Q5mCE=</latexit><latexit sha1_base64="X/hD3qRrqyBGHyCwDXVE49Ad/0Y=">AAACAXicbVDLSsNAFJ3UV62vqks3g0VwVRIRdOGi6MZlBfuANJTJdNIOnUeYmQghZOU3uNW1O3Hrl7j0T5y0WdjWAxcO59zLvfeEMaPauO63U1lb39jcqm7Xdnb39g/qh0ddLROFSQdLJlU/RJowKkjHUMNIP1YE8ZCRXji9K/zeE1GaSvFo0pgEHI0FjShGxkr+IORZmg+zG5MP6w236c4AV4lXkgYo0R7WfwYjiRNOhMEMae17bmyCDClDMSN5bZBoEiM8RWPiWyoQJzrIZifn8MwqIxhJZUsYOFP/TmSIa53y0HZyZCZ62SvE/zw/MdF1kFERJ4YIPF8UJQwaCYv/4Ygqgg1LLUFYUXsrxBOkEDY2pYUtIS8y8ZYTWCXdi6bnNr2Hy0brtkynCk7AKTgHHrgCLXAP2qADMJDgBbyCN+fZeXc+nM95a8UpZ47BApyvX6Q5mCE=</latexit><latexit sha1_base64="X/hD3qRrqyBGHyCwDXVE49Ad/0Y=">AAACAXicbVDLSsNAFJ3UV62vqks3g0VwVRIRdOGi6MZlBfuANJTJdNIOnUeYmQghZOU3uNW1O3Hrl7j0T5y0WdjWAxcO59zLvfeEMaPauO63U1lb39jcqm7Xdnb39g/qh0ddLROFSQdLJlU/RJowKkjHUMNIP1YE8ZCRXji9K/zeE1GaSvFo0pgEHI0FjShGxkr+IORZmg+zG5MP6w236c4AV4lXkgYo0R7WfwYjiRNOhMEMae17bmyCDClDMSN5bZBoEiM8RWPiWyoQJzrIZifn8MwqIxhJZUsYOFP/TmSIa53y0HZyZCZ62SvE/zw/MdF1kFERJ4YIPF8UJQwaCYv/4Ygqgg1LLUFYUXsrxBOkEDY2pYUtIS8y8ZYTWCXdi6bnNr2Hy0brtkynCk7AKTgHHrgCLXAP2qADMJDgBbyCN+fZeXc+nM95a8UpZ47BApyvX6Q5mCE=</latexit><latexit sha1_base64="X/hD3qRrqyBGHyCwDXVE49Ad/0Y=">AAACAXicbVDLSsNAFJ3UV62vqks3g0VwVRIRdOGi6MZlBfuANJTJdNIOnUeYmQghZOU3uNW1O3Hrl7j0T5y0WdjWAxcO59zLvfeEMaPauO63U1lb39jcqm7Xdnb39g/qh0ddLROFSQdLJlU/RJowKkjHUMNIP1YE8ZCRXji9K/zeE1GaSvFo0pgEHI0FjShGxkr+IORZmg+zG5MP6w236c4AV4lXkgYo0R7WfwYjiRNOhMEMae17bmyCDClDMSN5bZBoEiM8RWPiWyoQJzrIZifn8MwqIxhJZUsYOFP/TmSIa53y0HZyZCZ62SvE/zw/MdF1kFERJ4YIPF8UJQwaCYv/4Ygqgg1LLUFYUXsrxBOkEDY2pYUtIS8y8ZYTWCXdi6bnNr2Hy0brtkynCk7AKTgHHrgCLXAP2qADMJDgBbyCN+fZeXc+nM95a8UpZ47BApyvX6Q5mCE=</latexit>

y<t
<latexit sha1_base64="X/hD3qRrqyBGHyCwDXVE49Ad/0Y=">AAACAXicbVDLSsNAFJ3UV62vqks3g0VwVRIRdOGi6MZlBfuANJTJdNIOnUeYmQghZOU3uNW1O3Hrl7j0T5y0WdjWAxcO59zLvfeEMaPauO63U1lb39jcqm7Xdnb39g/qh0ddLROFSQdLJlU/RJowKkjHUMNIP1YE8ZCRXji9K/zeE1GaSvFo0pgEHI0FjShGxkr+IORZmg+zG5MP6w236c4AV4lXkgYo0R7WfwYjiRNOhMEMae17bmyCDClDMSN5bZBoEiM8RWPiWyoQJzrIZifn8MwqIxhJZUsYOFP/TmSIa53y0HZyZCZ62SvE/zw/MdF1kFERJ4YIPF8UJQwaCYv/4Ygqgg1LLUFYUXsrxBOkEDY2pYUtIS8y8ZYTWCXdi6bnNr2Hy0brtkynCk7AKTgHHrgCLXAP2qADMJDgBbyCN+fZeXc+nM95a8UpZ47BApyvX6Q5mCE=</latexit><latexit sha1_base64="X/hD3qRrqyBGHyCwDXVE49Ad/0Y=">AAACAXicbVDLSsNAFJ3UV62vqks3g0VwVRIRdOGi6MZlBfuANJTJdNIOnUeYmQghZOU3uNW1O3Hrl7j0T5y0WdjWAxcO59zLvfeEMaPauO63U1lb39jcqm7Xdnb39g/qh0ddLROFSQdLJlU/RJowKkjHUMNIP1YE8ZCRXji9K/zeE1GaSvFo0pgEHI0FjShGxkr+IORZmg+zG5MP6w236c4AV4lXkgYo0R7WfwYjiRNOhMEMae17bmyCDClDMSN5bZBoEiM8RWPiWyoQJzrIZifn8MwqIxhJZUsYOFP/TmSIa53y0HZyZCZ62SvE/zw/MdF1kFERJ4YIPF8UJQwaCYv/4Ygqgg1LLUFYUXsrxBOkEDY2pYUtIS8y8ZYTWCXdi6bnNr2Hy0brtkynCk7AKTgHHrgCLXAP2qADMJDgBbyCN+fZeXc+nM95a8UpZ47BApyvX6Q5mCE=</latexit><latexit sha1_base64="X/hD3qRrqyBGHyCwDXVE49Ad/0Y=">AAACAXicbVDLSsNAFJ3UV62vqks3g0VwVRIRdOGi6MZlBfuANJTJdNIOnUeYmQghZOU3uNW1O3Hrl7j0T5y0WdjWAxcO59zLvfeEMaPauO63U1lb39jcqm7Xdnb39g/qh0ddLROFSQdLJlU/RJowKkjHUMNIP1YE8ZCRXji9K/zeE1GaSvFo0pgEHI0FjShGxkr+IORZmg+zG5MP6w236c4AV4lXkgYo0R7WfwYjiRNOhMEMae17bmyCDClDMSN5bZBoEiM8RWPiWyoQJzrIZifn8MwqIxhJZUsYOFP/TmSIa53y0HZyZCZ62SvE/zw/MdF1kFERJ4YIPF8UJQwaCYv/4Ygqgg1LLUFYUXsrxBOkEDY2pYUtIS8y8ZYTWCXdi6bnNr2Hy0brtkynCk7AKTgHHrgCLXAP2qADMJDgBbyCN+fZeXc+nM95a8UpZ47BApyvX6Q5mCE=</latexit><latexit sha1_base64="X/hD3qRrqyBGHyCwDXVE49Ad/0Y=">AAACAXicbVDLSsNAFJ3UV62vqks3g0VwVRIRdOGi6MZlBfuANJTJdNIOnUeYmQghZOU3uNW1O3Hrl7j0T5y0WdjWAxcO59zLvfeEMaPauO63U1lb39jcqm7Xdnb39g/qh0ddLROFSQdLJlU/RJowKkjHUMNIP1YE8ZCRXji9K/zeE1GaSvFo0pgEHI0FjShGxkr+IORZmg+zG5MP6w236c4AV4lXkgYo0R7WfwYjiRNOhMEMae17bmyCDClDMSN5bZBoEiM8RWPiWyoQJzrIZifn8MwqIxhJZUsYOFP/TmSIa53y0HZyZCZ62SvE/zw/MdF1kFERJ4YIPF8UJQwaCYv/4Ygqgg1LLUFYUXsrxBOkEDY2pYUtIS8y8ZYTWCXdi6bnNr2Hy0brtkynCk7AKTgHHrgCLXAP2qADMJDgBbyCN+fZeXc+nM95a8UpZ47BApyvX6Q5mCE=</latexit>

(a) (b)

ŷ>
t+

n

<latexit sha1_base64="VvjgRliAZGlDz4KR3RR9nP5JI88=">AAACC3icdVBNSwMxEM3Wr/pd7dFLsAiCsGTbYtuLFL14VLAqtKVk09SGJtklmRWWZX+Cv8Grnr2JV3+ER/+Ju7WCij4YeLw3w8w8P5TCAiFvTmFufmFxqbi8srq2vrFZ2tq+tEFkGO+wQAbm2qeWS6F5BwRIfh0aTpUv+ZU/Ocn9q1turAj0BcQh7yt6o8VIMAqZNCiVe75KemMKSZymg+QIDnQ6KFWI26g2660aJi45bNXrJCe1RrXRxJ5LpqigGc4GpffeMGCR4hqYpNZ2PRJCP6EGBJM8XelFloeUTegN72ZUU8VtP5ken+K9TBniUWCy0oCn6veJhCprY+VnnYrC2P72cvEvrxvBqNlPhA4j4Jp9LhpFEkOA8yTwUBjOQMYZocyI7FbMxtRQBlleP7b4Ks/k63H8P7msuh5xvfN6pX08S6eIdtAu2kceaqA2OkVnqIMYitE9ekCPzp3z5Dw7L5+tBWc2U0Y/4Lx+APJ9nCE=</latexit><latexit sha1_base64="VvjgRliAZGlDz4KR3RR9nP5JI88=">AAACC3icdVBNSwMxEM3Wr/pd7dFLsAiCsGTbYtuLFL14VLAqtKVk09SGJtklmRWWZX+Cv8Grnr2JV3+ER/+Ju7WCij4YeLw3w8w8P5TCAiFvTmFufmFxqbi8srq2vrFZ2tq+tEFkGO+wQAbm2qeWS6F5BwRIfh0aTpUv+ZU/Ocn9q1turAj0BcQh7yt6o8VIMAqZNCiVe75KemMKSZymg+QIDnQ6KFWI26g2660aJi45bNXrJCe1RrXRxJ5LpqigGc4GpffeMGCR4hqYpNZ2PRJCP6EGBJM8XelFloeUTegN72ZUU8VtP5ken+K9TBniUWCy0oCn6veJhCprY+VnnYrC2P72cvEvrxvBqNlPhA4j4Jp9LhpFEkOA8yTwUBjOQMYZocyI7FbMxtRQBlleP7b4Ks/k63H8P7msuh5xvfN6pX08S6eIdtAu2kceaqA2OkVnqIMYitE9ekCPzp3z5Dw7L5+tBWc2U0Y/4Lx+APJ9nCE=</latexit><latexit sha1_base64="VvjgRliAZGlDz4KR3RR9nP5JI88=">AAACC3icdVBNSwMxEM3Wr/pd7dFLsAiCsGTbYtuLFL14VLAqtKVk09SGJtklmRWWZX+Cv8Grnr2JV3+ER/+Ju7WCij4YeLw3w8w8P5TCAiFvTmFufmFxqbi8srq2vrFZ2tq+tEFkGO+wQAbm2qeWS6F5BwRIfh0aTpUv+ZU/Ocn9q1turAj0BcQh7yt6o8VIMAqZNCiVe75KemMKSZymg+QIDnQ6KFWI26g2660aJi45bNXrJCe1RrXRxJ5LpqigGc4GpffeMGCR4hqYpNZ2PRJCP6EGBJM8XelFloeUTegN72ZUU8VtP5ken+K9TBniUWCy0oCn6veJhCprY+VnnYrC2P72cvEvrxvBqNlPhA4j4Jp9LhpFEkOA8yTwUBjOQMYZocyI7FbMxtRQBlleP7b4Ks/k63H8P7msuh5xvfN6pX08S6eIdtAu2kceaqA2OkVnqIMYitE9ekCPzp3z5Dw7L5+tBWc2U0Y/4Lx+APJ9nCE=</latexit><latexit sha1_base64="VvjgRliAZGlDz4KR3RR9nP5JI88=">AAACC3icdVBNSwMxEM3Wr/pd7dFLsAiCsGTbYtuLFL14VLAqtKVk09SGJtklmRWWZX+Cv8Grnr2JV3+ER/+Ju7WCij4YeLw3w8w8P5TCAiFvTmFufmFxqbi8srq2vrFZ2tq+tEFkGO+wQAbm2qeWS6F5BwRIfh0aTpUv+ZU/Ocn9q1turAj0BcQh7yt6o8VIMAqZNCiVe75KemMKSZymg+QIDnQ6KFWI26g2660aJi45bNXrJCe1RrXRxJ5LpqigGc4GpffeMGCR4hqYpNZ2PRJCP6EGBJM8XelFloeUTegN72ZUU8VtP5ken+K9TBniUWCy0oCn6veJhCprY+VnnYrC2P72cvEvrxvBqNlPhA4j4Jp9LhpFEkOA8yTwUBjOQMYZocyI7FbMxtRQBlleP7b4Ks/k63H8P7msuh5xvfN6pX08S6eIdtAu2kceaqA2OkVnqIMYitE9ekCPzp3z5Dw7L5+tBWc2U0Y/4Lx+APJ9nCE=</latexit>

ŷt
<latexit sha1_base64="l4NYCn8CzfUIb1/7pDuX8RR99pk=">AAACAXicdVDJSgNBEO2JW4xb1KOXxiB4GnqymOQW9OIxgjGBJISeTk/SpGehu0YIw5z8Bq969iZe/RKP/omdRTCiDwoe71VRVc+NpNBAyIeVWVvf2NzKbud2dvf2D/KHR3c6jBXjLRbKUHVcqrkUAW+BAMk7keLUdyVvu5Ormd++50qLMLiFacT7Ph0FwhOMgpG6vTGFZJoOEkgH+QKxSdmp1yqY2CVSqVaKhlzU66RUxo5N5iigJZqD/GdvGLLY5wEwSbXuOiSCfkIVCCZ5muvFmkeUTeiIdw0NqM91P5mfnOIzowyxFypTAeC5+nMiob7WU981nT6Fsf7tzcS/vG4MXq2fiCCKgQdssciLJYYQz/7HQ6E4Azk1hDIlzK2YjamiDExKK1tcf5bJ9+P4f3JXtB1iOzflQuNymU4WnaBTdI4cVEUNdI2aqIUYCtEjekLP1oP1Yr1ab4vWjLWcOUYrsN6/AGuHmKM=</latexit><latexit sha1_base64="l4NYCn8CzfUIb1/7pDuX8RR99pk=">AAACAXicdVDJSgNBEO2JW4xb1KOXxiB4GnqymOQW9OIxgjGBJISeTk/SpGehu0YIw5z8Bq969iZe/RKP/omdRTCiDwoe71VRVc+NpNBAyIeVWVvf2NzKbud2dvf2D/KHR3c6jBXjLRbKUHVcqrkUAW+BAMk7keLUdyVvu5Ormd++50qLMLiFacT7Ph0FwhOMgpG6vTGFZJoOEkgH+QKxSdmp1yqY2CVSqVaKhlzU66RUxo5N5iigJZqD/GdvGLLY5wEwSbXuOiSCfkIVCCZ5muvFmkeUTeiIdw0NqM91P5mfnOIzowyxFypTAeC5+nMiob7WU981nT6Fsf7tzcS/vG4MXq2fiCCKgQdssciLJYYQz/7HQ6E4Azk1hDIlzK2YjamiDExKK1tcf5bJ9+P4f3JXtB1iOzflQuNymU4WnaBTdI4cVEUNdI2aqIUYCtEjekLP1oP1Yr1ab4vWjLWcOUYrsN6/AGuHmKM=</latexit><latexit sha1_base64="l4NYCn8CzfUIb1/7pDuX8RR99pk=">AAACAXicdVDJSgNBEO2JW4xb1KOXxiB4GnqymOQW9OIxgjGBJISeTk/SpGehu0YIw5z8Bq969iZe/RKP/omdRTCiDwoe71VRVc+NpNBAyIeVWVvf2NzKbud2dvf2D/KHR3c6jBXjLRbKUHVcqrkUAW+BAMk7keLUdyVvu5Ormd++50qLMLiFacT7Ph0FwhOMgpG6vTGFZJoOEkgH+QKxSdmp1yqY2CVSqVaKhlzU66RUxo5N5iigJZqD/GdvGLLY5wEwSbXuOiSCfkIVCCZ5muvFmkeUTeiIdw0NqM91P5mfnOIzowyxFypTAeC5+nMiob7WU981nT6Fsf7tzcS/vG4MXq2fiCCKgQdssciLJYYQz/7HQ6E4Azk1hDIlzK2YjamiDExKK1tcf5bJ9+P4f3JXtB1iOzflQuNymU4WnaBTdI4cVEUNdI2aqIUYCtEjekLP1oP1Yr1ab4vWjLWcOUYrsN6/AGuHmKM=</latexit><latexit sha1_base64="l4NYCn8CzfUIb1/7pDuX8RR99pk=">AAACAXicdVDJSgNBEO2JW4xb1KOXxiB4GnqymOQW9OIxgjGBJISeTk/SpGehu0YIw5z8Bq969iZe/RKP/omdRTCiDwoe71VRVc+NpNBAyIeVWVvf2NzKbud2dvf2D/KHR3c6jBXjLRbKUHVcqrkUAW+BAMk7keLUdyVvu5Ormd++50qLMLiFacT7Ph0FwhOMgpG6vTGFZJoOEkgH+QKxSdmp1yqY2CVSqVaKhlzU66RUxo5N5iigJZqD/GdvGLLY5wEwSbXuOiSCfkIVCCZ5muvFmkeUTeiIdw0NqM91P5mfnOIzowyxFypTAeC5+nMiob7WU981nT6Fsf7tzcS/vG4MXq2fiCCKgQdssciLJYYQz/7HQ6E4Azk1hDIlzK2YjamiDExKK1tcf5bJ9+P4f3JXtB1iOzflQuNymU4WnaBTdI4cVEUNdI2aqIUYCtEjekLP1oP1Yr1ab4vWjLWcOUYrsN6/AGuHmKM=</latexit>

w
<latexit sha1_base64="sr79D3NkQoMrWj8gMBTNFQeIupM=">AAAB93icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoY5mA+YDkCHubuWTJ7t2xu6ccIb/AVms7sfXnWPpP3CRXmMQHA4/3ZpiZFySCa+O6305hY3Nre6e4W9rbPzg8Kh+ftHScKoZNFotYdQKqUfAIm4YbgZ1EIZWBwHYwvp/57SdUmsfRo8kS9CUdRjzkjBorNZ775Ypbdecg68TLSQVy1Pvln94gZqnEyDBBte56bmL8CVWGM4HTUi/VmFA2pkPsWhpRidqfzA+dkgurDEgYK1uRIXP178SESq0zGdhOSc1Ir3oz8T+vm5rw1p/wKEkNRmyxKEwFMTGZfU0GXCEzIrOEMsXtrYSNqKLM2GyWtgRyajPxVhNYJ62rqudWvcZ1pXaXp1OEMziHS/DgBmrwAHVoAgOEF3iFNydz3p0P53PRWnDymVNYgvP1C4m2k5E=</latexit><latexit sha1_base64="sr79D3NkQoMrWj8gMBTNFQeIupM=">AAAB93icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoY5mA+YDkCHubuWTJ7t2xu6ccIb/AVms7sfXnWPpP3CRXmMQHA4/3ZpiZFySCa+O6305hY3Nre6e4W9rbPzg8Kh+ftHScKoZNFotYdQKqUfAIm4YbgZ1EIZWBwHYwvp/57SdUmsfRo8kS9CUdRjzkjBorNZ775Ypbdecg68TLSQVy1Pvln94gZqnEyDBBte56bmL8CVWGM4HTUi/VmFA2pkPsWhpRidqfzA+dkgurDEgYK1uRIXP178SESq0zGdhOSc1Ir3oz8T+vm5rw1p/wKEkNRmyxKEwFMTGZfU0GXCEzIrOEMsXtrYSNqKLM2GyWtgRyajPxVhNYJ62rqudWvcZ1pXaXp1OEMziHS/DgBmrwAHVoAgOEF3iFNydz3p0P53PRWnDymVNYgvP1C4m2k5E=</latexit><latexit sha1_base64="sr79D3NkQoMrWj8gMBTNFQeIupM=">AAAB93icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoY5mA+YDkCHubuWTJ7t2xu6ccIb/AVms7sfXnWPpP3CRXmMQHA4/3ZpiZFySCa+O6305hY3Nre6e4W9rbPzg8Kh+ftHScKoZNFotYdQKqUfAIm4YbgZ1EIZWBwHYwvp/57SdUmsfRo8kS9CUdRjzkjBorNZ775Ypbdecg68TLSQVy1Pvln94gZqnEyDBBte56bmL8CVWGM4HTUi/VmFA2pkPsWhpRidqfzA+dkgurDEgYK1uRIXP178SESq0zGdhOSc1Ir3oz8T+vm5rw1p/wKEkNRmyxKEwFMTGZfU0GXCEzIrOEMsXtrYSNqKLM2GyWtgRyajPxVhNYJ62rqudWvcZ1pXaXp1OEMziHS/DgBmrwAHVoAgOEF3iFNydz3p0P53PRWnDymVNYgvP1C4m2k5E=</latexit><latexit sha1_base64="sr79D3NkQoMrWj8gMBTNFQeIupM=">AAAB93icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoY5mA+YDkCHubuWTJ7t2xu6ccIb/AVms7sfXnWPpP3CRXmMQHA4/3ZpiZFySCa+O6305hY3Nre6e4W9rbPzg8Kh+ftHScKoZNFotYdQKqUfAIm4YbgZ1EIZWBwHYwvp/57SdUmsfRo8kS9CUdRjzkjBorNZ775Ypbdecg68TLSQVy1Pvln94gZqnEyDBBte56bmL8CVWGM4HTUi/VmFA2pkPsWhpRidqfzA+dkgurDEgYK1uRIXP178SESq0zGdhOSc1Ir3oz8T+vm5rw1p/wKEkNRmyxKEwFMTGZfU0GXCEzIrOEMsXtrYSNqKLM2GyWtgRyajPxVhNYJ62rqudWvcZ1pXaXp1OEMziHS/DgBmrwAHVoAgOEF3iFNydz3p0P53PRWnDymVNYgvP1C4m2k5E=</latexit>

steps steps

ŷ t
+
1:
t+

w

<latexit sha1_base64="rElp/jCMzERk6fxCLDRzgImgapM=">AAACDnicdVBNS8NAEN34/W1V8OJlsQhCIWzaYqqnohePFawVmlI2261dupuE3YlSYv6Dv8Grnr2JV/+CR/+JSa2gog8GHu/NMDPPj6QwQMibNTU9Mzs3v7C4tLyyurZe2Ni8MGGsGW+yUIb60qeGSxHwJgiQ/DLSnCpf8pY/PMn91jXXRoTBOYwi3lH0KhB9wShkUrew7Q0oJJ6vklGadhMoOUdQukm7hSKx3XKteljBxCYHh9UqyUnFLbs17NhkjCKaoNEtvHu9kMWKB8AkNabtkAg6CdUgmOTpkhcbHlE2pFe8ndGAKm46yfj+FO9lSg/3Q51VAHisfp9IqDJmpPysU1EYmN9eLv7ltWPo1zqJCKIYeMA+F/VjiSHEeRi4JzRnIEcZoUyL7FbMBlRTBllkP7b4Ks/k63H8P7ko2w6xnbNqsX48SWcB7aBdtI8c5KI6OkUN1EQM3aJ79IAerTvryXq2Xj5bp6zJzBb6Aev1A8jVnRQ=</latexit><latexit sha1_base64="rElp/jCMzERk6fxCLDRzgImgapM=">AAACDnicdVBNS8NAEN34/W1V8OJlsQhCIWzaYqqnohePFawVmlI2261dupuE3YlSYv6Dv8Grnr2JV/+CR/+JSa2gog8GHu/NMDPPj6QwQMibNTU9Mzs3v7C4tLyyurZe2Ni8MGGsGW+yUIb60qeGSxHwJgiQ/DLSnCpf8pY/PMn91jXXRoTBOYwi3lH0KhB9wShkUrew7Q0oJJ6vklGadhMoOUdQukm7hSKx3XKteljBxCYHh9UqyUnFLbs17NhkjCKaoNEtvHu9kMWKB8AkNabtkAg6CdUgmOTpkhcbHlE2pFe8ndGAKm46yfj+FO9lSg/3Q51VAHisfp9IqDJmpPysU1EYmN9eLv7ltWPo1zqJCKIYeMA+F/VjiSHEeRi4JzRnIEcZoUyL7FbMBlRTBllkP7b4Ks/k63H8P7ko2w6xnbNqsX48SWcB7aBdtI8c5KI6OkUN1EQM3aJ79IAerTvryXq2Xj5bp6zJzBb6Aev1A8jVnRQ=</latexit><latexit sha1_base64="rElp/jCMzERk6fxCLDRzgImgapM=">AAACDnicdVBNS8NAEN34/W1V8OJlsQhCIWzaYqqnohePFawVmlI2261dupuE3YlSYv6Dv8Grnr2JV/+CR/+JSa2gog8GHu/NMDPPj6QwQMibNTU9Mzs3v7C4tLyyurZe2Ni8MGGsGW+yUIb60qeGSxHwJgiQ/DLSnCpf8pY/PMn91jXXRoTBOYwi3lH0KhB9wShkUrew7Q0oJJ6vklGadhMoOUdQukm7hSKx3XKteljBxCYHh9UqyUnFLbs17NhkjCKaoNEtvHu9kMWKB8AkNabtkAg6CdUgmOTpkhcbHlE2pFe8ndGAKm46yfj+FO9lSg/3Q51VAHisfp9IqDJmpPysU1EYmN9eLv7ltWPo1zqJCKIYeMA+F/VjiSHEeRi4JzRnIEcZoUyL7FbMBlRTBllkP7b4Ks/k63H8P7ko2w6xnbNqsX48SWcB7aBdtI8c5KI6OkUN1EQM3aJ79IAerTvryXq2Xj5bp6zJzBb6Aev1A8jVnRQ=</latexit><latexit sha1_base64="rElp/jCMzERk6fxCLDRzgImgapM=">AAACDnicdVBNS8NAEN34/W1V8OJlsQhCIWzaYqqnohePFawVmlI2261dupuE3YlSYv6Dv8Grnr2JV/+CR/+JSa2gog8GHu/NMDPPj6QwQMibNTU9Mzs3v7C4tLyyurZe2Ni8MGGsGW+yUIb60qeGSxHwJgiQ/DLSnCpf8pY/PMn91jXXRoTBOYwi3lH0KhB9wShkUrew7Q0oJJ6vklGadhMoOUdQukm7hSKx3XKteljBxCYHh9UqyUnFLbs17NhkjCKaoNEtvHu9kMWKB8AkNabtkAg6CdUgmOTpkhcbHlE2pFe8ndGAKm46yfj+FO9lSg/3Q51VAHisfp9IqDJmpPysU1EYmN9eLv7ltWPo1zqJCKIYeMA+F/VjiSHEeRi4JzRnIEcZoUyL7FbMBlRTBllkP7b4Ks/k63H8P7ko2w6xnbNqsX48SWcB7aBdtI8c5KI6OkUN1EQM3aJ79IAerTvryXq2Xj5bp6zJzBb6Aev1A8jVnRQ=</latexit>

ŷ t+
n:

t+
n+

w�
1

<latexit sha1_base64="O42AWTU/wgAW89vFJE3oqQQh6Ro=">AAACEnicdVBNSwMxEM36/W3Vm16CRRDEJVuLrZ5ELx4VrAptKdk0taFJdklmlbIs+CP8DV717E28+gc8+k/M1goq+mDg8d4MM/PCWAoLhLx5I6Nj4xOTU9Mzs3PzC4uFpeVzGyWG8RqLZGQuQ2q5FJrXQIDkl7HhVIWSX4S9o9y/uObGikifQT/mTUWvtOgIRsFJrcJqo0shbYQq7WdZK4Utve9q62Y7yFqFIvErpWp5bwcTn+zulcskJzuVUqWKA58MUERDnLQK7412xBLFNTBJra0HJIZmSg0IJnk200gsjynr0Sted1RTxW0zHfyQ4Q2ntHEnMq404IH6fSKlytq+Cl2notC1v71c/MurJ9CpNlOh4wS4Zp+LOonEEOE8ENwWhjOQfUcoM8LdilmXGsrAxfZjS6jyTL4ex/+T85IfED84LRcPDofpTKE1tI42UYAq6AAdoxNUQwzdonv0gB69O+/Je/ZePltHvOHMCvoB7/UDaAqecA==</latexit><latexit sha1_base64="O42AWTU/wgAW89vFJE3oqQQh6Ro=">AAACEnicdVBNSwMxEM36/W3Vm16CRRDEJVuLrZ5ELx4VrAptKdk0taFJdklmlbIs+CP8DV717E28+gc8+k/M1goq+mDg8d4MM/PCWAoLhLx5I6Nj4xOTU9Mzs3PzC4uFpeVzGyWG8RqLZGQuQ2q5FJrXQIDkl7HhVIWSX4S9o9y/uObGikifQT/mTUWvtOgIRsFJrcJqo0shbYQq7WdZK4Utve9q62Y7yFqFIvErpWp5bwcTn+zulcskJzuVUqWKA58MUERDnLQK7412xBLFNTBJra0HJIZmSg0IJnk200gsjynr0Sted1RTxW0zHfyQ4Q2ntHEnMq404IH6fSKlytq+Cl2notC1v71c/MurJ9CpNlOh4wS4Zp+LOonEEOE8ENwWhjOQfUcoM8LdilmXGsrAxfZjS6jyTL4ex/+T85IfED84LRcPDofpTKE1tI42UYAq6AAdoxNUQwzdonv0gB69O+/Je/ZePltHvOHMCvoB7/UDaAqecA==</latexit><latexit sha1_base64="O42AWTU/wgAW89vFJE3oqQQh6Ro=">AAACEnicdVBNSwMxEM36/W3Vm16CRRDEJVuLrZ5ELx4VrAptKdk0taFJdklmlbIs+CP8DV717E28+gc8+k/M1goq+mDg8d4MM/PCWAoLhLx5I6Nj4xOTU9Mzs3PzC4uFpeVzGyWG8RqLZGQuQ2q5FJrXQIDkl7HhVIWSX4S9o9y/uObGikifQT/mTUWvtOgIRsFJrcJqo0shbYQq7WdZK4Utve9q62Y7yFqFIvErpWp5bwcTn+zulcskJzuVUqWKA58MUERDnLQK7412xBLFNTBJra0HJIZmSg0IJnk200gsjynr0Sted1RTxW0zHfyQ4Q2ntHEnMq404IH6fSKlytq+Cl2notC1v71c/MurJ9CpNlOh4wS4Zp+LOonEEOE8ENwWhjOQfUcoM8LdilmXGsrAxfZjS6jyTL4ex/+T85IfED84LRcPDofpTKE1tI42UYAq6AAdoxNUQwzdonv0gB69O+/Je/ZePltHvOHMCvoB7/UDaAqecA==</latexit><latexit sha1_base64="O42AWTU/wgAW89vFJE3oqQQh6Ro=">AAACEnicdVBNSwMxEM36/W3Vm16CRRDEJVuLrZ5ELx4VrAptKdk0taFJdklmlbIs+CP8DV717E28+gc8+k/M1goq+mDg8d4MM/PCWAoLhLx5I6Nj4xOTU9Mzs3PzC4uFpeVzGyWG8RqLZGQuQ2q5FJrXQIDkl7HhVIWSX4S9o9y/uObGikifQT/mTUWvtOgIRsFJrcJqo0shbYQq7WdZK4Utve9q62Y7yFqFIvErpWp5bwcTn+zulcskJzuVUqWKA58MUERDnLQK7412xBLFNTBJra0HJIZmSg0IJnk200gsjynr0Sted1RTxW0zHfyQ4Q2ntHEnMq404IH6fSKlytq+Cl2notC1v71c/MurJ9CpNlOh4wS4Zp+LOonEEOE8ENwWhjOQfUcoM8LdilmXGsrAxfZjS6jyTL4ex/+T85IfED84LRcPDofpTKE1tI42UYAq6AAdoxNUQwzdonv0gB69O+/Je/ZePltHvOHMCvoB7/UDaAqecA==</latexit>

ŷt : t+n�1
<latexit sha1_base64="kRrST2gfxX3pFjXOG0aEqqcPCdI=">AAACFnicdVBNSysxFM348axfzz5dChIsgiAOGW211Y3oxqWCVaFTSiZNbTDJDMmdB2WY3fsR7ze41bU7cevWpf/ETK2gogcSDufcy733RIkUFgh59sbGJyZ/TZWmZ2bn5n8vlP8snts4NYw3WSxjcxlRy6XQvAkCJL9MDKcqkvwiuj4q/Iu/3FgR6zMYJLyt6JUWPcEoOKlTXgn7FLIwUtkgzzsZ4HAf7xUfbOjNIO+UK8Qn1aBRr2Hib5Pabm3LkZ1Gg2xXceCTISpohJNO+SXsxixVXAOT1NpWQBJoZ9SAYJLnM2FqeULZNb3iLUc1Vdy2s+EdOV5zShf3YuOeBjxUP3ZkVFk7UJGrVBT69qtXiN95rRR69XYmdJIC1+xtUC+VGGJchIK7wnAGcuAIZUa4XTHrU0MZuOg+TYlUkcn74fhncr7lB8QPTquVg8NROiW0jFbROgrQLjpAx+gENRFD/9ANukV33n/v3nvwHt9Kx7xRzxL6BO/pFc78nwg=</latexit><latexit sha1_base64="kRrST2gfxX3pFjXOG0aEqqcPCdI=">AAACFnicdVBNSysxFM348axfzz5dChIsgiAOGW211Y3oxqWCVaFTSiZNbTDJDMmdB2WY3fsR7ze41bU7cevWpf/ETK2gogcSDufcy733RIkUFgh59sbGJyZ/TZWmZ2bn5n8vlP8snts4NYw3WSxjcxlRy6XQvAkCJL9MDKcqkvwiuj4q/Iu/3FgR6zMYJLyt6JUWPcEoOKlTXgn7FLIwUtkgzzsZ4HAf7xUfbOjNIO+UK8Qn1aBRr2Hib5Pabm3LkZ1Gg2xXceCTISpohJNO+SXsxixVXAOT1NpWQBJoZ9SAYJLnM2FqeULZNb3iLUc1Vdy2s+EdOV5zShf3YuOeBjxUP3ZkVFk7UJGrVBT69qtXiN95rRR69XYmdJIC1+xtUC+VGGJchIK7wnAGcuAIZUa4XTHrU0MZuOg+TYlUkcn74fhncr7lB8QPTquVg8NROiW0jFbROgrQLjpAx+gENRFD/9ANukV33n/v3nvwHt9Kx7xRzxL6BO/pFc78nwg=</latexit><latexit sha1_base64="kRrST2gfxX3pFjXOG0aEqqcPCdI=">AAACFnicdVBNSysxFM348axfzz5dChIsgiAOGW211Y3oxqWCVaFTSiZNbTDJDMmdB2WY3fsR7ze41bU7cevWpf/ETK2gogcSDufcy733RIkUFgh59sbGJyZ/TZWmZ2bn5n8vlP8snts4NYw3WSxjcxlRy6XQvAkCJL9MDKcqkvwiuj4q/Iu/3FgR6zMYJLyt6JUWPcEoOKlTXgn7FLIwUtkgzzsZ4HAf7xUfbOjNIO+UK8Qn1aBRr2Hib5Pabm3LkZ1Gg2xXceCTISpohJNO+SXsxixVXAOT1NpWQBJoZ9SAYJLnM2FqeULZNb3iLUc1Vdy2s+EdOV5zShf3YuOeBjxUP3ZkVFk7UJGrVBT69qtXiN95rRR69XYmdJIC1+xtUC+VGGJchIK7wnAGcuAIZUa4XTHrU0MZuOg+TYlUkcn74fhncr7lB8QPTquVg8NROiW0jFbROgrQLjpAx+gENRFD/9ANukV33n/v3nvwHt9Kx7xRzxL6BO/pFc78nwg=</latexit><latexit sha1_base64="kRrST2gfxX3pFjXOG0aEqqcPCdI=">AAACFnicdVBNSysxFM348axfzz5dChIsgiAOGW211Y3oxqWCVaFTSiZNbTDJDMmdB2WY3fsR7ze41bU7cevWpf/ETK2gogcSDufcy733RIkUFgh59sbGJyZ/TZWmZ2bn5n8vlP8snts4NYw3WSxjcxlRy6XQvAkCJL9MDKcqkvwiuj4q/Iu/3FgR6zMYJLyt6JUWPcEoOKlTXgn7FLIwUtkgzzsZ4HAf7xUfbOjNIO+UK8Qn1aBRr2Hib5Pabm3LkZ1Gg2xXceCTISpohJNO+SXsxixVXAOT1NpWQBJoZ9SAYJLnM2FqeULZNb3iLUc1Vdy2s+EdOV5zShf3YuOeBjxUP3ZkVFk7UJGrVBT69qtXiN95rRR69XYmdJIC1+xtUC+VGGJchIK7wnAGcuAIZUa4XTHrU0MZuOg+TYlUkcn74fhncr7lB8QPTquVg8NROiW0jFbROgrQLjpAx+gENRFD/9ANukV33n/v3nvwHt9Kx7xRzxL6BO/pFc78nwg=</latexit>

ŷt+n
: t+

n+
w�1

<latexit sha1_base64="s2S1xFtplMgMmo+UlAT1aTEd9R4=">AAACGnicdVBNbxMxEPWWAiEUmsKRi5UIqVLEyptEJIFLRC8cU6n5kLJR5HWcxKrtXdmzRdFq7/wIfgNXOHNDvfbSI/+k3iRIBLVPsvX03oxm5kWJFBYIufUOHh0+fvK09Kz8/OjFy+PKyauhjVPD+IDFMjbjiFouheYDECD5ODGcqkjyUXR5VvijK26siPUFrBM+VXSpxUIwCk6aVarhikIWRipb5/ksg7rG4Uf8ofgcr395F+SzSo347Uan1W1i4pP33VaLFKTZbrQ7OPDJBjW0Q39W+RPOY5YqroFJau0kIAlMM2pAMMnzcphanlB2SZd84qimittptrklx2+dMseL2LinAW/Ufzsyqqxdq8hVKgor+79XiPd5kxQWnWkmdJIC12w7aJFKDDEugsFzYTgDuXaEMiPcrpitqKEMXHx7UyJVZPL3cPwwGTb8gPjBeavW+7RLp4TeoCo6RQFqox76jPpogBj6ir6jH+in98375f32rrelB96u5zXag3dzB4VZoG4=</latexit><latexit sha1_base64="s2S1xFtplMgMmo+UlAT1aTEd9R4=">AAACGnicdVBNbxMxEPWWAiEUmsKRi5UIqVLEyptEJIFLRC8cU6n5kLJR5HWcxKrtXdmzRdFq7/wIfgNXOHNDvfbSI/+k3iRIBLVPsvX03oxm5kWJFBYIufUOHh0+fvK09Kz8/OjFy+PKyauhjVPD+IDFMjbjiFouheYDECD5ODGcqkjyUXR5VvijK26siPUFrBM+VXSpxUIwCk6aVarhikIWRipb5/ksg7rG4Uf8ofgcr395F+SzSo347Uan1W1i4pP33VaLFKTZbrQ7OPDJBjW0Q39W+RPOY5YqroFJau0kIAlMM2pAMMnzcphanlB2SZd84qimittptrklx2+dMseL2LinAW/Ufzsyqqxdq8hVKgor+79XiPd5kxQWnWkmdJIC12w7aJFKDDEugsFzYTgDuXaEMiPcrpitqKEMXHx7UyJVZPL3cPwwGTb8gPjBeavW+7RLp4TeoCo6RQFqox76jPpogBj6ir6jH+in98375f32rrelB96u5zXag3dzB4VZoG4=</latexit><latexit sha1_base64="s2S1xFtplMgMmo+UlAT1aTEd9R4=">AAACGnicdVBNbxMxEPWWAiEUmsKRi5UIqVLEyptEJIFLRC8cU6n5kLJR5HWcxKrtXdmzRdFq7/wIfgNXOHNDvfbSI/+k3iRIBLVPsvX03oxm5kWJFBYIufUOHh0+fvK09Kz8/OjFy+PKyauhjVPD+IDFMjbjiFouheYDECD5ODGcqkjyUXR5VvijK26siPUFrBM+VXSpxUIwCk6aVarhikIWRipb5/ksg7rG4Uf8ofgcr395F+SzSo347Uan1W1i4pP33VaLFKTZbrQ7OPDJBjW0Q39W+RPOY5YqroFJau0kIAlMM2pAMMnzcphanlB2SZd84qimittptrklx2+dMseL2LinAW/Ufzsyqqxdq8hVKgor+79XiPd5kxQWnWkmdJIC12w7aJFKDDEugsFzYTgDuXaEMiPcrpitqKEMXHx7UyJVZPL3cPwwGTb8gPjBeavW+7RLp4TeoCo6RQFqox76jPpogBj6ir6jH+in98375f32rrelB96u5zXag3dzB4VZoG4=</latexit><latexit sha1_base64="s2S1xFtplMgMmo+UlAT1aTEd9R4=">AAACGnicdVBNbxMxEPWWAiEUmsKRi5UIqVLEyptEJIFLRC8cU6n5kLJR5HWcxKrtXdmzRdFq7/wIfgNXOHNDvfbSI/+k3iRIBLVPsvX03oxm5kWJFBYIufUOHh0+fvK09Kz8/OjFy+PKyauhjVPD+IDFMjbjiFouheYDECD5ODGcqkjyUXR5VvijK26siPUFrBM+VXSpxUIwCk6aVarhikIWRipb5/ksg7rG4Uf8ofgcr395F+SzSo347Uan1W1i4pP33VaLFKTZbrQ7OPDJBjW0Q39W+RPOY5YqroFJau0kIAlMM2pAMMnzcphanlB2SZd84qimittptrklx2+dMseL2LinAW/Ufzsyqqxdq8hVKgor+79XiPd5kxQWnWkmdJIC12w7aJFKDDEugsFzYTgDuXaEMiPcrpitqKEMXHx7UyJVZPL3cPwwGTb8gPjBeavW+7RLp4TeoCo6RQFqox76jPpogBj6ir6jH+in98375f32rrelB96u5zXag3dzB4VZoG4=</latexit>

w
<latexit sha1_base64="ocO9KNmdPOUHA59ncG4uOZRABnI=">AAAB93icdVDLSsNAFJ3UV62vqks3g0VwFSZtMO2u6MZlC/YBbSiT6aQdOnkwM1FC6Be41bU7cevnuPRPnLQVrOiBC4dz7uXee7yYM6kQ+jAKG5tb2zvF3dLe/sHhUfn4pCujRBDaIRGPRN/DknIW0o5iitN+LCgOPE573uwm93v3VEgWhXcqjakb4EnIfEaw0lL7YVSuINOp1u1GDSITXTVsG+Wk5lSdOrRMtEAFrNAalT+H44gkAQ0V4VjKgYVi5WZYKEY4nZeGiaQxJjM8oQNNQxxQ6WaLQ+fwQitj6EdCV6jgQv05keFAyjTwdGeA1VT+9nLxL2+QKL/uZiyME0VDslzkJxyqCOZfwzETlCieaoKJYPpWSKZYYKJ0NmtbvGCuM/l+HP5PulXTQqbVtivN61U6RXAGzsElsIADmuAWtEAHEEDBI3gCz0ZqvBivxtuytWCsZk7BGoz3Lwbnk+Q=</latexit><latexit sha1_base64="ocO9KNmdPOUHA59ncG4uOZRABnI=">AAAB93icdVDLSsNAFJ3UV62vqks3g0VwFSZtMO2u6MZlC/YBbSiT6aQdOnkwM1FC6Be41bU7cevnuPRPnLQVrOiBC4dz7uXee7yYM6kQ+jAKG5tb2zvF3dLe/sHhUfn4pCujRBDaIRGPRN/DknIW0o5iitN+LCgOPE573uwm93v3VEgWhXcqjakb4EnIfEaw0lL7YVSuINOp1u1GDSITXTVsG+Wk5lSdOrRMtEAFrNAalT+H44gkAQ0V4VjKgYVi5WZYKEY4nZeGiaQxJjM8oQNNQxxQ6WaLQ+fwQitj6EdCV6jgQv05keFAyjTwdGeA1VT+9nLxL2+QKL/uZiyME0VDslzkJxyqCOZfwzETlCieaoKJYPpWSKZYYKJ0NmtbvGCuM/l+HP5PulXTQqbVtivN61U6RXAGzsElsIADmuAWtEAHEEDBI3gCz0ZqvBivxtuytWCsZk7BGoz3Lwbnk+Q=</latexit><latexit sha1_base64="ocO9KNmdPOUHA59ncG4uOZRABnI=">AAAB93icdVDLSsNAFJ3UV62vqks3g0VwFSZtMO2u6MZlC/YBbSiT6aQdOnkwM1FC6Be41bU7cevnuPRPnLQVrOiBC4dz7uXee7yYM6kQ+jAKG5tb2zvF3dLe/sHhUfn4pCujRBDaIRGPRN/DknIW0o5iitN+LCgOPE573uwm93v3VEgWhXcqjakb4EnIfEaw0lL7YVSuINOp1u1GDSITXTVsG+Wk5lSdOrRMtEAFrNAalT+H44gkAQ0V4VjKgYVi5WZYKEY4nZeGiaQxJjM8oQNNQxxQ6WaLQ+fwQitj6EdCV6jgQv05keFAyjTwdGeA1VT+9nLxL2+QKL/uZiyME0VDslzkJxyqCOZfwzETlCieaoKJYPpWSKZYYKJ0NmtbvGCuM/l+HP5PulXTQqbVtivN61U6RXAGzsElsIADmuAWtEAHEEDBI3gCz0ZqvBivxtuytWCsZk7BGoz3Lwbnk+Q=</latexit><latexit sha1_base64="ocO9KNmdPOUHA59ncG4uOZRABnI=">AAAB93icdVDLSsNAFJ3UV62vqks3g0VwFSZtMO2u6MZlC/YBbSiT6aQdOnkwM1FC6Be41bU7cevnuPRPnLQVrOiBC4dz7uXee7yYM6kQ+jAKG5tb2zvF3dLe/sHhUfn4pCujRBDaIRGPRN/DknIW0o5iitN+LCgOPE573uwm93v3VEgWhXcqjakb4EnIfEaw0lL7YVSuINOp1u1GDSITXTVsG+Wk5lSdOrRMtEAFrNAalT+H44gkAQ0V4VjKgYVi5WZYKEY4nZeGiaQxJjM8oQNNQxxQ6WaLQ+fwQitj6EdCV6jgQv05keFAyjTwdGeA1VT+9nLxL2+QKL/uZiyME0VDslzkJxyqCOZfwzETlCieaoKJYPpWSKZYYKJ0NmtbvGCuM/l+HP5PulXTQqbVtivN61U6RXAGzsElsIADmuAWtEAHEEDBI3gCz0ZqvBivxtuytWCsZk7BGoz3Lwbnk+Q=</latexit> w

<latexit sha1_base64="ocO9KNmdPOUHA59ncG4uOZRABnI=">AAAB93icdVDLSsNAFJ3UV62vqks3g0VwFSZtMO2u6MZlC/YBbSiT6aQdOnkwM1FC6Be41bU7cevnuPRPnLQVrOiBC4dz7uXee7yYM6kQ+jAKG5tb2zvF3dLe/sHhUfn4pCujRBDaIRGPRN/DknIW0o5iitN+LCgOPE573uwm93v3VEgWhXcqjakb4EnIfEaw0lL7YVSuINOp1u1GDSITXTVsG+Wk5lSdOrRMtEAFrNAalT+H44gkAQ0V4VjKgYVi5WZYKEY4nZeGiaQxJjM8oQNNQxxQ6WaLQ+fwQitj6EdCV6jgQv05keFAyjTwdGeA1VT+9nLxL2+QKL/uZiyME0VDslzkJxyqCOZfwzETlCieaoKJYPpWSKZYYKJ0NmtbvGCuM/l+HP5PulXTQqbVtivN61U6RXAGzsElsIADmuAWtEAHEEDBI3gCz0ZqvBivxtuytWCsZk7BGoz3Lwbnk+Q=</latexit><latexit sha1_base64="ocO9KNmdPOUHA59ncG4uOZRABnI=">AAAB93icdVDLSsNAFJ3UV62vqks3g0VwFSZtMO2u6MZlC/YBbSiT6aQdOnkwM1FC6Be41bU7cevnuPRPnLQVrOiBC4dz7uXee7yYM6kQ+jAKG5tb2zvF3dLe/sHhUfn4pCujRBDaIRGPRN/DknIW0o5iitN+LCgOPE573uwm93v3VEgWhXcqjakb4EnIfEaw0lL7YVSuINOp1u1GDSITXTVsG+Wk5lSdOrRMtEAFrNAalT+H44gkAQ0V4VjKgYVi5WZYKEY4nZeGiaQxJjM8oQNNQxxQ6WaLQ+fwQitj6EdCV6jgQv05keFAyjTwdGeA1VT+9nLxL2+QKL/uZiyME0VDslzkJxyqCOZfwzETlCieaoKJYPpWSKZYYKJ0NmtbvGCuM/l+HP5PulXTQqbVtivN61U6RXAGzsElsIADmuAWtEAHEEDBI3gCz0ZqvBivxtuytWCsZk7BGoz3Lwbnk+Q=</latexit><latexit sha1_base64="ocO9KNmdPOUHA59ncG4uOZRABnI=">AAAB93icdVDLSsNAFJ3UV62vqks3g0VwFSZtMO2u6MZlC/YBbSiT6aQdOnkwM1FC6Be41bU7cevnuPRPnLQVrOiBC4dz7uXee7yYM6kQ+jAKG5tb2zvF3dLe/sHhUfn4pCujRBDaIRGPRN/DknIW0o5iitN+LCgOPE573uwm93v3VEgWhXcqjakb4EnIfEaw0lL7YVSuINOp1u1GDSITXTVsG+Wk5lSdOrRMtEAFrNAalT+H44gkAQ0V4VjKgYVi5WZYKEY4nZeGiaQxJjM8oQNNQxxQ6WaLQ+fwQitj6EdCV6jgQv05keFAyjTwdGeA1VT+9nLxL2+QKL/uZiyME0VDslzkJxyqCOZfwzETlCieaoKJYPpWSKZYYKJ0NmtbvGCuM/l+HP5PulXTQqbVtivN61U6RXAGzsElsIADmuAWtEAHEEDBI3gCz0ZqvBivxtuytWCsZk7BGoz3Lwbnk+Q=</latexit><latexit sha1_base64="ocO9KNmdPOUHA59ncG4uOZRABnI=">AAAB93icdVDLSsNAFJ3UV62vqks3g0VwFSZtMO2u6MZlC/YBbSiT6aQdOnkwM1FC6Be41bU7cevnuPRPnLQVrOiBC4dz7uXee7yYM6kQ+jAKG5tb2zvF3dLe/sHhUfn4pCujRBDaIRGPRN/DknIW0o5iitN+LCgOPE573uwm93v3VEgWhXcqjakb4EnIfEaw0lL7YVSuINOp1u1GDSITXTVsG+Wk5lSdOrRMtEAFrNAalT+H44gkAQ0V4VjKgYVi5WZYKEY4nZeGiaQxJjM8oQNNQxxQ6WaLQ+fwQitj6EdCV6jgQv05keFAyjTwdGeA1VT+9nLxL2+QKL/uZiyME0VDslzkJxyqCOZfwzETlCieaoKJYPpWSKZYYKJ0NmtbvGCuM/l+HP5PulXTQqbVtivN61U6RXAGzsElsIADmuAWtEAHEEDBI3gCz0ZqvBivxtuytWCsZk7BGoz3Lwbnk+Q=</latexit>

ŷ t+
1
: t

+
w

<latexit sha1_base64="cTROMIMSZgjgfe09GK2qjFosOLo=">AAACFnicdVBNSwMxEM3W7/pV9ShIsAiCsGTbYlu9iF48VrAqdEvJpmkbmuwuyaxSlr35I/wNXvXsTbx69eg/cbdWUNEHCY/3ZpiZ54VSGCDkzcpNTc/Mzs0v5BeXlldWC2vrFyaINONNFshAX3nUcCl83gQBkl+FmlPlSX7pDU8y//KaayMC/xxGIW8r2vdFTzAKqdQpbLkDCrHrqXiUJJ0Y9hzsHuKD7IO9m6RTKBK7WqpV6mVMbLJfr1RIRsrVUrWGHZuMUUQTNDqFd7cbsEhxH5ikxrQcEkI7phoEkzzJu5HhIWVD2uetlPpUcdOOx3ckeCdVurgX6PT5gMfq946YKmNGyksrFYWB+e1l4l9eK4JerR0LP4yA++xzUC+SGAKchYK7QnMGcpQSyrRId8VsQDVlkEb3Y4qnsky+Dsf/k4uS7RDbOasUj44n6cyjTbSNdpGDqugInaIGaiKGbtE9ekCP1p31ZD1bL5+lOWvSs4F+wHr9AN1EnxI=</latexit><latexit sha1_base64="cTROMIMSZgjgfe09GK2qjFosOLo=">AAACFnicdVBNSwMxEM3W7/pV9ShIsAiCsGTbYlu9iF48VrAqdEvJpmkbmuwuyaxSlr35I/wNXvXsTbx69eg/cbdWUNEHCY/3ZpiZ54VSGCDkzcpNTc/Mzs0v5BeXlldWC2vrFyaINONNFshAX3nUcCl83gQBkl+FmlPlSX7pDU8y//KaayMC/xxGIW8r2vdFTzAKqdQpbLkDCrHrqXiUJJ0Y9hzsHuKD7IO9m6RTKBK7WqpV6mVMbLJfr1RIRsrVUrWGHZuMUUQTNDqFd7cbsEhxH5ikxrQcEkI7phoEkzzJu5HhIWVD2uetlPpUcdOOx3ckeCdVurgX6PT5gMfq946YKmNGyksrFYWB+e1l4l9eK4JerR0LP4yA++xzUC+SGAKchYK7QnMGcpQSyrRId8VsQDVlkEb3Y4qnsky+Dsf/k4uS7RDbOasUj44n6cyjTbSNdpGDqugInaIGaiKGbtE9ekCP1p31ZD1bL5+lOWvSs4F+wHr9AN1EnxI=</latexit><latexit sha1_base64="cTROMIMSZgjgfe09GK2qjFosOLo=">AAACFnicdVBNSwMxEM3W7/pV9ShIsAiCsGTbYlu9iF48VrAqdEvJpmkbmuwuyaxSlr35I/wNXvXsTbx69eg/cbdWUNEHCY/3ZpiZ54VSGCDkzcpNTc/Mzs0v5BeXlldWC2vrFyaINONNFshAX3nUcCl83gQBkl+FmlPlSX7pDU8y//KaayMC/xxGIW8r2vdFTzAKqdQpbLkDCrHrqXiUJJ0Y9hzsHuKD7IO9m6RTKBK7WqpV6mVMbLJfr1RIRsrVUrWGHZuMUUQTNDqFd7cbsEhxH5ikxrQcEkI7phoEkzzJu5HhIWVD2uetlPpUcdOOx3ckeCdVurgX6PT5gMfq946YKmNGyksrFYWB+e1l4l9eK4JerR0LP4yA++xzUC+SGAKchYK7QnMGcpQSyrRId8VsQDVlkEb3Y4qnsky+Dsf/k4uS7RDbOasUj44n6cyjTbSNdpGDqugInaIGaiKGbtE9ekCP1p31ZD1bL5+lOWvSs4F+wHr9AN1EnxI=</latexit><latexit sha1_base64="cTROMIMSZgjgfe09GK2qjFosOLo=">AAACFnicdVBNSwMxEM3W7/pV9ShIsAiCsGTbYlu9iF48VrAqdEvJpmkbmuwuyaxSlr35I/wNXvXsTbx69eg/cbdWUNEHCY/3ZpiZ54VSGCDkzcpNTc/Mzs0v5BeXlldWC2vrFyaINONNFshAX3nUcCl83gQBkl+FmlPlSX7pDU8y//KaayMC/xxGIW8r2vdFTzAKqdQpbLkDCrHrqXiUJJ0Y9hzsHuKD7IO9m6RTKBK7WqpV6mVMbLJfr1RIRsrVUrWGHZuMUUQTNDqFd7cbsEhxH5ikxrQcEkI7phoEkzzJu5HhIWVD2uetlPpUcdOOx3ckeCdVurgX6PT5gMfq946YKmNGyksrFYWB+e1l4l9eK4JerR0LP4yA++xzUC+SGAKchYK7QnMGcpQSyrRId8VsQDVlkEb3Y4qnsky+Dsf/k4uS7RDbOasUj44n6cyjTbSNdpGDqugInaIGaiKGbtE9ekCP1p31ZD1bL5+lOWvSs4F+wHr9AN1EnxI=</latexit>

Figure 2: Illustration of SBS: (a) wait-k policy (Eqs. 4–
5); (b) adaptive policy (Eqs. 6–7). Speculations in red.

3.1 Single-Step SBS
The wait-k policy conducts translation concur-
rently with the source input, committing output
words one by one while the source sentence is still
growing. In this case, conventional beam search is
clearly inapplicable.

We propose to perform speculative beam search
at each step by hallucinating w more steps into
the future, and use the ranking after these w + 1
steps to make a more informed decision for the
current step. More formally, at step t, we generate
yt based on already committed prefix y<t:

〈ŷ, st〉 = top1
(
nextb1+w([〈y<t, 1〉])

)
(4)

y≤t = y<t ◦ ŷt (5)

where ŷ = y<t ◦ ŷt ◦ ŷt+1:t+w has three parts,
with the last one being a speculation of w steps
(see Fig. 2). We use nextb1+w(·) to speculate w
steps. The candidate ŷt is selected based on the
accumulative model score w steps later. Then we
commit ŷt and move on to step t+ 1.

In the running example in Fig. 1, we have w = 2
and b = 3. In the greedy mode, after the wait-
1 policy receives the first source word, “世行”
(world bank), the basic wait-1 model commits
“bank” which has the highest score. In SBS, we
perform a beam search for 1 + w = 3 steps with
the two speculative steps marked in red. After 3
steps, the path “world bank will” becomes the top
candidate, thus we choose to commit “world” in-
stead of “bank” and restart a new speculative beam
search with “world” when we receive a new source
word, “拟”(plan to); the speculative part from the
previous step (in red) is removed.

3.2 Chunk-based SBS
The RL-based adaptive policy system (Gu et al.,
2017) can commit a chunk of multiple words
whenever there is a series of consecutive WRITEs,
and conventional beam search can be applied on
each chunk to improve the search quality within
that chunk, which is already used in that work.

However, on top of the obvious per-chunk beam
search, we can still apply SBS to further speculate

w steps after the chunk. For a chunk of length n
starting at position t, we adapt SBS as:

〈ŷ, st〉 = top1
(
nextbn+w([〈y<t, 1〉])

)
(6)

y≤t+n−1 = y<t ◦ ŷt:t+n−1 (7)

Here nextbn+w(·) does a beam search of n + w
steps, with the last w steps speculated. Similarly,

ŷ = y<t ◦ ŷt:t+n−1 ◦ ŷt+n:t+n+w−1

has three parts, with the last being a speculation of
w steps, and the middle one being the chunk of n
steps returned and committed (see Fig. 2).

4 Experiments

4.1 Datasets and Latency Metrics

We evaluate our work on Chinese↔English simul-
taneous translation tasks. For the training data, we
use the NIST corpus for Chinese↔English (2M
sentence pairs). We first apply BPE (Sennrich
et al., 2015) on all texts in order to reduce the
vocabulary sizes. For Chinese↔English evalua-
tion, we use NIST 2006 and NIST 2008 as our
dev and test sets with 4 English references. For
English→Chinese, we use the second among the
four English references as the source text.

We re-implement wait-k model (Ma et al.,
2019a), test-time wait-k model (Dalvi et al., 2018)
and adaptive policy (Gu et al., 2017) based on
PyTorch-based OpenNMT (Klein et al., 2017). To
reach state-of-the-art performance, we use Trans-
former based wait-k model and also use Trans-
former based pre-trained full sentence model for
learning adaptive policy. The architecture of
Transformer is the same as the base model from
the original paper (Vaswani et al., 2017). We use
Average Lagging (AL) (Ma et al., 2019a) as the la-
tency metrics. AL measures the number of words
delay for translating a given source sentence.

b
w 0 1 2 3 4 5

1 34.57 - - - - -
3 - +1.3 +1.8 +1.2 +2.0 +1.7
5 - +1.6 +1.9 +1.3 +1.5 +1.3
7 - +1.5 +2.0 +1.0 +1.6 +1.4
10 - +1.4 +2.2 +1.4 +1.5 +1.7

Table 2: Zh→En wait-1 model BLEU improvement of
SBS against greedy search (b = 1, w = 0) on dev-set.
When w ≥ 5 the performance of SBS becomes stable.



1398

2 4 6 8
Average Lagging (zh en)

15

20

25

30

35

40
4-

re
f B

LE
U

k=1

k=3

k=5
k=7

Wait-k
Test-time wait-k

29.6 2 4 6 8
Average Lagging (en zh)

7.5

10.0

12.5

15.0

17.5

20.0

22.5

1-
re

f B
LE

U

k=1

k=3
k=5

k=7

Wait-k
Test-time wait-k

38.3

Figure 3: BLEU against AL using wait-k model. � �: conventional beam search only in target tail (when source
finishes). 44: speculative beam search. FI:full-sentence baseline (greedy and beam-search).

4 6 8 10
Average Lagging (zh en)

15

20

25

30

35

40

45

4-
re

f B
LE

U

Wait-k
Adaptive CW=2
Adaptive CW=5
Adaptive CW=8

29.6 4 6 8 10
Average Lagging (en zh)

10

12

14

16

18

20

22

24

1-
re

f B
LE

U

Wait-k
Adaptive CW=2
Adaptive CW=5

38.3

Figure 4: BLEU against AL using adaptive policy (compared with wait-k models) with different beam search
methods. � � �: conventional beam search in chunk of consecutive write (Gu et al., 2017). 444: speculative
beam search. FI:full-sentence baseline (greedy and beam-search).

shı̀háng nı̌ jı̌anmı̌an zùi qióng gúojiā zhàiwù

世行 拟 减免 最 穷 国家 债务
Gloss world bank plan to remit & reduce most poor country debt

k=1†
Greedy world bank to reduce poverty - stricken countries

SBS world bank to exemp- t po- or- est countries from debt

k=1‡
Greedy world bank to reduce or exemp- t debt of po- or- est countries

SBS world bank inten- ds to reduce or exemp- t debt of po- or- est countries

k=∞∗ world bank plans to remit and reduce debts of po- or- est countries

Figure 5: Chinese-to-English example on dev set. †: test-time wait-k; ‡: wait-k. ∗: full-sentence beam search.

4.2 Performance on Wait-k Policy

We perform experiments on validation set using
speculative beam search (SBS) with beam sizes
b ∈ {3, 5, 7, 10} and speculative window sizes
w ∈ {1, 2, 3, 4, 5}. Table 2 shows the BLEU score
of different b and w over wait-1 model. Compared
with greedy decoding, SBS improves at least 1.0
BLEU score in all cases and achieves best perfor-
mance by b = 10, w = 2. We search the best b
and w for each model on dev-set and apply them
on test-set in the following experiments.

Fig. 3 shows the performance of conventional
greedy decoding, trivial tail beam search (only
after source sentence is finished) and SBS on

test set on Chinese↔English tasks. SBS largely
boost test-time wait-k models with slightly worse
latency (especially in English→Chinese because
they tend to generate longer sentences). Wait-k
models also benefit from speculation (especially
in Chinese→English).

Fig. 5 shows a running example of greedy and
SBS output of both wait-k and test-time wait-
k models. SBS on test-time wait-k generates
much better outputs than the greedy search, which
misses some essential information. Wait-k mod-
els with speculation correctly translates “拟” into
“intends to” instead of “to” in greedy output.



1399

0 10 20 30 40 50
Target Word Index (GPU)

0.01

0.02

0.03

0.04

0.05
Ti

m
e 

(s
ec

)

Encoding
Encoding + Geedy Decoding
Encoding + SBS Decoding

0 10 20 30 40 50
Target Word Index (CPU)

0.00

0.05

0.10

0.15

0.20

Ti
m

e 
(s

ec
)

Encoding
Encoding + Greedy Decoding
Encoding + SBS Decoding

Figure 6: Average time of words with different indices (≤ 50) on zh→en wait-3 model. Results in left figure are
measured on GPU while results in right figure results are measured on CPU. The SBS results adopt w = 2, b = 5.

w=0

w=1

w=3

w=5
w=7

w=9

Normal Beam Search

Figure 7: BLEU of SBS over full sentence translation
with different window sizes w and beam sizes b. Win-
dow size w = 0 stands for greedy decoding. The top
line stands for results using normal beam search.

4.3 Performance on Adaptive Policy
Fig. 4 shows the performance of proposed SBS
on adaptive policies. We train adaptive policies
using the combination of Consecutive Wait (CW
∈ {2, 5, 8} (Gu et al., 2017)) and partial-BLEU as
reward in reinforcement learning. We vary beam
size b ∈ {5, 10} in both chunk-based beam search
(Gu et al., 2017) and our SBS with speculative
window size w ∈ {2, 4}. Our proposed beam
search achieves better results in most cases.

4.4 Running Time Analysis
Fig. 6 shows the average time for generating words
with different target word indices on a GeForce
GTX TITAN-X GPU and an Intel Core i7 2.8 GHz
CPU. According to Ma et al. (2019a), wait-k mod-
els use bi-directional Transformer as the encoder,
thus the time complexity of incrementally encod-
ing one more source word is O(m2) where m is
the source sentence length. This is the reason why
it takes more time to encode words with larger in-
dex especially using CPU. It is generally accepted
that Mandarin speech is about 120–150 syllables
per minute, and in our corpus each token (after

BPE) has on average 1.5 Chinese syllables (which
is 1.5 characters since each Chinese character is
monosyllabic), thus in the simultaneous Chinese-
to-English speech-to-text translation scenario, the
decoder receives a source token every 0.6–0.75
seconds which is much slower than our decoding
speed (less than 0.25 seconds per token) even on
a laptop CPU. Based on these statistics, our pro-
posed speculative beam search algorithm can be
used in real simultaneous translation.

4.5 Performance on Full Sentence MT
We analyze the performance of speculative beam
search on full-sentence translation (see Fig. 7). By
only performing beam search on a sliding spec-
ulative window, the proposed algorithm achieves
much better BLEU scores compared with greedy
decoding (w = 0) and even outperforms conven-
tional beam search when w = 9, b = 3. Please
note that the space complexity of this algorithm
is O((m + n + wb)d).1 This is better than con-
ventional beam search whose space complexity is
O((m+ nb)d) when w � n.

5 Conclusions and Future Work
We have proposed speculative beam search for si-
multaneous translation. Experiments on three ap-
proaches to simultaneous translation demonstrate
effectiveness of our method. This algorithm has
the potential in other incremental tasks such as
streaming ASR and incremental TTS.

Acknowledgments
We thank Kaibo Liu for his AL script2 and help
in training wait-k models, and the anonymous re-
viewers for suggestions.

1Here n is the length of target sentence and d is the repre-
sentation dimension.

2
https://github.com/SimulTrans-demo/STACL

https://github.com/SimulTrans-demo/STACL


1400

References
Naveen Arivazhagan, Colin Cherry, Wolfgang

Macherey, Chung-Cheng Chiu, Semih Yavuz,
Ruoming Pang, Wei Li, and Colin Raffel. 2019.
Monotonic infinite lookback attention for si-
multaneous machine translation. arXiv preprint
arXiv:1906.05218.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Srinivas Bangalore, Vivek Kumar Rangarajan Srid-
har, Prakash Kolan, Ladan Golipour, and Aura
Jimenez. 2012. Real-time incremental speech-to-
speech translation of dialogs. In Proc. of NAACL-
HLT.

Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Ro-
hit Prabhavalkar, Patrick Nguyen, Zhifeng Chen,
Anjuli Kannan, Ron J Weiss, Kanishka Rao, Eka-
terina Gonina, et al. 2018. State-of-the-art speech
recognition with sequence-to-sequence models. In
2018 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), pages
4774–4778. IEEE.

Fahim Dalvi, Nadir Durrani, Hassan Sajjad, and
Stephan Vogel. 2018. Incremental decoding and
training methods for simultaneous translation in
neural machine translation. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 2 (Short Pa-
pers), pages 493–499, New Orleans, Louisiana. As-
sociation for Computational Linguistics.

Alvin Grissom II, He He, Jordan Boyd-Graber, John
Morgan, and Hal Daumé III. 2014. Don’t until the
final verb wait: Reinforcement learning for simul-
taneous machine translation. In Proceedings of the
2014 Conference on empirical methods in natural
language processing (EMNLP), pages 1342–1352.

Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Vic-
tor O. K. Li. 2017. Learning to translate in real-
time with neural machine translation. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL 2017, Valencia, Spain, April 3-7, 2017, Vol-
ume 1: Long Papers, pages 1053–1062.

Liang Huang, Kai Zhao, and Mingbo Ma. 2017. When
to finish? optimal beam search for neural text gener-
ation (modulo beam size). In EMNLP.

Navdeep Jaitly, David Sussillo, Quoc V Le, Oriol
Vinyals, Ilya Sutskever, and Samy Bengio. 2016.
An online sequence-to-sequence model using par-
tial conditioning. In Advances in Neural Informa-
tion Processing Systems, pages 5067–5075.

G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M.
Rush. 2017. OpenNMT: Open-Source Toolkit for
Neural Machine Translation. ArXiv e-prints.

Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng,
Kaibo Liu, Baigong Zheng, Chuanqiang Zhang,
Zhongjun He, Hairong Liu, Xing Li, Hua Wu, and
Haifeng Wang. 2019a. STACL: Simultaneous trans-
lation with implicit anticipation and controllable la-
tency using prefix-to-prefix framework. In Proceed-
ings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 3025–3036.

Mingbo Ma, Renjie Zheng, and Liang Huang. 2019b.
Learning to stop in structured prediction for neu-
ral machine translation. In Proceedings of the 2019
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short
Papers), pages 1884–1889.

Kenton Murray and David Chiang. 2018. Correcting
length bias in neural machine translation. In Pro-
ceedings of WMT 2018.

Yusuke Oda, Graham Neubig, Sakriani Sakti, Tomoki
Toda, and Satoshi Nakamura. 2015. Syntax-based
simultaneous translation through prediction of un-
seen syntactic constituents. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), volume 1, pages 198–207.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2016. Sequence level train-
ing with recurrent neural networks. ICLR.

Alexander M Rush, Sumit Chopra, and Jason We-
ston. 2015. A neural attention model for ab-
stractive sentence summarization. arXiv preprint
arXiv:1509.00685.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015. Neural machine translation of rare words with
subword units. arXiv preprint arXiv:1508.07909.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 3156–3164.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron C Courville, Ruslan Salakhutdinov, Richard S
Zemel, and Yoshua Bengio. 2015. Show, attend and
tell: Neural image caption generation with visual at-
tention. In ICML, volume 14, pages 77–81.

https://doi.org/10.18653/v1/N18-2079
https://doi.org/10.18653/v1/N18-2079
https://doi.org/10.18653/v1/N18-2079
https://aclanthology.info/papers/E17-1099/e17-1099
https://aclanthology.info/papers/E17-1099/e17-1099
http://arxiv.org/abs/1701.02810
http://arxiv.org/abs/1701.02810


1401

Baigong Zheng, Renjie Zheng, Mingbo Ma, and Liang
Huang. 2019a. Simpler and faster learning of adap-
tive policies for simultaneous translation. In Pro-
ceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and 9th In-
ternational Joint Conference on Natural Language
Processing.

Baigong Zheng, Renjie Zheng, Mingbo Ma, and Liang
Huang. 2019b. Simultaneous translation with flexi-
ble policy via restricted imitation learning. In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 5816–
5822.



1402

A Supplemental Material

We also evaluate our work using Consecutive Wait
(CW) as latency metric, which measures the aver-
age lengths of consecutive wait segments, and per-
form experiments on German↔English corpora
available from WMT153. We use newstest-2013
as dev-set and newstest-2015 as test-set.4

Fig. 8 show the translation quality on
German↔English against AL of different
decoding methods. Consistent to the results of
Chinese↔English, our proposed speculative beam
search gain large performance boost especially
on test-time wait-k. Fig. 9 and Fig. 10 use CW
as latency metrics. Since both the wait-k and
test-time wait-k models use the same fixed policy,
the CW latencies of the same k are identical.

2 4 6 8
Average Lagging (de en)

15

20

25

30

1-
re

f B
LE

U

k=1

k=3

k=5
k=7

Wait-k
Test-time wait-k

28.6

2 4 6 8
Average Lagging (en de)

10

15

20

25

1-
re

f B
LE

U

k=1

k=3

k=5
k=7

Wait-k
Test-time wait-k

26.6

Figure 8: Translation quality against AL on
English↔German simultaneous translation using wait-
k model. ��: conventional beam search only on target
tail. 44: speculative beam search. FI:full-sentence
(greedy and beam-search).

3
http://www.statmt.org/wmt15/translation-task.html

4The German↔English results are slightly different from
those in Ma et al. (2019a) because of different decoding
settings. We do not allow that the decoder stops earlier
than the finish of source sentence while it is allowed in
German↔English experiments of Ma et al. (2019a). This
makes our generated sentences longer and further results in
worse AL compared with the results in Ma et al. (2019a).

1.00 1.25 1.50 1.75
Consecutive Wait (zh en)

15

20

25

30

35

40

4-
re

f B
LE

U

k=1

k=3

k=5
k=7

Wait-k
Test-time wait-k

29.6

1.4 1.6 1.8 2.0 2.2
Consecutive Wait (en zh)

7.5

10.0

12.5

15.0

17.5

20.0

22.5

1-
re

f B
LE

U

k=1

k=3
k=5

k=7

Wait-k
Test-time wait-k

38.3

Figure 9: Translation quality against CW on
Chinese↔English simultaneous translation using wait-
k model.

1.00 1.25 1.50 1.75 2.00
Consecutive Wait (de en)

15

20

25

30

1-
re

f B
LE

U

k=1

k=3

k=5
k=7

Wait-k
Test-time wait-k

28.6

1.00 1.25 1.50 1.75
Consecutive Wait (en de)

10

15

20

25

1-
re

f B
LE

U

k=1

k=3

k=5
k=7

Wait-k
Test-time wait-k

26.6

Figure 10: Translation quality against CW on
English↔German simultaneous translation using wait-
k model.

http://www.statmt.org/wmt15/translation-task.html

