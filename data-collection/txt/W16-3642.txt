



















































Reference Resolution in Situated Dialogue with Learned Semantics


Proceedings of the SIGDIAL 2016 Conference, pages 329–338,
Los Angeles, USA, 13-15 September 2016. c©2016 Association for Computational Linguistics

Reference Resolution in Situated Dialogue with Learned Semantics

Xiaolong Li
Computer & Information Science

& Engineering
University of Florida

xiaolongl@ufl.edu

Kristy Elizabeth Boyer
Computer & Information Science

& Engineering
University of Florida
keboyer@ufl.edu

Abstract

Understanding situated dialogue requires
identifying referents in the environment to
which the dialogue participants refer. This
reference resolution problem, often in a
complex environment with high ambigu-
ity, is very challenging. We propose an
approach that addresses those challenges
by combining learned semantic structure
of referring expressions with dialogue his-
tory into a ranking-based model. We eval-
uate the new technique on a corpus of
human-human tutorial dialogues for com-
puter programming. The experimental re-
sults show a substantial performance im-
provement over two recent state-of-the-art
approaches. The proposed work makes a
stride toward automated dialogue in com-
plex problem-solving environments.

1 Introduction

The content of a situated dialogue is very closely
related to the environment in which it happens
(Grosz and Sidner, 1986). As dialogue systems
move toward assisting users in increasingly com-
plex tasks, these systems must understand users’
language within the environment of the tasks. To
achieve this goal, dialogue systems must perform
reference resolution, which involves identifying
the referents in the environment that the user refers
to (Iida et al., 2010; Liu et al., 2014; Liu and Chai,
2015; Chai et al., 2004). Imagine a dialogue sys-
tem that assists a novice student in solving a pro-
gramming problem. To understand a question or
statement the student poses, such as, “Should I use
the 2 dimensional array?”, the system must link
the referring expression “the 2 dimensional array”
to an object1 in the environment.

1The word “object” has a technical meaning within the
domain of object-oriented programming, which is the domain

This process is illustrated in Figure 1, which
shows an excerpt from a corpus of tutorial dia-
logue situated in an introductory computer pro-
gramming task in the Java programming language.
The arrows link referring expressions in the sit-
uated dialogue to their referents in the environ-
ment. To identify the referent of each referring
expression, it is essential to capture the semantic
structure of the referring expression of the object
it refers to, such as “the 2 dimensional array” con-
tains two attributes, “2 dimensional” and “array”.
At the same time, the dialogue history and the his-
tory of user task actions (such as editing the code)
play a key role. To disambiguate the referent of
“my array”, temporal information is needed: in
this case, the referent is a variable named arra,
which is an array that the student has just created.

Reference resolution in situated dialogue is
challenging because of the ambiguity inherent
within dialogue utterances and the complexity of
the environment. Prior work has leveraged dia-
logue history and task history information to im-
prove the accuracy of reference resolution (Iida et
al., 2010; Iida et al., 2011; Funakoshi et al., 2012).
However, these prior approaches have employed
relatively simple semantic information from the
referring expressions, such as a manually created
lexicon, or have operated within an environment
with a limited set of pre-defined objects. Besides
reference resolution in situated dialogue, there is
also a research direction in which machine learn-
ing models are used to learn the semantics of
noun phrases in order to map noun phrases to ob-
jects in a related environment (Kennington and
Schlangen, 2015; Liang et al., 2009; Naim et al.,
2014; Kushman et al., 2014). However, these prior
approaches operated at the granularity of single

of the corpus utilized in this work. However, we follow the
standard usage of “object” in situated dialogue (Iida et al.,
2010), which for programming is any portion of code in the
environment.

329



	
	
	
	

Tutor: 
Tutor: 
 
… 

 
 
Tutor: 
… 
 
 
Student: 
 
Tutor: 
 
Student: 

table = new int[10][5]; 
that is where they initialize the size of the 2 
dimensional array 

… 

[student adds line of code: 
arra = new int[s.length()];] 

great! 
… 
[student adds line of code: 
new2=Integer.parseInt(parse1);] 

does my array look like it is set up correctly 
now 
umm...... in the for loop, what should you be 
storing in the array? 

:) 
 

	

                        
 
        setTitle("Postal Code Generator"); 
        setDefaultCloseOperation(EXIT_ON_CLOSE); 
        setVisible(true); 
        
        table = new int[10][5]; 
        initTable(); 
    } 
 
    /** 
     * Extract the individual digits stored in the ZIP code 
     * and store their values as private data 
     */ 
    private void extractDigits() { 
  //You must complete this method!! 
     String s = Integer.toString(zipCode); 
     String parse1; 
     Char num; 
     int arra[]; 
     int new2; 
     arra = new int[s.length()]; 
      
     for(int i=0, i<s.length(); i++) 
     { 
      num=s.charAt(i); 
      parse1=""+num; 
      new2=Integer.parseInt(parse1); 
      arra[i]=num; 
     } 
      

Dialogue and task history Environment 

Figure 1 Excerpt of tutorial dialogue illustrating reference resolution. Referring expressions are shown
in bold italics.2

spoken utterances not contextualized within a dia-
logue history, and they too focus on environments
with a limited number (and a pre-defined set) of
objects. As this paper demonstrates, these prior
approaches do not perform well in situated dia-
logues for complex problem solving, in which the
user creates, modifies, and removes objects from
the environment in unpredictable ways.

To tackle the problem of reference resolu-
tion in this type of situated dialogue, we pro-
pose an approach that combines semantics from
a conditional-random-field-based semantic parser
along with salient features from dialogue history
and task history. We evaluate this approach on
the JavaTutor corpus, a corpus of textual tutorial
dialogue collected within an online environment
for computer programming. The results show that
our approach achieves substantial improvement
over two existing state-of-the-art approaches, with
existing approaches achieving 55.2% accuracy at
best, and the new approach achieving 68.5% accu-
racy.

2Typos and syntactic errors are shown as they appear in
the original corpus.

2 Related Work

The work in this paper is informed by research in
coreference resolution for text as well as reference
resolution in situated dialogue and multi-modal
environments. This section describes related work
in those areas.

The classic reference resolution problem for
discourse aims to resolve coreference relation-
ships within a given text (Martschat and Strube,
2015; McCarthy and Lehnert, 1995; Soon et al.,
2001). Effective approaches for discourse cannot
be directly applied to the problem of linking re-
ferring expressions to their referents in a rich situ-
ated dialogue environment, because the informa-
tion embedded within the environment plays an
important role in understanding the referring rela-
tionships in the situated dialogue. Our approach
combines referring expressions’ semantic infor-
mation along with dialogue history, task history,
and a representation of the environment in which
the dialogue is situated.

Reference resolution in dialogue has been in-
vestigated in recent years. Some of the previous
work focuses on reference resolution in a multi-
modal setting (Chai et al., 2004; Liu et al., 2014;
Liu et al., 2013; Krishnamurthy and Kollar, 2013;
Matuszek et al., 2012). For this problem re-

330



searchers have used multimodal information, in-
cluding vision, gestures, speech, and eye gaze, to
contribute to the problem of reference resolution.
Given that the focus of these works is on employ-
ing rich multimodal information, the research is
usually conducted on a limited number of objects,
and typically uses spatial relationship between ob-
jects as constraints to solve the reference resolu-
tion problem. We conduct reference resolution in
an environment with a dynamic number of refer-
ents and there is no obvious spatial relationship
between the objects.

More closely related work to our own involves
reference resolution in dialogue situated within a
collaborative game (Iida et al., 2010; Iida et al.,
2011; Funakoshi et al., 2012). To link referring
expressions to one of the seven gamepiece ob-
jects, they encoded dialogue history and task his-
tory, and our proposed approach leverages these
features as well. However, in contrast to our com-
plex problem-solving domain of computer pro-
gramming, their domain has a small number of
possible referents, so they used a manually cre-
ated lexicon to extract semantic information from
referring expressions. Funakoshi et al. (2012)
went further, using Bayesian networks to model
the relationship between referents and words used
in referring expressions. That model is based on
a hand-crafted concept dictionary and distribution
over different referents. This approach cannot be
directly applied to a dialogue with a dynamic en-
vironment because it is not possible to manually
define the distribution over all possible referents
beforehand, since objects in the environment are
not known before they are created. So we chose
Iida et al.’s work (2010) as one of the two most
recent approaches to compare with.

Another closely related research direction in-
volves reference resolution in physical environ-
ments (Kennington and Schlangen, 2015; Kush-
man et al., 2014; Naim et al., 2014; Liang et al.,
2009). Although not within situated dialogue per
se (because only one participant speaks), these
lines of investigation have produced approaches
that link natural language noun phrases to objects
in an environment, such as a set of objects of dif-
ferent type and color on a table (Kennington and
Schlangen, 2015) or a variable in a mathemati-
cal formula (Kushman et al., 2014). Some of
these learn the mapping relationship by learning
the semantics of words in the referring expressions

(Kennington and Schlangen, 2015; Liang et al.,
2009) with referring expression-referent pairs as
input. Most recently, Kennington and Schlangen
(2015) used a word-as-classifier approach to learn
word semantics to map referring expressions to a
set of 36 Pentomino puzzle pieces on a table. We
implement their word-as-classifier approach and
compare it with our novel approach.

3 Reference Resolution Approach

This section describes a new approach to refer-
ence resolution in situated dialogue. It links each
referring expression from the dialogue to a most
likely referent object in the environment. Our ap-
proach involves three main steps. First, referring
expressions from the situated dialogue are seg-
mented and labeled according to their semantic
structure. Using a semantic segmentation and la-
beling approach we have previously developed (Li
and Boyer, 2015), we use a conditional random
field (CRF) for this joint segmentation and label-
ing task, and the values of the labeled attributes
are then extracted (Section 3.1). The result of
this step is learned semantics, which are attributes
of objects expressed within each referring expres-
sion. Then, these learned semantics are utilized
within the novel approach reported in this paper.
As Section 3.2 describes, dialogue and task history
are used to filter the objects in the environment to
build a candidate list of referents, and then as Sec-
tion 3.3 describes, a ranking-based classification
approach is used to select the best matching refer-
ent.

For situated dialogue we define Et as the state
of the environment at time t. Et consists of all
objects present in the environment. Importantly,
the objects in the environment vary along with the
dialogue: at each moment, new objects could be
created (|Et| > |Et−1|), and existing objects could
be removed (|Et| < |Et−1|) because of the task
performed by the user.

Et = {oi|oi is an object in the environment at time t}

We assume that all of the objects oi are observ-
able in the environment. For example, in situ-
ated dialogues about programming, we can find
all of the objects and extract their attributes using
a source code parser. Then, reference resolution
is defined as finding a best-matching oi in Et for
referring expression RE.

331



3.1 Referring Expression Semantic
Interpretation

In situated dialogues, a referring expression may
contain rich semantic information about the ref-
erent, especially when the context of the situated
dialogue is complex. Approaches such as domain-
specific lexicons are limited in their ability to ad-
dress this complexity, so we utilize a linear-chain
CRF to parse the semantic structure of the refer-
ring expression. This more automated approach
can also potentially avoid the manual labor re-
quired in creating and maintaining a lexicon.

In this approach, every object within the envi-
ronment must be represented according to its at-
tributes. We treat the set of all possible attributes
of objects as a vector, and for each object oi in
the environment we instantiate and populate an at-
tribute vector Att V eci. For example, the attribute
vector for a two-dimensional array in a computer
program could be [CATEGORY = ‘array, DIMEN-
SION = ‘2, LINE = ‘30, NAME = ‘table, ...]. We
ultimately represent Et = {oi} as the set of all
attribute vectors Att V eci, and for a referring ex-
pression we aim to identify Att V ecj , the actual
referent.

Since a referring expression describes its refer-
ents either implicitly or explicitly, the attributes
expressed in it should match the attributes of its
referent. We segment referring expressions and la-
bel the semantics of each segment using the CRF
and the result is a set of segments, each of which
represents some attribute of its referent. This pro-
cess is illustrated in (Figure 2 (a)). After segment-
ing and labeling attributes in the referring expres-
sions, the attribute values are extracted from each
semantic segment using regular expressions (Fig-
ure 2 (b)), e.g., value 2 is extracted from 2 di-
mensional to fill in the ARRAY DIM element in an
empty Att V ec. The result is an attribute vector
that represents the referring expression.

3.2 Generating a List of Candidate Referents

Once the referring expression is represented as an
object attribute vector as described above, we wish
to link that vector to the closest-matching object
in the environment. Each object is represented by
its own attribute vector, and there may be a large
number of objects in Et. Given a referring expres-
sion Rk, we would like to trim the list to keep only
those objects that are likely to be referent for Rk.

There are two desired criteria for generating the

Figure 2 Semantic interpretation of referring ex-
pressions.

list of candidate referents. First, the actual ref-
erent must be in the candidate list. At the same
time, the candidate list should be as short as pos-
sible. We can pare down the set of all objects
in Et by considering focus of attention in dia-
logue. Early approaches performed reference res-
olution by estimating each dialogue participant’s
focus of attention (Lappin and Leass, 1994; Grosz
et al., 1995). According to Ariel’s accessibility
theory (Ariel, 1988), people tend to use more pre-
cise descriptions such as proper names in refer-
ring expressions for referents in long term mem-
ory, and use less precise descriptions such as pro-
nouns for referents in short term memory. In a
precise description, there is more semantic infor-
mation, while in a more vague description like a
pronoun, there is less semantic information. Thus,
these two sources of information, semantics and
focus of attention, work together in identifying a
referent.

Our approach employs this idea in the process
of candidate referent selection by tracking the fo-
cus of attention of the dialogue participants from
the beginning of the dialogue through dialogue
history and task history, as has been done in prior
work we use for comparison within our experi-
ments (Iida et al., 2010). We also use the learned
semantics of the referring expression (represented
as the referring expression’s attribute vector) as fil-
tering conditions to select candidates.

The candidate generation process consists of
three steps.

1. Candidate generation from dialogue history
DH .

DH =< Od, Td >

Here, Od =< o1d, o
2
d, ..., o

m
d > is a se-

quence of objects that were mentioned since

332



the beginning of the dialogue. Td =<
t1d, t

2
d, ..., t

m
d > is a sequence of timestamps

when corresponding objects were mentioned.
All of the objects in Et that were ever men-
tioned in the dialogue history, {oi|oi ∈
DH & oi ∈ Et}, will also be added into the
candidate list.

2. Candidate generation from task history TH .
Similarly, TH =< Ob, Tb >, which is all of
the objects in Et that were ever manipulated
by the user, will be added into the candidate
list.

3. Candidate generation using learned seman-
tics, which are the referent’s attributes. Given
a set of attributes extracted from a referring
expression, all objects in Et with one of the
same attribute values will be added into the
candidate list. The attributes are considered
separately to avoid the case in which a single
incorrectly extracted attribute could rule out
the correct referent. Table 1 shows the algo-
rithm used in this step.

Given a referring expression Rk, whose at-
tribute vector Att V eck has been extracted.
for each element atti of Att V eck

if atti is not null
for each o in Et

if atti == o.atti
add o into candidate list Ck

Table 1 Algorithm to select candidates using
learned semantics

3.3 Ranking-based classification

With the list of candidate referents in hand, we em-
ploy a ranking-based classification model to iden-
tify the most likely referent. Ranking-based mod-
els have been shown to perform well for refer-
ence resolution problems in prior work (Denis and
Baldridge, 2008; Iida et al., 2010). For a given
referring expression Rk and its candidate referent
list Ck = {o1, o2, ..., oNk}, in which each oi is an
object identified as a candidate referent, we com-
pute the probability of each candidate oi being the
true referent of Rk, p(Rk, oi) = f(Rk, oi), where
f is the classification function. (Note that our ap-
proach is classifier-agnostic. As we describe in

Section 5, we experimented with several differ-
ent models.) Then, the candidates are ranked by
p(Rk, oi), and the object with the highest proba-
bility is taken as the referent of Rk.

4 Corpus

Human problem solving represents a highly com-
plex domain that poses great challenges for refer-
ence resolution. We evaluate our new reference
resolution approach on a corpus of human-human
textual dialogue in the domain of computer pro-
gramming (Boyer et al., 2011). In each dialogue,
a human tutor assisted a student remotely using
typed dialogue as the student completed given
programming tasks in the Java programming lan-
guage. The programming tasks involved array ma-
nipulation and control flow, which are challenging
for students with little programming experience.
Students’ and tutors’ view of the task were syn-
chronized in real time. At the beginning of each
problem-solving session students were provided a
framework of code to fill in, which is around 200
lines initially. The corpus contains 45 tutoring ses-
sions, 4857 utterances in total, 108 utterances for
each session on average. We manually annotated
the referring expressions in the dialogue and their
referents in the corresponding Java code for six
dialogues from the corpus (346 referring expres-
sions). These six sessions contain 758 utterances.
The dialogues focus on the details of solving the
programming problems, with very little social or
off-task talk. Figure 1 shows an excerpt of this
dialogue.

5 Experiments & Result

To evaluate the new approach, we performed a set
of experiments that compare our approach with
two state-of-the-art approaches.

5.1 Semantic Parsing

The referring expressions were extracted from the
tutorial dialogues and their semantic segments and
labels were manually annotated. A linear-chain
CRF was trained on that data and used to per-
form referring expression segmentation and label-
ing (Li and Boyer, 2015). The current paper re-
ports the first use of that learned semantics ap-
proach for reference resolution.

Next, we proceeded to extract the attribute val-
ues, a step that our previous work did not address.
For the example shown in Figure 2 (b), from the

333



learned semantic structure, we may know that 2
dimensional refers to the dimension of the array,
the attribute ARRAY DIM. (In the current domain
there are 14 attributes that comprise the generic
attribute vector V , such as ARRAY DIM, NUM,
and CATEGORY.) To actually extract the attribute
values, we use regular expressions that capture our
three types of attribute values: categorical, nu-
meric, and strings. For example, the value type
of CATEGORY is categorical, like method or vari-
able. Its values are taken from a closed set. NAME
has values that are strings. LINE NUMBER’s
value is numeric. For categorical attributes, we
add the categorical attribute values into the se-
mantic tag set of the CRF used for segmenta-
tion. In this way, the attribute values of categori-
cal attributes will be generated by the CRF. For at-
tributes with text string values, we take the whole
surface string of the semantic segment as its at-
tribute value. The accuracy of the entire seman-
tic parsing pipeline is 93.2% using 10-fold cross-
validation. The accuracy is defined as the percent-
age of manually labeled attribute values that were
successfully extracted from referring expressions.

5.2 Candidate Referent Generation

We applied the approach described in Section 3.2
on each session to generate a list of candidate ref-
erents for each referring expression. In a program,
there could be more than one appearance of the
same object. We take all of the appearances of
the same object to be the same, since they all re-
fer to the same artifact in the program. The aver-
age number of generated candidates for each refer-
ring expression was 44.8. The percentage of refer-
ring expressions whose actual referents were in the
generated candidate list, or ’“hit rate” is 90.5%,
based on manual tagging. This performance in-
dicates that the candidate referent list generation
performs well.

A referring expression could be a pronoun,
such as “it” or “that”, which does not contain at-
tribute information. In previous reference reso-
lution research, it was shown that training sep-
arate models for different kinds of referring ex-
pressions could improve performance (Denis and
Baldridge, 2008). We follow this idea and split the
dataset into two groups: referring expressions con-
taining attributes, REatt, (270 referring expres-
sions), and referring expressions that do not con-
tain attributes, REnon (76 referring expressions).

The candidate generation approach performed
better for the referring expressions without at-
tributes (hit rate 94.7%), compared to referring ex-
pressions with attributes (hit rate 89.3%). Since
the candidate list for referring expressions without
attributes relies solely on dialogue and task his-
tory, 94.7% of those referents had been mentioned
in the dialogue or manipulated by the user previ-
ously. For referring expressions with attribute in-
formation, the generation of the candidate list also
used learned semantic information. Only 70.0% of
those referents had been mentioned in the dialogue
or manipulated by the user before.

5.3 Identifying Most Likely Referent

We applied the approach described in section 3.3
to perform reference resolution on the corpus of
tutorial dialogue. The data from the six manually
labeled Java tutoring sessions were split into a
training set and a test set. We used leave-one-
dialogue-out cross validation (which leads to six
folds) for the reference resolution experiments. In
each fold, annotated referring expressions from
one of the tutoring sessions were taken as the test
set, and data from the other five sessions were
the training set. We tested logistic regression,
decision tree, naive Bayes, and neural networks
as classifiers to compute the p(Rk, oi) for each
(referring expression, candidate) pair for the
ranking-based model. The features provided to
each classifier are shown in Table 2.

To evaluate the performance of the new ap-
proach, we compare against two other recent ap-
proaches. First, we compare against a ranking-
based model that uses dialogue history and task
history features (Iida et al., 2010). This model
uses semantics from a domain-specific lexicon in-
stead of a semantic parser. (As described in Sec-
tion 2, their work was extended by Funakoshi et
al. (2012), but that work relies upon a handcrafted
probability distribution of referents to concepts,
which is not feasible in our domain since it has
no fixed set of possible referents.) Therefore, we
compare against their 2010 approach, implement-
ing it in a way that creates the strongest possible
baseline: we built a lexicon directly from our man-
ually labeled semantic segments. First, we split all
of the semantic segments into groups by their tags.
Then, for each group of segments, any token that
appeared twice or more was added into the lexi-

334



Learned Semantic Features (SF)
SF1: whether RE has CATEGORY attribute

SF2: whether RE.CATEGORY == o.CATEGORY

SF3: whether RE has RE.NAME

SF4: whether RE.NAME == o.NAME

SF5: RE.NAME ≈ o.NAME
SF6: RE.VAR TYPE exist

SF7: RE.VAR TYPE == o.VAR TYPE

SF8: RE.LINE NUMBER exist

SF9: RE.LINE NUMBER == o.LINE NUMBER

SF10: RE.ARRAY DIMENSION exist

SF11: RE.ARRAY DIMENSION ==
o.ARRAY DIMENSION

SF12: CATEGORY of o

Dialogue History (DH) Features
DH1: whether o is the latest mentioned object

DH2: whether o was mentioned in the last 30 seconds

DH3: whether o was mentioned in the last [30, 60] seconds

DH4: whether o was mentioned in the last [60, 180] sec-
onds

DH5: whether o was mentioned in the last [180, 300] sec-
onds

DH6: whether o was mentioned in the last [300, 600] sec-
onds

DH7: whether o was mentioned in the last [600, infinite]
seconds

DH8: whether o was never mentioned from the beginning

DH9: String matching between o and RE

Task History (TH) Features
TH1: whether o is the most recent object manipulated

TH2: whether o was manipulated in the last 30 seconds

TH3: whether o was manipulated in the last [30, 60] sec-
onds

TH4: whether o was manipulated in the last [60, 180] sec-
onds

TH5: whether o was manipulated in the last [180, 300]
seconds

TH6: whether o was manipulated in the last [300, 600]
seconds

TH7: whether o was manipulated in the last [600, infinite]
seconds

TH8: whether o was never manipulated from the beginning

TH9: whether o is in the current working window

Table 2 Features used for segmentation and label-
ing.

con. Although the necessary data to do this would
not be available in a real application of the tech-
nique, it ensures that the lexicon for the baseline
condition has good coverage and creates a high
baseline for our new approach to compare against.
Additionally, for fairness of comparison, for each

semantic feature used in our model, we extracted
the same feature using the lexicon. There were
three kinds of attribute values in the domain: cat-
egorical, string, and numeric (as described in Sec-
tion 5.1). We extracted categorical attribute values
using the appearance of tokens in the lexicon. We
used regular expressions to determine whether a
referring expression contains the name of a candi-
date referent. We also used regular expressions to
extract attribute values from referring expressions,
such as line number. We also provided the Iida
baseline model (2010) with a feature to indicate
string matching between referring expressions and
candidate referents, since this feature was captured
in our model as an attribute.

We also compared our approach against a
very recent technique that leveraged a word-as-
classifier approach to learn semantic compatibility
between referring expressions and candidate ref-
erents (Kennington and Schlangen, 2015). To
create this comparison model we used a word-as-
classifier to learn the semantics of referring ex-
pressions instead of CRF. This weakly supervised
approach relies on co-appearance between words
and object’s attributes. We then used the resulting
semantic compatibility in a ranking-based model
to select the most likely referent.

The three conditions for our experiment are as
follows.

• Iida Baseline Condition: Features including
dialogue history, task history, and semantics
from a handcrafted lexicon (Iida et al.,
2010).

• Kennington Baseline Condition: Features
including dialogue history, task history,
and learned semantics from a word-
as-classifier model (Kennington and
Schlangen, 2015).

• Proposed approach: Features including dia-
logue history, task history, and learned se-
mantics from CRF.

Within each of these experimental conditions,
we varied the classifier used to compute p(Rk, oi),
testing four classifiers: logistic regression (LR),
decision tree (DT), naive Bayes (NB), and neural
network (NN). The neural network has one hidden
layer and the best-performing number of percep-
trons was 100 (we experimented between 50 and
120).

335



To measure the performance of the reference
resolution approaches, we analyzed accuracy, de-
fined to be the percent of referring expressions
that were successfully linked to their referents.
We chose accuracy for our metric following stan-
dard practice (Iida et al., 2010; Kennington and
Schlangen, 2015) because it provides an overall
measure of the number of (Rk, oi) pairs that were
correctly identified. For the rare cases in which
one referring expression referred to multiple refer-
ents, the output referent of the algorithm was taken
as correct if it selected any of the multiple refer-
ents.

The results are shown in Table 3. We focus on
comparing the results on referring expressions that
contain attribute information, shown in the table
as REFATT . REFATT accounts for 78% of all
of the cases (270 out of 346). Among the three
approaches, our proposed approach outperformed
both prior approaches. Compared to the Iida 2010
approach which achieved a maximum of 55.2%
accuracy, our approach achieved 68.5% accuracy
using a neural net classifier, and this difference
is statistically significant based on the results of
a Wilcoxon signed-rank test (n = 6; p = 0.046).
Our approach outperformed the Kennington 2015
approach even more substantially, as its best per-
formance was 46.3% accuracy (p = 0.028). Intu-
itively, the better performance of our model com-
pared to the Iida approach is due to its ability to
more accurately model referring expressions’ se-
mantics. Compared to a lexicon, semantic parsing
finds optimal segmentation for a referring expres-
sion, while a lexicon approach extracts different
attribute information from referring expressions
separately. Note that our approach and the Iida
2010 approach achieved the same performance on
REFNON referring expressions. Since these re-
ferring expressions do not contain attribute infor-
mation, these two approaches used the same set of
features.

Interestingly, the model using a word-as-
classifier approach to learn the semantic compati-
bility between referring expressions and referent’s
attributes performs the worst. We believe that the
reason for this poor performance is mainly from
the way it performs semantic compositions. It
cannot learn structures in referring expressions,
such as that 2 dimensional is a segment, dimen-
sional represents the type of the attribute, and 2 is
the value of the attribute. The word-as-classifier

model cannot deal with this complex semantic
composition.

The results reported above relied on learned se-
mantics. We also performed experiments using
manually labeled, gold-standard semantics of re-
ferring expressions. The result in Table 4 shows
that ranking-based models have the potential to
achieve a considerably better result, 73.6%, with
more accurate semantic information. Given the
85.3% agreement between two human annotators,
the model performs very well, since the semantics
of whole utterances in situated dialogue also play a
very important role in identifying a given referring
expression’s referent.

experimental
condition

f(Rk, oi)
classi-
fier

accuracy

REFATT REFNON
LR 0.500 0.440

Iida DT 0.537 0.453
2010 NB 0.466 0.413

NN 0.552 0.373
LR 0.4627 0.3867

Kennington DT 0.3769 0.3333
2015 NB 0.3209 0.4000

NN 0.4216 0.4000
LR 0.631 0.440

Our DT 0.631 0.453
approach NB 0.493 0.413

NN 0.685 0.373

Table 3 Reference resolution results.

models accuracy
REFATT REFNON

LR + SEM gold 0.684 0.429
DT + SEM gold 0.643 0.429
NB + SEM gold 0.511 0.377
NN + SEM gold 0.736 0.325

Table 4 Reference resolution results with gold se-
mantic labels.

6 Conclusion

Dialogue systems need to move toward supporting
users in increasingly complex tasks. To do this ef-
fectively, accurate reference resolution is crucial.
We have presented a new approach that applies

336



learned semantics to reference resolution in situ-
ated dialogue for collaborative tasks. The exper-
iments with human-human dialogue on a collab-
orative programming task showed a tremendous
improvement using semantic information that was
learned with a CRF-based semantic parsing ap-
proach compared to the previous state-of-art ap-
proaches. The accuracy was improved substan-
tially, from 55.2% to 68.5%.

There are several important future research di-
rections in reference resolution for situated dia-
logues. First, models should incorporate more se-
mantic information from discourse structure and
utterance understanding besides semantics from
referring expressions. This is illustrated by the
observation that the reference resolution accuracy
using gold-standard semantic information from re-
ferring expressions is still substantially lower than
the agreement rate between human annotators.
Another research direction that holds promise is
to use an unsupervised approach to extract seman-
tic information from referring expressions. It is
hoped that this line of investigation will enable
rich natural language dialogue interactions to sup-
port users in a wide variety of complex situated
tasks.

Acknowledgments

The authors wish to thank the members of the
LearnDialogue group at the University of Florida
and North Carolina State University, particularly
Joseph Wiggins, along with Wookhee Min and
Adam Dalton for their helpful input. This work
is supported in part by Google through a Capac-
ity Research Award and by the National Science
Foundation through grant CNS-1622438. Any
opinions, findings, conclusions, or recommenda-
tions expressed in this report are those of the au-
thors, and do not necessarily represent the official
views, opinions, or policy of the National Science
Foundation.

References
Ariel, Mira. 1988. Referring and Accessibility. Jour-

nal of Linguistics, 24, 65-87.

Björkelund, Anders and Kuhn, Jonas. 2014. Learn-
ing Structured Perceptrons for Coreference Reso-
lution with Latent Antecedents and Non-local Fea-
tures. Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), 47-57.

Boyer, Kristy Elizabeth and Ha, Eun Young and
Phillips, Robert and Lester, James C.. 2011. The
Impact of Task-Oriented Feature Sets on HMMs for
Dialogue Modeling. Proceedings of the 12th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, 49–58.

Chai, Joyce and Hong, Pengyu and Zhou, Michelle.
2004. A Probabilistic Approach to Reference Reso-
lution in Multimodal User Interfaces. Proceedings
of the 9th International Conference on Intelligent
User Interfaces SE - IUI ’04, 70-77.

Denis, Pascal and Baldridge, Jason. 2008. Specialized
Models and Reranking for Coreference Resolution.
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, 660-669.

Devault, David and Kariaeva, Natalia and Kothari,
Anubha and Oved, Iris and Stone, Matthew. 2005.
An Information-State Approach to Collaborative
Reference. Proceedings of the ACL 2005 on Inter-
active Poster and Demonstration Sessions, 1-4.

Funakoshi, Kotaro and Nakano, Mikio and Tokunaga,
Takenobu and Iida, Ryu. 2012. A Unified Prob-
abilistic Approach to Referring Expressions. Pro-
ceedings of the 13th Annual Meeting of the Spe-
cial Interest Group on Discourse and Dialogue, 237-
246.

Graesser, A C and Lu, S and Jackson, G T and Mitchell,
H H and Ventura, M and Olney, A and Louwerse,
M M. 2004. AutoTutor: A Tutor with Dialogue
in Natural Language. Behavior Research Methods,
Instruments, & Computers, 36, 180-192.

Grosz, B J and Weinstein, S and Joshi, A K. 1995.
Centering - a Framework for Modeling the Local
Coherence of Discourse. Computational Linguis-
tics, 21(2), 203-225.

Grosz, Barbara J and Sidner, Candace L. 1986. At-
tention, Intentions, and the Structure of Discourse.
Computational Linguistics, 175-204.

Harabagiu, Sanda M. and Räzvan C. Bunescu and
Maiorano, Steven J.. 2001. Text and Knowledge
Minding for Coreference Resolution. Proceedings
of the second meeting of the North American Chap-
ter of the Association for Computational Linguistics
on Language technologies. (NAACL-HLT ), 1-8.

Heeman, Peter a. and Hirst, Graeme. 1995. Collab-
orating on Referring Expressions. Computational
Linguistics, 21, 351-382.

Huwel, Sonja and Wrede, Britta. 2006. Spon-
taneous Speech Understanding for Robust Multi-
modal Human-robot Communication. Proceedings
of the COLING/ACL, 391-398.

Iida, Ryu and Kobayashi, Shumpei and Tokunaga,
Takenobu. 2010. Incorporating Extra-linguistic In-
formation into Reference Resolution in Collabora-
tive Task Dialogue. Proceedings of the 48th Annual

337



Meeting of the Association for Computational Lin-
guistics, 1259-1267.

Iida, Ryu and Yasuhara, Masaaki and Tokunaga,
Takenobu. 2011. Multi-modal Reference Reso-
lution in Situated Dialogue by Integrating Linguis-
tic and Extra-Linguistic Clues. Proceedings of the
5th International Joint Conference on Natural Lan-
guage Processing (IJCNLP 2011), 84-92.

Kennington, Casey and Schlangen, David. 2015. Sim-
ple Learning and Compositional Application of Per-
ceptually Grounded Word Meanings for Incremental
Reference Resolution. Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing, 292-301.

Krishnamurthy, Jayant and Kollar, Thomas. 2013.
Jointly Learning to Parse and Perceive: Connecting
Natural Language to the Physical World. Associa-
tion for Computational Linguistics, 193-206.

Kruijff, Geert-Jan M and Lison, Pierre and Benjamin,
Trevor. 2010. Situated dialogue processing for
human-robot interaction. Cognitive Systems Mono-
graphs, 8, 311-364.

Kushman, Nate and Artzi, Yoav and Zettlemoyer, Luke
and Barzilay, Regina. 2014. Learning to Automat-
ically Solve Algebra Word Problems. Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, 271-281.

Lappin, Shalom and Leass, Herbert J.. 1994. An Algo-
rithm for Pronominal Anaphora Resolution. Com-
putational Linguistics, 535-561.

Li, Xiaolong and Boyer, Kristy Elizabeth. 2015. Se-
mantic Grounding in Dialogue for Complex Prob-
lem Solving. Proceedings of the Conference of
the North American Chapter of the Association for
Computational Linguistics and Human Language
Technology (NAACL HLT), 841-850.

Liang, Percy and Jordan, Michael I and Klein, Dan.
2009. Learning Semantic Correspondences with
Less Supervision. Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, 91–99.

Liu, Changsong and Chai, Joyce Y. 2015. Learning to
Mediate Perceptual Differences in Situated Human-
Robot Dialogue. Proceedings of AAAI 2015, 2288-
2294.

Liu, Changsong and She, Lanbo and Fang, Rui and
Chai, Joyce Y. 2014. Probabilistic Labeling for
Efficient Referential Grounding Based On Collab-
orative Discourse. Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL), 13-18.

Liu, Changsong and Fang, Rui and She, Lanbo and
Chai, Joyce. 2013. Modeling Collaborative Refer-
ring for Situated Referential Grounding. Proceed-
ings of the 14th Annual Meeting of the Special Inter-
est Group on Discourse and Dialogue, 78–86.

Manning, Christopher D and Bauer, John and Finkel,
Jenny and Bethard, Steven J. 2014. The Stanford
CoreNLP Natural Language Processing Toolkit.
Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics: System
Demonstrations, 55-60.

Martschat, Sebastian and Strube, Michael. 2015. La-
tent Structures for Coreference Resolution. Trans-
actions of the Association for Computational Lin-
guistics, 3, 405-418.

Matuszek, Cynthia and FitzGerald, Nicholas and
Zettlemoyer, Luke and Liefeng, Bo and Fox, Di-
eter. 2012. A Joint Model of Language and Per-
ception for Grounded Attribute Learning. Proceed-
ings of the 29th International Conference on Ma-
chine Learning, 1671-1678.

McCarthy, Joseph F. and Lehnert, Wendy G.. 1995.
Using Decision Trees for Coreference Resolution.
Proceedings of the Fourteenth International Joint
Conference on Artificial Intelligence (IJCAI), 1-5.

Naim, I and Song, Yc and Liu, Q and Kautz, H and
Luo, J and Gildea, D. 2014. Unsupervised Align-
ment of Natural Language Instructions with Video
Segments. Proceedings of AAAI 2014, 1558-1564.

Ponzetto, Simone Paolo and Strube, Michael. 2006.
Exploiting Semantic Role Labeling, WordNet and
Wikipedia for Coreference Resolution. Proceedings
of the Main Conference on Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguistics,
192-199.

Soon, Wee Meng and Ng, Hwee Tou and Lim, Daniel
Chung Yong. 2001. Incorporating Extra-linguistic
Information into Reference Resolution in Collabora-
tive Task Dialogue. Computational Linguistics, 27,
521-544.

VanLehn, Kurt and Jordan, P W and Rosé, C P and
Bhembe, D and Bottner, M and Gaydos, A and
Makatchev, M and Pappuswamy, U and Ringenberg,
M and Roque, A. 2002. The Architecture of Why2-
Atlas: A Coach for Qualitative Physics Essay Writ-
ing. Proceedings of the Sixth International Confer-
ence on Intelligent Tutoring System, 2363, 158-167.

338


