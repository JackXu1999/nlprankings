



















































Word Ordering Without Syntax


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2319–2324,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Word Ordering Without Syntax

Allen Schmaltz and Alexander M. Rush and Stuart M. Shieber
Harvard University

{schmaltz@fas,srush@seas,shieber@seas}.harvard.edu

Abstract

Recent work on word ordering has argued that
syntactic structure is important, or even re-
quired, for effectively recovering the order of
a sentence. We find that, in fact, an n-gram
language model with a simple heuristic gives
strong results on this task. Furthermore, we
show that a long short-term memory (LSTM)
language model is even more effective at re-
covering order, with our basic model outper-
forming a state-of-the-art syntactic model by
11.5 BLEU points. Additional data and larger
beams yield further gains, at the expense of
training and search time.

1 Introduction

We address the task of recovering the original word
order of a shuffled sentence, referred to as bag gen-
eration (Brown et al., 1990), shake-and-bake genera-
tion (Brew, 1992), or more recently, linearization, as
standardized in a recent line of research as a method
useful for isolating the performance of text-to-text
generation models (Zhang and Clark, 2011; Liu et
al., 2015; Liu and Zhang, 2015; Zhang and Clark,
2015). The predominant argument of the more re-
cent works is that jointly recovering explicit syn-
tactic structure is crucial for determining the correct
word order of the original sentence. As such, these
methods either generate or rely on given parse struc-
ture to reproduce the order.

Independently, Elman (1990) explored lineariza-
tion in his seminal work on recurrent neural net-
works. Elman judged the capacity of early recurrent
neural networks via, in part, the network’s ability to
predict word order in simple sentences. He notes,

The order of words in sentences reflects a num-
ber of constraints. . . Syntactic structure, selective
restrictions, subcategorization, and discourse con-
siderations are among the many factors which
join together to fix the order in which words oc-
cur. . . [T]here is an abstract structure which un-
derlies the surface strings and it is this structure
which provides a more insightful basis for under-
standing the constraints on word order. . . . It is,
therefore, an interesting question to ask whether a
network can learn any aspects of that underlying
abstract structure (Elman, 1990).

Recently, recurrent neural networks have
reemerged as a powerful tool for learning the latent
structure of language. In particular, work on long
short-term memory (LSTM) networks for language
modeling has provided improvements in perplexity.

We revisit Elman’s question by applying LSTMs
to the word-ordering task, without any explicit syn-
tactic modeling. We find that language models are
in general effective for linearization relative to ex-
isting syntactic approaches, with LSTMs in particu-
lar outperforming the state-of-the-art by 11.5 BLEU
points, with further gains observed when training
with additional text and decoding with larger beams.

2 Background: Linearization

The task of linearization is to recover the original or-
der of a shuffled sentence. We assume a vocabulary
V and are given a sequence of out-of-order phrases
x1, . . . , xN , with xn ∈ V+ for 1 ≤ n ≤ N . Define
M as the total number of tokens (i.e., the sum of the
lengths of the phrases). We consider two varieties of
the task: (1) WORDS, where each xn consists of a
single word and M = N , and (2) WORDS+BNPS,

2319



where base noun phrases (noun phrases not con-
taining inner noun phrases) are also provided and
M ≥ N . The second has become a standard for-
mulation in recent literature.

Given input x, we define the output set Y to be
all possible permutations over the N elements of x,
where ŷ ∈ Y is the permutation generating the true
order. We aim to find ŷ, or a permutation close to
it. We produce a linearization by (approximately)
optimizing a learned scoring function f over the set
of permutations, y∗ = arg maxy∈Y f(x, y).

3 Related Work: Syntactic Linearization

Recent approaches to linearization have been based
on reconstructing the syntactic structure to produce
the word order. Let Z represent all projective de-
pendency parse trees over M words. The objective
is to find y∗, z∗ = arg maxy∈Y,z∈Z f(x, y, z) where
f is now over both the syntactic structure and the lin-
earization. The current state of the art on the Penn
Treebank (PTB) (Marcus et al., 1993), without ex-
ternal data, of Liu et al. (2015) uses a transition-
based parser with beam search to construct a sen-
tence and a parse tree. The scoring function is a
linear model f(x, y) = θ>Φ(x, y, z) and is trained
with an early update structured perceptron to match
both a given order and syntactic tree. The feature
function Φ includes features on the syntactic tree.
This work improves upon past work which used
best-first search over a similar objective (Zhang and
Clark, 2011).

In follow-up work, Liu and Zhang (2015) argue
that syntactic models yield improvements over pure
surface n-gram models for the WORDS+BNPS case.
This result holds particularly on longer sentences
and even when the syntactic trees used in training
are of low quality. The n-gram decoder of this work
utilizes a single beam, discarding the probabilities
of internal, non-boundary words in the BNPs when
comparing hypotheses. We revisit this comparison
between syntactic models and surface-level models,
utilizing a surface-level decoder with heuristic fu-
ture costs and an alternative approach for scoring
partial hypotheses for the WORDS+BNPS case.

Additional previous work has also explored n-
gram models for the word ordering task. The work
of de Gispert et al. (2014) demonstrates improve-

ments over the earlier syntactic model of Zhang et al.
(2012) by applying an n-gram language model over
the space of word permutations restricted to concate-
nations of phrases seen in a large corpus. Horvat and
Byrne (2014) models the search for the highest prob-
ability permutation of words under an n-gram model
as a Travelling Salesman Problem; however, direct
comparisons to existing works are not provided.

4 LM-Based Linearization

In contrast to the recent syntax-based approaches,
we use an LM directly for word ordering. We
consider two types of language models: an n-
gram model and a long short-term memory network
(Hochreiter and Schmidhuber, 1997). For the pur-
pose of this work, we define a common abstraction
for both models. Let h ∈ H be the current state of
the model, with h0 as the initial state. Upon seeing
a word wi ∈ V , the LM advances to a new state
hi = δ(wi,hi−1). At any time, the LM can be
queried to produce an estimate of the probability of
the next word q(wi,hi−1) ≈ p(wi | w1, . . . , wi−1).
For n-gram language models, H, δ, and q can natu-
rally be defined respectively as the state space, tran-
sition model, and edge costs of a finite-state ma-
chine.

LSTMs are a type of recurrent neural network
(RNN) that are conducive to learning long-distance
dependencies through the use of an internal memory
cell. Existing work with LSTMs has generated state-
of-the-art results in language modeling (Zaremba et
al., 2014), along with a variety of other NLP tasks.

In our notation we define H as the hidden states
and cell states of a multi-layer LSTM, δ as the
LSTM update function, and q as a final affine trans-
formation and softmax given as q(∗,hi−1; θq) =
softmax(Wh

(L)
i−1 + b) where h

(L)
i−1 is the top hid-

den layer and θq = (W , b) are parameters. We di-
rect readers to the work of Graves (2013) for a full
description of the LSTM update.

For both models, we simply define the scoring
function as

f(x, y) =

N∑

n=1

log p(xy(n) | xy(1), . . . , xy(n−1))

where the phrase probabilities are calculated word-
by-word by our language model.

2320



Algorithm 1 LM beam-search word ordering
1: procedure ORDER(x1 . . . xN , K, g)
2: B0 ← 〈(〈〉, {1, . . . , N}, 0,h0)〉
3: for m = 0, . . . ,M − 1 do
4: for k = 1, . . . , |Bm| do
5: (y,R, s,h)← B(k)m
6: for i ∈ R do
7: (s′,h′)← (s,h)
8: for word w in phrase xi do
9: s′ ← s′ + log q(w,h′)

10: h′ ← δ(w,h′)
11: j ← m+ |xi|
12: Bj ← Bj + (y + xi,R− i, s′,h′)
13: keep top-K of Bj by f(x, y) + g(R)
14: return BM

Searching over all permutations Y is intractable,
so we instead follow past work on linearization (Liu
et al., 2015) and LSTM generation (Sutskever et
al., 2014) in adapting beam search for our genera-
tion step. Our work differs from the beam search
approach for the WORDS+BNPS case of previ-
ous work in that we maintain multiple beams, as
in stack decoding for phrase-based machine trans-
lation (Koehn, 2010), allowing us to incorporate
the probabilities of internal, non-boundary words
in the BNPs. Additionally, for both WORDS and
WORDS+BNPS, we also include an estimate of fu-
ture cost in order to improve search accuracy.

Beam search maintains M + 1 beams,
B0, . . . , BM , each containing at most the top-
K partial hypotheses of that length. A partial
hypothesis is a 4-tuple (y,R, s,h), where y is a
partial ordering,R is the set of remaining indices to
be ordered, s is the score of the partial linearization
f(x, y), and h is the current LM state. Each step
consists of expanding all next possible phrases and
adding the next hypothesis to a later beam. The full
beam search is given in Algorithm 1.

As part of the beam search scoring function we
also include a future cost g, an estimate of the score
contribution of the remaining elements in R. To-
gether, f(x, y) + g(R) gives a noisy estimate of the
total score, which is used to determine the K best
elements in the beam. In our experiments we use a
very simple unigram future cost estimate, g(R) =∑

i∈R
∑

w∈xi log p(w).

Model WORDS WORDS+BNPS

ZGEN-64 30.9 49.4
ZGEN-64+POS – 50.8

NGRAM-64 (NO g) 32.0 51.3
NGRAM-64 37.0 54.3
NGRAM-512 38.6 55.6
LSTM-64 40.5 60.9
LSTM-512 42.7 63.2

ZGEN-64+LM+GW+POS – 52.4
LSTM-64+GW 41.1 63.1
LSTM-512+GW 44.5 65.8

Table 1: BLEU score comparison on the PTB test
set. Results from previous works (for ZGEN) are
those provided by the respective authors, except for
the WORDS task. The final number in the model
identifier is the beam size, +GW indicates additional
Gigaword data. Models marked with +POS are pro-
vided with a POS dictionary derived from the PTB
training set.

5 Experiments

Setup Experiments are on PTB with sections 2-
21 as training, 22 as validation, and 23 as test1.
We utilize two UNK types, one for initial upper-
case tokens and one for all other low-frequency to-
kens; end sentence tokens; and start/end tokens,
which are treated as words, to mark BNPs for the
WORDS+BNPS task. We also use a special symbol
to replace tokens that contain at least one numeric
character. We otherwise train with punctuation and
the original case of each token, resulting in a vocab-
ulary containing around 16K types from around 1M
training tokens.

For experiments marked GW we augment the
PTB with a subset of the Annotated Gigaword cor-
pus (Napoles et al., 2012). We follow Liu and
Zhang (2015) and train on a sample of 900k Agence
France-Presse sentences combined with the full PTB
training set. The GW models benefit from both ad-
ditional data and a larger vocabulary of around 25K
types, which reduces unknowns in the validation and
test sets.

We compare the models of Liu et al. (2015)

1In practice, the results in Liu et al. (2015) and Liu and
Zhang (2015) use section 0 instead of 22 for validation (author
correspondence).

2321



BNP g GW 1 10 64 128 256 512

LSTM
• 41.7 53.6 58.0 59.1 60.0 60.6
• • 47.6 59.4 62.2 62.9 63.6 64.3
• • • 48.4 60.1 64.2 64.9 65.6 66.2

15.4 26.8 33.8 35.3 36.5 38.0
• 25.0 36.8 40.7 41.7 42.0 42.5
• • 23.8 35.5 40.7 41.7 42.9 43.7

NGRAM
• 40.6 49.7 52.6 53.2 54.0 54.7
• • 45.7 53.6 55.6 56.2 56.6 56.6

14.6 27.1 32.6 33.8 35.1 35.8
• 27.1 34.6 37.5 38.1 38.4 38.7

Table 2: BLEU results on the validation set for
the LSTM and NGRAM model with varying beam
sizes, future costs, additional data, and use of base
noun phrases.

(known as ZGEN), a 5-gram LM using Kneser-Ney
smoothing (NGRAM)2, and an LSTM. We experi-
ment on the WORDS and WORDS+BNPS tasks, and
we also experiment with including future costs (g),
the Gigaword data (GW), and varying beam size.
We retrain ZGEN using publicly available code3 to
replicate published results.

The LSTM model is similar in size and architec-
ture to the medium LSTM setup of Zaremba et al.
(2014)4. Our implementation uses the Torch5 frame-
work and is publicly available6.

We compare the performance of the models using
the BLEU metric (Papineni et al., 2002). In gener-
ation if there are multiple tokens of identical UNK
type, we randomly replace each with possible un-
used tokens in the original source before calculating
BLEU. For comparison purposes, we use the BLEU
script distributed with the publicly available ZGEN
code.

Results Our main results are shown in Table 1.
On the WORDS+BNPS task the NGRAM-64 model
scores nearly 5 points higher than the syntax-based
model ZGEN-64. The LSTM-64 then surpasses

2We use the KenLM Language Model Toolkit (https://
kheafield.com/code/kenlm/).

3https://github.com/SUTDNLP/ZGen
4We hypothesize that additional gains are possible via a

larger model and model averaging, ceteris paribus.
5http://torch.ch
6https://github.com/allenschmaltz/word_

ordering

5 10 15 20 25 30 35 40
Sentence length

30

50

70

90

B
L
E
U

(%
)

LSTM-512

LSTM-64

ZGen-64

LSTM-1

0.0 0.2 0.4 0.6 0.8 1.0
Distortion rate

0

4000

8000

12000

T
ok
en
s

ZGen-64

LSTM-64

Figure 1: Experiments on the PTB validation on the
WORDS+BNPS models: (a) Performance on the set
by length on sentences from length 2 to length 40.
(b) The distribution of token distortion, binned at 0.1
intervals.

NGRAM-64 by more than 5 BLEU points. Differ-
ences on the WORDS task are smaller, but show a
similar pattern. Incorporating Gigaword further in-
creases the result another 2 points. Notably, the
NGRAM model outperforms the combined result
of ZGEN-64+LM+GW+POS from Liu and Zhang
(2015), which uses a 4-gram model trained on Gi-
gaword. We believe this is because the combined
ZGEN model incorporates the n-gram scores as dis-
cretized indicator features instead of using the prob-
ability directly.7 A beam of 512 yields a further im-
provement at the cost of search time.

To further explore the impact of search accuracy,
Table 2 shows the results of various models with
beam widths ranging from 1 (greedy search) to 512,
and also with and without future costs g. We see that
for the better models there is a steady increase in ac-
curacy even with large beams, indicating that search
errors are made even with relatively large beams.

7In work of Liu and Zhang (2015), with the given decoder,
N-grams only yielded a small further improvement over the syn-
tactic models when discretized versions of the LM probabilities
were incorporated as indicator features in the syntactic models.

2322



Model WORDS WORDS+BNPS

ZGEN-64(z∗) 39.7 64.9
ZGEN-64 40.8 65.2

NGRAM-64 46.1 67.0
NGRAM-512 47.2 67.8
LSTM-64 51.3 71.9
LSTM-512 52.8 73.1

Table 3: Unlabeled attachment scores (UAS) on
the PTB validation set after parsing and aligning
the output. For ZGEN we also include a result us-
ing the tree z∗ produced directly by the system.
For WORDS+BNPS, internal BNP arcs are always
counted as correct.

One proposed advantage of syntax in lineariza-
tion models is that it can better capture long-distance
relationships. Figure 1 shows results by sentence
length and distortion, which is defined as the abso-
lute difference between a token’s index position in
y∗ and ŷ, normalized by M . The LSTM model ex-
hibits consistently better performance than existing
syntax models across sentence lengths and generates
fewer long-range distortions than the ZGEN model.

Finally, Table 3 compares the syntactic fluency
of the output. As a lightweight test, we parse the
output with the Yara Parser (Rasooli and Tetreault,
2015) and compare the unlabeled attachment scores
(UAS) to the trees produced by the syntactic system.
We first align the gold head to each output token. (In
cases where the alignment is not one-to-one, we ran-
domly sample among the possibilities.) The models
with no knowledge of syntax are able to recover a
higher proportion of gold arcs.

6 Conclusion

Strong surface-level language models recover word
order more accurately than the models trained with
explicit syntactic annotations appearing in a recent
series of papers. This has implications for the utility
of costly syntactic annotations in generation mod-
els, for both high- and low- resource languages and
domains.

Acknowledgments

We thank Yue Zhang and Jiangming Liu for assis-
tance in using ZGen, as well as verification of the

task setup for a valid comparison. Jiangming Liu
also assisted in pointing out a discrepancy in the im-
plementation of an earlier version of our NGRAM
decoder, the resolution of which improved BLEU
performance.

References

[Brew1992] Chris Brew. 1992. Letting the cat out of the
bag: Generation for shake-and-bake MT. In Proceed-
ings of the 14th Conference on Computational Lin-
guistics - Volume 2, COLING ’92, pages 610–616,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

[Brown et al.1990] Peter F Brown, John Cocke, Stephen
A Della Pietra, Vincent J Della Pietra, Fredrick Je-
linek, John D Lafferty, Robert L Mercer, and Paul S
Roossin. 1990. A statistical approach to machine
translation. Computational linguistics, 16(2):79–85.

[de Gispert et al.2014] Adrià de Gispert, Marcus Tomalin,
and Bill Byrne. 2014. Word ordering with phrase-
based grammars. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 259–268, Gothen-
burg, Sweden, April. Association for Computational
Linguistics.

[Elman1990] Jeffrey L. Elman. 1990. Finding structure
in time. Cognitive Science, 14(2):179 – 211.

[Graves2013] Alex Graves. 2013. Generating se-
quences with recurrent neural networks. CoRR,
abs/1308.0850.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
Jürgen Schmidhuber. 1997. Long short-term memory.
Neural Comput., 9(8):1735–1780, November.

[Horvat and Byrne2014] Matic Horvat and William
Byrne. 2014. A graph-based approach to string
regeneration. In Proceedings of the Student Research
Workshop at the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 85–95, Gothenburg, Sweden, April.
Association for Computational Linguistics.

[Koehn2010] Philipp Koehn. 2010. Statistical Machine
Translation. Cambridge University Press, New York,
NY, USA, 1st edition.

[Liu and Zhang2015] Jiangming Liu and Yue Zhang.
2015. An empirical comparison between n-gram and
syntactic language models for word ordering. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 369–378,
Lisbon, Portugal, September. Association for Compu-
tational Linguistics.

2323



[Liu et al.2015] Yijia Liu, Yue Zhang, Wanxiang Che, and
Bing Qin. 2015. Transition-based syntactic lineariza-
tion. In Proceedings of the 2015 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 113–122, Denver, Colorado, May–June.
Association for Computational Linguistics.

[Marcus et al.1993] Mitchell P. Marcus, Beatrice San-
torini, and Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of english: The penn tree-
bank. Computational Linguistics, 19(2):313–330.

[Napoles et al.2012] Courtney Napoles, Matthew Gorm-
ley, and Benjamin Van Durme. 2012. Annotated gi-
gaword. In Proceedings of the Joint Workshop on Au-
tomatic Knowledge Base Construction and Web-scale
Knowledge Extraction, AKBC-WEKEX ’12, pages
95–100, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

[Papineni et al.2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A
method for automatic evaluation of machine transla-
tion. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics, ACL ’02,
pages 311–318, Stroudsburg, PA, USA. Association
for Computational Linguistics.

[Rasooli and Tetreault2015] Mohammad Sadegh Rasooli
and Joel R. Tetreault. 2015. Yara parser: A fast and
accurate dependency parser. CoRR, abs/1503.06733.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and
Quoc VV Le. 2014. Sequence to sequence learning
with neural networks. In Advances in Neural Informa-
tion Processing Systems, pages 3104–3112.

[Zaremba et al.2014] Wojciech Zaremba, Ilya Sutskever,
and Oriol Vinyals. 2014. Recurrent neural network
regularization. CoRR, abs/1409.2329.

[Zhang and Clark2011] Yue Zhang and Stephen Clark.
2011. Syntax-based grammaticality improvement us-
ing ccg and guided search. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’11, pages 1147–1157, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

[Zhang and Clark2015] Yue Zhang and Stephen Clark.
2015. Discriminative syntax-based word ordering for
text generation. Comput. Linguist., 41(3):503–538,
September.

[Zhang et al.2012] Yue Zhang, Graeme Blackwood, and
Stephen Clark. 2012. Syntax-based word ordering in-
corporating a large-scale language model. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL ’12, pages 736–746, Stroudsburg, PA, USA.
Association for Computational Linguistics.

2324


