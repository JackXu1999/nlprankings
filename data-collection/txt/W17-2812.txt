



















































Towards Problem Solving Agents that Communicate and Learn


Proceedings of the First Workshop on Language Grounding for Robotics, pages 95–103,
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

Towards Problem Solving Agents that Communicate and Learn

Anjali Narayan-Chen1, Colin Graber1, Mayukh Das2, Md Rakibul Islam3,
Soham Dan1, Sriraam Natarajan2, Janardhan Rao Doppa3,

Julia Hockenmaier1,5, Martha Palmer4 and Dan Roth1
1University of Illinois at Urbana-Champaign, 2Indiana University, 3Washington State University,

4University of Colorado at Boulder, 5Allen Institute for Artificial Intelligence

Abstract

Agents that communicate back and forth
with humans to help them execute non-
linguistic tasks are a long sought goal of
AI. These agents need to translate be-
tween utterances and actionable meaning
representations that can be interpreted by
task-specific problem solvers in a context-
dependent manner. They should also be
able to learn such actionable interpreta-
tions for new predicates on the fly. We de-
fine an agent architecture for this scenario
and present a series of experiments in the
Blocks World domain that illustrate how
our architecture supports language learn-
ing and problem solving in this domain.

1 Introduction

An agent that can engage in natural, back-and-
forth communication with humans to help them
complete a real world task requires the ability to
understand and produce language in the context
of that task (i.e. to map between utterances and
meaning representations the problem solving com-
ponents of the agent can act on in a particular sit-
uation). The agent may also need to initiate clar-
ification requests when communication fails, and
to learn new domain (or conversation) specific vo-
cabulary and its meaning. This kind of symmetric,
grounded communication with a problem-solving
agent goes significantly beyond the one-step, sin-
gle direction understanding tasks considered in
standard semantic parsing (e.g. Zelle and Mooney,
1996; Zettlemoyer and Collins, 2005; Clarke et al.,
2010) or even short, simple instructions to robots
(e.g. Tellex et al., 2011).

In order to focus on these concept learning and
communication issues, we deliberately limit our-
selves here to a simple, simulated environment.

We operate in a two-dimensional Blocks World
domain where a human wants one or more shapes
to be constructed on a grid. The human needs to
communicate the goal of this planning task to the
agent. Once the agent has understood the instruc-
tions and its planning is done, it communicates its
plan to (possibly) another human who will then
execute this plan. Depending on the complexity of
the task and the linguistic capabilities of the agent,
this scenario may require a lot of back-and-forth
communication. If the human omits details from
their description and prevents the agent from ac-
complishing the task, we expect the agent to initi-
ate communication and ask clarification questions.
If the human uses vocabulary that is new to the
agent, we expect the agent to ask for a definition
and the human to teach the agent its meaning in
the domain.

We define an agent architecture named COG
that allows us to investigate the challenges arising
in this symmetric communication scenario. COG
combines a problem solving (planning) compo-
nent with a basic language understanding and gen-
eration system that is initially only equipped with
a limited vocabulary. We perform a sequence of
experiments of increasing complexity that illus-
trate how our architecture supports the problem
solving scenarios described above, and how lan-
guage learning is accomplished within this archi-
tecture. We argue that, within this architecture, all
the agent’s capabilities – comprehension, produc-
tion, problem solving and learning – can be im-
proved with additional communication.

Section 2 defines the domain and problem
setup. Section 3 provides an overview of the
COG architecture. Section 4 describes COG’s cur-
rent components (language comprehension, pro-
duction, memory, problem solving, and dialogue
mediation). Section 5 describes our experiments
and results.

95



Figure 1: A complex shape, which can be viewed
as conjunction of simpler known shapes: a row
(dark, width = 4) and a square (light, size = 3).

2 Domain and Problem Setup

We consider a two-dimensional (2D) Blocksworld
domain. There is a 2D grid and the goal is to
build different configurations with square blocks
based on their natural language descriptions. We
assume agents come equipped with the vocabulary
and definitions of primitive concepts, e.g. for sim-
ple shapes (row, square, etc.) and spatial relations
(adjacent, on top of, etc.), but may have to learn
the definitions of new terms and concepts that arise
in communication with the human user.

We define three different types of goal descrip-
tions and design corresponding tasks for evalua-
tion. During evaluation, the agent must automati-
cally identify the task and respond appropriately.

Task 1: Complete descriptions. For the first
task, the agent is provided with a complete de-
scription, in natural language, of a target configu-
ration consisting of one or two shapes. The defini-
tion is complete and does not require further clar-
ification. This tests the ability of the agent to un-
derstand and ground specific descriptions.

Task 2: Descriptions with missing information.
For the second task, the agent is again provided
with a description of a target configuration consist-
ing of one or two shapes; however, the description
will not be specific enough for the agent to be able
to ground it to a unique configuration. This tests
the ability of the agent to recognize when some
information is missing from a description and to
initiate a dialogue which will clarify these details.

Task 3: Descriptions of new shapes. For the
third task, the agent is asked to construct a com-
plex shape which is not contained within its initial
vocabulary of primitive concepts (e.g., the letter
“L”). This tests the ability of the agent to extend
its vocabulary through interaction with a human.

Figure 2: The agent architecture. Interaction
(thick arrows) is done via the Dialogue Mediator
(input) and the Language Production component
(output). Thin arrows indicate the flow of infor-
mation between the internal components.

3 The COG Agent Architecture

Agents that solve a task require explicit knowledge
of the corresponding real world concepts. Agents
that also communicate about solving tasks addi-
tionally need to be able to map linguistic expres-
sions such as “square” or “on top of” to these con-
cepts. They also need to know to which situations
these concepts apply (e.g. whether a given config-
uration can be referred to as a square, or whether
a row can be placed on top of that square). The
representations used by the different components
of our agent therefore vary in their specificity.
This requires additional work to bridge the result-
ing representational gaps. To represent the defini-
tion of concepts, we store “lifted” representations,
i.e. rules whose predicates only contain unin-
stantiated variables (e.g. rectangles have height
and width). By contrast, problem solving requires
fully grounded representations in which shapes are
placed at specific locations on the grid (e.g., a 3
by 4 rectangle located at (0, 0)). Language com-
prehension and production operate on a middle
ground where some parameters may be instanti-
ated, even if shapes are not necessarily placed at
specific locations (e.g., a 3 by 4 rectangle).

Figure 2 describes the architecture of our agent.
The Language Comprehension (LC) module
converts natural language into an executable,
grounded, declarative representation in a STRIPS-
like language (Fikes and Nilsson, 1971) that the
Problem Solving (PS) module can act on. PS re-
turns partial or complete problem solving plans,

96



also in this language. If grounding fails, LC pro-
duces queries that are sent to the Language Pro-
duction (LP) module.

The LP takes these queries or the plans pro-
duced by PS and returns natural language out-
put. The Memory (M) module stores lifted repre-
sentations of built-in and acquired predicates that
are used by LC and LP. The Dialogue Media-
tor (DM) is aware of the current dialogue state.
It can augment the natural language input before
transferring it to LC (e.g. if the human utters a
number, DM might provide the context, letting
LC know that the number refers to the size of a
specific shape). DM is also used by LP to gen-
erate utterances that are appropriate for the cur-
rent dialogue state. Instead of narrating the fully
grounded, block-wise sequential plans produced
by PS, LP may identify subsequences of steps that
correspond to instantiations of more abstract con-
cepts (e.g., a sequence of blocks placed horizon-
tally is a “row”), and generates language at this
more natural level.

4 Current Components of COG

In this section, we describe the current implemen-
tations of the different modules (language compre-
hension, memory, problem solving, language pro-
duction, and dialogue mediation) in COG, noting
that the architecture is flexible and allows for us to
plug-in other implementations as needed.

4.1 Language Comprehension

The LC module consists of semantic parsing and
language grounding components.

4.1.1 Semantic Parsing
Our semantic parser is implemented as a neural
sequence-to-sequence model with attention (Bah-
danau et al., 2014; Dong and Lapata, 2016; Jia
and Liang, 2016). The model consists of two
LSTMs. The first LSTM (the encoder) pro-
cesses the input sentence x = (x1, . . . , xm)
token-by-token, producing a sequence of hidden
states hs = (hs1, . . . , h

s
m) as output. The sec-

ond LSTM (the decoder) models a distribution
P (yi|y1, . . . , yi−1; hs) at each time step over out-
put tokens as a function of the encoder hidden
states and the previous outputs. The final parse
y = (y1, . . . , yn) is obtained by selecting the to-
ken at each time step that maximizes this proba-
bility and feeding a learned embedding for it into

the LSTM as part of the next input. Multiple sen-
tence inputs are processed sentence-by-sentence,
where the initial encoder hidden state for a given
sentence is set to the final encoder hidden state for
the previous sentence, and the initial encoder hid-
den state for the first sentence is the zero vector.
The final logical form is the conjunction of the log-
ical forms for each individual sentence.

The parser is trained with a small fixed-size
vocabulary; however, to represent new shapes it
needs to be able to output new predicates for
shapes that it has not encountered during train-
ing. We accomplish this by using an attention-
based copying mechanism (Jia and Liang, 2016;
Gulcehre et al., 2016; Gu et al., 2016). At every
time step, the decoder may either output a token
from the training vocabulary or copy a word from
the input sentence. Hence, when new shapes are
encountered in the input, the parser is able to copy
the shape name from the input sentence to define
a new predicate.

4.1.2 Language Grounding

Grounding is beyond the capabilities of a seman-
tic parser, which may interpret a sentence such as
“Place a square of size 4 on top of the rectan-
gle” without any understanding of the key pred-
icates (what is a “square”?) or relations (what is
“on-top-of”?). LC therefore includes a ground-
ing component which converts the output of the
semantic parser into executable grounded repre-
sentations that can be directly used by PS. This
component obtains definitions of predicates from
M and uses a geometric reasoner to identify feasi-
ble shape placements. The reasoner assigns loca-
tion coordinates to each shape relative to the best
bounding box it can find for the entire configura-
tion, given the grid boundaries. A full description
and its grounding process are given in Figure 3.

Grounding succeeds immediately if the agent is
given a complete goal description, as in Task 1.
Grounding fails if the goal description is incom-
plete or unknown, as in Tasks 2 and 3. In these
cases, a clarification query Q is issued and passed
to LP.

Formally, a complete goal description of
a target configuration is defined as a tuple
G = 〈{〈si, idi,∧kd(k)i 〉}i∈S ,∧j∈[S×S]fj〉, where
S is the list of shapes and si, idi and ∧kd(k)i are the
shape name, identifier, and dimension attributes
of shape i respectively. f encodes pair-wise

97



Human: “Build a square of size 3. Then, construct a
row of 4 blocks above the square. The left end
of the row should be above the upper right cor-
ner of the square.”

Parse: square(a) ∧ size(a, 3) ∧ row(b)
∧ width(b, 4) ∧ . . .

Ground: square(0, 2, 3), row(2, 5, 4)
Plan: (putdown b1 0.0 0.0), (putdown b2

1.0 0.0), (putdown b3 2.0 0.0), . . .

Figure 3: A fully-specified goal description for the
configuration in Figure 1 and its path through the
architecture. The geometric reasoner inferred that
the bounding box’s lower left corner is at (0, 2),
which is also the lower left corner of the square.

spatial relations between shapes. A complete goal
description for the configuration in Figure 1 is
〈{〈row, a, width = 4〉, 〈square, b, size = 3〉},
spatialRelation(a, b, upperRightCorner)〉.

An incomplete goal description GI is a
goal description where the values of one or
more dimensional or spatial attributes are miss-
ing: GI = G − x, where x = xd ∪ xf (with
xd ⊆ {di}i∈S and xf ⊆ {f}) is the missing infor-
mation. In this case, a query Q asks for values
of the missing dimensions or spatial relations (i.e.,
Q = x). An incomplete goal description for Fig-
ure 1 is 〈{〈row, a, width = null〉, 〈square, b,
size = 3〉},null〉. Here, the width of the row
and the spatial relation between the shapes are un-
known.

An unknown goal description GU occurs when
one or more of the shapes in a goal description are
not known to the agent (i.e., the memory module
M does not contain their definitions): GU = G
if ∃si: si ∈ S, si /∈ M. In this case, a query
Q asks for additional clarification about the un-
known concepts (i.e., Q = Define(si)). Figure 1
could be described with a new concept Foo /∈ M:
G = Foo(p) ∧ dim1(p, 3) ∧ dim2(p, 4).
4.2 Memory

A key challenge for communicating agents is the
necessity to learn the interpretation of new con-
cepts (e.g. names of unknown shapes, spatial re-
lations, or actions)1 that arise during the com-
munication with the human. In our agent, the
Memory module stores lifted representations of
these acquired concepts that are parameterized for
the configuration’s dimensions, and hence gener-
alize beyond the specific instances encountered by

1We currently restrict ourselves to unknown shapes.

Human: “Build a 3 by 4 Foo.”
Parse: Foo(p) ∧ dim1(p, 3) ∧ dim2(p, 4)
Query: Define(Foo)
System: “Sorry, I don’t know what ‘Foo’ is. Could you

describe how to build the Foo for me using
rows, columns, squares, and rectangles?”

Human: “Build a square of size 3. Then, construct a
row of 4 blocks above the square. The left end
of the row should be above the upper right cor-
ner of the square.”

Lift: Foo(p) ∧ dim1(p, ?d1) ∧ dim2(p, ?d2)→
square(a) ∧ size(a, ?d1) ∧
row(b) ∧ width(b, ?d2) ∧ spatial−rel(. . .

Figure 4: A dialogue triggered by the unknown
word “Foo” for the configuration in Figure 1. LC
issues a query that prompts a request to define
“Foo” in terms of known shapes. This definition
is first parsed and grounded as in Figure 3, then
learned by being lifted and stored in M (?dk iden-
tifies dimension variables).

the agent. When the agent receives an unknown
goal, it asks the human for a definition of the un-
known concept and expects a new goal descrip-
tion GN that defines it in terms of known concepts.
This definition is then added to the agent’s domain
knowledge stored in M.

Learning a new concept is done via a “lifting”
process, as follows:

1. When an unknown goal description GU is re-
ceived, LC issues a query QU which is then
realized and posed to the human by LP.

2. LC receives a natural language response con-
taining a new goal description GN . If GN
contains any unknown concepts, the previous
step must be called recursively. Once GN is
complete, it is grounded and passed to PS.

3. If a successful plan was generated for GN , the
concept declaration and goal definition are
lifted by converting the given dimension and
relative location constants to variables while
ensuring parameter sharing between the new
concept and its definition. A mapping is
created and inserted into M. Lifting ensures
generalization over arbitrary examples of the
new learned concept.

Figure 4 illustrates an example of a description
that elicits a query from the system for further in-
formation and the subsequent resolution process.
Our present implementation of lifting is restric-
tive. The challenges in handling the general learn-

98



ing setting and potential principled approaches
will be discussed in later sections.

4.3 Problem Solving

The problem solving module reasons about the
configurations communicated to it as conjunctive
logical expressions and generates a set of prob-
lem solving steps (plan) to achieve the given target
configuration as the output. The problem solving
module proceeds in an anytime fashion and gener-
ates a partial plan as far as possible. We employ
a Hierarchical Task Network (HTN) planner (Erol
et al., 1994; Nau et al., 2003; Ghallab et al., 2004)
that searches for plans in a space of task decom-
positions. HTN allows for reasoning over different
levels of abstraction, tasks and sub-tasks as well as
primitive actions and is more intuitive for humans.

4.4 Dialogue Mediation

The role of the dialogue mediator in our frame-
work is to guide the interaction with the human
and delegate the tasks of parsing, planning, and
query realization to the LC, PS, and LP modules
respectively. The DM interacts with a GUI frame-
work that allows for back-and-forth textual inter-
action between the human and the system as well
as a visualization component that displays the out-
put of the problem solver as block configurations
on a grid. Via this framework, the DM accepts
user-described goal descriptions and prompts the
user to reword their utterances, clarify missing in-
formation, and define new shapes as needed.

The DM is also responsible for keeping track
of the cumulative information gained about a goal
configuration over a dialogue sequence in or-
der to backtrack the states of the semantic pars-
ing and problem solving components if mistakes
occur during the interaction. In the semantic
parser, backtracking consists of restoring the hid-
den states of the parser that were seen at a par-
ticular time of the interaction before the mistake
was made. Backtracking in the problem solver in-
volves deleting or modifying items in its goal de-
scription G. But, since the parser conditions hid-
den states on all previously seen sentences, the
DM is not able to selectively delete or replace in-
formation at arbitrary points in the timeline of the
dialogue. Hence, our experiments process goal
descriptions on a sentence-by-sentence basis, al-
lowing for clarifying questions to be made and re-
solved per-sentence.

Our current agent uses a rule-based dialogue
mediator (implemented as a finite state machine),
which alternates between four sets of states:

1. Goal description parsing and planning.
Given an input user description of a goal
configuration, DM passes this input through
the LC and PS pipeline, backtracking and
requesting a rewording if either module en-
counters a failure.

2. Querying for and resolving clarifications.
When LC returns a a query Q asking for
values of missing dimensional or spatial fea-
tures, DM requests that information from the
user via the LP module (e.g., “What is the
width of the row?”). The user may respond
with a well-formed sentence describing the
missing feature value (“The width of the row
is 4”) or with a fragment containing the de-
sired information (“It’s 4,” “4,” “Width is 4”).
Given the context of the original query Q,
DM extracts the value via a heuristic and re-
forms the user input into a well formed sen-
tence, then returns to State 1 to handle the
updated description.

3. New shape learning. When LC returns a
query Q indicating an unknown shape or con-
cept, DM requests the user to describe the de-
sired configuration using known shapes (i.e.,
rows, columns, squares, and rectangles). The
description handling process proceeds regu-
larly as in State 1 until the user indicates they
are finished defining their new concept. This
triggers the learning process to lift and store
the definition in M; the new concept can then
immediately be used like any known concept
in future user descriptions.

4. Shape verification. Every time the plan for
an individual shape in a configuration has
been resolved, DM outputs the plan to the
visualization component and asks the user if
the configuration up until that point in the di-
alogue is correct. If the user indicates that
something is wrong, DM removes the entire
shape from the history of the parsing and
planning components and asks the user to
retry describing the shape from scratch. Once
the shape has been verified, however, no fur-
ther modifications may be made to that shape.

99



# Shapes Task 1 Task 2

1 100% 100%
2 64% 52%

Table 1: Accuracies for Task 1 and Task 2.

For our experiments, a simplified version of this
dialogue mediator was used to feed examples
through the system pipeline. More details can be
found in Section 5.

4.5 Language Generation
Our current system uses a predominantly
template-based approach to generate user queries.
For clarification questions, we use a simple
rule-based approach for clarifying questions that
increases the level of question explicitness if the
user continually fails to understand the question or
respond in an appropriate manner. For example,
if the query Q requests the width of a particular
row, clarification questions may range from the
very contextual “What’s the width?” to the fully
explicit “What is the width of the row you are
currently building?”

5 Methodology and Experiments

Although we ultimately wish to evaluate our
agent’s live interactions with real human users, our
current experiments feed it synthetically gener-
ated descriptions. For the descriptions with miss-
ing information, we randomly choose dimensional
and/or spatial information to omit for every shape,
providing the missing information in a follow-up
response upon system query. We also generate
sentences with descriptions with unknown terms
that have to be clarified in further interactions. Af-
ter a full interaction has been carried out (or halted
early due to system comprehension failure), we
automatically verify the system’s output configu-
ration against the gold configuration.

Input consisting of sentences describing various
block configurations and their corresponding log-
ical forms was produced using a template-based
generation system. A subset of this data was used
for training the semantic parser; the rest was used
for the following experiments. The generation sys-
tem was set up to create configurations random-
izing over shape types, sizes, and relative loca-
tions, and the templates for the sentences were de-
signed to introduce lexical and grammatical vari-
ety. Overall, the generation system is able to pro-

Human: Build a square of size 3. Then, construct a row
of 4 blocks above the square. The left end of
the row should be above the upper right corner
of the square.

System: Is this okay so far? (y/n)
Human: yes
System: Great!

Figure 5: Task 1 (Complete descriptions)

duce tens of thousands of different configuration
descriptions.

Generated descriptions contain one sentence
(for single shapes) and up to three sentences (for
two shapes). The sentences have an average length
of approximately ten words. Multi-sentence de-
scriptions pose special challenges for semantic
parsing due to the need to resolve coreference
across sentences. Thus, increasing the number of
shapes in a configuration dramatically increases
the complexity of the parsing task.

The COG system was evaluated on three sepa-
rate tasks as outlined in Section 2; we present the
results below.

5.1 Task 1: Complete Descriptions
In the first part of the experiment, we test COG’s
ability to handle complete descriptions of config-
urations (see Figure 5 for an example dialogue).
The parameter that we change in the course of this
experiment is the number of primitive shapes used
to build the configuration, which we vary from 1
to 2. For each parameter setting we have 50 test
instances; this totals 100 test instances of com-
plete descriptions with no missing information.
COG’s accuracy in interpreting these descriptions
is shown in the left column of Table 1. The drop in
accuracy in moving from one to two shapes is due
to the drop in performance of the semantic parser
when dealing with complex descriptions.

5.2 Task 2: Missing Information
In this experiment, we test COG’s ability to pro-
cess descriptions with missing information (see
Figure 6 for an example dialogue). The sys-
tem handles these situations by asking clarification
questions, obtaining responses and building the
configuration by incorporating these subsequent
responses. Again, we vary the number of primitive
shapes in the configuration from 1 to 2 and have 25
test instances for each case. Further, each primi-
tive shape could potentially have either or both the
dimension and relative spatial position missing in

100



Human: Form a column.
System: What is its height?
Human: 5 blocks
System: Is this okay so far? (y/n)
Human: yes
System: Cool! Please continue to describe your config-

uration.
Human: Finally, form a vertical line of size 5 to the

right of the column.
System: Can you describe the second column’s location

relative to the first?
Human: Ensure that the top end of the vertical line is

next to the third row of the column.
System: Is this okay so far? (y/n)

Figure 6: Task 2 (Missing information)

the initial description. For example, in a configu-
ration involving 2 shapes, there could potentially
be 3 pieces of information missing. The 25 test
instances are generated such that the number of
missing information pieces in a single instance is
uniformly random in [1, 3]. COG’s accuracy in in-
terpreting incomplete descriptions is shown in the
right column of Table 1. As before, the drop in
accuracy in moving from one to two shapes is due
to the drop in performance of the semantic parser
when dealing with complex descriptions. The ac-
curacy is further impacted by the interaction, there
are more opportunities for the parser to misinter-
pret a natural language utterance.

5.3 Task 3: Learning New Shapes
In the final experiment, we evaluate COG’s abil-
ity to learn new shapes (see Figure 7 for an ex-
ample dialogue). We teach it five descriptions of
new shapes, the results of which are presented in
Table 2. Each new shape is defined using descrip-
tions consisting of primitives (e.g., rows, columns,
squares, and rectangles) of certain dimensions. We
then tested the ability of the system to generalize
and build other instances of the new shapes with
altered dimensions. The “Outcome” column indi-
cates whether the system was able to correctly lift
all of the parameters of the input shape. Note that,
when the dimension of a new shape consists of the
sum of two dimensions (as in the third and fifth ex-
amples), parameter lifting fails for that dimension.

6 Discussion and Future Work

The goal of this paper was to present and study a
new agent architecture that supports natural, sym-
metric communication with humans in the context

Human: Construct a balloon of height 3 and width 4.
System: Sorry, I don’t know what ‘balloon’ is. Can you

describe how to build the balloon for me using
rows, columns, squares, and rectangles?

Human: Build a column of height 3. Then, build a 4
by 4 square above the column. Ensure that the
second column of the square is above the top
end of the column.

System: Is this okay so far? (y/n)
Human: yes
System: Great! I’ve now learned what a balloon is.

· · ·
Human: Build a balloon of height 2 and width 7.
System: Is this okay so far? (y/n)

Figure 7: Task 3 (Learning new shapes)

of performing a real world task. Even in the simu-
lated environment considered here, we had to ad-
dress a number of challenges. First, language un-
derstanding to support task execution differs from
e.g. standard semantic parsing in that it requires
grounded meaning representations that can be ex-
ecuted and whose grounding may depend on the
particular situation. Second, agents needs to be
able to identify, request, and incorporate missing
information that prevents task execution. Finally,
agents need to be able to identify and learn the in-
terpretation of new terms introduced during the in-
teraction with the user.

Our current implementation has a number of
obvious shortcomings. A primary bottleneck of
COG is the semantic parser. If the input descrip-
tions are not parsed correctly, there is very little
the system can do to recover beyond asking for a
rephrasing from the human. This creates problems
when we attempt to experiment with complex con-
figurations that involve three or more shapes. In
these cases, the parser struggles to correctly iden-
tify the spatial relations for the third shape. Future
work needs to address how to better handle longer
and more complex descriptions.

Additionally, as the performance of Task 2
indicates, the dialogue mediator sometimes has
problems translating partial responses into com-
plete sentences that the parser can handle robustly.
More work is required to develop a better treat-
ment of implicit arguments, coreference, and dis-
course referents that are commonly present in
these types of responses.

The generation component is also limited.
For example, we currently cannot produce in-
structions for new shapes. We have also de-

101



Initial
Descriptions

Further Explanations Outcomes

Box of width 5
and height 3

Construct a rectangle of width 5 blocks and height 3 blocks. All parameters
lifted correctly

Balloon of width 4
and height 3

Build a column of height 3. Then, build a 4 by 4 square above the column.
Ensure that the second column of the square is above the top end of the column.

All parameters
lifted correctly

Balloon of width 4
and height 7

Build a column of height 3. Then, build a 4 by 4 square above the column.
Ensure that the second column of the square is above the top end of the column.

Failed to lift
height of balloon

L of height 6 and
width 3

Build a column of 6 blocks. Then, build a row of 3 blocks to the right of the
column. Make sure that the left end of the row is to the right of the bottom end

of the column.

All parameters
lifted correctly

L of height 6 and
width 4

Build a column of 6 blocks. Then, build a row of 3 blocks to the right of the
column. Make sure that the left end of the row is to the right of the bottom end

of the column.

Failed to lift
width of L

Table 2: Task 3 results for learning new shapes. We initially provide descriptions containing new shape
terms with their parameters (left column). When prompted for further clarification, we then provide the
descriptions in the middle column. After the shapes were learned, we instructed COG to build more
instances of these new shapes while varying the size parameters to test how well the lifting worked; the
result of this is in the right column.

veloped a grammar-based realizer inspired by
OpenCCG (White and Baldridge, 2003; White,
2006) that operates over the first-order semantic
representations used by our agent. We plan to
augment the realizer’s semantic lexicon with the
learned definitions of predicates for new shapes,
allowing our system to generate natural language
instructions describing the new configurations.

One of the key challenges of the scenario we
envision (and a fundamental problem in language
acquisition) is the necessity to generalize across
situations. This is required in order to learn gen-
eral concepts from a few specific instances. At this
point, our agent is able to generalize from a sin-
gle example, but our learning mechanism is rather
naive, and we can only handle simple parame-
ter lifting from the primitive components of the
new shape. To illustrate this, consider an exam-
ple where we want to teach the system the shape
“balloon”. To do so, we must specify the dimen-
sions of the square and column separately; con-
sequently, the system will learn that the height of
a “balloon” corresponds only to the height of the
column (see Table 2). However, ideally we would
like the system to learn a parameterization of “bal-
loon” where the balloon’s height corresponds to
the sum of the square and column heights.

Our next step will focus on improving this
mechanism. One possible direction involves
the use of Inductive Logic Programming to in-
duce high-level hypotheses from observations and

background domain knowledge. One key chal-
lenge here, beyond learning, is a way to incorpo-
rate more complex learned hypotheses into our ar-
chitecture in such a way that other components in
our system can make use of it. A second chal-
lenge is that the agent is learning from a human
that has limited knowledge, and little patience, so
we cannot expect to see a large number of exam-
ples. We will consider the use of probabilistic
logic models which can handle both issues by ex-
plicitly including the trade-off in the optimization
function (Odom et al., 2015).

A final challenge is the application of our agent
to new domains. Currently, the memory module
contains all the knowledge required to plan and
produce comprehensible responses. This declara-
tive approach should generalize well to some sim-
ple enough domains, but will need to be extended
to deal with more involved tasks and domains.

Acknowledgements

This work was supported by Contract W911NF-
15-1-0461 with the US Defense Advanced Re-
search Projects Agency (DARPA) Communicating
with Computers Program and the Army Research
Office (ARO). Approved for Public Release, Dis-
tribution Unlimited. The views expressed are
those of the authors and do not reflect the official
policy or position of the Department of Defense or
the U.S. Government.

102



References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473 .

J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world’s
response. In Proc. of the Conference on Com-
putational Natural Language Learning (CoNLL).
http://cogcomp.cs.illinois.edu/papers/CGCR10.pdf.

Li Dong and Mirella Lapata. 2016. Language
to logical form with neural attention. In Pro-
ceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 33–43.
http://www.aclweb.org/anthology/P16-1004.

Kutluhan Erol, James Hendler, and Dana S Nau. 1994.
HTN planning: complexity and expressivity. In Pro-
ceedings of the Twelfth AAAI National Conference
on Artificial Intelligence. pages 1123–1128.

R. E. Fikes and N. Nilsson. 1971. STRIPS: A new
approach to the application of theorem proving to
problem solving. Artificial Intelligence 2(3-4):189–
208.

Malik Ghallab, Dana Nau, and Paolo Traverso. 2004.
Automated planning: theory & practice. Elsevier.

Jiatao Gu, Zhengdong Lu, Hang Li, and Vic-
tor O.K. Li. 2016. Incorporating copying mech-
anism in sequence-to-sequence learning. In Pro-
ceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Berlin, Germany, pages 1631–1640.
http://www.aclweb.org/anthology/P16-1154.

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallap-
ati, Bowen Zhou, and Yoshua Bengio. 2016.
Pointing the unknown words. In Proceed-
ings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Berlin, Germany, pages 140–149.
http://www.aclweb.org/anthology/P16-1014.

Robin Jia and Percy Liang. 2016. Data recom-
bination for neural semantic parsing. In Pro-
ceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 12–22.
http://www.aclweb.org/anthology/P16-1002.

Dana S Nau, Tsz-Chiu Au, Okhtay Ilghami, Ugur
Kuter, J William Murdock, Dan Wu, and Fusun Ya-
man. 2003. SHOP2: An HTN planning system.
Journal of Artificial Intelligence Research 20:379–
404.

P. Odom, T. Khot, R. Porter, and S. Natarajan. 2015.
Knowledge-based probabilistic logic learning. In
Proceedings of the Twenty-Ninth AAAI Conference
on Artificial Intelligence. pages 3564–3570.

Stefanie Tellex, Thomas Kollar, Steven Dickerson,
Matthew R. Walter, Ashis Gopal Banerjee, Seth J.
Teller, and Nicholas Roy. 2011. Understanding nat-
ural language commands for robotic navigation and
mobile manipulation. In Proceedings of the Na-
tional Conference on Artificial Intelligence (AAAI).

Michael White. 2006. Efficient realization of coordi-
nate structures in Combinatory Categorial Grammar.
Research on Language & Computation 4(1):39–75.

Michael White and Jason Baldridge. 2003. Adapting
chart realization to CCG. In Proceedings of the 9th
European Workshop on Natural Language Genera-
tion. pages 119–126.

J. M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming.
In Proceedings of the National Conference on Arti-
ficial Intelligence (AAAI). pages 1050–1055.

L. Zettlemoyer and M. Collins. 2005. Learning to map
sentences to logical form: Structured classification
with probabilistic categorial grammars. In Proceed-
ings of Uncertainty in Artificial Intelligence (UAI).

103


