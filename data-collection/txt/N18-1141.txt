



















































Learning to Collaborate for Question Answering and Asking


Proceedings of NAACL-HLT 2018, pages 1564–1574
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Learning to Collaborate for Question Answering and Asking

Duyu Tang‡, Nan Duan‡, Zhao Yan†, Zhirui Zhang[, Yibo Sun§,
Shujie Liu‡, Yuanhua Lv\, Ming Zhou‡
‡Microsoft Research Asia, Beijing, China

\Microsoft AI and Research, Sunnyvale CA, USA
†Beihang University, Beijing, China

[University of Science and Technology of China, Anhui, China
§Harbin Institute of Technology, Harbin, China

{dutang,nanduan,v-zhaoya,v-zhirzhi,v-yibsu,shujliu,yuanhual,mingzhou}@microsoft.com

Abstract

Question answering (QA) and question gener-
ation (QG) are closely related tasks that could
improve each other; however, the connection
of these two tasks is not well explored in lit-
erature. In this paper, we give a systemat-
ic study that seeks to leverage the connection
to improve both QA and QG. We present a
training algorithm that generalizes both Gen-
erative Adversarial Network (GAN) and Gen-
erative Domain-Adaptive Nets (GDAN) under
the question answering scenario. The two key
ideas are improving the QG model with QA
through incorporating additional QA-specific
signal as the loss function, and improving the
QA model with QG through adding artificially
generated training instances. We conduct ex-
periments on both document based and knowl-
edge based question answering tasks. We have
two main findings. Firstly, the performance
of a QG model (e.g in terms of BLEU score)
could be easily improved by a QA model vi-
a policy gradient. Secondly, directly applying
GAN that regards all the generated questions
as negative instances could not improve the ac-
curacy of the QA model. Learning when to re-
gard generated questions as positive instances
could bring performance boost.

1 Introduction

In this work, we consider the task of joint learn-
ing of question answering and question genera-
tion. Question answering (QA) and question gen-
eration (QG) are closely related natural language
processing tasks. The goal of QA is to obtain an
answer given a question. The goal of QG is almost
reverse which is to generate a question from the
answer. In this work, we consider answer selec-
tion (Yang et al., 2015; Balakrishnan et al., 2015)
as the QA task, which assigns a numeric score to
each candidate answer, and selects the top ranked
one as the answer. We consider QG as a generation

problem and exploit sequence-to-sequence learn-
ing (Seq2Seq) (Du et al., 2017; Zhou et al., 2017)
as the backbone of the QG model.

The key idea of this work is that QA and QG are
two closely tasks and we seek to leverage the con-
nection between these two tasks to improve both
QA and QG. Our primary motivations are twofold-
s. On one hand, the Seq2Seq based QG model
is trained by maximizing the literal similarity be-
tween the generated sentence and the ground truth
sentence with maximum-likelihood estimation ob-
jective function (Du et al., 2017). However, there
is no signal indicating whether or not the gener-
ated sentence could be correctly answered by the
input. This problem could be precisely mitigated
through incorporating QA-specific signal into the
QG loss function. On the other hand, the capac-
ity of a statistical model depends on the quality
and the amount of the training data (Sun et al.,
2017). In our scenario, the capacity of the QA
model depends on the difference between the pos-
itive and negative patterns embodied in the train-
ing examples. A desirable training dataset should
contain the question-answer pairs that are literal-
ly similar yet have different category labels, i.e.
some question-answer pairs are correct and some
are wrong. However, this kind of dataset is hard
to obtain in most situations because of the lack of
manual annotation efforts. From this perspective,
the QA model could exactly benefit from the QG
model through incorporating additional question-
answer pairs whose questions are automatically
generated by the QG model1.

To achieve this goal, we present a training al-
gorithm that improves the QA model and the

1An alternative way is to automatically generate answers
for each question. Solving the problem in this condition re-
quires an answer generation model (He et al., 2017), which
is out of the focus of this work. Our algorithm could also be
adapted to this scenario.

1564



QG model in a loop. The QA model improves
QG through introducing an additional QA-specific
loss function, the objective of which is to max-
imize the expectation of the QA scores of the
generated question-answer pairs. Policy gradi-
ent method (Williams, 1992; Yu et al., 2017) is
used to update the QG model. In turn, the QG
model improves QA through incorporating ad-
ditional training instances. Here the key prob-
lem is how to label the generated question-answer
pair. The application of Generative Adversarial
Network (GAN) (Goodfellow et al., 2014; Wang
et al., 2017) in this scenario regards every generat-
ed question-answer pair as a negative instance. On
the contrary, Generative Domain-Adaptive Nets
(GDAN) (Yang et al., 2017) regards every gener-
ated question-answer pair appended with special
domain tag as a positive instance. However, it is
non-trivial to label the generated question-answer
pairs because some of which are good paraphras-
es of the ground truth yet some might be nega-
tive instances with similar utterances. To address
this, we bring in a collaboration detector, which
takes two question-answer pairs as the input and
determines their relation as collaborative or com-
petitive. The output of the collaboration detector
is regarded as the label of the generated question-
answer pair.

We conduct experiments on both documen-
t based (Yang et al., 2015) and knowledge (e.g.
web table) based question answering tasks (Bal-
akrishnan et al., 2015). Results show that the per-
formance of a QG model (e.g in terms of BLEU
score) could be consistently improved by a QA
model via policy gradient. However, regarding
all the generated questions as negative instances
(competitive) could not improve the accuracy of
the QA model. Learning when to regard generat-
ed questions as positive instances (collaborative)
could improve the accuracy of the QA model.

2 Related Work

Our work connects to existing works on question
answering (QA), question generation (QG), and
the use of generative adversarial nets in question
answering and text generation.

We consider two kinds of answer selection tasks
in this work, one is table as the answer (Balakr-
ishnan et al., 2015) and another is sentence as
the answer (Yang et al., 2015). In natural lan-
guage processing community, there are also other

types of QA tasks including knowledge based QA
(Berant et al., 2013), community based QA (Qi-
u and Huang, 2015) and reading comprehension
(Rajpurkar et al., 2016). We believe that our algo-
rithm is generic and could also be applied to these
tasks with dedicated QA and QG model architec-
tures. Despite the use of sophisticated features
could learn a more accurate QA model, in this
work we implement a simple yet effective neural
network based QA model, which could be conven-
tionally jointly learned with the QG model through
back propagation.

Question generation draws a lot of attentions re-
cently, which is partly influenced by the remark-
able success of neural networks in natural lan-
guage generation. There are several works on gen-
erating questions from different resources, includ-
ing a sentence(Heilman, 2011), a topic (Chali and
Hasan, 2015), a fact from knowledge base (Serban
et al., 2016), etc. In computer vision community,
there are also recent studies on generating ques-
tions from an image (Mostafazadeh et al., 2016).
Our QG model belongs to sentence-based question
generation.

GAN has been successfully applied in comput-
er vision tasks (Goodfellow et al., 2014). There
are also some recent trials that adapt GAN to tex-
t generation (Yu et al., 2017), question answer-
ing (Wang et al., 2017), dialogue generation (Li
et al., 2016), etc. The relationship of the dis-
criminator and the generator in GAN are competi-
tive. The key finding of this work is that, directly
applying the idea of “competitive” in GAN does
not improve the accuracy of a QA model. We
contribute a generative collaborative network that
learns when to collaborate and yields empirical
improvements on two QA tasks.

This work relates to recent studies which at-
tempt to improve the performance of a discrimi-
native QA model with generative models (Wang
et al., 2017; Yang et al., 2017; Dong et al., 2017;
Duan et al., 2017). These works regard QA as the
primary task and use auxiliary task, such as ques-
tion generation and question paraphrasing, to im-
prove the primary task. This is one part of our goal
and our another goal is to improve the QG mod-
el with the QA system and further to increasingly
improve both tasks in a loop.

In terms of assigning category label to the gen-
erated question, Generative Adversarial Network
(GAN) (Goodfellow et al., 2014; Wang et al.,

1565



2017) and Generative Domain-Adaptive Nets (G-
DAN) (Yang et al., 2017) could be viewed as spe-
cial cases of our algorithm. Our algorithm learns
when to assign positive or negative labels, while
GAN always assigns negative labels and GDAN
always assigns positive labels. Besides, our work
differs from (Wang et al., 2017) in that our ques-
tion generation model is a generative model while
theirs is actually a discriminative matching mod-
el. The approach of (Dong et al., 2017) learns to
generate question from question via paraphrasing,
and use the generated questions in the inference
process. In this work, the QA model and the QG
model are applied separately in the inference pro-
cess. This inspires us to jointly conduct QA and
QG in the inference process, which we leave as a
future work.

3 Generative Collaborative Network

In this section, we first formulate the task of QA
and QG, and then present our algorithm that joint-
ly trains the QA and QG models.

3.1 Task Formulation

This work involves two tasks: question answering
(QA) and question generation (QG).

There are different kinds of QA tasks in the nat-
ural language processing area. To verify the scal-
ability of our algorithm, we consider two types of
answer selection tasks, both of which are funda-
mental QA tasks in research community and of
great importance in industrial applications includ-
ing web search and chatbot. Both tasks take a
question q and a list of candidate answers A =
{a1, a2, ..., an} as input, and outputs an answer
ai which has the largest probability to correctly
answer the question. The only difference is that
the answer in the task of answer sentence selec-
tion (Yang et al., 2015) is a natural language sen-
tence, while the answer in table search (Balakr-
ishnan et al., 2015) is a structured table consisting
of caption, attributes and cells. Our QA model is
abbreviated as Pqa(a, q; θqa), whose output is the
probability that q and a being a correct question-
answer pair, and the parameter is denoted as θqa.

The task of QG takes an answer a which is a
natural language sentence or a structured table,
and outputs a natural language question q which
could be answered by a. Inspired by the remark-
able progress of sequence-to-sequence (Seq2Seq)
learning in natural language generation, we deal

with QG in an end-to-end fashion and develop a
generative model based on Seq2Seq learning. Our
QG model is abbreviated as Pqg(q|a; θqg), whose
output is the probability of generating q from a
and the parameter is denoted as θqg.

3.2 Algorithm Description

We describe the joint learning algorithm in this
part. The end goal is to leverage the connection of
QA and QG to improve the performances on both
QA and QG tasks. A brief illustration of the train-
ing progress is given in Figure 1 , which includes
a QA model, a QG model and a collaboration de-
tector (CD). A formal description of the algorith-
m is given in Algorithm 1. We can see that the
QA model and the QG model both have two train-
ing objectives. One part is the standard supervised
learning objective based on task-specific supervi-
sions. Another part of the objective is obtained by
leveraging auxiliary tasks.

QG

answer

(a) QG improves QA (b) QA improves QG

samplesCD

QA

QG

answer

samplesQA

QG

Figure 1: An brief illustrating of the joint training pro-
cess. The red dashed line stands for the model being
updated. QA, QG and CD stand for question answer-
ing, question generation and collaboration detection,
respectively.

The supervised objective of the QA model is
to maximize the probability of the correct catego-
ry label, and the supervised objective of the QG
model is to maximize the probability of the cor-
rect question sequence. Since the goal of QA is to
predict whether a question-answer pair is correc-
t or not, training the QA model requires negative
QA pairs whose labels are zero. But these negative
QA pairs are not used for training the QG model as
the goal of QG is to generate the correct question.

The main contribution of this work is to explore
effective learning objectives that leverage auxil-
iary tasks. In order to improve the QA model,
we generate additional training instances, each of
which is composed of a question, an answer and
a category label. In this work, we clamp the an-
swer part and feed the answer to the QG model to

1566



Algorithm 1 Generative Collaborative Network for QA and QG
1: Input: training data D; the batch size for QG training m; the beam size for QG inference k; hyper parameters λqg and
λqg; hyper parameters bqa and bqg; pretrained collaboration detector Pcd(q, q′)

2: Output: QA model Pqa(a, q) parameterized by θqa; QG model Pqg(q|a) parameterized by θqg

3: pretrain Pqa(a, q) and Pqg(q|a) separately on D
4: repeat
5: get a minibatch of positive QA pairs PD = {qpi , api } ∈ D, 1 ≤ i ≤ m, in which api is the answer of qpi
6: get a minibatch of negative QA pairs ND = {qni , ani } ∈ D, 1 ≤ i ≤ m, in which ani is not the answer of qni
7: apply Pqg(q|a) on PD to generate in a list of question-answer beams GD = {qgij , agi }, 1 ≤ i ≤ m, 1 ≤ j ≤ k
8: apply Pqa(a, q) on GD to assign a QA-specific score to each generated instance
9: choose the top ranked result from each beam in GD, and then apply Pcd(q, q′) on the selected instance

10: update θqa by maximizing the following objective

m∑

i=1

(
logPqa(a

p
i , q

p
i ) + log

(
1− Pqa(ani , qni )

))
+ λqa

m∑

i=1

(
Ibqa [Pcd(q

p
i , q

g
i0)]logPqa(a

p
i , q

g
i0)
)

+ λqa

m∑

i=1

((
1− Ibqa [Pcd(qpi , qgi0)]

)
log
(
1− Pqa(api , qgi0)

))
(1)

11: update θqg by maximizing the following objective
m∑

i=1

logPqa(q
p
i |api ) + λqg

m∑

i=1

k∑

j=1

Pqa(a
p
i , q

g
ij)logPqg(q

g
ij |api ) (2)

12: until models converge

generate the question. We use beam search and s-
elect the top ranked result as the question.2 Here
the issue is how to infer the label of the generated
instance. We believe that heuristically assigning
the label as 0 or 1 is problematic. For instance,
let us suppose the answer sentence is “Microsoft
was founded by Paul Allen and Bill Gates on April
4, 1975.”, and the ground truth question is “who
founded Microsoft”. In this case, the generated
question “who is the founder of Microsoft” is a
good one yet “who is the founder of Google” and
“how old is Bill Gates” are both bad cases. To
address this, we introduce an additional collabo-
ration detector (CD) to infer the label of the gen-
erated instance. Intuitively, the CD acts as a dis-
criminative paraphrase model, which measures the
semantic similarity between the ground truth ques-
tion and generated question. In equation (1), the
Ibqa(x) is an indicator function whose value is 1 if
the value of x is larger than a threshold bqa, such
as 0.5 or 0.3. The hyper parameter λqa controls the
weight of the auxiliary objective to the QA model.

In turn, the QA model is used to assign a QA-
specific score Pqa(a, q′) to each generated ques-
tion q′. We follow the recent reinforcement learn-
ing based approach for dialogue prediction (Li
et al., 2016), and define simple heuristic reward-

2We also implemented using all the beam search results
or sampling one result from the beam. However, these tricks
do not bring performance boost.

s that characterize good questions. The goodness
of the generated question is measured by the pre-
diction of the QA model. Similar to the strat-
egy adopted by (Zaremba and Sutskever, 2015),
we use a baseline strategy bqg (e.g. 0.5) to de-
crease the learning variance. The expected reward
(Williams, 1992; Yu et al., 2017) for a generated
question is given in Equation (2). In this way, the
parameters of the QG model could be convention-
ally updated with stochastic gradient descent.

We pretrain the QA model and the QG model
before the joint learning process. The main reason
is that a randomized QA model will provide unre-
liable rewards to the QG model, and a randomized
QG model will generate bad questions.

4 The Neural Architecture of Each
Module

Our algorithm includes a question answer (QA)
model, a question generation (QG) model and a
collaboration detector (CD) model. We implement
these models with dedicated neural networks.

As we have mentioned before, our training algo-
rithm is applied to both document-based and we-
b table based question answer tasks. In this sec-
tion, we take table based QA and QG tasks to de-
scribe the neural architecture of each module. A
question/query q is a natural language expression
consisting of a list of words. A table t has fixed

1567



schema including one or more headers, one or
more cells, and a caption. A header indicates the
property of a column, and a cell is a unit where a
row and a column intersects. The caption is typi-
cally an explanatory text about the table.

4.1 The Question Answer (QA) Model

We develop a neural network to match a natural
language question/query to a structured table. S-
ince a table has multiple aspects including head-
ers, cells and the caption, the model is developed
to capture the semantic relevance between a query
and a table at different levels.

As the meaning of a query is sensitive to the
word order, i.e. the intentions of “list of flights lon-
don to berlin” and “list of flights berlin to london”
are totally different, we represent a query with a
sequential model. In this work, recurrent neural
network (RNN) is used to map a query of vari-
able length to a fixed-length vector. We use gated
recurrent unit (GRU) (Cho et al., 2014) as the ba-
sic computation unit, which adaptively forgets the
history and remembers the input.

zi = σ(Wze
q
i + Uzhi−1) (3)

ri = σ(Wre
q
i + Urhi−1) (4)

h̃i = tanh(Whe
q
i + Uh(ri � hi−1)) (5)

hi = zi � h̃i + (1− zi)� hi−1 (6)

where zi and ri are update and reset gates of GRU.
We use bi-directional RNN to get the meaning of
a query from forward and backward directions,
and concatenate two last hidden states as the query
vector.

An important property of a table is that ex-
changing two rows or two columns does not
change its meaning. To satisfy this condition,
we develop an attention based approach, in which
the header and cells are regarded as the exter-
nal memory. Each header/cell is represented as
a vector. Given a query vector, the model cal-
culates the weight of each memory unit and then
output a query-specific header/cell representation
through weighted average (Bahdanau et al., 2015;
Sukhbaatar et al., 2015). This process could be
repeated executed for several times, so that more
abstractive evidences could be retrieved and com-
posed to support the final decision. Similar tech-
niques have been successfully applied in table-
based question answering (Yin et al., 2015b; Nee-
lakantan et al., 2015).

We represent the table caption with RNN, the
same strategy we have adopted to represent the
query. Element-wised multiplication is used to
compose the query vector and the caption vec-
tor. Furthermore, since the number of co-occurred
words directly reflect the relatedness between the
question and the answer, we incorporate the em-
bedding of co-occurred word count as addition-
al features. Finally, we feed the concatenation
of all the vectors to a softmax layer whose out-
put length is 2. We have implemented a ranking
based loss function lqa = max(0, 1− Pqa(a, q) +
Pqa(a

′, q)) and a negative log-likelihood (NLL)
base loss function lqa = − log(Pqa(a, q)). We
find that NLL works better and use it in the fol-
lowing experiments.

4.2 The Question Generation (QG) Model

Inspired by the notable progress that sequence-
to-sequence learning (Seq2Seq) (Sutskever et al.,
2014) has made in natural language generation, we
implement a table-to-sequence (Table2Seq) ap-
proach that generates natural language question
from a structured table.

Table2Seq is composed of an encoder and a de-
coder. The encoder maps the caption, headers, and
cells into continuous vectors, which will be fed to
the decoder to generate a question in a sequential
way. Similar with the way we have adopted in the
QA model, we represent the caption with bidirec-
tional GRU based RNN. The vector of each word
in the caption is the concatenation of hidden states
from both directions. The vectors of headers and
cells are regarded as additional hidden states of the
encoder. The representation of each cell is also
mixed with the corresponding header representa-
tion. The initial vector of the decoder is the av-
erage of the caption vector, header vector, and the
cell vector.

The backbone of the decoder is an attention
based GRU RNN, which generates a word at each
time step and repeats the process until generating
the end-of-sentence symbol. We made two modifi-
cations to adapt the decoder to the table structure.
The first modification is that the attention model
is calculated over the headers, cells and the cap-
tion of a table. Ideally, the decoder should learn
to focus on a region of the table when generat-
ing a word. The second modification is a table
based copying mechanism. It has been proven that
the copying mechanism (Gulcehre et al., 2016; Gu

1568



et al., 2016) is an effective way to replicate low-
frequent words from the source to the target se-
quence in sequence-to-sequence learning. In the
decoding process, a word is generated either from
the target vocabulary via standard softmax or
from a table via the copy mechanism. A neu-
ral gate gt is used to trade-off between generat-
ing from the target vocabulary and copying from
the table. The probability of generating a word y
calculated as follows, where αt(y) is the attention
probability of the word y from the table at time
step t and βt(y) is the probability of predicting the
word y from the softmax at time step t.

pt(y) = gt � αt(y) + (1− gt)� βt(y) (7)

Since every component of the Table2Seq is d-
ifferentiable, the parameters could be optimized
in an end-to-end fashion with back-propagation.
Given a question-answer pair (x, y), the super-
vised training objective is to maximize the prob-
ability of question word at each time step. In the
inference process, beam search is used to generate
top-k confident results, where k is the beam size.

4.3 The Collaboration Detector (CD)

The goal of a collaboration detector is to deter-
mine the label of the instance generated by the QG
model. The positive prediction, namely the pre-
dicted value is equals to 1, stands for the collabo-
rative relationship between the generated instance
and the ground truth, while the negative prediction
stands for the competitive relationship.

We consider this task as predicting the catego-
ry of the given two question-answer pairs, one of
which is the ground truth, and another is the gener-
ated question-answer pair. Since the answer part is
the same, we simplify the problem as classifying
two questions as related or not, which is a binary
classification problem.

The neural architecture of the collaboration de-
tector (CD) is exactly the same as the caption com-
ponent in the QA model. We represent two ques-
tions with bidirectional RNN, and use element-
wise multiplication to do the composition. The
result is further concatenated with a co-occurred
word count embedding, followed by a softmax
layer. The model is trained by minimizing the neg-
ative log-likelihood label, which is provided in the
training data.

The training data of the CD model includes two

parts. The first part is from Quora dataset3, which
is built for detecting if pairs of question text are
semantically equivalent. The Quora dataset has
345,989 positive question pairs and 255,027 nega-
tive pairs. We further obtain the second part of the
training data from the web queries, which are more
similar to the web queries in our two QA task. We
obtain the query dataset from query logs through
clustering the web queries that click the same we-
b page. In this way, we obtain 6,118,023 positive
query pairs. We use a heuristic rule to generate the
negative instances for the query dataset. For each
pair of query text, we clamp the first query and
retrieve a query that is mostly similar to the sec-
ond query. To improve the efficiency of this pro-
cess, we randomly sample 10,000 queries and de-
fine the “similarity” as the number of co-occurred
words in two questions. In this way we collec-
t another 6,118,023 negative pairs of query tex-
t. We initialize the values of word embeddings
with 300d Glove vectors4, which is learned on
Wikipedia texts. We use a held-out data consist-
ing of 20K query pairs to check the performance
of the CD model. The accuracy of the CD model
on the held-out dataset is 83%. In the joint train-
ing process, we clamp the parameters of the CD
model and use its outputs to facilitate the learning
of the QA model.

5 Experiment

We conduct experiments on table-based QA and
document-based QA tasks. We will describe ex-
perimental settings and report results on these two
tasks in this section.

5.1 Table based QA and QG
Setting We take table retrieval (Balakrishnan
et al., 2015) as the table-based QA task. Given a
query and a collection of candidate table answers,
the task aims to return a table that is most relevan-
t to the query. Figure 2 gives an example of this
task, in which a query matches to different aspects
of a table. We regard document-based QA tasks as
a special case of the table-based QA task, in which
the cells and the headers are both empty.

We conduct experiments on the web data. The
queries come from real-world user queries which
we obtain from the search log of a commercial

3https://data.quora.com/
First-Quora-Dataset-Release-Question-Pairs

4https://nlp.stanford.edu/projects/
glove/

1569



star trek rts Query

Star Trek Games for the PC

Cells

Headers

Caption

Star Trek: Armada II RTS 2001 65

Star Trek: Away Team RTS 2001 64

Star Trek: Deep Space 
Nine: Dominion Wars

RTS 2001 64

Star Trek: New Worlds RTS 2000 52

Game Genre Year Metascore

Figure 2: An example illustrating the table-based QA
task.

search engine. We filter them down to only those
that are directly answered by a table. In this way,
we collect 1.49M query-table pairs. An example
of the data is given in Figure 2. We randomly se-
lect 1.29M as the training set, 0.1M as the dev set
and 0.1M as the test set.

We evaluate the performance on table-based QA
with Mean Average Precision (MAP) and Preci-
sion@1 (P@1) (Manning et al., 2008). We use
the same candidate retrieval adopted in (Yan et al.,
2017), namely representing a table as bag-of-
words, to guarantee the efficiency of the approach.
Each query has 50 candidate tables on average. It
is still an open problem to automatically evaluate
the performance of a natural language generation
system (Lowe et al., 2017). In this work, we use
BLEU-4 (Papineni et al., 2002) score as the evalu-
ation metric, which measures the overlap between
the generated question and the referenced ques-
tion. The hyper parameters are tuned on the vali-
dation set and the performance is reported on the
test set.

Results and Analysis We report the results and
our analysis on table-based QA and QG respec-
tively in this part.

We first report the results of single systems on
table-based QA. We compare to four single sys-
tems implemented by (Yan et al., 2017). In BM25,
each table is represented as a flattened vector, and
the similarity between a query and a table is cal-
culated with the BM25 algorithm. WordCnt uses
the number of co-occurred words in query-caption
pair, query-header pair, and query-cell pair, re-
spectively. MT based PP is a phrase-level fea-
ture. The features come from a phrase table which
is extracted from bilingual corpus via statistical
machine translation approach (Koehn et al., 2003).

LambdaMART (Burges, 2010) is used to train the
ranker. CNN uses convolutional neural network
to measure the similarity between the query and
table caption, table headers, and table cells, re-
spectively. TQNN is the table-based QA model
implemented in this work, which is regard as the
baseline for the joint learning algorithm. Results
of single systems are given in Table 1. We can see
that BM25 is a simple yet very effective baseline
method. Our basic model performs better than all
the single models in terms of MAP.

Method MAP Acc@1
BM25 0.429 0.294
WordCnt 0.318 0.190
MT based PP 0.327 0.213
CNN 0.359 0.238
TQNN (baseline) 0.439 0.285
Seq2SeqPara 0.437 0.283
GCN (competitive) 0.436 0.282
GCN (collaborative) 0.446 0.292
GCN (final) 0.456 0.301

Table 1: The performances of single systems on table-
based question answering (p-value < 0.01 with t-test
between TQNN and GCN).

We also implement four different joint learning
settings. In these settings, the QA model and the
QG model are all pretrained, and the same way
(policy gradient) is used to improve the QG mod-
el via the QA predictions. The only difference is
how the QA model benefits from the QG mod-
el. As we use external resources to train a CD
model, we also implement Seq2SeqPara for com-
parison. We train a question generator with a Se-
q2Seq model on the CD training data, and regard
the generated questions as positive instances. Our
generative collaborative network is abbreviated as
GCN. GCN (competitive) is analogous to (Good-
fellow et al., 2014), where all the generated ques-
tions are regarded as negative instances (with label
as zero). On the contrary, GCN (collaborative) is
analogous to (Yang et al., 2017), where the gen-
erated questions are regard as positive instances.
Our main observation from Table 1 is that simply
regarding all the generated questions as negative
instances (“competitive”) could not bring perfor-
mance boost. On the contrary, regarding the gen-
erated questions as positive ones (“collaborative”)
improves the QA model. Our algorithm (GCN)
significantly improves the TQNN model. Based
on these results, we believe that the relationship

1570



between the QA model and the QG model should
not be always competitive. Learning when to col-
laborate through leveraging a CD model is a prac-
tical way to improve the performance on question
answering.

As described in Equation (1), the influence of
the CD model on the QA model also depends on
the value of the hyper parameter bqa. A smal-
l value of bqa stands for a preference of “collab-
orative”, while a large value of bqa represents a
preference of “competitive”. Results are given in
Figure 3. The GCN model performs better when
bqa is in the range [0.3, 0.5], in which the model
prefers “collaborative”.

0.0 0.2 0.4 0.6 0.8 1.0
bqa

0.4350

0.4375

0.4400

0.4425

0.4450

0.4475

0.4500

0.4525

0.4550

M
ea

n 
Av

er
ag

e 
Pr

ec
isi

on

Figure 3: The performances of GCN on table-based
QA with different values of bqa. GCN falls back to
“competitive” when bqa equals to one. GCN is totally
collaborative when bqa equals to zero.

We conduct an additional experiment to test
whether our algorithm could improve an existing
system. We take BM25 as the baseline, and incor-
porate one of the five joint models as an additional
feature. LambdaMART is used to train the com-
bined ranker. Results are given in Table 2. We can
see that the baseline system could be dramatical-
ly improved by our system, despite the improve-
ments of different approaches are on par.

Method MAP Acc@1
BM25 0.429 0.294
BM25 + TQNN (baseline) 0.650 0.513
BM25 + GAN 0.654 0.519
BM25 + Seq2SeqPara 0.650 0.513
BM25 + GDAN 0.658 0.523
BM25 + GCN (this work) 0.660 0.526

Table 2: The performances of combined systems on
table-based question answering..

We have reported the results on table-based QA.

Here we show the performances of different ap-
proaches on table-based QG. Results in terms of
BLEU-4 are given in Table 3. Different from the
trends on QA, “competitive” performs better than
“collaborative” on QG. This is reasonable because
as the joint training progresses, the QA model in
“collaborative” keeps telling the QG model that
the generated instances are good enough. On the
contrary, the “competitive” model is more critical,
which tells the QG model how wrong the generat-
ed questions are. In this way, the QG model could
be increasingly improved by the QA signal. The
QG model is easier to be improved compared to
the QA model. Our GCN approach obtains a sig-
nificant improvement over the baseline model on
this task.

Method Dev Test
Table2Seq (baseline) 15.71 15.54
Seq2SeqPara 17.01 16.95
GCN (competitive) 17.28 17.34
GCN (collaborative) 16.77 16.66
GCN (final) 17.59 17.61

Table 3: The performances on table-based question
generation. Evaluation metric is BLEU-4 score.

We also report the learning curve of the GCN
model as the joint training progresses. The perfor-
mance on the dev set is given in Figure 4.

0 20000 40000 60000 80000
Number of Training Batches

0.450

0.452

0.454

0.456

0.458

0.460

0.462

0.464

0.466

QA
 p

er
fo

rm
an

ce
 (M

AP
)

16.4

16.6

16.8

17.0

17.2

17.4

17.6

QG
 p

er
fo

rm
an

ce
 (B

LE
U)

Figure 4: The learning curve of GCN on the dev data.
The evaluation metrics are MAP (for QA) and BLEU
(for QG).

5.2 Document based QA and QG
To test the scalability of the algorithm, we also ap-
ply it to document based QA and QG tasks. The
QA task is answer sentence selection (Yang et al.,

1571



Method MAP P@1
WordCnt 0.395 0.179
CDSSM (Shen et al., 2014) 0.442 0.228
ABCNN (Yin et al., 2015a) 0.469 0.263
DSL (Tang et al., 2017) 0.484 0.275
DQNN (baseline) 0.471 0.263
Seq2SeqPara 0.470 0.260
GCN (competitive) 0.468 0.257
GCN (collaborative) 0.476 0.272
GCN (final) 0.492 0.282

Table 4: The performance on document-based QA task
(p-value < 0.05 with t-test between DQNN and GCN).

2015). Given a question and a list of candidate
answer sentences from a document, the goal is to
find a most relevant answer sentence as the answer.
Since the WikiQA dataset (Yang et al., 2015) is
too small to learn a powerful question generator,
we use the MARCO dataset (Nguyen et al., 2016),
which is originally designed for reading compre-
hension yet also has manually annotated labels
for sentence/passage selection. A characteristic of
MARCO dataset is that the ground truth of the test
is invisible to the public. Therefore, we follow
(Tang et al., 2017) and split the original validation
set into the dev set and the test set. The results
on QA and QG are given in Table 4 and Table 5.
We can see that the results are almost consistent
with the results on table-based QA and QG tasks.
Our GCN algorithms achieves promising perfor-
mances compared to strong baseline methods.

Method BLEU-4
Seq2Seq (baseline) 8.87
DSL (Tang et al., 2017) 9.31
Seq2SeqPara 9.16
GCN (competitive) 9.22
GCN (collaborative) 9.04
GCN (final) 9.89

Table 5: The performance on document-based QG task.

6 Conclusion

We present an algorithm dubbed generative col-
laborative network for jointly training the question
answering (QA) model and the question genera-
tion (QG) model. Different from standard GAN,
the relationship between QA model (discrimina-
tor) and the QG model (generator) in our algo-
rithm is not always competitive. We show that

“collaborative” performs better than “competitive”
in terms of QA accuracy, and our algorithm that
learns when to collaborate obtains further im-
provement on both QA and QG tasks.

This work could be further improved from sev-
eral directions. Our current algorithm focuses on
the joint training of QA and QG models, while the
inferences of these two models are independent.
How to conduct joint inference is an interesting
future work. Besides, the samples are currently
generated from the QG model via beam search.
Improving the diversity of the samples requires d-
ifferent sampling mechanisms. Another potential
direction is to jointly learn the collaboration detec-
tor together with the QA and QG models.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. Proceeding of ICLR
.

Sreeram Balakrishnan, Alon Y Halevy, Boulos Har-
b, Hongrae Lee, Jayant Madhavan, Afshin Ros-
tamizadeh, Warren Shen, Kenneth Wilder, Fei Wu,
and Cong Yu. 2015. Applying webtables in prac-
tice. In CIDR.

Jonathan Berant, Andrew Chou, Roy Frostig, and Per-
cy Liang. 2013. Semantic parsing on freebase
from question-answer pairs. In EMNLP. volume 2,
page 6.

Christopher JC Burges. 2010. From ranknet to lamb-
darank to lambdamart: An overview. Microsoft Re-
search Technical Report MSR-TR-2010-82 11(23-
581):81.

Yllias Chali and Sadid A Hasan. 2015. Towards topic-
to-question generation. Computational Linguistics
.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings
of EMNLP. pages 1724–1734.

Li Dong, Jonathan Mallinson, Siva Reddy, and Mirella
Lapata. 2017. Learning to paraphrase for question
answering. arXiv preprint arXiv:1708.06022 .

Xinya Du, Junru Shao, and Claire Cardie. 2017. Learn-
ing to ask: Neural question generation for reading
comprehension. In Association for Computational
Linguistics (ACL).

Nan Duan, Duyu Tang, Peng Chen, and Ming Zhou.
2017. Question generation for question answering.
Proceeding of EMNLP .

1572



Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In Advances in neural information
processing systems. pages 2672–2680.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. arXiv preprint arX-
iv:1603.06393 .

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati,
Bowen Zhou, and Yoshua Bengio. 2016. Point-
ing the unknown words. arXiv preprint arX-
iv:1603.08148 .

Shizhu He, Cao Liu, Kang Liu, and Jun Zhao.
2017. Generating natural answers by incorporating
copying and retrieving mechanisms in sequence-to-
sequence learning. In Proceedings of ACL. pages
199–208.

Michael Heilman. 2011. Automatic factual question
generation from text. Ph.D. thesis, Carnegie Mellon
University.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. Proceed-
ings of NAACL-HLT 1:48–54.

Jiwei Li, Will Monroe, Alan Ritter, Michel Galley,
Jianfeng Gao, and Dan Jurafsky. 2016. Deep rein-
forcement learning for dialogue generation. arXiv
preprint arXiv:1606.01541 .

Ryan Lowe, Michael Noseworthy, Iulian Vlad Ser-
ban, Nicolas Angelard-Gontier, Yoshua Bengio, and
Joelle Pineau. 2017. Towards an automatic turing
test: Learning to evaluate dialogue responses. In
Proceedings of ACL. pages 1116–1126.

Christopher D Manning, Prabhakar Raghavan, Hinrich
Schütze, et al. 2008. Introduction to information re-
trieval, volume 1. Cambridge university press Cam-
bridge.

Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Mar-
garet Mitchell, Xiaodong He, and Lucy Vander-
wende. 2016. Generating natural questions about an
image. arXiv preprint arXiv:1603.06059 .

Arvind Neelakantan, Quoc V Le, and Ilya Sutskev-
er. 2015. Neural programmer: Inducing latent pro-
grams with gradient descent. arXiv preprint arX-
iv:1511.04834 .

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine
reading comprehension dataset. arXiv preprint arX-
iv:1611.09268 .

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic e-
valuation of machine translation. In Proceedings of
ACL. pages 311–318.

Xipeng Qiu and Xuanjing Huang. 2015. Convolutional
neural tensor network architecture for community-
based question answering. In IJCAI. pages 1305–
1311.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
EMNLP. pages 2383–2392.

Iulian Vlad Serban, Alberto Garcı́a-Durán, Caglar
Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron
Courville, and Yoshua Bengio. 2016. Generating
factoid questions with recurrent neural networks:
The 30m factoid question-answer corpus. In ACL.
pages 588–598.

Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Grégoire Mesnil. 2014. A latent semantic mod-
el with convolutional-pooling structure for informa-
tion retrieval. In Proceedings of the Conference on
Information and Knowledge Management (CIKM).
pages 101–110.

Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
and Rob Fergus. 2015. End-to-end memory net-
works. In Advances in Neural Information Process-
ing Systems (NIPS). pages 2431–2439.

Chen Sun, Abhinav Shrivastava, Saurabh Singh, and
Abhinav Gupta. 2017. Revisiting unreasonable ef-
fectiveness of data in deep learning era. arXiv
preprint arXiv:1707.02968 .

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural network-
s. In Advances in neural information processing sys-
tems. pages 3104–3112.

Duyu Tang, Nan Duan, Tao Qin, and Ming Zhou. 2017.
Question answering and question generation as dual
tasks. arXiv preprint arXiv:1706.02027 .

Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Y-
inghui Xu, Benyou Wang, Peng Zhang, and Dell
Zhang. 2017. Irgan: A minimax game for unifying
generative and discriminative information retrieval
models. arXiv preprint arXiv:1705.10513 .

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning 8(3-4):229–256.

Zhao Yan, Duyu Tang, Nan Duan, Junwei Bao, Yuan-
hua Lv, Ming Zhou, and Zhoujun Li. 2017. Content-
based table retrieval for web queries. arXiv preprint
arXiv:1706.02427 .

Yi Yang, Wen-tau Yih, and Christopher Meek. 2015.
Wikiqa: A challenge dataset for open-domain ques-
tion answering. In EMNLP. Citeseer, pages 2013–
2018.

Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, and
William W Cohen. 2017. Semi-supervised qa with
generative domain-adaptive nets. arXiv preprint
arXiv:1702.02206 .

1573



Jun Yin, Xin Jiang, Zhengdong Lu, Lifeng Shang,
Hang Li, and Xiaoming Li. 2015a. Neural gen-
erative question answering. arXiv preprint arX-
iv:1512.01337 .

Pengcheng Yin, Zhengdong Lu, Hang Li, and Ben
Kao. 2015b. Neural enquirer: Learning to query
tables with natural language. arXiv preprint arX-
iv:1512.00965 .

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017. Seqgan: Sequence generative adversarial nets
with policy gradient. In AAAI. pages 2852–2858.

Wojciech Zaremba and Ilya Sutskever. 2015. Rein-
forcement learning neural turing machines. arXiv
preprint arXiv:1505.00521 419.

Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan,
Hangbo Bao, and Ming Zhou. 2017. Neural ques-
tion generation from text: A preliminary study. arX-
iv preprint arXiv:1704.01792 .

1574


