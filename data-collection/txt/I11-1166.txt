















































Mining the Sentiment Expectation of Nouns Using Bootstrapping Method


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1423–1427,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Mining the Sentiment Expectation of Nouns Using Bootstrapping Method

Miaomiao Wen
Language Technologies Institute

Carnegie Mellon University

mwen@andrew.cmu.edu

Yunfang Wu
Key Laboratory of Computational

Linguistics (Peking University)
Ministry of Education, China

wuyf@pku.edu.cn

Abstract

We propose an unsupervised bootstrapping
method to generate a new type of affect knowl-
edge base: the sentiment expectation of nouns
(e.g., “high salary” is desirable while “high
price” is usually undesirable, because peo-
ple have opposite sentiment expectation to-
wards “salary” and “price”). A bootstrapping
framework is designed to retrieve patterns that
might be used to express complaints from the
Web. The sentiment expectation of a noun
could be automatically predicted with the out-
put patterns. We evaluate the retrieved pat-
terns and show that our method yields good
results. Also, they are applied to improve both
sentence and document level sentiment analy-
sis results.

1 Introduction

In recent years, sentiment analysis has attracted con-
siderable attention in the NLP community due to
its wide applications. The task is mining positive
and negative opinions from text and an in-depth
review of its literature can be found in Pang and
Lee (2008). Previous work on this problem falls into
three groups: opinion mining of documents, senti-
ment classification of sentences and polarity predic-
tion of words. Sentiment analysis both at document
and sentence level rely heavily on word level.

The most frequently explored task at the word
level is to determine the sentiment orientation (SO)
of words or word senses in the lexicon. While adjec-
tives and verbs are often considered, the sentiment
classification of nouns still poses a challenge. This

paper aims at identifying people’s sentiment expec-
tation towards a noun, even though the noun itself
does not carry polarity. We propose three categories
of sentiment expectation (SE) of nouns: positive ex-
pectation nouns (Pn), negative expectation nouns
(Nn) and neutral. For example, “ó]|salary” is a
Pn, as “high salaries” is desirable for most people.
Also, the noun “d|price” describes an object that
is generally neutral, but it is a Nn, as most people
in most cases expect that the product prices become
lower.

There are several significances lying in this study.
First, the SE of noun reflects world knowledge about
an object, which is not readily available in existing
semantic resources. This knowledge is useful in de-
termining the context dependent SO of adjectives or
verbs. For example, “high salary” is desirable while
“high price” is undesirable. Also, “receive money”
will probably impart positive state onto its patient
while “receive hepatitis” will impart negative state
onto its patient. Second, our method requires very
little human supervision.

We introduce an unsupervised bootstrapping ap-
proach. Our system is initialized with a very small
seed set of nouns, and then iterates between (a) re-
trieving a set of complaint patterns (CPs) - lexico-
syntactic patterns such as “<n> is a little a” 1 that
tend to occur only in people’s complaints - from
search engine snippets and (b) using the acquired
patterns to determine the SE of new nouns.

The rest of this paper is organized as follows: The
related work is discussed in Section 2. In Sections 3,

1In this paper, <n> represents a noun and a represents an
adj.

1423



we present our bootstrapping method. In Section 4,
we conduct evaluation experiments at both sentence
and document level sentiment analysis. The paper is
concluded in Section 5.

2 Related Work

Bootstrapping and pattern-based methods have been
shown to be very effective in previous informa-
tion extraction research (Riloff, 1996; Riloff and
Jones, 1999; Ravichandran and Hovy, 2002; The-
len and Riloff, 2002; Riloff et al., 2003; Mooney
and Bunescu, 2005; Wiebe and Mihalcea, 2006;
Kozareva et al., 2008). These previous works derive
patterns that reveal direct relationship between two
words or the property of the word. Though similar
in methodology, we focused on patterns that express
an implicit relationship between the target noun and
the opinion-bearing adjective.

There has been a large body of work on automatic
SO prediction of words (Hatzivassiloglou and McK-
eown, 1997; Turney and Littman, 2003; Kim and
Hovy, 2004; Esuli and Sebastiani, 2006), but un-
fortunately they did not consider the SE of nouns
in their research and regarded most of the nouns
as “neutral”. Recently, some studies try to disam-
biguate the context dependent SO of adjectives (e.g.
distinguish between “the battery life is very long”
and “it takes a long time to focus”) (Ding et al.,
2008). They infer the context dependent SO by in-
ferring with intra-sentence conjunction rule. Our
task is more challenging as we have no global or
domain information. Thus our method could be ap-
plied to isolated phrase or sentence and is domain
independent. Wu and Wen (2010) present the first
algorithm for retrieving SE of nouns automatically
but the results critically depends on access to a high
quality, carefully chosen collection of CPs.

3 Our Approach

Our main insight is to make use of CPs 2, which
co-occur frequently with the target noun and the ad-
jective that is opposite to the noun’s SE. E.g. people

2Wish patterns (WP), which are usually used in people’s
praises and wishes, might also serve our purpose (Goldberg et.
al., 2009). But using the Web as a corpus, we found that the
CPs are much easier to be retrieved than WPs. Thus we will
just focus on CPs in this paper and leave WPs for future work.

Input: Noun Pool = {Pn,Nn}, A = {Pa, Na};
Output: CP Pool = {N CPs}; i = 0;
Bootstrapping
1. Candi CP Pool← patterns extracted from snippets.
For each pattern p,
score(p) = TScore (p.CPFreq, p.WPFreq)

2. CP Pool = the N patterns with the highest score in
Candi CP Pool. If CP Pool remains unchanged for two
succeeding loops or if i > K, stop bootstrapping.
3. Candi Noun Pool = Extract new nouns from snip-
pets.
4. Train SVM with CP Pool. Training set ←
Noun Pool, testing set← Candi Noun Pool
For each noun n, the feature vector Fi = TScore(∑

a∈Na Hits(pi(n, a)),
∑

a∈Pa Hits(pi(n, a))), i =
1, ..., N .
5. Add the nouns with the largest posterior probability
and the smallest posterior probability to Pn and Nn
respectively.
6. i = i+ 1, Go to Step 1.

Table 1: The Whole Bootstrapping Process.

might say “ó]k:$| salary is a little low” but
seldom say “ó]k:p| salary is a little high”.
Utilizing this property, we try to (a) extract patterns
like ”< n > is a little a” from snippets returned
queries like “ó]$|salary low”;3 (b) Use SVM to
determine the SE of new nouns with page counts
based features. E.g. “dk:$| price is a lit-
tle low” obtains 1080 hits while “dk:p| price
is a little high” obtains 19400 hits.

We use a bootstrapping method to automatically
discover CPs and predict sentiment expectation of
nouns (Table 1). In iteration phase 1, with a few seed
nouns, the candidate CPs are retrieved from search
engine snippets. We rank these CPs according to
their ability to express SE. In iteration phase 2, we
infer the sentiment expectation of a noun by mining
the Web with CPs. The SVM is trained to classify
positive expectation nouns and negative expectation
nouns.

Initiation: The bootstrapping begins with a
seed noun set Noun Pool = {Pn, Nn}={{“ó
]|salary”}, {“d|price”}}, and an adjective set
A. A is grouped into two sets: positive-like adjec-
tives (Pa) and negative-like adjectives (Na):

3We use Baidu as our search engine in this paper, http:
//baidu.com.cn

1424



(1) Pa = {|large,õ|many,p|high,þ|thick,�|deep,
­|heavy}
(2) Na = {�|small,�|few,$|low,�|thin,f|shallow,
|light}
Phase 1. Extract CPs from Snippets: For each
snippet returned by the query “n a” ∈ Noun Pool :
A, extracts word n-grams after word segmenta-
tion. We select n-grams which contain exactly
one <n> and one a. Counts the frequency of
a pattern p appears as a CP (where (< n >
, a) ∈ (Pn,Na)or(Nn,Pa)) and the frequency
of p appears as a WP (where (< n >, a) ∈
(Nn,Na)or(Pn, Pa)). Finally, we adopt T-Score
to measure the confidence with which we can assert
whether this pattern is a CP (Step 1 in Table 1). The
top ranking N patterns are selected as CPs experi-
mentally.
Phase 2. Determine the SE of new nouns: We cre-
ate a feature vector F using the harvested CPs for
each noun. The two-class support vector machine
(SVM) is trained to find the optimal combination of
the page counts-based features (Step 4 in Table 1).
We define the SE of a noun as the posterior prob-
ability (converted from SVM output with a sigmoid
function (J. Platt., 2000)) that they belong to the pos-
itive expectation noun class. Then the nouns with
the largest and smallest probability are added to the
Noun Pool.

4 Evaluation

4.1 Direct Evaluation

We examine the harvested CPs directly to determine
whether they are actually CPs or not. Our evalua-
tion metric is the precision of the output CPs at each
bootstrapping iteration. We recruited two Chinese
speakers to label them as “being CP”, “not CP” or
“hard to decide”.

Figure 1 plots the number of correctly retrieved
CPs in the output at each iteration. Clearly, we
see general substantial improvement along with
the bootstrapping process, although the increases
level off in later iterations. As the number of
the output CPs increases, there are more pat-
terns that are labeled as “hard to decide”, but
some of these patterns could still serve our pur-
pose. E.g., “Ï<n><a>|because <n>is <a>”
and “·<n><a>|my <n>is <a>”, as people often

0

5

10

15

20

25

1 6 11 16 21 26

N=10

N=20

N=30

Iteration

N
u
m

be
r 

o
f 

C
P

s

Figure 1: Number of CPs in results returned by the boot-
strapping method at each iteration. N is the number of
output patterns. Items labeled as “hard to decide” are not
included.

post their problems and complaints on Web. These
patterns may serve as possible indicators of the pres-
ence of a noun and the reversed expectation ad-
jective, regardless of whether they are actually re-
stricted in negative contexts.

The top ranked 10 CPs are of high quality. The
manually chosen patterns in Wu and Wen (2010)
have been successfully retrieved. This shows that
our method yields good results. The 10 CPs and the
nouns added in the bootstrapping process are listed
below:

有点 太

是不是太 ’ 实在太

解决 怎么办

嫌 因 ,

过

空间 素质 ，水平 ，效率 ，

内存 ，收入 ，孩子 ，时间

要求 压力 ，成本 ，风险

，问题 ，花费 ，代价 ，房价

，损失

4.2 Sentiment Analysis at Sentence Level
We apply the SE of nouns to predict the SO of senti-
ment ambiguous adjectives, which is SemEval-2010
Task 18 (Wu and Jin, 2010).
Data: We use the benchmark dataset of SemEval-
2010 Task 18. The task consists of 14 sentiment am-
biguous adjectives (SAA) (devided to Pa and Na sets
same as in Section 3), which are all high-frequency
words in Mandarin Chinese. Each of the 2917 sen-
tences in the dataset contains a target noun and a
SAA.
Methods: The SO of SAA can be determined by the
target noun in noun-adjective phrases. If the SAA
has the same polarity as the SE of noun, then the
SAA has positive sentiment; if the SAA has the op-

1425



posite polarity to the SE of noun, the SAA has neg-
ative sentiment.
Results: Compared with the other 16 systems that
participated in Task 18, our system ranks fifth and
is substantially better than baseline (Table 2). Note
that all the top ranked 11 systems are supervised
or incorporate manually built library. Our system
also outperforms Wu and Wen (2010), indicating our
bootstrapping method works better than the manu-
ally selected patterns.

Micro Acc. Macro Acc.
Our Method 77.63 79.52
Wu & Wen 75.83 71.67

Baseline 61.20 62.37

Table 2: The scores on SemEval-2010 Task 18.

4.3 Sentiment Analysis at Document Level

We also investigated the impact of recognizing SE
of nouns and CPs on the sentiment classification of
product reviews. SAAs are frequently used in prod-
uct reviews and could be sentiment disambiguated
by the SE of nouns. Also, CPs usually indicate
the speaker is complaining and unsatisfied with the
product (i.e. negative reviews). For example, “U
�O�;| keyboards are designed too close” and
“d B| It is a little expensive”.
Data: Following the work of Wan (2008) and
Wan (2009), we selected the same dataset4. The
dataset of Wan (2008) contains 886 Chinese prod-
uct reviews. The dataset of Wan (2009) contains an-
other 1000 unlabeled Chinese product reviews. We
manually annotated these product reviews with pos-
itive or negative polarity labels. We use both these
two datasets as our test set, which includes 1886 re-
views. In order to examine the impact of recognizing
SE of nouns, we extracted the files that contain the
following strings, where the nouns are modified by
SAAs in most cases:
(3) noun+adjective (adjective∈SAA)

noun+adverb+adjective
noun+adverb+adverb+adjective.

We obtained 449 files (SAA-set for short), up to 24%
of the overall data.
Methods: The baseline method is the same algo-
rithm with Wan (2008). The semantic orientation

4Available here: http://sites.google.com/
site/wanxiaojun1979/publicationlist-1

value for a review is computed by summing the po-
larity values of all words in the review, making use
of both the word polarity defined in the positive and
negative lexicons and the contextual valence shifters
defined in the negation and intensifier lexicons. We
also use the same parameter setting and the same
sentiment lexicon 5.

Our method: (a) Add the disambiguation of SO of
SAAs to the algorithm. When a word ∈ SAA, com-
pute its SO with our method in Section 4.2, rather
than using its prior polarity specified in the senti-
ment lexicon. (b) Use the CPs as indicators of neg-
ative comments. If any CP appears in a review, then
it is judged as negative SO.
Results: Our method obviously outperforms the
baseline by 12.16% in f-score and 17.02% in accu-
racy (on SAA-set, see Table 3). The improvement in
recall is especially obvious. The results also indicate
using more CPs could bring further improvement.

Base SE SE SE
line N=10 N=20 N=30

Pre. 65.69 81.93 83.44 85.28
Pos. Rec. 76.40 74.16 76.40 75.96

F 70.16 79.07 79.77 80.35
Pre. 87.43 84.35 84.53 83.77

Neg. Rec. 60.96 88.05 89.24 90.24
F 71.83 86.16 86.82 86.89

Total MacroF 70.98 82.46 83.14 83.49
Acc. 66.90 83.46 83.92 84.15

Table 3: The sentiment classification results at document
level. SE denotes our method. N is the number of CPs.

5 Conclusions

This paper presents an unsupervised bootstrapping
method to retrieve the sentiment expectation of
nouns from the Web. We utilize the predicted SE of
nouns in determining the SO of sentiment ambigu-
ous adjectives. For the sentiment analysis at sen-
tence level, our method achieves promising result
that is significantly better than baseline and compa-
rable to the supervised methods. For the sentiment
analysis at document level, our method also achieves
obvious improvement in performance, which vali-
dates the effectiveness of our approach.

5Sentiment Hownet, a manually constructed Chinese
opinion lexicon: http://www.keenage.com/html/c_
index.html

1426



References
Ding, X., Liu, B. and Yu, P. 2008. A holistic lexicon-

based approach to opinion mining. In Proceedings of
WSDM’08.

Esuli, A. and Sebastiani, F. 2006. SentiWordNet: a pub-
licly available lexical resource for opinion mining. In
Proceedings of LREC’06.

Goyal, A., Riloff, E., Daume III, H. 2010. Automati-
cally Producing Plot Unit Representations for Narra-
tive Text. In Proceedings of EMNLP2010.

Goldberg, A., Fillmore, N., Andrze-jewski, D., Xu, Z.,
Gibson, B., and Zhu, X.. 2009. May all your wishes
come true: A study of wishes and how to recognize
them. In Proceedings of NAACL-HLT2009.

Hatzivassiloglou, V. and McKeown, K. 1997. Predicting
the semantic orientation of adjectives. In Proceedings
of ACL1997.

J. Platt. 2000. Probabilistic outputs for support vec-
tor machines and comparison to regularized likelihood
methods. Advances in Large Margin Classifiers.

Kozareva, Z., Riloff, E. and Hovy, E. 2008. Semantic
Class Learning from the Web with Hyponym Pattern
Linkage Graphs. In Proceedings of ACL-HLT2008.

Kim, S.and Hovy, E. 2004. Determining the sentiment
of opinions. In Proceedings of COLING2004.

Pang, B. and Lee, L. 2008. Opinion mining and senti-
ment analysis Foundations and Trends in Information
Retrieval.

Ravichandran, D and Hovy, E. 2002. Learning surface
text patterns for a question answering system. In Pro-
ceedings of ACL2002.

Riloff, E. 1996. Automatically generating extraction pat-
terns from untagged text. In Proceedings of the 13th
National Conference on Artificial Intelligence.

Riloff, E., Wiebe, J., and Wilson, T.. 2003. Learning sub-
jective nouns using extraction pattern bootstrapping.
In Proceedings of CoNLL2003.

Riloff, Ellen and Jones, Rosie. 1999. Learning dictionar-
ies for information extraction by multi-level bootstrap-
ping. In Proceedings of AAAI1999.

Raymond J. Mooney and Razvan Bunescu. 2005. Min-
ing knowledge from text using information extraction.
In ACM SIGKDD Exploration Newsletter, 7(1):3õ10.

Thelen, M. and Riloff, E. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pat-
tern Contexts. In Proceedings of EMNLP2002.

Turney, P. and Littman, M. 2003. Measuring praise and
criticism: inference of semantic orientation from asso-
ciation. In ACM transaction on information systems.

Turney, P. D., and Pantel, P. 2010. From frequency to
meaning: Vector space models of semantics. In Jour-
nal of Artificial Intelligence Research.

Wan, X. 2008. Using Bilingual Knowledge and Ensem-
ble Techniques for Unsupervised Chinese Sentiment
Analysis. In Proceedings of EMNLP2008.

Wan, X. 2009. Co-Training for Cross-Lingual Sentiment
Classification. In Proceedings of ACL-IJCNLP2009.

Wiebe, J. and Mihalcea, R. 2006. Word Sense and Sub-
jectivity. In Proceedings of OLING-ACL2006.

Wu, Y. and Jin, P. 2010. SemEval-2010 task 18: dis-
ambiguating sentiment ambiguous adjectives. In Pro-
ceedings of SemEval 2010.

Wu, Y and Wen, M. 2010. Disambiguating Dynamic
Sentiment Ambiguous Adjectives. In Proceedings of
COLING2010.

1427


