



















































Enhancing the Inside-Outside Recursive Neural Network Reranker for Dependency Parsing


Proceedings of the 14th International Conference on Parsing Technologies, pages 87–91,
Bilbao, Spain; July 22–24, 2015. c©2015 Association for Computational Linguistics

Enhancing the Inside-Outside Recursive Neural Network Reranker
for Dependency Parsing

Phong Le
Institute for Logic, Language and Computation

University of Amsterdam, the Netherlands
p.le@uva.nl

Abstract

We propose solutions to enhance the
Inside-Outside Recursive Neural Network
(IORNN) reranker of Le and Zuidema
(2014). Replacing the original softmax
function with a hierarchical softmax us-
ing a binary tree constructed by combining
output of the Brown clustering algorithm
and frequency-based Huffman codes, we
significantly reduce the reranker’s com-
putational complexity. In addition, en-
riching contexts used in the reranker
by adding subtrees rooted at (ancestors’)
cousin nodes, the accuracy is increased.

1 Introduction

Using neural networks for syntactic parsing has
become popular recently, thanks to promising re-
sults that those neural-net-based parsers achieved.
For constituent parsing, Socher et al. (2013) using
a recursive neural network (RNN) got an F1 score
close to the state-of-the-art on the Penn WSJ cor-
pus. For dependency parsing, the inside-outside
recursive neural net (IORNN) reranker proposed
by Le and Zuidema (2014) is among the top sys-
tems, including the Chen and Manning (2014)’s
extremely fast transition-based parser employing
a traditional feed-forward neural network.

There are many reasons why neural-net-based
systems perform very well. First, Bansal et al.
(2014) showed that using word-embeddings can
lead to significant improvement for dependency
parsing. Interestingly, those neural-net-based sys-
tems can transparently and easily employ word-
embeddings by initializing their networks with
those vectors. Second, by comparing a count-
based model with their neural-net-based model on

perplexity, Le and Zuidema (2014) suggested that
predicting with neural nets is an effective solution
for the problem of data sparsity. Last but not least,
as showed in the work of Socher and colleagues
on RNNs, e.g. Socher et al. (2013), neural net-
works are capable of ‘semantic transfer’, which is
essential for disambiguation.

We focus on how to enhance the IORNN
reranker of Le and Zuidema (2014) by both reduc-
ing its computational complexity and increasing
its accuracy. Although this reranker is among the
top systems in accuracy, its computation is very
costly due to its softmax function used to com-
pute probabilities of generating tokens: all possi-
ble words in the vocabulary are taken into account.
Solutions for this are to approximate the original
softmax function by using a hierarchical softmax
(Morin and Bengio, 2005), noise-contrastive esti-
mation (Mnih and Teh, 2012), or factorization us-
ing classes (Mikolov et al., 2011). A cost of using
those approximations is, however, drop of the sys-
tem performance. To reduce the drop, we use a hi-
erarchical softmax with a binary tree constructed
by combining Brown clusters and Huffman codes.

We show that, thanks to the reranking frame-
work and the IORNN’s ability to overcome the
problem of data sparsity, more complex contexts
can be employed to generate tokens. We intro-
duce a new type of contexts, named full-history.
By employing both the hierarchical softmax and
the new type of context, our new IORNN reranker
has significantly lower complexity but higher ac-
curacy than the original reranker.

2 The IORNN Reranker

We firstly introduce the IORNN reranker (Le and
Zuidema, 2014).

87



2.1 The∞-order Generative Model
The reranker employs the generative model pro-
posed by Eisner (1996). Intuitively, this model
is top-down: starting with ROOT, we generate its
left dependents and its right dependents. We then
generate dependents for each ROOT’s dependent.
The generative process recursively continues until
there is no dependent to generate. Formally, this
model is described by the following formula

P (T (H)) =

L∏
l=1

P
(
HLl |CHL

l

)
P
(
T (HLl )

)
×

R∏
r=1

P
(
HRr |CHRr

)
P
(
T (HRr )

)
(1)

where H is the current head, T (N) is the subtree
rooted at N , and CN is the context to generate N .
HL, HR are respectively H’s left dependents and
right dependents, plus EOC (End-Of-Children),
a special token to inform that there are no more
dependents to generate. Thus, P (T (ROOT )) is
the probability of generating the entire T .

Le and Zuidema’s∞-order generative model is
defined as the Eisner’s model in which the context
C∞D to generate D contains all of D’s generated
siblings, its ancestors and theirs siblings. Because
of very large fragments that contexts are allowed
to hold, traditional count-based methods are im-
practical (even if we use smart smoothing tech-
niques). They thus introduced the IORNN archi-
tecture to estimate the model.

2.2 Estimation with the IORNN

Each tree node y carries three vectors: inner rep-
resentation iy, representing y, outer representation
oy, representing the full context of y, and partial
outer representation ōy, representing the partial
context C∞y which generates the token of y.

Without loss of generality and ignoring direc-
tions for simplicity, we assume that the model is
generating dependent y for node h conditioning on
context C∞y (see Figure 1). Under the approxima-
tion that the inner representation of a phrase equals
the inner representation of its head, and thanks to
the recursive definition of full/partial contexts (C∞y
is a combination of C∞h and y’s previously gener-
ated sisters), the (partial) outer representations of
y are computed as follows.

ōy = f(Whiih + Whooh + bo + r)

Figure 1: The process to (a) generate y, (b) com-
pute outer representation oy, given head h and sib-
ling x. Black, grey, white boxes are respectively
inner, partial outer, and outer representations. (Le
and Zuidema, 2014)

where r = 0 if y is the first dependent of h; oth-
erwise, r = 1|S̄(y)|

∑
v∈S̄(y) Wdr(v)iv, where S̄(y)

is the set of y’s sisters generated before. And,

oy = f(Whiih + Whooh + bo + s)

where s = 0 if y is the only depen-
dent of h (ignoring EOC); otherwise s =

1
|S(y)|

∑
v∈S(y) Wdr(v)iv where S(y) is the set of

y’s sisters. dr(v) is the dependency relation of v
with h. Whi/ho/dr(v) are n× n real matrices, and
bo is an n-d real vector.

The probability P (w|C∞y ) of generating a token
w at node y is given by

softmax(w, ōy) =
eu(w,ōy)∑

w′∈V eu(w
′,ōy)

(2)

where
[
u(w1, ōy), ..., u(w|V |, ōy)

]T = Wōy + b
and V is the set of all possible tokens (i.e. vocab-
ulary). W is a |V | × n real matrix, b is an |V |-d
real vector.

2.3 The Reranker
Le and Zuidema’s (mixture) reranker is

T ∗ = arg max
T∈D(S)

α logP (T (ROOT ))+(1−α)s(S, T ) (3)

where D(S) and s(S, T ) are a k-best list and
scores given by a third-party parser, and α ∈ [0, 1].
3 Reduce Complexity

The complexity of the IORNN reranker above for
computing P (T (ROOT )) is approximately1

O = l × (3× n× n+ n× |V |)
1Look at Figure 1, we can see that each node requires

four matrix-vector multiplications: two for computing chil-
dren’s (partial) outer representation, one for computing sis-
ters’ (partial) outer representations, and one for computing
the softmax.

88



where l is the length of the given sentence, n and
|V | are respectively the dimensions of representa-
tions and the vocabulary size (sums of vectors are
ignored because their computational cost is small
w.r.t l × n × n). In Le and Zuidema’s reranker,
|V | ≈ 14000 � n = 200. It means that the
reranker spends most of its time on computing
softmax(w, ōy) in Equation 2. This is also true
for the complexity in the training phase.

To reduce the reranker’s complexity, we need
to approximate this softmax. Mnih and Teh
(2012) propose using the noise-contrastive esti-
mation method which is to force the system to
discriminate correct words from randomly chosen
candidates (i.e., noise). This approach is very fast
in training thanks to fixing normalization factors to
one, but slow in testing because normalization fac-
tors are explicitly computed. Vaswani et al. (2013)
use the same approach, and also fix normalization
factors to one when testing. This, however, doest
not guarantee to give us properly normalized prob-
abilities. We thus employ the hierarchical sofmax
proposed by Morin and Bengio (2005) which is
fast in both training and testing and outputs prop-
erly normalized probabilities.

Assume that there is a binary tree whose leaf
nodes each correspond to a word in the vocabulary.
Let (uw1 , u

w
2 , ..., u

w
L) be a path from the root to the

leaf node w (i.e. uw1 = root and u
w
L = w). Let

L(u) the left child of node u, and [x] be 1 if x true
and −1 otherwise. We then replace Equation 2 by

P (w|C∞y ) =
L−1∏
i=1

σ
(
[uwi+1 = L(u

w
i )]v

T
uwi
× ōy

)
where σ(z) = 1/(1 + e−z). If the binary tree is
perfectly balanced, the new complexity is approx-
imately l × (3× n× n+ n× log(|V |)), which is
less than 4l × n × n if |V | < 2n (1.6 × 1060 as
n = 200 in the Le and Zuidema’s reranker).

Constructing a binary tree for this hierarchi-
cal softmax turns out to be nontrivial. Morin
and Bengio (2005) relied on WordNet whereas
Mikolov et al. (2013) used only frequency-based
Huffmann codes. In our case, an ideal tree should
reflect both semantic similarities between words
(e.g. leaf nodes for ‘dog’ and ‘cat’ should be close
to each other), and word frequencies (since we
want to minimize the complexity). Therefore we
propose combining output of the Brown hierarchi-
cal clustering algorithm (Brown et al., 1992) and

frequency-based Huffman codes.2 Firstly, we use
the Brown algorithm to find c hierarchical clusters
(c = 500 in our experiments).3 We then, for each
cluster, compute the Huffman code for each word
in that cluster.

4 Enrich Contexts

Although suggesting that predicting with neural
networks is a solution to overcome the problem of
sparsity, Le and Zuidema’s reranker still relies on
two widely-used independence assumptions: (i)
the two Markov chains that generate dependents in
the two directions are independent, given the head,
and (ii) non-overlapping subtrees are generated in-
dependently.4 That is why its partial context (e.g.
the red-dashed shape in Figure 2) used to generate
a node ignores: (i) sisters in the other direction and
(ii) ancestors’ cousins and their descendants.

We, in contrast, eliminate those two assump-
tions by proposing the following top-down left-to-
right generative story. From the head node, we
generate its dependents from left to right. The par-
tial context to generate a dependent is the whole
fragment that is generated so far (e.g. the blue
shape in Figure 2). We then generate subtrees
rooted at those nodes also from left to right. The
full context given to a node to generate the sub-
tree rooted at this node is thus the whole fragment
that is generated so far (e.g. the combination of
the blue shape and the blue-dotted shape in Fig-
ure 2). In this way, the model always uses the
whole up-to-date fragment to generate a depen-
dent or to generate a subtree rooted at a node. To
our knowledge, these contexts, which contain full
derivation histories, are the most complete ones
ever used for graph-based parsing.

Extending the IORNN reranker in this way is
straight-forward. For example, we first generate
a subtree tr(x) rooted at node x in Figure 3. We
then compute the inner representation for tr(x): if
tr(x) contains only x then itr(x) = ix; otherwise

itr(x) = f(W
i
hix+

1
|S(x)|

∑
u∈S(x)

Widr(u)itr(u)+b
i)

2Another reason not to use the Brown clustering algo-
rithm alone is that the algorithm is inefficient with high num-
bers of clusters (|V | ≈ 14000 in this case).

3We run Liang (2005)’s implementation at https://
github.com/percyliang/brown-cluster on the
training data.

4Le and Zuidema rephrased this assumption by the ap-
proximation that the meaning of a phrase equals to the mean-
ing of its head.

89



Figure 2: Context used in Le and Zuidema’s reranker (red-dashed shape) and full-history context (blue-
solid shape) to generate token ‘authorization’.

Figure 3: Compute the full-history outer represen-
tation for y.

where S(x) is the set of x’s dependents, dr(u) is
the dependency relation of u with x, Wih/dr(u) are
n× n real matrices, and bi is an n-d real vector.5

5 Experiments

We use the same setting reported in Le and
Zuidema (2014, Section 5.3). The Penn WSJ Tree-
bank is converted to dependencies using the Ya-
mada and Matsumoto (2003)’s head rules. Sec-
tions 2-21 are for training, section 22 for devel-
opment, and section 23 for testing. The develop-
ment and test sets are tagged by the Stanford POS-
tagger trained on the whole training data, whereas
10-way jackknifing is used to generate tags for the
training set. For the new IORNN reranker, we set
n = 200, initialise it with the 50-dim word em-
beddings from Collobert et al. (2011). We use
the MSTParser (with the 2nd-order feature mode)
(McDonald et al., 2005) to generate k-best lists,
and optimize k and α (Equation 3) on the devel-
opment set.

Table 1 shows the comparison of our new
reranker against other systems. It is a surprise that
our reranker with the proposed hierarchical soft-
max alone can achieve an almost equivalent score
with Le and Zuidema’s reranker. We conjecture
that drawbacks of the hierarchical softmax com-
pared to the original can be lessened by probabil-
ities of generating other elements like POS-tags,

5Note that the overall computational complexity increases
linearly with l × n × n. For instance, for computing
P (T (ROOT )), the increase is approximately 2l × n × n
since each node requires maximally two more matrix-vector
multiplications.

System UAS
MSTParser (baseline) 92.06
Koo and Collins (2010) 93.04
Zhang and McDonald (2012) 93.06
Martins et al. (2013) 93.07
Bohnet and Kuhn (2012) 93.39
Reranking
Hayashi et al. (2013) 93.12
Le and Zuidema (2014) 93.12
Our reranker (h-softmax only, k = 45) 93.10
Our reranker (k = 47) 93.27

Table 1: Comparison with other systems on sec-
tion 23 (excluding punctuation).

dependency relations. Adding enriched contexts,
our reranker achieves the second best accuracy
among those systems.

Because in this experiment no words have paths
longer than 20 � n = 200, our new reranker has
a significantly lower complexity than the one of
Le and Zuidema’s reranker. On a computer with
an Intel Core-i5 3.3GHz CPU and 8GB RAM, it
takes 20 minutes to train this reranker, which is
implemented in C++, and 2 minutes to evaluate it
on the test set.

6 Conclusion

Solutions to enhance the IORNN reranker of Le
and Zuidema (2014) were proposed. We showed
that, by replacing the original softmax function
with a hierarchical softmax, the reranker’s com-
putational complexity significantly decreases. The
cost of this, which is drop on accuracy, is avoided
by enriching contexts with subtrees rooted at (an-
cestors’) cousin nodes. The new reranker, accord-
ing to experimental results on the Penn WSJ Tree-
bank, has even higher accuracy than the old one.

Acknowledgments

We thank Willem Zuidema and four anonymous
reviewers for helpful comments.

90



References
Mohit Bansal, Kevin Gimpel, and Karen Livescu.

2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.

Bernd Bohnet and Jonas Kuhn. 2012. The best of
both worlds: a graph-based completion model for
transition-based parsers. In Proceedings of the 13th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 77–87.
Association for Computational Linguistics.

Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467–479.

Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 740–750.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

Jason M Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Pro-
ceedings of the 16th conference on Computational
linguistics-Volume 1, pages 340–345. Association
for Computational Linguistics.

Katsuhiko Hayashi, Shuhei Kondo, and Yuji Mat-
sumoto. 2013. Efficient stacked dependency pars-
ing by forest reranking. Transactions of the Associ-
ation for Computational Linguistics, 1(1):139–150.

Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1–11. Association for
Computational Linguistics.

Phong Le and Willem Zuidema. 2014. The inside-
outside recursive neural network model for depen-
dency parsing. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics.

Percy Liang. 2005. Semi-supervised learning for nat-
ural language. In MASTERS THESIS, MIT.

Andre Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the turbo: Fast third-order non-
projective turbo parsers. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
617–622, Sofia, Bulgaria, August. Association for
Computational Linguistics.

Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 91–98. Association for Computa-
tional Linguistics.

Tomas Mikolov, Stefan Kombrink, Lukas Burget,
JH Cernocky, and Sanjeev Khudanpur. 2011.
Extensions of recurrent neural network language
model. In Acoustics, Speech and Signal Processing
(ICASSP), 2011 IEEE International Conference on,
pages 5528–5531. IEEE.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. In Proceedings of the 29th In-
ternational Conference on Machine Learning, pages
1751–1758.

Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
AISTATS, volume 5, pages 246–252. Citeseer.

Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
tional vector grammars. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 455–465.

Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with large-scale
neural language models improves translation. In
EMNLP, pages 1387–1392. Citeseer.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of International Conference
on Parsing Technologies (IWPT), pages 195–206.

Hao Zhang and Ryan McDonald. 2012. Generalized
higher-order dependency parsing with cube prun-
ing. In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 320–331. Association for Computational Lin-
guistics.

91


