



















































Personalized Neural Embeddings for Collaborative Filtering with Text


Proceedings of NAACL-HLT 2019, pages 2082–2088
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

2082

Personalized Neural Embeddings for Collaborative Filtering with Text

Guangneng Hu
Department of Computer Science and Engineering

Hong Kong University of Science and Technology, Hong Kong, China
njuhgn@gmail.com

Abstract
Collaborative filtering (CF) is a core tech-
nique for recommender systems. Tradition-
al CF approaches exploit user-item relations
(e.g., clicks, likes, and views) only and hence
they suffer from the data sparsity issue. Item-
s are usually associated with unstructured text
such as article abstracts and product reviews.
We develop a Personalized Neural Embedding
(PNE) framework to exploit both interaction-
s and words seamlessly. We learn such em-
beddings of users, items, and words jointly,
and predict user preferences on items based on
these learned representations. PNE estimates
the probability that a user will like an item by
two terms—behavior factors and semantic fac-
tors. On two real-world datasets, PNE shows
better performance than four state-of-the-art
baselines in terms of three metrics. We also
show that PNE learns meaningful word embed-
dings by visualization.

1 Introduction

Recommender systems are widely used in e-
commerce platforms, such as to help consumers
buy products at Amazon, watch videos on Youtube,
and read articles on Google News. They are useful
to alleviate the information overload and improve
user satisfaction. Given history records of con-
sumers such as the product transactions and movie
watching, collaborative filtering (CF) is among the
most effective approaches based on the simple in-
tuition that if users rated items similarly in the past
then they are likely to rate items similarly in the
future (Sarwar et al., 2001).

History records include both implicit (e.g., pur-
chase and clicks) and explicit (e.g., likes/dislikes
and ratings) feedback which can be represented as
a user-item interaction matrix. Typically, observed
user-item interactions are incomplete with a large
portion remaining not recorded. The goal of recom-
mendation is to predict user preferences on these

missing interactions. This setting requires to com-
plete the partial observed rating matrix. Matrix Fac-
torization (MF) techniques which can learn latent
factors for users and items are the main cornerstone
for CF (Mnih and Salakhutdinov, 2008; Koren,
2008; Koren et al., 2009). It is effective and flexible
to integrate with additional data sources (Hu et al.,
2015). Recently, neural networks like Multilayer
Perceptron (MLP) are used to learn an interaction
function from data with the power of learning high-
ly nonlinear relationships between users and item-
s (Dziugaite and Roy, 2015; Cheng et al., 2016;
He et al., 2017; Hu et al., 2018b). MF and neural
CF exploit user-item behavior interactions only and
hence they both suffer from the data sparsity and
cold-start issues.

Items are usually associated with unstructured
text, like news articles and product reviews. These
additional sources are essential for recommenda-
tion beyond user-item interactions since they con-
tain independent and diverse information. Hence,
they provide an opportunity to alleviate the da-
ta sparsity issue (Ganu et al., 2009; Hu et al.,
2018a). For application domains like recommend-
ing research papers and news articles, the unstruc-
tured text associated with the item is its text con-
tent (Wang and Blei, 2011; Wang et al., 2015;
Bansal et al., 2016). For some domains like recom-
mending products, the unstructured text associated
with the item is its user reviews which justify the
rating behavior (McAuley and Leskovec, 2013; He
and McAuley, 2016). These methods adopt top-
ic modelling techniques and neural networks to
exploit the item content leading to performance
improvement.

A typical way of exploiting text content is to
firstly extract a feature vector for each document
by averaging word embeddings in the document,
and then to learn a text factor corresponding to this
feature vector (Hu and Dai, 2017). These embed-



2083

dings are pre-trained from a large corpus such as
Wikipedia. This approach separates the extraction
of text feature from the learning of user-item inter-
action. These two processes cannot benefit from
each other and errors in the previous step maybe
propagate to the successive steps. Another way is
to learn a topic vector using topic modelling (Wang
and Blei, 2011; McAuley and Leskovec, 2013; Bao
et al., 2014) by aligning behavior factors and topic
factors with a link function such as softmax and
offset.

Recently, neural networks are used to learn a
representation from the text using autoencoder-
s (Wang et al., 2015; Zhang et al., 2016), recurrent
networks (Bansal et al., 2016), and convolutional
networks (Zheng et al., 2017; Catherine and Cohen,
2017). These methods treat different words in the
document as equal importance and do not match
word semantics with the specific user. Instead, we
achieve to learn a personalized word embedding
with the guidance of user-item interactions. That is,
the importance of words is learned to match user
preferences. The attention mechanism can be used
to learn these importance weights. Memory Net-
works (MemNet) have been used in recommenda-
tion to model item content (Hu et al., 2018c; Huang
et al., 2017), capture user neighborhood (Ebesu
et al., 2018), and learn latent relationships (Tay
et al., 2018). We follow this thread to adapt a Mem-
Net to match word semantics with user preferences.

In this paper, we propose a novel neural frame-
work to exploit relational interactions and text con-
tent seamlessly. The proposed Personalized Neural
Embedding (PNE) model fuses semantic represen-
tations learnt from unstructured text with behavior
representations learnt from user-item interactions
jointly for effective estimation of user preferences
on items. PNE estimates the preference probabil-
ity by two kinds of factors. The behavior factor
is to capture the personalized preference of a us-
er to an item learned from behavior interactions.
The semantic factor is to capture the high-level
representation attentively extracted from the un-
structured text by matching word semantics with
user preferences.

To model the behavior factor, we adopt a neural
CF approach, which learns the user-item nonlin-
ear interaction relationships using a neural network
(CFNet). To model the semantic factor, we adopt
a memory network to match word semantics with
the specific user via the attention mechanism inher-

ent in the memory module (MemNet), determining
which words are highly relevant to the user prefer-
ences. PNE integrates relational interactions with
unstructured text by bridging neural CF and memo-
ry networks. PNE can also learn meaningful word
embeddings.

2 Approach

We present PNE to jointly learn representations of
users, items, and words. PNE seamlessly captures
nonlinear user-item interaction relationships and
matches word semantics with user preferences.

Denote the set of users by U and items by I . We
use a rating matrix Y ∈ R|U|×|I| to describe user-
item interactions where each entry yui ∈ {0, 1}
is 1 (observed entries) if user u has an interaction
with item i and 0 (unobserved entries) otherwise.
Usually the interaction matrix is very sparse since
a user u ∈ U only consumed a very small subset
of all items. For the task of item recommendation,
each user is only interested in identifying topK
items (typically K is small e.g. tens or hundreds).
Items are ranked by their predicted scores:

ŷui = f(u, i|Θ), (1)

where f is an interaction function and Θ denotes
model parameters.

2.1 Architecture
PNE consists of a CF network (CFNet) to learn
a nonlinear interaction function and of a memory
network (MemNet) to match word semantics with
user preferences. The information flow in PNE
goes from the input (u, i) to the output ŷui through
the following five modules.

1. Input: (u, i)→ (~eu, ~ei) This module encodes
user-item interaction indices. We adopt the one-hot
encoding. It takes user u and item i, and maps them
into one-hot encodings ~eu ∈ {0, 1}|U| and ~ei ∈
{0, 1}|I| where only the element corresponding to
that user/item index is 1 and all others are 0.

2. Embedding: (~eu, ~ei) → xui This module
firstly embeds one-hot encodings into continuous
representations xu = P T~eu and xi = QT~ei by
embedding matrices P ∈ R|U|×d and Q ∈ R|I|×d
respectively, where d is the latent dimension. It
then concatenates them as xui = [xu,xi] to be the
input of following CFNet and MemNet modules.

3. CFNet: xui → zbehaviorui This module is a CF
approach to exploit user-item interactions. It takes
continuous representations from the embedding



2084

Dataset #user #item #rating #word #density avg. words
Amazon 8,514 28,262 56,050 1,845,387 0.023% 65.3
Cheetah 15,890 84,802 477,685 612,839 0.035% 7.2

Table 1: Datasets and statistics.

module and then transforms to a final behavior
factor representation:

zbehaviorui = ReLU(Wxui + b), (2)

where ReLU(x) = max(0, x) is an activation func-
tion, and W and b are connection weights and
biases.

4. MemNet: xui → zsemanticui This module is to
model the item content with the guidance of user-
item interaction. The item content is modelled by
memories. It takes representations from both the
embedding module and the review text dui associ-
ated with the corresponding user-item (u, i) into a
final semantic factor representation:

zsemanticui =
∑

j:wj∈dui
Softmax(au,ij )cj , (3)

where the external memory slot cj is an embed-
ding vector for word wj by mapping it with an
external memory matrix C. The attentive weight
au,ij encodes the relevance of user u to word wj by
content-based addressing:

au,ij = x
T
uim

u,i
j , (4)

where memory mu,ij is concatenated from internal
memory slots {muj ,mij} which are mapped from
word wj by internal memory matrices Au for user
attention and Ai for item attention.

5. Output: zui → ŷui This module predicts the
recommendation score ŷui for a given user-item
pair based on the representation of both behavior
factor and semantic factor from CFNet and Mem-
Net respectively: zui = [zbehaviorui , z

semantic
ui ]. The

output is the probability that the input pair is a posi-
tive interaction. This is achieved by a logistic layer:

ŷui =
1

1 + exp(−hTzui)
, (5)

where h is model parameter.

2.2 Learning
We adopt the binary cross-entropy loss:

L(Θ) = −
∑

(u,i)∈S

yui log ŷui+(1−yui) log(1−ŷui),

(6)

Baselines Shallow method Deep method
CF BPR MLP

CF w/ text HFT, TBPR LCMR, PNE (ours)

Table 2: Categorization of recommender approaches.

where S = Y + ∪ Y − is the union of observed
interactions and randomly sampled negative exam-
ples. Model parameters Θ = {P ,Q,W , b,A,h}
where we use a single word embedding matrix A
by sharing all memory matrices Au,Ai, and C in
order to reduce model complexity. The objective
function can be optimized by stochastic gradient
descent.

3 Experiment

In this section, we evaluate PNE on two datasets
with five baselines in terms of three metrics.

3.1 Datasets

We evaluate on two real-world datasets. The public
Amazon products (McAuley and Leskovec, 2013)
and a company Cheetah Mobile news (Hu et al.,
2018c; Liu et al., 2018) (see Table 1). We prepro-
cess the data following the strategy in (Wang and
Blei, 2011). The size of word vocabulary is 8,000.

3.2 Evaluation protocol

We adopt leave-one-out evaluation (Hu et al.,
2018b) and use three ranking metrics: hit ratio
(HR), normalized discounted cumulative gain (ND-
CG), and mean reciprocal rank (MRR).

We compare with five baselines (see Table 2).

• BPR (Rendle et al., 2009) is a latent factor
model based on matrix factorization.

• HFT (McAuley and Leskovec, 2013) adopts
topic distributions to learn latent factors from
text reviews.

• TBPR (Hu and Dai, 2017) extends BPR by
integrating text content via word embedding
features. Word embeddings used in TBPR
are pre-trained by GloVe (Pennington et al.,
2014).



2085

TopK Metric
Method

BPR HFT TBPR MLP LCMR PNE

5
HR 8.10 10.77 15.17 21.00* 20.24 23.52

NDCG 5.83 8.15 12.08 14.86* 14.51 16.46
MRR 5.09 7.29 11.04 12.83* 12.63 14.13

10
HR 12.04 13.60 17.77 28.36* 28.36* 31.86

NDCG 7.10 9.07 12.91 16.97* 16.78 19.15
MRR 5.61 7.67 11.38 13.71* 13.56 15.24

20
HR 18.21 27.82 22.68 38.20 39.51* 42.21

NDCG 8.64 12.52 14.14 18.99 19.18* 21.75
MRR 6.02 8.54 11.71 14.26* 14.20 15.95

Table 3: Results (×100) on Amazon dataset. Best base-
line marked with asterisk and best result in boldfaced.

TopK Metric
Method

BPR HFT TBPR MLP LCMR PNE

5
HR 43.80 49.66 49.48 53.80 54.76* 56.48

NDCG 39.71 36.17 42.98* 41.21 41.89 43.45
MRR 36.06 31.75 38.26* 37.02 37.62 39.11

10
HR 49.41 55.80 54.66 61.76 63.11* 64.24

NDCG 41.82 40.93 44.99* 43.81 44.60 45.98
MRR 36.94 33.65 39.13* 38.10 38.74 40.16

20
HR 53.98 65.47 61.23 67.93 69.27* 69.52

NDCG 43.16 43.79 46.82* 45.29 46.19 47.32
MRR 37.30 34.45 39.58* 38.51 39.18 40.53

Table 4: Results (×100) on Cheetah dataset. Best base-
line marked with asterisk and best result in boldfaced.

• MLP (He et al., 2017) is a neural CF approach.
Note that, CFNet of PNE is an MLP with only
one hidden layer.

• LCMR (Hu et al., 2018c) is a deep model for
CF with unstructured text. Note that, MemNet
of PNE is the same with the local MemNet of
LCMR with only one-hop hidden layer.

Our method is implemented by Tensor-
Flow (Abadi et al., 2016). Parameters are randomly
initialized from Gaussian with optimizer Adam (K-
ingma and Ba, 2015). Learning rate is 0.001, batch
size is 128, the ratio of negative sampling is 1.

3.3 Results

Results on two datasets are shown in Table 3 and
Table 4, respectively. We have some observations.
First, PNE outperforms the neural CF method MLP
on two datasets in terms of three ranking metric-
s. On Amazon dataset, PNE obtains a large im-
provement in performance gain with relative 12.3%
HR@10, 7.7% NDCG@10, and 6.2% MRR@5.
On Cheetah Mobile dataset, PNE obtains a large im-
provement in performance gain with relative 5.0%
HR@5, and 4.2% NDCG@5, and 3.9% MRR@5.
Since the CFNet component of PNE is a neural CF
method (with only one hidden layer), results show

Figure 1: Dimension of embedding.

Figure 2: Loss and performance@10.

the benefit of exploiting unstructured text to alle-
viate the data sparsity issue faced by CF methods
(BPR and MLP).

Second, PNE outperforms the traditional hybrid
methods HFT and TBPR on two datasets in terms
of three ranking metrics. On Amazon dataset, PNE
obtains a significantly large improvement in per-
formance gain with relative 55.0% HR@5, 28.9%
NDCG@5, and 20.4% MRR@5. On Cheetah Mo-
bile dataset, PNE still obtains reasonably large
improvements with relative 17.5% HR@10, 1.8%
NDCG@10, and 1.9% MRR@10. Compared with
traditional hybrid methods which integrate the text
content using topic modelling or word embeddings,
results show the benefit of integrating text informa-
tion through memory networks (and exploiting the
interaction data through neural CF).

Last, PNE outperforms neural hybrid method
LCMR by a large margin on Amazon dataset with
relative improvements of 16.2% HR@5, 9.6% ND-
CG, and 7.4% MRR@5. PNE obtains reasonable
improvements on Cheetah Mobile dataset with rel-
ative improvements of 3.1% HR@5, 2.8% NDCG,
and 2.7% MRR. The design of CFNet of PNE is



2086

Figure 3: Visualization of word embeddings.

more reasonable than that of centralized memory
module of LCMR which is equivalent to use a soft-
max activation between two hidden layers. The
results show the effectiveness of fusing strategy in
PNE to exploit unstructured text via MemNet and
the interaction data via CFNet.

3.4 Analysis

We first evaluate the effects of the dimensionality of
the embedding space. The x-axis in Figure 1 is the
dimension of user/item and hence the dimension-
ality of input to CFNet is double since we adopt
concatenation. It clearly indicates that the embed-
ding should not be too small due to the possibility
of information loss and the limits of expressive-
ness. The dimension 75 (and hence d = 150) is
a good tradeoff between recommendation perfor-
mance and computation burden.

We next show optimization curves of perfor-
mance and loss (averaged over all examples) a-
gainst iterations on Cheetah Mobile dataset in Fig-
ure 2. The model learns quickly in the first 20
iterations and improves slowly until 50, while train-
ing losses continue to go down and valid losses
stabilize. The average time per epoch of PNE takes
68.1s and as a reference it is 34.5s for MLP using
one NVIDIA TITAN Xp GPU.

3.5 Visualization

We visualize the learned word embeddings A. We
show that we can learn meaningful semantics for
word embeddings such that words are to cluster
when they have relevant semantics. We give an
example to show the neighbors of the word “drug”

in the 3D space by projecting the high-dimensional
word vectors using TensorFlow 1 as shown in Fig-
ure 3. The top nearest neighbors of drug are: shot,
shoots, gang, murder, killing, rape, stabbed, truck,
school, police, teenage. We can see they are highly
semantic relevant. We may also infer that school
teenagers have close relationships to the drug issue
from the Cheetah News corpus. This should raise a
concern for society and it shows the society impact
of natural language processing (Hovy and Spruit,
2016). Try our trained word embeddings 2.

4 Conclusion

We showed that relational interactions can be ef-
fectively integrated with unstructured text under a
neural embedding model. Our method attentively
focuses relevant words to match user preferences
with user and item attentions (semantic factor) and
captures nonlinear relationships between users and
items (behavior factor). Experiments show better
performance than five baselines on two real-world
datasets in terms of three ranking metrics. We learn
meaningful word embeddings and rethink the soci-
ety impact of language processing technology.

Acknowledgments

The work is supported by HK CERG projects
16211214/16209715/16244616, NSFC 61673202,
and HKPFS PF15-16701.

1https://projector.tensorflow.org/
2https://www.dropbox.

com/sh/ef74fpagf6sd137/
AACXF6EnEY6QBdmJcyIW4RE0a?dl=0

https://projector.tensorflow.org/
https://www.dropbox.com/sh/ef74fpagf6sd137/AACXF6EnEY6QBdmJcyIW4RE0a?dl=0
https://www.dropbox.com/sh/ef74fpagf6sd137/AACXF6EnEY6QBdmJcyIW4RE0a?dl=0
https://www.dropbox.com/sh/ef74fpagf6sd137/AACXF6EnEY6QBdmJcyIW4RE0a?dl=0


2087

References
Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng

Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
et al. 2016. Tensorflow: a system for large-scale
machine learning. In OSDI, volume 16, pages 265–
283.

Trapit Bansal, David Belanger, and Andrew McCallum.
2016. Ask the gru: Multi-task learning for deep text
recommendations. In Proceedings of the 10th ACM
Conference on Recommender Systems, pages 107–
114. ACM.

Yang Bao, Hui Fang, and Jie Zhang. 2014. Topicmf: si-
multaneously exploiting ratings and reviews for rec-
ommendation. In Proceedings of the Twenty-Eighth
AAAI Conference on Artificial Intelligence, pages 2–
8. AAAI Press.

Rose Catherine and William Cohen. 2017. Transnets:
Learning to transform for recommendation. In Pro-
ceedings of the Eleventh ACM Conference on Rec-
ommender Systems, pages 288–296. ACM.

Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal
Shaked, Tushar Chandra, Hrishi Aradhye, Glen An-
derson, Greg Corrado, Wei Chai, Mustafa Ispir, et al.
2016. Wide & deep learning for recommender sys-
tems. In Proceedings of the 1st Workshop on Deep
Learning for Recommender Systems, pages 7–10.
ACM.

Gintare Karolina Dziugaite and Daniel M Roy. 2015.
Neural network matrix factorization. arXiv preprint
arXiv:1511.06443.

Travis Ebesu, Bin Shen, and Yi Fang. 2018. Collabora-
tive memory network for recommendation system-
s. In The 41st International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval (SIGIR), pages 195–204, New York, NY,
USA. ACM.

Gayatree Ganu, Noemie Elhadad, and Amélie Marian.
2009. Beyond the stars: improving rating prediction-
s using review text content. In International Work-
shop on the Web and Databases (WebDB), volume 9,
pages 1–6.

Ruining He and Julian McAuley. 2016. Vbpr: visu-
al bayesian personalized ranking from implicit feed-
back. In Proceedings of the Thirtieth AAAI Con-
ference on Artificial Intelligence, pages 144–150.
AAAI Press.

Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie,
Xia Hu, and Tat-Seng Chua. 2017. Neural collabora-
tive filtering. In Proceedings of the 26th Internation-
al Conference on World Wide Web, pages 173–182.
International World Wide Web Conferences Steering
Committee.

Dirk Hovy and Shannon L Spruit. 2016. The social im-
pact of natural language processing. In Proceedings

of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Paper-
s), volume 2, pages 591–598.

Guang-Neng Hu and Xin-Yu Dai. 2017. Integrating re-
views into personalized ranking for cold start recom-
mendation. In Pacific-Asia Conference on Knowl-
edge Discovery and Data Mining, pages 708–720.
PAKDD.

Guang-Neng Hu, Xin-Yu Dai, Feng-Yu Qiu, Rui Xia,
Tao Li, Shu-Jian Huang, and Jia-Jun Chen. 2018a.
Collaborative filtering with topic and social latent
factors incorporating implicit feedback. ACM Trans-
actions on Knowledge Discovery from Data (TKD-
D), 12(2):23.

Guang-Neng Hu, Xin-Yu Dai, Yunya Song, Shu-Jian
Huang, and Jia-Jun Chen. 2015. A synthetic ap-
proach for recommendation: combining ratings, so-
cial relations, and reviews. In Twenty-Fourth Inter-
national Joint Conference on Artificial Intelligence.

Guangneng Hu, Yu Zhang, and Qiang Yang. 2018b.
Conet: Collaborative cross networks for cross-
domain recommendation. In Proceedings of the
27th ACM International Conference on Informa-
tion and Knowledge Management, pages 667–676.
ACM.

Guangneng Hu, Yu Zhang, and Qiang Yang. 2018c. L-
cmr: Local and centralized memories for collabora-
tive filtering with unstructured text. arXiv preprint
arXiv:1804.06201.

Haoran Huang, Qi Zhang, and Xuanjing Huang. 2017.
Mention recommendation for twitter with end-to-
end memory network. In Proceedings of the 26th
International Joint Conference on Artificial Intelli-
gence, pages 1872–1878. AAAI Press.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. International
Conference on Learning Representations.

Yehuda Koren. 2008. Factorization meets the neighbor-
hood: a multifaceted collaborative filtering model.
In Proceedings of the 14th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 426–434. ACM.

Yehuda Koren, Robert Bell, and Chris Volinsky. 2009.
Matrix factorization techniques for recommender
systems. IEEE Computer, (8):30–37.

Bo Liu, Ying Wei, Yu Zhang, Zhixian Yan, and Qiang
Yang. 2018. Transferable contextual bandit for
cross-domain recommendation. In AAAI Confer-
ence on Artificial Intelligence,.

Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: understanding rating dimen-
sions with review text. In Proceedings of the 7th
ACM conference on Recommender systems, pages
165–172. ACM.



2088

Andriy Mnih and Ruslan R Salakhutdinov. 2008. Prob-
abilistic matrix factorization. In Advances in neural
information processing systems, pages 1257–1264.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of the 2014 conference
on empirical methods in natural language process-
ing (EMNLP), pages 1532–1543.

Steffen Rendle, Christoph Freudenthaler, Zeno Gant-
ner, and Lars Schmidt-Thieme. 2009. Bpr: Bayesian
personalized ranking from implicit feedback. In Pro-
ceedings of the twenty-fifth conference on uncertain-
ty in artificial intelligence, pages 452–461. AUAI
Press.

Badrul Sarwar, George Karypis, Joseph Konstan, and
John Riedl. 2001. Item-based collaborative filtering
recommendation algorithms. In Proceedings of the
10th international conference on World Wide Web,
pages 285–295. ACM.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018. La-
tent relational metric learning via memory-based at-
tention for collaborative ranking. In Proceedings
of the 2018 World Wide Web Conference on World
Wide Web, pages 729–739. International World Wide
Web Conferences Steering Committee.

Chong Wang and David M Blei. 2011. Collaborative
topic modeling for recommending scientific articles.
In Proceedings of the 17th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 448–456. ACM.

Hao Wang, Naiyan Wang, and Dit-Yan Yeung. 2015.
Collaborative deep learning for recommender sys-
tems. In Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, pages 1235–1244. ACM.

Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing
Xie, and Wei-Ying Ma. 2016. Collaborative knowl-
edge base embedding for recommender systems. In
Proceedings of the 22nd ACM SIGKDD internation-
al conference on knowledge discovery and data min-
ing, pages 353–362. ACM.

Lei Zheng, Vahid Noroozi, and Philip S Yu. 2017. Joint
deep modeling of users and items using reviews for
recommendation. In Proceedings of the Tenth ACM
International Conference on Web Search and Data
Mining, pages 425–434. ACM.


