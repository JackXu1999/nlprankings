




































Ranking-Based Autoencoder for Extreme Multi-label Classification


Proceedings of NAACL-HLT 2019, pages 2820–2830
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

2820

Ranking-Based Autoencoder for Extreme Multi-label Classification

Bingyu Wang
Khoury College of CS

Northeastern University
rainicy@ccs.neu.edu

Li Chen
JD.com Inc.

li.chen@jd.com

Wei Sun
Department of ECE

North Carolina State University
wsun12@ncsu.edu

Kechen Qin
Khoury College of CS

Northeastern University
qin.ke@husky.neu.edu

Kefeng Li
JD.com Inc.

likefeng@jd.com

Hui Zhou
JD.com Inc.

hui.zhou@jd.com

Abstract

Extreme Multi-label classification (XML) is
an important yet challenging machine learn-
ing task, that assigns to each instance its most
relevant candidate labels from an extremely
large label collection, where the numbers of la-
bels, features and instances could be thousands
or millions. XML is more and more on de-
mand in the Internet industries, accompanied
with the increasing business scale / scope and
data accumulation. The extremely large label
collections yield challenges such as computa-
tional complexity, inter-label dependency and
noisy labeling. Many methods have been pro-
posed to tackle these challenges, based on dif-
ferent mathematical formulations. In this pa-
per, we propose a deep learning XML method,
with a word-vector-based self-attention, fol-
lowed by a ranking-based AutoEncoder archi-
tecture. The proposed method has three ma-
jor advantages: 1) the autoencoder simulta-
neously considers the inter-label dependencies
and the feature-label dependencies, by project-
ing labels and features onto a common em-
bedding space; 2) the ranking loss not only
improves the training efficiency and accuracy
but also can be extended to handle noisy la-
beled data; 3) the efficient attention mecha-
nism improves feature representation by high-
lighting feature importance. Experimental re-
sults on benchmark datasets show the pro-
posed method is competitive to state-of-the-art
methods.

1 Introduction and Related Work

In multi-label classification (Tsoumakas and
Katakis, 2007; Zhang and Zhou, 2014), one as-
signs multiple labels to each instance. Multi-label
classification has many real-word applications: for
example, a movie may be associated with multi-
ple genres, a web page may contain several top-

ics, and an image can be tagged with a few ob-
jects. In these classification tasks, labels often ex-
hibit complex dependencies: for example, Doc-
umentary and Sci-Fi are usually mutually exclu-
sive movie genres, while Horror and Thriller are
typically highly correlated. Predicting labels inde-
pendently fails to capture these dependencies and
suffers suboptimal performance (Tsoumakas and
Katakis, 2007; Ghamrawi and McCallum, 2005;
Li et al., 2016). Several methods that capture la-
bel dependencies have been proposed, including
Conditional Random Fields (CRF) (Lafferty et al.,
2001; Ghamrawi and McCallum, 2005), Classi-
fier Chains (CC) (Read et al., 2011; Dembczyn-
ski et al., 2010), Conditional Bernoulli Mixtures
(CBM) (Li et al., 2016), and Canonical Correlated
AutoEncoder (C2AE) (Yeh et al., 2017). However,
these methods typically only work well on small-
to-medium scale datasets.

Extreme multi-label classification (XML) is a
multi-label classification task in which the num-
ber of instances, features and labels are very large,
often on the order of thousands to millions (Zubi-
aga, 2012; Bhatia et al., 2015). It has numerous
real-world applications such as merchandise tag-
ging and text categorization. Although the label
vocabulary is large, typically each instance only
matches a few labels. The scale of the classifi-
cation task, the inter-dependent labels, and label
sparsity all pose significant challenges for accu-
rate and efficient classification.

Many methods have been proposed for extreme
multi-label classification. We group them into dif-
ferent categories and describe representative meth-
ods in each category.

Independent Classification: A popular method
is to divide the multi-label classification prob-
lem into multiple binary classification prob-



2821

lems (Tsoumakas and Katakis, 2007; Hariharan
et al., 2012; Babbar and Schölkopf, 2017; Yen
et al., 2016, 2017). A typical implementation is
to treat labels independently and train one-vs-all
classifiers for each of the labels. These inde-
pendent classifiers can be trained in parallel and
thus are computationally efficient in practice. Ig-
noring the inter-label dependency also enables ef-
ficient optimization algorithm, which further re-
duces computational cost. However, ignoring la-
bel dependency inherently limits prediction accu-
racy. A competitive method in this category is
called PD-Sparse (Yen et al., 2016), with a vari-
ant of the Block-Coordinate Frank-Wolfe training
algorithm that exploits data sparsity and achieves
complexity sub-linear in the number of primal
and dual variables. PD-Sparse (Yen et al., 2016)
shows better performance with less training and
prediction time than 1-vs-all Logistic Regression
or SVM on extreme multi-label datasets.

Tree Based Classifiers: Following the success
of tree-based algorithms in binary classification
problems, people also proposed tree-based al-
gorithms for multi-label classification (Agrawal
et al., 2013; Weston et al., 2013; Prabhu and
Varma, 2014), which achieve promising prediction
accuracy. Similar to decision trees, these methods
make classification decisions in each branch split.
Different from decision trees, each split evaluates
all features, instead of one, to make a decision.
Also, each decision is for a subset of labels rather
than one label. Finally, via ensembling and par-
allel implementation, trees can boost their predic-
tion accuracy with practically affordable compu-
tational cost. Among these tree based classifiers,
FastXML (Prabhu and Varma, 2014) further opti-
mizes an nDCG-based ranking loss function and
achieves significantly higher accuracy than other
peer methods.

Embedding: A major difficulty of extreme multi-
label classification is the large number of labels.
When labels are inter-dependent, one can attempt
to find a lower dimensional latent label space from
which one can fully reconstruct the original la-
bel space. Over the past decade, many meth-
ods were proposed to find this latent label space.
In early work, methods were proposed to lin-
early project the original label space into a lower-
dimension space and reconstruct predictions from
that space (Tai and Lin, 2012; Balasubramanian
and Lebanon, 2012). However, there are two as-

sumptions: (1) the label dependency is linear and
(2) the label matrix is low-rank, which do not al-
ways hold, as reflected by the low prediction ac-
curacy of these methods. To overcome the limi-
tation of the linear assumption, different methods
were proposed using non-linear embeddings, in-
cluding kernels, sub-sampling (Yu et al., 2014),
feature-aware (Lin et al., 2014; Yeh et al., 2017)
and pairwise distance preservation (Bhatia et al.,
2015). Among these methods, SLEEC (Bhatia
et al., 2015) stands out for less training time and
higher accuracies. SLEEC introduces a method
for learning a small ensemble of local pairwise
distance preserving embeddings which allows it
to avoid the low-rank and linear-dependency as-
sumption.

Deep Learning: Deep learning has not been well
studied for XML, although it has achieved great
successes in binary and multi-class classification
problems (Lin et al., 2017; Kim, 2014).

FastText (Grave et al., 2017) reconstructs a
document representation by averaging the em-
bedding of the words in the document, followed
by a softmax transformation. It is a simple but
very effective and accurate multi-class text clas-
sifier, as demonstrated in both sentiment analysis
and multi-class classification (Grave et al., 2017).
However, FastText may not be directly applicable
for more complicated problems, like XML.

BoW-CNN (Johnson and Zhang, 2014) learns
powerful embedding of small text regions by ap-
plying CNN to high-dimensional text data. The
embedding of all regions are sent to one or mul-
tiple convolutional layers, a pooling layer and the
output layer at the end.

XML-CNN (Liu et al., 2017) achieves compu-
tational efficiency by training a deep neural net-
work with a hidden bottleneck layer much smaller
than the output layer. However, this method has a
few drawbacks. First, it is trained using the binary
cross entropy loss. This loss tends to be sensitive
to label noise, which is frequently observed in ex-
treme multi-label data. Since the label vocabulary
is large, it is quite common for human annotator
to miss relevant tags. When the classifier’s predic-
tion (which might be correct) disagrees with the
annotation, the cross entropy loss can potentially
assign an unbounded penalty to the classifier dur-
ing training procedure. The second issue is that
because labels are trained independently as sep-
arate binary classification tasks, their prediction



2822

probabilities/scores may not be directly compara-
ble. This is problematic because in many applica-
tions the requirement is to rank all labels accord-
ing to their relevance, as opposed to making an
independent binary decision on each label. The
third defect is that XML-CNN requires raw docu-
ments as input since it adopts the CNN structure
on top of sentences (Kim, 2014); this is problem-
atic when datasets are given in other formats such
as bag-of-words for text.

C2AE (Yeh et al., 2017) uses a ranking loss as
the training objective. But the ranking loss em-
ployed there needs to compare all (positive la-
bel, negative label) pairs, and therefore does not
scale well to extreme data. Furthermore, C2AE
only takes the bag-of-words representation (one-
hot encoding) as the input, which makes it harder
to learn powerful representations from extreme
multi-label dataset.
Our Contribution In this paper, we propose a
new deep learning method to address extreme
multi-label classification. Our contributions are as
follows:

• Motivated by the recent success of attention
techniques, we propose an efficient attention
mechanism that can learn rich representations
from any type of input features, including but
not limited to bag-of-words, raw documents
and images.

• Inspired by C2AE, our proposed model
projects both features and labels onto com-
mon latent spaces wherein correlations be-
tween features and labels are exploited. By
decoding this latent space into the original
label space in the prediction stage, the de-
pendencies between labels are implicitly cap-
tured.

• We propose a margin-based ranking loss that
is simultaneously more effective for extreme
settings and more tolerant towards noisy la-
beling.

2 The Proposed Method (Rank-AE)

In this section, we introduce the data format in
XML, and the proposed networks, including the
marginal-based ranking loss and attentions.

fully
connected 

fully
connected 

fully
connected 

Encoder

Decoder 

Embedding
& Atten

fully
connected 

fully
connected 

x

y

      (xh, yh) y'

xh

yh

x'

Feature Emebdding  

Figure 1: Ranking-based AutoEncoder for XML.

2.1 Data Format
In XML, we are given a set of label candidates
Y = {1, 2, . . . , L}. The dataset D consists of fea-
tures and labels: D = {(xi, yi)}Ni=1, wherein N
is number of data, and each instance x ∈ RV (V
is the feature dimension) matches a label subset
y ⊆ Y , which can be written as a binary vec-
tor y = {0, 1}L, with each bit yl representing the
presence or absence of label l. Given such dataset,
our goal is to build a classifier c: RV → {0, 1}L,
mapping an instance to a subset of labels with ar-
bitrary size.

2.2 Auto-Encoder Network
Inspired by the C2AE (Yeh et al., 2017), we pro-
pose a Ranking-based Auto-Encoder (Rank-AE),
as depicted in Figure 1. Similar to C2AE, Rank-
AE includes three mapping functions to be trained:
a mapping from input features x to feature embed-
dings xh, denoted as F(x), where h is the em-
bedding size; an encoder from output labels y to
label embeddings yh as E(y); a decoder from la-
bel embeddings yh to output labels y′, written as
D(yh). The proposed model is built on two as-
sumptions: first, each instance can be represented
from two different aspects, features x and labels
y, so there exists a common latent space between
x and y; second, labels can be reproduced by an
autoencoder. Based on these two assumptions, we
design the object function as below:

L(D) = min
F ,E,D

Lh(xh, yh) + λLae(y, y′) (1)

wherein loss Lh(xh, yh) aims to find the common
latent space for input x and output y andLae(y, y′)
enforces the output to be reproducible. λ is a
hyper-parameter to balance these two losses. Dur-
ing the training, the model learns a joint network
including F , E and D to minimize the empirical
loss Eq (1).



2823

During inference, a given input x̂ will be first
transformed into a vector in latent space x̂h =
F(x̂), which will then be fed into the label decoder
to compute the predictions ŷ = D(x̂h). It is worth
mentioning that although the label encoder E is ig-
nored during the prediction, it is able to exploit
cross-label dependency during the label embed-
ding stage (Yeh et al., 2017). Recent work (Ku-
rata et al., 2016; Baker and Korhonen, 2017) also
show that using co-occurring labels information to
initialize the neural network can further improve
accuracy in multi-label classification.

2.3 Lh and Lae Loss Functions

Learning Common Embedding (Lh). Minimiz-
ing the common hidden space loss Lh has been
proposed based on different considerations (Zhang
and Schneider, 2011; Yeh et al., 2017; Shen et al.,
2018), ranging from canonical correlation anal-
ysis to alignment of two spaces with a perspec-
tive of cross-view. Since the hidden space is usu-
ally small and requires less computational cost, we
simply employ the mean squared loss for Lh.
Reconstructing Output (Lae). Unlike Lh with
small space,Lae loss usually involves a large num-
ber of labels. Moreover, Lae also directly affects
the classification performance significantly since
different loss functions lead to their own proper-
ties (Hajiabadi et al., 2017). Accordingly, solving
such problems with large scale and desirable prop-
erties presents open challenges in three aspects: 1)
how to improve time efficiency, 2) how to produce
comparable labels scores and 3) how to deal with
noise labels. Unfortunately, most of the related
deep learning methods only target one or two as-
pects. C2AE attempts to minimize the number of
misclassified pairs between relevant and irrelevant
labels, as a result its computational complexity is
quadratic with number of labels in the worst case;
also it fails to scale well on large number of in-
put features or labels due to its inefficient imple-
mentation1. XML-CNN (Liu et al., 2017) achieves
computational efficiency by training a deep neural
network with hidden layers much smaller than the
output layer with binary cross-entropy loss (BCE),
which has linear complexity in number of labels.
Despite this, BCE loss could neither capture la-
bel dependencies nor produce directly compara-
ble label scores, since each label is treated inde-

1https://github.com/dhruvramani/
C2AE-Multilabel-Classification

pendently. Moreover, BCE loss tends to be sensi-
tive to label noise, which is frequently observed in
XML data (Reed et al., 2014; Ghosh et al., 2017).

To void the aforementioned issues, we propose
a marginal-based ranking loss in AutoEncoder:

Lae(y, y′) = LP (y, y′) + LN (y, y′) (2)

LP (y, y′) =
∑

n∈N(y)

max
p∈P (y)

(m+ y′n − y′p)+ (3)

LN (y, y′) =
∑

p∈P (y)

max
n∈N(y)

(m+ y′n − y′p)+ (4)

wherein N(y) is the set of negative label indexes,
P (y) is the complement ofN(y), and marginm ∈
[0, 1] is a hyper-parameter for controlling the mini-
mal distance between positive and negative labels.
The loss consists of two parts: 1) LP targets to
raise the minimal score from positive labels over
all negative labels at least by m; 2) LN aims to
penalize the most violated negative label under all
positive labels by m. The proposed loss has the
following attractive properties: 1) having linear
complexity in number of labelsO(L); 2) capturing
the relative rankings between positive and negative
labels; 3) tolerating the noisy labels with a tunable
hyper-parameter m. To explain the last property,
assume y′n and y

′
p are the predicted probabilities

bounded in [0, 1], then on one extreme case with
noise-free labels and m = 1, all positive labels are
raised to probability 1, while negatives are penal-
ized to 0; on the other extreme case with all ran-
dom labels, e.g. from i.i.d. Bernoulli distribution,
setting m = 0 indicates that the annotated labels
are completely random noises.

2.4 Dual Attention

Extracting rich feature representations in XML
is helpful for predicting more complicated la-
bels structures, but on the other hand, requires
an efficient and feasible method. A recent work
(CBAM) (Woo et al., 2018) proposes a block at-
tention module, with a Channel-Attention and a
Spatial Attention for images tasks only, wherein
Channel-Attention emphasizes information from
channels, e.g. RBG, and Spatial-Attention pays
attention to partial areas in an image. By se-
quentially applying channel and spatial attentions,
CBAM is shown to be effective in images classi-
fication and object detection. We take advantage
of the attentions in CBAM and apply it on textual
data.



2824

Word
Embedding

e1
e2
e3
e4

v1e1

v3e3
v2e2

v4e4

X
x'

w1 w2 w4w3
v1 V2 V4V3
V 

I 

x

F2 F1 

Attention
a1
a2
a3
a4

X

A

m1
m2
m3
m4

P

E

V' M

Figure 2: Dual-Attention in Feature Embedding.

In our proposed attention module, it also con-
sists of spatial-wise and channel-wise attentions.
First, we force spatial-wise attention to attend on
a list of important words in a way that simply
multiply word embeddings by term-frequency or
tf-idf (whichever is provided in the feature ma-
trix). It is worth noting that spatial-wise mod-
ule does not involve any parameters, but it ef-
ficiently captures the importance of words with
numerical statistics, like tf-idf. We demonstrate
spatial-wise attention on the left side of Figure 2,
where the input x = (I, V ) contains bag-of-words
vector I = (w1, w2, . . . , wn)T ∈ Rn and tf-idf
vector V = (v1, v2, . . . , vn)T ∈ Rn. Bag-of-
words I are fed into an embedding layer E =
(e1, e2, . . . , en)

T ∈ Rn×C to get the word em-
beddings, where ej ∈ RC is word embedding
vector of wj . Then we multiply word embed-
dings by V to obtain weighted word embeddings:
V′ = (v1e1, v2e2, . . . , vnen)

T ∈ Rn×C .
The channel attention is designed to empha-

size the significant aspects by assigning different
weights on bits in a word embedding. For exam-
ple, in the word embedding of “apple”, some of the
bits may reflect fruit, while others may indicate the
company name. To achieve this, we adopt the ex-
citation network from the SENet (Hu et al., 2017)
with a slight increase in model complexity. The
excitation network includes two fully connected
layers with a non-linearity activation function in
between, see the top-right part in Figure 2:

AT = σ
(
F2δ(F1V

′T )
)
= (a1,a2, . . . ,an) (5)

wherein A ∈ Rn×C , δ and σ refer to the two
activation functions, ReLU and Sigmoid, F1 ∈
R

C
r
×C and F2 ∈ RC×

C
r are the two fully con-

nected layers, with word embedding size C and

Dataset train test label feature cardinality inst/L
Delicious* 12,920 3,185 983 500 19.03 312
Mediamill* 30,993 12,914 101 120 4.38 1,902

RCV* 623,847 155,962 2,456 47,236 4.79 1,219
IMDb 27,417 6,740 28 115,554 2.4 2,937

EURLex 15,539 3,909 3,993 5,000 5.31 26
Wiki10 14,146 6,616 30,938 101,938 18.64 9

Table 1: Dataset Characteristics: train, test, label and
feature are the numbers of training, testing, labels and
features respectively; cardinality is the average number
of label per instance; inst/L is the average number of
instances per label. * indicates only feature matrix is
provided and no raw document.

reduction ratio r. After obtaining the attention
matrix A, we can apply those attentions to the
weighted word embeddings to get a re-scaled
word embedding matrix M: M = V′ ◦ A =
(m1,m2, . . . ,mn)

T ∈ Rn×C , which is computed
via the element-wise product. The obtained at-
tention matrix A introduces dynamics conditioned
on the input weighted word embeddings and fur-
ther boosts feature discriminability. The last step
is to feed the re-scaled embedding matrix into an
average pooling to obtain the feature embedding
x′ ∈ RC . With the proposed spatial-wise and
channel-wise attentions, Rank-AE can learn rich
feature representations in an efficient way.

3 Experiments & Analysis

3.1 Dataset & Experiment Setup

Dataset. Our experiments are conducted on six
extreme multi-label datasets and their character-
istics are shown in Table 1, among which IMDb
is crawled from online movie database2 and the
rest five datasets are downloaded from the extreme
classification repository3. For datasets from the
repository, we adopt the provided train/test split,
and for IMDb we randomly choose 20% of the
data as test set and the rest of 80% as training set.
For all datasets, we reserve another 20% of train-
ing data as validation for tuning hyper-parameters.
After tuning, all models are trained on the entire
training set.

Among these datasets, three of them are only
provided with BoW feature matrix: Delicious,
Mediamill (dense feature matrix extracted from
image data) and RCV, which are only feasi-
ble for the non-deep learning methods (SLEEC,
FastXML, PDSparse) and Rank-AE. We provide

2https://www.imdb.com/
3http://manikvarma.org/downloads/XC/

XMLRepository.html



2825

both feature matrix and raw documents for IMDb,
EURLex and Wiki10, which are feasible for both
deep learning and non-deep learning methods. For
those data with both formats, we remove the words
from the raw documents that do not have corre-
sponding BoW features so that the vocabulary size
is the same for both deep and non-deep learning
methods.
Evaluation Metrics. To evaluate the perfor-
mances of each model, we adopt the metrics that
have been widely used in XML: Precision at top k
(P@k), and the Normalized Discounted Cummu-
lated Gains at top k (n@k) (Bhatia et al., 2015;
Prabhu and Varma, 2014; Yen et al., 2016; Liu
et al., 2017). P@k is a measure based on the frac-
tion of correct predictions in the top k predicted
scoring labels and n@k is a normalized metric for
Discounted Cumulative Gain:

P@k =
1

k

∑
l∈rankk(ŷ)

yl (6)

DCG@k =
∑

l∈rankk(ŷ)

yl
log(l + 1)

(7)

nDCG@k =
DCG@K∑min(k,|y|)

l=1
1

log(l+1)

(8)

wherein the rankk returns k largest indices of the
prediction ŷ in a descending order, and |y| is the
number of positive labels in ground truth. In the
results, we report the average P@k and n@k on
testing set with k = 1, 3, 5 respectively.
Hyper-parameters. In Rank-AE, we use the fixed
neural network architecture, with two fully con-
nected layers in both Encoder and Decoder, and
one fully connected layer following Embedding
& Atten network in Feature Embedding. We
also fix most of the hyper-parameters, including
hidden dimension h (100 for small number of la-
bels data and 200 for large ones), word embed-
ding size C = 100, and reduction ratio r = 4.
The remaining hyper-parameters, such as balance
λ between Lh and Lae, margin m in Lae, and oth-
ers (decay, learning rate) in the optimization algo-
rithms, are tuned on validation set. In addition,
if the vocabulary for BoW is available, e.g. IMDb
and Wiki10, the Word Embedding component is
initialized by Glove4, a pre-trained word embed-
dings of 100 dimensions; if it is not, e.g. Medi-
amill, Delicious and RCV, a random initialization
is employed.

4https://nlp.stanford.edu/projects/
glove/

For the existing methods with the same
train/test split, we take the scores from the orig-
inal papers for SLEEC, FastXML and PD-Sparse
directly. For the new datasets and splits, the hyper-
parameters are tuned on the validation set for all
methods, as suggested in their papers.

3.2 Comparisons with Related Methods

We evaluate the proposed Rank-AE with other
six state-of-the-art methods, SLEEC, FastXML,
PD-Sparse, FastText, Bow-CNN and XML-CNN,
which are the leading methods among their cat-
egories. Among them, FastText, Bow-CNN and
XML-CNN only take raw documents, which are
not available for Delicious, Mediamill and RCV
datasets. For Rank-AE, we adopt the raw text as
the input for IMDb, and feature matrix for the rest.

The performances evaluated on P@k and n@k
with k = 1, 3, 5 are summarized in Table 2 (a)
and (b) separately. As reported, Rank-AE reaches
the best performances on two datasets (IMDb and
EURLex) out of 6 datasets, while SLEEC achieves
the best performances on Mediamill and Wiki10,
and FastXML performs the best on Delicious and
RCV. In general, SLEEC and FastXML are very
competitive to each other in non-deep learning
methods, but PD-Sparse performs worse. Rank-
AE always performs better than PD-Sparse with
at least 1% increase, up to almost 20% improve-
ment on Delicious data. When compared with
FastXML, Rank-AE outperforms on 4 datasets
with 1% to 10% growth, but underperforms on De-
licious and RCV with 1% decrease. SLEEC, as
the best non-deep learning method in our experi-
ments, performs almost identical to Rank-AE, but
on IMDb data, it performs 7% ∼ 15% less than
non-deep methods, and even worse than Rank-AE.

Comparing Rank-AE with deep learning meth-
ods, we narrow down to three datasets with avail-
able raw documents: IMDb, EURLex and Wiki10.
As shown in Table 2, FastText and Bow-CNN,
not planned for XML but for multi-class, perform
much worse than XML-CNN and Rank-AE as ex-
pected. On the other hand, XML-CNN achieves
close performance to Rank-AE: with similar per-
formance on IMDb dataset, but lower scores on
EURLex and Wiki10 with 2% drop in P@k and
n@k. In spite of this, Rank-AE, trained on fea-
ture matrix for EURLex and Wiki10, surprisingly
performs better than XML-CNN on raw data.

In the comparisons, there is no such method that



2826

Dataset Metrics SLEEC FastXML PD-Sparse FastText Bow-CNN XML-CNN Rank-AE

Delicious
P@1 67.59 69.61 51.82 - - - 69.26
P@3 61.38 64.12 44.18 - - - 62.72
P@5 56.56 59.27 38.95 - - - 57.63

Mediamill
P@1 87.82 84.22 81.86 - - - 86.53
P@3 73.45 67.33 62.52 - - - 70.17
P@5 59.17 53.04 45.11 - - - 55.44

RCV
P@1 90.25 91.23 90.08 - - - 90.9
P@3 72.42 73.51 72.03 - - - 72.82
P@5 51.88 53.31 51.09 - - - 52.05

IMDb
P@1 51.37 66.45 66.84 69.55 66.59 75.55 75.91
P@3 34.46 48.32 46.29 48.76 48.42 52.59 52.66
P@5 27.34 36.28 35.04 36.53 36.56 38.90 38.48

EURLex
P@1 79.26 71.36 76.43 71.51 64.99 76.38 79.52
P@3 64.30 59.90 60.37 60.37 51.68 62.81 65.14
P@5 52.33 50.39 49.72 50.41 42.32 51.41 53.18

Wiki10
P@1 85.88 83.03 81.03 68.86 81.16 84.11 83.6
P@3 72.98 67.47 57.36 54.65 50.67 70.24 72.07
P@5 62.70 57.76 44.10 47.61 36.03 59.87 62.07

Rank Score avg 2.83 3.33 4.56 4.56 5.78 2.56 1.78
Dataset Metrics SLEEC FastXML PD-Sparse FastText Bow-CNN XML-CNN Rank-AE

Delicious
n@1 67.59 69.61 51.82 - - - 69.26
n@3 62.87 65.47 46.00 - - - 64.16
n@5 59.28 61.90 42.02 - - - 60.39

Mediamill
n@1 87.82 84.22 81.86 - - - 86.53
n@3 81.50 75.41 70.21 - - - 78.36
n@5 79.22 72.37 63.71 - - - 75.28

RCV
n@1 90.25 91.23 90.08 - - - 90.9
n@3 88.86 89.63 88.50 - - - 89.29
n@5 89.49 90.33 88.79 - - - 89.75

IMDb
n@1 51.37 66.45 66.84 69.55 66.59 75.55 75.91
n@3 49.75 67.14 64.84 68.47 67.26 74.02 73.5
n@5 54.43 71.72 69.69 72.99 72.07 78.48 77.37

EURLex
n@1 79.26 71.36 76.43 71.51 64.99 76.38 79.52
n@3 68.13 62.87 64.31 63.32 55.03 66.28 68.76
n@5 61.60 58.06 58.78 58.56 49.92 60.32 62.33

Wiki10
n@1 85.88 83.03 81.03 68.86 81.16 84.11 83.6
n@3 76.02 75.35 62.62 56.72 56.14 73.52 74.78
n@5 68.13 63.36 52.03 51.19 45.29 65.50 67.18

Rank Score avg 2.83 3.22 4.33 4.67 5.78 2.56 1.89

Table 2: Comparisons with other methods (P@k and n@k are reported in the top and bottom tables respectively).
’-’ indicates unavailable due to raw documents are not available for these deep learning methods, and number in
bold is the best result in the line.

could perform the best on all datasets. We discover
that each dataset has its own intrinsic properties,
such as diversity of labels, number of features, av-
erage number of relevant labels per instance and
average number of training instances per label, see
Table 1. All those properties will affect training
procedure, for example, how much flexibility a
model should be in order to explain labels well by
the given training data. Because those factors are
always changing from data to data, they also in-
fluence the performances on different models. In
order to have a reasonable comparisons, we report
the average ranking score for each method. To
compute the average ranks, we first rank the meth-
ods based on their performance in each row in Ta-
ble 2, then average them through all rows, and re-

port the final ranking scores in the last row of each
table. The average ranking scores show that Rank-
AE is the best model with ranking scores 1.78 in
P@k and 1.89 in n@k.

3.3 Comparisons with Noise Labels

As mentioned previously, noisy labels in XML are
a quite common issue in the real-world applica-
tions (Yeh et al., 2017; Ghosh et al., 2017), but
our proposed marginal ranking loss naturally mit-
igates this problem. Since IMDb is a real-world
dataset with relatively clean labels, we conduct
the noise experiments on it. In the experiments,
we control the noise labels in two different ways:
1) missing labels: changing each positive label
from yl = 1 to yl = 0 with certain rate, 2) both



2827

40
45
50
55
60
65
70
75

p@1

40
42
44
46
48
50
52

p@3
FastXML PD-Sparse XML-CNN Rank-AE BCE-AE

32

34

36

38

p@5

0 10 20 30 40 50 60
40
45
50
55
60
65
70
75

n@1

0 10 20 30 40 50 60

55

60

65

70

75 n@3

0 10 20 30 40 50 60
60

65

70

75

Missing Rate

Pe
rfo

rm
an

ce
(%

)

n@5

50
55
60
65
70
75

p@1

35

40

45

50

p@3

25.0
27.5
30.0
32.5
35.0
37.5

p@5

0 10 20 30 40 50 60
50
55
60
65
70
75

n@1

0 10 20 30 40 50 60
45
50
55
60
65
70
75

n@3

0 10 20 30 40 50 60
50
55
60
65
70
75
80

Missing and Invalid Rate

Pe
rfo

rm
an

ce
(%

)

n@5

Figure 3: Comparisons on noisy labelling IMDb data.

missing and invalid labels: flipping either from
positive to negative or from negative to positive
with a noise rate. The noise rates are varied from
0% to 60% on 80% of the training set, and the
rest of 20% is noise-free validation set for model
selection. We select five algorithms: FastXML,
PD-Sparse, XML-CNN, Rank-AE and BCE-AE,
wherein BCE-AE is our proposed method but us-
ing binary cross-entropy loss in Lae(y, y′). Com-
paring BCE-AE with Rank-AE can be used to ver-
ify whether the robustness to label noise is due to
the use of marginal ranking loss.

The performances are reported on the same
clean test set, shown in Figure 3. Rank-AE con-
sistently outperforms other four approaches and
has the best robustness tolerating noise labels. Be-
sides, FastXML and PD-Sparse are more toler-
ant to missing noises than XML-CNN, which may
due to XML-CNN has greater capacity and thus
more prone to over-fitting the noise. Furthermore,
when comparing Rank-AE with BCE-AE, both of
which share the same structure but have different
loss functions, the proposed marginal-based rank-
ing loss seems to be robuster than binary cross-
entropy loss.

3.4 More Analysis in Rank-AE

Ablation Study. The effectiveness and robust-
ness of Rank-AE have been demonstrated in the
previous section. However, it is not clear to us

50.0

52.5

55.0

57.5

60.0

62.5

65.0

67.5

70.0
Delicious

45

50

55

60

65

70

75

80

85

90
Mediamill

No attn No loss Rank-AE

40

50

60

70

80

90

RCV

P@1 P@3 P@5
35

40

45

50

55

60

65

70

75

80
IMDb

P@1 P@3 P@5
50

55

60

65

70

75

80

85
EURLex

P@1 P@3 P@5
20

30

40

50

60

70

80

Precision at Top K

Pe
rfo

rm
an

ce
(%

)

Wiki10

Figure 4: Precision at top k comparisons: No attn (/) is
no attention Rank-AE; No loss (+) is using binary cross
entropy lose instead of marginal ranking loss; Rank-AE
(x) is our proposed model.

yet that if the effectiveness benefits from the pro-
posed components, such as attention mechanism
and marginal ranking loss. To further understand
the impacts from these two factors, we conduct
a controlling experiment with three different set-
tings: 1) removing the Attention component A
in Figure 2 from Rank-AE, in which case V′ is
directly passed to the average pooling to obtain
x′, called No attn; or 2) examining the perfor-
mances by replacing the marginal ranking loss
(Lae) with a binary cross entropy loss, named No
loss; or 3) keeping the original Rank-AE without
any change. In Figure 4, P@k is reported on the
six datasets for the ablation experiment, because
n@k is similar to P@k, thus eliminated here. The
comparisons results show that Rank-AE without
any change works better than the other two on all
datasets consistently, especially on Wiki10. First,
channel-attention extracts richer information from
the word embeddings by introducing the channel
weights. Thus, it is more suitable when clas-
sification tasks become more complicated and a
word more likely represents multiple aspects. Sec-
ond, Rank-AE gains some advantage of tolerating
noise labels with marginal ranking loss compar-
ing to BCE loss. We could even further infer that
IMDb and RCV may have relatively less noise la-
bels since the performance does not benefit much
from the marginal ranking loss.
Channel-Attention Visualization. Our channel-
attention is implemented by an excitation network,
which is adopted from SENet (Hu et al., 2017)
and only applied to images before. To demonstrate
its effectiveness and feasibility on textual data, we



2828

Figure 5: Visualization for Attention in Rank-AE. The labels are movie genres split by underline, and the input is
a movie story line.

.

employ the visualization tool (Lin et al., 2017) to
highlight important words based on the attention
output. Specifically, we run our method on IMDb
dataset, wherein each instance is a movie story as-
sociated with relevant genres as labels. Instead of
extracting V′ matrix using the proposed spatial-
wise attention, we obtain a fixed size embeddings
from a bidirectional LSTM on variable length of
sentence, fed to our channel-attention network.
Through the channel-attention network, we can
observe the attention matrix A for each input doc-
ument. By summing up the attention weights of
each word embedding vector, we can visualize the
overall attention for that word with the visualiza-
tion tool5. We randomly select three movies from
IMDb testing set (See Figure 5). By looking at
the highlighted regions, we can see that the pro-
posed channel-attention is able to focus more on
the words that are highly related to the topics.

4 Conclusion

In this paper, we propose a marginal ranking loss,
which not only predicts comparable labels scores
between labels, more suitable for ranking metrics,

5The visualization tool is provided by
https://github.com/kaushalshetty/
Structured-Self-Attention

but also consistently performs better on noisy la-
beling data, with both missing and invalid labels.
In addition, the dual-attention component allows
Rank-AE to learn more powerful feature represen-
tations efficiently. By integrating those compo-
nents, Rank-AE usually achieves the best or the
second best on six benchmark XML datasets com-
paring with other outstanding methods in state-of-
the-art.

Acknowledgements

Most of this work was done when Bingyu Wang
and Wei Sun were interning at JD.Com Inc. We
thank Javed Aslam, Virgil Pavlu and Cheng Li
from Khoury College of Computer Sciences at
Northeastern University for comments that greatly
improved the manuscript. We would also like to
show our gratitude to our anonymous NAACL-
HLT reviwers for the helpful suggestions to make
the paper better.

References
Rahul Agrawal, Archit Gupta, Yashoteja Prabhu, and

Manik Varma. 2013. Multi-label learning with
millions of labels: Recommending advertiser bid
phrases for web pages. In Proceedings of the 22nd



2829

international conference on World Wide Web, pages
13–24. ACM.

Rohit Babbar and Bernhard Schölkopf. 2017. Dismec:
distributed sparse machines for extreme multi-label
classification. In Proceedings of the Tenth ACM
International Conference on Web Search and Data
Mining, pages 721–729. ACM.

Simon Baker and Anna Korhonen. 2017. Initializ-
ing neural networks for hierarchical multi-label text
classification. BioNLP 2017, pages 307–315.

Krishnakumar Balasubramanian and Guy Lebanon.
2012. The landmark selection method for multiple
output prediction. arXiv preprint arXiv:1206.6479.

Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik
Varma, and Prateek Jain. 2015. Sparse local embed-
dings for extreme multi-label classification. In Ad-
vances in Neural Information Processing Systems,
pages 730–738.

Krzysztof Dembczynski, Weiwei Cheng, and Eyke
Hullermeier. 2010. Bayes optimal multilabel classi-
fication via probabilistic classifier chains. In ICML,
volume 10, pages 279–286.

Nadia Ghamrawi and Andrew McCallum. 2005. Col-
lective multi-label classification. In Proceedings of
the 14th ACM international conference on Informa-
tion and knowledge management, pages 195–200.
ACM.

Aritra Ghosh, Himanshu Kumar, and PS Sastry. 2017.
Robust loss functions under label noise for deep neu-
ral networks. In AAAI, pages 1919–1925.

Edouard Grave, Tomas Mikolov, Armand Joulin, and
Piotr Bojanowski. 2017. Bag of tricks for efficient
text classification. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL, pages 3–7.

Hamideh Hajiabadi, Diego Molla-Aliod, and Reza
Monsefi. 2017. On extending neural networks with
loss ensembles for text classification. arXiv preprint
arXiv:1711.05170.

Bharath Hariharan, SVN Vishwanathan, and Manik
Varma. 2012. Efficient max-margin multi-label clas-
sification with applications to zero-shot learning.
Machine learning, 88(1-2):127–155.

Jie Hu, Li Shen, and Gang Sun. 2017. Squeeze-
and-excitation networks. arXiv preprint
arXiv:1709.01507, 7.

Rie Johnson and Tong Zhang. 2014. Effective
use of word order for text categorization with
convolutional neural networks. arXiv preprint
arXiv:1412.1058.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882.

Gakuto Kurata, Bing Xiang, and Bowen Zhou. 2016.
Improved neural network-based multi-label classifi-
cation with better initialization leveraging label co-
occurrence. In Proceedings of the 2016 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 521–526.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML 2001.

Cheng Li, Bingyu Wang, Virgil Pavlu, and Javed
Aslam. 2016. Conditional bernoulli mixtures for
multi-label classification. In International Confer-
ence on Machine Learning, pages 2482–2491.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. arXiv preprint arXiv:1703.03130.

Zijia Lin, Guiguang Ding, Mingqing Hu, and Jianmin
Wang. 2014. Multi-label classification via feature-
aware implicit label space encoding. In Interna-
tional conference on machine learning, pages 325–
333.

Jingzhou Liu, Wei-Cheng Chang, Yuexin Wu, and
Yiming Yang. 2017. Deep learning for extreme
multi-label text classification. In Proceedings of the
40th International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 115–124. ACM.

Yashoteja Prabhu and Manik Varma. 2014. Fastxml: A
fast, accurate and stable tree-classifier for extreme
multi-label learning. In Proceedings of the 20th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 263–272.
ACM.

Jesse Read, Bernhard Pfahringer, Geoff Holmes, and
Eibe Frank. 2011. Classifier chains for multi-label
classification. Machine learning, 85(3):333.

Scott Reed, Honglak Lee, Dragomir Anguelov, Chris-
tian Szegedy, Dumitru Erhan, and Andrew Rabi-
novich. 2014. Training deep neural networks on
noisy labels with bootstrapping. arXiv preprint
arXiv:1412.6596.

Xiaobo Shen, Weiwei Liu, Ivor Wai-Hung Tsang,
Quansen Sun, and Yew-Soon Ong. 2018. Compact
multi-label learning. In AAAI.

Farbound Tai and Hsuan-Tien Lin. 2012. Multilabel
classification with principal label space transforma-
tion. Neural Computation, 24(9):2508–2542.

Grigorios Tsoumakas and Ioannis Katakis. 2007.
Multi-label classification: An overview. Interna-
tional Journal of Data Warehousing and Mining
(IJDWM), 3(3):1–13.



2830

Jason Weston, Ameesh Makadia, and Hector Yee.
2013. Label partitioning for sublinear ranking.
In International Conference on Machine Learning,
pages 181–189.

Sanghyun Woo, Jongchan Park, Joon-Young Lee, and
In So Kweon. 2018. Cbam: Convolutional block
attention module. In Proc. of European Conf. on
Computer Vision (ECCV).

Chih-Kuan Yeh, Wei-Chieh Wu, Wei-Jen Ko, and Yu-
Chiang Frank Wang. 2017. Learning deep latent
space for multi-label classification. In AAAI, pages
2838–2844.

Ian EH Yen, Xiangru Huang, Wei Dai, Pradeep
Ravikumar, Inderjit Dhillon, and Eric Xing. 2017.
Ppdsparse: A parallel primal-dual sparse method for
extreme classification. In Proceedings of the 23rd
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 545–553.
ACM.

Ian En-Hsu Yen, Xiangru Huang, Pradeep Raviku-
mar, Kai Zhong, and Inderjit Dhillon. 2016. Pd-
sparse: A primal and dual sparse approach to ex-
treme multiclass and multilabel classification. In In-
ternational Conference on Machine Learning, pages
3069–3077.

Hsiang-Fu Yu, Prateek Jain, Purushottam Kar, and In-
derjit Dhillon. 2014. Large-scale multi-label learn-
ing with missing labels. In International conference
on machine learning, pages 593–601.

Min-Ling Zhang and Zhi-Hua Zhou. 2014. A re-
view on multi-label learning algorithms. IEEE
transactions on knowledge and data engineering,
26(8):1819–1837.

Yi Zhang and Jeff Schneider. 2011. Multi-label output
codes using canonical correlation analysis. In Pro-
ceedings of the fourteenth international conference
on artificial intelligence and statistics, pages 873–
882.

Arkaitz Zubiaga. 2012. Enhancing navigation on
wikipedia with social tags. arXiv preprint
arXiv:1202.5469.


