



















































Proceedings of the...


D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 261–267,
Trivandrum, India. December 2015. c©2015 NLP Association of India (NLPAI)

Mood Classification of Hindi Songs based on Lyrics   
 
 

Braja Gopal Patra, Dipankar Das, and Sivaji Bandyopadhyay 
Department of Computer Science & Engineering, Jadavpur University, Kolkata, India 

{brajagopal.cse, dipankar.dipnil2005}@gmail.com,  
sivaji_cse_ju@yahoo.com 

 
  

 

Abstract 

Digitization of music has led to easier ac-
cess to different forms music across the 
globe. Increasing work pressure denies 
the necessary time to listen and evaluate 
music for a creation of a personal music 
library. One solution might be develop-
ing a music search engine or recommen-
dation system based on different moods. 
In fact mood label is considered as an 
emerging metadata in the digital music 
libraries and online music repositories. In 
this paper, we proposed mood taxonomy 
for Hindi songs and prepared a mood an-
notated lyrics corpus based on this tax-
onomy. We also annotated lyrics with 
positive and negative polarity. Instead of 
adopting a traditional approach to music 
mood classification based solely on audio 
features, the present study describes a 
mood classification system from lyrics as 
well by combining a wide range of se-
mantic and stylistic features extracted 
from textual lyrics. We also developed a 
supervised system to identify the senti-
ment of the Hindi song lyrics based on 
the above features. We achieved the max-
imum average F-measure of 68.30% and 
38.49% for classifying the polarities and 
moods of the Hindi lyrics, respectively.  

1 Introduction 
Studies on Music Information Retrieval (MIR) 
have shown moods as a desirable access point to 
music repositories and collections (Hu and 
Downie, 2010a). In the recent decade, much 
work on western music mood classification has 
been performed using audio signals and lyrics 
(Hu and Downie, 2010a; Mihalcea and Strap-
parava, 2012). Studies indicating contradictory 

emphasis of lyrics or audio in predicting music 
moods are prevalent in literature (Hu and Down-
ie, 2010b). Indian music considered as one of the 
oldest musical traditions in the world. Indian mu-
sic can be divided into two broad categories, 
“classical” and “popular” (Ujlambkar and Attar, 
2012). Further, classical music tradition of India 
has two main variants; namely Hindustani and 
Carnatic. The prevalence of Hindustani classical 
music is found largely in north and central parts 
of India whereas Carnatic classical music domi-
nates largely in the southern parts of India.  

Indian popular music, also known as Hindi 
Bollywood music or Hindi music, is mostly pre-
sent in Hindi cinemas or Bollywood movies. 
Hindi is one of the official languages of India 
and is the fourth most widely spoken language in 
the World1. Hindi or Bollywood songs make up 
72% of the total music sales in India (Ujlambkar 
and Attar, 2012). Unfortunately, not much com-
putational and analytical work has been done in 
this area. 

Therefore, mood taxonomy especially for 
Hindi songs has been introduced here in order to 
closely investigate the role played by lyrics in 
music mood classification. The lyrics corpus is 
annotated in two steps. In the first step, mood is 
annotated based on the listener’s perspective. In 
the second step, the same corpus is annotated 
with polarity based on the reader’s perspective. 
Further, we developed a mood classification sys-
tem by incorporating different semantic and tex-
tual stylistic features extracted from the lyrics. In 
addition, we also developed a polarity classifica-
tion system based on the above features. 

The paper is organized as follows: Section 2 
reviews related work on music mood classifica-
tion. Section 3 introduces the proposed mood 
classes. The detailed annotation process and the 
dataset used in the study have been described in 
Section 4.  Section 5 describes the features of the 

                                                
1 www.redlinels.com/2014/01/10/most-widely-

261



lyrics used in the experiments, which is followed 
by the results obtained so far and our findings 
and further prospect are discussed in Section 6. 
Finally Section 7 concludes and suggests the fu-
ture work. 

2 Related Work 
Dataset and Taxonomy: Preparation of an an-
notated dataset requires the selection of proper 
mood classes to be used. With respect to Indian 
music, limited work on mood detection by con-
sidering audio features has been reported till to-
day. Koduri and Indurkhya (2010) worked on the 
mood classification of South Indian Classical 
music, i.e. Carnatic music. The main goal of their 
experiment was to verify the raagas that really 
evoke a particular rasa(s) (emotion) specific to 
each user. They considered the taxonomy con-
sisting of ten rasas e.g., Srungaram (Romance, 
Love), Hasyam (Laughter, Comedy) etc. Similar-
ly, Velankar and Sahasrabuddhe (2012) prepared 
data for mood classification of Hindustani classi-
cal music consisting of 13 mood taxonomies 
(Happy, Exciting, Satisfaction, Peaceful, Grace-
ful, Gentle, Huge, Surrender, Love, Request, 
Emotional, Pure, Meditative). Patra et al. (2013a) 
used the standard MIREX taxonomy for their 
experiments whereas Ujlambkar and Attar, 
(2012) experimented based on audio features for 
five mood classes, namely Happy, Sad, Silent, 
Excited and Romantic along with three or more 
subclasses based on two dimensional “Energy 
and Stress” model.  

Mood Classification using Audio Features: 
Automatic music mood classification systems 
based on the audio features where spectral, 
rhythm and intensity are the most popular fea-
tures, have been developed in the last few dec-
ades. The Music Information Retrieval eX-
change2 (MIREX) is an annual evaluation cam-
paign of different Music Information Retrieval 
(MIR) related systems and algorithms. The “Au-
dio Mood Classification (AMC)” task has been 
running each year since 2007 (Hu et al., 2008). 
Among the various audio-based approaches test-
ed at MIREX, spectral features and Support Vec-
tor Machine (SVM) classifiers were widely used 
and found quite effective (Hu and Downie, 
2010a). The “Emotion in Music” task was started 
in the year 2014 at MediaEval Benchmark 
Workshop. In the above task, the arousals and 
valence scores were estimated continuously in 

                                                
2 www.music-ir.org/mirex/wiki/MIREX_HOME 

time for every music piece using several regres-
sion models3. 

Notable work on music mood classification 
using audio features can be found on several mu-
sic categories, such as Hindi music (Ujlambkar 
and Attar, 2012; Patra et al., 2014a, 2014b), Hin-
dustani classical music (Velankar and Sahasrab-
uddhe, 2012) and Carnatic classical music (Ko-
duri and Indurkhya, 2010).  

Mood Classification from Lyric Features: 
Multiple experiments have been carried out on 
western music mood classification based on bag 
of words (BOW), emotion lexicons and other 
stylistic features (Zaanen and Kanters, 2010; Hu 
and Downie, 2010a, 2010b).  

Multi-modal Music Mood Classification:  
Much literature on mood classification on west-
ern music has been published based on both au-
dio and lyrics (Hu and Downie, 2010). The sys-
tem developed by Yang and Lee, (2004) is often 
regarded as one of the earliest studies on combin-
ing lyrics and audio features to develop a multi-
modal music mood classification.  

To the best of our knowledge, Indian music 
mood classification based on lyrics has not been 
attempted yet. Moreover, in context to Indian 
music, multi-modal music mood classification 
also has not been explored either.  

3 Taxonomy  
In context to the western music, the Adjective 
list (Hevner, 1936), Russell’s circumplex model 
(Russell, 1980) and MIREX taxonomy (Hu et al., 
2008) are the most popular mood taxonomies 
used by several worldwide researchers in this 
arena. Though, several mood taxonomies have 
been proposed by different researchers, all such 
psychological models were proposed in laborato-
ry settings and thus were criticized for the lack of 
social context of music listening (Hu, 2010; Lau-
rier et al., 2009).  

Russell (1980) proposed the circumplex model 
of affect (consisting of 28 affect words) based on 
the two dimensions denoted as “pleasant-
unpleasant” and “arousal-sleep” (as shown in 
Figure 1). The most well-known example of such 
taxonomy is the Valence-Arousal (V-A) repre-
sentation which has been used in several previ-
ous experiments (Soleymani et al., 2013). Va-
lence indicates positive versus negative polarity 
whereas arousal indicates the intensity of moods.  

                                                
3 http://www.multimediaeval.org/mediaeval2015/ 
emotion inmusic2015/ 

262



We opted to use Russel’s circumplex model by 
clustering the similar affect words (as shown in 
Figure 1). For example, we have considered the 
affect words calm, relaxed, and satisfied together 
to form one mood class i.e., Calm, denoted as 
Class_Ca. The present mood taxonomy contains 
five mood classes with three subclasses in each.  

One of the main reasons of developing such 
taxonomy was to collect similar songs and clus-
ter them into a single mood class. Preliminary 
observations showed significant invariability in 
case of audio features of the subclasses over its 
corresponding main or coarse class. Basically the 
preliminary observations of annotation are relat-
ed with the psychological factors that influence 
the annotation process while annotating a piece 
of music after listening to the song. For example, 
a happy and a delighted song have high valence, 
whereas an aroused and an excited songs have 
high arousal. The final mood taxonomy used in 
our experiment is shown in Table 1. 

Class_Ex Class_Ha Class_Ca Class_Sa Class_An 
Excited Delighted Calm Sad Angry 
Astonished Happy Relaxed Gloomy Alarmed 
Aroused Pleased Satisfied Depressed Tensed 

Table 1. Proposed Mood Taxonomy 

4 Dataset Annotation Perspective based 
on Listeners and Readers 

Till date, there is no such mood annotated lyrics 
corpus available on the web. In the present work, 
we collected the lyrics data from different web 
archives corresponding to the audio data that was 
developed by Patra et al. (2013). Some more lyr-
ics were added as per the increment of the audio 
data in Patra et al. (2013). The lyrics are basical-

ly written in Romanized English characters. The 
pre-requisite resources like Hindi sentiment lexi-
cons and stopwords are available in utf-8 charac-
ter encoding. Thus, we transliterated the Roman-
ized English lyric to utf-8 characters using the 
transliteration tool available in the EILMT pro-
ject4. As we observed several errors in the trans-
literation process and hence corrected the mis-
takes manually.  

It has to be mentioned that we have only used 
the coarse grain classes for all of our experi-
ments. Also to be noted that, we started annotat-
ing the lyrics at the same time of annotating their 
corresponding audio files by listening to them. 
All of the annotators were undergraduate stu-
dents worked voluntarily and belong to the age 
group of 18-24. Each of the songs was annotated 
by five annotators. We achieved the inter-
annotator agreement of 88% for the lyrics data 
annotated with five coarse grain mood classes (as 
mentioned in bold face in Table 1). While anno-
tating the songs, we observed that the confusions 
occur between the pair of mood classes like 
“Class_An and Class_Ex”, “Class_Ha and 
Class_Ex” and “Class_Sa and Class_Ca” as the-
se classes have similar acoustic features.  

To validate the annotation in a consistent way, 
we tried to assign our proposed coarse mood 
classes (e.g., Class_Ha) to a lyric after reading 
its lexical contents. But, it was too difficult to 
annotate a lyric with such coarse mood classes as 
a lyric of a single song may contain multiple 
emotions within it. On the other hand, the anno-
tators felt different emotions while listening to 
audio and reading its corresponding lyrics, sepa-
rately. For example, Bhaag D.K.Bose Aandhi 
Aayi5 is annotated as Calss_An while listening to 
it, whereas annotated as Class_Sa while reading 
the corresponding lyric. Therefore, in order to 
avoid such problem and confusion, we decided to 
annotate lyrics with one of the coarse grained 
sentiment classes, viz. positive or negative.  

We calculated the inter-annotator agreement 
and obtained 95% agreement on the lyrics data 
annotated with two coarse grained sentiment 
classes. In order to emphasize the annotation 
schemes, we could argue that a song is generally 
considered as positive if it belongs to the happy 
mood class. But, in our case, we observed a dif-
ferent scenario. Initially, the annotators annotated 

                                                
4 http://tdil-dc.in/index.php?option=com_ verti-
cal&parentid=72 
5 http://www.lyricsmint.com/2011/05/bhaag-dk-bose-
aandhi-aayi-delhi-belly.html 

 
Figure 1. Russell’s circumplex model of  

28 affect words. 

Valence à 

A
ro

us
al

 à
 

Class_An 

Class_Sa 
Class_Ca 

Class_Ha 

Class_Ex 

263



a lyric with Class_Ha after listening to audio, 
but, later on, the same annotator annotated the 
same lyric with negative polarity while finished 
reading of its contents. Therefore, a few cases 
where the mood class does not always coincide 
with the conventional moods at lyrics level (e.g., 
Class_Ha and positive, Class_An and negative) 
are identified and we presented a confusion ma-
trix in Table 2. 

 
 Positive Negative No. of Songs 

Class_An 1 49 50 
Class_Ca 83 12 95 
Class_Ex 85 6 91 
Class_Ha 96 4 100 
Class_Sa 7 117 125 

Total Songs 461 
Table 2. Confusion matrix of two annotation 

schemes and statistics of total songs.  

5 Classification Framework 
We adopted a wide range of textual features such 
as sentiment Lexicons, stylistic features and n-
grams in order to develop the music mood classi-
fication framework. We have illustrated all the 
features below.  

5.1 Features based on Sentiment Lexicons: 
We used three Hindi sentiment lexicons to classi-
fy the sentiment words present in the lyrics texts, 
which are Hindi Subjective Lexicon (HSL) 
(Bakliwal et al., 2012), Hindi SentiWordnet 
(HSW) (Joshi et al., 2010) and Hindi Wordnet 
Affect (HWA) (Das et al., 2012). HSL contains 
two lists, one is for adjectives (3909 positive, 
2974 negative and 1225 neutral) and another is 
for adverbs (193 positive, 178 negative and 518 
neutral). HSW consists of 2168 positive, 1391 
negative and 6426 neutral words along with their 
parts-of-speech (POS) and synset id extracted 
from the Hindi WordNet.  HWA contains 2986, 
357, 500, 3185, 801 and 431 words with their 
parts-of-speech from angry, disgust, fear, happy, 
sad and surprise classes, respectively. The statis-
tics of the sentiment words found in the whole 
corpus using three sentiment lexicons are shown 
in Table 3. 

5.2 Text Stylistic Features: The text stylistic 
features such as the number of unique words, 
number of repeated words, number of lines, 
number of unique lines and number of lines end-
ed with same words were considered in our ex-
periments.  

 

Classes HWA Classes HSL HSW 
Angry 241 

Positive 1172 857 Disgust 13 
Fear 13 
Happy 349 

Negative 951 628 Sad 107 
Surprise 38 
Table 3. Sentiment words identified using HWA, 

HSL and HSW 
5.3 Features based on N-grams: Many re-

searches showed that the N-Gram feature works 
well for lyrics mood classification (Zaanen and 
Kanters, 2010) as compared to the stylistic or 
sentiment features. Thus, we considered Term 
Frequency-Inverse Document Frequency (TF-
IDF) scores of up to trigrams as the results get 
worsen after including the higher order N-
Grams. However, we removed the stopwords 
while considering the n-grams and considered 
the N-Grams having document frequency more 
than one. 

We used the correlation based supervised fea-
ture selection technique available in the WEKA6 
toolkit. Finally, we performed our experiments 
with 10 sentiment features, 13 textual stylistic 
features and 1561 N-Gram features. 

6 Results and Discussion 
Support Vector Machines (SVM) is widely used 
for the for the western songs lyrics mood classi-
fication (Hu et al., 2009; Hu and Downie, 
2010a). Even for the mood classification from 
audio data at MIREX showed that the LibSVM 
performed better than the SMO algorithm, K-
Nearest Neighbors (KNN) implemented in the 
WEKA machine learning software (Hu et al., 
2008).  

To develop the automatic system for mood 
classification from lyrics, we have used several 
machine learning algorithms, but the LibSVM 
implemented in the WEKA tool performs better 
than the other classifiers available for the classi-
fication purpose in our case also. Initially, we 
tried LibSVM with the polynomial kernel, but 
the radial basic function kernel gave better re-
sults. In order to get reliable accuracy, we have 
performed 10-fold cross validation for both the 
systems.  

We developed two systems for the data anno-
tated with two different annotation schemes. In 
the first system, we tried to classify the lyrics 
into five coarse grained moods classes. In the 
                                                
6 http://www.cs.waikato.ac.nz/ml/weka/ 

264



second system, we classified the polarities (posi-
tive or negative) of the lyrics that were assigned 
to a song only after reading its corresponding 
lyrics. We have shown the system F-measure in 
Table 4. 

In Table 4, we observed that the F-measure of 
the second system is high compared to the first 
system. In case of English, the maximum accura-
cy achieved in Hu and Downie (2010) is 61.72 
over the dataset of 5,296 unique lyrics compris-
ing of 18 mood categories. But, in case of Hindi, 
we achieved F-score of 38.49 only on a dataset 
of 461 lyrics and with five mood classes. The 
observations yield the facts that the lyrics pat-
terns for English and Hindi are completely dif-
ferent. We have observed various dissimilarities 
(w.r.t. singer and instruments) of the Hindi Songs 
over the English music. There are multiple 
moods in a Hindi lyric and the mood changes 
while annotating a song at the time of listening to 
the audio and reading its corresponding lyric. 

To the best of our knowledge, there is no ex-
isting system available for lyrics based mood 
classification in Hindi. As the lyrics data is de-
veloped on the audio dataset in Patra et al., 
(2013a), thus, we compared our lyrics based 
mood classification system with the audio based 
mod classification system developed in the Patra 
et al., (2013a; 2013b). Our lyrics based system 
performed poorly as compared to the audio based 
systems (accuracies of 51.56% and 48%), alt-
hough lyrics dataset contain more instances than 
the audio based system. They divided a song into 
multiple audio clips of 60 seconds, whereas we 
considered the total lyrics of a song for our ex-
periment. This may be one of the reasons for the 
poor performance of the lyrics based mood clas-
sification system as the mood varies over a full 
length song. But in the present task, we per-
formed classification task on a whole lyric. It is 
also observed that, in context of Hindi songs, the 
mood aroused while listening to the audio is dif-

ferent from the mood aroused at the time of read-
ing a lyric. The second system achieves the best 
F-measure of 68.30. We can observe that the po-
larity all over the music does not change, i.e. if a 
lyric is positive, then the positivity is observed 
through the lyric. We also observed that the N-
Gram features yield F-measure of 35.05% and 
64.2% alone for the mood and polarity classifica-
tion systems respectively. The main reason may 
be that the Hindi is free word order language. 
The Hindi lyrics are also more free word order 
than the Hindi language itself as it matches the 
end of each line. 

7 Conclusion and Future Work 
In this paper, we proposed mood and polarity 
classification systems based on the lyrics of the 
songs. We achieved the best F-measure of 38.49 
and 68.3 in case of the mood and polarity classi-
fication of Hindi songs, respectively. We also 
observed that the listener’s perspective and read-
er’s perspective of emotion are different in case 
of audio and its corresponding lyrics. The mood 
is transparent while adopting the audio only, 
where the polarity is transparent in case of lyrics. 

In future, we plan to perform the same exper-
iment on a wider set of textual features. Later on, 
we plan to develop a hybrid mood classification 
system based audio and lyrics features. We also 
plan to improve accuracy of the lyrics mood 
classification system using multi-level classifica-
tion. 

Acknowledgments 

The first author is supported by Visvesvaraya 
Ph.D. Fellowship funded by Department of Elec-
tronics and Information Technology (DeitY), 
Government of India. The authors are also thank-
ful to the anonymous reviewers for their helpful 
comments. 

     
Systems Features Precision Recall F-Measure 
System 1: 
Mood  
Classification 

Sentiment Lexicon (SL) 29.82 29.8 29.81 
SL+Text Stylistic (TS) 33.60 33.56 33.58 
N-Gram (NG) 34.1 36.0 35.05 
SL+TS+ NG 40.58 36.4 38.49 

System 2: 
Polarity  
Classification 

SL 62.30 62.26 65.28 
SL+TS 65.54 65.54 65.54 
NG 65.4 63.0 64.2 
SL+TS+NG 70.30 66.30 68.30 

Table 4. System performance 
  265



References 
Aditya Joshi, A. R. Balamurali and Pushpak 

Bhattacharyya. 2010. A fall-back strategy for sen-
timent analysis in Hindi: a case study. In: Proc.  of 
the 8ht International Conference on Natural Lan-
guage Processing (ICON -2010). 

Akshat Bakliwal, Piyush Arora and Vasudeva Varma. 
2012. Hindi subjective lexicon: A lexical resource 
for Hindi polarity classification. In: Proc. of the 8th 
International Conference on Language Resources 
and Evaluation (LREC).  

Aniruddha M. Ujlambkar and Vahida Z. Attar. 2012. 
Mood classification of Indian popular music. 
In: Proc. of the CUBE International Information 
Technology Conference, pp. 278-283. ACM. 

Braja G. Patra, Dipankar Das and Sivaji Bandyopadh-
yay. 2013a. Automatic music mood classification 
of Hindi songs. In: Proc. of 3rd Workshop on Sen-
timent Analysis where AI meets Psychology 
(IJCNLP 2013), Nagoya, Japan, pp. 24-28. 

Braja G. Patra, Dipankar Das and Sivaji Bandyopadh-
yay. 2013b. Unsupervised approach to Hindi music 
mood classification. In: Mining Intelligence and 
Knowledge Exploration, pp. 62-69. Springer Inter-
national Publishing. 

Braja G. Patra, Promita Maitra, Dipankar Das and 
Sivaji Bandyopadhyay. 2015. MediaEval 2015: 
Feed-Forward Neural Network based Music Emo-
tion Recognition. In MediaEval 2015 Workshop, 
September 14-15, 2015, Wurzen, Germany. 

Braja G. Patra, Dipankar Das and Sivaji Bandyopadh-
yay. 2015. Music Emotion Recognition. In Interna-
tional Symposium Frontiers of Research Speech 
and Music (FRSM - 2015). 

Cyril Laurier, Mohamed Sordo, Joan Serra and Per-
fecto Herrera. 2009. Music mood representations 
from social tags. In: Proc. of the ISMIR, pp. 381-
386.  

Dan Yang and Won-Sook Lee. 2004. Disambiguating 
music emotion using software agents. In: proc. of 
the ISMIR, pp. 218-223. 

Dipankar Das, Soujanya Poria and Sivaji Bandyo-
padhyay. 2012. A classifier based approach to 
emotion lexicon construction. In: proc. of the 
17th International conference on Applications of 
Natural Language Processing to Information Sys-
tems (NLDB - 2012), G. Bouma et al. 
(Eds.), Springer, LNCS vol. 7337, pp. 320–326. 

Gopala K. Koduri and Bipin Indurkhya. 2010. A be-
havioral study of emotions in south Indian classical 
music and its implications in music recommenda-
tion systems. In: Proc. of the ACM workshop on 
Social, adaptive and personalized multimedia in-
teraction and access, pp. 55-60. ACM. 

James A. Russell. 1980. A Circumplx Model of Af-
fect. Journal of Personality and Social Psychology, 
39(6):1161-1178. 

Kate Hevner. 1936. Experimental studies of the ele-
ments of expression in music. The American Jour-
nal of Psychology: 246-268. 

Makarand R. Velankar and Hari V. Sahasrabuddhe. 
2012. A pilot study of Hindustani music senti-
ments. In: Proc. of 2nd Workshop on Sentiment 
Analysis where AI meets Psychology (COLING-
2012). IIT Bombay, Mumbai, India, pages 91-98. 

Menno V. Zaanen and Pieter Kanters. 2010. Automat-
ic mood classification using TF*IDF based on lyr-
ics. In: proc. of the ISMIR, pp. 75-80. 

Mohammad Soleymani, Micheal N. Caro, Erik M. 
Schmidt, Cheng-Ya Sha and Yi-Hsuan Yang. 
2013. 1000 songs for emotional analysis of music. 
In: Proc. of the 2nd ACM international workshop 
on Crowdsourcing for multimedia, pp. 1-6. ACM. 

Philip J. Stone, Dexter C. Dunphy and Marshall S. 
Smith. 1966. The General Inquirer: A Computer 
Approach to Content Analysis. MIT Press, Cam-
bridge, US. 

Rada Mihalcea and Carlo Strapparava. 2012. Lyrics, 
music, and emotions. In: Proc. of the 2012 Joint 
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural 
Language Learning, pp. 590-599. Association for 
Computational Linguistics. 

Xiao Hu, J. Stephen Downie, Cyril Laurier, Mert Bay 
and Andreas F. Ehmann. 2008. The 2007 MIREX 
Audio Mood Classification Task: Lessons Learned. 
In proceedings of the 9th International Society for 
Music Information Retrieval Conference (ISMIR 
2008), pages 462-467.  

Xiao Hu, Stephen J. Downie and Andreas F. Eh- 
mann. 2009. Lyric text mining in music mood clas-
sification. In proceedings of 10th International So-
ciety for Music Information Retrieval Conference 
(ISMIR 2009), pages 411-416.  

Xiao Hu and J. Stephen Downie. 2010a. Improving 
mood classification in music digital libraries by 
combining lyrics and audio. In: Proc. of the 10th 
annual joint conference on Digital libraries, pp. 
159-168. ACM. 

Xiao Hu and J. Stephen Downie. 2010b. When lyrics 
outperform audio for music mood classification: A 
feature analysis. In: proc. of the ISMIR, pp. 619-
624.  

Xiao Hu. 2010. Music and mood: Where theory and 
reality meet. In: proc. of the iConference. 

Xiao Hu, J. Stephen Downie, Cyril Laurier, Mert Bay 
and Andreas F. Ehmann. 2008. The 2007 MIREX 

266



audio mood classification task: lessons learned. In: 
proc. of the ISMIR, pp. 462-467. 

Yi-Hsuan Yang, Yu-Ching Lin, Ya-Fan Su and 
Homer H. Chen. 2008. A regression approach to 
music emotion recognition. IEEE Transactions 
on Audio, Speech, and Language Processing, 
16(2): 448-457. 

Youngmoo E. Kim, Erik M. Schmidt, Raymond Mi-
gneco, Brandon G. Morton, Patrick Richardson, 
Jeffrey Scott, Jacquelin A. Speck and Douglas 
Turnbull. 2010. Music emotion recognition: A state 
of the art review. In: proc. of the ISMIR, pp. 255-
266.

 
 
 

  

267


