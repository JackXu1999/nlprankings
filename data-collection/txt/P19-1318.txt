



















































Relational Word Embeddings


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3286–3296
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3286

Relational Word Embeddings

Jose Camacho-Collados Luis Espinosa-Anke Steven Schockaert
School of Computer Science and Informatics

Cardiff University, United Kingdom
{camachocolladosj,espinosa-ankel,schockaerts1}@cardiff.ac.uk

Abstract

While word embeddings have been shown to
implicitly encode various forms of attribu-
tional knowledge, the extent to which they
capture relational information is far more lim-
ited. In previous work, this limitation has been
addressed by incorporating relational knowl-
edge from external knowledge bases when
learning the word embedding. Such strategies
may not be optimal, however, as they are lim-
ited by the coverage of available resources and
conflate similarity with other forms of related-
ness. As an alternative, in this paper we pro-
pose to encode relational knowledge in a sep-
arate word embedding, which is aimed to be
complementary to a given standard word em-
bedding. This relational word embedding is
still learned from co-occurrence statistics, and
can thus be used even when no external knowl-
edge base is available. Our analysis shows that
relational word vectors do indeed capture in-
formation that is complementary to what is en-
coded in standard word embeddings.

1 Introduction

Word embeddings are paramount to the success of
current natural language processing (NLP) meth-
ods. Apart from the fact that they provide a con-
venient mechanism for encoding textual informa-
tion in neural network models, their importance
mainly stems from the remarkable amount of lin-
guistic and semantic information that they capture.
For instance, the vector representation of the word
Paris implicitly encodes that this word is a noun,
and more specifically a capital city, and that it
describes a location in France. This information
arises because word embeddings are learned from
co-occurrence counts, and properties such as be-
ing a capital city are reflected in such statistics.
However, the extent to which relational knowledge
(e.g. Trump was the successor of Obama) can be
learned in this way is limited.

Previous work has addressed this by incorporat-
ing external knowledge graphs (Xu et al., 2014;
Celikyilmaz et al., 2015) or relations extracted
from text (Chen et al., 2016). However, the suc-
cess of such approaches depends on the amount
of available relational knowledge. Moreover, they
only consider well-defined discrete relation types
(e.g. is the capital of, or is a part of ), whereas
the appeal of vector space representations largely
comes from their ability to capture subtle aspects
of meaning that go beyond what can be expressed
symbolically. For instance, the relationship be-
tween popcorn and cinema is intuitively clear, but
it is more subtle than the assertion that “popcorn
is located at cinema”, which is how ConceptNet
(Speer et al., 2017), for example, encodes this re-
lationship1.

In fact, regardless of how a word embedding is
learned, if its primary aim is to capture similarity,
there are inherent limitations on the kinds of re-
lations they can capture. For instance, such word
embeddings can only encode similarity preserving
relations (i.e. similar entities have to be related to
similar entities) and it is often difficult to encode
that w is in a particular relationship while prevent-
ing the inference that words with similar vectors
to w are also in this relationship; e.g. Bouraoui
et al. (2018) found that both (Berlin,Germany) and
(Moscow,Germany) were predicted to be instances
of the capital-of relation due to the similarity of
the word vectors for Berlin and Moscow. Fur-
thermore, while the ability to capture word analo-
gies (e.g. king-man+woman≈queen) emerged as
a successful illustration of how word embeddings
can encode some types of relational information
(Mikolov et al., 2013b), the generalization of this
interesting property has proven to be less success-
ful than initially anticipated (Levy et al., 2014;

1http://conceptnet.io/c/en/popcorn



3287

Linzen, 2016; Rogers et al., 2017).
This suggests that relational information has to

be encoded separately from standard similarity-
centric word embeddings. One appealing strategy
is to represent relational information by learning,
for each pair of related words, a vector that en-
codes how the words are related. This strategy was
first adopted by Turney (2005), and has recently
been revisited by a number of authors (Washio and
Kato, 2018a; Jameel et al., 2018; Espinosa Anke
and Schockaert, 2018; Washio and Kato, 2018b;
Joshi et al., 2019). However, in many applications,
word vectors are easier to deal with than vector
representations of word pairs.

The research question we consider in this pa-
per is whether it is possible to learn word vectors
that capture relational information. Our aim is for
such relational word vectors to be complementary
to standard word vectors. To make relational in-
formation available to NLP models, it then suffices
to use a standard architecture and replace normal
word vectors by concatenations of standard and
relational word vectors. In particular, we show
that such relational word vectors can be learned
directly from a given set of relation vectors.

2 Related Work

Relation Vectors. A number of approaches have
been proposed that are aimed at learning relation
vectors for a given set of word pairs (a,b), based
on sentences in which these word pairs co-occur.
For instance, Turney (2005) introduced a method
called Latent Relational Analysis (LRA), which
relies on first identifying a set of sufficiently fre-
quent lexical patterns and then constructs a ma-
trix which encodes for each considered word pair
(a,b) how frequently each pattern P appears in
between a and b in sentences that contain both
words. Relation vectors are then obtained us-
ing singular value decomposition. More recently,
Jameel et al. (2018) proposed an approach inspired
by the GloVe word embedding model (Pennington
et al., 2014) to learn relation vectors based on co-
occurrence statistics between the target word pair
(a, b) and other words. Along similar lines, Es-
pinosa Anke and Schockaert (2018) learn relation
vectors based on the distribution of words occur-
ring in sentences that contain a and b by averag-
ing the word vectors of these co-occurring words.
Then, a conditional autoencoder is used to obtain
lower-dimensional relation vectors.

Taking a slightly different approach, Washio
and Kato (2018a) train a neural network to pre-
dict dependency paths from a given word pair.
Their approach uses standard word vectors as in-
put, hence relational information is encoded im-
plicitly in the weights of the neural network, rather
than as relation vectors (although the output of this
neural network, for a given word pair, can still
be seen as a relation vector). An advantage of
this approach, compared to methods that explic-
itly construct relation vectors, is that evidence ob-
tained for one word is essentially shared with sim-
ilar words (i.e. words whose standard word vec-
tor is similar). Among others, this means that
their approach can in principle model relational
knowledge for word pairs that never co-occur in
the same sentence. A related approach, presented
in (Washio and Kato, 2018b), uses lexical patterns,
as in the LRA method, and trains a neural network
to predict vector encodings of these patterns from
two given word vectors. In this case, the word vec-
tors are updated together with the neural network
and an LSTM to encode the patterns. Finally, sim-
ilar approach is taken by the Pair2Vec method pro-
posed in (Joshi et al., 2019), where the focus is
on learning relation vectors that can be used for
cross-sentence attention mechanisms in tasks such
as question answering and textual entailment.

Despite the fact that such methods learn word
vectors from which relation vectors can be pre-
dicted, it is unclear to what extent these word vec-
tors themselves capture relational knowledge. In
particular, the aforementioned methods have thus
far only been evaluated in settings that rely on
the predicted relation vectors. Since these pre-
dictions are made by relatively sophisticated neu-
ral network architectures, it is possible that most
of the relational knowledge is still captured in the
weights of these networks, rather than in the word
vectors. Another problem with these existing ap-
proaches is that they are computationally very ex-
pensive to train; e.g. the Pair2Vec model is re-
ported to need 7-10 days of training on unspecified
hardware2. In contrast, the approach we propose
in this paper is computationally much simpler,
while resulting in relational word vectors that en-
code relational information more accurately than
those of the Pair2Vec model in lexical semantics
tasks, as we will see in Section 5.

Knowledge-Enhanced Word Embeddings. Sev-
2github.com/mandarjoshi90/pair2vec

github.com/mandarjoshi90/pair2vec


3288

eral authors have tried to improve word embed-
dings by incorporating external knowledge bases.
For example, some authors have proposed models
which combine the loss function of a word em-
bedding model, to ensure that word vectors are
predictive of their context words, with the loss
function of a knowledge graph embedding model,
to encourage the word vectors to additionally be
predictive of a given set of relational facts (Xu
et al., 2014; Celikyilmaz et al., 2015; Chen et al.,
2016). Other authors have used knowledge bases
in a more restricted way, by taking the fact that
two words are linked to each other in a given
knowledge graph as evidence that their word vec-
tors should be similar (Faruqui et al., 2015; Speer
et al., 2017). Finally, there has also been work that
uses lexicons to learn word embeddings which are
specialized towards certain types of lexical knowl-
edge, such as hypernymy (Nguyen et al., 2017;
Vulic and Mrksic, 2018), antonymy (Liu et al.,
2015; Ono et al., 2015) or a combination of var-
ious linguistic constraints (Mrkšić et al., 2017).

Our method differs in two important ways from
these existing approaches. First, rather than rely-
ing on an external knowledge base, or other forms
of supervision, as in e.g. (Chen et al., 2016), our
method is completely unsupervised, as our only
input consists of a text corpus. Second, whereas
existing work has focused on methods for improv-
ing word embeddings, our aim is to learn vec-
tor representations that are complementary to stan-
dard word embeddings.

3 Model Description

We aim to learn representations that are comple-
mentary to standard word vectors and are special-
ized towards relational knowledge. To differenti-
ate them from standard word vectors, they will be
referred to as relational word vectors. We write
ew for the relational word vector representation of
w. The main idea of our method is to first learn, for
each pair of closely related words w and v, a rela-
tion vector rwv that captures how these words are
related, which we discuss in Section 3.1. In Sec-
tion 3.2 we then explain how we learn relational
word vectors from these relation vectors.

3.1 Unsupervised Relation Vector Learning

Our goal here is to learn relation vectors for
closely related words. For both the selection of the
vocabulary and the method to learn relation vec-

tors we mainly follow the initialization method of
Camacho-Collados et al. (2019, RELATIVEinit) ex-
cept for an important difference explained below
regarding the symmetry of the relations. Other re-
lation embedding methods could be used as well,
e.g., (Jameel et al., 2018; Washio and Kato, 2018b;
Espinosa Anke and Schockaert, 2018; Joshi et al.,
2019), but this method has the advantage of being
highly efficient. In the following we describe this
procedure for learning relation vectors: we first
explain how a set of potentially related word pairs
is selected, and then focus on how relation vectors
rwv for these word pairs can be learned.

Selecting Related Word Pairs. Starting from a
vocabulary V containing the words of interest (e.g.
all sufficiently frequent words), as a first step we
need to choose a set R ⊆ V × V of potentially
related words. For each of the word pairs in R
we will then learn a relation vector, as explained
below. To select this set R, we only consider
word pairs that co-occur in the same sentence in
a given reference corpus. For all such word pairs,
we then compute their strength of relatedness fol-
lowing Levy et al. (2015a) by using a smoothed
version of pointwise mutual information (PMI),
where we use 0.5 as exponent factor. In particular,
for each word w ∈ V , the set R contains all suf-
ficiently frequently co-occurring pairs (w, v) for
which v is within the top-100 most closely related
words to w, according to the following score:

PMI0.5(u, v) = log
(
nwv · s∗∗
nw∗ · sv∗

)
(1)

where nwv is the harmonically weighted3 number
of times the words w and v occur in the same sen-
tence within a distance of at most 10 words, and:

nw∗ =
∑
u∈V

nwu; sv∗ = n
0.5
v∗ ; s∗∗ =

∑
u∈V

su∗

This smoothed variant of PMI has the advantage
of being less biased towards infrequent (and thus
typically less informative) words.

Learning Relation Vectors. In this paper, we
will rely on word vector averaging for learning
relation vectors, which has the advantage of be-
ing much faster than other existing approaches,
and thus allows us to consider a higher number
of word pairs (or a larger corpus) within a fixed

3A co-occurrence in which there are k words in between
w and v then receives a weight of 1

k+1
.



3289

time budget. Word vector averaging has moreover
proven surprisingly effective for learning relation
vectors (Weston et al., 2013; Hashimoto et al.,
2015; Fan et al., 2015; Espinosa Anke and Schock-
aert, 2018), as well as in related tasks such as sen-
tence embedding (Wieting et al., 2016).

Specifically, to construct the relation vec-
tor rwv capturing the relationship between the
words w and v we proceed as follows. First,
we compute a bag of words representation
{(w1, f1), ..., (wn, fn)}, where fi is the number of
times the word wi occurs in between the words w
and v in any given sentence in the corpus. The re-
lation vector rwv is then essentially computed as
a weighted average:

rwv = norm

(
n∑

i=1

fi ·wi

)
(2)

where we write wi for the vector representation
of wi in some given pre-trained word embedding,
and norm(v) = v‖v‖ .

In contrast to other approaches, we do not dif-
ferentiate between sentences where w occurs be-
fore v and sentences where v occurs before w.
This means that our relation vectors are symmet-
ric in the sense that rwv = rvw. This has the
advantage of alleviating sparsity issues. While the
directionality of many relations is important, the
direction can often be recovered from other infor-
mation we have about the words w and v. For in-
stance, knowing that w and v are in a capital-of
relationship, it is trivial to derive that “v is the cap-
ital of w”, rather than the other way around, if we
also know that w is a country.

3.2 Learning Relational Word Vectors

The relation vectors rwv capture relational infor-
mation about the word pairs in R. The rela-
tional word vectors will be induced from these
relation vectors by encoding the requirement that
ew and ev should be predictive of rwv, for each
(w, v) ∈ R. To this end, we use a simple neu-
ral network with one hidden layer,4 whose input
is given by (ew + ev) ⊕ (ew � ev), where we
write ⊕ for vector concatenation and � for the
component-wise multiplication. Note that the in-
put needs to be symmetric, given that our relation

4More complex architectures could be used, e.g., (Joshi
et al., 2019), but in this case we decided to use a simple archi-
tecture as the main aim of this paper is to encode all relational
information into the word vectors, not in the network itself.

Figure 1: Relational word embedding architecture. At
the bottom of the figure, the input layer is constructed
from the relational word embeddings ew and ev , which
are the vectors to be learnt. As shown at the top, we
aim to predict the target relation vector rwv .

vectors are symmetric, which makes the vector
addition and component-wise multiplication two
straightforward encoding choices. Figure 1 de-
picts an overview of the architecture of our model.
The network is defined as follows:

iwv = (ew + ev)⊕ (ew � ev)
hwv = f(Xiwv + a)

owv = f(Yhwv + b)

(3)

for some activation function f . We train this net-
work to predict the relation vector rwv, by mini-
mizing the following loss:

L =
∑

(w,v)∈R

(
owv − rwv

)2
(4)

The relational word vectors ew can be initialized
using standard word embeddings trained on the
same corpus.

4 Experimental Setting

In what follows, we detail the resources and train-
ing details that we used to obtain the relational
word vectors.

Corpus and Word Embeddings. We followed
the setting of Joshi et al. (2019) and used the En-
glish Wikipedia5 as input corpus. Multiwords (e.g.
Manchester United) were grouped together as a

5Tokenized and lowercased dump of January 2018.



3290

single token by following the same approach de-
scribed in Mikolov et al. (2013a). As word embed-
dings, we used 300-dimensional FastText vectors
(Bojanowski et al., 2017) trained on Wikipedia
with standard hyperparameters. These embed-
dings are used as input to construct the relation
vectors rwv (see Section 3.1),6 which are in turn
used to learn relational word embeddings ew (see
Section 3.2). The FastText vectors are additionally
used as our baseline word embedding model.

Word pair vocabulary. As our core vocabulary
V , we selected the 100, 000 most frequent words
from Wikipedia. To construct the set of word pairs
R, for each word from V , we selected the 100
most closely related words (cf. Section 3.1), con-
sidering only consider word pairs that co-occur at
least 25 times in the same sentence throughout the
Wikipedia corpus. This process yielded relation
vectors for 974,250 word pairs.

Training. To learn our relational word embed-
dings we use the model described in Section 3.2.
The embedding layer is initialized with the stan-
dard FastText 300-dimensional vectors trained on
Wikipedia. The method was implemented in Py-
Torch, employing standard hyperparameters, us-
ing ReLU as the non-linear activation function f
(Equation 3). The hidden layer of the model was
fixed to the same dimensionality as the embedding
layer (i.e. 600). The stopping criterion was de-
cided based on a small development set, by set-
ting aside 1% of the relation vectors. Code to
reproduce our experiments, as well as pre-trained
models and details of the implementation such as
other network hyperparameters, are available at
https://github.com/pedrada88/rwe.

5 Experimental Results

A natural way to assess the quality of word vectors
is to test them in lexical semantics tasks. However,
it should be noted that relational word vectors be-
have differently from standard word vectors, and
we should not expect the relational word vectors
to be meaningful in unsupervised tasks such as
semantic relatedness (Turney and Pantel, 2010).
In particular, note that a high similarity between
ew and ev should mean that relationships which
hold for w have a high probability of holding for
v as well. Words which are related, but not syn-

6We based our implementation to learn relation vec-
tors on the code available at https://github.com/
pedrada88/relative

onymous, may thus have very dissimilar relational
word vectors. Therefore, we test our proposed
models on a number of different supervised tasks
for which accurately capturing relational informa-
tion is crucial to improve performance.

Comparison systems. Standard FastText vec-
tors, which were used to construct the relation
vectors, are used as our main baseline. In ad-
dition, we also compare with the word embed-
dings that were learned by the Pair2Vec system7

(see Section 2). We furthermore report the results
of two methods which leverage knowledge bases
to enrich FastText word embeddings: Retrofitting
(Faruqui et al., 2015) and Attract-Repel (Mrkšić
et al., 2017). Retrofitting exploits semantic rela-
tions from a knowledge base to re-arrange word
vectors of related words such that they become
closer to each other, whereas Attract-Repel makes
use of different linguistic constraints to move word
vectors closer together or further apart depend-
ing on the constraint. For Retrofitting we make
use of WordNet (Fellbaum, 1998) as the input
knowledge base, while for Attract-Repel we use
the default configuration with all constraints from
PPDB (Pavlick et al., 2015), WordNet and Babel-
Net (Navigli and Ponzetto, 2012). All compari-
son systems are 300-dimensional and trained on
the same Wikipedia corpus.

5.1 Relation Classification

Given a pre-defined set of relation types and a pair
of words, the relation classification task consists
in selecting the relation type that best describes
the relationship between the two words. As test
sets we used DiffVec (Vylomova et al., 2016) and
BLESS8 (Baroni and Lenci, 2011). The DiffVec
dataset includes 12,458 word pairs, covering fif-
teen relation types including hypernymy, cause-
purpose or verb-noun derivations. On the other
hand, BLESS includes semantic relations such
as hypernymy, meronymy, and co-hyponymy.9

BLESS includes a train-test partition, with 13,258
and 6,629 word pairs, respectively. This task is
treated as a multi-class classification problem

As a baseline model (Diff), we consider the
usual representation of word pairs in terms of their
vector differences (Fu et al., 2014; Roller et al.,

7We used the pre-trained model of its official repository.
8http://clic.cimec.unitn.it/distsem
9Note that both datasets exhibit overlap in a number of

relations as some instances from DiffVec were taken from
BLESS.

https://github.com/pedrada88/rwe
https://github.com/pedrada88/relative
https://github.com/pedrada88/relative
http://clic.cimec.unitn.it/distsem


3291

Encoding Model Reference
DiffVec BLESS

Acc. F1 Prec. Rec. Acc. F1 Prec. Rec.

Mult+Avg

RWE (This paper) 85.3 64.2 65.1 64.5 94.3 92.8 93.0 92.6
Pair2Vec (Joshi et al., 2019) 85.0 64.0 65.0 64.5 91.2 89.3 88.9 89.7
FastText (Bojanowski et al., 2017) 84.2 61.4 62.6 61.9 92.8 90.4 90.7 90.2
Retrofitting† (Faruqui et al., 2015) 86.1* 64.6* 66.6* 64.5* 90.6 88.3 88.1 88.6
Attract-Repel† (Mrkšić et al., 2017) 86.0* 64.6* 66.0* 65.2* 91.2 89.0 88.8 89.3

Mult+Conc
Pair2Vec (Joshi et al., 2019) 84.8 64.1 65.7 64.4 90.9 88.8 88.6 89.1
FastText (Bojanowski et al., 2017) 84.3 61.3 62.4 61.8 92.9 90.6 90.8 90.4

Diff (only) FastText (Bojanowski et al., 2017) 81.9 57.3 59.3 57.8 88.5 85.4 85.7 85.4

Table 1: Accuracy and macro-averaged F-Measure, precision and recall on BLESS and DiffVec. Models marked
with † use external resources. The results with * indicate that WordNet was used for both the development of the
model and the construction of the dataset. All models concatenate their encoded representations with the baseline
vector difference of standard FastText word embeddings.

2014; Weeds et al., 2014), using FastText word
embeddings. Since our goal is to show the com-
plementarity of relational word embeddings with
standard word vectors, for our method we con-
catenate the difference wj − wi with the vectors
ei + ej and ei · ej (referred to as the Mult+Avg
setting; our method is referred to as RWE). We
use a similar representation for the other methods,
simply replacing the relational word vectors by
the corresponding vectors (but keeping the Fast-
Text vector difference). We also consider a vari-
ant in which the FastText vector difference is con-
catenated with wi + wj and wi · wj, which of-
fers a more direct comparison with the other meth-
ods. This goes in line with recent works that have
shown how adding complementary features on top
of the vector differences, e.g. multiplicative fea-
tures (Vu and Shwartz, 2018), help improve the
performance. Finally, for completeness, we also
include variants where the average ei + ej is re-
placed by the concatenation ei ⊕ ej (referred to as
Mult+Conc), which is the encoding considered in
Joshi et al. (2019).

For these experiments we train a linear SVM
classifier directly on the word pair encoding, per-
forming a 10-fold cross-validation in the case of
DiffVec, and using the train-test splits of BLESS.

Results Table 1 shows the results of our rela-
tional word vectors, the standard FastText embed-
dings and other baselines on the two relation clas-
sification datasets (i.e. BLESS and DiffVec). Our
model consistently outperforms the FastText em-
beddings baseline and comparison systems, with
the only exception being the precision score for
DiffVec. Despite being completely unsupervised,
it is also surprising that our model manages to
outperform the knowledge-enhanced embeddings

of Retrofitting and Attract-Repel in the BLESS
dataset. For DiffVec, let us recall that both these
approaches have the unfair advantage of having
had WordNet as source knowledge base, used both
to construct the test set and to enhance the word
embeddings. In general, the improvement of RWE
over standard word embeddings suggests that our
vectors capture relations in a way that is compati-
ble to standard word vectors (which will be further
discussed in Section 6.2).

5.2 Lexical Feature Modelling

Standard word embedding models tend to cap-
ture semantic similarity rather well (Baroni et al.,
2014; Levy et al., 2015a). However, even though
other kinds of lexical properties may also be en-
coded (Gupta et al., 2015), they are not explic-
itly modeled. Based on the hypothesis that rela-
tional word embeddings should allow us to model
such properties in a more consistent and transpar-
ent fashion, we select the well-known McRae Fea-
ture Norms benchmark (McRae et al., 2005) as
testbed. This dataset10 is composed of 541 words
(or concepts), each of them associated with one or
more features. For example, ‘a bear is an animal’,
or ‘a bowl is round’. As for the specifics of our
evaluation, given that some features are only asso-
ciated with a few words, we follow the setting of
Rubinstein et al. (2015) and consider the eight fea-
tures with the largest number of associated words.
We carry out this evaluation by treating the task
as a multi-class classification problem, where the
labels are the word features. As in the previous
task, we use a linear SVM classifier and perform
3-fold cross-validation. For each input word, the

10Downloaded from https://sites.google.com/
site/kenmcraelab/norms-data

https://sites.google.com/site/kenmcraelab/norms-data
https://sites.google.com/site/kenmcraelab/norms-data


3292

Model
McRae Feature Norms

QVEC
Overall metal is small is large animal is edible wood is round is long

RWE 55.2 73.6 46.7 45.9 89.2 61.5 38.5 39.0 46.8 55.4
Pair2Vec 55.0 71.9 49.2 43.3 88.9 68.3 37.7 35.0 45.5 52.7
Retrofitting† 50.6 72.3 44.0 39.1 90.6 75.7 15.4 22.9 44.4 56.8*
Attract-Repel† 50.4 73.2 44.4 33.3 88.9 71.8 31.1 24.2 35.9 55.9*
FastText 54.6 72.7 48.4 45.2 87.5 63.2 33.3 39.0 47.8 54.6

Table 2: Results on the McRae feature norms dataset (Macro F-Score) and QVEC (correlation score). Models
marked with † use external resources. The results with * indicate that WordNet was used for both the development
of the model and the construction of the dataset.

word embedding of the corresponding feature is
fed to the classifier concatenated with its baseline
FastText embedding.

Given that the McRae Feature Norms bench-
mark is focused on nouns, we complement this
experiment with a specific evaluation on verbs. To
this end, we use the verb set of QVEC11 (Tsvetkov
et al., 2015), a dataset specifically aimed at mea-
suring the degree to which word vectors capture
semantic properties which has shown to strongly
correlate with performance in downstream tasks
such as text categorization and sentiment analy-
sis. QVEC was proposed as an intrinsic evalua-
tion benchmark for estimating the quality of word
vectors, and in particular whether (and how much)
they predict lexical properties, such as words be-
longing to one of the fifteen verb supersenses con-
tained in WordNet (Miller, 1995). As is custom-
ary in the literature, we compute Pearson cor-
relation with respect to these predefined seman-
tic properties, and measure how well a given set
of word vectors is able to predict them, with
higher being better. For this task we compare the
300-dimensional word embeddings of all models
(without concatenating them with standard word
embeddings), as the evaluation measure only as-
sures a fair comparison for word embedding mod-
els of the same dimensionality.

Results Table 2 shows the results on the McRae
Feature Norms dataset12 and QVEC. In the case
of the McRae Feature Norms dataset, our rela-
tional word embeddings achieve the best overall
results, although there is some variation for the in-
dividual features. These results suggest that attri-
butional information is encoded well in our rela-
tional word embeddings. Interestingly, our results
also suggest that Retrofitting and Attract-Repel,

11https://github.com/ytsvetko/qvec
12Both metal and wood correspond to made of relations.

which use pairs of related words during training,
may be too naı̈ve to capture the complex relation-
ships proposed in these benchmarks. In fact, they
perform considerably lower than the baseline Fast-
Text model. On the other hand, Pair2Vec, which
we recall is the most similar to our model, yields
slightly better results than the FastText baseline,
but still worse than our relational word embedding
model. This is especially remarkable considering
its much lower computational cost.

As far as the QVEC results are concerned, our
method is only outperformed by Retrofitting and
Attract-Repel. Nevertheless, the difference is min-
imal, which is surprising given that these methods
leverage the same WordNet resource which is used
for the evaluation.

6 Analysis

To complement the evaluation of our relational
word vectors on lexical semantics tasks, in this
section we provide a qualitative analysis of their
intrinsic properties.

6.1 Word Embeddings: Nearest Neighbours

First, we provide an analysis based on the nearest
neighbours of selected words in the vector space.
Table 4 shows nearest neighbours of our relational
word vectors and the standard FastText embed-
dings.13 The table shows that our model captures
some subtle properties, which are not normally en-
coded in knowledge bases. For example, geomet-
ric shapes are clustered together around the sphere
vector, unlike in FastText, where more loosely re-
lated words such as “dimension” are found. This
trend can easily be observed as well in the philol-
ogy and assassination cases.

In the bottom row, we show cases where rela-
tional information is somewhat confused with col-

13Recall from Section 4 that both were trained on
Wikipedia with the same dimensionality, i.e., 300.

https://github.com/ytsvetko/qvec


3293

SPHERE PHILOLOGY ASSASSINATION DIVERSITY

RWE FastText RWE FastText RWE FastText RWE FastText

rectangle spheres metaphysics philological riot assassinate connectedness cultural diversity
conic spherical pedagogy philologist premeditate attempt openness diverse

hexagon dimension docent literature bombing attempts creativity genetic diversity
INTERSECT BEHAVIOUR CAPABILITY EXECUTE

RWE FastText RWE FastText RWE FastText RWE FastText

tracks intersection aggressive behaviour refueling capabilities murder execution
northbound bisect detrimental behavioural miniaturize capable interrogation executed

northwesterly intersectional distasteful misbehaviour positioning survivability incarcerate summarily executed

Table 3: Nearest neighbours for selected words in our relational word embeddings (RWE) and FastText embeddings

locationality, leading to undesired clusters, such as
intersect being close in the space with “tracks”,
or behaviour with “aggressive” or “detrimental”.
These examples thus point towards a clear direc-
tion for future work, in terms of explicitly differ-
entiating collocations from other relationships.

6.2 Word Relation Encoding

Unsupervised learning of analogies has proven to
be one of the strongest selling points of word em-
bedding research. Simple vector arithmetic, or
pairwise similarities (Levy et al., 2014), can be
used to capture a surprisingly high number of se-
mantic and syntactic relations. We are thus in-
terested in exploring semantic clusters as they
emerge when encoding relations using our rela-
tional word vectors. Recall from Section 3.2 that
relations are encoded using addition and point-
wise multiplication of word vectors.

Table 4 shows, for a small number of selected
word pairs, the top nearest neighbors that were
unique to our 300-dimensional relational word
vectors. Specifically, these pairs were not found
among the top 50 nearest neighbors for the Fast-
Text word vectors of the same dimensionality, us-
ing the standard vector difference encoding. Sim-
ilarly, we also show the top nearest neighbors that
were unique to the FastText word vector difference
encoding. As can be observed, our relational word
embeddings can capture interesting relationships
which go beyond what is purely captured by sim-
ilarity. For instance, for the pair “innocent-naive”
our model includes similar relations such as vain-
selfish, honest-hearted or cruel-selfish as nearest
neighbours, compared with the nearest neighbours
of standard FastText embeddings which are harder
to interpret.

Interestingly, even though not explicitly en-
coded in our model, the table shows some exam-
ples that highlight one property that arises often,

which is the ability of our model to capture co-
hyponyms as relations, e.g., wrist-knee and anger-
despair as nearest neighbours of “shoulder-ankle”
and “shock-grief”, respectively. Finally, one last
advantage that we highlight is the fact that our
model seems to perform implicit disambiguation
by balancing a word’s meaning with its paired
word. For example, the “oct-feb” relation vec-
tor correctly brings together other month abbrevia-
tions in our space, whereas in the FastText model,
its closest neighbour is ‘doppler-wheels’, a rela-
tion which is clearly related to another sense of
oct, namely its use as an acronym to refer to ‘opti-
cal coherence tomography’ (a type of x-ray proce-
dure that uses the doppler effect principle).

6.3 Lexical Memorization

One of the main problems of word embedding
models performing lexical inference (e.g. hyper-
nymy) is lexical memorization. Levy et al.
(2015b) found that the high performance of super-
vised distributional models in hypernymy detec-
tion tasks was due to a memorization in the train-
ing set of what they refer to as prototypical hyper-
nyms. These prototypical hypernyms are general
categories which are likely to be hypernyms (as
occurring frequently in the training set) regardless
of the hyponym. For instance, these models could
equally predict the pairs dog-animal and screen-
animal as hyponym-hypernym pairs. To measure
the extent to which our model is prone to this prob-
lem we perform a controlled experiment on the
lexical split of the HyperLex dataset (Vulić et al.,
2017). This lexical split does not contain any word
overlap between training and test, and therefore
constitutes a reliable setting to measure the gener-
alization capability of embedding models in a con-
trolled setting (Shwartz et al., 2016). In HyperLex,
each pair is provided by a score which measures
the strength of the hypernymy relation.



3294

INNOCENT-NAIVE POLES-SWEDES SHOULDER-ANKLE
RWE FastText RWE FastText RWE FastText

vain-selfish murder-young lithuanians-germans polish-swedish wrist-knee oblique-ligament
honest-hearted imprisonment-term germans-lithuanians poland-sweden thigh-knee pick-ankle injury
cruel-selfish conspiracy-minded russians-lithuanians czechoslovakia-sweden neck-knee suffer-ankle injury

SHOCK-GRIEF STRENGTHEN-TROPICAL CYCLONE OCT-FEB
RWE FastText RWE FastText RWE FastText

anger-despair overcome-sorrow intensify-tropical cyclone name-tropical cyclones aug-nov doppler-wheels
anger-sorrow overcome-despair weaken-tropical storm bias-tropical cyclones sep-nov scanner-read
anger-sadness moment-sadness intensify-tropical storm scheme-tropical cyclones nov-sep ultrasound-baby

Table 4: Three nearest neighbours for selected word pairs using our relational word vector’s relation encoding
(RWE) and the standard vector difference encoding of FastText word embeddings. In each column only the word
pairs which were on the top 50 NNs of the given model but not in the other are listed. Relations which include one
word from the original pair were not considered.

Encoding Model r ρ

Mult+Avg

RWE 38.8 38.4
Pair2Vec 28.3 26.5
FastText 37.2 35.8
Retrofitting† 29.5* 28.9*
Attract-Repel† 29.7* 28.9*

Mult+Conc
Pair2Vec 29.8 30.0
FastText 35.7 33.3

Diff (only) FastText 29.9 30.1

Table 5: Pearson (r) and Spearman (ρ) correlation on
a subset of the HyperLex lexical split. Models marked
with † use external resources. All models concatenate
their encoded representations with the baseline vector
difference of standard FastText word embeddings.

For these experiments we considered the same
experimental setting as described in Section 4. In
this case we only considered the portion of the Hy-
perLex training and test sets covered in our vocab-
ulary14 and used an SVM regression model over
the word-based encoded representations. Table 5
shows the results for this experiment. Even though
the results are low overall (noting e.g. that results
for the random split are in some cases above 50%
as reported in the literature), our model can clearly
generalize better than other models. Interestingly,
methods such as Retrofitting and Attract-Repel
perform worse than the FastText vectors. This can
be attributed to the fact that these models have
been mainly tuned towards similarity, which is a
feature that loses relevance in this setting. Like-
wise, the relation-based embeddings of Pair2Vec
do not help, probably due to the high-capacity of
their model, which makes the word embeddings
less informative.

14Recall from Section 4 that this vocabulary is shared by
all comparison systems.

7 Conclusions

We have introduced the notion of relational word
vectors, and presented an unsupervised method for
learning such representations. Parting ways from
previous approaches where relational information
was either encoded in terms of relation vectors
(which are highly expressive but can be more dif-
ficult to use in applications), represented by trans-
forming standard word vectors (which capture re-
lational information only in a limited way), or by
taking advantage of external knowledge reposito-
ries, we proposed to learn an unsupervised word
embedding model that is tailored specifically to-
wards modelling relations. Our model is intended
to capture knowledge which is complementary
to that of standard similarity-centric embeddings,
and can thus be used in combination.

We tested the complementarity of our relational
word vectors with standard FastText word embed-
dings on several lexical semantic tasks, capturing
different levels of relational knowledge. The eval-
uation indicates that our proposed method indeed
results in representations that capture relational
knowledge in a more nuanced way. For future
work, we would be interested in further explor-
ing the behavior of neural architectures for NLP
tasks which intuitively would benefit from hav-
ing access to relational information, e.g., text clas-
sification (Espinosa Anke and Schockaert, 2018;
Camacho-Collados et al., 2019) and other lan-
guage understanding tasks such as natural lan-
guage inference or reading comprehension, in the
line of Joshi et al. (2019).

Acknowledgments. Jose Camacho-Collados and
Steven Schockaert were supported by ERC Start-
ing Grant 637277.



3295

References
Marco Baroni, Georgiana Dinu, and Germán

Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, pages 238–247.

Marco Baroni and Alessandro Lenci. 2011. How we
blessed distributional semantic evaluation. In Proc.
GEMS Workshop, pages 1–10.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5.

Zied Bouraoui, Shoaib Jameel, and Steven Schockaert.
2018. Relation induction in word embeddings revis-
ited. In Proceedings of COLING, pages 1627–1637.

Jose Camacho-Collados, Luis Espinosa-Anke, Shoaib
Jameel, and Steven Schockaert. 2019. A latent vari-
able model for learning distributional relation vec-
tors. In Proceedings of IJCAI.

Asli Celikyilmaz, Dilek Hakkani-Tr, Panupong Pasu-
pat, and Ruhi Sarikaya. 2015. Enriching word em-
beddings using knowledge graph for semantic tag-
ging in conversational dialog systems. In AAAI
Spring Symposium.

Jiaqiang Chen, Niket Tandon, Charles Darwis Hari-
man, and Gerard de Melo. 2016. Webbrain: Joint
neural learning of large-scale commonsense knowl-
edge. In Proceedings of ISWC, pages 102–118.

Luis Espinosa Anke and Steven Schockaert. 2018.
SeVeN: Augmenting word embeddings with unsu-
pervised relation vectors. In Proceedings of COL-
ING, pages 2653–2665.

Miao Fan, Kai Cao, Yifan He, and Ralph Grishman.
2015. Jointly embedding relations and mentions for
knowledge population. In Proceedings of RANLP,
pages 186–191.

Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,
Chris Dyer, Eduard H. Hovy, and Noah A. Smith.
2015. Retrofitting word vectors to semantic lexi-
cons. In Proceedings of NAACL, pages 1606–1615.

Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.

Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng
Wang, and Ting Liu. 2014. Learning semantic hier-
archies via word embeddings. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics, pages 1199–1209.

Abhijeet Gupta, Gemma Boleda, Marco Baroni, and
Sebastian Padó. 2015. Distributional vectors encode
referential attributes. In Proceedings of EMNLP,
pages 12–21.

Kazuma Hashimoto, Pontus Stenetorp, Makoto Miwa,
and Yoshimasa Tsuruoka. 2015. Task-oriented
learning of word embeddings for semantic relation
classification. In Proceedings of CoNLL, pages
268–278.

Shoaib Jameel, Zied Bouraoui, and Steven Schockaert.
2018. Unsupervised learning of distributional rela-
tion vectors. In Proceedings of ACL, pages 23–33.

Mandar Joshi, Eunsol Choi, Omer Levy, Daniel S
Weld, and Luke Zettlemoyer. 2019. pair2vec: Com-
positional word-pair embeddings for cross-sentence
inference. In Proceedings of NAACL.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015a.
Improving distributional similarity with lessons
learned from word embeddings. Transactions of the
Association for Computational Linguistics, 3:211–
225.

Omer Levy, Yoav Goldberg, and Israel Ramat-Gan.
2014. Linguistic regularities in sparse and explicit
word representations. In Proceedings of CoNLL,
pages 171–180.

Omer Levy, Steffen Remus, Chris Biemann, Ido Da-
gan, and Israel Ramat-Gan. 2015b. Do supervised
distributional methods really learn lexical inference
relations? In Proceedings of NAACL.

Tal Linzen. 2016. Issues in evaluating semantic spaces
using word analogies. In Proceedings of the 1st
Workshop on Evaluating Vector-Space Representa-
tions for NLP, pages 13–18.

Quan Liu, Hui Jiang, Si Wei, Zhen-Hua Ling, and
Yu Hu. 2015. Learning semantic word embeddings
based on ordinal knowledge constraints. In Pro-
ceedings of ACL, pages 1501–1511.

Ken McRae, George S Cree, Mark S Seidenberg, and
Chris McNorgan. 2005. Semantic feature produc-
tion norms for a large set of living and nonliving
things. Behavior research methods, 37(4):547–559.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013a. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In Proceedings of NIPS, pages 3111–
3119.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the Annual
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746–751.

George A Miller. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39–41.

Nikola Mrkšić, Ivan Vulić, Diarmuid Ó Séaghdha, Ira
Leviant, Roi Reichart, Milica Gašić, Anna Korho-
nen, and Steve Young. 2017. Semantic special-
ization of distributional word vector spaces using



3296

monolingual and cross-lingual constraints. Transac-
tions of the Association of Computational Linguis-
tics, 5(1):309–324.

Roberto Navigli and Simone Paolo Ponzetto. 2012.
Babelnet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217–
250.

Kim Anh Nguyen, Maximilian Köper, Sabine Schulte
im Walde, and Ngoc Thang Vu. 2017. Hierarchical
embeddings for hypernymy detection and direction-
ality. In Proceedings of EMNLP, pages 233–243.

Masataka Ono, Makoto Miwa, and Yutaka Sasaki.
2015. Word embedding-based antonym detection
using thesauri and distributional information. In
Proceedings of NAACL-HLT, pages 984–989.

Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch,
Benjamin Van Durme, and Chris Callison-Burch.
2015. Ppdb 2.0: Better paraphrase ranking, fine-
grained entailment relations, word embeddings, and
style classification. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 2:
Short Papers), pages 425–430.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors
for word representation. In Proceedings of EMNLP,
pages 1532–1543.

Anna Rogers, Aleksandr Drozd, and Bofang Li. 2017.
The (too many) problems of analogical reasoning
with word vectors. In Proceedings of the 6th Joint
Conference on Lexical and Computational Seman-
tics (* SEM 2017), pages 135–148.

Stephen Roller, Katrin Erk, and Gemma Boleda. 2014.
Inclusive yet selective: Supervised distributional hy-
pernymy detection. In Proceedings of COLING,
pages 1025–1036.

Dana Rubinstein, Effi Levi, Roy Schwartz, and Ari
Rappoport. 2015. How well do distributional mod-
els capture different types of semantic knowledge?
In Proceedings of ACL, pages 726–730.

Vered Shwartz, Yoav Goldberg, and Ido Dagan. 2016.
Improving hypernymy detection with an integrated
path-based and distributional method. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics, pages 2389–2398.

Robyn Speer, Joshua Chin, and Catherine Havasi.
2017. Conceptnet 5.5: An open multilingual graph
of general knowledge. In Proceedings of AAAI,
pages 4444–4451.

Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guil-
laume Lample, and Chris Dyer. 2015. Evaluation of
word vector representations by subspace alignment.
In Proceedings of EMNLP, pages 2049–2054.

Peter D. Turney. 2005. Measuring semantic similarity
by latent relational analysis. In Proceedings of IJ-
CAI, pages 1136–1141.

Peter D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141–188.

Tu Vu and Vered Shwartz. 2018. Integrating mul-
tiplicative features into supervised distributional
methods for lexical entailment. In Proceedings of
the Seventh Joint Conference on Lexical and Com-
putational Semantics, pages 160–166.

Ivan Vulić, Daniela Gerz, Douwe Kiela, Felix Hill,
and Anna Korhonen. 2017. Hyperlex: A large-scale
evaluation of graded lexical entailment. Computa-
tional Linguistics, 43(4):781–835.

Ivan Vulic and Nikola Mrksic. 2018. Specialising word
vectors for lexical entailment. In Proceedings of
NAACL-HLT, pages 1134–1145.

Ekaterina Vylomova, Laura Rimell, Trevor Cohn, and
Timothy Baldwin. 2016. Take and took, gaggle and
goose, book and read: Evaluating the utility of vec-
tor differences for lexical relation learning. In Pro-
ceedings of ACL, pages 1671–1682.

Koki Washio and Tsuneaki Kato. 2018a. Filling miss-
ing paths: Modeling co-occurrences of word pairs
and dependency paths for recognizing lexical se-
mantic relations. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1123–1133.

Koki Washio and Tsuneaki Kato. 2018b. Neural latent
relational analysis to capture lexical semantic rela-
tions in a vector space. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 594–600.

Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir,
and Bill Keller. 2014. Learning to distinguish hy-
pernyms and co-hyponyms. In Proceedings of the
25th International Conference on Computational
Linguistics, pages 2249–2259.

Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proceedings of EMNLP, pages
1366–1371.

John Wieting, Mohit Bansal, Kevin Gimple, and Karen
Livescu. 2016. Towards universal paraphrastic sen-
tence embeddings. In Proceedings of ICLR.

C. Xu, Y. Bai, J. Bian, B. Gao, G. Wang, X. Liu, and
T.-Y. Liu. 2014. RC-NET: A general framework for
incorporating knowledge into word representations.
In Proc. CIKM, pages 1219–1228.


