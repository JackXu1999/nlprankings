



















































Who Sides with Whom? Towards Computational Construction of Discourse Networks for Political Debates


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2841–2847
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

2841

Who Sides With Whom? Towards Computational Construction of
Discourse Networks for Political Debates

Sebastian Padó1, André Blessing1, Nico Blokker2,
Erenay Dayanik1, Sebastian Haunss2, and Jonas Kuhn1

1IMS, University of Stuttgart, Germany
2SOCIUM, University of Bremen, Germany

Abstract
Understanding the structures of political de-
bates (which actors make what claims) is es-
sential for understanding democratic political
decision-making. The vision of computational
construction of such discourse networks from
newspaper reports brings together political sci-
ence and natural language processing. This
paper presents three contributions towards this
goal: (a) a requirements analysis, linking the
task to knowledge base population; (b) a first
release of an annotated corpus of claims on the
topic of migration, based on German newspa-
per reports; (c) initial modeling results.

1 Introduction

Democratic decision making can follow broadly
two logics: In a technocratic, depoliticized mode,
decision-making is carried out by administrative
staff and experts. However, arguably most political
decisions affecting large populations attract public
attention and thus happen in a politicized mode, in
which public debates accompany decision making
(de Wilde, 2011; Zürn, 2014; Haunss and Hofmann,
2015). Understanding the structure and evolution
of political debates is therefore essential for un-
derstanding democratic decision making. Recent
innovations that combine political claims analysis
(Koopmans and Statham, 1999) with network sci-
ence under the name of discourse network analysis
(Leifeld, 2016a) allow us to systematically analyze
the dynamics of political debates based on the an-
notation of large newspaper corpora. So far, such
studies have been carried out manually.

In this paper, we outline the road towards using
computational methods from natural language pro-
cessing for the construction of discourse networks
– working towards an integrated methodological
framework for Computational Social Science. We
make three contributions: (a) a requirements analy-
sis; (b) a manually annotated corpus of claims from

affiliation networkactors claims

actor network

(discourse coalition)

concept network

(argumentative cluster)

c1

c2

c3

c5
c4

a1

a2

a3

a5

a4

Figure 1: Actor, affiliation, and concept networks

debates about migration found in German newspa-
per reports; (c) initial modeling results that already
demonstrate the usefulness of computational meth-
ods in this context.

2 Discourse Networks: Actors and Claims

Discursive interventions are one element among
several that influence policy making (Schmidt and
Radaelli, 2004). But the exact mechanisms of polit-
ical discourse and under which condition discursive
interventions do or do not translate into political de-
cisions are largely unknown. At least there seems
to be a general agreement that the formation and
evolution of discourse coalitions is a core mecha-
nism (Hajer, 1993; Sabatier and Weible, 2007).

A discourse coalition can be generally defined
as “a group of actors who share a social construct”
(Hajer, 1993, p. 43). Political Claims Analysis
(Koopmans and Statham, 1999) provides a frame-
work in which claims, that is demands, proposals,
criticisms, or decisions in the form of statements or
collective actions reported in newspaper articles are
attributed to (groups of) actors and are categorized.
Actors and claims can be represented as the two
classes of nodes in a bipartite affiliation network.
In Figure 1, actors are circles, claims are squares,
and they are linked by edges that indicate support
(green) or opposition (orange). A discourse coali-



2842

tion is then the projection of the affiliation network
on the actor side (dotted edges), while the projec-
tion on the concept side yields the argumentative
clusters present in the debate.

3 NLP and Political Science

Our analytical goals have connecting points with a
range of activities in NLP. There has been consid-
erable work in Social Media Analysis using NLP
– in particular sentiment analysis (e.g. Ceron et al.
2014), but also going into fine-grained analysis of
groups of users/actors (Cesare et al., 2017). Nev-
ertheless, most analyses in social media concern
typically relatively broad categories, such as party
preferences (see Hong et al. 2016 for a comparison
of social media and news texts). NLP techniques
are also used for stance classification (e.g. Vilares
and He 2017) and measuring ideology in speeches
(Sim et al., 2013), and there is a fair amount of
work on agenda-setting and framing (e.g. Tsur et al.
2015; Field et al. 2018). To our knowledge, fine-
grained distinctions both for actors and claims that
are necessary for discourse network consideration
(cf. Section 4) have not been explored in depth.

Also related is the growing field of argumenta-
tion analysis/mining (e.g. Peldszus and Stede 2013;
Swanson et al. 2015; Stab and Gurevych 2017).
However, a core interest there is analyzing the ar-
gument structure of longer pieces of argumentative
text (i.e., claims and their (recursive) justifications),
whereas we focus on the core claims that actors put
forward in news coverage.

The aspect of dynamics in interaction among
actors is shared with work on the extraction of ac-
tor/character networks from texts, which has been
applied mostly to literary texts (Elson et al., 2010;
Hassan et al., 2012; Iyyer et al., 2016).

4 Computational Construction of
Discourse Networks

Seen as an end-to-end task, the computational con-
struction of affiliation networks from newspaper
articles as introduced in Section 2 represents a task
that combines binary relation extraction (Dodding-
ton et al., 2004; Hendrickx et al., 2010) with ontol-
ogization (Pennacchiotti and Pantel, 2006; Hachey
et al., 2013, i.a.). The task can be decomposed con-
ceptually as shown in Figure 2. From bottom to top,
the first task is to identify claims and actors in the
text (Tasks 1 and 2). Then, they need to be mapped
onto entities that are represented in the affiliation

Category A13: 
delay Brexit

Task 2: actor detection Task 1: claim detection

Task 4: claim
mapping 

Task 5: claim attribution
support

Labour has said it will support the amendment

Labor
party A13strong support

Task 6: 
aggregation

Labor
party

Task 3: actor
mapping

Figure 2: Construction of affiliation network construc-
tion (top) from text (bottom) as relation extraction

graph, that is, discourse referents for actors (Task
3: entity linking) and categories for claims (Task
4). Next, claims need to be attributed to actors and
classified as support or opposition (Task 5). Finally,
relations need to be aggregated across documents
(Task 6).

This setup is related to Knowledge Base Popula-
tion (McNamee et al., 2010) and presents itself as
a series of rather challenging tasks:

Actor and claim ontologies. The actors and
claims can either be known a priori (then Tasks 3
and 4 amount to classification) or can emerge from
the data (then they become clustering tasks). We
assume that there is a limited set of claims that
structures public debates on a given topic (Koop-
mans and Statham, 2010). We thus build on an
expert-defined ontology of claims (cf. Section 5).
With regard to actors, the issue is less clear: knowl-
edge bases such as Wikidata cover many persons
in the public eye. However new actors can appear
and take on importance at any time.

Discourse context. Tasks 3 and 4 regularly in-
volves coreference resolution: in the example, the
expression the amendment can only be mapped
to the correct claim if its content can be inferred.
Similarly, actors realized as pronouns have to be
resolved. Coreference resolution is still a difficult
problem (Martschat and Strube, 2014).

Dependencies among tasks. The various tasks
are clearly not independent of one another, and
joint models have been developed for a subset of
the tasks, such as coreference and relation detec-
tion (Almeida et al., 2014) or entity and relation
classification (Miwa and Sasaki, 2014; Adel and
Schütze, 2017; Bekoulis et al., 2018). However,
state-of-the-art models still struggle with sentence
complexity, and there are no comprehensive mod-
els of the complete task including aggregation.



2843

C1: Steuerung von Migration (Controlling Migration)
C2: Aufenthalt (Residency)
C3: Integration (Integration)
C4: Innere Sicherheit (Domestic Security)
C5: Aussenpolitik (Foreign Policy)
C6: Ökonomie, Arbeitsmarkt (Economy, Labor Market)
C7: Gesellschaft (Society)
C8: Verfahren (Procedures)

Table 1: Migration: Main categories in claim ontology

5 Claim Ontology and Corpus
Annotation

We now demonstrate the first steps of computa-
tional discourse network construction in a concrete
political context, namely the major topic of Ger-
man politics of 2015: the domestic debate on (im-)
migration precipitated by the war in Syria.

Claim Ontology. Following established ap-
proaches to content analysis from political science
(Leifeld, 2016b), we chose an approach that com-
bines deductive and inductive elements to iden-
tify an initial set of topic-specific claim categories.
First, we review the literature, extract relevant cat-
egories, and validate and extend them based on
an initial sample of newspaper articles from Die
Tageszeitung, a large left-leaning German quality
newspaper (www.taz.de). This results in eight
superordinate categories (cf. Table 1) and 89 sub-
categories, capturing a variety of different political
positions. These categories and their definitions
form the codebook that the annotation is based on.1

Annotation Process. Annotation follows a pro-
cedure successfully used by Haunss et al. (2013)
in the analysis of the German nuclear phase-out
debate (2011). The analysis of articles is carried
out in double, independent annotation by trained
student research assistants. An example of a text
passage and its corresponding annotation is pre-
sented in the following sentence:

(1) [Flüchtlinge zum Erlernen der deutschen
Sprache [...] verpflichten]Claim, will [die
CDU in Niedersachsen]Actor.
[Requiring refugees to learn the German
language]Claim [...] is what [the CDU party
in Lower Saxony]Actor wants.

Annotators mark the claim and the actor, clas-
sify the claim as (a subtype of) C3, integration, link
them, and mark the position (support/opposition).
That is, Tasks 1–5 from Section 4 are all carried

1For the full codebook, see the supplementary material.

Figure 3: Screenshot of annotation platform, with text
(back) and annotation window (front)

out. Crucially, cross-cutting (“multi-label”) claims
can instantiate multiple categories. In our annota-
tion, about 17% of all claims carry multiple labels.
Frequent combinations at the top level are C2+C8
(procedural aspects of residency) and C1+C5 (in-
ternational perspective on migration control).

Building on experience and tool components
from text annotation efforts in Digital Human-
ities projects (in particular the Center for Re-
flected Text Analytics, https://www.creta.
uni-stuttgart.de/en/), we developed a
web-based annotation tool, shown in Figure 3,
which both streamlines annotation and encourages
consistency. Annotation involves first marking
claim and actor spans in the text and then selecting
the correct categories for the claims and the correct
referent for the actor from drop-down lists. See
Blessing et al. (2019) for details.

Reliability and Adjudication. We compute an-
notation reliability of the original student annota-
tors for the two initial and most immediate anno-
tation steps (cf. Figure 2), namely claim detection
(Task 1) and classification (Task 4). For claim de-
tection, a classical single-label classification task,
we use Cohen’s Kappa: For each sentence, we com-
pare whether the two annotators classified the sen-
tence as part of a claim or not. We obtain a Kappa
value of 0.58. For claim classification, a multi-label
classification task, we cannot use Kappa. Instead,
we compute Macro-F1 for all top level categories,
and obtain an average F1 score of 63.5%.

These numbers, while still leaving room for im-
provement, indicate moderate to substantial agree-
ment among the student annotators. The two sets
of annotations per document are subsequently re-
viewed and adjudicated by senior domain experts

www.taz.de
https://www.creta.uni-stuttgart.de/en/
https://www.creta.uni-stuttgart.de/en/


2844

to create a reliable gold standard.
Dataset Release. With this paper, we pub-

licly release 423 fully annotated articles from the
2015 Tageszeitung. 179 articles contain at least
one claim. In total, 982 Claims in 764 differ-
ent text passages have been annotated. This in-
cludes additional information such as actor at-
tributes (name, party membership, etc.), date and
position. This dataset – together with documen-
tation and annotation guidelines is available for
research purposes at https://github.com/
mardy-spp/mardy_acl2019.

Remaining Challenges. A number of chal-
lenges remain. A technical one is the identification
of relevant documents: keyword-based methods
turn out to be insufficient. A conceptual one is that
not all decisions made in the design of the claim
ontology hold up to broad-coverage annotation. Po-
litical science has defined the ideal of ‘multi-pass
coding’ (Leifeld, 2016b) according to which the
researcher constantly reviews and updates annota-
tion in an iterative process, adding and collapsing
categories as needed. We perform such updates at
regular intervals, but they can only be meaningfully
applied to the adjudicated gold standard, not indi-
vidual annotations. Thus, our reliability is likely
underestimated by the analysis above.

6 Modeling results

Due to space restrictions, this paper only reports
on first steps towards computational construction
of discourse networks. Specifically, we present
pilot models for Tasks 1 and 4 (claim identification
and attribution), the two tasks for which we also
presented reliability analyses in Section 5.

Data setup. We randomly sampled 90% of our
dataset for training and evaluate on the other 10%;
the split is published with the dataset. We discarded
articles with no claims.

Claim Identification. We model claim identifi-
cation as a sequence labeling task: The model la-
bels each token in a sentence as B-Claim, I-Claim
or Outside, adopting a BIO schema.

We experiment with two model architectures.
The first one is BERT (Devlin et al., 2018), a state-
of-the-art transformer-based neural network model,
which we fine-tune on our training data. The sec-
ond is a current architecture for sequence labeling
that consists of an embedding layer, an LSTM layer,

and a CRF layer.2 We use word embeddings from
FastText (Bojanowski et al., 2017). In order to
add task and domain specific representations and
resolve Out-Of-Vocabulary (OOV) problem, we
experiment with a second embedding approach,
namely learning character-based embeddings from
which we compute word-level embeddings by feed-
ing the character embeddings through a CNN and
max-pooling the out. Depending on the experimen-
tal condition (see below), we use either just the
word-based or a concatenation of the word-based
and character-based embeddings, and train the em-
beddings on different corpora.

All embeddings are fed to a bidirectional LSTM
layer for contextualization. To jointly model the
label sequence, we use a CRF layer on top. For a
sequence with n words, we parameterize the dis-
tribution over all possible label sequences, Y , as

p(y|d;W) =

n∏
i=1

φi (yi−1, yi,d)∑
y′∈Y

n∏
i=1

φi
(
y′i−1, y

′
i,d
) (1)

where d = [d1, d2, . . . dn] is the set of represen-
tation produced by BiLSTM for each input word
and φi (yi−1, yi,d) is a function calculating emis-
sion and transition potentials between the tags yi−1
and yi. During training, we maximize the log-
likelihood function over the training set

L(W) =
∑
i

log p(y|d;W) (2)

During inference, the sequence with highest condi-
tional probability is predicted by a Viterbi decoder:

argmax
y∈Y

p(y|d;W) (3)

Experimentally, we compared BERT against ver-
sions of our own model which (a) do and do not
include the CRF layer; (b) do or do not use the
character-level embeddings; (c) train embeddings
on different corpora. We measure performance as
F1 scores per-class, and macro F1 scores overall.

We started with a simple model, (1), using the
default Wikipedia FastText word-level embeddings
and without CRF layer. Moving to in-domain TAZ
embeddings, (2), improves performance by 4 points
macro F1, with a slight further improvement of 0.5
points by adding character-level embeddings in

2See supplement for details and hyperparameters.

https://github.com/mardy-spp/mardy_acl2019
https://github.com/mardy-spp/mardy_acl2019


2845

Method B-C I-C O Macro

(1) EmbWiki:w+BiLSTM 31.3 37.5 93.5 54.1
(2) EmbTAZ:w+BiLSTM 38.5 43.9 93.6 58.7
(3) EmbTAZ:w,c+BiLSTM 40.0 44.1 93.1 59.1
(4) EmbTAZ:w,c+BiLSTM+CRF 49.4 53.8 95.5 66.3
(5) EmbWiki:w,c+BiLSTM+CRF 35.1 39.1 90.6 55.0

(6) BERT 49.5 52.4 94.7 65.5

Table 2: Claim identification scores on evaluation
set: F1 for BIO labels and Macro average (Mac).
EmbCorpus:type: Corpus used to train embeddings and
type (w: word, c: char). For example, EmbTAZ:w,c rep-
resents version for which character and word level em-
beddings trained on TAZ corpus.

Method C1 C2 C3 C4 C5 C6 C7 C8 Macro

NB 46 50 0 0 43 0 29 0 21
MLP 73 53 0 0 67 0 57 46 37
BiLSTM 71 71 0 0 63 0 78 24 38
BERT 44 82 54 29 50 0 53 57 46

Table 3: Claim categorization performance of several
models. Columns C1-C8 show F1 score for each cat-
egory. Macro reports macro average F1 score. NB:
Naive Bayes, MLP: Multi-Layer Perceptron.

(3). Adding a CRF layer to obtain the full model,
(4), yields a further major increase by 7 points F-
score and results in the best overall model with 66.3
macro F1. This model also outperforms BERT, (6),
numerically in macro F1 and for the two classes
(I-C) and (O). This model still profits substantially
from the in-domain embeddings: replacing them
by Wikipedia-trained ones in model (5) results in a
drop of 11 points.

Claim Classification. For our experiments on
claim classification, we assume that claims have
already been detected. To each claim span, we
assign one or more of the top categories from the
claim ontology (cf. Section 5), i.e., we perform
multi-class multi-label classification.

In terms of models, we evaluate a fine-tuned ver-
sion of BERT against three standard classification
architectures: a unigram Naive Bayes model and
Multi-Layer Perceptron and BiLSTM architectures
based on TAZ-trained FastText embeddings that
performed well in the previous experiment. All
models perform multi-class classification by mak-
ing a binary decision for each class.

Table 3 shows the results, using the same F1
measures as before. BERT excels at this task, fol-
lowed by the two embedding-based models; Naive
Bayes comes last. Interestingly, the models differ

in their performance across classes. BERT tends
to make better predictions than the other models
for small, homogeneous classes (C3: integration,
C4: security) while MLP and BiLSTM do better on
the larger and less clearly delineated classes (C1:
migration control, C7: society).

7 Conclusion

In this paper, we have sketched the way towards
a Computational Social Science (CSS) framework
for the construction of discourse networks (claims
and actors) from news coverage of political debates,
which has great potential for expanding the empir-
ical basis for research in political science. The
complexity of the scenario (fine-grained categories,
multi-category claims, complex relations, aggre-
gation) suggests that an attempt at automating the
construction in its entirety is currently not realis-
tic at a quality that makes it useful for political
scientists.

In the broader picture of a project that derives
its motivation both from NLP and from CSS, scal-
ing the computational component is an important
objective, but one that should never come at the
cost of reliability of the analytical components and
methodological validity from the point of view of
political science. A carefully laid out task analysis,
as put forward in this paper, provides the basis for
exploring more interactive “mixed methods” frame-
works (see the discussion in Kuhn (to appear)):
Computational models for a given set of claim cat-
egories can feed semi-automatic corpus annotation
through manual post correction of predictions.

Finally, an interleaved cross-disciplinary collab-
oration may support the future research process
further: the claim ontology for a new field of de-
bate could be constructed in a bootstrapping pro-
cess, combining the political scientists’ analytical
insights with (preliminary) predictions of computa-
tional seed models from partially overlapping fields.
In our collaboration, systematic tool support has al-
ready made the process of codebook development
considerably more effective.

Acknowledgments

We acknowledge funding by Deutsche Forschungs-
gemeinschaft (DFG) through MARDY (Modeling
Argumentation Dynamics) within SPP RATIO and
by Bundesministerium für Bildung und Forschung
(BMBF) through Center for Reflected Text Analyt-
ics (CRETA).



2846

References
Heike Adel and Hinrich Schütze. 2017. Global normal-

ization of convolutional neural networks for joint en-
tity and relation classification. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1723–1729, Copenhagen,
Denmark.

Mariana S. C. Almeida, Miguel B. Almeida, and André
F. T. Martins. 2014. A joint model for quotation attri-
bution and coreference resolution. In Proceedings of
the Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 39–48,
Gothenburg, Sweden.

Giannis Bekoulis, Johannes Deleu, Thomas Demeester,
and Chris Develder. 2018. Joint entity recogni-
tion and relation extraction as a multi-head selection
problem. Expert Systems with Applications, 114:34–
45.

André Blessing, Nico Blokker, Sebastian Haunss,
Jonas Kuhn, Gabriella Lapesa, and Sebastian Padó.
2019. An environment for the relational annotation
of political debates. In Proceedings of ACL System
Demonstrations, Florence, Italy.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Andrea Ceron, Luigi Curini, Stefano M. Iacus, and
Giuseppe Porro. 2014. Every tweet counts? how
sentiment analysis of social media can improve our
knowledge of citizens’ political preferences with an
application to italy and france. New Media & Soci-
ety, 16(2):340–358.

Nina Cesare, Christan Grant, and Elaine Okanyene
Nsoesie. 2017. Detection of user demographics on
social media: A review of methods and recommen-
dations for best practices. CoRR, abs/1702.01807.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

George R. Doddington, Alexis Mitchell, Mark A. Przy-
bocki, Lance A. Ramshaw, Stephanie Strassel, and
Ralph M. Weischedel. 2004. The automatic content
extraction (ACE) program – tasks, data, and evalua-
tion. In Proceedings of LREC, Lisbon, Portugal.

David K Elson, Nicholas Dames, and Kathleen R McK-
eown. 2010. Extracting social networks from lit-
erary fiction. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics,
pages 138–147, Uppsala, Sweden.

Anjalie Field, Doron Kliger, Shuly Wintner, Jennifer
Pan, Dan Jurafsky, and Yulia Tsvetkov. 2018. Fram-
ing and agenda-setting in Russian news: a compu-
tational analysis of intricate political strategies. In

Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 3570–
3580, Brussels, Belgium.

Ben Hachey, Will Radford, Joel Nothman, Matthew
Honnibal, and James R. Curran. 2013. Evaluat-
ing entity linking with Wikipedia. Artificial Intel-
ligence, 194:130 – 150.

Maarten A Hajer. 1993. Discourse Coalitions and the
Institutionalization of Practice: The Case of Acid
Rain in Britain. In Frank Fischer and John Forester,
editors, The Argumentative Turn in Policy Analysis
and Planning, pages 43–76. Duke University Press.

Ahmed Hassan, Amjad Abu-Jbara, and Dragomir
Radev. 2012. Extracting signed social networks
from text. In Workshop Proceedings of TextGraphs-
7 on Graph-based Methods for Natural Language
Processing, pages 6–14, Jeju, South Korea.

Sebastian Haunss, Matthias Dietz, and Frank
Nullmeier. 2013. Der Ausstieg aus der Atom-
energie. Diskursnetzwerkanalyse als Beitrag zur
Erklärung einer radikalen Politikwende. Zeitschrift
für Diskursforschung, 1(3):288–316.

Sebastian Haunss and Jeanette Hofmann. 2015. Entste-
hung von Politikfeldern – Bedingungen einer
Anomalie. dms – der moderne staat, 8(1):29–49.

Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian
Padó, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2010. Multi-way classification of
semantic relations between pairs of nominals. In
Proceedings of the International Workshop on Se-
mantic Evaluation, pages 33–38, Uppsala, Sweden.

Lingzi Hong, Weiwei Yang, Philip Resnik, and Vanessa
Frı́as-Martı́nez. 2016. Uncovering topic dynamics
of social media and news: The case of Ferguson. In
Proceedings of Social Informatics, pages 240–256,
Bellevue, WA.

Mohit Iyyer, Anupam Guha, Snigdha Chaturvedi, Jor-
dan Boyd-Graber, and Hal Daumé III. 2016. Feud-
ing families and former friends: Unsupervised learn-
ing for dynamic fictional relationships. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 1534–1544,
San Diego, CA.

Ruud Koopmans and Paul Statham. 1999. Polit-
ical Claims Analysis: Integrating Protest Event
And Political Discourse Approaches. Mobilization,
4(2):203–221.

Ruud Koopmans and Paul Statham. 2010. Theoreti-
cal Framework, Research Design, and Methods. In
Ruud Koopmans and Paul Statham, editors, The
Making of a European Public Sphere, pages 34–59.
Cambridge University Press.

https://www.aclweb.org/anthology/D17-1181
https://www.aclweb.org/anthology/D17-1181
https://www.aclweb.org/anthology/D17-1181
https://doi.org/10.3115/v1/E14-1005
https://doi.org/10.3115/v1/E14-1005
https://doi.org/https://doi.org/10.1016/j.eswa.2018.07.032
https://doi.org/https://doi.org/10.1016/j.eswa.2018.07.032
https://doi.org/https://doi.org/10.1016/j.eswa.2018.07.032
https://aclweb.org/anthology/Q17-1010
https://aclweb.org/anthology/Q17-1010
https://doi.org/https://doi.org/10.1177/1461444813480466
https://doi.org/https://doi.org/10.1177/1461444813480466
https://doi.org/https://doi.org/10.1177/1461444813480466
https://doi.org/https://doi.org/10.1177/1461444813480466
https://arxiv.org/pdf/1702.01807.pdf
https://arxiv.org/pdf/1702.01807.pdf
https://arxiv.org/pdf/1702.01807.pdf
https://arxiv.org/abs/1810.04805
https://arxiv.org/abs/1810.04805
https://arxiv.org/abs/1810.04805
http://www.lrec-conf.org/proceedings/lrec2004/pdf/5.pdf
http://www.lrec-conf.org/proceedings/lrec2004/pdf/5.pdf
http://www.lrec-conf.org/proceedings/lrec2004/pdf/5.pdf
https://www.aclweb.org/anthology/P10-1015
https://www.aclweb.org/anthology/P10-1015
http://aclweb.org/anthology/D18-1393
http://aclweb.org/anthology/D18-1393
http://aclweb.org/anthology/D18-1393
https://doi.org/https://doi.org/10.1016/j.artint.2012.04.005
https://doi.org/https://doi.org/10.1016/j.artint.2012.04.005
https://doi.org/https://doi.org/10.1215/9780822381815-003
https://doi.org/https://doi.org/10.1215/9780822381815-003
https://doi.org/https://doi.org/10.1215/9780822381815-003
https://aclweb.org/anthology/papers/W/W12/W12-4102/
https://aclweb.org/anthology/papers/W/W12/W12-4102/
https://shaunss.ipgovernance.eu/wp-content/uploads/2013/12/haunss-et-al-atomausstieg1.pdf
https://shaunss.ipgovernance.eu/wp-content/uploads/2013/12/haunss-et-al-atomausstieg1.pdf
https://shaunss.ipgovernance.eu/wp-content/uploads/2013/12/haunss-et-al-atomausstieg1.pdf
https://doi.org/https://doi.org/10.3224/dms.v8i1.19109
https://doi.org/https://doi.org/10.3224/dms.v8i1.19109
https://doi.org/https://doi.org/10.3224/dms.v8i1.19109
http://www.aclweb.org/anthology/S10-1006.pdf
http://www.aclweb.org/anthology/S10-1006.pdf
https://doi.org/10.1007/978-3-319-47880-7_15
https://doi.org/10.1007/978-3-319-47880-7_15
https://www.aclweb.org/anthology/N16-1180
https://www.aclweb.org/anthology/N16-1180
https://www.aclweb.org/anthology/N16-1180
https://doi.org/https://doi.org/10.1017/CBO9780511761010.004
https://doi.org/https://doi.org/10.1017/CBO9780511761010.004


2847

Jonas Kuhn. to appear. Computational text analysis
within the humanities: How to combine working
practices from the contributing fields? Language
Resources and Evaluation.

Philip Leifeld. 2016a. Discourse Network Analysis:
Policy Debates as Dynamic Networks. In Jen-
nifer Nicoll Victor, Alexander H. Montgomery, and
Mark Lubell, editors, The Oxford Handbook of Polit-
ical Networks. Oxford University Press.

Philip Leifeld. 2016b. Policy Debates as Dynamic Net-
works: German Pension Politics and Privatization
Discourse. Campus Verlag, Frankfurt/New York.

Sebastian Martschat and Michael Strube. 2014. Recall
error analysis for coreference resolution. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 2070–2081,
Doha, Qatar.

Paul McNamee, Hoa Trang Dang, Heather Simpson,
Patrick Schone, and Stephanie Strassel. 2010. An
evaluation of technologies for knowledge base pop-
ulation. In Proceedings of the Seventh Interna-
tional Language Resources and Evaluation Confer-
ence, Valletta, Malta.

Makoto Miwa and Yutaka Sasaki. 2014. Modeling
joint entity and relation extraction with table rep-
resentation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 1858–1869, Doha, Qatar.

Andreas Peldszus and Manfred Stede. 2013. From ar-
gument diagrams to argumentation mining in texts:
A survey. International Journal of Cognitive Infor-
matics and Natural Intelligence, 7(1):1–31.

Marco Pennacchiotti and Patrick Pantel. 2006. Ontolo-
gizing semantic relations. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the Association
for Computational Linguistics, pages 793–800, Syd-
ney, Australia.

Paul A. Sabatier and Cristopher M. Weible. 2007. The
Advocacy Coalition Framework: Innovations and
Clarifications. In Paul A. Sabatier, editor, Theo-
ries of the Policy Process, pages 189–220. Westview
Press.

Vivien A. Schmidt and Claudio M. Radaelli. 2004. Pol-
icy Change and Discourse in Europe: Conceptual
and Methodological Issues. West European Politics,
27(2):183.

Yanchuan Sim, Brice D. L. Acree, Justin H. Gross, and
Noah A. Smith. 2013. Measuring ideological pro-
portions in political speeches. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 91–101, Seattle, WA.

Christian Stab and Iryna Gurevych. 2017. Parsing ar-
gumentation structures in persuasive essays. Com-
putational Linguistics, 43(3):619–659.

Reid Swanson, Brian Ecker, and Marilyn Walker. 2015.
Argument mining: Extracting arguments from on-
line dialogue. In Proceedings of the Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, pages 217–226, Prague, Czech Republic.

Oren Tsur, Dan Calacci, and David Lazer. 2015. A
frame of mind: Using statistical models for detec-
tion of framing and agenda setting campaigns. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, pages 1629–
1638, Beijing, China.

David Vilares and Yulan He. 2017. Detecting perspec-
tives in political debates. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1573–1582, Copenhagen,
Denmark.

Pieter de Wilde. 2011. No Polity for Old Politics? A
Framework for Analyzing the Politicization of Euro-
pean Integration. Journal of European Integration,
33(5):559–575.

Michael Zürn. 2014. The politicization of world pol-
itics and its effects: Eight propositions. European
Political Science Review, 6(01):47–71.

https://core.ac.uk/download/pdf/46165054.pdf
https://core.ac.uk/download/pdf/46165054.pdf
http://www.aclweb.org/anthology/D14-1221
http://www.aclweb.org/anthology/D14-1221
http://www.lrec-conf.org/proceedings/lrec2010/pdf/634_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2010/pdf/634_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2010/pdf/634_Paper.pdf
http://www.aclweb.org/anthology/D14-1200
http://www.aclweb.org/anthology/D14-1200
http://www.aclweb.org/anthology/D14-1200
https://doi.org/10.4018/jcini.2013010101
https://doi.org/10.4018/jcini.2013010101
https://doi.org/10.4018/jcini.2013010101
https://doi.org/10.3115/1220175.1220275
https://doi.org/10.3115/1220175.1220275
http://hdl.handle.net/10919/68212
http://hdl.handle.net/10919/68212
http://hdl.handle.net/10919/68212
https://doi.org/10.1080/0140238042000214874
https://doi.org/10.1080/0140238042000214874
https://doi.org/10.1080/0140238042000214874
http://aclweb.org/anthology/D13-1010
http://aclweb.org/anthology/D13-1010
https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00295
https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00295
https://www.sigdial.org/files/workshops/conference16/proceedings/.../SIGDIAL31.pdf
https://www.sigdial.org/files/workshops/conference16/proceedings/.../SIGDIAL31.pdf
https://aclweb.org/anthology/papers/P/P15/P15-1157/
https://aclweb.org/anthology/papers/P/P15/P15-1157/
https://aclweb.org/anthology/papers/P/P15/P15-1157/
https://aclanthology.info/papers/D17-1165/d17-1165
https://aclanthology.info/papers/D17-1165/d17-1165
https://doi.org/https://doi.org/10.1080/07036337.2010.546849
https://doi.org/https://doi.org/10.1080/07036337.2010.546849
https://doi.org/https://doi.org/10.1080/07036337.2010.546849
https://doi.org/10.1017/S1755773912000276
https://doi.org/10.1017/S1755773912000276

