



















































Global Relation Embedding for Relation Extraction


Proceedings of NAACL-HLT 2018, pages 820–830
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Global Relation Embedding for Relation Extraction

Yu Su∗, Honglei Liu∗, Semih Yavuz, Izzeddin Gür
University of California, Santa Barbara

{ysu,honglei,syavuz,izzeddingur}@cs.ucsb.edu

Huan Sun
The Ohio State University
sun.397@osu.edu

Xifeng Yan
University of California, Santa Barbara

xyan@cs.ucsb.edu

Abstract
We study the problem of textual relation em-
bedding with distant supervision. To combat
the wrong labeling problem of distant super-
vision, we propose to embed textual relations
with global statistics of relations, i.e., the co-
occurrence statistics of textual and knowledge
base relations collected from the entire cor-
pus. This approach turns out to be more ro-
bust to the training noise introduced by distant
supervision. On a popular relation extraction
dataset, we show that the learned textual re-
lation embedding can be used to augment ex-
isting relation extraction models and signifi-
cantly improve their performance. Most re-
markably, for the top 1,000 relational facts dis-
covered by the best existing model, the preci-
sion can be improved from 83.9% to 89.3%.

1 Introduction

Relation extraction requires deep understanding of
the relation between entities. Early studies mainly
use hand-crafted features (Kambhatla, 2004; Zhou
et al., 2005), and later kernel methods are intro-
duced to automatically generate features (Zelenko
et al., 2003; Culotta and Sorensen, 2004; Bunescu
and Mooney, 2005; Zhang et al., 2006). Recently
neural network models have been introduced to
embed words, relations, and sentences into con-
tinuous feature space, and have shown a remark-
able success in relation extraction (Socher et al.,
2012; Zeng et al., 2014; Xu et al., 2015b; Zeng
et al., 2015; Lin et al., 2016). In this work, we
study the problem of embedding textual relations,
defined as the shortest dependency path1 between
two entities in the dependency graph of a sentence,
to improve relation extraction.

Textual relations are one of the most discrimina-
tive textual signals that lay the foundation of many

∗∗ Equally contributed.
1We use fully lexicalized shortest dependency path with

directional and typed dependency relations.

relation extraction models (Bunescu and Mooney,
2005). A number of recent studies have explored
textual relation embedding under the supervised
setting (Xu et al., 2015a,b, 2016; Liu et al., 2016),
but the reliance on supervised training data lim-
its their scalability. In contrast, we embed textual
relations with distant supervision (Mintz et al.,
2009), which provides much larger-scale training
data without the need of manual annotation. How-
ever, the assertion of distant supervision, “any sen-
tence containing a pair of entities that participate
in a knowledge base (KB) relation is likely to ex-
press the relation,” can be violated more often than
not, resulting in many wrongly labeled training ex-
amples. A representative example is shown in Fig-
ure 1. Embedding quality is thus compromised by
the noise in training data.

Our main contribution is a novel way to com-
bat the wrong labeling problem of distant supervi-
sion. Traditional embedding methods (Xu et al.,
2015a,b, 2016; Liu et al., 2016) are based on lo-
cal statistics, i.e., individual textual-KB relation
pairs like in Figure 1 (Left). Our key hypothesis is
that global statistics is more robust to noise than
local statistics. For individual examples, the rela-
tion label from distant supervision may be wrong
from time to time. But when we zoom out to
consider the entire corpus, and collect the global
co-occurrence statistics of textual and KB rela-
tions, we will have a more comprehensive view
of relation semantics: The semantics of a tex-
tual relation can then be represented by its co-
occurrence distribution of KB relations. For ex-
ample, the distribution in Figure 1 (Right) indi-
cates that the textual relation SUBJECT nsubjpass←−−−− born
nmod:in−−−−→ OBJECT mostly means place of birth, and
is also a good indicator of nationality, but not
place of death. Although it is still wrongly la-
beled with place of death a number of times, the
negative impact becomes negligible. Similarly,

820



Michael_Jackson        was        born        in        the        US

nsubjpass nmod:in

Michael_Jackson US

place_of_birth

place_of_death

Text Corpus Knowledge Base

Michael_Jackson        died      in        the        US

nsubj nmod:in

Michael_Jackson US

nsubjpass←−−−−− born nmod:in−−−−→ nsubj←−−− died nmod:in−−−−→
place of birth 1868 14
nationality 389 20
place of death 37 352
. . . . . . . . .

Figure 1: The wrong labeling problem of distant supervision, and how to combat it with global statistics. Left:
conventional distant supervision. Each of the textual relations will be labeled with both KB relations, while only
one is correct (blue and solid), and the other is wrong (red and dashed). Right: distant supervision with global
statistics. The two textual relations can be clearly distinguished by their co-occurrence distribution of KB relations.
Statistics are based on the annotated ClueWeb data released in (Toutanova et al., 2015).

we can confidently believe that SUBJECT nsubj←−− died
nmod:in−−−−→ OBJECT means place of death in spite of
the noise. Textual relation embedding learned on
such global statistics is thus more robust to the
noise introduced by the wrong labeling problem.

We augment existing relation extractions using
the learned textual relation embedding. On a pop-
ular dataset introduced by Riedel et al. (2010),
we show that a number of recent relation extrac-
tion models, which are based on local statistics,
can be greatly improved using our textual relation
embedding. Most remarkably, a new best perfor-
mance is achieved when augmenting the previous
best model with our relation embedding: The pre-
cision of the top 1,000 relational facts discovered
by the model is improved from 83.9% to 89.3%, a
33.5% decrease in error rate. The results suggest
that relation embedding with global statistics can
capture complementary information to existing lo-
cal statistics based models.

The rest of the paper is organized as follows. In
Section 2 we discuss related work. For the mod-
eling part, we first describe how to collect global
co-occurrence statistics of relations in Section 3,
then introduce a neural network based embedding
model in Section 4, and finally discuss how to
combine the learned textual relation embedding
with existing relation extraction models in Section
5. We empirically evaluate the proposed method
in Section 6, and conclude in Section 7.

2 Related Work

Relation extraction is an important task in infor-
mation extraction. Early relation extraction meth-
ods are mainly feature-based (Kambhatla, 2004;
Zhou et al., 2005), where features in various lev-
els, including POS tags, syntactic and dependency
parses, are integrated in a max entropy model.
With the popularity of kernel methods, a large
number of kernel-based relation extraction meth-

ods have been proposed (Zelenko et al., 2003; Cu-
lotta and Sorensen, 2004; Bunescu and Mooney,
2005; Zhang et al., 2006). The most related work
to ours is by Bunescu and Mooney (Bunescu and
Mooney, 2005), where the importance of shortest
dependency path for relation extraction is first val-
idated.

More recently, relation extraction research has
been revolving around neural network models,
which can alleviate the problem of exact feature
matching of previous methods and have shown
a remarkable success (e.g., (Socher et al., 2012;
Zeng et al., 2014)). Among those, the most related
are the ones embedding shortest dependency paths
with neural networks (Xu et al., 2015a,b, 2016;
Liu et al., 2016). For example, Xu et al. (2015b)
use a RNN with LSTM units to embed shortest
dependency paths without typed dependency rela-
tions, while a convolutional neural network is used
in (Xu et al., 2015a). However, they are all based
on the supervised setting with a limited scale. In
contrast, we embed textual relations with distant
supervision (Mintz et al., 2009), which provides
much larger-scale training data at a low cost.

Various efforts have been made to combat the
long-criticized wrong labeling problem of dis-
tant supervision. Riedel et al. (2010), Hoffmann
et al. (2011), and Surdeanu et al. (2012) have
attempted a multi-instance learning (Dietterich
et al., 1997) framework to soften the assumption
of distant supervision, but their models are still
feature-based. Zeng et al. (2015) combine multi-
instance learning with neural networks, with the
assumption that at least one of the contextual sen-
tences of an entity pair is expressing the target re-
lation, but this will lose useful information in the
neglected sentences. Instead, Lin et al. (2016)
use all the contextual sentences, and introduce
an attention mechanism to weight the contextual
sentences. Li et al. (2017) also use an attention

821



mechanism to weight contextual sentences, and in-
corporate additional entity description information
from knowledge bases. Luo et al. (2017) manage
to alleviate the negative impact of noise by mod-
eling and learning noise transition patterns from
data. Liu et al. (2017) propose to infer the true
label of a context sentence using a truth discov-
ery approach (Li et al., 2016). Wu et al. (2017)
incorporate adversarial training, i.e., injecting ran-
dom perturbations in training, to improve the ro-
bustness of relation extraction. Using PCNN+ATT
(Lin et al., 2016) as base model, they show that ad-
versarial training can improve its performance by a
good margin. However, the base model implemen-
tation used by them performed inferior to the one
in the original paper and in ours, and therefore the
results are not directly comparable. No prior study
has exploited global statistics to combat the wrong
labeling problem of distant supervision. Another
unique aspect of this work is that we focus on com-
pact textual relations, while previous studies along
this line have focused on whole sentences.

In universal schema (Riedel et al., 2013) for KB
completion and relation extraction as well as its
extensions (Toutanova et al., 2015; Verga et al.,
2016), a binary matrix is constructed from the
entire corpus, with entity pairs as rows and tex-
tual/KB relations as columns. A matrix entry is
1 if the relational fact is observed in training, and
0 otherwise. Embeddings of entity pairs and re-
lations, either directly or via neural networks, are
then learned on the matrix entries, which are still
individual relational facts, and the wrong label-
ing problem remains. Global co-occurrence fre-
quencies (see Figure 1 (Right)) are not taken into
account, which is the focus of this study. An-
other distinction is that our method directly mod-
els the association between textual and KB rela-
tions, while universal schema learns embedding
for shared entity pairs and use that as a bridge be-
tween the two types of relations. It is an interest-
ing venue for future research to comprehensively
compare these two modeling approaches.

3 Global Statistics of Relations

When using a corpus to train statistical models,
there are two levels of statistics to exploit: local
and global. Take word embedding as an exam-
ple. The skip-gram model (Mikolov et al., 2013)
is based on local statistics: During training, we
sweep through the corpus and slightly tune the

nsubjpass

SUBJECT born

nmod:in

OBJECT

nsubj

SUBJECT died

nmod:in

OBJECT

place_of_birth

place_of_death

... ...

0.73

0.89

Figure 2: Relation graph. The left node set is textual
relations, and the right node set is KB relations. The
raw co-occurrence counts are normalized such that the
KB relations corresponding to the same textual rela-
tion form a valid probability distribution. Edges are
colored by textual relation and weighted by normalized
co-occurrence statistics.

embedding model in each local window (e.g., 10
consecutive words). In contrast, in global statis-
tics based methods, exemplified by latent semantic
analysis (Deerwester et al., 1990) and GloVe (Pen-
nington et al., 2014), we process the entire cor-
pus to collect global statistics like word-word co-
occurrence counts, normalize the raw statistics,
and train an embedding model directly on the nor-
malized global statistics.

Most existing studies on relation extraction are
based on local statistics of relations, i.e., models
are trained on individual relation examples. In this
section, we describe how we collect global co-
occurrence statistics of textual and KB relations,
and how to normalize the raw statistics. By the
end of this section a bipartite relation graph like
Figure 2 will be constructed, with one node set
being textual relations T , and the other being KB
relations R. The edges are weighted by the nor-
malized co-occurrence statistics of relations.

3.1 Relation Graph Construction
Given a corpus and a KB, we first do entity link-
ing on each sentence, and do dependency pars-
ing if at least two entities are identified2. For
each entity pair (e, e′) in the sentence, we ex-
tract the fully lexicalized shortest dependency path
as a textual relation t, forming a relational fact
(e, t, e′). There are two outcomes from this step:
a set of textual relations T = {ti}, and the sup-
port S(ti) for each ti. The support of a textual
relation is a multiset containing the entity pairs of
the textual relation. The multiplicity of an entity
pair, mS(ti)(e, e

′), is the number of occurrences
of the corresponding relational fact (e, ti, e′) in

2In the experiments entity linking is assumed given, and
dependency parsing is done using Stanford Parser (Chen and
Manning, 2014) with universal dependencies.

822



the corpus. For example, if the support of ti is
S(ti) = {(e1, e′1) , (e1, e′1) , (e2, e′2) , . . . }, entity
pair (e1, e′1) has a multiplicity of 2 because the re-
lational fact (e1, ti, e′1) occur in two sentences. We
also get a set of KB relations R = {rj}, and the
support S(rj) of a KB relation rj is the set of en-
tity pairs having this relation in the KB, i.e., there
is a relational fact (e, rj , e′) in the KB. The num-
ber of co-occurrences of a textural relation ti and
a KB relation rj is

nij =
∑

(e,e′)∈S(rj)
mS(ti)(e, e

′), (1)

i.e., every occurrence of relational fact (e, ti, e′) is
counted as a co-occurrence of ti and rj if (e, e′) ∈
S(rj). A bipartite relation graph can then be con-
structed, with T and R as the node sets, and the
edge between ti and rj has weight nij (no edge if
nij = 0), which will be normalized later.

3.2 Normalization

The raw co-occurrence counts have a heavily
skewed distribution that spans several orders of
magnitude: A small portion of relation pairs co-
occur highly frequently, while most relation pairs
co-occur only a few times. For example, a textual
relation, SUBJECT nsubjpass←−−−− born nmod:in−−−−→ OBJECT, may
co-occur with the KB relation place of birth
thousands of times (e.g., “Michelle Obama was
born in Chicago”), while a synonymous but
slightly more compositional textual relation, SUB-
JECT

nsubjpass←−−−− born nmod:in−−−−→ city nmod:of−−−−→ OBJECT, may only
co-occur with the same KB relation a few times in
the entire corpus (e.g., “Michelle Obama was born
in the city of Chicago”). Learning directly on the
raw co-occurrence counts, an embedding model
may put a disproportionate amount of weight on
the most frequent relations, and may not learn well
on the majority of rarer relations. Proper normal-
ization is therefore necessary, which will encour-
age the embedding model to learn good embed-
ding not only for the most frequent relations, but
also for the rarer relations.

A number of normalization strategies have been
proposed in the context of word embedding, in-
cluding correlation- and entropy-based normal-
ization (Rohde et al., 2005), positive pointwise
mutual information (PPMI) (Bullinaria and Levy,
2007), and some square root type transforma-
tion (Lebret and Collobert, 2014). A shared goal
is to reduce the impact of the most frequent words,

e.g., “the” and “is,” which tend to be less informa-
tive for the purpose of embedding.

We have experimented with a number of nor-
malization strategies and found that the following
strategy works best for textual relation embedding:
For each textual relation, we normalize its co-
occurrence counts to form a probability distribu-
tion over KB relations. The new edge weights of
the relation graph thus become wij = p̃(rj |ti) =
nij/

∑
j′ nij′ . Every textual relation is now asso-

ciated with a set of edges whose weights sum up to
1. We also experimented with PPMI and smoothed
PPMI with α = 0.75 (Levy et al., 2015) that are
commonly used in word embedding. However, the
learned textual relation embedding turned out to
be not very helpful for relation extraction. One
possible reason is that PPMI (even the smoothed
version) gives inappropriately large weights to rare
relations (Levy et al., 2015). There are many tex-
tual relations that correspond to none of the tar-
get KB relations but are falsely labeled with some
KB relations a few times by distant supervision.
PPMI gives large weights to such falsely labeled
cases because it thinks these events have a chance
significantly higher than random.

4 Textual Relation Embedding

Next we discuss how to learn embedding of textual
relations based on the constructed relation graph.
We call our approach Global Relation Embedding
(GloRE) in light of global statistics of relations.

4.1 Embedding via RNN

Given the relation graph, a straightforward way of
relation embedding is matrix factorization, simi-
lar to latent semantic analysis (Deerwester et al.,
1990) for word embedding. However, textual re-
lations are different from words in that they are
sequences composed of words and typed depen-
dency relations. Therefore, we use recurrent neu-
ral networks (RNNs) for embedding, which re-
spect the compositionality of textual relations and
can learn the shared sub-structures of different tex-
tual relations (Toutanova et al., 2015). For the ex-
amples in Figure 1, an RNN can learn, from both
textual relations, that the shared dependency rela-
tion “nmod:in” is indicative of location modifiers.
It is worth noting that other models like convolu-
tional neural networks can also be used, but it is
not the focus of this paper to compare all the alter-
native embedding models; rather, we aim to show

823



-nsubjpass born nmod:in-nsubjpass born nmod:in <GO>

place_of_birth : 0.73

Figure 3: Embedding model. Left: A RNN with GRU
for embedding. Middle: embedding of textual relation.
Right: a separate GRU cell to map a textual relation
embedding to a probability distribution over KB rela-
tions.

the effectiveness of global statistics with a reason-
able embedding model.

For a textual relation, we first decompose it
into a sequence of tokens {x1, ..., xm}, which in-
cludes lexical words and directional dependency
relations. For example, the textual relation SUB-
JECT

nsubjpass←−−−− born nmod:in−−−−→ OBJECT is decomposed to a
sequence of three tokens {−nsubjpass, born, nmod:in},
where “−” represents a left arrow. Note that we
include directional dependency relations, because
both the relation type and the direction are critical
in determining the meaning of a textual relation.
For example, the dependency relation “nmod:in”
often indicates a location modifier and is thus
strongly associated with location-related KB re-
lations like place of birth. The direction also
plays an important role. Without knowing the di-
rection of the dependency relations, it is impossi-
ble to distinguish child of and parent of.

An RNN with gated recurrent units
(GRUs) (Cho et al., 2014) is then applied to
consecutively process the sequence as shown in
Figure 3. We have also explored more advanced
constructs like attention, but the results are simi-
lar, so we opt for a vanilla RNN in consideration
of model simplicity.

Let φ denote the function that maps a token xl
to a fixed-dimensional vector, the hidden state vec-
tors of the RNN are calculated recursively:

hl = GRU
(
φ(xl),hl−1

)
. (2)

GRU follows the definition in Cho et al. (2014).

4.2 Training Objective

We use global statistics in the relation graph to
train the embedding model. Specifically, we
model the semantics of a textual relation as its co-
occurrence distribution of KB relations, and learn
textual relation embedding to reconstruct the cor-
responding co-occurrence distributions.

We use a separate GRU cell followed by soft-
max to map a textual relation embedding to a
distribution over KB relations; the full model
thus resembles the sequence-to-sequence architec-
ture (Sutskever et al., 2014). Given a textual rela-
tion ti and its embedding hm, the predicted condi-
tional probability of a KB relation rj is thus:

p(rj |ti) = softmax(GRU(φ(<GO>),hm))j ,
(3)

where ()j denotes the j-th element of a vector, and
<GO>is a special token indicating the start of de-
coding. The training objective is to minimize

Θ =
1

|E|
∑

i,j:p̃(rj |ti)>0
(log p(rj |ti)− log p̃(rj |ti))2 ,

(4)
where E is the edge set of the relation graph. It
is modeled as a regression problem, similar to
GloVe (Pennington et al., 2014).

Baseline. We also define a baseline approach
where the unnormalized co-occurrence counts are
directly used. The objective is to maximize:

Θ′ =
1∑

i,j nij

∑

i,j:nij>0

nij log p(rj |ti). (5)

It also corresponds to local statistics based embed-
ding, i.e., when the embedding model is trained
on individual occurrences of relational facts with
distant supervision. Therefore, we call it Local
Relation Embedding (LoRE).

5 Augmenting Relation Extraction

Learned from global co-occurrence statistics of re-
lations, our approach provides semantic matching
information of textual and KB relations, which is
often complementary to the information captured
by existing relation extraction models. In this sec-
tion we discuss how to combine them together to
achieve better relation extraction performance.

We follow the setting of distantly supervised re-
lation extraction. Given a text corpus and a KB
with relation set R, the goal is to find new rela-
tional facts from the text corpus that are not al-
ready contained in the KB. More formally, for
each entity pair (e, e′) and a set of contextual sen-
tences C containing this entity pair, a relation ex-
traction model assigns a scoreE(z|C) to each can-
didate relational fact z = (e, r, e′), r ∈ R. On the

824



other hand, our textual relation embedding model
works on the sentence level. It assign a score
G(z|s) to each contextual sentence s in C as for
how well the textual relation t between the entity
pair in the sentence matches the KB relation r, i.e.,
G(z|s) = p(r|t). It poses a challenge to aggregate
the sentence-level scores to get a set-level score
G(z|C), which can be used to combine with the
original score E(z|C) to get a better evaluation of
the candidate relational fact.

One straightforward aggregation is max
pooling, i.e., only using the largest score
maxs∈C G(z|s), similar to the at-least-one
strategy used by Zeng et al. (2015). But it will
lose the useful signals from those neglected
sentences (Lin et al., 2016). Because of the wrong
labeling problem, mean pooling is problematic as
well. The wrongly labeled contextual sentences
tend to make the aggregate scores more evenly
distributed and therefore become less informative.
The number of contextual sentences positively
supporting a relational fact is also an important
signal, but is lost in mean pooling.

Instead, we use summation with a trainable cap:

G(z|C) = min (cap,
∑

s∈C
G(z|s)), (6)

In other words, we additively aggregate the sig-
nals from all the contextual sentences, but only to
a bounded degree.

We simply use a weighted sum to combine
E(z|C) and G(z|C), where the trainable weights
will also handle the possibly different scale of
scores generated by different models:

Ẽ(z|C) = w1E(z|C) + w2G(z|C). (7)

The original score E(z|C) is then replaced by the
new score Ẽ(z|C). To find the optimal values for
w1, w2 and cap, we define a hinge loss:

ΘMerge =
1

K

K∑

k=1

max
{

0, 1 + Ẽ(z−k )− Ẽ(z+k )
}
,

(8)
where {z+k }Kk=1 are the true relational facts from
the KB, and {z−k }Kk=1 are false relational facts gen-
erated by replacing the KB relation in true rela-
tional facts with incorrect KB relations.

Data # of sentences # of entity pairs # of relational facts from KB

Train 570,088 291,699 19,429
Test 172,448 96,678 1,950

Table 1: Statistics of the NYT dataset.

6 Experiments

In this experimental study, we show that GloRE
can greatly improve the performance of several re-
cent relation extraction models, including the pre-
vious best model on a standard dataset.

6.1 Experimental Setup
Dataset. Following the literature (Hoffmann
et al., 2011; Surdeanu et al., 2012; Zeng et al.,
2015; Lin et al., 2016), we use the relation extrac-
tion dataset introduced in (Riedel et al., 2010),
which was generated by aligning New York Times
(NYT) articles with Freebase (Bollacker et al.,
2008). Articles from year 2005-2006 are used
as training, and articles from 2007 are used as
testing. Some statistics are listed in Table 1.
There are 53 target KB relations, including a
special relation NA indicating that there is no
target relation between entities.

We follow the approach described in Section 3
to construct the relation graph from the NYT train-
ing data. The constructed relation graph contains
321,447 edges with non-zero weight. We further
obtain a training set and a validation set from the
edges of the relation graph. We have observed
that using a validation set totally disjoint from the
training set leads to unstable validation loss, so we
randomly sample 300K edges as the training set,
and another 60K as the validation set. The two sets
can have some overlap. For the merging model
(Eq. 8), 10% of the edges are reserved as the vali-
dation set.

Relation extraction models. We evaluate with
four recent relation extraction models whose
source code is publicly available3. We use the op-
timized parameters provided by the authors.

• CNN+ONE and PCNN+ONE (Zeng et al.,
2015): A convolutional neural network
(CNN) is used to embed contextual sentences
for relation classification. Multi-instance
learning with at-least-one (ONE) assumption
is used to combat the wrong labeling prob-
lem. In PCNN, piecewise max pooling is

3https://github.com/thunlp/NRE

825



0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40

Recall

0.4

0.5

0.6

0.7

0.8

0.9

1.0

P
re
ci
si
o
n

CNN+ATT

CNN+ATT+GloRE

0.00 0.05 0.10 0.15 0.20 0.25 0.30

Recall

0.4

0.5

0.6

0.7

0.8

0.9

1.0

P
re
ci
si
o
n

CNN+ONE

CNN+ONE+GloRE

0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40

Recall

0.4

0.5

0.6

0.7

0.8

0.9

1.0

P
re
ci
si
o
n

PCNN+ONE

PCNN+ONE+GloRE

Figure 4: Held-out evaluation: other base relation extraction models and the improved versions when augmented
with GloRE.

0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40

Recall

0.4

0.5

0.6

0.7

0.8

0.9

1.0

P
re
ci
si
o
n

CNN+ONE

CNN+ATT

PCNN+ONE

PCNN+ATT

PCNN+ATT+TM

PCNN+ATT+GloRE

Figure 5: Held-out evaluation: the previous best-
performing model can be further improved when aug-
mented with GloRE. PCNN+ATT+TM is a recent
model (Luo et al., 2017) whose performance is slightly
inferior to PCNN+ATT. Because the source code is
not available, we did not experiment to augment this
model with GloRE. Another recent method (Wu et al.,
2017) incorporates adversarial training to improve
PCNN+ATT, but the results are not directly comparable
(see Section 2 for discussion). Finally, Ji et al. (2017)
propose a model similar to PCNN+ATT, but the perfor-
mance is inferior to PCNN+ATT and is not shown here
for clarity.

used to handle the three pieces of a contextual
sentence (split by the two entities) separately.

• CNN+ATT and PCNN+ATT (Lin et al.,
2016): Different from the at-least-one as-
sumption which loses information in the ne-
glected sentences, these models learn soft at-
tention weights (ATT) over contextual sen-
tences and thus can use the information of all
the contextual sentences. PCNN+ATT is the
best-performing model on the NYT dataset.

Evaluation settings and metrics. Similar to pre-
vious work (Riedel et al., 2010; Zeng et al., 2015),
we use two settings for evaluation: (1) Held-out
evaluation, where a subset of relational facts in
KB is held out from training (Table 1), and is later
used to compare against newly discovered rela-

0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40

Recall

0.4

0.5

0.6

0.7

0.8

0.9

1.0

P
re

ci
si
o
n

BASE

BASE+(CNN+ONE)

BASE+(CNN+ATT)

BASE+(PCNN+ONE)

BASE+GloRE

Figure 6: Held-out evaluation: GloRE brings the
largest improvement to BASE (PCNN+ATT), which
further shows that GloRE captures useful information
for relation extraction that is complementary to existing
models.

tional facts. This setting avoids human labor but
can introduce some false negatives because of the
incompleteness of the KB. (2) Manual evaluation,
where the discovered relational facts are manually
judged by human experts. For held-out evaluation,
we report the precision-recall curve. For manual
evaluation, we report Precision@N , i.e., the pre-
cision of the top N discovered relational facts.

Implementation. Hyper-parameters of our model
are selected based on the validation set. For the
embedding model, the mini-batch size is set to
128, and the state size of the GRU cells is 300.
For the merging model, the mini-batch size is set
to 1024. We use Adam with parameters recom-
mended by the authors for optimization. Word em-
beddings are initialized with the 300-dimensional
word2vec vectors pre-trained on the Google News
corpus4. Early stopping based on the validation
set is employed. Our model is implemented us-
ing Tensorflow (Abadi et al., 2016), and the source
code is available at https://github.com/
ppuliu/GloRE.

4https://code.google.com/archive/p/
word2vec/

826



0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40

Recall

0.4

0.5

0.6

0.7

0.8

0.9

1.0

P
re
ci
si
o
n

PCNN+ATT

PCNN+ATT+LoRE

PCNN+ATT+GloRE

Figure 7: Held-out evaluation: LoRE vs. GloRE.

6.2 Held-out Evaluation

Existing Models + GloRE. We first show that
our approach, GloRE, can improve the perfor-
mance of the previous best-performing model,
PCNN+ATT, leading to a new state of the art
on the NYT dataset. As shown in Figure 5,
when PCNN+ATT is augmented with GloRE, a
consistent improvement along the precision-recall
curve is observed. It is worth noting that al-
though PCNN+ATT+GloRE seems to be inferior
to PCNN+ATT when recall < 0.05, as we will
show via manual evaluation, it is actually due to
false negatives.

We also show in Figure 4 that the improvement
brought by GloRE is general and not specific to
PCNN+ATT; the other models also get a consis-
tent improvement when augmented with GloRE.

To investigate whether the improvement
brought by GloRE is simply from ensemble, we
also augment PCNN+ATT with the other three
base models in the same way as described in
Section 5. The results in Figure 6 show that
pairwise ensemble of existing relation extraction
models does not yield much improvement, and
GloRE brings much larger improvement than the
other models.

In summary, the held-out evaluation results sug-
gest that GloRE captures useful information for re-
lation extraction that is not captured by these local
statistics based models.

LoRE v.s. GloRE. We compare GloRE with the
baseline approach LoRE (Section 4) to show the
advantage of normalization on global statistics.
We use PCNN+ATT as the base relation extrac-
tion model. As shown in Figure 7, GloRE consis-
tently outperforms LoRE. It is worth noting that
LoRE can still improve the base relation extrac-
tion model when recall > 0.15, further confirming

Precision@N 100 300 500 700 900 1000

PCNN+ATT 97.0 93.7 92.8 89.1 85.2 83.9
PCNN+ATT+LoRE 97.0 95.0 94.2 91.6 89.6 87.0
PCNN+ATT+GloRE 97.0 97.3 94.6 93.3 90.1 89.3

Table 2: Manual evaluation: false negatives from held-
out evaluation are manually corrected by human ex-
perts.

the usefulness of directly embedding textual rela-
tions in addition to sentences.

6.3 Manual Evaluation

Due to the incompleteness of the knowledge base,
held-out evaluation introduces some false nega-
tives. The precision from held-out evaluation is
therefore a lower bound of the true precision.
To get a more accurate evaluation of model per-
formance, we have human experts to manually
check the false relational facts judged by held-
out evaluation in the top 1,000 predictions of
three models, PCNN+ATT, PCNN+ATT+LoRE
and PCNN+ATT+GloRE, and report the corrected
results in Table 2. Each prediction is examined
by two human experts who reach agreement with
discussion. To ensure fair comparison, the experts
are not aware of the provenance of the predictions.
Under manual evaluation, PCNN+ATT+GloRE
achieves the best performance in the full range of
N . In particular, for the top 1,000 predictions,
GloRE improves the precision of the previous best
model PCNN+ATT from 83.9% to 89.3%. The
manual evaluation results reinforce the previous
observations from held-out evaluation.

6.4 Case Study

Table 3 shows two examples. For better illustra-
tion, we choose entity pairs that have only one
contextual sentence.

For the first example, PCNN+ATT predicts that
most likely there is no KB relation between the
entity pair, while both LoRE and GloRE identify
the correct relation with high confidence. The tex-
tual relation clearly indicates that the head entity
is (appos) a criminologist at (nmod:at) the tail entity.

For the second example, there is no KB rela-
tion between the entity pair, and PCNN+ATT is
indeed able to rank NA at the top. However, it
is still quite confused by nationality, prob-
ably because it has learned that sentences about
a person and a country with many words about
profession (“poet,” “playwright,” and “novelist”)

827



Contextual Sentence Textual Relation PCNN+ATT Predictions LoRE Predictions GloRE Predictions

[Alfred Blumstein]head, a
criminologist at [Carnegie Mellon
University]tail, called . . .

appos←−−− criminologist
nmod:at−−−−→

NA (0.63) employee of (1.00) employee of (0.96)
employee of (0.36) NA (0.00) NA (0.02)
founder of (0.00) founder of (0.00) founder of (0.02)

[Langston Hughes]head, the
American poet, playwright and
novelist, came to [Spain]tail to . . .

-nsubj←−−− came to−→
NA (0.58) place of death (0.35) NA (0.73)
nationality (0.38) NA (0.33) contain location (0.07)
place lived (0.01) nationality (0.21) employee of (0.06)

Table 3: Case studies. We select entity pairs that have only one contextual sentence, and the head and tail entities
are marked. The top 3 predictions from each model with the associated probabilities are listed, with the correct
relation bold-faced.

likely express the person’s nationality. As a re-
sult, its prediction on NA is not very confident.
On the other hand, GloRE learns that if a person
“came to” a place, likely it is not his/her birth-
place. In the training data, due to the wrong label-
ing problem of distant supervision, the textual re-
lation is wrongly labeled with place of death
and nationality a couple of times, and both
PCNN+ATT and LoRE suffer from the train-
ing noise. Taking advantage of global statistics,
GloRE is more robust to such noise introduced by
the wrong labeling problem.

7 Conclusion

Our results show that textual relation embedding
trained on global co-occurrence statistics captures
useful relational information that is often comple-
mentary to existing methods. As a result, it can
greatly improve existing relation extraction mod-
els. Large-scale training data of embedding can be
easily solicited from distant supervision, and the
global statistics of relations provide a natural way
to combat the wrong labeling problem of distant
supervision.

The idea of relation embedding based on global
statistics can be further expanded along several di-
rections. In this work we have focused on embed-
ding textual relations, but it is in principle bene-
ficial to jointly embed knowledge base relations
and optionally entities. Recently a joint embed-
ding approach has been attempted in the context
of knowledge base completion (Toutanova et al.,
2015), but it is still based on local statistics, i.e.,
individual relational facts. Joint embedding with
global statistics remains an open problem. Com-
pared with the size of the training corpora for word
embedding (up to hundred of billions of tokens),
the NYT dataset is quite small in scale. Another
interesting venue for future research is to construct
much larger-scale distant supervision datasets to
train general-purpose textual relation embedding

that can help a wide range of downstream rela-
tional tasks such as question answering and textual
entailment.

Acknowledgements

The authors would like to thank the anonymous
reviewers for their thoughtful comments. This re-
search was sponsored in part by the Army Re-
search Laboratory under cooperative agreements
W911NF09-2-0053 and NSF IIS 1528175. The
views and conclusions contained herein are those
of the authors and should not be interpreted as rep-
resenting the official policies, either expressed or
implied, of the Army Research Laboratory or the
U.S. Government. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernment purposes notwithstanding any copyright
notice herein.

References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eu-

gene Brevdo, Zhifeng Chen, Craig Citro, Greg S
Corrado, Andy Davis, Jeffrey Dean, Matthieu
Devin, et al. 2016. Tensorflow: Large-scale ma-
chine learning on heterogeneous distributed sys-
tems. arXiv:1603.04467 .

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the ACM SIG-
MOD International conference on Management of
data. ACM, pages 1247–1250.

John A Bullinaria and Joseph P Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior re-
search methods 39(3):510–526.

Razvan C Bunescu and Raymond J Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics, pages 724–731.

828



Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics, pages 740–750.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv:1406.1078
.

Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.

Scott Deerwester, Susan T Dumais, George W Furnas,
Thomas K Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American society for information science 41(6):391.

Thomas G Dietterich, Richard H Lathrop, and Tomás
Lozano-Pérez. 1997. Solving the multiple instance
problem with axis-parallel rectangles. Artificial in-
telligence 89(1):31–71.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics. Association for Computational Linguis-
tics, pages 541–550.

Guoliang Ji, Kang Liu, Shizhu He, Jun Zhao, et al.
2017. Distant supervision for relation extraction
with sentence-level attention and entity descriptions.
In Proceedings of the AAAI Conference on Artificial
Intelligence.

Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy mod-
els for extracting relations. In Proceedings of the
ACL on Interactive poster and demonstration ses-
sions. Association for Computational Linguistics.

Rémi Lebret and Ronan Collobert. 2014. Word embed-
dings through hellinger PCA. European Chapter of
the Association for Computational Linguistics .

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics 3:211–225.

Yaliang Li, Jing Gao, Chuishi Meng, Qi Li, Lu Su,
Bo Zhao, Wei Fan, and Jiawei Han. 2016. A sur-
vey on truth discovery. Acm Sigkdd Explorations
Newsletter 17(2):1–16.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extraction

with selective attention over instances. In Proceed-
ings of the Annual Meeting of the Association for
Computational Linguistics. Association for Compu-
tational Linguistics, pages 2124–2133.

Liyuan Liu, Xiang Ren, Qi Zhu, Shi Zhi, Huan Gui,
Heng Ji, and Jiawei Han. 2017. Heterogeneous su-
pervision for relation extraction: A representation
learning approach. In Proceedings of Conference on
Empirical Methods in Natural Language Process-
ing.

Yang Liu, Sujian Li, Furu Wei, and Heng Ji. 2016.
Relation classification via modeling augmented de-
pendency paths. IEEE/ACM Transactions on Au-
dio, Speech and Language Processing (TASLP)
24(9):1585–1594.

Bingfeng Luo, Yansong Feng, Zheng Wang, Zhanx-
ing Zhu, Songfang Huang, Rui Yan, and Dongyan
Zhao. 2017. Learning with noise: Enhance distantly
supervised relation extraction with dynamic transi-
tion matrix. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguis-
tics. pages 430–439.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality. In Proceedings of the Annual Conference
on Neural Information Processing Systems. pages
3111–3119.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics. Association for Computational Linguis-
tics, pages 1003–1011.

Jeffrey Pennington, Richard Socher, and Christo-
pher D Manning. 2014. GloVe: Global vectors for
word representation. In Proceedings of Conference
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics,
pages 1532–1543.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Joint European Conference
on Machine Learning and Knowledge Discovery in
Databases. Springer, pages 148–163.

Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of the Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.

Douglas LT Rohde, Laura M Gonnerman, and David C
Plaut. 2005. An improved model of semantic simi-
larity based on lexical co-occurrence. Communica-
tions of the ACM 8:627–633.

829



Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning. Association
for Computational Linguistics, pages 1201–1211.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallap-
ati, and Christopher D Manning. 2012. Multi-
instance multi-label learning for relation extraction.
In Proceedings of Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 455–465.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of the Annual Conference
on Neural Information Processing Systems. pages
3104–3112.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-
fung Poon, Pallavi Choudhury, and Michael Gamon.
2015. Representing text for joint embedding of text
and knowledge bases. In Proceedings of Conference
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics,
pages 1499–1509.

Patrick Verga, David Belanger, Emma Strubell, Ben-
jamin Roth, and Andrew McCallum. 2016. Multi-
lingual relation extraction using compositional uni-
versal schema. In Proceedings of the Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics.

Yi Wu, David Bamman, and Stuart Russell. 2017. Ad-
versarial training for relation extraction. In Proceed-
ings of Conference on Empirical Methods in Natural
Language Processing.

Kun Xu, Yansong Feng, Songfang Huang, and
Dongyan Zhao. 2015a. Semantic relation classifica-
tion via convolutional neural networks with simple
negative sampling. arXiv:1506.07650 .

Yan Xu, Ran Jia, Lili Mou, Ge Li, Yunchuan Chen,
Yangyang Lu, and Zhi Jin. 2016. Improved relation
classification by deep recurrent neural networks with
data augmentation. arXiv:1601.03651 .

Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015b. Classifying relations via long
short term memory networks along shortest depen-
dency paths. In Proceedings of Conference on Em-
pirical Methods in Natural Language Processing.
Association for Computational Linguistics, pages
1785–1794.

Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. Journal of machine learning research
3(Feb):1083–1106.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction
via piecewise convolutional neural networks. In
Proceedings of Conference on Empirical Methods
in Natural Language Processing. Association for
Computational Linguistics, pages 1753–1762.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
Jun Zhao, et al. 2014. Relation classification via
convolutional deep neural network. In Proceedings
of the International Conference on Computational
Linguistics. pages 2335–2344.

Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring
syntactic features for relation extraction using a con-
volution tree kernel. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics. Association
for Computational Linguistics, pages 288–295.

Zhou Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005.
Exploring various knowledge in relation extraction.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics. Association for
Computational Linguistics, pages 427–434.

830


