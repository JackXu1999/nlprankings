



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 985–995
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1091

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 985–995
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1091

Argument Mining with Structured SVMs and RNNs

Vlad Niculae
Cornell University

vlad@cs.cornell.edu

Joonsuk Park
Williams College

jpark@cs.williams.edu

Claire Cardie
Cornell University

cardie@cs.cornell.edu

Abstract

We propose a novel factor graph model
for argument mining, designed for settings
in which the argumentative relations in a
document do not necessarily form a tree
structure. (This is the case in over 20%
of the web comments dataset we release.)
Our model jointly learns elementary unit
type classification and argumentative re-
lation prediction. Moreover, our model
supports SVM and RNN parametrizations,
can enforce structure constraints (e.g.,
transitivity), and can express dependencies
between adjacent relations and proposi-
tions. Our approaches outperform unstruc-
tured baselines in both web comments and
argumentative essay datasets.

1 Introduction

Argument mining consists of the automatic identi-
fication of argumentative structures in documents,
a valuable task with applications in policy mak-
ing, summarization, and education, among others.
The argument mining task includes the tightly-knit
subproblems of classifying propositions into ele-
mentary unit types and detecting argumentative re-
lations between the elementary units. The desired
output is a document argumentation graph struc-
ture, such as the one in Figure 1, where proposi-
tions are denoted by letter subscripts, and the asso-
ciated argumentation graph shows their types and
support relations between them.

Most annotation and prediction efforts in ar-
gument mining have focused on tree or forest
structures (Peldszus and Stede, 2015; Stab and
Gurevych, 2016), constraining argument struc-
tures to form one or more trees. This makes the
problem computationally easier by enabling the
use of maximum spanning tree–style parsing ap-

[ Calling a debtor at work is counter-intuitive; ]a
[ if collectors are continuously calling someone
at work, other employees may report it to the
debtor’s supervisor. ]b [ Most companies have es-
tablished rules about receiving or making per-
sonal calls during working hours. ]c [ If a col-
lector or creditor calls a debtor on his/her cell
phone and is informed that the debtor is at work,
the call should be terminated. ]d [ No calls to em-
ployers should be allowed, ]e [ as this jeopardizes
the debtor’s job. ]f

b (VALUE)

a (VALUE)

d (POLICY)

c (FACT) f (VALUE)

e (POLICY)

Figure 1: Example annotated CDCP comment.1

proaches. However, argumentation in the wild can
be less well-formed. The argument put forth in
Figure 1, for instance, consists of two compo-
nents: a simple tree structure and a more com-
plex graph structure (c jointly supports b and d).
In this work, we design a flexible and highly
expressive structured prediction model for argu-
ment mining, jointly learning to classify elemen-
tary units (henceforth propositions) and to identify
the argumentative relations between them (hence-
forth links). By formulating argument mining as
inference in a factor graph (Kschischang et al.,
2001), our model (described in Section 4) can ac-
count for correlations between the two tasks, can
consider second order link structures (e.g., in Fig-
ure 1, c → b → a), and can impose arbitrary con-
straints (e.g., transitivity).

To parametrize our models, we evaluate two
alternative directions: linear structured SVMs

1We describe proposition types (FACT, etc.) in Section 3.

985

https://doi.org/10.18653/v1/P17-1091
https://doi.org/10.18653/v1/P17-1091


(Tsochantaridis et al., 2005), and recurrent neural
networks with structured loss, extending (Kiper-
wasser and Goldberg, 2016). Interestingly, RNNs
perform poorly when trained with classification
losses, but become competitive with the feature-
engineered structured SVMs when trained within
our proposed structured learning model.

We evaluate our approach on two argument
mining datasets. Firstly, on our new Cornell
eRulemaking Corpus – CDCP,2 consisting of ar-
gument annotations on comments from an eRule-
making discussion forum, where links don’t al-
ways form trees (Figure 1 shows an abridged
example comment, and Section 3 describes the
dataset in more detail). Secondly, on the UKP ar-
gumentative essays v2 (henceforth UKP), where
argument graphs are annotated strictly as multiple
trees (Stab and Gurevych, 2016). In both cases,
the results presented in Section 5 confirm that our
models outperform unstructured baselines. On
UKP, we improve link prediction over the best re-
ported result in (Stab and Gurevych, 2016), which
is based on integer linear programming postpro-
cessing. For insight into the strengths and weak-
nesses of the proposed models, as well as into the
differences between SVM and RNN parameteriza-
tions, we perform an error analysis in Section 5.1.
To support argument mining research, we also re-
lease our Python implementation, Marseille.3

2 Related work

Our factor graph formulation draws from ideas
previously used independently in parsing and ar-
gument mining. In particular, maximum spanning
tree (MST) methods for arc-factored dependency
parsing have been successfully used by McDon-
ald et al. (2005) and applied to argument min-
ing with mixed results by Peldszus and Stede
(2015). As they are not designed for the task, MST
parsers cannot directly handle proposition classifi-
cation or model the correlation between proposi-
tion and link prediction—a limitation our model
addresses. Using RNN features in an MST parser
with a structured loss was proposed by Kiper-
wasser and Goldberg (2016); their model can be
seen as a particular case of our factor graph ap-
proach, limited to link prediction with a tree struc-
ture constraint. Our models support multi-task
learning for proposition classification, parameter-

2Dataset available at http://joonsuk.org.
3Available at https://github.com/vene/marseille.

izing adjacent links with higher-order structures
(e.g., c → b → a) and enforcing arbitrary con-
straints on the link structure, not limited to trees.
Such higher-order structures and logic constraints
have been successfully used for dependency and
semantic parsing by Martins et al. (2013) and Mar-
tins and Almeida (2014); to our knowledge we are
the first to apply them to argument mining, as well
as the first to parametrize them with neural net-
works. Stab and Gurevych (2016) used an inte-
ger linear program to combine the output of in-
dependent proposition and link classifiers using a
hand-crafted scoring formula, an approach simi-
lar to our baseline. Our factor graph method can
combine the two tasks in a more principled way,
as it fully learns the correlation between the two
tasks without relying on hand-crafted scoring, and
therefore can readily be applied to other argumen-
tation datasets. Furthermore, our model can en-
force the tree structure constraint, required on the
UKP dataset, using MST cycle constraints used by
Stab and Gurevych (2016), thanks to the AD3 in-
ference algorithm (Martins et al., 2015).

Sequence tagging has been applied to the re-
lated structured tasks of proposition identifica-
tion and classification (Stab and Gurevych, 2016;
Habernal and Gurevych, 2016; Park et al., 2015b);
integrating such models is an important next step.
Meanwhile, a new direction in argument mining
explores pointer networks (Potash et al., 2016); a
promising method, currently lacking support for
tree structures and domain-specific constraints.

3 Data

We release a new argument mining dataset consist-
ing of user comments about rule proposals regard-
ing Consumer Debt Collection Practices (CDCP)
by the Consumer Financial Protection Bureau col-
lected from an eRulemaking website, http://
regulationroom.org.

Argumentation structures found in web discus-
sion forums, such as the eRulemaking one we use,
can be more free-form than the ones encountered
in controlled, elicited writing such as (Peldszus
and Stede, 2015). For this reason, we adopt the
model proposed by Park et al. (2015a), which does
not constrain links to form tree structures, but un-
restricted directed graphs. Indeed, over 20% of
the comments in our dataset exhibit local struc-
tures that would not be allowable in a tree. Possi-
ble link types are reason and evidence, and propo-

986



sition types are split into five fine-grained cate-
gories: POLICY and VALUE contain subjective judge-
ments/interpretations, where only the former spec-
ifies a specific course of action to be taken. On
the other hand, TESTIMONY and FACT do not con-
tain subjective expressions, the former being about
personal experience, or “anecdotal.” Lastly, REFER-
ENCE covers URLs and citations, which are used to
point to objective evidence in an online setting.

In comparison, the UKP dataset (Stab and
Gurevych, 2016) only makes the syntactic dis-
tinction between CLAIM, MAJOR CLAIM, and PREMISE
types, but it also includes attack links. The per-
missible link structure is stricter in UKP, with
links constrained in annotation to form one or
more disjoint directed trees within each paragraph.
Also, since web arguments are not necessarily
fully developed, our dataset has many argumenta-
tive propositions that are not in any argumentation
relations. In fact, it isn’t unusual for comments to
have no argumentative links at all: 28% of CDCP
comments have no links, unlike UKP, where all
essays have complete argument structures. Such
comments with no links make the problem harder,
emphasizing the importance of capturing the lack
of argumentative support, not only its presence.

3.1 Annotation results
Each user comment was annotated by two anno-
tators, who independently annotated the bound-
aries and types of propositions, as well as the links
among them.4 To produce the final corpus, a third
annotator manually resolved the conflicts,5 and
two automatic preprocessing steps were applied:
we take the link transitive closure, and we remove
a small number of nested propositions.6 The re-
sulting dataset contains 731 comments, consist-
ing of about 3800 sentences (≈4700 propositions)
and 88k words. Out of the 43k possible pairs of
propositions, links are present between only 1300
(roughly 3%). In comparison, UKP has fewer doc-
uments (402), but they are longer, with a total
of 7100 sentences (6100 propositions) and 147k

4The annotators used the GATE annotation tool (Cun-
ningham et al., 2011).

5Inter-annotator agreement is measured with Krippen-
dorf’s α (Krippendorff, 1980) with respect to elementary unit
type (α=64.8%) and links (α=44.1%). A separate paper de-
scribing the dataset is under preparation.

6When two propositions overlap, we keep the one that re-
sults in losing the fewest links. For generality, we release the
dataset without this preprocessing, and include code to repro-
duce it; we believe that handling nested argumentative units
is an important direction for further research.

words. Since UKP links only occur within the
same paragraph and propositions not connected to
the argument are removed in a preprocessing step,
link prediction is less imbalanced in UKP, with
3800 pairs of propositions being linked out of a to-
tal of 22k (17%). We reserve a test set of 150 doc-
uments (973 propositions, 272 links) from CDCP,
and use the provided 80-document test split from
UKP (1266 propositions, 809 links).

4 Structured learning
for argument mining

4.1 Preliminaries
Binary and multi-class classification have been ap-
plied with some success to proposition and link
prediction separately, but we seek a way to jointly
learn the argument mining problem at the docu-
ment level, to better model contextual dependen-
cies and constraints. We therefore turn to struc-
tured learning, a framework that provides the de-
sired level of expressivity.

In general, learning from a dataset of documents
xi ∈ X and their associated labels yi ∈ Y involves
seeking model parameters w that can “pick out”
the best label under a scoring function f :

ŷ := argmaxy∈Y f(x, y;w). (1)

Unlike classification or regression, where X is usu-
ally a feature space Rd and Y ⊆ R (e.g., we predict
an integer class index or a probability), in struc-
tured learning, more complex inputs and outputs
are allowed. This makes the argmax in Equa-
tion 1 impossible to evaluate by enumeration, so
it is desirable to find models that decompose over
smaller units and dependencies between them; for
instance, as factor graphs. In this section, we give
a factor graph description of our proposed struc-
tured model for argument mining.

4.2 Model description
An input document is a string of words with
proposition offsets delimited. We denote the
propositions in a document by {a, b, c, ...} and the
possible directed link between a and b as a → b.
The argument structure we seek to predict consists
of the type of each proposition ya ∈ P and a bi-
nary label for each link ya→b ∈ R = {on, off}.7

7For simplicity and comparability, we follow Stab and
Gurevych (2016) in using binary link labels even if links
could be of different types. This can be addressed in our
model by incorporating “labeled link” factors.

987



a b c

a→ b b→ ca→ c

a← b b← ca← c
(a) CDCP

a b c

a→ b b→ ca→ c

a← b b← ca← c
(b) UKP

Figure 2: Factor graphs for a document with three propositions (a, b, c) and the six possible edges be-
tween them, and some of the factors used, illustrating differences and similarities between our models
for the two datasets. Unary factors are light gray; compatibility factors are black. Factors not part of the
basic model have curved edges: higher-order factors are orange and on the right; link structure factors
are hollow, as that they don’t have any parameters. Strict constraint factors are omitted for simplicity.

The possible proposition types P differ for the two
datasets; such differences are documented in Ta-
ble 1. As we describe the variables and factors
constituting a document’s factor graph, we shall
refer to Figure 2 for illustration.

Unary potentials. Each proposition a and each
link a → b has a corresponding random variable
in the factor graph (the circles in Figure 2). To
encode the model’s belief in each possible value
for these variables, we parametrize the unary fac-
tors (gray boxes in Figure 2) with unary poten-
tials: φ(a) ∈ R|P| is a score of ya for each pos-
sible proposition type. Similarly, link unary po-
tentials φ(a → b) ∈ R|R| are scores for ya→b be-
ing on/off. Without any other factors, this would
amount to independent classifiers for each task.

Compatibility factors. For every possible link
a → b, the variables (a, b, a → b) are bound
by a dense factor scoring their joint assignment
(the black boxes in Figure 2). Such a factor
could automatically learn to encourage links from
compatible types (e.g., from TESTIMONY to POLICY)
or discourage links between less compatible ones
(e.g., from FACT to TESTIMONY). In the simplest
form, this factor would be parametrized as a ten-
sor T ∈ R|P|×|P|×|R|, with tijk retaining the score
of a source proposition of type i to be (k = on)
or not to be (k = off) in a link with a proposi-
tion of type j. For more flexibility, we parametrize
this factor with compatibility features depending

only on simple structure: tijk becomes a vector,
and the score of configuration (i, j, k) is given by
v⊤abtijk where vab consists of three binary features:

• bias: a constant value of 1, allowing T to
learn a base score for a label configuration
(i, j, k), as in the simple form above,

• adjacency: when there are no other proposi-
tions between the source and the target,

• order: when the source precedes the target.

Second order factors. Local argumentation
graph structures such as a → b → c might be
modeled better together rather than through sep-
arate link factors for a → b and b → c. As in
higher-order structured models for semantic and
dependency parsing (Martins et al., 2013; Martins
and Almeida, 2014), we implement three types of
second order factors: grandparent (a → b → c),
sibling (a ← b → c), and co-parent (a → b ←
c). Not all of these types of factors make sense
on all datasets: as sibling structures cannot exist
in directed trees, we don’t use sibling factors on
UKP. On CDCP, by transitivity, every grandpar-
ent structure implies a corresponding sibling, so it
is sufficient to parametrize siblings. This differ-
ence between datasets is emphasized in Figure 2,
where one example of each type of factor is pic-
tured on the right side of the graphs (orange boxes
with curved edges): on CDCP we illustrate a co-
parent factor (top right) and a sibling factor (bot-

988



tom right), while on UKP we show a co-parent
factor (top right) and a grandparent factor (bottom
right). We call these factors second order because
they involve two link variables, scoring the joint
assignment of both links being on.

Valid link structure. The global structure of ar-
gument links can be further constrained using do-
main knowledge. We implement this using con-
straint factors; these have no parameters and are
denoted by empty boxes in Figure 2. In general,
well-formed arguments should be cycle-free. In
the UKP dataset, links form a directed forest and
can never cross paragraphs. This particular con-
straint can be expressed as a series of tree factors,8

one for each paragraph (the factor connected to all
link variables in Figure 2). In CDCP, links do not
form a tree, but we use logic constraints to enforce
transitivity (top left factor in Figure 2) and to pre-
vent symmetry (bottom left); the logic formulas
implemented by these factors are described in Ta-
ble 1. Together, the two constraints have the desir-
able side effect of preventing cycles.

Strict constraints. We may include further
domain-specific constraints into the model, to ex-
press certain disallowed configurations. For in-
stance, proposition types that appear in CDCP data
can be ordered by the level of objectivity (Park
et al., 2015a), as shown in Table 1. In a well-
formed argument, we would want to see links from
more objective to equally or less objective propo-
sitions: it’s fine to provide FACT as reason for
VALUE, but not the other way around. While the
training data sometimes violates this constraint,
enforcing it might provide a useful inductive bias.

Inference. The argmax in Equation 1 is a MAP
over a factor graph with cycles and many overlap-
ping factors, including logic factors. While ex-
act inference methods are generally unavailable,
our setting is perfectly suited for the Alternat-
ing Directions Dual Decomposition (AD3) algo-
rithm: approximate inference on expressive factor
graphs with overlapping factors, logic constraints,
and generic factors (e.g., directed tree factors) de-
fined through maximization oracles (Martins et al.,
2015). When AD3 returns an integral solution, it
is globally optimal, but when solutions are frac-

8A tree factor regards each bound variable as an edge in
a graph and assigns −∞ scores to configurations that are not
valid trees. For inference, we can use maximum spanning
arborescence algorithms such as Chu-Liu/Edmonds.

tional, several options are available. At test time,
for analysis, we retrieve exact solutions using the
branch-and-bound method. At training time, how-
ever, fractional solutions can be used as-is; this
makes better use of each iteration and actually in-
creases the ratio of integral solutions in future iter-
ations, as well as at test time, as proven by Meshi
et al. (2016). We also find that after around 15
training iterations with fractional solutions, over
99% of inference calls are integral.

Learning. We train the models by minimizing
the structured hinge loss (Taskar et al., 2004):

∑

(x,y)∈D
max
y′∈Y

(f(x, y′;w) + ρ(y, y′))− f(x, y;w)

(2)
where ρ is a configurable misclassification cost.
The max in Equation 2 is not the same as the one
used for prediction, in Equation 1. However, when
the cost function ρ decomposes over the variables,
cost-augmented inference amounts to regular in-
ference after augmenting the potentials accord-
ingly. We use a weighted Hamming cost:

ρ(y, ŷ) :=
∑

v

ρ(yv)I[yv = ŷv]

where v is summed over all variables in a docu-
ment {a} ∪ {a → b}, and ρ(yv) is a misclassifi-
cation cost. We assign uniform costs ρ to 1 for all
mistakes except false-negative links, where we use
higher cost proportional to the class imbalance in
the training split, effectively giving more weight
to positive links during training.

4.3 Argument structure SVM

One option for parameterizing the potentials of
the unary and higher-order factors is with linear
models, using proposition, link, and higher-order
features. This gives birth to a linear structured
SVM (Tsochantaridis et al., 2005), which, when
using l2 regularization, can be trained efficiently in
the dual using the online block-coordinate Frank-
Wolfe algorithm of Lacoste-Julien et al. (2013), as
implemented in the pystruct library (Müller and
Behnke, 2014). This algorithm is more convenient
than subgradient methods, as it does not require
tuning a learning rate parameter.

Features. For unary proposition and link fea-
tures, we faithfully follow Stab and Gurevych
(2016, Tables 9 and 10): proposition features are

989



Model part CDCP dataset UKP dataset

proposition types REFERENCE ≻ TESTIMONY ≻ FACT ≻ VALUE ≻ POLICY CLAIM, MAJOR CLAIM, PREMISE
links all possible within each paragraph

2nd order factors siblings, co-parents grandparents, co-parents
link structure transitive acyclic:

• a → b & b → c =⇒ a → c
• ATMOSTONE(a → b, b → a)

directed forest:
• TREEFACTOR over each paragraph
• zero-potential “root” links a → ∗

strict constraints link source must be as least as objective as the target:
a → b =⇒ a � b

link source must be premise:
a → b =⇒ a = PREMISE

Table 1: Instantiation of model design choices for each dataset.

lexical (unigrams and dependency tuples), struc-
tural (token statistics and proposition location),
indicators (from hand-crafted lexicons), contex-
tual, syntactic (subclauses, depth, tense, modal,
and POS), probability, discourse (Lin et al., 2014),
and average GloVe embeddings (Pennington et al.,
2014). Link features are lexical (unigrams), syn-
tactic (POS and productions), structural (token
statistics, proposition statistics and location fea-
tures), hand-crafted indicators, discourse triples,
PMI, and shared noun counts.

Our proposed higher-order factors for grandpar-
ent, co-parent, and sibling structures require fea-
tures extracted from a proposition triplet a, b, c.
In dependency and semantic parsing, higher-order
factors capture relationships between words, so
sparse indicator features can be efficiently used.
In our case, since propositions consist of many
words, BOW features may be too noisy and too
dense; so for simplicity we again take a cue
from the link-specific features used by Stab and
Gurevych (2016). Our higher-order factor fea-
tures are: same sentence indicators (for all 3 and
for each pair), proposition order (one for each of
the 6 possible orderings), Jaccard similarity (be-
tween all 3 and between each pair), presence of
any shared nouns (between all 3 and between each
pair), and shared noun ratios: nouns shared by
all 3 divided by total nouns in each proposition
and each pair, and shared nouns between each pair
with respect to each proposition. Up to vocabulary
size difference, our total feature dimensionality is
approximately 7000 for propositions and 2100 for
links. The number of second order features is 35.

Hyperparameters. We pick the SVM regular-
ization parameter C ∈ {0.001, 0.003, 0.01, 0.03,
0.1, 0.3} by k-fold cross validation at document
level, optimizing for the average between link and
proposition F1 scores.

4.4 Argument structure RNN
Neural network methods have proven effective for
natural language problems even with minimal-
to-no feature engineering. Inspired by the use
of LSTMs (Hochreiter and Schmidhuber, 1997)
for MST dependency parsing by Kiperwasser and
Goldberg (2016), we parametrize the potentials in
our factor graph with an LSTM-based neural net-
work,9 replacing MST inference with the more
general AD3 algorithm, and using relaxed solu-
tions for training when inference is inexact.

We extract embeddings of all words with a
corpus frequency > 1, initialized with GloVe
word vectors. We use a deep bidirectional LSTM
to encode contextual information, representing a
proposition a as the average of the LSTM outputs

of its words, henceforth denoted
↔
a.

Proposition potentials. We apply a multi-layer
perceptron (MLP) with rectified linear activations
to each proposition, with all layer dimensions
equal except the final output layer, which has size
|P| and is not passed through any nonlinearities.
Link potentials. To score a dependency a → b,
Kiperwasser and Goldberg (2016) pass the con-

catenation [
↔
a;

↔
b ] through an MLP. After trying

this, we found slightly better performance by first
passing each proposition through a slot-specific

dense layer
(
a := σsrc(

↔
a), b := σtrg(

↔
b)

)
fol-

lowed by a bilinear transformation:

φon(a→ b) := a⊤Wb+w⊤srca+w⊤trgb+ w(on)0 .
Since the bilinear expression returns a scalar, but
the link potentials must have a value for both the
on and off states, we set the full potential to
φ(a → b) := [φon(a → b), w(off)0 ] where w(off)0
is a learned scalar bias. We initialize W to the
diagonal identity matrix.

9We use the dynet library (Neubig et al., 2017).

990



Second order potentials. Grandparent poten-
tials φ(a → b → c) score two adjacent directed
edges, in other words three propositions. We again
first pass each proposition representation through
a slot-specific dense layer. We implement a multi-
linear scorer analogously to the link potentials:

φ(a→ b→ c) :=
∑

i,j,k

aibjckwijk

where W = (w)ijk is a third-order cube ten-
sor. To reduce the large numbers of parameters,
we implicitly represent W as a rank r tensor:
wijk =

∑r
s=1 u

(1)
is u

(2)
js u

(3)
ks . Notably, this model

captures only third-order interactions between the
representation of the three propositions. To cap-
ture first-order “bias” terms, we could include slot-
specific linear terms, e.g., w⊤a a; but to further
capture quadratic backoff effects (for instance, if
two propositions carry a strong signal of being
siblings regardless of their parent), we would re-
quire quadratically many parameters. Instead of
explicit lower-order terms, we propose augment-
ing a, b, and c with a constant feature of 1, which
has approximately the same effect, while benefit-
ing from the parameter sharing in the low-rank
factorization; an effect described by Blondel et al.
(2016). Siblings and co-parents factors are simi-
larly parametrized with their own tensors.

Hyperparameters. We perform grid search us-
ing k-fold document-level cross-validation, tun-
ing the dropout probability in the dense MLP lay-
ers over {0.05, 0.1, 0.15, 0.2, 0.25} and the opti-
mal number of passes over the training data over
{10, 25, 50, 75, 100}. We use 2 layers for the
LSTM and the proposition classifier, 128 hidden
units in all layers, and a multilinear decomposition
with rank r = 16, after preliminary CV runs.

4.5 Baseline models

We compare our proposed models to equivalent in-
dependent unary classifiers. The unary-only ver-
sion of a structured SVM is an l2-regularized lin-
ear SVM.10 For the RNN, we compute unary po-
tentials in the same way as in the structured model,
but apply independent hinge losses at each vari-
able, instead of the global structured hinge loss.
Since the RNN weights are shared, this is a form of
multi-task learning. The baseline predictions can

10We train our SVM using SAGA (Defazio et al., 2014) in
lightning (Blondel and Pedregosa, 2016).

be interpreted as unary potentials, therefore we
can simply round their output to the highest scor-
ing labels, or we can, alternatively, perform test-
time inference, imposing the desired structure.

5 Results

We evaluate our proposed models on both datasets.
For model selection and development we used k-
fold cross-validation at document level: on CDCP
we set k = 3 to avoid small validation folds, while
on UKP we follow Stab and Gurevych (2016) set-
ting k = 5. We compare our proposed structured
learning systems (the linear structured SVM and
the structured RNN) to the corresponding baseline
versions. We organize our experiments in three in-
cremental variants of our factor graph: basic, full,
and strict, each with the following components:11

component basic full strict (baseline)

unaries X X X X
compat. factors X X X
compat. features X X
higher-order X X
link structure X X X
strict constraints X X

Following Stab and Gurevych (2016), we compute
F1 scores at proposition and link level, and also
report their average as a summary of overall per-
formance.12 The results of a single prediction run
on the test set are displayed in Table 2. The over-
all trend is that training using a structured objec-
tive is better than the baseline models, even when
structured inference is applied on the baseline pre-
dictions. On UKP, for link prediction, the linear
baseline can reach good performance when us-
ing inference, similar to the approach of Stab and
Gurevych (2016), but the improvement in propo-
sition prediction leads to higher overall F1 for the
structured models. Meanwhile, on the more dif-
ficult CDCP setting, performing inference on the
baseline output is not competitive. While feature
engineering still outperforms our RNN model, we
find that RNNs shine on proposition classification,
especially on UKP, and that structured training can
make them competitive, reducing their observed
lag on link prediction (Katiyar and Cardie, 2016),
possibly through mitigating class imbalance.

11Components are described in Section 4. The baselines
with inference support only unaries and factors with no pa-
rameters, as indicated in the last column.

12For link F1 scores, however, we find it more intuitive
to only consider retrieval of positive links rather than macro-
averaged two-class scores.

991



Baseline Structured

SVM RNN SVM RNN

Metric basic full strict basic full strict basic full strict basic full strict

CDCP dataset
Average 47.4 47.3 47.9 40.8 38.0 38.0 48.1 49.3 50.0 43.5 33.5 38.2
Link (272) 22.0 21.9 23.8 9.9 12.8 12.8 24.7 25.1 26.7 14.4 14.6 10.5
Proposition 72.7 72.7 72.0 71.8 63.2 63.2 71.6 73.5 73.2 72.7 52.4 65.9

VALUE (491) 75.3 75.3 74.4 74.1 74.8 74.8 73.4 75.7 76.4 73.7 73.1 69.7
POLICY (153) 78.7 78.7 78.5 74.3 72.2 72.2 72.3 77.3 76.8 73.9 74.4 76.8
TESTIMONY (204) 70.3 70.3 68.6 74.6 71.8 71.8 69.8 71.7 71.5 74.2 72.3 75.8
FACT (124) 39.2 39.2 38.3 35.8 30.5 30.5 42.4 42.5 41.3 41.5 42.2 40.5
REFERENCE (1) 100.0 100.0 100.0 100.0 66.7 66.7 100.0 100.0 100.0 100.0 0.0 66.7

UKP dataset
Average 64.7 66.6 66.5 58.7 57.4 58.7 67.1 68.9 67.1 59.0 63.6 64.7
Link (809) 55.8 59.7 60.3 44.8 43.8 44.0 56.9 60.1 56.9 44.1 50.4 50.1
Proposition 73.5 73.5 72.6 72.6 70.9 73.3 77.2 77.6 77.3 74.0 76.9 79.3

MAJOR CLAIM (153) 76.7 76.7 77.6 81.4 75.1 81.3 77.0 78.2 80.0 83.6 84.6 88.3
CLAIM (304) 55.4 55.4 52.0 51.7 52.7 53.5 64.3 64.5 62.8 53.2 60.2 62.0
PREMISE (809) 88.4 88.4 88.3 84.8 84.8 85.2 90.3 90.2 89.2 85.0 85.9 87.6

Table 2: Test set F1 scores for link and proposition classification, as well as their average, on the two
datasets. The number of test instances is shown in parentheses; best scores on overall tasks are in bold.

5.1 Discussion and analysis
Contribution of compatibility features. The
compatibility factor in our model can be visual-
ized as conditional odds ratios given the source
and target proposition types. Since there are only
four possible configurations of the compatibility
features, we can plot all cases in Figure 3, along-
side the basic model. Not using compatibility
features, the basic model can only learn whether
certain configurations are more likely than others
(e.g. a REFERENCE supporting another REFERENCE
is unlikely, while a REFERENCE supporting a FACT
is more likely; essentially a soft version of our
domain-specific strict constraints. The full model
with compatibility features is finer grained, captur-
ing, for example, that links from REFERENCE to FACT
are more likely when the reference comes after,
or that links from VALUE to POLICY are extremely
likely only when the two are adjacent.

Proposition errors. The confusion matrices in
Figure 4 reveal that the most common confusion
is misclassifying FACT as VALUE. The strongest dif-
ference between the various models tested is that
the RNN-based models make this error less often.
For instance, in the proposition:

And the single most frequently used excuse
of any debtor is “I didn’t receive the let-
ter/invoice/statement”

the pronouns in the nested quote may be mistaken
for subjectivity, leading to the structured SVMs

predictions of VALUE or TESTIMONY, while the ba-
sic structured RNN correctly classifies it as FACT.

Link errors. While structured inference cer-
tainly helps baselines by preventing invalid struc-
tures such as cycles, it still depends on local deci-
sions, losing to fully structured training in cases
where joint proposition and link decisions are
needed. For instance, in the following conclusion
of an UKP essay, the annotators found no links:

In short, [ the individual should finance his
or her education ]a because [ it is a personal
choice. ]b Otherwise, [ it would cause too much
cost from taxpayers and the government. ]c

Indeed, no reasons are provided, but baseline are
misled by the connectives: the SVM baseline out-
puts that b and c are PREMISEs supporting the CLAIM
a. The full structured SVM combines the two
tasks and correctly recognizes the link structure.

Linear SVMs are still a very good baseline, but
they tend to overgenerate links due to class imbal-
ance, even if we use class weights during training.
Surprisingly, RNNs are at the opposite end, be-
ing extremely conservative, and getting the high-
est precision among the models. On CDCP, where
the number of true links is 272, the linear base-
line with strict inference predicts 796 links with a
precision of only 16%, while the strict structured
RNN only predicts 52 links, with 33% precision;
the example in Figure 5 illustrates this. In terms of
higher-order structures, we find that using higher-
order factors increases precision, at a cost in recall.

992



P V F T R
Target

Policy

Value

Fact

Testimony

Reference

So
ur

ce
-0.3 -0.1 -0.1 -0.2 -0.1

+0.1 -0.0 -0.1 -0.1 -0.2

-0.0 +0.0 -0.1 -0.2 -0.1

-0.2 -0.1 -0.1 -0.3 +0.1

-0.3 +0.0 +0.6 +0.1 -0.4

Non-adjacent,
trg precedes src

P V F T R
Target

-0.2 -0.3 -0.0 -0.2 -0.2

-0.4 -0.3 -0.2 -0.1 -0.3

-0.3 -0.1 +0.0 -0.0 -0.3

-0.3 -0.0 -0.1 +0.1 -0.2

-0.2 -0.0 +0.4 +0.1 -0.4

Non-adjacent,
src precedes trg

P V F T R
Target

+0.6 +0.9 +0.3 +0.1 -0.1

+2.2 +1.7 +1.0 +0.9 -0.1

+2.0 +1.7 +1.0 +0.6 -0.1

+1.5 +1.5 +0.9 +0.9 +0.1

-0.2 +0.1 +0.5 +0.1 -0.8

Adjacent,
trg precedes src

P V F T R
Target

+0.7 +0.7 +0.3 +0.1 -0.2

+1.7 +1.4 +0.9 +0.9 -0.2

+1.7 +1.5 +1.1 +0.7 -0.3

+1.4 +1.5 +0.9 +1.4 -0.1

-0.1 +0.1 +0.3 +0.1 -0.9

Adjacent,
src precedes trg

P V F T R
Target

-0.8 -0.7 -1.1 -1.0 -0.4

+0.9 +0.3 -0.5 -0.5 -0.5

+0.6 +0.6 -0.2 -0.5 -0.3

+0.1 +0.3 -0.3 -0.2 -0.1

-0.7 -0.0 +1.3 -0.0 -1.0

Basic (no
compatibility features)

Figure 3: Learned conditional log-odds log p(on|·)p(off|·) , given the source and target proposition types and
compatibility feature settings. First four figures correspond to the four possible settings of the compati-
bility features in the full structured SVM model. For comparison, the rightmost figure shows the same
parameters in the basic structured SVM model, which does not use compatibility features.

P V F T R
Predicted

P

V

F

T

R

Tr
ue

0.77 0.10 0.06 0.07 0.00

0.05 0.75 0.10 0.10 0.00

0.02 0.50 0.39 0.09 0.00

0.01 0.20 0.06 0.73 0.00

0.00 0.00 0.00 0.00 1.00

Baseline SVM basic

P V F T R
Predicted

0.76 0.16 0.05 0.04 0.00

0.05 0.76 0.11 0.08 0.00

0.04 0.42 0.44 0.10 0.00

0.01 0.21 0.06 0.72 0.00

0.00 0.00 0.00 0.00 1.00

Structured SVM full

P V F T R
Predicted

P

V

F

T

R

Tr
ue

0.72 0.14 0.12 0.02 0.00

0.04 0.74 0.15 0.07 0.00

0.06 0.48 0.40 0.06 0.00

0.02 0.20 0.06 0.72 0.00

0.00 0.00 0.00 0.00 1.00

Baseline RNN basic

P V F T R
Predicted

0.73 0.17 0.10 0.00 0.00

0.05 0.71 0.15 0.08 0.00

0.07 0.38 0.48 0.06 0.00

0.01 0.19 0.08 0.73 0.00

0.00 0.00 0.00 0.00 1.00

Structured RNN basic

Figure 4: Normalized confusion matrices for
proposition type classification.

This is most beneficial for the 856 co-parent struc-
tures in the UKP test set: the full structured SVM
has 53% F1, while the basic structured SVM and
the basic baseline get 47% and 45% respectively.
On CDCP, while higher-order factors help, perfor-
mance on siblings and co-parents is below 10% F1
score. This is likely due to link sparsity and sug-
gests plenty of room for further development.

6 Conclusions and future work

We introduce an argumentation parsing model
based on AD3 relaxed inference in expressive fac-
tor graphs, experimenting with both linear struc-

[ I think the cost of education needs to be re-
duced (...) or repayment plans need to be income
based. ]a [ As far as consumer protection, legal
aid needs to be made available, affordable and
effective, ]b [ and consumers need to take time
to really know their rights and stop complaining
about harassment ]c [ because that’s a completely
different cause of action than restitution. ]d

a (P)

c (P)

b (P) d (V)

(a) Ground truth

a (V)

c (V)

b (P)

d (V)

(b) Baseline linear strict

a (P)

c (V)

b (P)

d (V)

(c) Structured linear full

a (P)

c (P)

b (P) d (F)

(d) Structured RNN strict

Figure 5: Predictions on a CDCP comment where
the structured RNN outperforms the other models.

tured SVMs and structured RNNs, parametrized
with higher-order factors and link structure con-
straints. We demonstrate our model on a new
argumentation mining dataset with more permis-
sive argument structure annotation. Our model
also achieves state-of-the-art link prediction per-
formance on the UKP essays dataset.

Future work. Stab and Gurevych (2016) found
polynomial kernels useful for modeling feature
interactions, but kernel structured SVMs scale
poorly, we intend to investigate alternate ways
to capture feature interactions. While we focus
on monological argumentation, our model could
be extended to dialogs, for which argumentation
theory thoroughly motivates non-tree structures
(Afantenos and Asher, 2014).

993



Acknowledgements

We are grateful to André Martins, Andreas Müller,
Arzoo Katyiar, Chenhao Tan, Felix Wu, Jack Hes-
sel, Justine Zhang, Mathieu Blondel, Tianze Shi,
Tobias Schnabel, and the rest of the Cornell NLP
seminar for extremely helpful discussions. We
thank the anonymous reviewers for their thorough
and well-argued feedback.

References
Stergos Afantenos and Nicholas Asher. 2014. Counter-

argumentation and discourse: A case study. In Pro-
ceedings of ArgNLP.

Mathieu Blondel, Masakazu Ishihata, Akinori Fujino,
and Naonori Ueda. 2016. Polynomial networks and
factorization machines: New insights and efficient
training algorithms. In Proceedings of ICML.

Mathieu Blondel and Fabian Pedregosa. 2016.
Lightning: large-scale linear classifica-
tion, regression and ranking in Python.
https://doi.org/10.5281/zenodo.200504.

Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Niraj Aswani, Ian
Roberts, Genevieve Gorrell, Adam Funk, Angus
Roberts, Danica Damljanovic, Thomas Heitz,
Mark A. Greenwood, Horacio Saggion, Johann
Petrak, Yaoyong Li, and Wim Peters. 2011. Text
Processing with GATE (Version 6).

Aaron Defazio, Francis Bach, and Simon Lacoste-
Julien. 2014. SAGA: A fast incremental gradient
method with support for non-strongly convex com-
posite objectives. In Proceedings of NIPS.

Ivan Habernal and Iryna Gurevych. 2016. Argumenta-
tion mining in user-generated web discourse. Com-
putational Linguistics .

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation
9(8):1735–1780.

Arzoo Katiyar and Claire Cardie. 2016. Investigating
LSTMs for joint extraction of opinion entities and
relations. In Proceedings of ACL.

Eliyahu Kiperwasser and Yoav Goldberg. 2016.
Simple and accurate dependency parsing us-
ing bidirectional LSTM feature representations.
arXiv:1603.04351 preprint.

Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to Its Methodology. Commtext. Sage.

Frank R Kschischang, Brendan J Frey, and H-A
Loeliger. 2001. Factor graphs and the sum-product
algorithm. IEEE Transactions on Information The-
ory 47(2):498–519.

Simon Lacoste-Julien, Martin Jaggi, Mark Schmidt,
and Patrick Pletscher. 2013. Block-coordinate
Frank-Wolfe optimization for structural SVMs. In
Proceedings of ICML.

Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering 20(02):151–184.

André FT Martins and Mariana SC Almeida. 2014.
Priberam: A Turbo Semantic Parser with second or-
der features. In Proceedings of SemEval.

André FT Martins, Miguel B Almeida, and Noah A
Smith. 2013. Turning on the Turbo: Fast third-
order non-projective Turbo Parsers. In Proceedings
of ACL.

André FT Martins, Mário AT Figueiredo, Pedro MQ
Aguiar, Noah A Smith, and Eric P Xing. 2015.
AD3: Alternating directions dual decomposition for
MAP inference in graphical models. Journal of Ma-
chine Learning Research 16:495–545.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajič. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of EMNLP.

Ofer Meshi, Mehrdad Mahdavi, Adrian Weller, and
David Sontag. 2016. Train and test tightness of LP
relaxations in structured prediction. In Proceedings
of ICML.

Andreas C Müller and Sven Behnke. 2014. PyStruct:
learning structured prediction in Python. Journal of
Machine Learning Research 15(1):2055–2060.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng
Ji, Lingpeng Kong, Adhiguna Kuncoro, Gau-
rav Kumar, Chaitanya Malaviya, Paul Michel,
Yusuke Oda, Matthew Richardson, Naomi Saphra,
Swabha Swayamdipta, and Pengcheng Yin. 2017.
DyNet: The dynamic neural network toolkit.
arXiv:1701.03980 preprint.

Joonsuk Park, Cheryl Blake, and Claire Cardie. 2015a.
Toward machine-assisted participation in eRulemak-
ing: An argumentation model of evaluability. In
Proceedings of ICAIL.

Joonsuk Park, Arzoo Katiyar, and Bishan Yang. 2015b.
Conditional random fields for identifying appropri-
ate types of support for propositions in online user
comments. In Proceedings of the 2nd Workshop
on Argumentation Mining. Association for Compu-
tational Linguistics, Denver, CO, pages 39–44.

Andreas Peldszus and Manfred Stede. 2015. Joint pre-
diction in MST-style discourse parsing for argumen-
tation mining. In Proceedings of EMNLP.

994



Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of EMNLP.

Peter Potash, Alexey Romanov, and Anna Rumshisky.
2016. Here’s my point: Argumentation mining with
pointer networks. arXiv:1612.08994 preprint.

Christian Stab and Iryna Gurevych. 2016. Pars-
ing argumentation structures in persuasive essays.
arXiv:1604.07370 preprint.

Ben Taskar, Carlos Guestrin, and Daphne Koller. 2004.
Max-margin Markov networks. In Proceedings of
NIPS.

Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output
variables. Journal of Machine Learning Research
6(Sep):1453–1484.

995


	Argument Mining with Structured SVMs and RNNs

