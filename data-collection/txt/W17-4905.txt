



















































Is writing style predictive of scientific fraud?


Proceedings of the Workshop on Stylistic Variation, pages 37–42
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Is writing style predictive of scientific fraud?

Chloé Braud and Anders Søgaard
CoAStaL DIKU

University of Copenhagen
University Park 5, 2100 Copenhagen

chloe.braud@gmail.com soegaard@di.ku.dk

Abstract

The problem of detecting scientific fraud
using machine learning was recently intro-
duced, with initial, positive results from
a model taking into account various gen-
eral indicators. The results seem to sug-
gest that writing style is predictive of sci-
entific fraud. We revisit these initial ex-
periments, and show that the leave-one-out
testing procedure they used likely leads to
a slight over-estimate of the predictability,
but also that simple models can outper-
form their proposed model by some mar-
gin. We go on to explore more abstract
linguistic features, such as linguistic com-
plexity and discourse structure, only to ob-
tain negative results. Upon analyzing our
models, we do see some interesting pat-
terns, though: Scientific fraud, for exam-
ples, contains less comparison, as well as
different types of hedging and ways of pre-
senting logical reasoning.

1 Introduction

Cases of scientific misconduct are identified every
year. Scientific papers are retracted because of er-
rors, or for suspected fraud, ranging from plagia-
rism and minor manipulations to faking the data
and disguising the results. It has been shown that,
however, among the retracted articles indexed in
PubMed, only 21.3% are retracted due to error,
while 67.4% were removed due to misconduct,
among which suspected fraud amounts to 43.4%,
the others being due to duplicate publications or
plagiarism (Fang et al., 2012).

In a recent paper, Markowitz and Hancock
(2015) proposed the first analysis of writing style
in fraudulent papers across authors and disci-
plines. They approached the question of whether

these authors have a specific writing style, from a
psychological perspective. They found that these
papers exhibit a higher rate of jargon, make a
higher use of references, and have a lower read-
ability rate, suggesting that the authors try to ob-
fuscate their writing, making them harder to read
and analyze. They report classification results us-
ing a leave-one-out strategy over the dataset, with
a classification accuracy of 57.2%. As suggested
in the paper, we propose to improve this perfor-
mance by evaluating different classification mod-
els.

In this paper, we first show that much better re-
sults can be obtained using a simple bag-of-words
representation and Logistic Regression. Our best
model is a syntax-enhanced trigram-model. We
also show that the leave-one-out strategy used by
the authors leads to an over-estimation of model
precision, and we report new results based on a
more robust strategy, taking into account the low
number of instance available; namely a nested
cross-validation (Varma and Simon, 2006; Schef-
fer, 1999). We also considered semantic and dis-
course features, but we did not observe improve-
ments with such features.

Of course, that a bag-of-words model outper-
forms a model based on psychologically motivated
features, may simply be the result of overfitting.
We present an extensive feature analysis to vali-
date our models, as well as to test psychologically
motivated hypotheses from the literature.

Contributions (i) We present a simple model
with high accuracy, and show that it implicitly
captures the previously-proposed psychologically-
motivated features. (ii) We show that adding se-
mantics and discourse features does not lead to
improvements. (iii) On the other hand, our feature
analysis suggests that the models do learn to focus
on concepts that are intuitively related to scientific

37



misconduct, e.g., that scientific fraud contains less
comparison.

2 Related work

Markowitz and Hancock (2015) were the first to
study writing style in fraudulent papers. They
gathered a corpus of 253 articles indexed in
PubMed that have been retracted for fraudulent
data, as well as 253 unretracted papers (see Sec-
tion 3). They define five indicators of obfuscation,
and show that fraudulent papers tend to demon-
strate a higher rate of linguistics obfuscation, cor-
responding to a lower readability, an higher use of
jargon and a higher degree of abstraction. Linked
to studies on deception identification, they also re-
port a lower rate of positive emotion terms and a
higher rate of causal terms (e.g. “depend”, “in-
duce”, “manipulated”) in fraudulent papers. The
readability score was computed using Coh-Metrix
(McNamara et al., 2013), while the other scores
were based on the Linguistic Inquiry and Word
Count (LIWC; (Pennebaker et al., 2007)), a dic-
tionary associating a word to various scores such
as abstraction (a word is considered as jargon if
it is not found in the dictionary). Finally, they
report 57.2% in accuracy using these five indica-
tors as features, a score that we show is probably
a little too optimistic, since it is based on a leave-
one-out procedure (see Section 5). We extend their
work by first showing that a simple unigram model
outperforms their model by a large margin, but
also by considering more indicators, including dis-
course and syntax, and by showing, as mentioned,
that their scores were probably over-estimated due
to their validation strategy.

Our work is also inspired by another related
field of research concerned with deception detec-
tion. Mihalcea and Strapparava (2009) built three
datasets consisting of 100 true and 100 deceptive
short statements on three different topics (abor-
tion, death penalty, best friend). Using only un-
igrams, they report 70.8% accuracy in a 10-fold
cross validation. They found that specific word
classes, as defined in the LIWC, were predictive
of deceptive texts, especially classes indicating de-
tachment from self or related to certainty.

Feng et al. (2012a) investigate syntactic fea-
tures, using lexicalized and unlexicalized produc-
tion rules in addition to shallow features (words
unigram and bigram, and POS unigram). They
experiment on truthful and deceptive reviews

from TripAdvisor, either gold (Ott et al., 2011)
or retrieved using a fake review detector (Feng
et al., 2012b), reviews automatically extracted
from Yelp, and the corpus introduced in (Mihal-
cea and Strapparava, 2009). They report scores be-
tween 64.3 and 91.2% accuracy, depending on the
dataset. They found that, for all datasets, syntax
helps, and that deceptive reviews more frequently
use VP, SBAR and WHADVP.

We also consider n-gram features, syntactic fea-
tures, as well as discourse features. Our task is
however a bit different, since authors of fraudu-
lent papers are not directly lying, rather trying to
conceal their fraud. Moreover, our documents are
longer and are of a different genre, i.e. scientific
articles.

3 Data

We use the dataset proposed in (Markowitz and
Hancock, 2015) containing 253 publications re-
tracted for data fraud and 253 unretracted publi-
cations. These publications were taken from the
PubMed archives from 1973 through 2013.

The unretracted papers are extracted by consid-
ering one retracted paper and taking a control pa-
per published the same year, in the same journal,
and with some common keywords when possible.
When no such paper exists (around 19% of the pa-
pers), a paper from an adjacent year, or using the
same words in the abstract, was selected.

The data used is the pre-processed version pre-
sented in (Markowitz and Hancock, 2015): Words
were converted from British English to American
English forms. Brackets, parentheses, and percent
signs were removed. Periods were removed from
certains words, such as ‘Dr.’ or nc.’. The docu-
ments only contain the main body text (no section
titles, figures, or tables).

4 Methodology

We investigate different types of features, from n-
grams to discourse. In large vocabulary feature
spaces, we perform feature reduction, to reduce
sparsity. We then provide an analysis of the fea-
tures to identify the most informative indicators.

Word features We use word n-grams as fea-
tures, with n ∈ {1, 2, 3}. In order to test the
hypotheses presented in previous studies, we also
use lexicons to extract information about the to-
kens. We use the General Inquirer (Stone and

38



Kirsh, 1966) to extract words expressing a polarity
– the features built represent the polarity between
positive, negative, both and neutral –, and words
corresponding to a causal term. We also use this
lexicon to map the words to a more general seman-
tic category (Inquirer).

We identify all the personal pronouns using
manually defined lists. Finally, we also include
as features hedge and modal words, also using a
pre-defined list.1

Syntactic features In order to obtain syntactic
information, we parse the data using UDPipe2

(Straka et al., 2016), and a prebuilt model avail-
able online for English.3 We follow (Johannsen
et al., 2015) in extracting all subtrees of up to three
tokens (treelets).

Discourse features Finally, we automatically
annotate all the data with discourse connec-
tives and explicit discourse relations using simple
models trained on the Penn Discourse Treebank
(PDTB) (Prasad et al., 2008), a corpus of news ar-
ticles from the Wall Street Journal. Discourse co-
herence is an indicator of the quality of a text (Lin
et al., 2011), of its reasoning that could reveal an
attempt to deceive. Some specific semantic rela-
tions could also be good indicators (e.g. Cause).

We used models to identify the discourse con-
nectives (Connectives) and to identify the explicit
discourse relation4 (Explicit relations) they trig-
ger, either among the 4 coarse-grained classes
(lvl1) at the top of the hierarchy of sense or using
the 11 more fine-grained relations at the second
level (lvl2). Our models use Logistic Regression
and the connective and the surrounding words and
their POS as features (Lin et al., 2009). They are
trained on the sections 2-21 of the PDTB. Our re-
sults on the section 23 are close to the state-of-the-
art (Pitler and Nenkova, 2009; Pitler et al., 2008;
Lin et al., 2014): 92.9% in accuracy for identi-
fying the connectives, 95.1% for the level-1 rela-
tions, and 86.2% for the level-2 relations.

Feature analysis In addition to presenting ac-
curacies obtained with these feature sets, we

1https://github.com/wooorm/hedges/
blob/master/index.json

2http://ufal.mff.cuni.cz/udpipe
3UD 1.2, https://lindat.mff.cuni.cz/

repository/xmlui/handle/11234/1-1659
4We ignore the non explicit relations for which the in-

domain scores are very low – around 40-57% in accuracy
(Rutherford and Xue, 2015; Lin et al., 2014).

Category # Orig. feat. # Selec. feat.

Unigrams 65, 798 118
2-3-grams 1, 745, 188 154
Polarity 4 −
Causal 68 −
Inquirer 180 −
Pronouns 7 −
Hedges 121 −
Treelets 50, 522 136

Connectives 70 −
Explicit relations lvl1 4 −
Explicit relations lvl2 10 −

Table 1: Size of the original vocabulary and num-
ber of selected features for n-grams and treelets.

also perform a feature analysis. For this pur-
pose we use a combination of correlation coeffi-
cients, logistic regression coefficients, and stabil-
ity selection (Meinshausen and Bühlmann, 2010)
– a method that consists in repeatedly fitting the
model across different random subsamples, and
counting how many times features are selected in
`1-regularized logistic regression models. For sta-
bility selection, we use the implementation avail-
able in scikit-learn (Pedregosa et al., 2011) with its
default parameters, run it on the whole dataset and
keep features selected more than 50% of the time.

We indicate the size of the original vocabulary
and the number of selected features for each cate-
gory in Table 1.

5 Classification

Representation We test separately count vec-
torizations with each set of features – unigrams,
2-3-grams, polarity, causality, Inquirer categories,
pronouns (grouping per person, or considering
each lemma), treelets, connectives, hedge words,
level-1 relations and level-2 relations, and combi-
nations of these features.

Model We use a binary logistic regression clas-
sifier, optimizing the norm (`1 or `2) and strength
(c ∈ {0.001, 0.005, 0.01, 0.1, 0.5, 1, 5, 10, 100} of
the regularization term on held-out data.

Validation schemes Markowitz and Hancock
(2015) report results with a leave-one-out strat-
egy (LOO). However, LOO often under-estimates
the error rate. We compare with a nested cross-
validation procedure that can provide an almost

39



(a) Unigrams (b) 2-3-grams (c) Treelets

Figure 1: Accuracy difference between LOO and Nested LOO for each trial for different features.

System LOO N-LOO

(Markowitz and Hancock, 2015) 57.2 -

Unigrams 72.1 71.7
2-3-grams 70.8 69.6
Polarity 50.0 45.3
Causal 59.9 58.4
Inquirer 58.7 54.3
Pronouns 54.5 52.2
Hedges 56.7 54.1

Treelets 72.9 71.7

Connectives 60.1 58.3
Explicit Relations lvl1 54.3 53.2
Explicit Relations lvl2 54.5 54.3

1-2-3-grams+treelets 76.3 76.0
All 70.3 69.8

Table 2: Results (accuracy, in %).

unbiased estimate of the true error (Varma and Si-
mon, 2006; Scheffer, 1999).

Specifically, we use two cross-validation loops:
the inner loop is used for tuning the hyper-
parameters, and the outer loop estimates the gener-
alization error. The data are first split intoN folds,
the fold k (1 ≤ k ≤ N ) is the current evaluation
set, and the N − 1 other folds are used as training
data and split into M folds used for model fitting.
The best model is then evaluated on fold k. Final
scores are averages over the N folds.

For comparison with Markowitz and Hancock
(2015), we report performance with LOO and
with nested cross-validation using LOO as outer
loop, the inner loop being a random 5-fold cross-
validation. We repeat each evaluation 10 times,
and report a mean over these trials.

Results Our results are summarized in Table 2.
Our results are generally higher than the 57.2% re-
ported in (Markowitz and Hancock, 2015), with at
best 71.7% with a nested LOO and a single group
of features (unigrams or treelets) and 76.0% when

n-grams and treelets are combined.
Using all the n-grams already leads to a bet-

ter accuracy score (+1.3%) compared to us-
ing only unigrams (73.0% in accuracy for 1+2-
3-grams with N-LOO). On the other hand,
combining discourse features to the n-grams
does not allow improvements over using only
the n-grams (72.8% with N-LOO for 1+2-3-
grams+Connectives+Explicit Relations lvl1).

The scores obtained with LOO are over-
estimate performance, compared to nested cross-
validation, see for example Figure 1: Even if the
differences are low, they are consistent across the
trials and the feature sets.

6 Feature analysis

We use Pearson’s ρ (w. Bonferroni correction) to
establish what features are predictive of fraud and
non-fraud. We report the values for the features
cited in Table 3.

Hedging There is an interesting contrast be-
tween adverbial hedges (conceivably, presumably,
surely, effectively) and verbal hedges (suggest) in-
dicative of fraud, and adverbial hedges (practi-
cally, occasionally) and verbal hedges assume,
speculate) indicative of non-fraud: It seems ad-
verbs and verbs used in fraud are for interpret-
ing the data on behalf of the reader, whereas the
adverbs and verbs indicative of fraud are more
observer-aware (e.g., we speculate). This sug-
gest that a fraud strategy is to hide observers bias,
rather than being explicit about it.

Comparison Both the discourse relation and the
Inquirer class for comparison are predictive of
non-fraud. Scientific fraud thus seems less likely
to compare. On the other hand, neither the causal
relations or the presence of causal terms were sig-
nificantly linked to fraudulent papers.

Therefore vs. since A peculiar, but statistically
significant difference between fraud and non-fraud
articles, is that fraud articles prefer therefore over

40



since, and vice versa. We speculate that it may
be a fraud strategy to make the reasoning more
verbose by separating out premises (because the
authors are, consciously or not, afraid the readers
will not accept them). This is in slight contrast
with or qualifies the main hypothesis in Markowitz
and Hancock (2015), that fraudulent writers try to
obfuscate their writing.

Other markers of fraud Many technical con-
cepts were highly correlated with fraud, but we
suspect these are cases of overfitting. More in-
terestingly, the bigram described previously was
among the top-5 most highly correlated features,
indicating fraud. From our syntactic treelets,
proper nouns and interjections were both slightly
indicative of fraud (p < 0.01).

Other markers of non-fraud From our syntac-
tic treelets, conjunctions of numbers were indica-
tive of non-fraud, suggesting maybe a higher level
of technical detail. Non-fraud articles are also
more likely to use the pronoun they, as compared
to we, compared to fraud papers.

7 Conclusion

We show that a simple unigram model outper-
forms previous work on scientific fraud detection.
Overall, more high-level linguistic features, be-
yond syntactic treelets, do not lead to improve-
ments, but we also presented a feature analysis
showing, for example, that comparison and expla-
nation (at the semantic and discourse level) are in-
dicators of non-fraud, and that fraudulent writing
uses slightly different hedging strategies.

References

Ferric C. Fang, R. Grant Steen, and Arturo Casadevall.
2012. Misconduct accounts for the majority of re-
tracted scientific publications. Proceedings of the
National Academy of Sciences of the United States
of America, 109(42):17028–17033.

Song Feng, Ritwik Banerjee, and Yejin Choi. 2012a.
Syntactic stylometry for deception detection. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Short
Papers-Volume 2, pages 171–175. Association for
Computational Linguistics.

Song Feng, Longfei Xing, Anupam Gogar, and Yejin
Choi. 2012b. Distributional footprints of deceptive
product reviews. ICWSM, 12:98–105.

Hedges
assume -0.121 p=0.006
practically -0.118 p=0.008
occasionally -0.112 p=0.012
conceivably 0.089 p=0.045
assumed -0.086 p=0.052
surely 0.077 p=0.083
effectively 0.075 p=0.090
presumably 0.058 p=0.195

Inquirer
compare -0.158 p=0.0003

Explicit Relations lvl1
comparison -0.096 p=0.031
cause 0.008 p=0.863

Connectives
since -0.102 p=0.022
therefore 0.064 p=0.147

2-3-grams
described previously 0.115 p=0.009

Treelets
intj 0.126 p=0.004
propn 0.110 p=0.013

Pronouns
we 0.071 p=0.112
they -0.059 p=0.182

Table 3: Pearson ρ and original p-value (before
Bonferroni correction) for some features.

Anders Trrup Johannsen, Dirk Hovy, and Anders
Søgaard. 2015. Cross-lingual syntactic variation
over age and gender. In Proceedings of the Nine-
teenth Conference on Computational Natural Lan-
guage Learning.

Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of EMNLP.

Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating text coherence using dis-
course relations. In Proceedings of ACL-HLT.

Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014.
A pdtb-styled end-to-end discourse parser. Natural
Language Engineering, 20:151–184.

David M. Markowitz and Jeffrey T. Hancock. 2015.
Linguistic Obfuscation in Fraudulent Science. Jour-
nal of Language and Social Psychology.

41



DS McNamara, MM Louwerse, Z Cai, and A Graesser.
2013. Coh-metrix version 3.0. Retrieved [4/1/15]
from http://cohmetrix. com.

Nicolai Meinshausen and Peter Bühlmann. 2010. Sta-
bility selection. Journal of the Royal Statisti-
cal Society: Series B (Statistical Methodology),
72(4):417–473.

Rada Mihalcea and Carlo Strapparava. 2009. The lie
detector: Explorations in the automatic recognition
of deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 309–
312. Association for Computational Linguistics.

Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T Han-
cock. 2011. Finding deceptive opinion spam by any
stretch of the imagination. In Proceedings of ACL
HLT.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research,
12:2825–2830.

James W Pennebaker, Roger J Booth, and Martha E
Francis. 2007. Linguistic inquiry and word count:
Liwc. Austin, TX: Pennebaker Conglomerates.

Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text.
In Proceedings of the ACL-IJCNLP.

Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind Joshi. 2008. Easily
identifiable discourse relations. In Proceedings of
COLING (Posters).

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0.
In Proceedings of LREC.

Attapol Rutherford and Nianwen Xue. 2015. Improv-
ing the inference of implicit discourse relations via
classifying explicit discourse connectives. In Pro-
ceedings of NAACL-HLT.

Tobias Scheffer. 1999. Error Estimation and Model
Selection. Ph.D. thesis, Technischen Universitet
Berlin, School of Computer Science.

Philip J. Stone and John Kirsh. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press.

Milan Straka, Jan Hajič, and Straková. 2016. UDPipe:
Trainable Pipeline for Processing CoNLL-U Files
Performing Tokenization, Morphological Analysis,
POS Tagging and Parsing. In Proceedings of the
Tenth International Conference on Language Re-
sources and Evaluation (LREC’16).

Sudhir Varma and Richard Simon. 2006. Bias in error
estimation when using cross-validation for model
selection. BMC bioinformatics.

42


