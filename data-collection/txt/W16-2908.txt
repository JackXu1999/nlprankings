



















































Character based String Kernels for Bio-Entity Relation Detection


Proceedings of the 15th Workshop on Biomedical Natural Language Processing, pages 66–71,
Berlin, Germany, August 12, 2016. c©2016 Association for Computational Linguistics

Character based String Kernels for Bio-Entity Relation Detection

Ritambhara Singh
Department of Computer Science

University of Virginia
Charlottesville

rs3zz@virginia.edu

Yanjun Qi
Department of Computer Science

University of Virginia
Charlottesville

yanjun@virginia.edu

Abstract

Extracting bio-entity relations has
emerged as an important task due to the
ever-growing number of bio-medical doc-
uments. In this paper, we present a simple
and novel representation for extracting
bio-entity relationships. The state-of-the-
art systems for such tasks rely on word
based representations and variations of
linguistic driven features. In contrast, we
model bio-text by the most basic character
based string representation with a family
of string kernels. This eliminates time
consuming parsing, issue of rare words
and domain specific pre-processing. This
simple representation makes our approach
fast and flexible for any bio-NLP dataset.
We demonstrate comparable performance
and faster computation time of our ap-
proach versus previous state-of-the-art
kernel methods.

1 Introduction
Relation extraction from biomedical documents

is an important task in knowledge representation
and inference. It helps to construct and enhance
structured knowledge-bases and in turn support
automatic question answering and decision mak-
ing. In today’s era of vast amount of information
collection and retrieval, the task of naming and
identifying the relations between annotated bio-
entities can become complex and time consuming.
This can be deducted from the fact that the MED-
LINE database has more than 22 million journal
articles related to biomedicine. Many state-of-
the art methods have been applied for the popu-
lar tasks of extracting protein-protein interaction
(PPI) and drug-drug interaction (DDI) as a part of
BioCreative shared task challenges (Segura Bed-
mar et al., 2011; Segura Bedmar et al., 2013;

Krallinger et al., 2008). While these methods
have achieved good performance, they mostly rely
on word-level features, are dependent on time-
consuming parsers or require domain knowledge
for pre-processing.

This paper uses characters instead of words
for bio-entity relation extraction. Characters are
the most fundamental building blocks in any lan-
guage. We propose to model bio-text using its
most basic character-based string representation.
Through a string kernel implementation, in the
framework of support vector machine (SVM),
we separate positive and negative interaction in-
stances to detect bio-entity relationships. This
basic representation is independent of parsers,
does not require domain-related pre-processing
and eliminates the rare words problem. It not
only performs comparable to other state-of-the-art
methods but also provides an exploration of new
and simple feature sets (complementary to exist-
ing features) that have not been previously studied
for bio-NLP shared tasks.

2 Related Work
Convolution-based kernel methods have been

used extensively in the tasks of PPI and DDI ex-
traction, and differ in the feature sets they ex-
plore. While the shallow linguistic (SL) ker-
nel (Giuliano et al., 2006) uses simple linguis-
tic features, others utilize more complex fea-
tures. Constituent parse tree-based kernels, like
subtree (ST) (Vishwanathan et al., 2004), sub-
set tree (SST) (Collins and Duffy, 2001), partial
tree (PT) (Moschitti, 2006) kernels, and spectrum
tree (SpT) (Kuboyama et al., 2007) kernel, use
subtree forms or path structures from constituent
parse trees. Another category of methods use de-
pendency parse tree-based features. This includes
edit distance and cosine similarity kernels (using
shortest paths) (Erkan et al., 2007), k-band short-
est path spectrum (kBSPS) (Tikk et al., 2010) (a

66



Kip1 binds to CDK2 

protein1 binds to protein2 

k=4 
or 

  Family of String Kernels  

ϕ 

Support Vector Machine 

Negative instance 

Positive instance 

g=9 

(1) Original Instance 

(2) Tokenization 

(3) Character-level features 

(4) Mapping of strings using k-mer 
      or g-mer level features into 
      numerical feature space 

(5) Binary classification using SVM 

b	   i	   n	   d	   s	   t	   o	   .	  .	  .	  .	  	  	  .	  .	  .	  .	  	  	  

Figure 1: End-to-end implementation of
character-based string kernels for bio-entity re-
lation detection.

k-band extension of shortest paths), all-path graph
(APG) kernel (Airola et al., 2008) (weighing dif-
ferently shortest paths), and Kim’s kernels (Kim
et al., 2008) (combines shortest path with differ-
ent lexical, part-of-speech and syntactic features).
Benchmark papers, such as (Tikk et al., 2010) and
(Tikk et al., 2013), have performed thorough com-
parative and error analyses of all these different
kernels. They concluded that APG, kBSPS and SL
kernels give the best performance. Therefore, we
use these three kernels as baselines in our experi-
mental comparisons. Some studies include a com-
bination of kernels and parsers for PPI extraction
task, e.g. (Miwa et al., 2009). Similarly, (Thomas
et al., 2013) implemented a two-step approach to
first detect general DDIs and then classify detected
DDIs into subtypes. For the general DDI task, they
used voting to combine kernels including APG,
subtree (ST), SST, SpT, and SL kernels.

All the above discussed methods suffer from
the rare words problem, and require time consum-
ing and domain specific pre-processing steps like
parsing to obtain lexical features, constituent and
dependency trees. Several recent studies have dis-
covered that character-based representation pro-
vides simple and powerful models for sentiment
classification (Zheng et al., 2015) and transition-
based parsing (Ballesteros et al., 2015). (Lodhi
et al., 2002) first used string kernels with charac-
ter level features for text categorization. However,
their kernel computation used dynamic program-
ming which is computationally intensive. Over
recent years, more efficient string kernel meth-
ods have been devised (Leslie and Kuang, 2004;

Corpus Task Sent. Pos Neg Total
MEDLINE DDI 1301 232 1555 1787
AIMed PPI 1955 1000 4834 5834
LLL PPI 77 164 166 330

Table 1: Statistics (number of sentences, pos-
itive, negative and total instances) of the MED-
LINE corpus about DDI and, AIMed and LLL cor-
pus about PPI extraction respectively.

Kuksa et al., 2009). Therefore, we apply a fam-
ily of state-of-the-art string kernels using sim-
ple character-based string representation for bio-
entity relation detection in this work.

3 Approach
Figure 1 shows our end-to-end implementation

of character-based string kernel approach for bio-
entity relation detection.

3.1 Character-level features
Without relying on any pre-processing, we di-

rectly use the instance sentences of bio-NLP
datasets as input to the string kernels. Here,
each instance (whole sentence) is viewed as one
long contiguous string comprised of characters. A
string kernel is then used to convert these strings
into a feature space (implicitly through kernel cal-
culation) that can be used as input for support vec-
tor machine (SVM) classification algorithm.

3.2 Family of string kernels
The key idea of string kernels is to apply a func-

tion φ(·), which maps strings of arbitrary length
into a vectorial feature space of fixed length. In
this space, a standard classifier such as SVM
(Vapnik, 1998) can then be applied. Kernel-
version of SVMs calculate the decision function
for an input sample x :

f(x) =
N∑
i=1

αiK(xi, x) + b (1)

where N is the total number of training samples.
String kernels (Leslie and Kuang, 2004; Kuksa et
al., 2009; Ghandi et al., 2014a), implicitly com-
pute an inner product in the mapped feature space
φ(x) as:

K(x, x′) = 〈φ(x), φ(x′)〉, (2)

where x = (s1, . . . , s|x|). x, x′ ∈ S. |x| denotes
the length of the string x. S represents the set of

67



all strings composed of dictionary Σ. φ : S → Rm
defines the mapping from a sequence x ∈ S to a
m-dimensional feature vector.

The feature representation φ(·) plays a key role
in the effectiveness of string analysis since strings
cannot be readily described as feature vectors. We
have implemented the following string kernels on
the character representation.

Spectrum Kernel (SK): One classic represen-
tation is to represent a string as unordered set of
k-mers, that is, combinations of k adjacent char-
acters. A feature vector indexed by all k-mers
records the number of occurrences of each k-mer
in the current string. The string kernel using this
representation is called spectrum kernel (Leslie
et al., 2002), where the spectrum representation
counts the occurrences of each k-mer in a string.
Kernel scores between strings are then computed
by taking an inner product between corresponding
“k-mer - indexed” feature vectors:

K(x, x′) =
∑
γ∈Γk

cx(γ) · cx′(γ) (3)

where γ represents a k-mer, Γk is the set of all pos-
sible k-mers, and cx(γ) is the number of occur-
rences (with normalization) of k-mer γ in string
x. (Kuboyama et al., 2007) applied spectrum ker-
nel on the constituent parse tree features.

Mismatch Kernel (MK): The spectrum kernel
implementation is modified to include m number
of mismatches in the k-mers (Leslie and Kuang,
2004; Kuksa et al., 2009). Thus, for a given k-mer
γ in a string x, the (k,m)-neighborhood is gener-
ated. This consists of all k-length strings α from
dictionary Σk such that they differ from original
k-mer by at most m mismatches. The feature map
of mismatch kernel can be defined as:

φ(k,m)(γ) = (φα(γ))α∈Σk (4)

where φα(γ) = 1 if α ∈ (k,m)-neighborhood of
γ, otherwise φα(γ) = 0. A mismatch kernel with
m = 0 is essentially a spectrum kernel.

Wildcard Kernel (WK): For implementation of
wildcard kernel, the dictionary Σ is augmented
with a wildcard character ? (Leslie and Kuang,
2004). Thus, the feature space consists of set of
k-mers Γk obtained from Σ ∪ {?} that consists of
m occurrences of wildcard character (?). The fea-
ture map of wildcard kernel can be defined as:

φ(k,m,λ)(γ) =
∑

Γkinx

(φα(γ))α∈(Σ∪{?}) (5)

where φα(γ) = λm if γ matches α with m occur-
rences of character ?, otherwise φα(γ) = 0. Here
0 < λ ≤ 1.
Gapped k-mer based Kernel (GK): The previ-
ously described k-mer based string kernels gener-
ate extremely sparse feature vectors for even mod-
erately sized values of k, resulting in overfitting.
(Ghandi et al., 2014b) introduced a new feature
set, called gapped k-mers, resolving the sparsity
limitation with k-mer features. It is characterized
by two parameters; (1) g, size of a gapped instance
which is a segment of string including gaps and (2)
k, the number of non-gapped k-mers or positions
in each segment of size g. Thus, the number of
gaps d = g − k. The inner product in equation 3
includes sum over all gapped k-mers features:

K(x, x′) =
∑
γ∈Θg

cx(γ) · cx′(γ) (6)

where γ represents a k-mer, Θg is the set of all
possible g-mers in the given data.

3.3 Classification
Once the kernel matrix K is calculated, we

input it into an SVM classifier as an empirical
feature map using SVM light (Joachims, 1999;
Schölkopf and Burges, 1999). SVM maximizes
the margin between the positive and negative in-
stances of bio-entity interactions in the kernel de-
fined feature space.

4 Experiment Setup
Datasets We demonstrate the benchmark imple-
mentations of our approach on three datasets with
different sample sizes. They include MEDLINE
corpus from the DDI extraction task (Segura Bed-
mar et al., 2011; Segura Bedmar et al., 2013), and
the AIMed and LLL corpus from the PPI extrac-
tion task (Krallinger et al., 2008). 1. The details of
the datasets have been presented in Table 1.

Baselines We selected SL (Giuliano et al.,
2006), APG (Airola et al., 2008), and kBSPS

1We use the same format as used in previous studies, that
is, each interaction is represented as a separate input instance.
Thus, a sentence about multiple interactions is represented as
multiple instances. The protein name entities are replaced by
special tokens. See details in (Tikk et al., 2010)

68



Corpus Task kBSPS APG SL SK (k) MK (k,m) WK (k,m) GK (g, k)
MEDLINE DDI - 82.3 78.9 82.1 (7) 82.7 (7,3) 83 (7,3) 82.4 (7,4)
AIMed PPI 75.1 84.6 83.5 75.6 (8) 74.9 (10,5) 75.2 (10,5) 75.4 (8,6)
LLL PPI 84.3 83.5 81.2 67.9 (7) 77.9 (7,3) 78.4 (8,5) 78.1 (7,5)

Table 2: Using AUC score to compare four character-based string kernels with APG, kBSPS and SL
baselines. The best performing kernel parameters are also presented. AUC scores for APG and SL
kernels for MEDLINE corpus have been reported in (Thomas et al., 2013), while scores of all three
baseline kernels for AIMed and LLL corpus are reported in (Tikk et al., 2010).

Corpus Task kBSPS APG SL SK MK WK GK
MEDLINE DDI 169.13 169.13 5.2 0.4 2.6 3.1 2.6
AIMed PPI 254.15 254.14 7.82 76.8 79.5 78 41.3
LLL PPI 10 10 0.3 0.2 1.3 1 0.2

Table 3: Comparing the kernel computation time (in seconds) for all four character-based string kernels
versus the estimated parsing times of state-of-the-art baselines reported from (Tikk et al., 2010).

(Tikk et al., 2010) kernels as baselines for com-
paring with character-based string kernels. These
kernels are the top-performing approaches, as re-
ported by(Tikk et al., 2010; Tikk et al., 2013) and
(Luo et al., 2016).

Parameters We ran all our string kernels across
multiple kernel parameter settings as follows: (1)
SK : k = {6, 7, 8, 9, 10}, (2) MK,WK : k =
{6, 7, 8, 9, 10} and m = {1, .., k− 1}, and (3) GK
: g = {6, 7, 8, 9, 10} and k = {1, ..., g−1}. These
string kernels were implemented using gkmsvm
(Ghandi et al., 2014a) tool. The character level
dictionary, Σ = {a, ..., z, 0, 1, ...9} (size=36), is
consistent for all the datasets and kernels.

Evaluation Metrics We performed 10-fold doc-
ument level cross-validation on each selected cor-
pus and calculated the AUC score (area under the
receiver operating characteristic curve) for perfor-
mance evaluation. (Tikk et al., 2010) confirmed
that AUC score is more stable to parameter mod-
ifications and less sensitive to the ratio of pos-
itive/negative pairs in the corpus than F-score.
Hence, AUC score is our choice for performance
metric. We also recorded the kernel calculation
times (in seconds) for all four string kernels.

5 Results
Table 2 summarizes the performance evalua-

tion. The AUC scores for APG and SL kernels for
MedLine corpus have been reported in (Thomas et
al., 2013), while scores of all baseline kernels for
AIMed and LLL corpus are reported in (Tikk et

al., 2010). Our string kernel approaches, with sim-
ple character features, (WK,MK, and GK), outper-
form the baseline kernels (Table 2) on the Med-
Line corpus. For the AIMed corpus, SK, GK, and
WK give higher AUC score than the baseline kB-
SPS kernel. Our methods give reasonable perfor-
mance for LLL corpus as well, however not as
good as the three baseline kernels. The parameters
giving the best AUC performance are also reported
(Table 2). Our representation is complementary
and can be plugged into state-of-the-art baselines
to further improve their systems.

Table 3 presents the kernel computation time
comparison of all four character-based string ker-
nels versus the baselines. We compare this with
the estimated parsing times. For the baseline ker-
nels, these have been reported and calculated in
(Tikk et al., 2010). Unlike baseline kernels, we
use character level features directly and thus do
not need the parsing step.

6 Discussion
We have proposed a simple and novel char-

acter representation for bio-entity relation detec-
tion task. We implement a family of string ker-
nels on such simple features extracted directly
from instances of PPI and DDI extraction task
datasets. This eliminates time-consuming and
domain-specific pre-processing steps, making our
approach fast and flexible for any bio-NLP dataset.
Hence, our work opens new avenues to explore
different and simpler feature sets at the character
level.

69



References
Antti Airola, Sampo Pyysalo, Jari Björne, Tapio

Pahikkala, Filip Ginter, and Tapio Salakoski. 2008.
All-paths graph kernel for protein-protein interac-
tion extraction with evaluation of cross-corpus learn-
ing. BMC bioinformatics, 9(11):1.

Miguel Ballesteros, Chris Dyer, and Noah A Smith.
2015. Improved transition-based parsing by model-
ing characters instead of words with LSTMs. arXiv
preprint arXiv:1508.00657.

Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in neural
information processing systems, pages 625–632.

Günes Erkan, Arzucan Özgür, and Dragomir R Radev.
2007. Semi-supervised classification for extract-
ing protein interaction sentences using dependency
parsing. In EMNLP-CoNLL, volume 7, pages 228–
237.

Mahmoud Ghandi, Dongwon Lee, Morteza
Mohammad-Noori, and Michael A Beer. 2014a.
Enhanced regulatory sequence prediction using
gapped k-mer features. PLoS Comput Biol,
10(7):e1003711.

Mahmoud Ghandi, Morteza Mohammad-Noori, and
Michael A Beer. 2014b. Robust k-mer frequency
estimation using gapped k-mers. Journal of mathe-
matical biology, 69(2):469–500.

Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-
mano. 2006. Exploiting shallow linguistic informa-
tion for relation extraction from biomedical litera-
ture. In EACL, volume 18, pages 401–408. Citeseer.

Thorsten Joachims. 1999. Making large scale SVM
learning practical. Technical report, Universität
Dortmund.

Seonho Kim, Juntae Yoon, and Jihoon Yang. 2008.
Kernel approaches for genic interaction extraction.
Bioinformatics, 24(1):118–126.

Martin Krallinger, Florian Leitner, Carlos Rodriguez-
Penagos, Alfonso Valencia, et al. 2008. Overview
of the protein-protein interaction annotation ex-
traction task of BioCreative II. Genome biology,
9(Suppl 2):S4.

Tetsuji Kuboyama, Kouichi Hirata, Hisashi Kashima,
Kiyoko F Aoki-Kinoshita, and Hiroshi Yasuda.
2007. A spectrum tree kernel. Information and Me-
dia Technologies, 2(1):292–299.

Pavel P Kuksa, Pai-Hsi Huang, and Vladimir Pavlovic.
2009. Scalable algorithms for string kernels with in-
exact matching. In Advances in Neural Information
Processing Systems, pages 881–888.

Christina Leslie and Rui Kuang. 2004. Fast string
kernels using inexact matching for protein se-
quences. The Journal of Machine Learning Re-
search, 5:1435–1455.

Christina S Leslie, Eleazar Eskin, and William Stafford
Noble. 2002. The spectrum kernel: A string kernel
for SVM protein classification. In Pacific sympo-
sium on biocomputing, volume 7, pages 566–575.

Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. The Journal of
Machine Learning Research, 2:419–444.

Yuan Luo, Özlem Uzuner, and Peter Szolovits.
2016. Bridging semantics and syntax with graph
algorithms–state-of-the-art of extracting biomedical
relations. Briefings in bioinformatics, page bbw001.

Makoto Miwa, Rune Sætre, Yusuke Miyao, and
Jun’ichi Tsujii. 2009. Protein–protein interac-
tion extraction by leveraging multiple kernels and
parsers. International journal of medical informat-
ics, 78(12):e39–e46.

Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Machine Learning: ECML 2006, pages 318–329.
Springer.

Bernhard Schölkopf and Christopher JC Burges. 1999.
Advances in kernel methods: support vector learn-
ing. MIT press.

Isabel Segura Bedmar, Paloma Martinez, and Daniel
Sánchez Cisneros. 2011. The 1st DDIextraction-
2011 challenge task: Extraction of drug-drug inter-
actions from biomedical texts.

Isabel Segura Bedmar, Paloma Martı́nez, and Marı́a
Herrero Zazo. 2013. Semeval-2013 task 9: Ex-
traction of drug-drug interactions from biomedical
texts (ddiextraction 2013). Association for Compu-
tational Linguistics.

Philippe Thomas, Mariana Neves, Tim Rocktäschel,
and Ulf Leser. 2013. WBI-DDI: drug-drug inter-
action extraction using majority voting. In Second
Joint Conference on Lexical and Computational Se-
mantics (* SEM), volume 2, pages 628–635.

Domonkos Tikk, Philippe Thomas, Peter Palaga, Jörg
Hakenberg, and Ulf Leser. 2010. A comprehensive
benchmark of kernel methods to extract protein–
protein interactions from literature. PLoS Comput
Biol, 6(7):e1000837.

Domonkos Tikk, Illés Solt, Philippe Thomas, and Ulf
Leser. 2013. A detailed error analysis of 13 kernel
methods for protein–protein interaction extraction.
BMC bioinformatics, 14(1):1.

Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. Wiley-Interscience, September.

SVN Vishwanathan, Alexander Johannes Smola, et al.
2004. Fast kernels for string and tree matching. Ker-
nel methods in computational biology, pages 113–
130.

70



Xiaoqing Zheng, Haoyuan Peng, Yi Chen, Pengjing
Zhang, and Wenqiang Zhang. 2015. Character-
based parsing with convolutional neural network. In
Proceedings of the 24th International Conference
on Artificial Intelligence, pages 1054–1060. AAAI
Press.

71


