



















































What to Write? A topic recommender for journalists


Proceedings of the 2017 EMNLP Workshop on Natural Language Processing meets Journalism, pages 19–24
Copenhagen, Denmark, September 7, 2017. c©2017 Association for Computational Linguistics

What to Write? A topic recommender for journalists

Giovanni Stilo and Paola Velardi
Sapienza University of Rome, Italy
{stilo, velardi}@di.uniroma1.it

Alessandro Cucchiarelli and Giacomo Marangoni and Christian Morbidoni
Università Politecnica delle Marche, Italy

{a.cucchiarelli, g.marangoni, c.morbidoni}@univpm.it

Abstract

In this paper we present a recommender
system, What To Write and Why (W 3),
capable of suggesting to a journalist, for
a given event, the aspects still uncov-
ered in news articles on which the read-
ers focus their interest. The basic idea is
to characterize an event according to the
echo it receives in online news sources
and associate it with the corresponding
readers’ communicative and informative
patterns, detected through the analysis of
Twitter and Wikipedia, respectively. Our
methodology temporally aligns the results
of this analysis and recommends the con-
cepts that emerge as topics of interest from
Twitter and Wikipedia, either not covered
or poorly covered in the published news
articles.

1 Introduction

In a recent study on the use of social media sources
by journalists (Knight, 2012) the author concludes
that ”social media are changing the way news
are gathered and researched”. In fact, a growing
number of readers, viewers and listeners access
online media for their news (Gloviczki, 2015).
When readers feel involved by news stories they
may react by trying to deepen their knowledge
on the subject, and/or confronting their opinions
with peers. Stories may then solicit a reader’s in-
formation and communication needs. The inten-
sity and nature of both needs can be measured
on the web, by tracking the impact of news on
users’ search behavior on on-line knowledge bases
as well as their discussions on popular social plat-
forms. What is more, on-line public’s reaction to
news is almost immediate (Leskovec et al., 2009)
and even anticipated, as for the case of planned

media events and performances, or for disasters
(Lehmann et al., 2012). Assessing the focus, dura-
tion and outcomes of news stories on public atten-
tion is paramount for both public bodies and me-
dia in order to determine the issues around which
the public opinion forms, and in framing the issues
(i.e., how they are being considered) (Brooker and
Schaefer, 2005). Futhermore, real-time analysis
of public reaction to news items may provide use-
ful feedback to journalists, such as highlighting as-
pects of a story that needs to be further addressed,
issues that appear to be of interest for the public
but have been ignored, or even to help local news-
papers echo international press releases.

The aim of this paper is to present a news me-
dia recommender, What to Write and Why (W 3),
for analyzing the impact of news stories on the
readers, and finding aspects – still uncovered in
news articles – on which the public has focused
their interest. The purpose of W 3 is to support
journalists in the task of reshaping and extend-
ing their coverage of breaking news, by suggesting
topics to address when following up on such news
items. For example, we have found that a com-
mon pattern for news readers is to search events of
the same type occurred in the past on Wikipedia,
which is not surprising per se: however, among
the many possible similar events, our system is
able to identify those that the majority of read-
ers consider (sometimes surprisingly) highly as-
sociated with breaking news, e.g., searching for
the 2013 CeaseFire program in Baltimore during
Egypt’s ceasefire proposal in Gaza on July 2014.

2 Methodology

Our methodology is in five steps, as shown in the
workflow of Figure 1:

Step 1. Event detection: We use SAX*, an
unsupervised temporal mining algorithm that we

19



Intra source
clustering

C2 C3 C4

C1

Event 
Detection

t1 t3 t4
t2

Intra source
clustering

C2 C3 C4

C1

Reccomendations      
based on

Information Needs

Reccomendations
based on

Communication Needs       

Intra source
clustering

C2 C3 C4

C1

Event 
Detection

t1 t3 t4
t2

Wikipedia 
page-views

Twitter 
Stream

News Stream

Event 
Detection

t1 t3 t4
t2

Inter Sources
alignement     

E'1

E'2

E''1

E''2

E'''1

E'''2

E2 E1

Classification      
of

Information Needs

Classification
of

Communication Needs       

Figure 1: Workflow of W 3

-1.4	

-0.9	

-0.4	

0.1	

0.6	

1.1	

1.6	

2.1	

2.6	

7/1
5/1

4	

7/1
6/1

4	

7/1
7/1

4	

7/1
8/1

4	

7/1
9/1

4	

7/2
0/1

4	

7/2
1/1

4	

7/2
2/1

4	

7/2
3/1

4	

7/2
4/1

4	

Siberia_Airlines_Flight_1812	 Ukraine	 Malaysia_Airlines_Flight_17	

2014_pro-Russian_unrest_in_Ukraine	 Surface-to-air_missile	 Pan_Am_Flight_103	

2014_Russian_military_intervenKon_in_Ukraine	

Figure 2: Cluster of normalized time series of Wikipedia
page views for Malaysia Airline crash on July 2014.

introduced in (Stilo and Velardi, 2016), to cluster
tokens – words, entities, hashtags, page views –
based on the shape similarity of their associated
signals s(t). In SAX*, signals observed in tempo-
ral windowsLk are first transformed into strings of
symbols of an alphabet Σ; next, strings associated
to active tokens (those corresponding to patterns
of public attention) are clustered based on their
similarity. Each cluster is interpreted as related to
an event ei. Clusters are extracted independently
from on-line news (N ), Twitter messages (T ) and
Wikipedia page views (W ).
For example, the cluster in Figure 2 shows
Wikipedia page views related to the Malaysia Air-
line crash on July 2014. We remark that SAX*
blindly clusters signals without prior knowledge
of the event and its occurrence date, and further-
more, it avoids time-consuming processing of text
strings, since it only considers active tokens.

Step 2. Intra-source clustering: Since clus-
ters are generated in sliding windows Lk of equal
length L and temporal increment ∆, clusters refer-
ring to the same event but extracted in partly over-
lapping windows may slightly differ, especially
for long-lasting events, when news updates moti-
vate the emergence of new sub-topics and the de-
cay of others. An example is in Figure 3, show-
ing for simplicity a cluster with a unique signal
s(t) which we can also interpret as the cluster cen-
troid. The Figure also shows the string of symbols

a a a a b b a a a a a

a a a a a a a a a b b

a b b a a a a a a b a

a a a a a b a a a a a

a a a a a a a b b a a

1

2

3

4

5

Figure 3: SAX* strings associated to a temporal series s(t)
in 5 adjacent or overlapping windows.

associated with the signal in each window (with
Σ = {a, b}).

For a better characterization of an event, we
merge clusters referring to the same event and ex-
tracted in adjacent windows, based on their sim-
ilarity. Merged clusters form meta-clusters, de-
noted with mSi , where the index i refers to the
event and S ∈ {N,T,W} to the data source. With
reference to Figure 3, the signals in windows 1, 2,
3 and 4 would be merged, but not the signal in
window 5.
An example from the T dataset is shown in Table
1: note that the first two clusters show that ini-
tially Twitter users where concerned mainly about
the tragedy (clusters C9 and C5), and only later
did their interest focus on political aspects (e.g.,
Barack Obama, Vladimir Putin in C17 and C18).

Step 3. Inter-source alignment: Next, an
alignment algorithm explores possible matches
across the three data sources N , T and W . For
any event ei, we eventually obtain three ”aligned”
meta-clusters mNi , m

T
i and m

W
i mirroring respec-

tively the media coverage of the considered event
and its impact on readers’ communication and in-
formation needs.

Step 4. Generating a recommendation: The
input to our recommender is the news meta-
clusters mNi related to an event ei first reported
on day d0 and extracted during an interval I :
d0 ≤ d ≤ d0+x, where d0+x is the day in which
the query is performed by the journalist. The
system compares the three aligned meta-clusters

20



Table 1: The Twitter meta-cluster capturing the Malaysia Airlines flight crash event and its composing clusters

Clusters
C9 [tragic, crash, tragedi, Ukraine 1.0, Malaysia Airlines 0.6, Airline 0.66, Malaysia Airlines Flight 17 0.65, Malaysia 0.60, Russia 0.51, Avia-
tion accidents and incidents 0.36, Airliner 0.35, Malaysia Airlines Flight 37 0.28, United States 0.21, Tragedy 0.20, Boeing 777 ... ]

C5 [tragic, tragedi, Airline 1.0, Malaysia Airlines Flight 17 0.97, Malaysia Airlines 0.70, Malaysia 0.58, Ukraine 0.40, Twitter 0.39, Gaza Strip 0.32, CNN 0.26,
Tragedy 0.25, God 0.24, Airliner 0.22, Israel 0.22, Malaysia Airlines Flight 370 0.22, Netherlands 0.21,..]

C17 [tragedi, tragic, Malaysia Airlines Flight 17 1.0, Airline 0.89, Malaysia Airlines 0.62, Malaysia 0.54, Gaza Strip 0.42, Twitter 0.38, Ukraine 0.38, Hamas 0.33,
Barack Obama 0.32, Israel 0.29, Vladimir Putin 0.27, God 0.26, CNN 0.25, Hell 0.25, Airliner 0.23, Malaysia Airlines Flight 37 0.20,...]

T C18 [tragedi, tragic, Malaysia Airlines Flight 17 1.0, Airline 0.98, Malaysia Airlines 0.80, Tragedy , Malaysia 0.54, Gaza Strip 0.50, Ukraine 0.48, Hamas 0.408,
Israel 0.38, Barack Obama 0.7, Twitter 0.37, Vladimir Putin 0.36, CNN 0.32, Airliner 0.28, Malaysia Airlines Flight 370 0.26, Hell 0.252, ...]

Meta-cluster
[tragedi 0.22, tragic 0.22, airline 0.20, malaysia airlines flight 17 0.20, ukraine 0.19, malaysia airlines 0.19, malaysia 0.17, russia 0.129, tragedy 0.12, vladimir putin
0.12, airliner 0.12, crash 0.12, gaza strip 0.11, barack obama 0.11, aviation accidents and incidents 0.11, cnn 0.106, malaysia airlines flight 370 0.10, god 0.10, ... ]

to identify in mTi and m
W
i the set of most rele-

vant entities 1, respectively ETi and E
W
i . A set

of entities Ei in either T or W is further parti-
tioned in Rin newsi and R

novel
i , representing, re-

spectively, the event-related topics already dis-
cussed, and those not yet considered in news
items. The first set is interesting for journal-
ists in order to understand which topics mostly
attracted the attention of the public, while the
second set includes event-related, but still un-
covered, topics that W 3 recommends to discuss.
For example, the following is a recommenda-
tion generated from the analysis of Wikipedia
page views, related to Scottish Independence elec-
tions on September 17th, 2014: [scotland, wales,
alex salmond, united kingdom, scottish national
party, flag of scotland, william wallace, coun-
tries of the united kingdom, mary queen of sco-
ts, tony blair, braveheart, flag of the united
kingdom, republic of ireland]. When com-
paring these entities with the aligned news
meta-clusters, the set of novel entities Rnoveli
is: [flag of scotland, william wallace, countries
of the united kingdom, mary queen of scots, tony
blair, braveheart] and all the others are also found

in news.
Step 5. Classification of information and

communication needs: In addition to recom-
mendations, we automatically assign a category
both to event clusters mNi in news, and to related
entities in Twitter and Wikipedia aligned meta-
clusters mTi and m

W
i , in order to detect recur-

rent discussion topics and search patterns in re-
lation to specific event types. To do so, we ex-
ploit both BabelNet (Navigli and Ponzetto, 2010),

1We used TextRazor https://www.textrazor.
com and DataTXT https://dandelion.eu/
semantic-text/entity-extraction-demo/
to extract entities respectively from Twitter and news items

dataset # clusters #
m.clusters

av. size
m.clusters

News 9396 829 122.46
Twitter 4737 413 136.76
Wikipedia 5450 535 6.44

Table 2: Statistics on data and results

a large-scale multilingual semantic network2, and
the Wikipedia Category graph.

3 Discussion

To conduct our study, we created three datasets:
Wikipedia PageViews (W), On-line News (N) and
Twitter messages (T). Data was collected during
4 months from June 1st, 2014 to September 30th.
Table 2 shows some statistics. Note that Wikipedia
clusters are smaller, since cluster members are
only named entities (page views).
We defined the following evaluation framework:

i) Given an event ei and related news ni ∈ Ni, we
generate recommendations as explained in Step
4, in a selected interval prior to the day of the
query. ii) Automated evaluation: we we select the
top K scored recommendations and measure the
saliency of Rin newsi and serendipity of R

novel
i in

an automated fashion, and we compare the perfor-
mance against a primitive recommender, in anal-
ogy with (Murakami et al., 2008) and (Ge et al.,
2010); ii) Manual evaluation: we select the top K
scored recommendations in Rnoveli for a restricted
number of 21 high-impact world-wide events, and
we perform manual evaluation using the Crowd-
flower.com platform, providing detailed evalua-
tion guidelines for human annotators. Using this
ground truth, we measure the global serendipity of
W 3 recommendations.

2http://babelnet.org/about

21



3.1 Automated Evaluation

We first build two primitive recommenders (PRs)
for Wikipedia and Twitter, which we use as a base-
line. The input to a PR is the same as for W 3 (see
Step 4).
Wikipedia PR: The Wikipedia PR is based on find-
ing connected components of the Wikipedia hy-
perlink page graph (like in (Hu et al., 2009)), when
considering only the topmost visited pages in a
temporal slot. More precisely, for each day d in
the interval I ′ : d0−x ≤ d ≤ d0+x 3, we se-
lect the top H ≥ K visited named entities of
the day EWd . Entities are ranked by frequency of
page views4. Next, we create clusters cdj obtained
by extracting the connected components of EWd in
the Wikipedia hyperlink graph. Let CI

′
be the set

of all clusters cI
′

j in I
′. From this set, we select

the top r clusters based on the Jaccard similarity
with news meta-clusters mNi . A ”primitive” rec-
ommendation for event ei on day d0+x is the set
PRWi of topmost K ranked entities in the r pre-
viously selected clusters. Like in W 3 recommen-
dations, PRWi is a ranked list of entities some of
which are also found in mNi , and some others are
novel.
Twitter PR: For each entity e ∈ mNi we retrieve
and recommend the top K co-occurring entities in
tweets in the considered interval.

Note that both primitive recommenders are far
from being naive. A hyperlink graph to character-
ize users’ intent in Wikipedia search is used in (Hu
et al., 2009) (although the authors use Random
Walks rather than connected components analy-
sis to identify related pages). Co-occurrences with
top ranked news terms has been used in (Weiler
et al., 2014) to track on Twitter the evolution and
the context around events. We generate recom-
mendations using four systems: W 3(T ), W 3(W ),
PR(T ) and PR(W ). The first two originate from
What To Write and Why when applied to Twitter
and Wikipedia, respectively. The second two are
generated by the two primitive recommenders de-
scribed above. For all systems, we consider the
first K top ranked entities, as we said.

To assess the quality of ”not novel” recom-
mended entities in W 3 (and similarly for the other
systems), for any rj ∈ Rin newsi we retrieve all the

3Since rumors on an event can be anticipated wrt the day
d0 in which the first news item is published

4Note that EWd could be straightly used for recommenda-
tion, however it would be an excessively rough strategy.

news Ni related to mNi meta-clusters, and com-
pute the saliency of rj as follows:

saliency (rj , ni) = β × occtitle (rj , ni) + (1− β)× occsnip (rj , ni)
(1)

where ni ∈ Ni, occtitle(rj , ni) is the number
of occurrences of rj in the title of ni, while
occsnip(rj , ni) is the number of occurrences of rj
in the text snippet of ni and β has been experimen-
tally set to 0.7. The intuition is that recommended
entities in Rin newsi are salient if they frequently
occur in the title and text of news snippets, where
occurrences in the title have a higher weight. The
total saliency of rj is then:

saliency (rj) =
∑

ni∈Ni saliency(rj ,ni)
|Ni| × IDF (rj)

(2)

where IDF (rj) is the inverse document fre-
quency of rj in all news of the considered tem-
poral slot, and is used to smooth the relevance of
terms with high probability of occurrence in all
documents. The average saliency of Rin newsi is:

saliency
(
Rin newsi

)
=

∑
rj∈Rin newsi

saliency(rj)

|Rin newsi | (3)

To provide an estimate of the serendipity
of novel recommendations, we compute the
NASARI similarity (Camacho-Collados et al.,
2016) of entities rk ∈ Rnoveli with in-news entities
rj ∈ ENi and we weight these values with the
saliency of rj . The intuition is that serendipitous
recommendations are those concerning topics
which have not been discussed so far in on-line
news, but are highly semantically related with
highly salient topics in news:

serend.
(
rk ∈ Rnoveli

)
=

∑
rk∈Rnoveli ,rj∈ENi

(NASARI(rk,rj)×saliency(rj))
|RSi |

(4)

Note that this formulation is not conceptually
different from other measures used in literature
(e.g, (Tran et al., 2015),(Murakami et al., 2008)),
that commonly assign a value to novel recom-
mendations proportionally to their relevance and
informativeness, however given the absence of
prior knowledge on users’ choices, we assume that
semantic similarity with salient entities in news
items is a clue for relevance.

In Table 3 we summarize the results of our ex-
periments, that we run over the full dataset (see

22



Table 3: Percentage difference in performances between W 3
and PRs on Twitter and Wikipedia

Source Saliency Serendipity F-Value
Twitter d0 -28% +91% +15%
Wikipedia d0 +172% +656% +371%
Twitter d2 -34% +81% +8%
Wikipedia d2 +106% +547% +286%

Table 2). We set the maximum number of pro-
vided recommendations K = 10 for Wikipedia
(where clusters are smaller) and K = 50 for
Twitter. All recommendations are gathered ei-
ther the same day (d0) of the first news item on
the event ei, or two days after (d2 = d0 + 2).
In analogy with (Murakami et al., 2008) and (Ge
et al., 2010), we show the percentage difference in
performance between W 3 and Primitive Recom-
menders (PRs). Besides saliencey and serendip-
ity, we also compute the harmonic mean between
the two (the F value). The Table shows that
for Wikipedia, W 3 outperforms the PR both in
saliency and serendipity (it is up to 656% more
serendipitous than the baseline) while in Twitter,
W 3 shows better serendipity (+91%) but lower
salience (-28%). Comparatively, the performance
of W 3 is much better on Wikipedia than on Twit-
ter, probably due to the limited evidence provided
by the 1% available traffic. We also noted that two
days after the main event (x=2), both serendipity
and saliency only slightly decrease showing that
newswires have covered only a small portion of
users’ communication and information needs.

3.2 Manual Evaluation

In manual evaluation, in order to start from a clean
representation of each event for all systems, we se-
lected 21 relevant (with topmost number of news,
tweets and wikipedia views) events in the consid-
ered 4-months period, and we manually identified
the relevant news items Ni for each event ei in a
± 1-day interval around the event peak day d0. An
excerpt of 5 events is shown in Table 4. We then
automatically extracted named entities from these
news items.

For each of the four systems W 3(T ), W 3(W ),
PR(T ) and PR(W ) and each event ei, we gen-
erate the first K = 5 novel recommendations,
and we use the CrowdFlower.com platform to as-
sess the relevance of these recommendations5. For
each item of news, annotators are asked to decide

5The saliency of Rin newsi is well assessed by formula (2)

Table 4: Excerpt of selected events

Date Event
11/06/2014 Al-Qaeda Faction Seizes Key Iraqi City
14/06/2014 England vs. Italy at the 2014 World

Cup
30/06/2014 Limiting Rights: Imposing Religion on

Workers
05/07/2014 Wimbledon: Novak Djokovic

and Roger Federer Reach Men’s Final
. . .

22/09/2014 Nasa’s Newest Mars Mission Space-
craft Enters Orbit Around Mars

if an entity IS or IS NOT relevant with reference
to the reported news (”not sure” is also allowed).
”Relevant” means that either the entity is semanti-
cally related to the domain of the news, or that it is
factually related. The task was run on April 23rd,
2017, and we collected 1344 total judgements. To
compute the performance of each system, we use
the Mean Average Precision (MAP)6, which takes
into account the rank of recommendations. The re-
sults are shown in Table 5, which shows, in agree-
ment with the automated evaluation of Table 3, a
superiority ofW 3 and also confirms that the differ-
ence between W 3 and the primitive recommender
is much higher in Wikipedia than in Twitter. We
also note that the absolute performance of the rec-
ommender is higher in Twitter, which is not in
contradiction with Table 3, since here we are fo-
cusing on world-wide high impact news, those for
which our 1% Twitter stream provides sufficient
evidence to obtain clean clusters, such as those in
Table 1.

3.3 Analysis of Information Needs

To analyze readers’ behavior more systematically,
we classified events meta-clusters automatically,
extending the work in (Košmerlj et al., 2015),
were the authors have manually classified 13,883
Wikipedia event-related articles in 9 categories.
Furthermore, we classified recommendations, i.e.,
tokens in mTi and m

W
i meta-clusters associated to

each event ei, using BabelNet hypernymy (IS A)
relations7, and their mapping onto Wikipedia Cat-
egories. In Figure 4 we plot the category dis-
tribution of Wikipedia articles (more specifically,
we plot only novel recommendations extracted
by W 3) that readers have accessed in correspon-
dence of different event types. The Bubble plot
shows several interesting patterns: for example,

6https://www.kaggle.com/wiki/MeanAveragePrecision
7http://babelnet.org/about

23



Source W3 BR
Twitter 0.934 0.851
Wikipedia 0.789 0.363

Table 5: MAP (mean average precision) of compared sys-
tems in Crowdflower.com evaluation (on a sample of 21
breakings news)

Technology	

Television	

Actor	

M
usic	

Video	G
am

e	

Character	

G
eographic	Area	

Event	

Accident	

Corpora;on	

W
ar	

Religion	

Science	

Poli;cs	

Recurring	Events	

Terrorism
	

Literature	

M
odel	(person)		

Docum
ent	

Sport	

Racing	

AircraD
	

Businessm
an	

Ar;st	

M
ilitary	O

pera;on	

Armed	conflicts	and	
aHacks	

Arts	and	culture	

Business	and	economy	

Disasters	and	accidents	

Health	and	environment	

Law	and	crime	

Poli;cs	and	elec;ons	

Science	and	technology	

Sport	

Figure 4: Bubble plot of event categories and associated in-
formation needs (during Summer 2014)

Religion is the main searched category for events
classified as Armed Conflicts and Attacks, mir-
roring the fact that religion is perceived as be-
ing highly related with latest world-wide conflicts.
Accordingly, users try to deepen their knowledge
on these aspects. Disasters and accidents mostly
include members in the same Wikipedia category
(Disasters) and also Aircraft, since the Malaysia
crash was the dominating event in the considered
period. Business and Economy draw the atten-
tion of readers mostly when related to Technol-
ogy, e.g., new devices being launched. Law and
Crime events induce in readers the need to find
out more about specific laws and treaties (the cat-
egory Documents). Finally, we note that Sport is
the event category showing the highest dispersion
of information needs. While many of the bubbles
in Figure 4 indeed show real information needs
(e.g, VideoGames refers to the many sport games
launched on the market, Model (person) refers to
gossip about football players, and in general all
people and media related categories refer to the
participation of celebrities in sporting events), a
number of bubbles can be considered as noise,
e.g., Literature, Politics. In fact, Sport was the
dominating event type during the considered pe-
riod (2014 World Football Cup), therefore it is rea-
sonable that sport-related clusters are those cumu-
lating the highest number of system errors.

References
R. Brooker and T. Schaefer. 2005. Public Opinion in

the 21st Century: Let the People Speak?. New di-
rections in political behavior series. Houghton Mif-
flin Company.

J. Camacho-Collados, M. Taher Pilehvar, and R. Nav-
igli. 2016. Nasari: Integrating explicit knowledge
and corpus statistics for a multilingual representa-
tion of concepts and entities. Artificial Intelligence
240:36–64.

M. Ge, C. Delgado-Battenfeld, and D. Jannach. 2010.
Beyond accuracy: Evaluating recommender systems
by coverage and serendipity. In Proc. of RecSys’10.
pages 257–260.

P. J. Gloviczki. 2015. Journalism in the Age of Social
Media, Palgrave Macmillan US, pages 1–23.

J. Hu, G. Wang, F. Lochovsky, J. Sun, and Z. Chen.
2009. Understanding user’s query intent with
wikipedia. In Proc. of WWW’09. pages 471–480.

M. Knight. 2012. Journalism as usual: The use of so-
cial media as a newsgathering tool in the coverage
of the iranian elections in 2009. Journal of Media
Practice 13(1):61–74.

A. Košmerlj, E. Belyaeva, G. Leban, M. Grobelnik, and
B. Fortuna. 2015. Towards a complete event type
taxonomy. In Proc. of WWW’15. pages 899–902.

J. Lehmann, B. Gonçalves, J. J. Ramasco, and C. Cat-
tuto. 2012. Dynamical classes of collective attention
in twitter. In Proc. of WWW’12. pages 251–260.

J. Leskovec, L. Backstrom, and J. Kleinberg. 2009.
Meme-tracking and the dynamics of the news cycle.
In Proc. of KDD ’09. pages 497–506.

T. Murakami, K. Mori, and R. Orihara. 2008. Met-
rics for evaluating the serendipity of recommenda-
tion lists. In Proc. of the 2007 Conf. on New Fron-
tiers in AI. Springer, pages 40–46.

R. Navigli and S. P. Ponzetto. 2010. BabelNet: Build-
ing a very large multilingual semantic network. In
Proc. of the 48th Annual Meeting of the ACL. pages
216–225.

G. Stilo and P. Velardi. 2016. Efficient temporal mining
of micro-blog texts and its application to event dis-
covery. Data Min. Knowl. Discov. 30(2):372–402.

T. Tran, C. Niedere, N. Kanhabua, U. Gadiraju, and
A. Anand. 2015. Balancing novelty and salience:
Adaptive learning to rank entities for timeline sum-
marization of high-impact events. In Proc. of
CICM’15. Springer, volume 19, pages 1201–1210.

A. Weiler, M. Grossniklaus, and M.H. Scholl. 2014.
Event identification and tracking in social media
streaming data. In Proc. of the Work. of the
EDBT/ICDT’14. CEUR-WS, pages 282–287.

24


