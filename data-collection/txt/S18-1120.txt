



















































Yuanfudao at SemEval-2018 Task 11: Three-way Attention and Relational Knowledge for Commonsense Machine Comprehension


Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 758–762
New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics

Yuanfudao at SemEval-2018 Task 11: Three-way Attention and Relational
Knowledge for Commonsense Machine Comprehension

Liang Wang Meng Sun Wei Zhao Kewei Shen Jingming Liu
Yuanfudao Research

Beijing, China
{wangliang01,sunmeng,zhaowei01,shenkw,liujm}@fenbi.com

Abstract

This paper describes our system for SemEval-
2018 Task 11: Machine Comprehension
using Commonsense Knowledge (Oster-
mann et al., 2018b). We use Three-
way Attentive Networks (TriAN) to model
interactions between the passage, question
and answers. To incorporate commonsense
knowledge, we augment the input with re-
lation embedding from the graph of gen-
eral knowledge ConceptNet (Speer et al.,
2017). As a result, our system achieves
state-of-the-art performance with 83.95% ac-
curacy on the official test data. Code is pub-
licly available at https://github.com/
intfloat/commonsense-rc.

1 Introduction

It is well known that humans have a vast amount of
commonsense knowledge acquired from everyday
life. For machine reading comprehension, natu-
ral language inference and many other NLP tasks,
commonsense reasoning is one of the major obsta-
cles to make machines as intelligent as humans.

A large portion of previous work focus on com-
monsense knowledge acquisition with unsuper-
vised learning (Chambers and Jurafsky, 2008;
Tandon et al., 2017) or crowdsourcing approach
(Singh et al., 2002; Wanzare et al., 2016). Con-
ceptNet (Speer et al., 2017), WebChild (Tan-
don et al., 2017) and DeScript (Wanzare et al.,
2016) etc are all publicly available knowledge re-
sources. However, resources based on unsuper-
vised learning tend to be noisy, while crowdsourc-
ing approach has scalability issues. There is also
some research on incorporating knowledge into
NLP tasks such as reading comprehension (Lin
et al., 2017; Yang and Mitchell, 2017) neural ma-
chine translation (Zhang et al., 2017a) and text
classification (Zhang et al., 2017b) etc. Though

experiments show performance gains over base-
lines, these gains are often quite marginal over
the state-of-the-art system without external knowl-
edge.

In this paper, we present Three-way Attentive
Networks(TriAN) for multiple-choice common-
sense reading comprehension. The given task re-
quires modeling interactions between the passage,
question and answers. Different questions need to
focus on different parts of the passage, attention
mechanism is a natural choice and turns out to be
effective for reading comprehension. Due to the
relatively small size of training data, TriAN use
word-level attention and consists of only one layer
of LSTM (Hochreiter and Schmidhuber, 1997).
Deeper models result in serious overfitting and
poor generalization empirically.

To explicitly model commonsense knowledge,
relation embeddings based on ConceptNet (Speer
et al., 2017) are used as additional features.
ConceptNet is a large-scale graph of general
knowledge from both crowdsourced resources and
expert-created resources. It consists of over 21
million edges and 8 million nodes. ConceptNet
shows state-of-the-art performance on tasks like
word analogy and word relatedness.

Besides, we also find that pretraining our net-
work on other datasets helps to improve the overall
performance. There are some existing multiple-
choice English reading comprehension datasets
contributed by NLP community such as MCTest
(Richardson et al., 2013) and RACE (Lai et al.,
2017). Although those datasets don’t focus specif-
ically on commonsense comprehension, they pro-
vide a convenient way for data augmentation.
Augmented data can be used to learn shared regu-
larities of reading comprehension tasks.

Combining all of the aforementioned tech-
niques, our system achieves competitive perfor-
mance on the official test set.

758



Related_to

At_location

passage question answer

BiLSTM BiLSTM BiLSTM

y

self-attentionself-attention

seq attentiontransport
bus

bus stop
person

passenger

Used_for

Type_of

query

ConceptNet

p q a

Figure 1: TriAN Model Architecture.

2 Model

The overall architecture of TriAN is shown in Fig-
ure 1. It consists of an input layer, an attention
layer and an output layer.
Input Layer. A training example consists of a
passage {Pi}|P |i=1, a question {Qi}

|Q|
i=1, an answer

{Ai}|A|i=1 and a label y∗ ∈ {0, 1} as input. P , Q
and A are all sequences of word indices. For a
word Pi in the given passage, the input represen-
tation of Pi is the concatenation of several vectors:

• GloVe embeddings. Pretrained 300-
dimensional GloVe vector EglovePi .

• Part-of-speech and named-entity embed-
dings. Randomly initialized 12-dimensional
part-of-speech embedding EposPi and 8-
dimensional named-entity embedding EnerPi .

• Relation embeddings. Randomly initialized
10-dimensional relation embedding ErelPi . The
relation is determined by querying ConceptNet
whether there is an edge between Pi and any
word in question {Qi}|Q|i=1 or answer {Ai}

|A|
i=1. If

there exist multiple different relations, just ran-
domly choose one.

• Handcrafted features. We also add logarith-
mic term frequency feature and co-occurrence
feature fPi . Term frequency is calculated based
on English Wikipedia. Co-occurrence feature is
a binary feature which is true if Pi appears in
question {Qi}|Q|i=1 or answer {Ai}

|A|
i=1.

The input representation for Pi is wPi :

wPi = [E
glove
Pi

;EposPi ;E
ner
Pi ;E

rel
Pi ; fPi ] (1)

In a similar way, we can get input representation
for question wQi and answer wAi .
Attention Layer. We use word-level attention
to model interactions between the given passage
{Pi}|P |i=1, the question {Qi}

|Q|
i=1 and the answer

{Ai}|A|i=1. First, let’s define a sequence attention
function (Chen et al., 2017):

Attseq(u, {vi}ni=1) =
n∑

i=1

αivi

αi = softmaxi(f(W1u)
T f(W1vi))

(2)

u and vi are vectors and W1 is a matrix. f is a
non-linear activation function and is set to ReLU .

Question-aware passage representa-
tion {wqPi}

|P |
i=1 can be calculated as:

wqPi = Attseq(E
glove
Pi

, {EgloveQi }
|Q|
i=1). Similarly,

we can get passage-aware answer representation
{wpAi}

|A|
i=1 and question-aware answer representa-

tion {wqAi}
|A|
i=1. Then three BiLSTMs are applied

to the concatenation of those vectors to model the
temporal dependency:

hq = BiLSTM({wQi}
|Q|
i=1)

hp = BiLSTM({[wPi ;wqPi ]}
|P |
i=1)

ha = BiLSTM({[wAi ;wpAi ;w
q
Ai
]}|A|i=1)

(3)

hp,hq,ha are the new representation vectors
that incorporates more context information.
Output Layer. Question sequence and answer
sequence representation hq,ha are summarized
into fixed-length vectors with self-attention (Yang
et al., 2016). Self-attention function is defined as

759



follows:

Attself ({ui}ni=1) =
n∑

i=1

αiui

αi = softmaxi(W
T
2 ui)

(4)

Then we have question representation q =
Attself ({hqi }

|Q|
i=1), answer representation a =

Attself ({hai }
|A|
i=1) and passage representation p =

Attseq(q, {hpi }
|P |
i=1). The final output y is based on

their bilinear interactions:

y = σ(pTW3a+ q
TW4a) (5)

Model Learning. We first pretrain TriAN on
RACE dataset for 10 epochs. Then our model is
fine-tuned on official training data. Standard cross
entropy function is used as the loss function to
minimize.

3 Experiments

3.1 Setup

Data. For data preprocessing, we use spaCy 1 for
tokenization, part-of-speech tagging and named-
entity recognition. Statistics for official dataset
MCScript (Ostermann et al., 2018a) are shown
in Table 1. RACE 2 dataset is used for net-
work pretraining. English stop words are ignored
when computing handcrafted features. Input word
embeddings are initialized with 300-dimensional
GloVe (Pennington et al., 2014) vectors 3.

train dev test
# of examples 9731 1411 2797

Table 1: Official dataset statistics.

Hyperparameters. Our model TriAN is imple-
mented based on PyTorch 4. Models are trained
on a single GPU(Tesla P40) and each epoch takes
about 80 seconds. Only the word embeddings of
top 10 frequent words are fine-tuned during train-
ing. The dimension of both forward and back-
ward LSTM hidden state is set to 96. Dropout rate
is set to 0.4 for both input embeddings and BiL-
STM outputs (Srivastava et al., 2014). For param-
eter optimization, we use Adamax (Kingma and
1https://github.com/explosion/spaCy
2http://www.cs.cmu.edu/˜glai1/data/race/
3http://nlp.stanford.edu/data/glove.
840B.300d.zip

4http://pytorch.org/

Ba, 2014) with an initial learning rate 2 × 10−3.
Learning rate is then halved after 10 and 15 train-
ing epochs. The model converges after 50 epochs.
Gradients are clipped to have a maximum L2 norm
of 10. Minibatch with batch size 32 is used.
Hyperparameters are optimized by random search
strategy (Bergstra and Bengio, 2012). Our model
is quite robust over a wide range of hyperparame-
ter configurations.

3.2 Main Results
The experimental results are shown in Table 2.
Human performance is shared by task organiz-
ers. For TriAN-ensemble, we average the out-
put probabilities of 9 models trained with the
same datasets and network architecture but differ-
ent random seeds. TriAN-ensemble is the model
that we used for official submission.

model dev test
Random 50.00% 50.00%
TriAN-RACE 64.78% 64.28%
TriAN-single 83.84% 81.94%
TriAN-ensemble 85.27% 83.95%
HFL – 84.13%
Human – 98.00%

Table 2: Main results. TriAN-RACE only use RACE
dataset for training; HFL is the 1st place team for
SemEval-2018 Task 11. The evaluation metric is ac-
curacy.

From Table 2, we can see that even though
RACE dataset contains nearly 100k questions,
TriAN-RACE achieves quite poor results. The ac-
curacy on development set is only 64.78%, which
is worse than most participants’ systems. How-
ever, pretraining acts as a way of implicit knowl-
edge transfer and is beneficial for overall perfor-
mance, as will be seen in Section 3.3. The accu-
racy of our system TriAN-ensemble is very close to
the 1st place team HFL with 0.18% difference. Yet
there is still a large gap between machine learning
models and human.

We also compared the performances of shal-
low and deep TriAN models. On datasets such as
SQuAD (Rajpurkar et al., 2016), deep models typ-
ically works better than shallow ones. Notice that
the attention layer in our proposed TriAN model
can be stacked multiple times if we treat the output
vectors of BiLSTMs as new input representations.

Maybe a little bit surprising, Table 3 shows that
2-layer TriAN model performs worse than 1-layer

760



model dev test
1-layer TriAN-single 83.84% 81.94%
2-layer TriAN-single 82.71% 80.55%

Table 3: Accuracy comparison of shallow and deep
TriAN models.

TriAN. One possible explanation is that the labeled
dataset is relatively small and deeper models tend
to easily overfit.

3.3 Ablation Study
The input representation consists of several com-
ponents: part-of-speech embedding, relation em-
bedding and handcrafted features etc. We conduct
an ablation study to investigate the effects of each
component. The results are in Table 4.

model dev test
TriAN-single 83.84% 81.94%
w/o pretraining 82.71% 80.51%
w/o ConceptNet 82.78% 81.08%
w/o POS 82.84% 81.27%
w/o features 82.92% 81.35%
w/o NER 83.60% 81.87%

Table 4: Ablation study for input representation.

Pretraining on RACE dataset turns out to be the
most important factor. Without pretraining, the ac-
curacy drops by more than 1% on both develop-
ment and test set. Relation embeddings based on
ConceptNet make approximately 1% difference.
Part-of-speech and named-entity embeddings are
also helpful. In fact, combining input represen-
tations from multiple sources has been a standard
practice for reading comprehension tasks.

At attention layer, our proposed TriAN involves
applying several attention functions to model in-
teractions between different text sequences. It
would be interesting to examine the importance of
each attention function, as shown in Table 5.

model dev test
TriAN-single 83.84% 81.94%
w/o passage-question attention 83.51% 82.20%
w/o passage-answer attention 83.07% 81.39%
w/o question-answer attention 83.23% 81.84%
w/o attention 81.93% 80.65%

Table 5: Ablation study for attention. The last one “w/o
attention” removes all word-level attentions.

Interestingly, removing any of the three word-

level sequence attentions does not seem to hurt
the performance much. In fact, removing passage-
question attention even results in higher accuracy
on test set than TriAN-single. However, if we re-
move all word-level attentions, the performance
drastically drops by 1.9% on development set and
1.3% on test set.

3.4 Discussion

Even though our system is built for commonsense
reading comprehension, it doesn’t have any ex-
plicit knowledge reasoning component. Relation
embeddings based on ConceptNet only serve as
additional input features. Methods like event cal-
culus (Mueller, 2014) are more rigorous math-
ematically and resemble the way of how human
brain works. The problem of event calculus is that
it requires large amounts of domain-specific ax-
ioms and therefore doesn’t scale well.

Another limitation is that our system relies
on hard-coded commonsense knowledge bases,
just like most systems for commonsense reason-
ing. For humans, commonsense knowledge comes
from constant interactions with the real-world en-
vironment. From our point of view, it is quite
hopeless to enumerate all of them.

There are a lot of reading comprehension
datasets available. When the size of training data
is relatively small like this SemEval-2018 task,
transfer learning among different datasets is a use-
ful technique. This paper shows that pretraining
is a simple and effective method. However, it still
remains to be seen whether there is a better alter-
native approach.

4 Conclusion

In this paper, we present the core ideas and de-
sign philosophy for our system TriAN at SemEval-
2018 Task 11: Machine Comprehension using
Commonsense Knowledge. We build upon recent
progress on neural models for reading compre-
hension and incorporate commonsense knowledge
from ConceptNet. Pretraining and handcrafted
features are also proved to be helpful. As a result,
our proposed model TriAN achieves near state-of-
the-art performance.

Acknowledgements

We would like to thank SemEval 2018 task orga-
nizers and several anonymous reviewers for their
helpful comments.

761



References
James Bergstra and Yoshua Bengio. 2012. Random

search for hyper-parameter optimization. Journal of
Machine Learning Research, 13(Feb):281–305.

Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. Pro-
ceedings of ACL-08: HLT, pages 789–797.

Danqi Chen, Adam Fisch, Jason Weston, and An-
toine Bordes. 2017. Reading wikipedia to an-
swer open-domain questions. arXiv preprint
arXiv:1704.00051.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard Hovy. 2017. Race: Large-scale reading
comprehension dataset from examinations. arXiv
preprint arXiv:1704.04683.

Hongyu Lin, Le Sun, and Xianpei Han. 2017. Rea-
soning with heterogeneous knowledge for common-
sense machine comprehension. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2032–2043.

Erik T Mueller. 2014. Commonsense reasoning: an
event calculus based approach. Morgan Kaufmann.

Simon Ostermann, Ashutosh Modi, Michael Roth, Ste-
fan Thater, and Manfred Pinkal. 2018a. MCScript:
A Novel Dataset for Assessing Machine Compre-
hension Using Script Knowledge. In Proceedings
of the 11th International Conference on Language
Resources and Evaluation (LREC 2018), Miyazaki,
Japan.

Simon Ostermann, Michael Roth, Ashutosh Modi, Ste-
fan Thater, and Manfred Pinkal. 2018b. SemEval-
2018 Task 11: Machine Comprehension using Com-
monsense Knowledge. In Proceedings of Interna-
tional Workshop on Semantic Evaluation(SemEval-
2018), New Orleans, LA, USA.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250.

Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. Mctest: A challenge dataset for
the open-domain machine comprehension of text.

In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
193–203.

Push Singh, Thomas Lin, Erik T Mueller, Grace Lim,
Travell Perkins, and Wan Li Zhu. 2002. Open mind
common sense: Knowledge acquisition from the
general public. In OTM Confederated International
Conferences” On the Move to Meaningful Internet
Systems”, pages 1223–1237. Springer.

Robert Speer, Joshua Chin, and Catherine Havasi.
2017. Conceptnet 5.5: An open multilingual graph
of general knowledge. In AAAI, pages 4444–4451.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Niket Tandon, Gerard de Melo, and Gerhard Weikum.
2017. Webchild 2.0: fine-grained commonsense
knowledge distillation. Proceedings of ACL 2017,
System Demonstrations, pages 115–120.

Lilian DA Wanzare, Alessandra Zarcone, Stefan
Thater, and Manfred Pinkal. 2016. Descript: A
crowdsourced corpus for the acquisition of high-
quality script knowledge. In The International Con-
ference on Language Resources and Evaluation.

Bishan Yang and Tom Mitchell. 2017. Leveraging
knowledge bases in lstms for improving machine
reading. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 1436–
1446.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1480–1489.

Jiacheng Zhang, Yang Liu, Huanbo Luan, Jingfang Xu,
and Maosong Sun. 2017a. Prior knowledge integra-
tion for neural machine translation using posterior
regularization. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
1514–1523.

Ye Zhang, Matthew Lease, and Byron C Wallace.
2017b. Exploiting domain knowledge via grouped
weight sharing with application to text categoriza-
tion. arXiv preprint arXiv:1702.02535.

762


