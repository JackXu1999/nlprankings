



















































Playing with Embeddings : Evaluating embeddings for Robot Language Learning through MUD Games


Proceedings of the 2nd Workshop on Evaluating Vector-Space Representations for NLP, pages 27–30,
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Playing with Embeddings : Evaluating embeddings for Robot Language
Learning through MUD Games

Anmol Gulati∗ and Kumar Krishna Agrawal∗
anmol01gulati@gmail.com, kumarkagrawal@gmail.com

Indian Institute of Technology Kharagpur

Abstract

Acquiring language provides a ubiquitous
mode of communication, across humans
and robots. To this effect, distributional
representations of words based on co-
occurrence statistics, have provided sig-
nificant advancements ranging across ma-
chine translation to comprehension. In this
paper, we study the suitability of using
general purpose word-embeddings for lan-
guage learning in robots. We propose us-
ing text-based games as a proxy to evaluat-
ing word embedding on real robots. Based
in a risk-reward setting, we review the
effectiveness of the embeddings in navi-
gating tasks in fantasy games, as an ap-
proximation to their performance on more
complex scenarios, like language assisted
robot navigation.

1 Introduction

Language provides a natural interface for humans
to communicate with robots. With their increasing
public presence, from self-driving cars and rescue
operations to warehouses, it is imperative to re-
duce this barrier of communication, by improving
language learning in mobile robots. For instance,
in search and rescue operations one might want to
instruct the agents to ”Reach the third floor of the
blue building”. Given the nature of the tasks, it is
essential for the agent to accurately infer its cur-
rent state and parse natural language instructions
to corresponding actions.

In recent years, learning continuous represen-
tations of words and symbols have become cen-
tral to dealing with several key problems in natural

∗Equally contributed to the project. Author names listed
in alphabetical order.

Figure 1: An example of environment generated
in the MUD environment. While the blue blob de-
notes the start state, the red blobs denote deter-
ministic intermediate negative rewards. The green
blob on the right denotes positive reward on com-
pletion of quest.

language understanding. Usually trained to maxi-
mize the likelihood of next utterance, these models
effectively learn co-occurrence statistics based on
corpora, motivated by the distributional hypothe-
sis (Harris, 1954). This objective function, often
organizes objects and actions with similar seman-
tic information, to close neighborhoods in the cor-
responding embedding space. Here, the similarity
between words is often defined by some metric for
similarity of the word vectors.

Though evidently successful across several
tasks, common general purpose word embeddings
with fixed dimensions are often inadequate in their
representation capacity. In recent works, (Gau-
thier and Mordatch, 2016; Lucy and Gauthier,
2017) argue that language learning grounded in
perception, motion or actions, are important to
learn meaningful representations corresponding to
consequences of actions on objects. Specifically,
in a scenario where robots are deployed in real
world, it is important to distinguish even between

27



Fantasy Environment
You are standing very close to the bridges
eastern foundation. If you go east you will
be back on solid ground ...
The bridge sways in the wind.

MARCO dataset (Chen and Mooney, 2011)
face the octagon carpet. move until you
see red brick floor to your right.
turn and walk down the red brick until you get
to an alley with grey floor. you should be two alleys
away from a lamp and then an easel beyond that.

Table 1: Examples of instructions from MUD vs
MARCO dataset for navigation.

similar actions based context and their effect in a
given situation.

In this paper, we address this task of evaluating
word embeddings for language learning in robots
by introducing text-based MUD games (Curtis,
1992) as a proxy. Traditionally in robotics, elab-
orate simulators have been used to evaluate motor
control policies of motion planning for the physi-
cal agents. In a similar spirit, MUD games provide
a rich playground for define complex scenarios
solely with textual descriptions. Based on these,
the agents are required to take certain actions with
the objective of clearing the quest. (Fig. 1)

To evaluate, we train a LSTM-DQN follow-
ing (Narasimhan et al., 2015), in a reinforcement
learning framework. The agent is trained to learn
control policies, with the objective of maximizing
the rewards obtained. We demonstrate that using
general purpose embeddings improve on the aver-
age rewards per quest.

2 Multi-User Dungeon Games

A Multi-User Dungeon Game, as defined in
(Curtis, 1992) ”is a network-accessible, multi-
participant, user-extensible virtual reality whose
user interface is entirely textual”. In essence, it
describes an elaborate environment with multiple
characters and tasks for the player, the only con-
straint being that all interactions with the game are
purely textual.

In (Amir and Doyle, 2002), the authors elabo-
rately elucidate the richness of such environments,
with insights specific to robotics. Primarily in our
context of robot language learning, we make the
following observations :

• The environment is partially observable, with

stochasticity depending on the actions taken
by the agent. This inherently allows us to cast
the problem as an MDP, and studying it in the
light of recent advances in Deep Reinforce-
ment Learning.

• Intermediate rewards provide feedback to the
agent, as proxy for a human in a real-life set-
ting. This allows the agent to learn from con-
sequences and formulating the problem in a
risk-reward setting.

• Language based symbolic interface for inter-
action with the environment. Since language
remains consistent across simulator and real
world, we assume that the text-world is an ac-
curate replica of the real environment.

Among others, the above characteristics suggest
that MUD games provide an ideal sandbox for
evaluating word-embeddings for robot language
learning.

3 Dataset

We propose using text-based MUD games as
proxy for evaluating language learning in robotics.
As illustrated in (Table 1) the nature of instructions
generated by the game environment are quite sim-
ilar when compared to real datasets, in this case
(Chen and Mooney, 2011).

In this work, we use the Evennia1game environ-
ment, an open-source library to build online tex-
tual MUD games. Evennia allows users to cre-
ate complex environments with elaborate textu-
ral descriptions, by simply writing a batch file to
describe the objects, actions and possible inter-
actions in the environment. Compared to other
available datasets for navigation, the MUD envi-
ronment provides a risk-reward scenario, where
the environment can also be designed to have de-
terministic negative feedback to represent undesir-
able outcomes.

To provide feedback on the action taken, the
agents are provided positive/negative rewards de-
pending on the state of the game. In case of quest
completion, the agents are provided a large pos-
itive reward, while predefined bad intermediate
checkpoints like colliding with walls, or falling
off bridges are penalized with negative rewards.
Building on extensive literature in reward shaping
in the Deep Reinforcement Learning framework,

1http://www.evennia.com/

28



0 20 40 60 80 100
Epochs

20

15

10

5

0

R
e
w

a
rd

 (
lo

g
 s

ca
le

)

BOW-DQN

LSTM-DQN

Glove

Word2Vec

Figure 2: Evaluating agents trained across various
embeddings based on average reward (log scale).
Pretrained word-vectors outperform random ini-
tialized LSTM-DQN and BOW-DQN.

more complex environment can also be easily con-
structed to evaluate the generalizing capability of
the embeddings.

4 Model Architecture

Formulating the problem in a reinforcement learn-
ing setting, we train the agent to learn a good Q-
value functionQ(s, a), for all possible state-action
pairs. More concretely the agent iteratively opti-
mizes the Bellman objective

Qi+1(s, a) = E[r + γmax
a′

Qi(s′, a′)|s, a] (1)

where, γ is the discount factor. With the changing
distribution on Q-values the agent decides action
a to maximize the future rewards.

In this regard, we follow the LSTM-DQN
model by (Narasimhan et al., 2015) which builds
on recent developments in Deep Q-Learning
(Mnih et al., 2015).

4.1 LSTM-DQN

The input to the agent is the textual description
of the current state of the environment. However,
the model is required to keep a track over previous
states, as well generate a compact representation
for the same. Therefore we define a representa-
tion generator (φR), which is an LSTM module
to keep a track of the current state of the environ-
ment. The individual time step rollouts are further
aggregated (φA) to convert the textual description
into compact vector representation (s).

Following the context, the model is required to
choose an action(a) to be taken, and an object(o)

on which the action is applied in the MUD envi-
ronment. As a result, we define two Q-value func-
tions Q(s, a) and Q(s, o), which share the same
state, and generate distributions on actions and ob-
jects respectively. This (a, o) tuple, together de-
fines the action executed by the game environment
to update it’s state. For detailed description of the
model please refer (Narasimhan et al., 2015).

5 Evaluation

We experiment on the Fantasy World following
the environment in (Narasimhan et al., 2015). The
vocabulary consists of 1340 unique tokens, with
100 different descriptions for the room. Visiting
the room provides random sequence of description
as developed by the designers. Average length of
descriptions in the rooms was 65 words per de-
scriptions. The possible actions per state average
to 222 per state. Please refer (Narasimhan et al.,
2015) for elaborate game statistics.

To evaluate the performance of agents when us-
ing different word-embeddings, different metrics
could be defined depending on the requirement of
the task. In the particular case of evaluating the
performance of the agent on navigation task, we
follow common defined metrics as in (Narasimhan
et al., 2015)

• cumulative reward per episode averaged over
the number of episodes.

• fraction of quests successfully completed by
the agent.

Considering the task as an downstream proxy for
the actual environment, this does not evaluate
the individual embeddings by similarity metrics,
rather the generalization across environments. In
this work we compare word2vec (Mikolov et al.,
2013), GloVe (Pennington et al., 2014) general
use embeddings against a BOW-DQN baseline
and LSTM-DQN trained with random initializa-
tion (Fig 2). The baseline here (BOW-DQN) uses
a simple bag-of-words model to represent the tex-
tual description.

We observe that using pre-trained embeddings
generally improve the performance of the model.
The rewards in our primary models have high vari-
ance due to the stochastic nature of the policy. In
further work, we wish to build on recent advance-
ments in Deep Reinforcement Learning literature,
for training better models.

29



6 Discussion and Future Work

Using continuous vector representations of words
for language learning in mobile robots poses the
question of choice of embeddings which capture
task specific semantics. In this work we propose
using MUD games as proxy for this task, rather
than elaborate testing on robotic platforms.

Similar to real world situation, all interac-
tions between the agent and the environment are
through language, without any other supervision.
The rewards provide the models with required su-
pervision of tuning the word embeddings to com-
plete the quest. Preliminary evaluation on stan-
dard embeddings show that average rewards for
pretrained embeddings are better than BOW mod-
els and randomly initialized LSTM embeddings.

In future work we wish to extensively evaluate
the model on more standard word-embeddings and
against real datasets to establish empirical correla-
tion between the proposed proxy and actual task.

References
Eyal Amir and Patrick Doyle. 2002. Adventure games:

A challenge for cognitive robotics. pages 148–155.

David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions fro mobservations. In Proceedings of the 25th
AAAI Conference on Artificial Intelligence (AAAI-
2011). San Francisco, CA, USA.

Pavel Curtis. 1992. Mudding: Social phenomena in
text-based virtual realities. High noon on the elec-
tronic frontier: Conceptual issues in cyberspace
pages 347–374.

Jon Gauthier and Igor Mordatch. 2016. A paradigm for
situated and goal-driven language learning. arXiv
preprint arXiv:1610.03585 .

Zellig S Harris. 1954. Distributional structure. Word
10(2-3):146–162.

L. Lucy and J. Gauthier. 2017. Are distributional repre-
sentations ready for the real world? Evaluating word
vectors for grounded perceptual meaning. ArXiv e-
prints .

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems. pages 3111–3119.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. 2015. Human-level

control through deep reinforcement learning. Na-
ture 518(7540):529–533.

Karthik Narasimhan, Tejas Kulkarni, and Regina
Barzilay. 2015. Language understanding for text-
based games using deep reinforcement learning.
arXiv preprint arXiv:1506.08941 .

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation.

30


