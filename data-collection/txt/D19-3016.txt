



















































Honkling: In-Browser Personalization for Ubiquitous Keyword Spotting


Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 91–96
Hong Kong, China, November 3 – 7, 2019. c©2019 Association for Computational Linguistics

91

Honkling: In-Browser Personalization for Ubiquitous Keyword Spotting

Jaejun Lee, Raphael Tang, and Jimmy Lin
David R. Cheriton School of Computer Science

University of Waterloo

Abstract

Used for simple commands recognition on de-
vices from smart speakers to mobile phones,
keyword spotting systems are everywhere.
Ubiquitous as well are web applications,
which have grown in popularity and complex-
ity over the last decade. However, despite their
obvious advantages in natural language inter-
action, voice-enabled web applications are still
few and far between. We attempt to bridge this
gap with Honkling, a novel, JavaScript-based
keyword spotting system. Purely client-side
and cross-device compatible, Honkling can be
deployed directly on user devices. Our in-
browser implementation enables seamless per-
sonalization, which can greatly improve model
quality; in the presence of underrepresented,
non-American user accents, we can achieve up
to an absolute 10% increase in accuracy in the
personalized model with only a few examples.

1 Introduction

With the rapid proliferation of voice-enabled de-
vices such as the Amazon Echo and the Apple
iPhone, speech recognition systems are becoming
increasingly prevalent in our daily lives. Impor-
tantly, these systems improve safety and conve-
nience in hands-free interactions, such as using
Apple’s Siri to dial contacts while driving. How-
ever, a prominent drawback is that most of these
systems perform speech recognition in the cloud,
where a remote server receives from the device all
audio to be transcribed. Clearly, the privacy and
security implications are significant: servers may
be accessed by other people, authorized or not.
Thus, it is important to capture only the relevant
speech and not all incoming audio, while provid-
ing a pleasant hands-free experience.

Enter keyword spotting systems. They solve
the aforementioned issues by implementing an on-
device mechanism to awaken the intelligent agent,

e.g., “Okay, Google” for triggering the Google As-
sistant. This then allows the device to record and
transmit only a limited segment of speech, obvi-
ating the need to send everything to the cloud.
The task of keyword spotting (KWS) is to detect
the presence of specific phrases in a stream of au-
dio, often with the end goal of wake-word detec-
tion or simple command recognition on the device.
Currently, the state of the art uses lightweight
neural networks (Sainath and Parada, 2015; Tang
and Lin, 2018), which can perform inference in
real-time, even on low-end devices (Fernández-
Marqués et al., 2018; Tang et al., 2018).

Despite the popularity of voice-enabled prod-
ucts, web applications have yet to make use of
KWS. This is surprising, since modern web appli-
cations are supported on billions of devices rang-
ing from desktops to smartphones. We close the
gap between KWS systems and web applications
by building and evaluating a JavaScript-based, in-
browser KWS system. Exploiting the pervasive-
ness of JavaScript, our system can be deployed
directly on user devices, facilitating the develop-
ment of JavaScript-based, voice-enabled applica-
tions (Lee et al., 2019).

We observe, however, that the quality of
our models suffers on various accents that are
scarcely represented in our dataset—a problem
common in speech recognition (Huang et al.,
2004; Humphries et al., 1996). Fortunately,
our JavaScript-based application runs completely
client-side, enabling the development of personal-
ized models. To further improve the universality
of our system, we explore the benefits and costs of
fine-tuning an existing KWS model on a few user-
provided recordings, personalizing the application
for a given user.

Our main demonstration is Honkling,1 a novel

1http://honkling.ai

http://honkling.ai


92

in-browser KWS system running previous state-
of-the-art models (Tang and Lin, 2018). We pro-
vide a set of comprehensive experimental results
for the latency of an in-browser KWS system on a
broad range of devices. We also evaluate the accu-
racy of KWS on various user accents and present a
mechanism for in-browser user accent adaptation.
On the Google Speech Commands dataset (War-
den, 2018), our most accurate in-browser model
achieves an accuracy of 94% while performing in-
ference in less than 30 milliseconds. With only
five user-recorded audio clips per keyword, Honk-
ling can be fine-tuned to improve accuracy by up
to an absolute 10%. Using hardware acceleration,
users only need to wait for eight seconds before
their Honkling becomes personalized.

2 Background and Related Work

KWS is the task of detecting a spoken phrase
in audio, applicable to simple command recog-
nition (Warden, 2018) and wake-word detec-
tion (Arik et al., 2017). Typically, KWS systems
must be small footprint, since the target platforms
are mobile phones, Internet-of-Things (IoT) de-
vices, and other portable electronics. To achieve
this goal, resource-efficient architectures using
convolutional neural networks (CNNs) (Tang and
Lin, 2018; Sainath and Parada, 2015) and recur-
rent neural networks (RNNs) (Arik et al., 2017)
have been proposed, while other techniques make
use of low-bitwidth weights (Fernández-Marqués
et al., 2018; Zhang et al., 2017). However, de-
spite the pervasiveness of modern web browsers
on a broad range of devices and the availability of
deep learning toolkits in JavaScript, a personaliz-
able, on-device KWS system in web applications
has not to our knowledge been explored.

In automatic speech recognition (ASR), the
presence of user accents often degrades recogni-
tion quality (Huang et al., 2004; Humphries et al.,
1996). Unfortunately, the solutions proposed in
previous work vary depending on the underlying
system, and little prior art exists using deep learn-
ing. The idea of fine-tuning neural networks to
increase accuracy for a group of accents is found
in Najafian et al. (2016); however, full ASR in-
volves much larger datasets and models, and thus
hours of extra training data are necessary for suc-
cessful adaptation. Surprisingly, there has been lit-
tle work on building KWS systems for user accent
personalization.

3 Data and Models

For consistency with past results (Tang and Lin,
2018; Tang et al., 2018), we train our models on
the first version of the Google Speech Commands
dataset (Warden, 2018), comprising 65,000 spo-
ken utterances for 30 short, one-second phrases.
As with Tang and Lin (2018), we pick the fol-
lowing twelve classes: “yes”, “no”, “stop”, “go”,
“left”, “right”, “on”, “off”, “up”, “down”, un-
known, and silence. The dataset contains roughly
2,000 examples per class, including a few back-
ground noise samples of both man-made and arti-
ficial noises, e.g., washing dishes and white noise.
As is standard in the speech processing literature,
all audio is in 16-bit PCM, 16kHz mono-channel
WAV format. We use the standard 80%, 10%, and
10% splits from the Speech Commands dataset for
the training, validation, and test sets, respectively.

3.1 Input Preprocessing

First, for dataset augmentation, the input is ran-
domly mixed with additive noise from the back-
ground noise set—this helps to decrease the gen-
eralization error and improve the robustness of the
model under noise (Ko et al., 2015). Following the
official TensorFlow implementation, we also ap-
ply a random time shift of UNIFORM[−100, 100]
milliseconds (ms). For feature extraction, we com-
pute 40-dimensional Mel-frequency cepstral co-
efficients (MFCCs), with a window size of 30ms
and a frame shift of 10ms, yielding a final prepro-
cessed input size of 101× 40 for each one-second
audio sample.

3.2 Model Architecture

We use the res8 and res8-narrow archi-
tectures from Tang and Lin (2018) as starting
points, which represent the prior state of the art
in residual CNNs (He et al., 2016) for KWS. In
both models, given the input X ∈ R101×40, we
first apply a 2D convolution layer with weights
W ∈ RCout×1×(3×3) and a padding of one on
all sides. This step results in an output of X̃ ∈
RCout×101×40, which we then downsample using
an average pooling layer with a kernel size of
(4 × 3). Next, the output is passed through a se-
ries of three residual blocks comprising convolu-
tion and batch normalization (Ioffe and Szegedy,
2015) layers. Finally, we average pool across the
channels and pass the features through a softmax
across the twelve classes.



93

Device Processor Platform
res8 res8-narrow

Lat. (ms) Acc. (%) Lat. (ms) Acc. (%)

GPU

Desktop GTX 1080 Ti PyTorch 1 94.3 1 91.2
Desktop GTX 1080 Ti Firefox 12 94.0 10 90.9
MacBook Pro (2017) Intel Iris Plus 650 Firefox 29 94.0 15 90.8
MacBook Air (2013) Intel HD 6000 Firefox 34 94.0 19 90.8
Galaxy S8 (2017) Adreno 540 Firefox 60 94.1 43 89.0

CPU

Desktop i7-4790k (quad) PyTorch 10 94.3 2 91.2
MacBook Pro (2017) i5-7287U (quad) PyTorch 12 94.3 3 91.2
Desktop i7-4790k (quad) Firefox 371 94.1 94 90.9
MacBook Pro (2017) i5-7287U (quad) Firefox 361 94.0 107 90.8
MacBook Air (2013) i5-4260U (dual) Firefox 485 94.0 115 90.8
Galaxy S8 (2017) Snapdragon 835 (octa) Firefox 1105 94.1 265 89.0

Table 1: Latency (lat.; 90th percentile) and accuracy (acc.) results on different platforms for the res8-* models.

In the previous description, we are free to
choose Cout to dictate the expressiveness and
computational footprint of the model; res8 and
res8-narrow choose 45 and 19, respectively.
In total, res8 contains 110K parameters and in-
curs 30 million multiplies per second of audio,
while res8-narrow uses 19.9K parameters and
incurs 5.7 million multiplies.

4 Honkling

Training neural networks in JavaScript from
scratch is ill-advised due to poorly optimized
computation routines such as matrix multiplica-
tion. Therefore, we use the official PyTorch
model implementations2 at training time. At in-
ference time, weights are transferred from Py-
Torch to a web application implemented in Tensor-
Flow.js.3 Since the official implementation uti-
lizes LibROSA (McFee et al., 2015) for audio
feature extraction, we instead use Meyda (Rawl-
inson et al., 2015), a JavaScript version of Lib-
ROSA. However, unlike Python, which is well
suited for developing audio processing applica-
tions, in-browser JavaScript presents challenges in
manipulating audio; for example, many browsers
restrict the sample rate of input audio to 44.1kHz
only. Due to such restrictions, the processed audio
in JavaScript differ from MFCCs extracted by Lib-
ROSA. Therefore, we have patched Meyda com-
prehensively to minimize the mismatches.

Overall, we successfully enable KWS func-
tionality in browsers without any server-side in-
ference. Since the audio data is quickly pro-
cessed within the browser, it is much more effi-
cient than transferring data over the network for
inference. Furthermore, users are now freed from

2http://honk.ai
3https://js.tensorflow.org

security and privacy implications, such as eaves-
dropping of network traffic and collection of per-
sonal speech data.

Measured with our university WiFi connection,
the average latency to Google servers is about
25ms with a standard deviation of 20ms. Network
latency is much higher for transferring audio data.
With a server written in Python, we measure an av-
erage latency of 481ms with a standard deviation
of 183ms for one second of 16kHz mono-channel
audio data. With in-browser inference, we achieve
a pure client-side architecture that does not suffer
from variable network latency.

The two main metrics for our KWS application
are accuracy and inference latency. To be consis-
tent with previous work, our experiments use the
same test set. We conduct experiments on desktop,
laptop, and smartphone configurations to demon-
strate the feasibility of our system on a broad range
of devices. These include the following: a desktop
with 16GB RAM, an i7-4790k CPU, and a GTX
1080 Ti; two laptop configurations, the MacBook
Pro (2017), with a quad-core i5-7287U CPU and
an Intel Iris Plus 650 GPU, and the MacBook Air
(2013), with a dual-core i5-4260U CPU and an In-
tel HD 6000 GPU; the Galaxy S8 as our smart-
phone configuration. Given this wide range of
devices, we decide to include results from Fire-
fox only, but Honkling is also available on other
browsers. Since TensorFlow.js is GPU capable,
experiments are conducted both with and with-
out GPU acceleration, which can be toggled in the
browser settings.

Table 1 summarizes 90th percentile latency and
recognition accuracy results for both res8 and
res8-narrow on various devices. Note that re-
sults with the PyTorch implementation on our lap-
top and desktop setups are included to compare

http://honk.ai
https://js.tensorflow.org


94

Figure 1: Accuracy varying the number of epochs (left) and the learning rate (right), with 95% confidence intervals
(shaded). original * and personalized * denote accuracy on the original and user test sets.

with our in-browser configurations. The origi-
nal implementation achieves an accuracy of 94.3%
for res8 and 91.2% for res8-narrow (see
the first few rows in Table 1). Slight differences
are observed across platforms due to a mismatch
of MFCC computations between LibROSA and
Meyda. However, the accuracy for each model is
consistent on every platform, confirming that our
in-browser implementation is robust.

Even though latency is processor dependent,
the res8-narrow model performs inference in
real time on every platform. Given that these de-
lays are perceived by humans to be near instanta-
neous (Miller, 1968), our system is sufficient for
real-time interactive web applications.

5 Personalization and Accent Adaptation

JavaScript applications run fully client-side and
are cross-device compatible, meaning that they
can be deployed directly on user devices, which
enables seamless personalization. In this paper,
to design a cross-device, cross-human KWS sys-
tem, we extend Honkling to adapt to various
user accents that are uncommon in the primarily
American-accented dataset.

To measure the effects of different user accents
on the KWS quality of Honkling, we evaluate
the accuracy of res8-narrow on datasets com-
prised of recordings from different people. In

these experiments, there are four participants: A,
B, C, and D. Users A and B are native speakers
of Canadian English while C has a British accent
and D has a Korean accent. From each person,
we collected 50 recordings for each of the twelve
classes, setting aside 40 recordings of each class
to construct a test set of 480 samples. We conduct
evaluations on the same twelve classes.

To quantify the amount of effort required from
the user for personalization, we begin by find-
ing the minimum number of recordings that leads
to improvements in accuracy. Next, we experi-
ment with different numbers of epochs and learn-
ing rates. Since we have shown that our JavaScript
implementation reproduces the PyTorch imple-
mentation with minimal differences, unless in-
dicated otherwise the following experiments are
conducted using PyTorch for convenience.

We report accuracy metrics under two dif-
ferent sets of conditions: original * and
personalized *, denoting accuracy on the
original test set and the user test set, respec-
tively (see Figure 1, dashed and solid lines). For
each condition, we conduct experiments with three
variations of training data size. The number that
follows each name denotes the number of record-
ings per keyword in the fine-tuning dataset. To
reduce the effects of outliers, we report averaged
results from 60 experiments with different ran-



95

Device Processor Platform
Number of Recordings

1 3 5

GPU
Desktop GTX 1080 Ti PyTorch 0.2 sec 0.2 sec 0.2 sec
Desktop GTX 1080 Ti Firefox 3.9 sec 5.9 sec 7.6 sec
MacBook Pro (2017) Intel Iris Plus 650 Firefox 7.2 sec 12.6 sec 27.0 sec

CPU

Desktop i7-4790k (quad) PyTorch 3.3 sec 6.0 sec 8.0 sec
MacBook Pro (2017) i5-7287U (quad) PyTorch 2.0 sec 5.9 sec 10.7 sec
Desktop i7-4790k (quad) Firefox 25.4 min 75.8 min 128.1 min
MacBook Pro (2017) i5-7287U (quad) Firefox 29.5 min 86.3 min 139.2 min

Table 2: Average in-browser fine-tuning efficiency for res8-narrow under different configurations.

dom seeds, where the training data is constructed
by randomly selecting the appropriate number of
recordings from the fine-tuning set. The test set
stays the same for each experiment.

Fine-tuning progress across epochs. In Figure 1,
we include in the labels of the plots both the accu-
racy of the base model and the accuracy on each
user test set. The left half of the figure shows the
fine-tuning progress across epochs, with the learn-
ing rate fixed to 0.01. As expected, accuracy on
the user test set is lower than on the original test
set prior to fine-tuning. For users A and B, the dif-
ferences are only a few percent, which seems ac-
ceptable in practice. However, the model achieves
an accuracy of only 76.8% and 80.5% for users
C and D, respectively, demonstrating the need for
personalization.

As fine-tuning proceeds across epochs, accu-
racy on the user recordings increases while accu-
racy on the original data decreases. We observe
diminishing returns with more epochs; 50 epochs
seem to be sufficient to maximize accuracy. After
convergence, the models generally achieve higher
accuracy on the user recordings than on the orig-
inal data, thus demonstrating successful adapta-
tion. We find that a single recording per keyword
is sufficient for personalization, as the fine-tuned
models exhibit higher accuracy on the user record-
ings for every user except user C.

Fine-tuning dataset size. Since more train-
ing data leads to a better representation of a
user’s speech patterns, it is no surprise that
an increase in accuracy is observed as more
recordings are added to the dataset; compare
personalized {1,3,5} in Figure 1.

However, we find that the accuracy converges
rapidly after a mere five recordings per key-
word; such a trend is evident for every user. Con-
cretely, the accuracy gap between one and three
recordings is substantially greater than the gap be-

tween three and five, suggesting that each addi-
tional recording provides rapidly diminishing re-
turns. Although using one sample per keyword
helps, results suggest that having at least three
recordings is desirable, since the marginal bene-
fit of two more recordings is quite large; in user
C’s case (see Figure 1, bottom left), we observe an
absolute improvement of almost 10 points.

Learning rate. In this experiment, we fix the
number of epochs to 25 and perform grid search
on the learning rate from 0.1 to 0.0001, stepping
by a factor of ten—see the right half of Figure 1.
Among the four different learning rates, we find
that choosing 0.01 consistently leads to the best
accuracy on user recordings for all three variations
of training data size.

Efficiency and application evaluation. Bring-
ing all the previous threads together, Honkling
supports in-browser personalization by fine-tuning
with user recordings. As we find that the num-
ber of recordings has a high correlation with the
quality of personalization, users have the option to
choose the number of recordings. Once recording
is completed, the base model is fine-tuned with the
best hyperparameter settings, for 50 epochs with
a learning rate of 0.01. The fine-tuned model is
then stored in the browser. At startup, Honkling
loads the stored model if it exists so users can keep
the application personalized even after the current
browser session ends.

Table 2 summarizes the in-browser fine-tuning
efficiency for res8-narrow on the 2017 Mac-
Book Pro and our desktop. We average results
over ten trials, where fine-tuning data is randomly
selected from the four user datasets from the pre-
vious experiment. To be consistent with our pre-
vious study on in-browser inference efficiency, we
conduct the experiments in Firefox. From the re-
sults, we find that, unsurprisingly, personalization
time increases with the data size. Since training



96

data size correlates with the final accuracy, users
have the option to trade off time and quality. For-
tunately, GPU acceleration can significantly de-
crease fine-tuning time. The same process that
consumes up to 2.3 hours on a CPU can be com-
pleted within 27 seconds on a GPU. With a GTX
1080 Ti, only 8 seconds are necessary to achieve
personalization with Honkling.

6 Conclusion

In this paper, we realize a new paradigm for serv-
ing neural network applications by implementing
Honkling, a JavaScript-based, in-browser KWS
system. On the popular Google Speech Com-
mands dataset, our model achieves an accuracy
of 94% while maintaining an inference latency of
less than 30 milliseconds on modern devices. The
purely client-side architecture allows our applica-
tion to be efficient and cross-device compatible,
with the additional benefit of supporting user per-
sonalization. Since many speech-based systems
suffer from accuracy degradation caused by dif-
ferences in accents and speaking styles, we sup-
port per-user personalization in Honkling. Based
on a small-scale study, we observe a substan-
tial increase in accuracy after spending only a
short amount of time fine-tuning on a few sample
recordings. These results pave the way for future
web applications that seamlessly support built-in
speech-based interactions.

Acknowledgments

This research was supported by the Natu-
ral Sciences and Engineering Research Council
(NSERC) of Canada.

References

Sercan O. Arik, Markus Kliegl, Rewon Child, Joel
Hestness, Andrew Gibiansky, Chris Fougner, Ryan
Prenger, and Adam Coates. 2017. Convolutional re-
current neural networks for small-footprint keyword
spotting. INTERSPEECH.

Javier Fernández-Marqués, W.-S. Tseng Vincent,
Sourav Bhattachara, and Nicholas D. Lane. 2018.
BinaryCmd: Keyword spotting with deterministic
binary basis. SysML.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. CVPR.

Chao Huang, Tao Chen, and Eric Chang. 2004. Accent
issues in large vocabulary continuous speech recog-
nition. International Journal of Speech Technology,
7:141–153.

Jason J. Humphries, Philip C. Woodland, and David
J. B. Pearce. 1996. Using accent-specific pronunci-
ation modelling for robust speech recognition. Inter-
national Conference on Spoken Language Process-
ing.

Sergey Ioffe and Christian Szegedy. 2015. Batch nor-
malization: Accelerating deep network training by
reducing internal covariate shift. ICML.

Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-
jeev Khudanpur. 2015. Audio augmentation for
speech recognition. INTERSPEECH.

Jaejun Lee, Raphael Tang, and Jimmy Lin. 2019.
Universal voice-enabled user interfaces using
JavaScript. IUI.

Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.
Ellis, Matt McVicar, Eric Battenberg, and Oriol Ni-
eto. 2015. librosa: Audio and music signal analysis
in Python. Python in Science Conference.

Robert B. Miller. 1968. Response time in man-
computer conversational transactions. Fall Joint
Computer Conference, Part I.

Maryam Najafian, Saeid Safavi, John H. L. Hansen,
and Martin Russell. 2016. Improving speech recog-
nition using limited accent diverse British English
training data with deep neural networks. Interna-
tional Workshop on Machine Learning for Signal
Processing.

Hugh Rawlinson, Nevo Segal, and Jakub Fiala. 2015.
Meyda: An audio feature extraction library for the
web audio API. Web Audio Conference.

Tara N. Sainath and Carolina Parada. 2015. Convolu-
tional neural networks for small-footprint keyword
spotting. INTERSPEECH.

Raphael Tang and Jimmy Lin. 2018. Deep resid-
ual learning for small-footprint keyword spotting.
ICASSP.

Raphael Tang, Weijie Wang, Zhucheng Tu, and Jimmy
Lin. 2018. An experimental analysis of the power
consumption of convolutional neural networks for
keyword spotting. ICASSP.

Pete Warden. 2018. Speech Commands: A
dataset for limited-vocabulary speech recognition.
arXiv:1804.03209.

Yundong Zhang, Naveen Suda, Liangzhen Lai, and
Vikas Chandra. 2017. Hello edge: Keyword spot-
ting on microcontrollers. arXiv:1711.07128.


