










































Capturing Long-distance Dependencies in Sequence Models: A Case Study of Chinese Part-of-speech Tagging


International Joint Conference on Natural Language Processing, pages 180–188,
Nagoya, Japan, 14-18 October 2013.

Capturing Long-distance Dependencies in Sequence Models:
A Case Study of Chinese Part-of-speech Tagging

Weiwei Sun and Xiaochang Peng and Xiaojun Wan
Institute of Computer Science and Technology, Peking University

The MOE Key Laboratory of Computational Linguistics
{ws,wanxiaojun}@pku.edu.cn; pxc.pku@gmail.com

Abstract

This paper is concerned with capturing
long-distance dependencies in sequence
models. We propose a two-step strat-
egy. First, the stacked learning technique
is applied to integrate sequence models
that are good at exploring local informa-
tion and other high complexity models that
are good at capturing long-distance de-
pendencies. Second, the structure com-
pilation technique is employed to trans-
fer the predictive power of hybrid models
to sequence models via large-scale unla-
beled data. To investigate the feasibility
of our idea, we study Chinese POS tag-
ging. Experiments on the Chinese Tree-
bank data demonstrate the effectiveness of
our methods. The re-compiled models not
only achieve high accuracy with respect to
per token classification, but also serve as a
front-end to a parser well.

1 Introduction

Sequential classification models provide very im-
portant solutions to pattern recognition tasks that
involve the automatic assignment of a categorical
label to each token of a sequence of observed val-
ues. A common example is part-of-speech (POS)
tagging, which seeks to assign a grammatical cat-
egory to each word in an input sentence. Standard
machine learning algorithms to sequential tagging,
e.g. linear-chain conditional random fields and
max-margin Markov network, directly exploit lo-
cal dependencies and perform quite well for a
large number of sequence labeling tasks. In these
models, usually, the relationships between two (or
three) successive labels are parameterized and en-
coded as a single feature, and Viterbi style dy-
namic programming algorithms are applied to in-
ference over a lattice. Although sequence models

perform well for many applications, they are inad-
equate for tasks where many long-distance depen-
dencies are involved.

Sequential classification models play an impor-
tant role in natural language processing (NLP).
Several fundamental NLP tasks, including named
entity recognition, POS tagging, text chunking,
supertagging, etc., employ sequential classifiers
for lexical and syntactic disambiguation. In ad-
dition to learning linear chain structures, sequence
models can even be applied to acquire hierarchical
syntactic structures (Tsuruoka et al., 2009). How-
ever, long-distance dependencies widely exist in
linguistic structures, and many NLP systems suf-
fer from the incapability of capturing these depen-
dencies. For example, previous work has shown
that sequence models alone cannot deal with syn-
tactic ambiguities well (Clark and Curran, 2004;
Tsuruoka et al., 2009). On the contrary, state-
of-the-art systems usually utilize high complexity
models, such as lexicalized PCFG models for syn-
tactic parsing, to achieve high accuracy. Unfortu-
nately, they are not suitable for many real world
applications due to the sacrifice of efficiency.

In this paper, we are concerned with capturing
long-distance dependencies in sequence models.
Our goal is to develop efficient models with lin-
ear time complexity that are also capable to cap-
ture non-local dependencies. Two techniques are
studied to achieve this goal. First, stacked learn-
ing (Breiman, 1996) is employed to integrate se-
quence models that are good at exploring local in-
formation and other high complexity models that
are good at capturing non-local dependencies. By
combining complementary strengths of heteroge-
neous models, hybrid systems can obtain more
accurate results. Second, structure compilation
(Liang et al., 2008) is employed to transfer the pre-
dictive power of hybrid models to sequence mod-
els via large-scale unlabeled data. In particular,
hybrid systems are utilized to create large-scale

180



pseudo training data for cheap sequence models.
A discriminative model can be improved by in-
corporating more features, while a generative la-
tent variable model can be improved by increasing
the number of latent variables. By using stacking
and structure compilation techniques, a sequence
model can be enhanced to better capture long-
distance dependencies and to achieve more accu-
rate results.

To demonstrate the feasibility to capture long-
distance dependencies in a sequence model, we
present our work on Chinese POS tagging. The
Chinese language has a number of characteristics
that make Chinese POS tagging particularly chal-
lenging. While simple sequential classifiers can
easily achieve tagging accuracies of above 97%
on English, Chinese POS tagging has proven to
be more challenging and has obtained accuracies
of about 93-94% (Huang et al., 2009; Sun and
Uszkoreit, 2012) when applying sequence mod-
els. Recent work shows that higher accuracy (c.a.
95%) can be achieved by applying advanced learn-
ing techniques to capture deep lexical relations
(Sun and Uszkoreit, 2012). Especially, syntag-
matic lexical relations have been shown playing an
essential role in Chinese POS tagging. To capture
such relations, an accurate POS tagging model
should know more information about long range
dependencies. Previous work has used syntactic
parsers in either constituency or dependency for-
malisms to exploit such useful information (Sun
and Uszkoreit, 2012; Hatori et al., 2011). How-
ever, it is inapproporiate to employ computation-
ally expensive parsers to improve POS tagging for
many realistic NLP applications, mainly due to ef-
ficiency considerations.

In this paper, we study several hybrid systems
that are built upon various complementary tagging
systems. We investigate stacked learning to build
more accurate solutions by integrating heteroge-
neous models. Experiments on the Chinese Tree-
bank (CTB) data show that stacking is very effec-
tive to build high-accuracy tagging systems. Al-
though predictive powers of hybrid systems are
significantly better than individual systems, they
are not suitable for large-scale real word applica-
tions that have stringent time requirements. To im-
prove POS tagging efficiency without loss of ac-
curacy, we explore unlabeled data to transfer the
predictive power of complex, inefficient models to
simple, efficient models. Experiments show that

unlabeled data is effective to re-compile simple
models, including latent variable hidden Markov
models, local and global linear classifiers. On one
hand, the precison in terms of word classification
is improved to 95.33%, which reachs the state-of-
the-art. On the other hand, re-compiled models
are adapted based on parsing results, and as a re-
sult the ability to capture syntagmatic lexical re-
lations is improved as well. Different from the
purely supervised sequence models, re-compiled
models also serve as a front-end to a parser well.

2 Background

The Chinese language has a number of character-
istics that make Chinese POS tagging particularly
challenging. For example, Chinese is character-
ized by the lack of formal devices such as morpho-
logical tense and number that often provide impor-
tant clues for syntactic processing. Chinese POS
tagging has proven to be very difficult and has ob-
tained accuracies of about 93-94% (Huang et al.,
2009; Li et al., 2011; Hatori et al., 2011; Sun and
Uszkoreit, 2012). On the other hand, Chinese POS
information is very important for advanced NLP
tasks, e.g. supertagging, full parsing and seman-
tic role labeling. Previous work has repeatedly
demonstrated the significant performance gap of
NLP systems while using gold standard and au-
tomatically predicted POS tags (Zhang and Clark,
2009; Li et al., 2011; Tse and Curran, 2012). In
this section, we give a brief introduction and a
comparative analysis to several models that are re-
cently designed to resolve the Chinese POS tag-
ging problem.

2.1 Various Chinese POS Tagging Models

Local linear model (LLM) A very simple ap-
proach to POS tagging is to formulate it as a local
word classification problem. Various features can
be drawn upon information sources such as word
forms and characters that constitute words. Pre-
vious studies on many languages have shown that
local classification is inadequate to capture struc-
tural information of output labels, and thus does
not perform as well as structured models.

Linear-chain global linear model (LGLM)
Sequence labeling models can capture output
structures by exploiting local dependencies among
words. A global linear model is flexible to in-

181



clude linguistic knowledge from multiple informa-
tion sources, and thus suitable to recognize more
new words. A majority of state-of-the-art English
POS taggers are based on LGLMs, e.g. struc-
tured perceptron (Collins, 2002) and conditional
random fields (Lafferty et al., 2001). Such mod-
els are also very popular for building Chinese POS
taggers (Sun and Uszkoreit, 2012).

Hidden Markov model with latent variables
(HMMLA) Generative models with latent an-
notations (LA) obtain state-of-the-art performance
for a number of NLP tasks. For example,
both PCFG and TSG with refined latent vari-
ables achieve excellent results for syntactic pars-
ing (Matsuzaki et al., 2005; Shindo et al., 2012).
For Chinese POS tagging, Huang, Eidelman and
Harper (2009) described and evaluated a bi-gram
HMM tagger that utilizes latent annotations. The
use of latent annotations substantially improves
the performance of a simple generative bigram
tagger, outperforming a trigram HMM tagger with
sophisticated smoothing.

PCFG Parsing with latent variables (PCFGLA)
POS tags can be taken as preterminals of a con-
stituency parse tree, so a constituency parser can
also provide POS information. The majority of
the state-of-the-art constituent parsers are based
on generative PCFG learning, with lexicalized
(Collins, 2003; Charniak, 2000) or latent annota-
tion (Matsuzaki et al., 2005; Petrov et al., 2006)
refinements. Compared to complex lexicalized
parsers, the PCFGLA parsers leverage on an au-
tomatic procedure to learn refined grammars and
are more robust to parse many non-English lan-
guages that are not well studied. For Chinese, a
PCFGLA parser achieves the state-of-the-art per-
formance and outperforms many other types of
parsers (Zhang and Clark, 2009).

2.1.1 Joint POS Tagging and Dependency
Parsing (DEP)

(Hatori et al., 2011) proposes an incremental pro-
cessing model for the task of joint POS tagging
and dependency parsing, which is built upon a
shift-reduce parsing framework with dynamic pro-
gramming. Given a segmented sentence, a joint
model simultaneously considers possible POS tags
and dependency relations. In this way, the learner
can better predict POS tags by using bi-lexical de-
pendency information. Their experiments show
that the joint approach achieved substantial im-

provements over the pipeline systems in both POS
tagging and dependency parsing tasks.

2.2 Comparison
We can distinguish the five representative tagging
models from two views (see Table 2). From a lin-
guistic view, we can distinguish syntax-free and
syntax-based models. In a syntex-based model,
POS tagging is integrated into parsing, and thus
(to some extent) is capable of capturing long range
syntactic information. From a machine learning
view, we can distinguish generative and discrim-
inative models. Compared to generative models,
discriminative models define expressive features
to classify words. Note that the two generative
models employ latent variables to refine the out-
put spaces, which significantly boost the accuracy
and increase the robustness of simple generative
models.

Generative Discriminative
Syntax-free HMMLA LLM, LGLM
Syntax-based PCFGLA DEP

Table 2: Two views of different tagging models.

2.3 Evaluation
2.3.1 Experimental Setting
Penn Chinese Treebank (CTB) (Xue et al., 2005)
is a popular data set to evaluate a number of
Chinese NLP tasks, including word segmenta-
tion, POS tagging, syntactic parsing in both con-
stituency and dependency formalisms. In this pa-
per, we use CTB 6.0 as the labeled training data for
the study. In order to obtain a representative split
of data sets, we conduct experiments following the
setting of the CoNLL 2009 shared task (Hajič et
al., 2009), which is also used by (Sun and Uszkor-
eit, 2012). The setting is provided by the principal
organizer of the CTB project, and has considered
many annotation details. This setting is very ro-
bust for evaluating Chinese language processing
algorithms.

We present an empirical study of the five typ-
ical approaches introduced above. In our exper-
iments, to build local and global word classifiers
(i.e. LLMs and LGLMs), we implement the fea-
ture set used in (Sun and Uszkoreit, 2012). De-
note a word w in focus with a fixed window
w−2w−1ww+1w+2. The features include:

• Word unigrams: w−2, w−1, w, w+1, w+2;

182



Devel. LLM LGLM(SP) LGLM(PA) HMMLA PCFGLA DEP
Overall 93.96% 94.30%/94.49% 94.24%/94.33% 94.16% 93.69% 94.58%
NR 95.07 94.47/94.85 94.41/94.56 94.22 89.84 93.55
NT 97.61 97.22/97.75 97.66/97.59 97.18 96.70 96.84
NN 94.89 94.67/94.79 94.72/94.71 94.30 93.56 94.55
DEC 78.61 81.98/82.36 80.68/81.76 80.60 85.78 86.73
DEG 82.44 85.58/86.72 85.37/85.00 85.19 88.94 89.45
UNK - - 80.0%/81.1% - - 78.2% - - - -

Table 1: Tagging accuracies of different supervised models on the development data.

• Word bigrams: w−2 w−1, w−1 w, w w+1,
w+1 w+2;

• Character n-gram prefixes and suffixes for n
up to 3.

To train LLMs, we use the open source linear clas-
sifier – LIBLINEAR1. To train LGLMs, we choose
structured perceptron (SP) (Collins, 2002) and
passive aggressive (PA) (Crammer et al., 2006)
learning algorithms. For the LAHMM and DEP
models, we use the systems discribed in (Huang
et al., 2009; Hatori et al., 2011); for the PCFGLA
models, we use the Berkeley parser2.

2.3.2 Results
Table 1 summarizes the performance in terms
of per word classification of different supervised
models on the development data. We present the
results of both first order (on the left) and second
order (on the right) LGLMs. We can see that the
perceptron algorithm performs a little better than
the PA algorithm for Chinese POS tagging. There
is only a slight gap between the local classification
model and various structured models. This is very
different from English POS tagging. Although the
local classifier achieves comparable results when
respectively applied to English and Chinese, there
is much more significant gap between the corre-
sponding structured models. Similarly, the gap be-
tween the first and second order LGLMs is very
modest too.

From the linguistic view, we mainly consider
the disambiguiation ability of local and non-local
dependencies. Table 1 presents accuracy results
of several POS types, including nouns and func-
tional words. The POS types NR, NT and NN re-
spectively represent proper nouns, temporal nouns
and other common nouns. We can clearly see that
models which only explore local dependencies are

1www.csie.ntu.edu.tw/˜cjlin/liblinear/
2code.google.com/p/berkeleyparser/

good enough to deal with nouns. Surprisingly, the
local classifier that does not directly define fea-
tures of possible POS tags of other surrounding
words performs even better than structured mod-
els for proper nouns and other common nouns.

The tag DEC denotes a complementizer or a
nominalizer, while the tag DEG denotes a genitive
marker and an associative marker. These two types
only include two words: “的” and “之.” The lat-
ter one is mainly used in ancient Chinese. 5.19%
of words appearing in the training data set is
DEC/DEG. The pattern of the DEC recognition is
clause/verb phrase+DEC+noun phrase, and The
pattern of the DEG recognition is nominal modi-
fier+DEC+noun phrase. To distinguish the sen-
tential/verbal and nominal modification phrases,
the DEC and DEG words usually need long range
syntactic information for accurate disambiguation.
We claim that the prediction performance of the
two specific types is a good clue of how well a tag-
ging model resolves long distance dependencies.
We can see that the two syntactic parsers signifi-
cantly outperform local models on the prediction
of these types of words.

The weak ability for non-local disambiguation
also imposes restrictions on using a sequence POS
tagging model as front module for parsing. To
evaluate the impact, we employ the PCFGLA
parser to parse a sentence based on the POS tags
provided by sequence models. Table 4 shows the
parsing performance. Note that the overall tag-
ging performance of the Berkeley parser is sig-
nificantly worse than sequence models. However,
better POS tagging does not lead to better pars-
ing. The experiments suggest that sequence mod-
els propagate too many errors to the parser. Our
linguistic analysis can also well explain the poor
performance of Chinese CCG parsing when apply-
ing the C&C parser (Tse and Curran, 2012). We
think the failure is mainly due to overplaying se-
quence models in both POS tagging and supertag-

183



LLM First order LGLM Second order LGLM
SP PA SP PA

Baseline 93.96% 94.30% 94.24% 94.49% 94.33%
+Word clustering 94.75% 94.90% 94.80% 95.05% 94.96%
+Word clustering+HMMLA 95.12% 95.19% 95.18% 95.14% 95.22%
+Word clustering+PCFGLA 95.42% 95.50% 95.40% 95.56% 95.44%
+Word clustering+DEP 95.28% 95.22% 95.26% 95.29% 95.25%
+ALL 95.56% 95.61% 95.60% 95.53% 95.53%

Table 3: Tagging accuracies of different stacking models on the development data.

ging.

Devel. LP LR F1
Berkeley 80.44 80.31 81.36
1or LGLM 80.38 79.48 79.93↓
2or LGLM 80.98 79.93 80.45↓
HMMLA 80.65 79.62 80.13↓
1or LGLM(HMMLA) 81.55 80.80 81.17↓
1or LGLM(PCFGLA) 82.84 81.75 82.29↑
1or LGLM(DEP) 82.69 81.68 82.18↑

Table 4: Parsing accuracies on the development
data. 1or and 2or respectively denote first order
and second order. LGLM(X) denotes a stacking
model with X as the level-0 processing. All stack-
ing models incorporate word clusters to improve
the tagging accuracy.

To distinguish the predictive abilities of genera-
tive and discriminative models, we report the pre-
cison of the prediction of unknown words (UNK).
Discriminative learning can define arbitrary (even
overlapping) features which play a central role in
tagging English unknown words. The difference
between generative and discriminative learning in
Chinese POS tagging is not that much, mainly
because most Chinese words are compactly com-
posed by a very few Chinese characters that are
usually morphemes. This language-specific prop-
erty makes it relatively easy to smooth parameters
of a generative model.

3 Improving Tagging Accuracy via
Stacking

In this section, we study a simple way of inte-
grating multiple heterogeneous models in order to
exploit their complementary strength and thereby
improve tagging accuracy beyond what is possi-
ble by either model in isolation. The method in-
tegrates the heterogeneous models by allowing the
outputs of the HMMLA, PCFGLA and DEP to de-

fine features for the LLM/LGLM.

3.1 Stacked Learning
Stacked generalization is a meta-learning algo-
rithm that has been first proposed in (Wolpert,
1992) and (Breiman, 1996). Stacked learning has
been applied as a system ensemble method in sev-
eral NLP tasks, such as joint word segmentation
and POS tagging (Sun, 2011), and dependency
parsing (Nivre and McDonald, 2008). The idea
is to include two “levels” of predictors. The first
level includes one or more predictors g1, ..., gK :
Rd → R; each receives input x ∈ Rd and out-
puts a prediction gk(x). The second level consists
of a single function h : Rd+K → R that takes
as input 〈x, g1(x), ..., gK(x)〉 and outputs a final
prediction ŷ = h(x, g1(x), ..., gK(x)). The pre-
dictor, then, combines an ensemble (the gk’s) with
a meta-predictor (h).

3.2 Applying Stacking to POS Tagging
We use the LLMs or LGLMs (as h) for the level-1
processing, and other models (as gk) for the level-
0 processing. The characteristic of discrimina-
tive learning makes LLMs/LGLMs very easy to
integrate the outputs of other models as new fea-
tures. We are relying on the ability of discrim-
inative learning to explore informative features,
which play a central role in boosting the tagging
accuracy. For output labels produced by each aux-
iliary model, five new label uni/bi-gram features
are added: w−1, w, w+1, w−1 w, w w+1. This
choice is tuned on the development data.

Word clusters that are automatically acquired
from large-scale unlabeled data have been shown
to be very effective to bridge the gap between high
and low frequency words, and therefore signifi-
cantly improve tagging, as well as other syntactic
processing tasks. Our stacking models are all built
on word clustering enhanced discriminative linear
models. Five word cluster uni/bi-gram features are

184



added: w−1, w, w+1, w−1 w, w w+1. The clus-
ters are acquired based on the Chinese giga-word
data with the MKCLS tool. The number of total
clusters is set to 500, which is tuned by (Sun and
Uszkoreit, 2012).

3.3 Evaluation
Table 3 summarizes the tagging accuracy of dif-
ferent stacking models. From this table, we can
clearly see that the new features derived from the
outputs of other models lead to substantial im-
provements over the baseline LLM/LGLM. The
output structures provided by the PCFGLA model
are most effective in improving the LLM/LGLM
baseline systems. Among different stacking mod-
els, the syntax-free hybrid one (i.e., stacking
LLM/LGLM with HMMLA) does not need any
treebank to train their systems. For the situa-
tions that parsers are not available, this is a good
solution. Moreover, the decoding algorithms for
linear-chain Markov models are very fast. There-
fore the syntax-free hybrid system is more appeal-
ing for many NLP applications.

Table 5 is the F1 scores of the DEC/DEG pre-
diction which are obtained by different stacking
models. Compared to Table 1, we can see that the
hybrid sequence model is still not good at handling
long-distance ambiguities. As a result, it harms
the parsing performance (see Table 4), though it
achieves higher overall precison.

Devel. DEC DEG
1or LGLM(HMMLA) 82.93 86.64
1or LGLM(PCFGLA) 88.11 91.12
1or LGLM(DEP) 87.46 89.86

Table 5: F1 score of the DEC/DEG prediction
of different stacking models on the development
data.

3.4 Related Work
(Sun and Uszkoreit, 2012) introduced a Bagging
model to effectively combine the outputs of in-
dividual systems. In the training phase, given a
training set D of size n, the Bagging model gener-
ates m new training sets Di’s by sampling exam-
ples from D. Each Di is separately used to train
k individual models. In the tagging phase, the km
models outputs km tagging results, each word is
assigned one POS label. The final tagging is the
voting result of these km labels. Although this
model is effective, it is too expensive in the sense

that it uses parser multiple times. We also imple-
ment their method and compare the results with
our stacking model. We find the accuracy perfor-
mance produced by the two different methods are
comparable.

(Rush et al., 2010) introduced dual decomposi-
tion as a framework for deriving inference algo-
rithms for serious combinatorial problems in NLP.
They successfully applied dual decomposition to
the combination of a lexicalized parsing model
and a trigram POS tagger. Despite the effective-
ness, their method iteratively parses a sentence
many times to achieve convergence, and thus is
not as efficient as stacking.

4 Improving Tagging Efficiency through
Unlabeled Data

4.1 The Idea

Hybrid structured models often achieve excellent
performance but can be slow at test time. In our
problem, it is obviously too inefficient to improve
POS tagging by parsing a sentence first. In this
section, we explore unlabeled data to transfer the
predictive power of hybrid models to sequence
models. The main idea behind this is to use a
fast model to approximate the function learned by
a slower, larger, but better performing ensemble
model. Unlike the true function that is unknown,
the function learned by a high performing model is
available and can be used to label large amounts of
pseudo data. A fast and expressive model trained
on large scale pseudo data will not overfit and will
approximate the function learned by the high per-
forming model well. This allows a slow, complex
model such as massive ensemble to be compressed
into a fast sequence model such as a first order
LGLM with very little loss in performance.

This idea to use unlabeled data to transfer the
predictive power of one model to another has been
investigated in many areas, for example, from high
accuracy neural networks to more interpretable de-
cision trees (Craven, 1996), from high accuracy
ensembles to faster and more compact neural net-
works (Bucila et al., 2006), or from structured
prediction models to local classification models
(Liang et al., 2008),

4.2 Reducing Hybrid Models to Sequence
Models

For English POS tagging, Liang, Daumé and
Klein (2008) have done some experiments to

185



Size of data HMMLA LLM LGLM LLM LGLM Voting
win size=3 win size=4 DEC/DEG

+100k 94.72% 95.05% 95.07% 95.04% 95.10% 95.36% - -
+200k 94.77% 95.06% 95.18% 95.20% 95.23% 95.43% - -
+500k 94.97% 95.11% 95.21% 95.15% 95.23% 95.43% - -
+1000k 95.09% 95.19% 95.23% 95.22% 95.31% 95.49% 85.75/89.01

Table 6: Tagging accuracies of different re-compiled models on the development data.

transfer the power of a chain conditional ran-
dom field to a logistic regression model. Simi-
larly, we do some experiments to explore the fea-
sibility of reducing hybrid tagging models to a
HMMLA, LLM or LGLM, for Chinese POS tag-
ging. The large-scale unlabeled data we use in our
experiments comes from the Chinese Gigaword
(LDC2005T14), which is a comprehensive archive
of newswire text data that has been acquired over
several years by the Linguistic Data Consortium
(LDC). We choose the Mandarin news text, i.e.
Xinhua newswire. We tag giga-word sentences
by applying the stacked first order LGLMs with
all other models. In other words, the HMMLA,
PCFGLA and DEP systems are applied to tag un-
labeled data features and their outputs are utilized
to define features for first-order and second-order
LGLMs which produce pseudo training data. Both
original gold standard training data and pseudo
training data are used to re-train a HMMLA, a
LLM/LGLM with extended features.

The key for the success of hybrid tagging mod-
els is the existence of a large diversity among
learners. Zhou (2009) argued that when there are
lots of labeled training examples, unlabeled in-
stances are still helpful for hybrid models since
they can help to increase the diversity among the
base learners. The author also briefly introduced
a preliminary theoretical study. In this paper, we
also combine the re-trained models to see if we
can benefit more. We utilize voting as the strategy
for final combination. In the tagging phase, the re-
trained LLM, LGLM and HMMLA systems out-
puts 3 tagging results, each word is assigned one
POS label. The final tagging is the voting result of
these 3 labels.

4.3 Experiments

4.3.1 Reducing Hybrid Models to HMMLA

With the increase of (pseudo) training data, a
HMMLA may learn better latent variables to sub-
categorize POS tags, which could significantly im-

prove a purely supervised HMMLA. In our experi-
ments, all HMMLA models are trained with 8 iter-
ations of split, merge, smooth. The second column
of Table 6 shows the performance of the re-trained
HMMLAs. The first column is the number of sen-
tences of pseudo sentences. The pseudo sentences
are selected from the begining of the Chinese giga-
word. We can clearly see that the idea to lever-
age unlabeled data to transfer the predictive ability
of the hybrid model works. Self-training can also
slightly improve a HMMLA (Huang et al., 2009).
Our auxiliary experiments show that self-training
is not as effective as our methods.

4.3.2 Reducing Hybrid Models to
LLM/LGLM

To increase the expressive power of a discrimi-
native classification model, we extend the feature
templates. This strategy is proposed by (Liang et
al., 2008). In our experiments, we increase the
window size of word uni/bi-gram features to ap-
proximate long distance dependencies. For win-
dow size 3, we will add w−3, w3, w−3w−2 and
w2w3 as new features; for size 4, we will add
w−4, w−3, w3, w4, w−4w−3, w−3w−2, w2w3 and
w3w4; Column 3 to 6 of Table 6 show the perfor-
mance of the re-compiled LLMs/LGLMs. Sim-
ilar to the generative model, the discriminative
LLM/LGLM can be improved too.

4.3.3 Voting

The last two columns of Table 6 are the final vot-
ing results of the HMMLA, LLM and LGLM. The
window size of word uni/bi-gram features for the
LLM and LGLM is set to 4. Obviously, the re-
trained models are still diverse and complemen-
tary, so the voting can further improve the se-
quence models. The result of the best hybrid
sequence model is very close to the best stack-
ing models. Furthermore, the F1 scores of the
DEC/DEG prediction are 85.75 and 89.01, which
are very close to parsers too.

186



4.3.4 Improving Parsing
Purely supervised sequence models are not good
at predicting function words, and accordingly are
not good enough to be used as front modules to
parsers. The re-compiled models can mimic some
behaviors of parsers, and therefore are suitable for
parsing. Our evaluation shows that the signifi-
cant improvement of the POS tagging stop harm-
ing syntactic parsing. Results in Table 7 indicate
that the parsing accuracy of the Berkeley parser
can be simply improved by inputting the Berke-
ley parser with the re-trained sequential tagging
results. Additionally, the success to separate tag-
ging and parsing can improve the whole syntactic
processing efficiency.

Devel. LP LR F1
HMMLA 82.18 81.16 81.66↑
LLM 81.86 80.93 81.40↑
LGLM 82.07 81.21 81.64↑
Voting 82.34 81.42 81.88↑

Table 7: Accuracies of parsing based on re-
compiled tagging.

4.3.5 Final results
Table 8 shows the performance of different sys-
tems evaluated on the test data. Our final se-
quence model achieve the state-of-the-art perfor-
mance, which is once obtained by combining mul-
tiple parsers as well as sequence models.

Systems Acc.
(Sun and Uszkoreit, 2012) 95.34%
Our system 95.33%

Table 8: Tagging accuracies on the test data.

5 Conclusion

In this paper, we study two techniques to build
accurate and fast sequence models for Chinese
POS tagging. In particular, our goal is to capture
long-distance dependencies in sequence models.
To improve tagging accuracy, we study stacking
to integrate multiple models with heterogeneous
views. To improve tagging efficiency at test time,
we explore unlabeled data to transfer the predic-
tive power of hybrid models to simple sequence
or even local classification models. Hybrid sys-
tems are utilized to create large-scale pseudo train-
ing data for cheap models. By applying complex

machine learning techniques, we are able to build
good sequential POS taggers. Another advantage
of our system is that it serves as a front-end to a
parser very well. Our study suggests that compli-
cated structured models can be well simulated by
simple sequence models through unlabeled data.

Acknowledgement

The work was supported by NSFC (61170166),
Beijing Nova Program (2008B03) and National
High-Tech R&D Program (2012AA011101).

References
Leo Breiman. 1996. Stacked regressions. Machine

Learning, 24:49–64, July.

Cristian Bucila, Rich Caruana, and Alexandru
Niculescu-Mizil. 2006. Model compression. In
KDD, pages 535–541.

Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL.

Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage ccg pars-
ing. In Proceedings of Coling 2004, pages 282–288,
Geneva, Switzerland, Aug 23–Aug 27. COLING.

Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of EMNLP, pages 1–8. Association for Computa-
tional Linguistics, July.

Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Lin-
guistics, 29(4):589–637.

Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. JOURNAL OF MA-
CHINE LEARNING RESEARCH, 7:551–585.

Mark Craven. 1996. Extracting Comprehensible Mod-
els from Trained Neural Networks. Ph.D. thesis,
University of Wisconsin-Madison, Department of
Computer Sciences. Also appears as UW Technical
Report CS-TR-96-1326.

Jan Hajič, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Antònia Martı́, Lluı́s
Màrquez, Adam Meyers, Joakim Nivre, Sebastian
Padó, Jan Štěpánek, Pavel Straňák, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.

187



Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun’ichi Tsujii. 2011. Incremental joint POS tag-
ging and dependency parsing in chinese. In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 1216–1224,
Chiang Mai, Thailand, November. Asian Federation
of Natural Language Processing.

Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram hmm
part-of-speech tagger by latent annotation and self-
training. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Companion Volume: Short Pa-
pers, pages 213–216, Boulder, Colorado, June. As-
sociation for Computational Linguistics.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
’01, pages 282–289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.

Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu,
Wenliang Chen, and Haizhou Li. 2011. Joint mod-
els for Chinese POS tagging and dependency pars-
ing. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1180–1191, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.

Percy Liang, Hal Daumé, III, and Dan Klein. 2008.
Structure compilation: trading structure for features.
In Proceedings of the 25th international conference
on Machine learning, ICML ’08, pages 592–599,
New York, NY, USA. ACM.

Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsu-
jii. 2005. Probabilistic cfg with latent annota-
tions. In Proceedings of ACL, ACL ’05, pages 75–
82, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL-08: HLT, pages
950–958, Columbus, Ohio, June. Association for
Computational Linguistics.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433–440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.

Alexander M Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natu-
ral language processing. In Proceedings of EMNLP,

pages 1–11, Cambridge, MA, October. Association
for Computational Linguistics.

Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined
tree substitution grammars for syntactic parsing. In
Proceedings of ACL, pages 440–448, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.

Weiwei Sun and Hans Uszkoreit. 2012. Capturing
paradigmatic and syntagmatic lexical relations: To-
wards accurate Chinese part-of-speech tagging. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics. Association
for Computational Linguistics, July.

Weiwei Sun. 2011. A stacked sub-word model for
joint Chinese word segmentation and part-of-speech
tagging. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1385–
1394, Portland, Oregon, USA, June. Association for
Computational Linguistics.

Daniel Tse and James R. Curran. 2012. The chal-
lenges of parsing chinese with combinatory cate-
gorial grammar. In Proceedings of the 2012 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies, pages 295–304, Montréal,
Canada, June. Association for Computational Lin-
guistics.

Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ana-
niadou. 2009. Fast full parsing by linear-chain
conditional random fields. In Proceedings of the
12th Conference of the European Chapter of the
ACL (EACL 2009), pages 790–798, Athens, Greece,
March. Association for Computational Linguistics.

David H. Wolpert. 1992. Original contribution:
Stacked generalization. Neural Netw., 5:241–259,
February.

Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn Chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207–238.

Yue Zhang and Stephen Clark. 2009. Transition-
based parsing of the Chinese treebank using a global
discriminative model. In Proceedings of the 11th
International Conference on Parsing Technologies
(IWPT’09), pages 162–171, Paris, France, October.
Association for Computational Linguistics.

Zhi-Hua Zhou. 2009. When semi-supervised learning
meets ensemble learning. In Proceedings of the 8th
International Workshop on Multiple Classifier Sys-
tems, MCS ’09, pages 529–538, Berlin, Heidelberg.
Springer-Verlag.

188


