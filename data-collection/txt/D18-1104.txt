



















































Compact Personalized Models for Neural Machine Translation


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 881–886
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

881

Compact Personalized Models for Neural Machine Translation

Joern Wuebker, Patrick Simianer, John DeNero
Lilt, Inc.

first_name@lilt.com

Abstract

We propose and compare methods for gradient-
based domain adaptation of self-attentive neu-
ral machine translation models. We demon-
strate that a large proportion of model param-
eters can be frozen during adaptation with
minimal or no reduction in translation qual-
ity by encouraging structured sparsity in the
set of offset tensors during learning via group
lasso regularization. We evaluate this tech-
nique for both batch and incremental adapta-
tion across multiple data sets and language
pairs. Our system architecture—combining a
state-of-the-art self-attentive model with com-
pact domain adaptation—provides high quality
personalized machine translation that is both
space and time efficient.

1 Introduction

Professional translators typically translate a collec-
tion of related documents drawn from a domain
for which they have a set of previously translated
examples. Domain adaptation is critical to provid-
ing high quality suggestions for interactive machine
translation and post-editing interfaces. When many
translators use the same shared service, the system
must train and apply a personalized adapted model
for each user. We describe a system architecture and
training method that achieve high space efficiency,
time efficiency, and translation performance by en-
couraging structured sparsity in the set of offset
tensors stored for each user.
Effective model personalization requires both

batch adaptation to an in-domain training set, as
well as incremental adaptation to the test set. Batch
adaptation is applied when a user uploads relevant
translated documents before starting to work. Incre-
mental adaptation is applied when a user provides a
correct translation of each segment just after receiv-
ing machine translation suggestions, and the system
is able to train on that correction before generating

suggestions for the next segment. This is referred
to as a posteriori adaptation by Turchi et al. (2017).
Our experiments compare both types of adaptation.
There are cases for which incremental adaptation
achieves better performance using fewer examples,
as examples drawn directly from the test set are
often highly relevant to subsequent parts of that test
set. There are also cases for which the gains from
both types of domain adaptation are additive.
The time required to translate and to adapt both

must be minimal in a personalized translation ser-
vice. Interactive translation requires suggestions to
be generated at typing speed, and incremental adap-
tationmust occur within a few hundredmilliseconds
to keep up with a translator’s typical workflow. The
service can be expected to store models for a large
number of users and dynamically load and adapt
models for many active users concurrently. There-
fore, minimizing the number of parameters stored
for each user’s personalizedmodel is important both
for reducing storage requirements and latency. We
achieve space and time efficiency by representing
each user’s model as an offset from the unadapted
baseline parameters and encouraging most offset
tensors to be zero during adaptation.
We show that group lasso regularization can be

applied to a self-attentive Transformer model to
freeze up to 75% of the parameters with minimal
or no loss of adapted translation quality across ex-
periments on four English→German data sets. We
confirm these findings for six additional language
pairs.

2 Related Work

There is extensive work on incremental adapta-
tion from human post edits or simulated post edits,
both for statistical machine translation (Green et al.,
2013; Denkowski et al., 2014a,b; Wuebker et al.,
2015) and neural machine translation (Peris et al.,



882

2017; Turchi et al., 2017; Karimova et al., 2017).
Both Turchi et al. (2017) and Karimova et al. (2017)
apply vanilla fine-tuning algorithms. In addition
to fine-tuning towards user corrections, the former
applies a priori adaptation to retrieved data that
is similar to the incoming source sentences. Peris
et al. (2017) propose a variant of fine-tuning with
passive-aggressive learning algorithms. In contrast
to these papers, where all model parameters are
possibly altered during training, this work focuses
on space efficiency of the adapted models.

Regularization methods that promote or enforce
sparsity have been previously used in the context of
sparse feature models for SMT: Duh et al. (2010)
presented an application of multi-task learning via
`1/`2 regularization for feature selection in an N -
best reranking task. A similar approach, employ-
ing `1/`2 regularization for feature selection and
multi-task learning, was developed by Simianer
et al. (2012) and Simianer and Riezler (2013) for
tuning of SMT systems. Both works report improve-
ments from regularization.

Techniques for enforcing sparse models using `1
regularization during stochastic gradient descent
optimization were previously developed for linear
models (Tsuruoka et al., 2009).

An extremely space efficient method for person-
alized model adaptation is presented by Michel
and Neubig (2018). Here, adaptation is performed
solely on the output vocabulary bias vector. An-
other notable approach for creating compact mod-
els is student-teacher-training or knowledge distil-
lation (Kim and Rush, 2016). To the best of our
knowledge, this has not been applied in a domain
adaptation setting.

3 Self-Attentive Translation Model

The neural machine translation systems used in this
work are based on the Transformer model intro-
duced by Vaswani et al. (2017), which uses self-
attention rather than recurrent or convolutional lay-
ers to aggregate information across words. In addi-
tion to its superior performance, its main practical
advantage over recurrent models is faster training.
The Transformer follows the encoder-decoder

paradigm. Source word vectors x1, . . . , xm are
chosen from an embedding matrix Xe. A series
of stacked encoder layers generate intermediate
representations z1, . . . , zm. Each layer of the en-
coder consists of two sub-layers: a multi-head self-
attention layer that uses scaled dot-product atten-

tion over all source positions, followed by a feed-
forward filter layer. Layer normalization (Ba et al.,
2016), dropout (Srivastava et al., 2014), and resid-
ual connections (He et al., 2016) are applied to each
sub-layer.
A series of stacked decoder layers produces a

sequence of target word vectors y1, . . . , yn. Each
decoder layer has three sub-layers: self-attention,
encoder-attention, and a filter. For target position j,
the self-attention layer can attend to any previous
target position j′ ∈ [1, j], with target words offset
by one so that representations at j can observe word
j−1, but not word j. The encoder-attention layer
can attend to the final encoder state zi for any source
position i ∈ [1,m]. Observed target word vectors
are chosen from an embedding matrix Ye, and target
word j is predicted from yj via a soft-max layer
parameterized by an output projection matrix Yo.
The encoders in this work have six layers that

have a self-attention sub-layer size of 256 and a
filter sub-layer size of 512. Each filter performs
two linear transformations and a ReLU activation:

f(x) = max(0, xW1 + b1)W2 + b2.

The decoders in this work have three layers,
and all sub-layer sizes are 256. The decoder sub-
layers are simplified versions of those described in
Vaswani et al. (2017): The filter sub-layers perform
only a single linear transformation, and layer nor-
malization is only applied once per decoder layer
after the filter sub-layer.

Unlike in Vaswani et al. (2017), none of Xe, Ye,
or Yo share parameters in our TensorFlow1 imple-
mentation. Baseline models are optimized with
Adam (Kingma and Ba, 2015).

4 Compact Adaptation

4.1 Fine Tuning
Personalized machine translation requires batch
adaptation to a domain-relevant bitext (such as a
user provided translation memory) as well as in-
cremental adaptation to the test set. We apply fine-
tuning, which involves continuing to train model pa-
rameters with a gradient-based method on domain-
relevant data, as a simple and effective method for
neural translation domain adaptation (Luong and
Manning, 2015). The fine-tuned model without
regularization and clipping is denoted as the Full
Model. Confirming previous work, we found that

1https://www.tensorflow.org/

https://www.tensorflow.org/


883

stochastic gradient descent (SGD) is the most effec-
tive optimizer for fine tuning (Turchi et al., 2017).
In our experiments, batch adaptation uses a batch
size of 7000 words for 10 Epochs and a fixed learn-
ing rate of 0.1, dropout of 0.1, and label smoothing
with �ls = 0.1 (Szegedy et al., 2016).

Incremental adaptation uses a batch size of one
and a learning rate of 0.01. To ensure a strong
adaptation effect within a single document, we set
dropout and label smoothing to zero and perform up
to three SGD updates on each segment. After each
update, we measure the model perplexity on the
current training example and continue with another
update if the perplexity is still above 1.5.

4.2 Offset Tensors

In a personalized translation service, adapted mod-
els need to be loaded quickly, so a space-efficient
representation is critical for time efficiency as well.
Production speed requirements using contemporary
cloud hardware limit model sizes to roughly 10M
parameters per user, while a high-quality baseline
Transformer model typically requires 35M parame-
ters or more. We propose to store the parameters
of an adapted model as an offset from the baseline
model. Each tensor is a sumW = Wb+Wu, where
Wb is from the baseline model and is shared across
all adapted models, while the offsetWu is specific
to an individual user domain. Space efficiency is
achieved by only storingWu for a subset of tensors
and setting the rest of the offset tensors to zero.
One approach to achieving model sparsity is to

manually partition the network into a small number
of regions and systematically evaluate translation
performance when storing offsets for only one re-
gion. We define five distinct regions, which are
evaluated in isolation: Outer layers (the first and
last layers of both encoder and decoder), inner lay-
ers (all the remaining layers), the two embedding
matrices Xe and Ye, and the output projection ma-
trix Yo. The latter three are each stored as a single
matrix and each contributes 10.3M parameters to
the full model size in English→German. During
adaptation, the embedding matrices are only up-
dated for vocabulary present in the training exam-
ples, and so the offsets can be stored efficiently as
a sparse collection of columns. The same principle
can be applied to the output projection matrix by
only updating parameters corresponding to vocabu-
lary items that appears in the adaptation examples
(denoted Sparse Output Proj. in Table 1).

A second approach to achieving model sparsity
is to use a procedure to select the subset of offset
tensors that are stored. For example, we evaluate
a simple policy that stores an offset for all tensors
whose average change in parameter values is higher
than a threshold. This set is selected on a develop-
ment domain and held fixed for all other domains.
We refer to this method as fixed adaptation.

4.3 Tensor Selection via Group Lasso

A group sparse regularization penalty such as group
lasso can be applied to the offset tensors for simul-
taneous regularization and tensor selection. This
penalty drives entire offset tensors to zero, so that
they do not need to be stored or loaded. We add the
following regularization term to the loss function
(Scardapane et al., 2017):

R`1,2(T ) =
∑
T∈T

√
|T |‖∆T‖2 (1)

‖∆T‖2 =
∑
τ∈T

∆τ2 (2)

Here, each tensor corresponds to one group. T
denotes the set of all tensors in the model, τ ∈ T
the set of all weights within a single tensor and ∆τ
the size of the offset for τ . Note that we are regu-
larizing the difference between the parameters of
the adapted model and the baseline model, rather
than regularizing the full network parameters di-
rectly. In this way, we maintain the expressive
power of the full network while minimizing the
size of the adapted models. Group lasso regular-
ization is equivalent to `1 regularization when the
group size is 1. Sparsity among groups is encour-
aged because the `1 norm serves as a convex proxy
for the `0 norm, which would explicitly penalize
the number of non-zero elements (Yuan and Lin,
2006). To facilitate tensor selection, we define a
threshold ϑ to clip offset tensors ∆T with aver-
age weight 1|T |

∑
τ∈T ∆τ < ϑ to zero. Both the

threshold ϑ and the regularization weight λ were
manually tuned on a development domain and set
to ϑ = 10−4 and λ = 10−6. We apply clipping to
all tensors except the embedding and output pro-
jection matrices Xe, Ye and Yo. As our production
constraints allow us to retain only one of the three,
we pre-select the sparse output projection as part
of the model and exclude the embedding matrices
from adaptation. This method will be denoted as
Lasso.



884

User1 User2 Autodesk IWSLT
# Param. Batch Incr. Batch Incr. Batch Incr. Batch Incr.

Baseline 36.2M 35.7 32.7 40.3 25.9

Full Model 25.8M 47.5 48.2 44.2 34.8 47.7 46.6 27.5 26.3

Outer Layers 2.2M 45.0 47.9 36.4 33.1 45.5 44.7 27.3 26.1
Inner Layers 2.7M 45.4 47.2 36.7 33.6 45.5 43.9 27.8 26.5
Encoder Embed. 5.0M 41.7 41.8 33.6 32.9 42.5 41.6 27.4 26.4
Decoder Embed. 5.5M 36.6 37.6 33.0 33.0 40.8 40.5 26.2 25.9
Output Proj. 10.3M 44.2 46.0 38.1 34.5 45.2 42.9 27.1 26.5
Sparse Output Proj. (*) 5.5M 43.5 46.7 39.7 34.7 45.5 43.3 27.1 26.7

(*) + Fixed 6.9M 46.4 47.8 42.3 30.9 47.6 43.7 27.3 26.0
(*) + Lasso 6.7M 47.6 46.6 43.1 33.2 47.9 41.5 27.5 27.0

Full Model Batch+Incr. 25.9M 50.6 41.8 52.6 27.0
(*) + Lasso Batch+Incr. 9.2M 51.3 39.1 51.1 27.6

Repetition Rate Source 11.0 8.8 18.3 9.2
Repetition Rate Target 9.4 8.7 17.5 7.5

Table 1: Experimental results in Bleu (%) on the English→German data. We evaluate changes to each
region of the network separately. In combination with sparse output projection, we also evaluate a fixed
selection of parameters chosen by thresholding and a set selected dynamically for each data set using group
lasso. The two bottom rows show repetition rates in % for the source and target sides of the test data.

5 Experiments
5.1 Data
We first evaluate all techniques on an
English→German Transformer network trained on
98M parallel sentence pairs. We apply byte pair
encoding (Sennrich et al., 2016) separately to each
language and obtain vocabularies with 40K unique
tokens each. We refer to the unadapted model
as Baseline. We evaluate on four domains. For
development, we use a data set labeled User1 that
was gathered from a user of the browser-based CAT
(computer-aided translation) tool Lilt2 and contains
documents from the financial domain with 48K
segments for batch adaptation and 1790 segments
for testing and incremental adaptation. We further
evaluate on a second user test set User2 (technical
support, 31k batch adaptation, 1000 test segments);
the public Autodesk corpus3, where we select the
first 20k segments for batch adaptation and the
next 1000 segments for testing; and the IWSLT
corpus4 (semi-technical talks), where we use all
provided 206K sentences for batch adaptation

2https://lilt.com
3https://autodesk.app.box.com/

Autodesk-PostEditing
4http://workshop2017.iwslt.org/

and the dev2010 set (888 sentences) for testing.
The overall best performing compact adaptation
technique, group lasso regularization, is further
evaluated on six other language pairs trained using
production data sets collected from Lilt’s user
base: English↔French, English↔Russian and
English↔Chinese. Adaptation is performed on
user data from various domains (technical manuals,
finance, legal), each with 8k-10k segments for
batch adaptation and 2000 segments for testing
and incremental adaptation. Translation quality is
evaluated using the cased Bleu (Papineni et al.,
2002) measure.

5.2 Results

Table 1 shows English→German results. Full
model adaptation, where all offsets are stored, im-
proves over the baseline in all cases to various de-
grees. This full model contains only 25.8M param-
eters, as offsets for both embedding matrices are
stored as sparse collections of columns for the vo-
cabulary present in the adaptation data. Next, we
evaluate the impact of storing offsets only for one
region at a time. We observe that among the three
vocabulary matrices, the output projection Yo has
the strongest impact on quality, which is not dimin-

https://lilt.com
https://autodesk.app.box.com/Autodesk-PostEditing
https://autodesk.app.box.com/Autodesk-PostEditing
http://workshop2017.iwslt.org/


885

en→fr fr→en en→ru ru→en en→zh zh→en Avg.

Baseline 28.8 35.8 10.7 29.2 19.9 18.9 23.9
Full Model Batch+Incr. 36.6 49.6 21.0 42.1 40.6 46.6 39.4
(*) + Lasso Batch+Incr. 36.0 46.3 19.8 42.7 39.3 45.0 38.2

Table 2: Experimental results in Bleu (%) on six production language pairs. We compare the unadapted
baseline model with a full model and the model with sparse output projection and group lasso, both with
application of batch and incremental adaptation.

ished by storing a sparse variant that is restricted
only to observed vocabulary.

In addition, we evaluate twomethods of choosing
a subset of tensors procedurally. We first experi-
ment with a fixed subset of tensor offsets that was
chosen by selecting all tensors for which parame-
ters were offset by more than 0.002 on average after
batch adaptation on the User1 data set. This sim-
ple procedure approaches the performance of full
model adaptation, but stores only 27% of its param-
eters. Dynamically selecting tensor offsets for each
data set using group lasso regularization improves
performance on 6 out of 8 data conditions.

The combination of batch and incremental adap-
tation yields further improvements, with the excep-
tion of the User2 and IWSLT tasks, where incre-
mental adaptation overall performs not as well as
batch adaptation. For these tasks, both tests sets
exhibit lower repetition rates5 (Cettolo et al., 2014)
than the test sets for the two other tasks (see the two
bottom lines in Table 1). The User2 test set is fur-
thermore a random sample of non-consecutive text
from a translation memory, which is suboptimal for
incremental learning.
Altogether, we are able to achieve translation

performance similar to full model adaptation with
25% of the total network parameters. Note that due
to the selection of entire tensors with groupwise
regularization, there is nearly zero space overhead
incurred by storing a sparse set of offset tensors.
Table 2 confirms our main findings on six other

language pairs. We observe average improvements
of 14.3 Bleu with our final compact model, which
compares to 15.5 Bleu for full model adaptation.

6 Conclusion

We describe an efficient approach to personalized
machine translation that stores a sparse set of ten-

5Repetition rates have been confirmed to be a suitable in-
dicator for gains through incremental adaptation in numerous
works (Wuebker et al., 2015; Bertoldi et al., 2014).

sor offsets for each user domain. Group lasso regu-
larization applied to the offsets during adaptation
achieves high space and time efficiency while yield-
ing translation performance close to a full adapted
model, for both batch and incremental adaptation
and their combination.

References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-

ton. 2016. Layer normalization. arXiv preprint
arXiv:1607.06450.

Nicola Bertoldi, Patrick Simianer, Mauro Cettolo,
Katharina Wäschle, Marcello Federico, and Stefan
Riezler. 2014. Online adaptation to post-edits for
phrase-based statistical machine translation. Ma-
chine Translation, 28(3-4):309–339.

Mauro Cettolo, Nicola Bertoldi, andMarcello Federico.
2014. The repetition rate of text as a predictor of the
effectiveness of machine translation adaptation. In
Conference of the Association for Machine Transla-
tion in the Americas (AMTA), pages 166–179, Van-
couver, Canada.

Michael Denkowski, Chris Dyer, and Alon Lavie.
2014a. Learning from post-editing: Online model
adaptation for statistical machine translation. In Pro-
ceedings of the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 395–404, Gothenburg, Sweden. As-
sociation for Computational Linguistics.

Michael Denkowski, Alon Lavie, Isabel Lacruz, and
Chris Dyer. 2014b. Real time adaptive machine
translation for post-editing with cdec and transcen-
ter. In Proceedings of the EACL 2014 Workshop on
Humans and Computer-assisted Translation, pages
72–77, Gothenburg, Sweden. Association for Com-
putational Linguistics.

Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, Hideki
Isozaki, and Masaaki Nagata. 2010. N-best rerank-
ing by multitask learning. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 375–383, Uppsala,
Sweden. Association for Computational Linguistics.



886

Spence Green, Sida Wang, Daniel Cer, and Christo-
pher D. Manning. 2013. The efficacy of human post-
editing for language translation. InACMCHIConfer-
ence on Human Factors in Computing Systems, Paris,
France.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770–
778.

Sariya Karimova, Patrick Simianer, and Stefan Riezler.
2017. A user-study on online adaptation of neu-
ral machine translation to human post-edits. CoRR,
abs/1712.04853.

Yoon Kim and Alexander M. Rush. 2016. Sequence-
level knowledge distillation. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1317–1327, Austin,
Texas. Association for Computational Linguistics.

Diederik Kingma and Jimmy Lei Ba. 2015. Adam: A
method for stochastic optimization. In ICLR.

Minh-Thang Luong and Christopher D. Manning. 2015.
Stanford neural machine translation systems for spo-
ken language domain. In International Workshop on
Spoken Language Translation, Da Nang, Vietnam.

Paul Michel and Graham Neubig. 2018. Extreme adap-
tation for personalized neural machine translation.
In Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 312–318. Association for Com-
putational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL.

Álvaro Peris, Luis Cebrián, and Francisco Casacuberta.
2017. Online learning for neural machine translation
post-editing. CoRR, abs/1706.03196.

Simone Scardapane, Danilo Comminiello, Amir Hus-
sain, and Aurelio Uncini. 2017. Group sparse regu-
larization for deep neural networks. Neurocomput.,
241(C):81–89.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715–
1725, Berlin, Germany. Association for Computa-
tional Linguistics.

Patrick Simianer and Stefan Riezler. 2013. Multi-
task learning for improved discriminative training in

SMT. In Proceedings of the EighthWorkshop on Sta-
tistical Machine Translation, pages 292–300, Sofia,
Bulgaria.

Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in SMT. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, Volume 1:
Long Papers, pages 11–21, Jeju Island, South Korea.
Association for Computational Linguistics.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jon Shlens, and Zbigniew Wojna. 2016. Rethinking
the inception architecture for computer vision. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2818–2826.

Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumula-
tive penalty. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1-Volume 1,
pages 477–485. Association for Computational Lin-
guistics.

Marco Turchi, Matteo Negri, M Amin Farajian, and
Marcello Federico. 2017. Continuous learning
from human post-edits for neural machine transla-
tion. The Prague Bulletin of Mathematical Linguis-
tics, 108(1):233–244.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, 4-9 Decem-
ber 2017, Long Beach, CA, USA, pages 6000–6010.

Joern Wuebker, Spence Green, and John DeNero. 2015.
Hierarchical incremental adaptation for statistical
machine translation. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1059–1065.

Ming Yuan and Yi Lin. 2006. Model selection and es-
timation in regression with grouped variables. Jour-
nal of the Royal Statistical Society: Series B (Statis-

tical Methodology), 68(1):49–67.


