















































Transductive Minimum Error Rate Training for Statistical Machine Translation


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 641–648,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Transductive Minimum Error Rate Training for Statistical Machine
Translation

Yinggong Zhao1∗, Shujie Liu2, Yangsheng Ji1, Jiajun Chen1, Guodong Zhou3
1State Key Laboratory for Novel Software Technology at Nanjing University

Nanjing 210093, China
{zhaoyg,jiys,chenjj}@nlp.nju.edu.cn

2School of Computer Science and Technology, Harbin Institute of Technology
Harbin, China

shujieliu@mtlab.hit.edu.cn
3School of Computer Science and Technology, Soochow University

Suzhou, China
gdzhou@suda.edu.cn

Abstract

This paper investigates parameter adap-
tation in Statistical Machine Transla-
tion(SMT). To overcome the parameter
bias-estimation problem with Minimum
Error Rate Training(MERT), we extend it
under a transductive learning framework,
by iteratively re-estimating the parame-
ters using both development and test da-
ta, in which the translation hypotheses
of the test data are used as pseudo ref-
erences. Furthermore, in order to over-
come the over-training and unstableness
problems respectively in employing such
pseudo references, a termination criterion
using a hyper-parameter and a Minimum
Bayes Risk(MBR)-based hypothesis se-
lection method are proposed in our work.
Experimental results show that the trans-
ductive MERT method could yield signif-
icant performance improvements over a
strong baseline on a large-scale Chinese-
to-English translation task.

1 Introduction

Machine translation (MT) is the automatic trans-
lation from one natural language into another us-
ing computer, while SMT is an approach to MT
that is characterized by the use of machine learn-
ing methods (Lopez, 2008). Nowadays, SMT is
usually built on a log-linear model (Och and Ney,
2002), which can be abstracted into two steps: the

∗Part of this work is done during the first author’s intern-
ship at Microsoft Research Asia.

first one is model training, i.e., learning features
from large collection of bilingual parallel corpus;
and the other is parameter estimation, in which the
feature weight is tuned on an independent devel-
opment dataset.

More specifically, for each source sentence f ,
we search for its final translation e∗ among all pos-
sible translations based on the following equation:

P (e∗|f) = argmax
e
Pr(e|f) (1)

Under the log-linear model, the posterior proba-
bility Pr(e|f) can be decomposed as:

Pr(e|f) = pλ(e|f)

=
exp(

∑M
m=1(λm · hm(e, f)))∑

e′ exp(
∑M
m=1(λm · hm(e′, f)))

(2)

where each hm(e, f) is a feature function and λm
is the corresponding weight for m=1,. . . ,M.

In SMT, there are three sections of data: the
training data for feature estimation, the develop-
ment data for weight tuning, and the test data for
final evaluation. However, these three parts may
belong to different domains, leading to distribu-
tion variations, which indicates that the features
and weights learned from the data may be bias-
estimated. As a result, the adaptation for features
and their corresponding weights are both impor-
tant issues in SMT.

In this article we focus on the latter issue, i.e.,
the model parameter adaptation. From the view-
point of machine learning, development data is la-
beled data used for parameter learning, while test
data is unlabeled and applied for evaluation. In

641



the previous works of transductive learning(Liu et
al., 2010; Chan and Ng, 2007), the unlabeled da-
ta can be used to improve the model training so
as to tackle the bias-estimation problem. Under
such framework, the weight learned on both devel-
opment and test dataset, in which the test dataset
is constructed using n-best translations as pseudo
references, moves towards the test data with reg-
ularization of development data, which alleviates
the overtraining in normal MERT and matches the
test data better.

The remainder of this paper is structured as fol-
lows: Related works on model adaptation in SMT
are presented in Section 2, and our transductive
MERT is proposed in Section 3. Experimental re-
sults are shown in Section 4, followed by conclu-
sions and future work in the last section.

2 Related Work

Model adaptation in SMT has attracted increasing
attentions in recent years. As mentioned in the
previous section, corresponding to the two steps in
SMT pipeline, there are two directions for adapta-
tions.

The first one is feature adaptation, which tries to
build model (translation model & language model)
that could fit the development or test dataset better.
This direction includes data selection (Lü et al.,
2007; Hildebrand et al., 2005) and data weighting
(Foster and Kuhn, 2007; Matsoukas et al., 2009).
However, efficiency is the main obstacle for these
methods (esp. data selection approach) since mod-
el building is time consuming.

The second direction is model parameter adap-
tation, which includes the transductive MERT
method we propose in this article. Nevertheless,
little attention has been paid to this direction to
date. Mohit et al (2009) tried to build a classifier
to predict whether or not a phrase is difficult. The
language model weight is then adapted for each
phrase segment based on this difficulty. In Li et
al (2010), a related subset of development dataset
is extracted for given test dataset. The test dataset
is then translated under weight learned on this
subset. Besides, Sanchis-Trilles and Casacuber-
ta (2010) propose Bayesian adaptation for weight
optimization based on a small amount of labeled
test data, which is not necessary in our work.

The most similar previous work with ours is U-
effing et al (2007), who also propose a transduc-
tive learning framework for SMT. However, our

method is different from their in the following
three aspects:

Firstly, our method focuses on model parameter
adaptation, while Ueffing et al (2007) pays atten-
tion to feature adaptation. In their work, the train-
ing model is rebuilt by combining original train-
ing data with n-best translation outputs of devel-
opment and test data, in order to overcome the da-
ta sparseness problem. In contrast, we try to solve
the parameter bias-estimated problem using the in-
formation of both development and test data.

Secondly, the parameter adaptation problem is
more complicated in SMT, since overtraining is
serious due to the limited size of development da-
ta. In this work, we use hyper-parameter to indi-
cate the overtraining in the estimation step.

Finally, our method is more efficient than adap-
tation on the translation & language model. In U-
effing et al.(2007), training model building is nec-
essary for each round, which is time consuming.
By comparison, the running time is much short-
er for our method, since no model building is re-
quired, although it is still longer than simple one-
pass translation under baseline.

3 Transductive MERT for Machine
Translation

3.1 Minimum Error Rate Training

In SMT, given a development dataset containing
source sentences FS1 with corresponding refer-
ence translations RS1 , the purpose of MERT (Och,
2003) is to find a set of parameters λM1 which
optimizes an automated evaluation metric (e.g.,
BLEU) under a log-linear model:

λ̂M1 = argmin
λM1

S∑

s=1

(Err(Rs, Ê(Fs;λ
M
1 ))) (3)

in which the number of errors in sentence E is ob-
tained by comparing it with a reference sentence
R using function Err(R,E) and

Ê(Fs;λ
M
1 ) = argmax

E

S∑

s=1

(λmhm(E,Fs)) (4)

As shown in Algorithm 1, the decoder translates
development dataset under current weight(default
weight for first round), and generates N-best trans-
lation hypotheses for each sentence. The weight is
then updated according to equation 3. This proce-
dure repeats until performance converges.

642



Algorithm 1 MERT for SMT
Input: Development data :{FS1 , RS1 , CS1 }
Set λ= init-weight and CS1 ={}
Translate FS1 and get N-Best list L

S
1

while CS1 != CS1
⋃
LS1 do

CS1 = C
S
1

⋃
LS1

Update λ using translation candidates CS1
Translate FS1 using λ to generate N-Best list
LS1

end while

3.2 Transductive MERT

The basic idea of transductive learning is to use
predicted labels from unlabeled data to improve
learning performance. Based on the this assump-
tion and normal MERT method, our transductive
MERT (T-MERT) works as follows: Firstly, the
feature weight is estimated on the developmen-
t data with references. Test dataset is then trans-
lated using current weight. For each source sen-
tence of test data, its 4-best translations are used
as pseudo references. The feature weight is fur-
ther re-estimated based on both the development
dataset and the test dataset with pseudo references.
Meanwhile, the pseudo references of test dataset
are replaced in each round, while the development
dataset is fixed throughout the procedure. The w-
hole process runs M rounds so that we could get M
different results, which are used in the hypothesis
selection step (discussed in section 3.4).

As shown in Algorithm 2, the T-MERT algo-
rithm could be divided into two loops: in the out-
er loop (outer-translation step), the test dataset is
translated under current weight and new pseudo
labeled test dataset is constructed; while in inner
loop (inner-MERT step), the parameter weight is
learned from the combined dataset. Meanwhile,
there still remains two problems in algorithm 2:
when the loop will terminate in inner-MERT step,
and how we can select final hypothesis from the
multiple results for test data T . These two issues
will be discussed respectively in following parts of
this section.

3.3 Stop Criterion

In MERT, the loop terminates until the perfor-
mance converges. While in T-MERT, the weight
would be overtrained to the pseudo references of
the test data, which could not guarantee that the
translations obtained in each iteration are good e-

Algorithm 2 Transductive MERT for SMT
Input: Development data {DS1 , RS1 , CS1 }, Test
data {TW1 }, total round M
Let L={DS1 , RS1 , CS1 } and U= {TW1 }
Do MERT based on L and get weight λ
Let CS1 ={}
Translate U under λ and get N-Best list NW1
for i = 1 to M do

Select 4-best translations to build ŨW1 from
NW1
Let L = {{DS1 , RS1 , CS1 }

⋃ {TW1 , ŨW1 ,
NW1 }}
Set λ= init-weight
repeat

Translation L and get N-Best translations
LBS+W1
Let L = L

⋃
LBS+W1

Update λ on L
until Certain condition satisfies(Section 3.3)
Translate U under λ and get N-Best list NW1 ,
in which we select 1-best translation as Ti

end for
Select final translation(Section 3.4) from collec-
tions Ti (i=1, . . . , M)

nough. Here we introduce a hyper-parameter H to
indicate the overtraining. In each inner round i, let
SDi stands for the BLEU score of developmen-
t data D, SpTi represents the BLEU score of test
data T under pseudo references and SDpTi indi-
cates the BLEU score of combined dataset L , then
we define the hyper-parameter Hi as follows:

Hi =
SDi
SDi−1

· exp SpTi
SpTi−1

· exp SDpTi
SDpTi−1

(5)

Here, Hi represents the relative improvement be-
tween the performance of inner round i and that of
the previous round. A smaller Hi value indicates
the inner-MERT turns to be converged on com-
bined dataset L, showing that the weight would
be overtrained. Due to the fact that the test dataset
owns no references, we cannot attain its BLEU s-
core in each round. As an alternative, we could
only obtain SDi, SpTi and SDpTi, as shown in
above equation. In optimization step of normal
MERT, what we need to do is to update the param-
eter to maximize the score on development data.
While here we encounter the overtraining, we use
ratio of scores to indicate the training. Instead of
maximizing the score, we want to optimize the rel-
ative improvement of the system performance. In

643



T-MERT, we observe that the performance is al-
ways the best when the inner-MERT terminates as
Hi reaches peak1.

3.4 Hypothesis Selection with Minimum
Bayes Risk(MBR)

From T-MERT algorithm, we can get M different
results from M outer-translation rounds. Due to in-
trinsic property and the randomness in MERT, the
results from outer-translation step of T-MERT are
not quite stable, making the hypotheses selection
a necessity.

According to (Ehling et al., 2007), for each
source sentence with N different translations, we
could select the final translation based on the fol-
lowing Minimum Bayes Risk principal:

ê = argmin
e
{
∑

e′
(Pr(e′|f) ·(1−BLEU(e′, e)))}

(6)
Here Pr(e′|f) denotes the posterior probability
for translation e′ and BLEU(e′, e) represents the
sentence-level BLEU score for e′ using e as refer-
ence.

However, since the translation hypotheses are
generated under different groups of weights, the
corresponding posterior probability is no longer
comparable. Here we simplify this problem un-
der the assumption that all available translations
are generated equally. Then equation 6 could be
converted into:

ê = argmin
e
{
∑

e′
(1−BLEU(e′, e))} (7)

Based on equation 7, we can select the hypoth-
esis from the collections of translations efficient-
ly. And the primary purpose of using MBR se-
lection in this work is to stabilize the translation
performance, as we select final result using only
sentence-level BLEU scores between different hy-
potheses.

4 Experimental Results

4.1 Experimental Setup
In the experiments, we re-implement a hierar-
chical phrase-based decoder based on (Chiang,
2005). The word alignment is trained by GIZA++
under an intersect-diag-grow heuristics refinemen-
t. The plain phrases are extracted from all bilin-
gual training data available from LDC, includ-
ing LDC2002E18, LDC2003E07, LDC2003E14,

1And we find that in experiments, hyper-parameter Hi of
the second round is always maximal.

Table 1: Statistics on development and test data
sets.

DATA SET #SENTENCE #WORD
MT03 919 36,021
MT05 1,082 43,765
MT06 1,664 38,209
MT08 1,357 33,042

LDC2004E12, LDC2004T08, LDC2005E83, LD-
C2005T06, LDC2005T10, LDC2006E26, LD-
C2006E34, LDC2006E85, LDC2006E92, and
LDC2007T09, which consists in total of about
8.5M sentence pairs while hierarchical rules are
only extracted from selected data sets, includ-
ing LDC2003E14, LDC2003E07, LDC2005T10,
LDC2006E34, LDC2006E85, and LDC2006E92,
which contain about 467K sentence pairs. We
build the 5-gram language model on the English
section of all bilingual training data together with
the Xinhua portion of the English Gigaword cor-
pus.

The development and test dataset pairs are
selected from NIST2003 (MT03), NIST2005
(MT05), NIST2006 (MT06) and NIST2008
(MT08). The data statistics are shown in Table 1.
In the experiments, all translation results are mea-
sured in case-insensitive BLEU scores (Papineni
et al., 2002).

4.2 Results under Transductive MERT

Figure 1 and Figure 2 show the hyper-parameter
H(iternumber) for each iteration in the 10-round
inner-MERT step. In both figures, MT03 is de-
velopment dataset while MT05 & MT08 are test
datasets. We find that the hyper-parameter H al-
ways reaches the peak at the 2nd iteration, show-
ing fast convergence during parameter estimation
on the combination of development data and pseu-
do labeled test data. Similar phenomenon could
also be observed on other dataset pairs.

Here, the reason might be that the pseudo trans-
late references for the test data are generated with
the current SMT model and its parameters. So
the newly generated translation references on the
test data are intuitively similar to translations ob-
tained using the current model parameters. When
we re-estimate the parameters on the combined
dataset starting from the initial parameters, the
learning procedure can quickly fit the newly gen-
erated data. While parameter estimation step con-
tinues iteratively, the learning algorithm may fa-

644



vor those incorrectly generated translation refer-
ences, which makes the overtraining more serious
and hurts the final performance. By applying the
hyper-parameter as the stop metric, we could con-
trol the learning procedure to avoid the overtrain-
ing.

We can also review the roles that the develop-
ment and test datasets play in the procedure of
avoiding over-training. The reason for that we
transductively generate translations as pseudo ref-
erences for test data is that we expect the estima-
tion procedure biases towards the test data when
incorporated in the learning procedure. Mean-
while, the development data also plays an impor-
tant role in the learning process. Because develop-
ment data owns true references, it acts as a regular-
ization term to ensure that the feature weight will
not excessively biased toward the test data with
generated pseudo references in the learning pro-
cedure.

1 2 3 4 5 6 7 8 9 10
1.5

2

2.5

3

3.5

4

4.5

5

inner−MERT Round

ln
(H

yp
er

−
pa

ra
m

et
er

)

Hyper−parameter VS Round, MT03:Dev and MT05:Test

 

 
OuterRound−1
OuterRound−2
OuterRound−3
OuterRound−4
OuterRound−5

Figure 1: Hyper-parameter of 10 inner-MERT
loops under 5 outer-translation rounds(OutRound-
i) in T-MERT algorithm(without MBR),
MT03:Dev and MT05:Test.

Based on the above discussion, we also compare
results under different rounds for inner-MERT to
verify the role of the hyper-parameter. As shown
in figure 3(MT03 development and MT05 test)
and figure 4(MT03 development and MT08 test),
the results under T-MERT with 2-rounds inner-
MERT are always best among different rounds,
which is close to the baseline in figure 3 and much
better in figure 4. Here the baseline for the test
dataset is translated under weight learned from
normal MERT on the development data, and re-
mains constant for the following parts. For both

1 2 3 4 5 6 7 8 9 10
1.5

2

2.5

3

3.5

4

4.5

5

5.5

6

ln
(H

yp
er

−
pa

ra
m

et
er

)

Hyper−parameter VS Round, MT03:Dev and MT08:Test

inner−MERT Round

 

 
OuterRound−1
OuterRound−2
OuterRound−3
OuterRound−4
OuterRound−5

Figure 2: Hyper-parameter of 10 inner-MERT
loops under 5 outer-translation rounds in T-
MERT algorithm(without MBR), MT03:Dev and
MT08:Test.

figures, we observe that 1-round inner-MERT is
not sufficient to learn the weight well, while inner-
MERT using more than 2 rounds leads to signifi-
cant overtraining, which is consistent with the re-
sults obtained from the hyper-parameter.

0 5 10 15 20
0.26

0.28

0.3

0.32

0.34

0.36

0.38

Round

B
LE

U

Different Rounds Transductive MERT under MT03:DEV and MT05:TEST

 

 

1−round
2−round
3−round
4−round
baseline

Figure 3: Results under T-MERT algorith-
m(without MBR) with fixed inner-MERT loops
from 1 to 4, 20 outer-translation rounds, and base-
line. MT03:Dev and MT05:Test.

Although the score of T-MERT under 2-round
inner-MERT is comparable to or even better
than baseline, the performance is still unsta-
ble, changing drastically for different rounds of
outer-translation step (over 4 BLEU points for
MT03:Dev and MT05:Test, and even larger for
MT03:Dev and MT08:Test). We use the MBR s-

645



0 5 10 15 20

0.16

0.18

0.2

0.22

0.24

0.26

0.28

0.3

0.32

Round

B
LE

U
Different Rounds Transductive MERT under MT03:DEV and MT08:TEST

 

 

1−round
2−round
3−round
4−round
baseline

Figure 4: Results under T-MERT algorith-
m(without MBR) with fixed inner-MERT loops
from 1 to 4, 20 outer-translation rounds, and base-
line. MT03:Dev and MT08:Test.

election proposed in section 3.4 to choose a suit-
able hypothesis, and the corresponding results are
shown in figure 5 and figure 6. It could be found
that as the number of outer-translation rounds in-
creases, the algorithm generates more groups of
translation outputs, from which the performance
under MBR selection turns to be more and more
stable.

0 5 10 15 20
0.345

0.35

0.355

0.36

0.365

0.37

0.375

0.38

Round

B
LE

U

Comparison of different Settings under MT03:DEV and MT05:TEST

 

 

baseline
transductive
transductive+mbr

Figure 5: Result of baseline,T-MERT under 2
inner-MERT rounds without and with MBR selec-
tion. MT03:DEV and MT05:Test.

The above parts discuss two solutions for the
problems we encounter in the transductive MERT,
i.e., the inner-MERT stop criterion and MBR s-
election. We further evaluate our method (under
2 round inner-MERT and MBR selection) on all

0 5 10 15 20

0.15

0.2

0.25

0.3

Round

B
LE

U

Comparison of different Settings under MT03:DEV and MT08:TEST

 

 

baseline
transductive
transductive+mbr

Figure 6: Result of baseline,T-MERT under 2
inner rounds without and with MBR selection.
MT03:DEV and MT08:Test.

dataset pairs, which is any pair of MT03, MT05,
MT06 and MT08. The final results are shown in
table 2, from which we observe that for the dataset
pair MT03 and MT05 the result under T-MERT
is close to baseline, while for the pair MT03 and
MT08, the improvement is significant in both di-
rections. The result is similar with the observa-
tion on these evaluation datasets, i.e., MT03 and
MT05 are under similar distribution, while MT03
and MT08 are quite different. Generally speaking,
MT03 and MT05 are both composed of only news
data, while MT06 and MT08 are consisted of news
and web-blog data. As we know, the news data is
significantly different from the web-blog data. We
can find that our method could achieve significant
improvement on 9 of total 12 dataset pairs, indicat-
ing that the distribution variation between dataset
pairs is quite a common phenomenon. For dataset-
s under similar distribution, the baseline perfor-
mance is close to oracle, which means that poten-
tial space for improvement under the adaptation is
limited; while for datasets that are quite differen-
t, there is much room for further increase in per-
formance, since the baseline weight estimated on
development is seriously biased for the test.

Besides, we try one extra comparison, i.e., us-
ing same dataset for both development and test un-
der T-MERT. The result is also shown in table 2.
We find that for MT03 and MT05, the result un-
der T-MERT is close to baseline (a little higher),
while for MT06 and MT08, the result is fairly low-
er. The reason that the adapted result on MT03 is
slightly higher than baseline is that the result in

646



Table 2: Results of baseline and T-MERT under MBR Selections for different dataset pairs. Here symbol
↑ shows that the improvement is significant, ↓ indicates decrease is significant, and |means no significant
changes

DEV MT03 MT05 MT06 MT08
TEST BASELINE T-MERT BASELINE T-MERT BASELINE T-MERT BASELINE T-MERT

MT03 0.3914 0.3933(|) 0.3861 0.3908(|) 0.3731 0.3830(↑) 0.3586 0.3704(↑)
MT05 0.3733 0.3739(|) 0.3687 0.3724(|) 0.3592 0.3700(↑) 0.3414 0.3576(↑)
MT06 0.3358 0.3582(↑) 0.3344 0.3569(↑) 0.3636 0.3579(↓) 0.3504 0.3653(↑)
MT08 0.2486 0.2892(↑) 0.2543 0.2755(↑) 0.2774 0.2768(|) 0.2929 0.2809(↓)

each round is close to baseline, making it possible
for the selection performance to be slightly higher
than baseline, while for others the result in each
round is lower than baseline. However, we do not
hope that our method could be significantly better
than baseline in this case, as baseline performance
is also the oracle performance for the test dataset.
We assume that we know the development and the
test datasets are distinct in advance before apply-
ing the T-MERT algorithm.

5 Conclusion and Future Work

In this paper, we investigate the parameter adap-
tation issue in SMT. In particular, a transductive
MERT algorithm is proposed to better explore
both development and test datasets. Besides, a
hyper-parameter is proposed to control the over-
training problem in the parameter estimation step
and a Minimum Bayes Risk (MBR)-based hypoth-
esis selection method is adopted to stabilize the fi-
nal performance. Compared with a state-of-the-art
baseline, our method achieves significant and sus-
tainable improvement.

In future, we plan to incorporate better hypothe-
sis selection algorithms to choose high quality sen-
tences from the test dataset, since sentences with
bad translations would bring side effect during the
learning procedure. Besides, we plan to further in-
vestigate the mechanism of transductive MERT in
boosting the performance of SMT.

Acknowledgments

We thank Ning Xi and Shujian Huang for their
meaningful suggestions. We would also like to
thank the anonymous reviewers for their help-
ful comments. This work is supported by the
National Natural Science Foundation of China
(No.61003112) and the National Fundamental Re-
search Program of China (2010CB327903)

References
Yee S. Chan and Hwee T. Ng. 2007. Domain adapta-

tion with active learning forword sense disambigua-
tion. In In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pp.
49–56, Prague, Czech Republic, 2007.

David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pp. 263–270,
Ann Arbor, 2005.

Nicola Ehling, Richard Zens and Hermann Ney. 2007.
Minimum bayes risk decoding for bleu. In In Pro-
ceedings of the ACL 2007 Demo and Poster Session-
s, pp. 101–104, Prague, 2007.

George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for smt. In In Proceedings of
the Second ACL Workshop on Statistical Machine
Translation, Prague, Czech Republic, 2007.

Almut Silja Hildebrand, Matthias Eck, Stephan Vo-
gel and Alex Waibel. 2005. Adaptation of the
translation model for statistical machine translation
based on information retrieval. In In Proceedings of
EAMT, Budapest, Hungary, 2005.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In In Proc. of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pp. 160–167, 2004.

Mu Li, Yinggong Zhao, Dongdong Zhang and Ming
Zhou. 2010. Adaptive development data selection
for log-linear model in statistical machine transla-
tion. In In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling
2010), pp. 662–670, Beijing, 2010.

Yang Liu, Steve Hanneke and Jaime Carbonell. 2010.
A theory of transfer learning with applications to
active learning. Technical report, Machine Learn-
ing Department, Carnegie Mellon University, New
Brunswick, MA, 2010.

Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys, 40(3), 2008.

Yajuan Lü, Jin Huang, and Qun Liu. 2007. Improv-
ing statistical machine translation performance by

647



training data selection and optimization. In In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing, pp. 343–
350, Czech Republic, 2007.

Spyros Matsoukas and Antti-Veikko I. Rosti and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In In Proc. of the Con-
ference on Empirical Methods in Natural Language
Processing, pp. 160–167, 2009.

Behrang Mohit, Frank Liberato and Rebecca Hwa.
2009. Language model adaptation for difficult to
translate phrases. In In Proceedings of the 13th An-
nual Conference of the EAMT, pp. 160–167, 2009.

Franz Och. 2003. Minimum error rate training in sta-
tistical machine translation. In In Proceedings of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), Sapporo, Japan, 2003.

Franz Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statisti-
cal machine translation. In In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pp. 295–302, Philadel-
phia, 2002.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. Bleu: a method for automatic evaluation
of machine translation. In In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pp. 311–318, Philadel-
phia, 2002.

German Sanchis-Trilles and Francisco Casacuberta.
2010. Log-linear weight optimisation via bayesian
adaptation in statistical machine translation. In In
Proceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pp. 662–
670, Beijing, 2010.

Nicola Ueffing, Gholamreza Haffari and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pp. 25–32, Czech Republic, 2007.

648


