



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 321–331
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1030

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 321–331
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1030

Scalable Bayesian Learning of Recurrent Neural Networks for
Language Modeling

Zhe Gan∗, Chunyuan Li∗†, Changyou Chen, Yunchen Pu, Qinliang Su, Lawrence Carin
Department of Electrical and Computer Engineering, Duke University
{zg27, cl319, cc448, yp42, qs15, lcarin}@duke.edu

Abstract

Recurrent neural networks (RNNs) have
shown promising performance for lan-
guage modeling. However, traditional
training of RNNs using back-propagation
through time often suffers from overfit-
ting. One reason for this is that stochastic
optimization (used for large training sets)
does not provide good estimates of model
uncertainty. This paper leverages recent
advances in stochastic gradient Markov
Chain Monte Carlo (also appropriate for
large training sets) to learn weight uncer-
tainty in RNNs. It yields a principled
Bayesian learning algorithm, adding gra-
dient noise during training (enhancing ex-
ploration of the model-parameter space)
and model averaging when testing. Ex-
tensive experiments on various RNN mod-
els and across a broad range of applica-
tions demonstrate the superiority of the
proposed approach relative to stochastic
optimization.

1 Introduction

Language modeling is a fundamental task, used
for example to predict the next word or charac-
ter in a text sequence given the context. Recently,
recurrent neural networks (RNNs) have shown
promising performance on this task (Mikolov
et al., 2010; Sutskever et al., 2011). RNNs with
Long Short-Term Memory (LSTM) units (Hochre-
iter and Schmidhuber, 1997) have emerged as a
popular architecture, due to their representational
power and effectiveness at capturing long-term de-
pendencies.

RNNs are usually trained via back-propagation
through time (Werbos, 1990), using stochastic op-

∗Equal contribution. †Corresponding author.

timization methods such as stochastic gradient de-
scent (SGD) (Robbins and Monro, 1951); stochas-
tic methods of this type are particularly important
for training with large data sets. However, this
approach often provides a maximum a posteriori
(MAP) estimate of model parameters. The MAP
solution is a single point estimate, ignoring weight
uncertainty (Blundell et al., 2015; Hernández-
Lobato and Adams, 2015). Natural language of-
ten exhibits significant variability, and hence such
a point estimate may make over-confident predic-
tions on test data.

To alleviate overfitting RNNs, good regular-
ization is known as a key factor to successful
applications. In the neural network literature,
Bayesian learning has been proposed as a princi-
pled method to impose regularization and incor-
porate model uncertainty (MacKay, 1992; Neal,
1995), by imposing prior distributions on model
parameters. Due to the intractability of poste-
rior distributions in neural networks, Hamiltonian
Monte Carlo (HMC) (Neal, 1995) has been used to
provide sample-based approximations to the true
posterior. Despite the elegant theoretical prop-
erty of asymptotic convergence to the true poste-
rior, HMC and other conventional Markov Chain
Monte Carlo methods are not scalable to large
training sets.

This paper seeks to scale up Bayesian learning
of RNNs to meet the challenge of the increasing
amount of “big” sequential data in natural lan-
guage processing, leveraging recent advances in
stochastic gradient Markov Chain Monte Carlo
(SG-MCMC) algorithms (Welling and Teh, 2011;
Chen et al., 2014; Ding et al., 2014; Li et al.,
2016a,b). Specifically, instead of training a sin-
gle network, SG-MCMC is employed to train an
ensemble of networks, where each network has its
parameters drawn from a shared posterior distri-
bution. This is implemented by adding additional

321

https://doi.org/10.18653/v1/P17-1030
https://doi.org/10.18653/v1/P17-1030


Encoding weights

Recurrent weights

Decoding weights

Output

Input

Hidden

Figure 1: Illustration of different weight learning
strategies in a single-hidden-layer RNN. Stochas-
tic optimization used for MAP estimation puts
fixed values on all weights. Naive dropout is al-
lowed to put weight uncertainty only on encoding
and decoding weights, and fixed values on recur-
rent weights. The proposed SG-MCMC scheme
imposes distributions on all weights.

gradient noise during training and utilizing model
averaging when testing.

This simple procedure has the following salu-
tary properties for training neural networks: (i)
When training, the injected noise encourages
model-parameter trajectories to better explore the
parameter space. This procedure was also empiri-
cally found effective in Neelakantan et al. (2016).
(ii) Model averaging when testing alleviates over-
fitting and hence improves generalization, trans-
ferring uncertainty in the learned model parame-
ters to subsequent prediction. (iii) In theory, both
asymptotic and non-asymptotic consistency prop-
erties of SG-MCMC methods in posterior estima-
tion have been recently established to guarantee
convergence (Chen et al., 2015a; Teh et al., 2016).
(iv) SG-MCMC is scalable; it shares the same
level of computational cost as SGD in training,
by only requiring the evaluation of gradients on
a small mini-batch. To the authors’ knowledge,
RNN training using SG-MCMC has not been in-
vestigated previously, and is a contribution of this
paper. We also perform extensive experiments on
several natural language processing tasks, demon-
strating the effectiveness of SG-MCMC for RNNs,
including character/word-level language model-
ing, image captioning and sentence classification.

2 Related Work

Several scalable Bayesian learning methods have
been proposed recently for neural networks. These
come in two broad categories: stochastic vari-
ational inference (Graves, 2011; Blundell et al.,
2015; Hernández-Lobato and Adams, 2015) and

SG-MCMC methods (Korattikara et al., 2015; Li
et al., 2016a). While prior work focuses on
feed-forward neural networks, there has been lit-
tle if any research reported for RNNs using SG-
MCMC.

Dropout (Hinton et al., 2012; Srivastava et al.,
2014) is a commonly used regularization method
for training neural networks. Recently, several
works have studied how to apply dropout to
RNNs (Pachitariu and Sahani, 2013; Bayer et al.,
2013; Pham et al., 2014; Zaremba et al., 2014;
Bluche et al., 2015; Moon et al., 2015; Semeniuta
et al., 2016; Gal and Ghahramani, 2016b). Among
them, naive dropout (Zaremba et al., 2014) can im-
pose weight uncertainty only on encoding weights
(those that connect input to hidden units) and de-
coding weights (those that connect hidden units to
output), but not the recurrent weights (those that
connect consecutive hidden states). It has been
concluded that noise added in the recurrent con-
nections leads to model instabilities, hence dis-
rupting the RNN’s ability to model sequences.

Dropout has been recently shown to be a varia-
tional approximation technique in Bayesian learn-
ing (Gal and Ghahramani, 2016a; Kingma et al.,
2015). Based on this, (Gal and Ghahramani,
2016b) proposed a new variant of dropout that can
be successfully applied to recurrent layers, where
the same dropout masks are shared along time for
encoding, decoding and recurrent weights, respec-
tively. Alternatively, we focus on SG-MCMC,
which can be viewed as the Bayesian interpreta-
tion of dropout from the perspective of posterior
sampling (Li et al., 2016c); this also allows im-
position of model uncertainty on recurrent layers,
enhancing performance. A comparison of naive
dropout and SG-MCMC is illustrated in Fig. 1.

3 Recurrent Neural Networks

3.1 RNN as Bayesian Predictive Models

Consider data D = {D1, · · · ,DN}, where Dn ,
(Xn,Yn), with input Xn and output Yn. Our
goal is to learn model parameters θ to best
characterize the relationship from Xn to Yn,
with corresponding data likelihood p(D|θ) =∏N
n=1 p(Dn|θ). In Bayesian statistics, one sets

a prior on θ via distribution p(θ). The posterior
p(θ|D) ∝ p(θ)p(D|θ) reflects the belief concern-
ing the model parameter distribution after observ-
ing the data. Given a test input X̃ (with miss-
ing output Ỹ), the uncertainty learned in training

322



is transferred to prediction, yielding the posterior
predictive distribution:

p(Ỹ|X̃,D)=
∫

θ
p(Ỹ|X̃,θ)p(θ|D)dθ . (1)

When the input is a sequence, RNNs may be
used to parameterize the input-output relation-
ship. Specifically, consider input sequence X =
{x1, . . . ,xT }, where xt is the input data vector at
time t. There is a corresponding hidden state vec-
tor ht at each time t, obtained by recursively ap-
plying the transition function ht = H(ht−1,xt)
(specified in Section 3.2; see Fig. 1). The output Y
differs depending on the application: a sequence
{y1, . . . ,yT } in language modeling or a discrete
label in sentence classification. In RNNs the cor-
responding decoding function is p(y|h), described
in Section 3.3.

3.2 RNN Architectures

The transition function H(·) can be implemented
with a gated activation function, such as Long
Short-Term Memory (LSTM) (Hochreiter and
Schmidhuber, 1997) or a Gated Recurrent Unit
(GRU) (Cho et al., 2014). Both the LSTM and
GRU have been proposed to address the issue of
learning long-term sequential dependencies.

Long Short-Term Memory The LSTM archi-
tecture addresses the problem of learning long-
term dependencies by introducing a memory cell,
that is able to preserve the state over long periods
of time. Specifically, each LSTM unit has a cell
containing a state ct at time t. This cell can be
viewed as a memory unit. Reading or writing the
cell is controlled through sigmoid gates: input gate
it, forget gate ft, and output gate ot. The hidden
units ht are updated as

it = σ(Wixt +Uiht−1 + bi) ,

ft = σ(Wfxt +Ufht−1 + bf ) ,

ot = σ(Woxt +Uoht−1 + bo) ,

c̃t = tanh(Wcxt +Ucht−1 + bc) ,

ct = ft � ct−1 + it � c̃t ,
ht = ot � tanh(ct) ,

where σ(·) denotes the logistic sigmoid func-
tion, and � represents the element-wise matrix
multiplication operator. W{i,f,o,c} are encoding
weights, and U{i,f,o,c} are recurrent weights, as
shown in Fig. 1. b{i,f,o,c} are bias terms.

Variants Similar to the LSTM unit, the GRU
also has gating units that modulate the flow of
information inside the hidden unit. It has been
shown that a GRU can achieve similar perfor-
mance to an LSTM in sequence modeling (Chung
et al., 2014). We specify the GRU in the Supple-
mentary Material.

The LSTM can be extended to the bidirec-
tional LSTM and multilayer LSTM. A bidirec-
tional LSTM consists of two LSTMs that are run
in parallel: one on the input sequence and the other
on the reverse of the input sequence. At each time
step, the hidden state of the bidirectional LSTM
is the concatenation of the forward and backward
hidden states. In multilayer LSTMs, the hidden
state of an LSTM unit in layer ` is used as input
to the LSTM unit in layer ` + 1 at the same time
step (Graves, 2013).

3.3 Applications
The proposed Bayesian framework can be applied
to any RNN model; we focus on the following
tasks to demonstrate the ideas.

Language Modeling In word-level language
modeling, the input to the network is a sequence
of words, and the network is trained to predict the
next word in the sequence with a softmax classi-
fier. Specifically, for a length-T sequence, denote
yt = xt+1 for t = 1, . . . , T − 1. x1 and yT are
always set to a special START and END token,
respectively. At each time t, there is a decoding
function p(yt|ht) = softmax(Vht) to compute
the distribution over words, where V are the de-
coding weights (the number of rows of V corre-
sponds to the number of words/characters). We
also extend this basic language model to consider
other applications: (i) a character-level language
model can be specified in a similar manner by
replacing words with characters (Karpathy et al.,
2016). (ii) Image captioning can be considered
as a conditional language modeling problem, in
which we learn a generative language model of the
caption conditioned on an image (Vinyals et al.,
2015; Gan et al., 2017).

Sentence Classification Sentence classification
aims to assign a semantic category label y to a
whole sentence X. This is usually implemented
through applying the decoding function once at
the end of sequence: p(y|hT ) = softmax(VhT ),
where the final hidden state of a RNN hT is often
considered as the summary of the sentence (here

323



the number of rows of V corresponds to the num-
ber of classes).

4 Scalable Learning with SG-MCMC

4.1 The Pitfall of Stochastic Optimization
Typically there is no closed-form solution for the
posterior p(θ|D), and traditional Markov Chain
Monte Carlo (MCMC) methods (Neal, 1995) scale
poorly for largeN . To ease the computational bur-
den, stochastic optimization is often employed to
find the MAP solution. This is equivalent to min-
imizing an objective of regularized loss function
U(θ) that corresponds to a (non-convex) model
of interest: θMAP = argminU(θ), U(θ) =
− log p(θ|D). The expectation in (1) is approxi-
mated as:

p(Ỹ|X̃,D)= p(Ỹ|X̃,θMAP) . (2)

Though simple and effective, this procedure
largely loses the benefit of the Bayesian approach,
because the uncertainty on weights is ignored.
To more accurately approximate (1), we employ
stochastic gradient (SG) MCMC (Welling and
Teh, 2011).

4.2 Large-scale Bayesian Learning
The negative log-posterior is

U(θ) , − log p(θ)−
N∑

n=1

log p(Dn|θ). (3)

In optimization,E = −∑Nn=1 log p(Dn|θ) is typ-
ically referred to as the loss function, and R ∝
− log p(θ) as a regularizer.

For large N , stochastic approximations are of-
ten employed:

Ũt(θ),− log p(θ)−
N

M

M∑

m=1

log p(Dim |θ), (4)

where Sm = {i1, · · · , iM} is a random subset of
the set {1, 2, · · · , N}, with M � N . The gradi-
ent on this mini-batch is denoted as f̃t = ∇Ũt(θ),
which is an unbiased estimate of the true gradi-
ent. The evaluation of (4) is cheap even when N
is large, allowing one to efficiently collect a suf-
ficient number of samples in large-scale Bayesian
learning, {θs}Ss=1, where S is the number of sam-
ples (this will be specified later). These samples
are used to construct a sample-based estimation to
the expectation in (1):

Table 1: SG-MCMC algorithms and their optimiza-
tion counterparts. Algorithms in the same row share
similar characteristics.

Algorithms SG-MCMC Optimization
Basic SGLD SGD
Precondition pSGLD RMSprop/Adagrad
Momentum SGHMC momentum SGD
Thermostat SGNHT Santa

p(Ỹ|X̃,D)≈ 1
S

S∑

s=1

p(Ỹ|X̃,θs) . (5)

The finite-time estimation errors of SG-MCMC
methods are bounded (Chen et al., 2015a), which
guarantees (5) is an unbiased estimate of (1)
asymptotically under appropriate decreasing step-
sizes.

4.3 SG-MCMC Algorithms

SG-MCMC and stochastic optimization are par-
allel lines of work, designed for different pur-
poses; their relationship has recently been re-
vealed in the context of deep learning. The most
basic SG-MCMC algorithm has been applied to
Langevin dynamics, and is termed SGLD (Welling
and Teh, 2011). To help convergence, a momen-
tum term has been introduced in SGHMC (Chen
et al., 2014), a “thermostat” has been devised
in SGNHT (Ding et al., 2014; Gan et al., 2015)
and preconditioners have been employed in pS-
GLD (Li et al., 2016a). These SG-MCMC algo-
rithms often share similar characteristics with their
counterpart approaches from the optimization lit-
erature such as the momentum SGD, Santa (Chen
et al., 2016) and RMSprop/Adagrad (Tieleman
and Hinton, 2012; Duchi et al., 2011). The interre-
lationships between SG-MCMC and optimization-
based approaches are summarized in Table 1.

SGLD Stochastic Gradient Langevin Dynamics
(SGLD) (Welling and Teh, 2011) draws posterior
samples, with updates

θt = θt−1 − ηtf̃t−1 +
√
2ηtξt , (6)

where ηt is the learning rate, and ξt ∼ N (0, Ip) is
a standard Gaussian random vector. SGLD is the
SG-MCMC analog to stochastic gradient descent
(SGD), whose parameter updates are given by:

θt = θt−1 − ηtf̃t−1 . (7)

324



Algorithm 1: pSGLD
Input: Default hyperparameter settings:

ηt = 1×10−3, λ = 10−8, β1 = 0.99.
Initialize: v0 ← 0, θ1 ∼ N (0, I) ;
for t = 1, 2, . . . , T do

% Estimate gradient from minibatch St
f̃t = ∇Ũt(θ);
% Preconditioning

vt ← β1vt−1 + (1− β1)f̃t � f̃t;
G−1t ← diag

(
1�

(
λ1+ v

1
2
t

))
;

% Parameter update

ξt ∼ N (0, ηtG−1t );
θt+1← θt + ηt2 G−1t f̃t+ ξt;

end

SGD is guaranteed to converge to a local mini-
mum under mild conditions (Bottou, 2010). The
additional Gaussian term in SGLD helps the learn-
ing trajectory to explore the parameter space to ap-
proximate posterior samples, instead of obtaining
a local minimum.

pSGLD Preconditioned SGLD (pSGLD) (Li
et al., 2016a) was proposed recently to improve
the mixing of SGLD. It utilizes magnitudes of re-
cent gradients to construct a diagonal precondi-
tioner to approximate the Fisher information ma-
trix, and thus adjusts to the local geometry of
parameter space by equalizing the gradients so
that a constant stepsize is adequate for all dimen-
sions. This is important for RNNs, whose parame-
ter space often exhibits pathological curvature and
saddle points (Pascanu et al., 2013), resulting in
slow mixing. There are multiple choices of pre-
conditioners; similar ideas in optimization include
Adagrad (Duchi et al., 2011), Adam (Kingma and
Ba, 2015) and RMSprop (Tieleman and Hinton,
2012). An efficient version of pSGLD, adopt-
ing RMSprop as the preconditioner G, is summa-
rized in Algorithm 1, where � denotes element-
wise matrix division. When the preconditioner is
fixed as the identity matrix, the method reduces to
SGLD.

4.4 Understanding SG-MCMC

To further understand SG-MCMC, we show its
close connection to dropout/dropConnect (Srivas-
tava et al., 2014; Wan et al., 2013). These methods
improve the generalization ability of deep models,
by randomly adding binary/Gaussian noise to the

local units or global weights. For neural networks
with the nonlinear function q(·) and consecutive
layers h1 and h2, dropout and dropConnect are
denoted as:

Dropout: h2 = ξ0 � q(θh1),
DropConnect: h2 = q((ξ0 � θ)h1),

where the injected noise ξ0 can be binary-valued
with dropping rate p or its equivalent Gaussian
form (Wang and Manning, 2013):

Binary noise: ξ0 ∼ Ber(p),
Gaussian noise: ξ0 ∼ N (1,

p

1− p).

Note that ξ0 is defined as a vector for dropout, and
a matrix for dropConnect. By combining drop-
Connect and Gaussian noise from the above, we
have the update rule (Li et al., 2016c):

θt+1 = ξ0 � θt −
η

2
f̃t = θt −

η

2
f̃t + ξ

′
0 , (8)

where ξ′0 ∼ N
(
0, p(1−p)diag(θ

2
t )
)

; (8) shows
that dropout/ dropConnect and SGLD in (6) share
the same form of update rule, with the distinc-
tion being that the level of injected noise is dif-
ferent. In practice, the noise injected by SGLD
may not be enough. A better way that we find to
improve the performance is to jointly apply SGLD
and dropout. This method can be interpreted as
using SGLD to sample the posterior distribution
of a mixture of RNNs, with mixture probability
controlled by the dropout rate.

5 Experiments

We present results on several tasks, including
character/word-level language modeling, image
captioning, and sentence classification. We do
not perform any dataset-specific tuning other than
early stopping on validation sets. When dropout is
utilized, the dropout rate is set to 0.5. All experi-
ments are implemented in Theano (Theano Devel-
opment Team, 2016), using a NVIDIA GeForce
GTX TITAN X GPU with 12GB memory.

The hyper-parameters for the proposed algo-
rithm include step size, minibatch size, thinning
interval, number of burn-in epochs and variance
of the Gaussian priors. We list the specific val-
ues used in our experiments in Table 2. The ex-
planation of these hyperparameters, the initializa-
tion of model parameters and model specifications
on each dataset are provided in the Supplementary
Material.

325



Table 2: Hyper-parameter settings of pSGLD for different datasets. For PTB, SGLD is used.

Datasets WP PTB Flickr8k Flickr30k MR CR SUBJ MPQA TREC
Minibatch Size 100 32 64 64 50 50 50 50 50
Step Size 2×10−3 1 10−3 10−3 10−3 10−3 10−3 10−3 10−3
# Total Epoch 20 40 20 20 20 20 20 20 20
Burn-in (#Epoch) 4 4 3 3 1 1 1 1 1
Thinning Interval (#Epoch) 1/2 1/2 1 1/2 1 1 1 1 1
# Samples Collected 32 72 17 34 19 19 19 19 19

5.1 Language Modeling
We first test character-level and word-level lan-
guage modeling. The setup is as follows.

• Following Karpathy et al. (2016), we test
character-level language modeling on the
War and Peace (WP) novel. The train-
ing/validation/test sets contain 260/32/33
batches, in which there are 100 characters.
The vocabulary size is 87, and we consider
a 2-hidden-layer RNN of dimension 128.
• The Penn Treebank (PTB) corpus (Marcus

et al., 1993) is used for word-level lan-
guage modeling. The dataset adopts the
standard split (929K training words, 73K
validation words, and 82K test words) and
has a vocabulary of size 10K. We train
LSTMs of three sizes; these are denoted the
small/medium/large LSTM. All LSTMs have
two layers and are unrolled for 20 steps. The
small, medium and large LSTM has 200, 650
and 1500 units per layer, respectively.

We consider two types of training schemes
on PTB corpus: (i) Successive minibatches:
Following Zaremba et al. (2014), the final
hidden states of the current minibatch are
used as the initial hidden states of the subse-
quent minibatch (successive minibatches se-
quentially traverse the training set). (ii) Ran-
dom minibatches: The initial hidden states of
each minibatch are set to zero vectors, hence
we can randomly sample minibatches in each
update.

We study the effects of different types of architec-
ture (LSTM/GRU/Vanilla RNN (Karpathy et al.,
2016)) on the WP dataset, and effects of differ-
ent learning algorithms on the PTB dataset. The
comparison of test cross-entropy loss on WP is
shown in Table 3. We observe that pSGLD con-
sistently outperforms RMSprop. Table 4 summa-
rizes the test set performance on PTB1. It is clear

1The results reported here do not match Zaremba et al.
(2014) due to the implementation details. However, we pro-

Table 3: Test cross-entropy loss on WP dataset.
Methods LSTM GRU RNN
RMSprop 1.3607 1.2759 1.4239
pSGLD 1.3375 1.2561 1.4093

10 20 30 40 50 60
Individual Sample

110
120
130
140
150
160
170
180

Pe
rp

le
xi

ty
0 10 20 30 40 50 60
Number of Samples for Model Averaging

110
120
130
140
150
160
170
180

Pe
rp

le
xi

ty

forward collection
backward collection
thinned collection

(a) Single sample (b) Different collections

Figure 2: Effects of collected samples.

that our sampling-based method consistently out-
performs the optimization counterpart, where the
performance gain mainly comes from adding gra-
dient noise and model averaging. When com-
pared with dropout, SGLD performs better on the
small LSTM model, but worse on the medium
and large LSTM model. This may imply that
dropout is suitable to regularizing large networks,
while SGLD exhibits better regularization ability
on small networks, partially due to the fact that
dropout may inject a higher level of noise during
training than SGLD. In order to inject a higher
level of noise into SGLD, we empirically apply
SGLD and dropout jointly, and found that this
provided the best performace on the medium and
large LSTM model.

We study three strategies to do model averaging,
i.e., forward collection, backward collection and
thinned collection. Given samples (θ1, · · · ,θK)
and the number of samples S used for averaging,
forward collection refers to using (θ1, · · · ,θS) for
the evaluation of a test function, backward col-
lection refers to using (θK−S+1, · · · ,θK), while
thinned collection chooses samples from θ1 to θK
with interval K/S. Fig. 2 plots the effects of
these strategies, where Fig. 2(a) plots the perplex-
ity of every single sample, Fig. 2(b) plots the per-
plexities using the three schemes. Only after 20

vide a fair comparison to all methods.

326



Table 4: Test perplexity on Penn Treebank.
Methods Small Medium Large

Random minibatches

SGD 123.85 126.31 130.25
SGD+Dropout 136.39 100.12 97.65
SGLD 117.36 109.14 105.86
SGLD+Dropout 139.54 99.58 94.03

Successive minibatches

SGD 113.45 123.14 127.68
SGD+Dropout 117.85 84.60 80.85
SGLD 108.61 121.16 131.40
SGLD+Dropout 125.44 82.71 78.91

Literature

Moon et al. (2015) − 97.0 118.7
Moon et al. (2015)+ emb. dropout − 86.5 86.0
Zaremba et al. (2014) − 82.7 78.4
Gal and Ghahramani (2016b) − 78.6 73.4

samples is a converged perplexity achieved in the
thinned collection, while it requires 30 samples
for forward collection or 60 samples for backward
collection. This is unsurprising, because thinned
collection provides a better way to select samples.
Nevertheless, averaging of samples provides sig-
nificantly lower perplexity than using single sam-
ples. Note that the overfitting problem in Fig. 2(a)
is also alleviated by model averaging.

To better illustrate the benefit of model averag-
ing, we visualize in Fig. 3 the probabilities of each
word in a randomly chosen test sentence. The first
3 rows are the results predicted by 3 distinctive
model samples, respectively; the bottom row is the
result after averaging. Their corresponding per-
plexities for the test sentence are also shown on
the right of each row. The 3 individual samples
provide reasonable probabilities. For example, the
consecutive words “New York”, “stock exchange”
and “did not” are assigned with a higher proba-
bility. After averaging, we can see a much lower
perplexity, as the samples can complement each
other. For example, though the second sample can
yield the lowest single-model perplexity, its pre-
diction on word “York” is still benefited from the
other two via averaging.

5.2 Image Caption Generation

We next consider the problem of image caption
generation, which is a conditional RNN model,
where image features are extracted by residual net-
work (He et al., 2016), and then fed into the RNN
to generate the caption. We present results on
two benchmark datasets, Flickr8k (Hodosh et al.,
2013) and Flickr30k (Young et al., 2014). These

25.55the 25.55new 25.55york 25.55stock 25.55exchange 25.55did 25.55not 25.55fall 25.55apart
22.24the 22.24new 22.24york 22.24stock 22.24exchange 22.24did 22.24not 22.24fall 22.24apart
29.83the 29.83new 29.83york 29.83stock 29.83exchange 29.83did 29.83not 29.83fall 29.83apart

21.98the 21.98new 21.98york 21.98stock 21.98exchange 21.98did 21.98not 21.98fall 21.98apart
0

0.2

0.4

0.6

0.8

1

Figure 3: Predictive probabilities obtained by 3
samples and their average. Colors indicate nor-
malized probability of each word. Best viewed in
color.

a"tan"dog"is"playing"in"the"grass
a"tan"dog"is"playing"with"a"red"ball"in"the"grass
a"tan"dog"with"a"red"collar"is"running"in"the"grass

a"yellow"dog"runs"through"the"grass
a"yellow"dog"is"running"through"the"grass
a"brown"dog"is"running"through"the"grass

a"group"of"people"stand"in"front"of"a"building
a"group"of"people"stand"in"front"of"a"white"building
a"group"of"people"stand"in"front"of"a"large"building

a"man"and"a"woman"walking"on"a"sidewalk
a"man"and"a"woman"stand"on"a"balcony
a"man"and"a"woman"standing"on"the"ground

Figure 4: Image captioning with different sam-
ples. Left are the given images, right are the cor-
responding captions. The captions in each box are
from the same model sample.

datasets contain 8,000 and 31,000 images, respec-
tively. Each image is annotated with 5 sentences.
A single-layer LSTM is employed with the num-
ber of hidden units set to 512.

The widely used BLEU (Papineni et al., 2002),
METEOR (Banerjee and Lavie, 2005), ROUGE-
L (Lin, 2004), and CIDEr-D (Vedantam et al.,
2015) metrics are used to evaluate the perfor-
mance. All the metrics are computed by us-
ing the code released by the COCO evaluation
server (Chen et al., 2015b).

Table 5 presents results for pSGLD/RMSprop

327



Table 5: Performance on Flickr8k & Flickr30k: BLEU’s, METEOR, CIDEr, ROUGE-L and perplexity.

Methods B-1 B-2 B-3 B-4 METEOR CIDEr ROUGE-L Perp.
Results on Flickr8k
RMSprop 0.640 0.427 0.288 0.197 0.205 0.476 0.500 16.64
RMSprop + Dropout 0.647 0.444 0.305 0.209 0.208 0.514 0.510 15.72
RMSprop + Gal’s Dropout 0.651 0.443 0.305 0.209 0.206 0.501 0.509 14.70
pSGLD 0.669 0.463 0.321 0.224 0.214 0.535 0.522 14.29
pSGLD + Dropout 0.656 0.450 0.309 0.211 0.209 0.512 0.512 14.26
Results on Flickr30k
RMSprop 0.644 0.422 0.279 0.184 0.180 0.372 0.476 17.80
RMSprop + Dropout 0.656 0.435 0.295 0.200 0.185 0.396 0.481 18.05
RMSprop + Gal’s Dropout 0.636 0.429 0.290 0.197 0.190 0.408 0.480 17.27
pSGLD 0.657 0.438 0.300 0.206 0.192 0.421 0.490 15.61
pSGLD + Dropout 0.666 0.448 0.308 0.209 0.189 0.419 0.487 17.05

with or without dropout. In addition to (naive)
dropout, we further compare pSGLD with the
Gal’s dropout, recently proposed in Gal and
Ghahramani (2016b), which is shown to be ap-
plicable to recurrent layers. Consistent with
the results in the basic language modeling, pS-
GLD yields improved performance compared to
RMSprop. For example, pSGLD provides 2.7
BLEU-4 score improvement over RMSprop on the
Flickr8k dataset. By comparing pSGLD with RM-
Sprop with dropout, we conclude that pSGLD ex-
hibits better regularization ability than dropout on
these two datasets.

Apart from modeling weight uncertainty, differ-
ent samples from our algorithm may capture dif-
ferent aspects of the input image. An example
with two images is shown in Fig. 4, where 2 ran-
domly chosen model samples are considered for
each image. For each model sample, the top 3 gen-
erated captions are presented. We use the beam
search approach (Vinyals et al., 2015) to gener-
ate captions, with a beam of size 5. In Fig. 4,
the two samples for the first image mainly differ
in the color and activity of the dog, e.g., “tan” or
“yellow”, “playing” or “running”, whereas for the
second image, the two samples reflect different un-
derstanding of the image content.

5.3 Sentence Classification
We study the task of sentence classification on 5
datasets as in Kiros et al. (2015): MR (Pang and
Lee, 2005), CR (Hu and Liu, 2004), SUBJ (Pang
and Lee, 2004), MPQA (Wiebe et al., 2005) and
TREC (Li and Roth, 2002). A single-layer bidi-
rectional LSTM is employed with the number of
hidden units set to 400. Table 6 shows the test-

5 10 15
#Epoch

0.00

0.05

0.10

0.15

0.20

0.25

Er
ro

r

Train
RMSprop
RMSprop + Dropout
pSGLD
pSGLD + Dropout

5 10 15
#Epoch

0.10

0.12

0.14

0.16

0.18

0.20

0.22

0.24

0.26

Er
ro

r

Validation

5 10 15
#Epoch

0.10

0.15

0.20

Er
ro

r

Test

Figure 5: Learning curves on TREC dataset.

ing classification errors. 10-fold cross-validation
is used for evaluation on the first 4 datasets, while
TREC has a pre-defined training/test split, and we
run each algorithm 10 times on TREC. The com-
bination of pSGLD and dropout consistently pro-
vides the lowest errors.

In the following, we focus on the analysis of
TREC. Each sentence of TREC is a question, and
the goal is to decide which topic type the ques-
tion is most related to: location, human, numeric,
abbreviation, entity or description. Fig. 5 plots
the learning curves of different algorithms on the
training, validation and testing sets of the TREC
dataset. pSGLD and dropout have similar behav-
ior: they explore the parameter space during learn-
ing, and thus coverge slower than RMSprop on the
training dataset. However, the learned uncertainty
alleviates overfitting and results in lower errors on
the validation and testing datasets.

To further study the Bayesian nature of the pro-
posed approach, in Fig. 6 we choose two test-
ing sentences with high uncertainty (i.e., standard
derivation in prediction) from the TREC dataset.
Interestingly, after embedding to 2d-space with
tSNE (Van der Maaten and Hinton, 2008), the two

328



Table 6: Sentence classification errors on five benchmark datasets.

Methods MR CR SUBJ MPQA TREC
RMSprop 21.86±1.19 20.20±1.35 8.13±1.19 10.60±1.28 8.14±0.63
RMSprop + Dropout 20.52±0.99 19.57±1.79 7.24±0.86 10.66±0.74 7.48±0.47
RMSprop + Gal’s Dropout 20.22±1.12 19.29±1.93 7.52±1.17 10.59±1.12 7.34±0.66
pSGLD 20.36±0.85 18.72±1.28 7.00±0.89 10.54±0.99 7.48±0.82
pSGLD + Dropout 19.33±1.10 18.18±1.32 6.61±1.06 10.22±0.89 6.88±0.65

Whatdoes cc in engines mean?

Whatdoes adefibrillatordo?

True5Type Predicted5 Type

Description

Description

Testing5Question

Entity

Abbreviation

Figure 6: Visualization. Top two rows show se-
lected ambiguous sentences, which correspond to
the points with black circles in tSNE visualization
of the testing dataset.

sentences correspond to points lying on the bound-
ary of different classes. We use 20 model sam-
ples to estimate the prediction mean and standard
derivation on the true type and predicted type. The
classifier yields higher probability on the wrong
types, associated with higher standard derivations.
One can leverage the uncertainty information to
make decisions: either manually make a human
judgement when uncertainty is high, or automat-
ically choose the one with lower standard deriva-
tions when both types exhibits similar prediction
means. A more rigorous usage of the uncertainty
information is left as future work.

5.4 Discussion

Ablation Study We investigate the effectivenss
of each module in the proposed algorithm in Ta-
ble 7 on two datasets: TREC and PTB. The small
network size is used on PTB. Let M1 denote only
gradient noise, and M2 denote only model averag-
ing. As can be seen, The last sample in pSGLD
(M1) does not necessarily bring better results than
RMSprop, but the model averaging over the sam-
ples of pSGLD indeed provide better results than
model averaging of RMSprop (M2). This indi-
cates that both gradient noise and model averaging
are crucial for good performance in pSGLD.

Table 7: Ablation study on TREC and PTB.
Datasets RMSprop M1 M2 pSGLD

TREC 8.14 8.34 7.54 7.48
PTB 120.45 122.14 114.86 109.44

Table 8: Running time on Flickr30k in seconds.
Stages pSGLD RMSprop+Dropout
Training 20324 12578
Testing 7047 1311

Running Time We report the training and test-
ing time for image captioning on the Flickr30k
dataset in Table 8. For pSGLD, the extra cost in
training comes from adding gradient noise, and the
extra cost in testing comes from model averaging.
However, the cost in model averaging can be alle-
viated via the distillation methods: learning a sin-
gle neural network that approximates the results
of either a large model or an ensemble of mod-
els (Korattikara et al., 2015; Kim and Rush, 2016;
Kuncoro et al., 2016). The idea can be incorpo-
rated with our SG-MCMC technique to achieve
the same goal, which we leave for our future work.

6 Conclusion

We propose a scalable Bayesian learning frame-
work using SG-MCMC, to model weight uncer-
tainty in recurrent neural networks. The learn-
ing framework is tested on several tasks, includ-
ing language models, image caption generation
and sentence classification. Our algorithm outper-
forms stochastic optimization algorithms, indicat-
ing the importance of learning weight uncertainty
in recurrent neural networks. Our algorithm re-
quires little additional computational overhead in
training, and multiple times of forward-passing for
model averaging in testing.

Acknowledgments This research was supported
by ARO, DARPA, DOE, NGA, ONR and NSF. We
acknowledge Wenlin Wang for the code on lan-
guage modeling experiment.

329



References
S. Banerjee and A. Lavie. 2005. Meteor: An automatic

metric for mt evaluation with improved correlation
with human judgments. In ACL workshop.

J. Bayer, C. Osendorfer, D. Korhammer, N. Chen,
S. Urban, and P. van der Smagt. 2013. On fast
dropout and its applicability to recurrent networks.
arXiv:1311.0701 .

T. Bluche, C. Kermorvant, and J. Louradour. 2015.
Where to apply dropout in recurrent neural networks
for handwriting recognition? In ICDAR.

C. Blundell, J. Cornebise, K. Kavukcuoglu, and
D. Wierstra. 2015. Weight uncertainty in neural net-
works. In ICML.

L Bottou. 2010. Large-scale machine learning with
stochastic gradient descent. In COMPSTAT .

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin.
2016. Bridging the gap between stochastic gradient
MCMC and stochastic optimization. In AISTATS.

C. Chen, N. Ding, and L. Carin. 2015a. On the conver-
gence of stochastic gradient MCMC algorithms with
high-order integrators. In NIPS.

T. Chen, E. B. Fox, and C. Guestrin. 2014. Stochastic
gradient Hamiltonian Monte Carlo. In ICML.

X. Chen, H. Fang, T. Lin, R. Vedantam, S. Gupta,
P. Dollár, and C. L. Zitnick. 2015b. Microsoft
coco captions: Data collection and evaluation server.
arXiv:1504.00325 .

K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bah-
danau, F. Bougares, H. Schwenk, and Y. Bengio.
2014. Learning phrase representations using RNN
encoder-decoder for statistical machine translation.
In EMNLP.

J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. 2014.
Empirical evaluation of gated recurrent neural net-
works on sequence modeling. arXiv:1412.3555 .

N. Ding, Y. Fang, R. Babbush, C. Chen, R. D. Skeel,
and H. Neven. 2014. Bayesian sampling using
stochastic gradient thermostats. In NIPS.

J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR .

Y. Gal and Z. Ghahramani. 2016a. Dropout as a
Bayesian approximation: Representing model un-
certainty in deep learning. In ICML.

Y. Gal and Z. Ghahramani. 2016b. A theoretically
grounded application of dropout in recurrent neural
networks. In NIPS.

Z. Gan, C. Chen, R. Henao, D. Carlson, and L. Carin.
2015. Scalable deep poisson factor analysis for topic
modeling. In ICML.

Z. Gan, C. Gan, X. He, Y. Pu, K. Tran, J. Gao, L. Carin,
and L. Deng. 2017. Semantic compositional net-
works for visual captioning. In CVPR.

A. Graves. 2011. Practical variational inference for
neural networks. In NIPS.

A. Graves. 2013. Generating sequences with recurrent
neural networks. arXiv:1308.0850 .

K. He, X. Zhang, S. Ren, and J. Sun. 2016. Deep resid-
ual learning for image recognition. In CVPR.

J. M. Hernández-Lobato and R. P. Adams. 2015. Prob-
abilistic backpropagation for scalable learning of
Bayesian neural networks. In ICML.

G. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever,
and R Salakhutdinov. 2012. Improving neural net-
works by preventing co-adaptation of feature detec-
tors. arXiv:1207.0580 .

S. Hochreiter and J. Schmidhuber. 1997. Long short-
term memory. In Neural computation.

M. Hodosh, P. Young, and J. Hockenmaier. 2013.
Framing image description as a ranking task: Data,
models and evaluation metrics. JAIR .

M. Hu and B. Liu. 2004. Mining and summarizing cus-
tomer reviews. SIGKDD .

A. Karpathy, J. Johnson, and L. Fei-Fei. 2016. Visu-
alizing and understanding recurrent networks. In
ICLR Workshop.

Y. Kim and A. M. Rush. 2016. Sequence-level knowl-
edge distillation. In EMNLP.

D. Kingma and J. Ba. 2015. Adam: A method for
stochastic optimization. In ICLR.

D. Kingma, T. Salimans, and M. Welling. 2015. Varia-
tional dropout and the local reparameterization trick.
In NIPS.

R. Kiros, Y. Zhu, R. Salakhutdinov, R. Zemel, R. Urta-
sun, A. Torralba, and S. Fidler. 2015. Skip-thought
vectors. In NIPS.

A. Korattikara, V. Rathod, K. Murphy, and M. Welling.
2015. Bayesian dark knowledge. In NIPS.

A. Kuncoro, M. Ballesteros, L. Kong, C. Dyer, and
N. A. Smith. 2016. Distilling an ensemble of greedy
dependency parsers into one mst parser. In EMNLP.

C. Li, C. Chen, D. Carlson, and L. Carin. 2016a. Pre-
conditioned stochastic gradient Langevin dynamics
for deep neural networks. In AAAI.

C. Li, C. Chen, K. Fan, and L. Carin. 2016b. High-
order stochastic gradient thermostats for Bayesian
learning of deep models. In AAAI.

C. Li, A. Stevens, C. Chen, Y. Pu, Z. Gan, and L. Carin.
2016c. Learning weight uncertainty with stochastic
gradient mcmc for shape classification. In CVPR.

330



X. Li and D. Roth. 2002. Learning question classifiers.
ACL .

C. Lin. 2004. Rouge: A package for automatic evalua-
tion of summaries. In ACL workshop.

D. J. C. MacKay. 1992. A practical Bayesian frame-
work for backpropagation networks. In Neural com-
putation.

M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of english:
The penn treebank. Computational linguistics .

T. Mikolov, M. Karafiát, L. Burget, J. Cernockỳ, and
S. Khudanpur. 2010. Recurrent neural network
based language model. In INTERSPEECH.

T. Moon, H. Choi, H. Lee, and I. Song. 2015. Rnndrop:
A novel dropout for rnns in asr. ASRU .

R. M. Neal. 1995. Bayesian learning for neural net-
works. PhD thesis, University of Toronto.

A. Neelakantan, L. Vilnis, Q. Le, I. Sutskever,
L. Kaiser, K. Kurach, and J. Martens. 2016. Adding
gradient noise improves learning for very deep net-
works. In ICLR workshop.

M. Pachitariu and M. Sahani. 2013. Regularization and
nonlinearities for neural language models: when are
they needed? arXiv:1301.5650 .

B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. ACL .

B. Pang and L. Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. ACL .

K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In ACL.

R. Pascanu, T. Mikolov, and Y. Bengio. 2013. On the
difficulty of training recurrent neural networks. In
ICML.

V. Pham, T. Bluche, C. Kermorvant, and J. Louradour.
2014. Dropout improves recurrent neural networks
for handwriting recognition. In ICFHR.

H. Robbins and S. Monro. 1951. A stochastic ap-
proximation method. In The annals of mathematical
statistics.

S. Semeniuta, A. Severyn, and E. Barth. 2016.
Recurrent dropout without memory loss.
arXiv:1603.05118 .

N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,
and R. Salakhutdinov. 2014. Dropout: A simple
way to prevent neural networks from overfitting.
JMLR .

I. Sutskever, J. Martens, and G. E. Hinton. 2011. Gen-
erating text with recurrent neural networks. In
ICML.

Y. W. Teh, A. H. Thiéry, and S. J. Vollmer. 2016.
Consistency and fluctuations for stochastic gradient
Langevin dynamics. JMLR .

Theano Development Team. 2016. Theano: A Python
framework for fast computation of mathematical ex-
pressions. arXiv:1605.02688 .

T. Tieleman and G. Hinton. 2012. Lecture 6.5-
rmsprop: Divide the gradient by a running average
of its recent magnitude. Coursera: Neural Networks
for Machine Learning .

L. Van der Maaten and G. E. Hinton. 2008. Visualizing
data using t-SNE. JMLR .

R. Vedantam, C. L. Zitnick, and D. Parikh. 2015.
Cider: Consensus-based image description evalua-
tion. In CVPR.

O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. 2015.
Show and tell: A neural image caption generator. In
CVPR.

L. Wan, M. Zeiler, S. Zhang, Y. LeCun, and R. Fer-
gus. 2013. Regularization of neural networks using
DropConnect. In ICML.

S. Wang and C. Manning. 2013. Fast Dropout training.
In ICML.

M. Welling and Y. W. Teh. 2011. Bayesian learning via
stochastic gradient Langevin dynamics. In ICML.

P. Werbos. 1990. Backpropagation through time: what
it does and how to do it. In Proceedings of the IEEE.

J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language.
Language resources and evaluation .

P. Young, A. Lai, M. Hodosh, and J. Hockenmaier.
2014. From image descriptions to visual denota-
tions: New similarity metrics for semantic inference
over event descriptions. TACL .

W. Zaremba, I. Sutskever, and O. Vinyals. 2014.
Recurrent neural network regularization.
arXiv:1409.2329 .

331


	Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling

