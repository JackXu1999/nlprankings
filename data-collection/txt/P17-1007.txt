



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 69–76
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1007

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 69–76
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1007

Skip-Gram – Zipf + Uniform = Vector Additivity

Alex Gittens
Dept. of Computer Science

Rensselaer Polytechnic Institute
gittea@rpi.edu

Dimitris Achlioptas
Dept. of Computer Science

UC Santa Cruz
optas@soe.ucsc.edu

Michael W. Mahoney
ICSI and Dept. of Statistics

UC Berkeley
mmahoney@stat.berkeley.edu

Abstract

In recent years word-embedding models
have gained great popularity due to their
remarkable performance on several tasks,
including word analogy questions and cap-
tion generation. An unexpected “side-
effect” of such models is that their vectors
often exhibit compositionality, i.e., adding
two word-vectors results in a vector that is
only a small angle away from the vector
of a word representing the semantic com-
posite of the original words, e.g., “man” +
“royal” = “king”.

This work provides a theoretical justifica-
tion for the presence of additive composi-
tionality in word vectors learned using the
Skip-Gram model. In particular, it shows
that additive compositionality holds in an
even stricter sense (small distance rather
than small angle) under certain assump-
tions on the process generating the corpus.
As a corollary, it explains the success of
vector calculus in solving word analogies.
When these assumptions do not hold, this
work describes the correct non-linear com-
position operator.

Finally, this work establishes a con-
nection between the Skip-Gram model
and the Sufficient Dimensionality Reduc-
tion (SDR) framework of Globerson and
Tishby: the parameters of SDR models
can be obtained from those of Skip-Gram
models simply by adding information on
symbol frequencies. This shows that Skip-
Gram embeddings are optimal in the sense
of Globerson and Tishby and, further, im-
plies that the heuristics commonly used to
approximately fit Skip-Gram models can
be used to fit SDR models.

1 Introduction

The strategy of representing words as vectors has
a long history in computational linguistics and
machine learning. The general idea is to find
a map from words to vectors such that word-
similarity and vector-similarity are in correspon-
dence. Whilst vector-similarity can be readily
quantified in terms of distances and angles, quan-
tifying word-similarity is a more ambiguous task.
A key insight in that regard is to posit that the
meaning of a word is captured by “the company it
keeps” (Firth, 1957) and, therefore, that two words
that keep company with similar words are likely to
be similar themselves.

In the simplest case, one seeks vectors whose
inner products approximate the co-occurrence fre-
quencies. In more sophisticated methods co-
occurrences are reweighed to suppress the effect
of more frequent words (Rohde et al., 2006) and/or
to emphasize pairs of words whose co-occurrence
frequency maximally deviates from the indepen-
dence assumption (Church and Hanks, 1990).

An alternative to seeking word-embeddings that
reflect co-occurrence statistics is to extract the
vectorial representation of words from non-linear
statistical language models, specifically neural
networks. (Bengio et al., 2003) already proposed
(i) associating with each vocabulary word a fea-
ture vector, (ii) expressing the probability func-
tion of word sequences in terms of the feature vec-
tors of the words in the sequence, and (iii) learn-
ing simultaneously the vectors and the parame-
ters of the probability function. This approach
came into prominence recently through works of
Mikolov et al. (see below) whose main departure
from (Bengio et al., 2003) was to follow the sug-
gestion of (Mnih and Hinton, 2007) and trade-
away the expressive capacity of general neural-
network models for the scalability (to very large

69

https://doi.org/10.18653/v1/P17-1007
https://doi.org/10.18653/v1/P17-1007


corpora) afforded by (the more restricted class of)
log-linear models.

An unexpected side effect of deriving word-
embeddings via neural networks is that the word-
vectors produced appear to enjoy (approximate)
additive compositionality: adding two word-
vectors often results in a vector whose nearest
word-vector belongs to the word capturing the
composition of the added words, e.g., “man” +
“royal” = “king” (Mikolov et al., 2013c). This un-
expected property allows one to use these vectors
to answer word-analogy questions algebraically,
e.g., answering the question “Man is to king as
woman is to ” by returning the word whose
word-vector is nearest to the vector

v(king) - v(man) + v(woman).

In this work we focus on explaining the source
of this phenomenon for the most prominent such
model, namely the Skip-Gram model introduced
in (Mikolov et al., 2013a). The Skip-Gram model
learns vector representations of words based on
their patterns of co-occurrence in the training cor-
pus as follows: it assigns to each word c in the
vocabulary V , a “context” and a “target” vector,
respectively uc and vc, which are to be used in or-
der to predict the words that appear around each
occurrence of c within a window of ∆ tokens.
Specifically, the log probability of any target word
w to occur at any position within distance ∆ of
a context word c is taken to be proportional to
the inner product between uc and vw, i.e., letting
n = |V |,

p(w|c) = e
uTc vw

∑n
i=1 e

uTc vi
. (1)

Further, Skip-Gram assumes that the conditional
probability of each possible set of words in a win-
dow around a context word c factorizes as the
product of the respective conditional probabilities:

p(w−∆, . . . , w∆|c) =
∆∏

δ=−∆
δ 6=0

p(wδ|c). (2)

(Mikolov et al., 2013a) proposed learning the
Skip-Gram parameters on a training corpus by us-
ing maximum likelihood estimation under (1) and
(2). Thus, if wi denotes the i-th word in the train-
ing corpus and T the length of the corpus, we seek

the word vectors that maximize

1

T

T∑

i=1

∆∑

δ=−∆
δ 6=0

log p(wi+δ|wi) . (3)

As mentioned, the normalized context vectors
obtained from maximizing (3) under (1) and (2)
exhibit additive compositionality. For example,
the cosine distance between the sum of the context
vectors of the words “Vietnam” and “capital” and
the context vector of the word “Hanoi” is small.

While there has been much interest in using
algebraic operations on word vectors to carry
out semantic operations like composition, and
mathematically-flavored explanations have been
offered (e.g., in the recent work (Paperno and Ba-
roni, 2016)), the only published work which at-
tempts a rigorous theoretical understanding of this
phenomenon is (Arora et al., 2016). This work
guarantees that word vectors can be recovered by
factorizing the so-called PMI matrix, and that al-
gebraic operations on these word vectors can be
used to solve analogies, under certain conditions
on the process that generated the training corpus.
Specifically, the word vectors must be known a
priori, before their recovery, and to have been
generated by randomly scaling uniformly sampled
vectors from the unit sphere1. Further, the ith word
in the corpus must have been selected with proba-
bility proportional to eu

T
wci , where the “discourse”

vector ci governs the topic of the corpus at the
ith word. Finally, the discourse vector is assumed
to evolve according to a random walk on the unit
sphere that has a uniform stationary distribution.

By way of contrast, our results assume nothing
a priori about the properties of the word vectors.
In fact, the connection we establish between the
Skip-Gram and the Sufficient Dimensionality Re-
duction model of (Globerson and Tishby, 2003)
shows that the word vectors learned by Skip-Gram
are information-theoretically optimal. Further, the
context word c in the Skip-Gram model essentially
serves the role that the discourse vector does in
the PMI model of (Arora et al., 2016): the words
neighboring c are selected with probability propor-
tional to eu

T
c vw . We find the exact non-linear com-

position operator when no assumptions are made
on the context word. When an analogous assump-
tion to that of (Arora et al., 2016) is made, that the

1More generally, it suffices that the word vectors have cer-
tain properties consistent with this sampling process.

70



context words are uniformly distributed, we prove
that the composition operator reduces to vector ad-
dition.

While our primary motivation has been to pro-
vide a better theoretical understanding of word
compositionality in the popular Skip-Gram model,
our connection with the SDR method illuminates
a much more general point about the practical ap-
plicability of the Skip-Gram model. In particular,
it addresses the question of whether, for a given
corpus, fitting a Skip-Gram model will give good
embeddings. Even if we are making reasonable
linguistic assumptions about how to model words
and the interdependencies of words in a corpus,
it’s not clear that these have to hold universally
on all corpuses to which we apply Skip-Gram.
However, the fact that when we fit a Skip-Gram
model we are fitting an SDR model (up to fre-
quency information), and the fact that SDR mod-
els are information-theoretically optimal in a cer-
tain sense, argues that regardless of whether the
Skip-Gram assumptions hold, Skip-Gram always
gives us optimal features in the following sense:
the learned context embeddings and target embed-
dings preserve the maximal amount of mutual in-
formation between any pair of random variablesX
and Y consistent with the observed co-occurence
matrix, where Y is the target word and X is the
predictor word (in a min-max sense, since there
are many ways of coupling X and Y , each of
which may have different amounts of mutual in-
formation). Importantly, this statement requires no
assumptions on the distribution P (X,Y ).

2 Compositionality of Skip-Gram

In this section, we first give a mathematical formu-
lation of the intuitive notion of compositionality of
words. We then prove that the composition oper-
ator for the Skip-Gram model in full generality is
a non-linear function of the vectors of the words
being composed. Under a single simplifying as-
sumption, the operator linearizes and reduces to
the addition of the word vectors. Finally, we ex-
plain how linear compositionality allows for solv-
ing word analogies with vector algebra.

A natural way of capturing the compositional-
ity of words is to say that the set of context words
c1, . . . , cm has the same meaning as the single
word c if for every other word w,

p(w|c1, . . . , cm) = p(w|c) .

Although this is an intuitively satisfying defini-
tion, we never expect it to hold exactly; instead,
we replace exact equality with the minimization
of KL-divergence. That is, we state that the best
candidate for having the same meaning as the set
of context words C is the word

arg min
c∈V

DKL(p(·|C) | p(·|c)) . (4)

We refer to any vector that minimizes (4) as a
paraphrase of the set of words C.

There are two natural concerns with (4). The
first is that, in general, it is not clear how to define
p(·|C). The second is that KL-divergence min-
imization is a hard problem, as it involves opti-
mization over many high dimensional probability
distributions. Our main result shows that both of
these problems go away for any language model
that satisfies the following two assumptions:

A1. For every word c, there exists Zc such that for
every word w,

p(w|c) = 1
Zc

exp(uTc vw) . (5)

A2. For every set of words C = {c1, c2, . . . , cm},
there exists ZC such that for every word w,

p(w|C) = p(w)
1−m

ZC

m∏

i=1

p(w|ci) . (6)

Clearly, the Skip-Gram model satisfies A1 by
definition. We prove that it also satisfies A2 when
m ≤ ∆ (Lemma 1). Next, we state a theorem
that holds for any model satisfying assumptions
A1 and A2, including the Skip-Gram model when
m ≤ ∆.
Theorem 1. In every word model that satisfies A1
and A2, for every set of words C = {c1, . . . , cm},
any paraphase c of C satisfies

∑

w∈V
p(w|c)vw =

∑

w∈V
p(w|C)vw . (7)

Theorem 1 characterizes the composition opera-
tor for any language model which satisfies our two
assumptions; in general, this operator is not addi-
tion. Instead, a paraphrase c is a vector such that
the average word vector under p(·|c) matches that
under p(·|C). When the expectations in (7) can
be computed, the composition operator can be im-
plemented by solving a non-linear system of equa-
tions to find a vector u for which the left-hand side
of (7) equals the right-hand side.

71



Our next result proves that although the compo-
sition operator is nontrivial in the general case, to
recover vector addition as the composition opera-
tor, it suffices to assume that the word frequency
is uniform.

Theorem 2. In every word model that satisfies A1,
A2, and where p(w) = 1/|V | for everyw ∈ V , the
paraphrase of C = {c1, . . . , cm} is

u1 + . . .+ um .

As word frequencies are typically much closer
to a Zipf distribution (Piantadosi, 2014), the uni-
formity assumption of Theorem 2 is not realistic.
That said, we feel it is important to point out that,
as reported in (Mikolov et al., 2013b), additivity
captures compositionality more accurately when
the training set is manipulated so that the prior dis-
tribution of the words is made closer to uniform.

Using composition to solve analogies. It has
been observed that word vectors trained using non-
linear models like Skip-Gram tend to encode se-
mantic relationships between words as linear re-
lationships between the word vectors (Mikolov
et al., 2013b; Pennington et al., 2014; Levy and
Goldberg, 2014). In particular, analogies of the
form “man:woman::king:?” can often be solved
by taking ? to be the word in the vocabulary
whose context vector has the smallest angle with
uwoman + (uking − uman). Theorems 1 and 2 offer
insight into the solution such analogy questions.

We first consider solving an analogy of the form
“m:w::k:?”” in the case where the composition
operator is nonlinear. The fact that m and w share
a relationship means m is a paraphrase of the set
of words {w,R}, whereR is a set of words encod-
ing the relationship between m and w. Similarly,
the fact that k and ? share the same relationship
means k is a paraphrase of the set of words {?, R}.
By Theorem 1, we have that R and ? must satisfy

∑

`∈V
p(`|m)v` =

∑

`∈V
p(`|w,R)v` and

∑

`∈V
p(`|k)v` =

∑

`∈V
p(`|?, R)v`.

We see that solving analogies when the compo-
sition operator is nonlinear requires the solution
of two highly nonlinear systems of equations. In
sharp contrast, when the composition operator is
linear, the solution of analogies delightfully re-
duces to elementary vector algebra. To see this,

we again begin with the assertion that the fact that
m and w share a relationship means m is a para-
phrase of the set of words {w,R}; Similarly, k is
a paraphrase of {?, R}. By Theorem 2,

um = uw + ur and

uk = u? + ur,

which gives the expected relationship

u? = uk + (uw − um).

Note that because this expression for u? is in terms
of k, w, andm, there is actually no need to assume
that R is a set of actual words in V .

2.1 Proofs
Proof of Theorem 1. Note that p(w|C) equals

p(w)1−m

ZC

m∏

i=1

p(w|ci)

=
p(w)1−m

ZC
exp

(
m∑

i=1

uTcivw −
m∑

i=1

logZci

)

=
1

Z
p(w)1−m exp(uTCvw) ,

where Z = ZC
∏m
i=1 Zi, and uC =

∑m
i=1 ui.

Minimizing the KL-divergence

DKL(p(·|c1, . . . , cm)‖p(·|c))

as a function of c is equivalent to maximizing the
negative cross-entropy as a function of uc, i.e., as
maximizing

Q(uc) = Z
∑

w

exp(uTCvw)

p(w)m−1
(uTc vw − logZc) .

Since Q is concave, the maximizers occur where
its gradient vanishes. As∇ucQ equals

Z
∑

w

exp(uTCvw)

p(w)m−1

[
vw −

∑n
`=1 exp(u

T
c v`)v`∑n

k=1 exp(u
T
c vk)

]

=

∑n
`=1 exp(u

T
c v`)v`∑n

k=1 exp(u
T
c vk)

− Z
∑

w

exp(uTCvw)vw
p(w)m−1

=
∑

w∈V
p(w|c)vw −

∑

w∈V
p(w|c1, . . . , cm)vw ,

we see that (7) follows.

Proof of Theorem 2. Recall that uC =
∑m

i=1 ui.
When p(w) = 1/|V | for all w ∈ V , the negative
cross-entropy simplifies to

Q(uc) = Z
∑

w

exp
(
uTCvw

)
(uTc vw − logZc) ,

72



and its gradient∇ucQ to

Z
∑

w

exp(uC
Tvw)

[
vw −

∑n
`=1 exp(u

T
c v`)v`∑n

k=1 exp(u
T
c vk)

]

= Z
∑

w

exp(uC
Tvw)vw −

∑

w

exp(uTc vw)vw .

Thus,∇Q(uC) = 0 and since Q is concave, uC is
its unique maximizer.

Lemma 1. The Skip-Gram model satisfies as-
sumption A2 when m ≤ ∆.
Proof of Lemma 1. First, assume that m = ∆. In
the Skip-Gram model target words are condition-
ally independent given a context word, i.e.,

p(c1, . . . , cm|w) =
m∏

i=1

p(ci|w).

Applying Baye’s rule,

p(w|c1, . . . , cm) =
p(c1, . . . , cm|w)p(w)

p(c1, . . . , cm)

=
p(w)

p(c1, . . . , cm)

m∏

i=1

p(ci|w)

=
p(w)

p(c1, . . . , cm)

m∏

i=1

p(w|ci)p(ci)
p(w)

=
p(w)1−m

ZC

m∏

i=1

p(w|ci) , (8)

where ZC = 1/ (
∏m
i=1 p(ci)). This establishes the

result when m = ∆. The cases m < ∆ follow
by marginalizing out ∆ −m context words in the
equality (8).

Projection of paraphrases onto the vocabulary
Theorem 2 states that if there is a word c in the vo-
cabulary V whose context vector equals the sum of
the context vectors of the words c1, . . . , cm, then
c has the same “meaning”, in the sense of (4), as
the composition of the words c1, . . . , cm. For any
given set of words C = {c1, . . . , cm}, it is un-
likely that there exists a word c ∈ V whose con-
text vector is exactly equal to the sum of the con-
text vectors of the words c1, . . . , cm. Similarly, in
Theorem 1, the solution(s) to (7) will most likely
not equal the context vector of any word in V . In
both cases, we thus need to project the vector(s)
onto words in our vocabulary in some manner.

Since Theorem 1 holds for any prior over V ,
in theory, we could enumerate all words in V and

find the word(s) that minimize the difference of
the left hand side of (7) from the right hand side.
In practice, it turns out that the angle between the
context vector of a word w ∈ V and solution-
vector(s) is a good proxy and one gets very good
experimental results by selecting as the paraphrase
of a collection of words, the word that minimizes
the angle to the paraphrase vector.

Minimizing the angle has been empirically suc-
cessful at capturing composition in multiple log-
linear word models. One way to understand the
success of this approach is to recall that each word
c is characterized by a categorical distribution over
all other words w, as stated in (1). The peaks
of this categorical distribution are precisely the
words with which c co-occurs most often. These
words characterize c more than all the other words
in the vocabulary, so it is reasonable to expect that
a word c′ whose categorical distribution has simi-
lar peaks as the categorical distribution of c is sim-
ilar in meaning to c. Note that the location of the
peaks of p(·|c) are immune to the scaling of uc
(athough the values of p(·|c) may change); thus,
the wordsw which best characterize c are those for
which vw has a high inner product with uc/‖uc‖2.
Since

∣∣∣∣
uTc vw
‖uc‖2

− u
T
c′vw
‖uc′‖2

∣∣∣∣≤
√

2

(
1− u

T
c uc′

‖uc‖2‖uc′‖2

)
‖vw‖2,

it is clear that if the angle between the context
representations of c and c′ is small, the distribu-
tions p(w|c) and p(w|c′) will tend to have similar
peaks.

3 Skip-Gram learns a Sufficient
Dimensionality Reduction Model

The Skip-Gram model assumes that the distribu-
tion of the neighbors of a word follows a specific
exponential parametrization of a categorical distri-
bution. There is empirical evidence that this model
generates features that are useful for NLP tasks,
but there is no a priori guarantee that the training
corpus was generated in this manner. In this sec-
tion, we provide theoretical support for the useful-
ness of the features learned even when the Skip-
Gram model is misspecified.

To do so, we draw a connection between Skip-
Gram and the Sufficient Dimensionality Reduc-
tion (SDR) factorization of Globerson and Tishby
(Globerson and Tishby, 2003). The SDR model

73



learns optimal2 embeddings for discrete random
variables X and Y without assuming any para-
metric form on the distributions of X and Y , and
it is useful in a variety of applications, includ-
ing information retrieval, document classification,
and association analysis (Globerson and Tishby,
2003). As it turns out, these embeddings, like
Skip-Gram, are obtained by learning the param-
eters of an exponentially parameterized distribu-
tion. In Theorem 3 below, we show that if a Skip-
Gram model is fit to the cooccurence statistics of
X and Y , then the output can be trivially modified
(by adding readily-available information on word
frequencies) to obtain the parameters of an SDR
model.

This connection is significant for two reasons:
first, the original algorithm of (Globerson and
Tishby, 2003) for learning SDR embeddings is
expensive, as it involves information projections.
Theorem 3 shows that if one can efficiently fit
a Skip-Gram model, then one can efficiently fit
an SDR model. This implies that Skip-Gram
specific approximation heuristics like negative-
sampling, hierarchical softmax, and Glove, which
are believed to return high-quality approxima-
tions to Skip-Gram parameters (Mikolov et al.,
2013b; Pennington et al., 2014), can be used to
efficiently approximate SDR model parameters.
Second, (Globerson and Tishby, 2003) argues for
the optimality of the SDR embedding in any do-
main where the training information on X and Y
consists of their coocurrence statistics; this op-
timality and the Skip-Gram/SDR connection ar-
gues for the use of Skip-Gram approximations in
such domains, and supports the positive experi-
mental results that have been observed in appli-
cations in network science (Grover and Leskovec,
2016), proteinomics (Asgari and Mofrad, 2015),
and other fields.

As stated above, the SDR factorization solves
the problem of finding information-theoretically
optimal features, given co-occurrence statistics
for a pair of discrete random variables X and
Y . Associate a vector wi to the ith state of
X , a vector hj to the jth state of Y , and let
W = [wT1 · · ·wT|X|]T and H be defined similarly.
Globerson and Tishby show that such optimal fea-
tures can be obtained from a low-rank factoriza-

2Optimal in an information-theoretic sense: they preserve
the maximal mutual information between any pair of random
variables with the observed coocurrence statistics, without re-
gard to the underlying joint distribution.

tion of the matrix G of co-occurence measure-
ments: Gij counts the number of times state i of
X has been observed to co-occur with state j of
Y. The loss of this factorization is measured using
the KL-divergence, and so the optimal features are
obtained from solving the problem

arg min
W,H

DKL

(
G

ZG

∥∥∥∥
1

ZW,H
eWH

T

)
.

Here, ZG =
∑

ij Gij normalizes G into an esti-
mate of the joint pmf of X and Y , and similarly
ZW,H is the constant that normalizes eWH

T
into

a joint pmf. The expression eWH
T

denotes entry-
wise exponentiation of WHT .

Now we revisit the Skip-Gram training objec-
tive, and show that it differs from the SDR ob-
jective only slightly. Whereas the SDR objective
measures the distance between the pmfs given by
(normalized versions of) G and eWH

T
, the Skip-

Gram objective measures the distance between the
pmfs given by (normalized versions of) the rows
of G and eWH

T
. That is, SDR emphasizes fitting

the entire pmfs, while Skip-Gram emphasizes fit-
ting conditional distributions.

Before presenting our main result, we state and
prove the following lemma, which is of indepen-
dent interest and is used in the proof of our main
theorem. Recall that Skip-Gram represents each
word c as a multinomial distribution over all other
words w, and it learns the parameters for these
distributions by a maximum likelihood estima-
tion. It is known that learning model parameters
by maximum likelihood estimation is equivalent
to minimizing the KL-divergence of the learned
model from the empirical distribution; the fol-
lowing lemma establishes the KL-divergence that
Skip-Gram minimizes.

Lemma 2. Let G be the word co-occurrence ma-
trix constructed from the corpus on which a Skip-
Gram model is trained, in which case Gcw is the
number of times word w occurs as a neighboring
word of c in the corpus. For each word c, let gc
denote the empirical frequency of the word in the
corpus, so that

gc =
∑

w

Gcw/
∑

t,w

Gt,w.

Given a positive vector x, let x̂ = x/‖x‖1.
Then, the Skip-Gram model parameters U =[
u1 · · · u|V |

]T and V =
[
v1 · · · u|V |

]T

74



minimize the objective
∑

c

gcDKL(ĝc ‖ êuTc VT ),

where gc is the cth row of G.

Proof. Recall that Skip-Gram chooses U and V
to maximize

Q =
1

T

T∑

i=1

C∑

δ=−C
δ 6=0

log p(wi+δ|wi) ,

where

p(w|c) = e
uTc vw

∑n
i=1 e

uTc vi
.

This objective can be rewritten using the pairwise
cooccurence statistics as

Q=
1

T

∑

c,w

Gcw log p(w|c)

=
1

T

∑

c

[(∑

t

Gct

)∑

w

Gcw∑
tGct

log p(w|c)
]

∝ 1
T

∑

c

[
(
∑

tGct)

(
∑

twGtw)

∑

w

Gcw∑
tGct

log p(w|c)
]

=
∑

c

gc

(∑

w

(
ĝc
)
w

log p(w|c)
)

=
∑

c

gc
(
−DKL(ĝc ‖ p(·|c))−H(ĝc)

)
,

where H(·) denotes the entropy of a distribution.
It follows that since Skip-Gram maximizes Q, it
minimizes
∑

c

gcDKL(ĝc ‖ p(·|c))=
∑

c

gcDKL(ĝc ‖ êuTc VT ).

We now prove our main theorem of this section,
which states that SDR parameters can be obtained
by augmenting the Skip-Gram embeddings to ac-
count for word frequencies.
Theorem 3. Let U,V be the results of fitting a
Skip-Gram model to G, and consider the aug-
mented matrices

Ũ = [U |α] and Ṽ = [V |1],
where

αc = log

(
gc∑

w e
uTc vw

)
and gc =

∑
wGc,w∑
t,wGt,w

.

Then, the features (Ũ, Ṽ) constitute a sufficient
dimensionality reduction of G.

Proof. For convenience, let G denote the joint
pdf matrix G/ZG, and let Ĝ denote the matrix
obtained by normalizing each row of G to be a
probability distribution. Then, it suffices to show
that DKL(G ‖ qW,H) is minimized over the set of
probability distributions
{
qW,H

∣∣∣∣ qW,H(w, c) =
1

Z

(
eWH

T
)
cw

}
,

when W = Ũ and H = Ṽ.
To establish this result, we use a chain rule for

the KL-divergence. Recall that if we denote the
expected KL-divergence between two marginal
pmfs by

DKL(p(·|c)‖q(·|c))

=
∑

c

p(c)

(∑

w

p(w|c) log
(
p(w|c)
q(w|c)

))
,

then the KL-divergence satisfies the chain rule:

DKL(p(w, c)‖q(w, c))
= DKL(p(c)‖q(c)) +DKL(p(w|c)‖q(w|c)).

Using this chain rule, we get

DKL(G ‖ qW,H(w, c)) (9)
=DKL(g ‖ qW,H(c))+DKL(Ĝ‖qW,H(w|c)).

Note that the second term in this sum is, in the
notation of Lemma 2,

DKL(Ĝ‖qW,H(w|c)) =
∑

c

gcDKL(ĝc ‖ êwTc HT ),

so the matrices U and V that are returned by fit-
ting the Skip-Gram model minimize the second
term in this sum. We now show that the augmented
matrices W = Ũ and H = Ṽ also minimize this
second term, and in addition they make the first
term vanish.

To see that the first of these claims holds, i.e.,
that the augmented matrices make the second term
in (9) vanish, note that

qŨ,Ṽ(w|c) ∝ eũ
T
c ṽw = eu

T
c vw+αc ∝ qU,V(w|c),

and the constant of proportionality is independent
of w. It follows that qŨ,Ṽ(w|c) = qU,V(w|c) and

DKL(Ĝ ‖ qŨ,Ṽ(w|c)) = DKL(Ĝ ‖ qU,V(w|c)).

Thus, the choice W = Ũ and H = Ṽ minimizes
the second term in (9).

75



To see that the augmented matrices make the
first term in (9) vanish, observe that when W = Ũ
and H = Ṽ, we have that qŨ,Ṽ(c) = g by con-
struction. This can be verified by calculation:

qŨ,Ṽ(c) =

∑
w qŨ,Ṽ(w, c)∑
w,t qŨ,Ṽ(w, t)

=

∑
w e

uTc vw+αc

∑
w,t e

uTt vw+αt

=

(∑
w e

uTc vw
)

eαc

∑
t

(∑
w e

uTt vw
)

eαt

=

[
(eUV

T
1)� eα

]
c

1T
[
(eUV

T
1)� eα

] .

Here, the notation x � y denotes entry-wise mul-
tiplication of vectors.

Since αc = log(gc) − log
((

eUV
T
1
)
c

)
, we

have

qŨ,Ṽ(c) =

[
(eUV

T
1)� eα

]
c

1T
[
(eUV

T
1)� eα

] = gc∑
t gt

= gc.

The choice W = Ũ and H = Ṽ makes the
first term in (9) vanish, and it also minimizes the
second term in (9). Thus, it follows that the fea-
tures (Ũ, Ṽ) constitute a sufficient dimensionality
reduction of G.

References
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,

and Andrej Risteski. 2016. A latent variable model
approach to PMI-based word embeddings. Transac-
tions of the Association for Computational Linguis-
tics 4:385–399.

Ehsaneddin Asgari and Mohammad R.K. Mofrad.
2015. Continuous distributed representation of bi-
ological sequences for deep proteomics and ge-
nomics. PloS One 10(11).

Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A Neural Probabilistic Lan-
guage Model. Journal Of Machine Learning Re-
search 3:1137–1155.

Kenneth Ward Church and Patrick Hanks. 1990. Word
Association Norms, Mutual Information, and Lexi-
cography. Computational Linguistics 16(1):22–29.

J.R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in Linguistic Analysis pages 1–32.

Amir Globerson and Naftali Tishby. 2003. Sufficient
Dimensionality Reduction. Journal of Machine
Learning Research 3:1307–1331.

Aditya Grover and Jure Leskovec. 2016. node2vec:
Scalable Feature Learning for Networks. In Pro-
ceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing. pages 855–864.

Omer Levy and Yoav Goldberg. 2014. Linguistic Reg-
ularities in Sparse and Explicit Word Representa-
tions. In Proceedings of the Eighteenth Confer-
ence on Computational Natural Language Learning.
pages 171–180.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Repre-
sentations in Vector Space. In International Confer-
ence on Learning Representations.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed Rep-
resentations of Words and Phrases and their Com-
positionality. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference
on Neural Information Processing Systems. pages
3111–3119.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Human Language Tech-
nologies: Conference of the North American Chap-
ter of the Association of Computational Linguistics,
Proceedings. pages 746–751.

Andriy Mnih and Geoffrey Hinton. 2007. Three New
Graphical Models for Statistical Language Mod-
elling. In Proceedings of the 24th International
Conference on Machine Learning. ACM, pages
641–648.

Denis Paperno and Marco Baroni. 2016. When the
Whole is Less than the Sum of Its Parts: How Com-
position Affects PMI Values in Distributional Se-
mantic Vectors. Computational Linguistics 42:345–
350.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global Vectors for
Word Representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP). pages 1532–
1543.

Steven T. Piantadosi. 2014. Zipf’s word frequency
law in natural language: A critical review and fu-
ture directions. Psychonomic Bulletin & Review
21(5):1112–1130.

Douglas L. T. Rohde, Laura M. Gonnerman, and
David C. Plaut. 2006. An improved model of
semantic similarity based on lexical co-occurence.

Communications of the ACM 8:627–633.

76


	Skip-Gram - Zipf + Uniform = Vector Additivity

