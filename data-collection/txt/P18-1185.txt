



















































Visual Attention Model for Name Tagging in Multimodal Social Media


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1990–1999
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

1990

Visual Attention Model for Name Tagging in Multimodal Social Media

Di Lu∗1, Leonardo Neves2, Vitor Carvalho3, Ning Zhang2, Heng Ji1
1Computer Science, Rensselaer Polytechnic Institute

{lud2,jih}@rpi.edu
2Snap Research

{lneves, ning.zhang}@snap.com
3Intuit

vitor carvalho@intuit.com

Abstract

Everyday billions of multimodal posts
containing both images and text are shared
in social media sites such as Snapchat,
Twitter or Instagram. This combination of
image and text in a single message allows
for more creative and expressive forms of
communication, and has become increas-
ingly common in such sites. This new
paradigm brings new challenges for nat-
ural language understanding, as the tex-
tual component tends to be shorter, more
informal, and often is only understood if
combined with the visual context. In this
paper, we explore the task of name tag-
ging in multimodal social media posts.
We start by creating two new multimodal
datasets: one based on Twitter posts1 and
the other based on Snapchat captions (ex-
clusively submitted to public and crowd-
sourced stories). We then propose a novel
model based on Visual Attention that not
only provides deeper visual understanding
on the decisions of the model, but also sig-
nificantly outperforms other state-of-the-
art baseline methods for this task. 2

1 Introduction

Social platforms, like Snapchat, Twitter, Insta-
gram and Pinterest, have become part of our
lives and play an important role in making com-
munication easier and accessible. Once text-
centric, social media platforms are becoming in-

∗∗This work was mostly done during the first author’s in-
ternship at Snap Research.

1The Twitter data and associated images presented in this
paper were downloaded from https://archive.org/
details/twitterstream

2We will make the annotations on Twitter data available
for research purpose upon request.

creasingly multimodal, with users combining im-
ages, videos, audios, and texts for better expres-
siveness. As social media posts become more mul-
timodal, the natural language understanding of the
textual components of these messages becomes in-
creasingly challenging. In fact, it is often the case
that the textual component can only be understood
in combination with the visual context of the mes-
sage.

In this context, here we study the task of Name
Tagging for social media containing both image
and textual contents. Name tagging is a key task
for language understanding, and provides input to
several other tasks such as Question Answering,
Summarization, Searching and Recommendation.
Despite its importance, most of the research in
name tagging has focused on news articles and
longer text documents, and not as much in mul-
timodal social media data (Baldwin et al., 2015).

However, multimodality is not the only chal-
lenge to perform name tagging on such data. The
textual components of these messages are often
very short, which limits context around names.
Moreover, there linguistic variations, slangs, ty-
pos and colloquial language are extremely com-
mon, such as using ‘looooove’ for ‘love’, ‘LosAn-
geles’ for ‘Los Angeles’, and ‘#Chicago #Bull’ for
‘Chicago Bulls’. These characteristics of social
media data clearly illustrate the higher difficulty
of this task, if compared to traditional newswire
name tagging.

In this work, we modify and extend the current
state-of-the-art model (Lample et al., 2016; Ma
and Hovy, 2016) in name tagging to incorporate
the visual information of social media posts us-
ing an Attention mechanism. Although the usually
short textual components of social media posts
provide limited contextual information, the ac-
companying images often provide rich informa-
tion that can be useful for name tagging. For ex-

https://archive.org/details/twitterstream
https://archive.org/details/twitterstream


1991

Figure 1: Examples of Modern Baseball associ-
ated with different images.

ample, as shown in Figure 1, both captions in-
clude the phrase ‘Modern Baseball’. It is not easy
to tell if each Modern Baseball refers to a name
or not from the textual evidence only. However
using the associated images as reference, we can
easily infer that Modern Baseball in the first sen-
tence should be the name of a band because of the
implicit features from the objects like instruments
and stage, and the Modern Baseball in the second
sentence refers to the sport of baseball because of
the pitcher in the image.

In this paper, given an image-sentence pair as
input, we explore a new approach to leverage vi-
sual context for name tagging in text. First, we
propose an attention-based model to extract visual
features from the regions in the image that are
most related to the text. It can ignore irrelevant
visual information. Secondly, we propose to use
a gate to combine textual features extracted by a
Bidirectional Long Short Term Memory (BLSTM)
and extracted visual features, before feed them
into a Conditional Random Fields(CRF) layer for
tag predication. The proposed gate architecture
plays the role to modulate word-level multimodal
features.

We evaluate our model on two labeled datasets
collected from Snapchat and Twitter respectively.
Our experimental results show that the proposed
model outperforms state-for-the-art name tagger
in multimodal social media.

The main contributions of this work are as fol-
lows:

• We create two new datasets for name tag-
ging in multimedia data, one using Twitter
and the other using crowd-sourced Snapchat
posts. These new datasets effectively consti-
tute new benchmarks for the task.

• We propose a visual attention model specif-
ically for name tagging in multimodal social
media data. The proposed end-to-end model

only uses image-sentence pairs as input with-
out any human designed features, and a Vi-
sual Attention component that helps under-
stand the decision making of the model.

2 Model

Figure 2 shows the overall architecture of our
model. We describe three main components of
our model in this section: BLSTM-CRF sequence
labeling model (Section 2.1), Visual Attention
Model (Section 2.3) and Modulation Gate (Sec-
tion 2.4).

Given a pair of sentence and image as input,
the Visual Attention Model extracts regional vi-
sual features from the image and computes the
weighted sum of the regional visual features as the
visual context vector, based on their relatedness
with the sentence. The BLSTM-CRF sequence la-
beling model predicts the label for each word in
the sentence based on both the visual context vec-
tor and the textual information of the words. The
modulation gate controls the combination of the
visual context vector and the word representations
for each word before the CRF layer.

2.1 BLSTM-CRF Sequence Labeling

We model name tagging as a sequence labeling
problem. Given a sequence of words: S =
{s1, s2, ..., sn}, we aim to predict a sequence of
labels: L = {l1, l2, ..., ln}, where li ∈ L and L is
a pre-defined label set.
Bidirectional LSTM. Long Short-term Memory
Networks (LSTMs) (Hochreiter and Schmidhuber,
1997) are variants of Recurrent Neural Networks
(RNNs) designed to capture long-range dependen-
cies of input. The equations of a LSTM cell are as
follows:

it = σ(Wxixt +Whiht−1 + bi)

ft = σ(Wxfxt +Whfht−1 + bf )

c̃t = tanh(Wxcxt +Whcht−1 + bc)

ct = ft � ct−1 + it � c̃t
ot = σ(Wxoxt +Whoht−1 + bo)

ht = ot � tanh(ct)

where xt, ct and ht are the input, memory and hid-
den state at time t respectively. Wxi, Whi, Wxf ,
Whf , Wxc, Whc, Wxo, and Who are weight matri-
ces. � is the element-wise product function and σ
is the element-wise sigmoid function.



1992

Florence and the Machine
surprises ill teen with

private concert 

C
N

N

LSTM

LSTM

LSTM

CRF

LSTM

LSTM

CRF

LSTM

LSTM

CRF

LSTM

LSTM

CRF

Forward
LSTM

Backward
LSTM

CRF

word
embedding

LSTM

encoded
text

Florence and the Machine

B-PER I-PER I-PER I-PER

char
representations

Multimodal Input

Visual Attention Model

Modulation 
Gate 

Attention Model

Visual
Gate

Visual
Gate

Visual
Gate

Visual
Gate

Figure 2: Overall Architecture of the Visual Attention Name Tagging Model.

Name Tagging benefits from both of the past
(left) and the future (right) contexts, thus we im-
plement the Bidirectional LSTM (Graves et al.,
2013; Dyer et al., 2015) by concatenating the left
and right context representations, ht = [

−→
ht ,
←−
ht ],

for each word.
Character-level Representation. Follow-
ing (Lample et al., 2016), we generate the
character-level representation for each word using
another BLSTM. It receives character embeddings
as input and generates representations combining
implicit prefix, suffix and spelling information.
The final word representation xi is the concate-
nation of word embedding ei and character-level
representation ci.

ci = BLSTMchar(si) si ∈ S
xi = [ei, ci]

Conditional random fields (CRFs). For name
tagging, it is important to consider the constraints
of the labels in neighborhood (e.g., I-LOC must
follow B-LOC). CRFs (Lafferty et al., 2001) are
effective to learn those constraints and jointly pre-
dict the best chain of labels. We follow the imple-
mentation of CRFs in (Ma and Hovy, 2016).

2.2 Visual Feature Representation
We use Convolutional Neural Networks
(CNNs) (LeCun et al., 1989) to obtain the
representations of images. Particularly, we use
Residual Net (ResNet) (He et al., 2016), which

Figure 3: CNN for visual features extraction.

achieves state-of-the-art on ImageNet (Rus-
sakovsky et al., 2015) detection, ImageNet
localization, COCO (Lin et al., 2014) detection,
and COCO segmentation tasks. Given an input
pair (S, I), where S represents the word sequence
and I represents the image rescaled to 224x224
pixels, we use ResNet to extract visual features
for regional areas as well as for the whole image
(Fig 3):

Vg = ResNetg(I)

Vr = ResNetr(I)

where the global visual vector Vg, which repre-
sents the whole image, is the output before the
last fully connected layer3. The dimension of Vg
is 1,024. Vr are the visual representations for re-
gional areas and they are extracted from the last
convolutional layer of ResNet, and the dimension
is 1,024x7x7 as shown in Figure 3. 7x7 is the
number of regions in the image and 1,024 is the

3the last fully connect layer outputs the probabilities over
1,000 classes of objects.



1993

dimension of the feature vector. Thus each feature
vector of Vr corresponds to a 32x32 pixel region
of the rescaled input image.

2.3 Visual Attention Model

Figure 4: Example of partially related image and
sentence. (‘I have just bought Jeremy Pied.’)

The global visual representation is a reason-
able representation of the whole input image, but
not the best. Sometimes only parts of the im-
age are related to the associated sentence. For
example, the visual features from the right part
of the image in Figure 4 cannot contribute to in-
ferring the information in the associated sentence
‘I have just bought Jeremy Pied.’ In this work
we utilize visual attention mechanism to combat
the problem, which has been proven effective for
vision-language related tasks such as Image Cap-
tioning (Xu et al., 2015) and Visual Question An-
swering (Yang et al., 2016b; Lu et al., 2016), by
enforcing the model to focus on the regions in im-
ages that are mostly related to context textual in-
formation while ignoring irrelevant regions. Also
the visualization of attention can also help us to
understand the decision making of the model. At-
tention mechanism is mapping a query and a set
of key-value pairs to an output. The output is
a weighted sum of the values and the assigned
weight for each value is computed by a function
of the query and corresponding key. We encode
the sentence into a query vector using an LSTM,
and use regional visual representations Vr as both
keys and values.
Text Query Vector. We use an LSTM to encode
the sentence into a query vector, in which the in-
puts of the LSTM are the concatenations of word
embeddings and character-level word representa-
tions. Different from the LSTM model used for
sequence labeling in Section 2.1, the LSTM here
aims to get the semantic information of the sen-

tence and it is unidirectional:

Q = LSTMquery(S) (1)

Attention Implementation. There are many im-
plementations of visual attention mechanism such
as Multi-layer Perceptron (Bahdanau et al., 2014),
Bilinear (Luong et al., 2015), dot product (Lu-
ong et al., 2015), Scaled Dot Product (Vaswani
et al., 2017), and linear projection after summa-
tion (Yang et al., 2016b). Based on our experi-
mental results, dot product implementations usu-
ally result in more concentrated attentions and lin-
ear projection after summation results in more dis-
persed attentions. In the context of name tagging,
we choose the implementation of linear projec-
tion after summation because it is beneficial for
the model to utilize as many related visual fea-
tures as possible, and concentrated attentions may
make the model bias. For implementation, we first
project the text query vector Q and regional visual
features Vr into the same dimensions:

Pt = tanh(WtQ)

Pv = tanh(WvVr)

then we sum up the projected query vector with
each projected regional visual vector respectively:

A = Pt ⊕ Pv

the weights of the regional visual vectors:

E = softmax(WaA+ ba)

whereWa is weights matrix. The weighted sum of
the regional visual features is:

vc =
∑

αivi αi ∈ E, vi ∈ Vr

We use vc as the visual context vector to initial-
ize the BLSTM sequence labeling model in Sec-
tion 2.1. We compare the performances of the
models using global visual vector Vg and attention
based visual context vector Vc for initialization in
Section 4.

2.4 Visual Modulation Gate
The BLSTM-CRF sequence labeling model ben-
efits from using the visual context vector to ini-
tialize the LSTM cell. However, the better way
to utilize visual features for sequence labeling is
to incorporate the features at word level individ-
ually. However visual features contribute quite



1994

differently when they are used to infer the tags
of different words. For example, we can easily
find matched visual patterns from associated im-
ages for verbs such as ‘sing’, ‘run’, and ‘play’.
Words/Phrases such as names of basketball play-
ers, artists, and buildings are often well-aligned
with objects in images. However it is difficult to
align function words such as ‘the’, ‘of ’ and ‘well’
with visual features. Fortunately, most of the chal-
lenging cases in name tagging involve nouns and
verbs, the disambiguation of which can benefit
more from visual features.

We propose to use a visual modulation gate,
similar to (Miyamoto and Cho, 2016; Yang et al.,
2016a), to dynamically control the combination of
visual features and word representation generated
by BLSTM at word-level, before feed them into
the CRF layer for tag prediction. The equations
for the implementation of modulation gate are as
follows:

βv = σ(Wvhi + Uvvc + bv)

βw = σ(Wwhi + Uwvc + bw)

m = tanh(Wmhi + Umvc + bm)

wm = βw · hi + βv ·m

where hi is the word representation generated by
BLSTM, vc is the computed visual context vector,
Wv, Ww, Wm, Uv, Uw and Um are weight matri-
ces, σ is the element-wise sigmoid function, and
wm is the modulated word representations fed into
the CRF layer in Section 2.1. We conduct experi-
ments to evaluate the impact of modulation gate in
Section 4.

3 Datasets

We evaluate our model on two multimodal
datasets, which are collected from Twitter and
Snapchat respectively. Table 1 summarizes
the data statistics. Both datasets contain four
types of named entities: Location, Person,
Organization and Miscellaneous. Each
data instance contains a pair of sentence and im-
age, and the names in sentences are manually
tagged by three expert labelers.
Twitter name tagging. The Twitter name tagging
dataset contains pairs of tweets and their associ-
ated images extracted from May 2016, January
2017 and June 2017. We use sports and social
event related key words, such as concert, festi-
val, soccer, basketball, as queries. We don’t take

into consideration messages without images for
this experiment. If a tweet has more than one im-
age associated to it, we randomly select one of the
images.
Snap name tagging. The Snap name tagging
dataset consists of caption and image pairs exclu-
sively extracted from snaps submitted to public
and live stories. They were collected between May
and July of 2017. The data contains captions sub-
mitted to multiple community curated stories like
the Electric Daisy Carnival (EDC) music festival
and the Golden State Warrior’s NBA parade.

Both Twitter and Snapchat are social media
with plenty of multimodal posts, but they have ob-
vious differences with sentence length and image
styles. In Twitter, text plays a more important role,
and the sentences in the Twitter dataset are much
longer than those in the Snap dataset (16.0 tokens
vs 8.1 tokens). The image is often more related to
the content of the text and added with the purpose
of illustrating or giving more context. On the other
hand, as users of Snapchat use cameras to commu-
nicate, the roles of text and image are switched.
Captions are often added to complement what is
being portrayed by the snap. On our experiment
section we will show that our proposed model out-
performs baseline on both datasets.

We believe the Twitter dataset can be an im-
portant step towards more research in multimodal
name tagging and we plan to provide it as a bench-
mark upon request.

4 Experiment

4.1 Training
Tokenization. To tokenize the sentences, we use
the same rules as (Owoputi et al., 2013), except we
separate the hashtag ‘#’ with the words after.
Labeling Schema. We use the standard BIO
schema (Sang and Veenstra, 1999), because we
see little difference when we switch to BIOES
schema (Ratinov and Roth, 2009).
Word embeddings. We use the 100-dimensional
GloVe4 (Pennington et al., 2014) embeddings
trained on 2 billions tweets to initialize the lookup
table and do fine-tuning during training.
Character embeddings. As in (Lample et al.,
2016), we randomly initialize the character em-
beddings with uniform samples. Based on exper-
imental results, the size of the character embed-
dings affects little, and we set it as 50.

4https://nlp.stanford.edu/projects/glove/



1995

Training Development Testing

Snapchat Sentences 4,817 1,032 1,033
Tokens 39,035 8,334 8,110

Twitter Sentences 4,290 1,432 1,459
Tokens 68,655 22,872 23,051

Table 1: Sizes of the datasets in numbers of sentence and token.

Pretrained CNNs. We use the pretrained ResNet-
152 (He et al., 2016) from Pytorch.
Early Stopping. We use early stopping (Caruana
et al., 2001; Graves et al., 2013) with a patience of
15 to prevent the model from over-fitting.
Fine Tuning. The models are optimized with fine-
tuning on both the word-embeddings and the pre-
trained ResNet.
Optimization. The models achieve the best per-
formance by using mini-batch stochastic gradient
descent (SGD) with batch size 20 and momentum
0.9 on both datasets. We set an initial learning rate
of η0 = 0.03 with decay rate of ρ = 0.01. We use
a gradient clipping of 5.0 to reduce the effects of
gradient exploding.
Hyper-parameters. We summarize the hyper-
parameters in Table 2.

Hyper-parameter Value
LSTM hidden state size 300

Char LSTM hidden state size 50
visual vector size 100

dropout rate 0.5

Table 2: Hyper-parameters of the networks.

4.2 Results
Table 3 shows the performance of the baseline,
which is BLSTM-CRF with sentences as input
only, and our proposed models on both datasets.
BLSTM-CRF + Global Image Vector: use
global image vector to initialize the BLSTM-CRF.
BLSTM-CRF + Visual attention: use atten-
tion based visual context vector to initialize the
BLSTM-CRF.
BLSTM-CRF + Visual attention + Gate: modu-
late word representations with visual vector.

Our final model BLSTM-CRF + VISUAL AT-
TENTION + GATE, which has visual attention
component and modulation gate, obtains the best
F1 scores on both datasets. Visual features suc-
cessfully play a role of validating entity types. For
example, when there is a person in the image, it

is more likely to include a person name in the as-
sociated sentence, but when there is a soccer field
in the image, it is more likely to include a sports
team name.

All the models get better scores on Twitter
dataset than on Snap dataset, because the average
length of the sentences in Snap dataset (8.1 tokens)
is much smaller than that of Twitter dataset (16.0
tokens), which means there is much less contex-
tual information in Snap dataset.

Also comparing the gains from visual features
on different datasets, we find that the model bene-
fits more from visual features on Twitter dataset,
considering the much higher baseline scores on
Twitter dataset. Based on our observation, users
of Snapchat often post selfies with captions, which
means some of the images are not strongly related
to their associated captions. In contrast, users of
Twitter prefer to post images to illustrate texts

4.3 Attention Visualization

Figure 5 shows some good examples of the atten-
tion visualization and their corresponding name
tagging results. The model can successfully focus
on appropriate regions when the images are well
aligned with the associated sentences. Based on
our observation, the multimodal contexts in posts
related to sports, concerts or festival are usually
better aligned with each other, therefore the visual
features easily contribute to these cases. For ex-
ample, the ball and shoot action in example (a) in
Figure 5 indicates that the context should be re-
lated to basketball, thus the ‘Warriors’ should be
the name of a sports team. A singing person with a
microphone in example (b) indicates that the name
of an artist or a band (‘Radiohead’) may appear in
the sentence.

The second and the third rows in Figure 5 show
some more challenging cases whose tagging re-
sults benefit from visual features. In example (d),
the model pays attention to the big Apple logo,
thus tags the ‘Apple’ in the sentence as an Orga-
nization name. In example (e) and (i), a small



1996

Model
Snap Captions Twitter

Precision Recall F1 Precision Recall F1
BLSTM-CRF 57.71 58.65 58.18 78.88 77.47 78.17

BLSTM-CRF + Global Image Vector 61.49 57.84 59.61 79.75 77.32 78.51
BLSTM-CRF + Visual attention 65.53 57.03 60.98 80.81 77.36 79.05

BLSTM-CRF + Visual attention + Gate 66.67 57.84 61.94 81.62 79.90 80.75

Table 3: Results of our models on noisy social media data.

group of people indicates that it is likely to include
names of bands (‘Florence and the Machine’ and
‘BTS’). And a crowd can indicate an organization
(‘Warriorette’ in example (i)). A jersey shirt on
the table indicates a sports team. (‘Leicester’ in
example (h) can refer to both a city and a soccer
club based in it.)

4.4 Error Analysis

Figure 6 shows some failed examples that are cat-
egorized into three types: (1) bad alignments be-
tween visual and textual information; (2) blur im-
ages; (3) wrong attention made by the model.

Name tagging greatly benefits from visual fea-

tures when the sentences are well aligned with the
associated image as we show in Section 4.3. But it
is not always the case in social media. The exam-
ple (a) in Figure 6 shows a failed example resulted
from poor alignment between sentences and im-
ages. In this image, there are two bins standing in
front of a wall, but the sentence talks about bas-
ketball players. The unrelated visual information
makes the model tag ‘Cleveland’ as a Location,
however it refers to the basketball team ‘Cleveland
Cavaliers’.

The image in example (b) is blur, so the ex-
tracted visual information extracted actually intro-
duces noise instead of additional information. The

Figure 5: Examples of visual attentions and NER outputs.



1997

(a). Nice image of [PER Kevin Love] and [PER
Kyle Korver] during 1st half #NBAFinals #Cavsin9

#[LOC Cleveland]

(b). Very drunk in a #magnum concert (c). Looking forward to editing some SBU
baseball shots from Saturday.

Figure 6: Examples of Failed Visual Attention.

image in example (c) is about a baseball pitcher,
but our model pays attention to the top right cor-
ner of the image. The visual context feature com-
puted by our model is not related to the sentence,
and results in missed tagging of ‘SBU’, which is
an organization name.

5 Related Work

In this section, we summarize relevant background
on previous work on name tagging and visual at-
tention.
Name Tagging. In recent years, (Chiu and
Nichols, 2015; Lample et al., 2016; Ma and Hovy,
2016) proposed several neural network architec-
tures for named tagging that outperform tradi-
tional explicit features based methods (Chieu and
Ng, 2002; Florian et al., 2003; Ando and Zhang,
2005; Ratinov and Roth, 2009; Lin and Wu, 2009;
Passos et al., 2014; Luo et al., 2015). They all
use Bidirectional LSTM (BLSTM) to extract fea-
tures from a sequence of words. For character-
level representations, (Lample et al., 2016) pro-
posed to use another BLSTM to capture prefix
and suffix information of words, and (Chiu and
Nichols, 2015; Ma and Hovy, 2016) used CNN
to extract position-independent character features.
On top of BLSTM, (Chiu and Nichols, 2015) used
a softmax layer to predict the label for each word,
and (Lample et al., 2016; Ma and Hovy, 2016)
used a CRF layer for joint prediction. Com-
pared with traditional approaches, neural networks
based approaches do not require hand-crafted fea-
tures and achieved state-of-the-art performance
on name tagging (Ma and Hovy, 2016). How-
ever, these methods were mainly developed for
newswire and paid little attention to social me-
dia. For name tagging in social media, (Ritter
et al., 2011) leveraged a large amount of unla-
beled data and many dictionaries into a pipeline
model. (Limsopatham and Collier, 2016) adapted
the BLSTM-CRF model with additional word

shape information, and (Aguilar et al., 2017) uti-
lized an effective multi-task approach. Among
these methods, our model is most similar to (Lam-
ple et al., 2016), but we designed a new visual at-
tention component and a modulation control gate.
Visual Attention. Since the attention mechanism
was proposed by (Bahdanau et al., 2014), it has
been widely adopted to language and vision re-
lated tasks, such as Image Captioning and Visual
Question Answering (VQA), by retrieving the vi-
sual features most related to text context (Zhu
et al., 2016; Anderson et al., 2017; Xu and Saenko,
2016; Chen et al., 2015). (Xu et al., 2015) pro-
posed to predict a word based on the visual patch
that is most related to the last predicted word for
image captioning. (Yang et al., 2016b; Lu et al.,
2016) applied attention mechanism for VQA, to
find the regions in images that are most related to
the questions. (Yu et al., 2016) applied the visual
attention mechanism on video captioning. Our at-
tention implementation approach in this work is
similar to those used for VQA. The model finds
the regions in images that are most related to the
accompanying sentences, and then feed the visual
features into an BLSTM-CRF sequence labeling
model. The differences are: (1) we add visual con-
text feature at each step of sequence labeling; and
(2) we propose to use a gate to control the combi-
nation of the visual information and textual infor-
mation based on their relatedness. 2

6 Conclusions and Future Work

We propose a gated Visual Attention for name
tagging in multimodal social media. We con-
struct two multimodal datasets from Twitter and
Snapchat. Experiments show an absolute 3%-4%
F-score gain. We hope this work will encour-
age more research on multimodal social media in
the future and we plan on making our benchmark
available upon request.

Name Tagging for more fine-grained types (e.g.



1998

soccer team, basketball team, politician, artist) can
benefit more from visual features. For example, an
image including a pitcher indicates that the ‘Gi-
ants’ in context should refer to the baseball team
‘San Francisco Giants’. We plan to expand our
model to tasks such as fine-grained Name Tagging
or Entity Liking in the future.

Acknowledgments

This work was partially supported by the U.S.
DARPA AIDA Program No. FA8750-18-2-0014
and U.S. ARL NS-CTA No. W911NF-09-2-0053.
The views and conclusions contained in this doc-
ument are those of the authors and should not be
interpreted as representing the official policies, ei-
ther expressed or implied, of the U.S. Govern-
ment. The U.S. Government is authorized to re-
produce and distribute reprints for Government
purposes notwithstanding any copyright notation
here on.

References
Gustavo Aguilar, Suraj Maharjan, Adrian Pastor López

Monroy, and Thamar Solorio. 2017. A multi-task
approach for named entity recognition in social me-
dia data. In Proceedings of the 3rd Workshop on
Noisy User-generated Text.

Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei
Zhang. 2017. Bottom-up and top-down attention
for image captioning and vqa. arXiv preprint
arXiv:1707.07998.

Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple tasks
and unlabeled data. Journal of Machine Learning
Research.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. In Proceedings of the
2015 International Conference on Learning Repre-
sentations.

Timothy Baldwin, Marie-Catherine de Marneffe,
Bo Han, Young-Bum Kim, Alan Ritter, and Wei
Xu. 2015. Shared tasks of the 2015 workshop on
noisy user-generated text: Twitter lexical normaliza-
tion and named entity recognition. In Proceedings
of the Workshop on Noisy User-generated Text.

Rich Caruana, Steve Lawrence, and C Lee Giles. 2001.
Overfitting in neural nets: Backpropagation, conju-
gate gradient, and early stopping. In Proceedings of
the 2001 Advances in Neural Information Process-
ing Systems.

Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan
Gao, Wei Xu, and Ram Nevatia. 2015. Abc-
cnn: An attention based convolutional neural net-
work for visual question answering. arXiv preprint
arXiv:1511.05960.

Hai Leong Chieu and Hwee Tou Ng. 2002. Named en-
tity recognition: a maximum entropy approach using
global information. In Proceedings of the 19th inter-
national conference on Computational Linguistics.

Jason PC Chiu and Eric Nichols. 2015. Named entity
recognition with bidirectional lstm-cnns. Transac-
tions of the Association of Computational Linguis-
tics.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing.

Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In Proceedings of
the seventh conference on Natural language learn-
ing at HLT-NAACL.

Alex Graves, Abdel-rahman Mohamed, and Geoffrey
Hinton. 2013. Speech recognition with deep recur-
rent neural networks. In Proceedings of the 2013
IEEE international conference on acoustics, speech
and signal processing.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.

Yann LeCun, Bernhard Boser, John S Denker, Don-
nie Henderson, Richard E Howard, Wayne Hubbard,
and Lawrence D Jackel. 1989. Backpropagation ap-
plied to handwritten zip code recognition. Neural
computation.

Nut Limsopatham and Nigel Henry Collier. 2016.
Bidirectional lstm for named entity recognition in
twitter messages. In Proceedings of the 2nd Work-
shop on Noisy User-generated Text.



1999

Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. 2014. Microsoft coco:
Common objects in context. In Proceedings of the
2014 European Conference on Computer Vision.

Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi
Parikh. 2016. Hierarchical question-image co-
attention for visual question answering. In Proceed-
ings of the 2016 Advances In Neural Information
Processing Systems.

Gang Luo, Xiaojiang Huang, Chin-Yew Lin, and Za-
iqing Nie. 2015. Joint entity recognition and disam-
biguation. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Pro-
cessing.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Proceedings of
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics.

Yasumasa Miyamoto and Kyunghyun Cho. 2016.
Gated word-character recurrent language model. In
Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing.

Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.

Alexandre Passos, Vineet Kumar, and Andrew McCal-
lum. 2014. Lexicon infused phrase embeddings for
named entity resolution. In Proceedings of the Eigh-
teenth Conference on Computational Natural Lan-
guage Learning.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing.

Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning.

Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the conference on Empiri-
cal Methods in Natural Language Processing.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, et al.
2015. Imagenet large scale visual recognition chal-
lenge. International Journal of Computer Vision.

Erik F Sang and Jorn Veenstra. 1999. Representing text
chunks. In Proceedings of the ninth conference on
European chapter of the Association for Computa-
tional Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proceedings of the 2017 Advances in
Neural Information Processing Systems.

Huijuan Xu and Kate Saenko. 2016. Ask, attend and
answer: Exploring question-guided spatial attention
for visual question answering. In Proceedings of the
2016 European Conference on Computer Vision.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:
Neural image caption generation with visual atten-
tion. In Proceedings of the 2015 International Con-
ference on Machine Learning.

Zhilin Yang, Bhuwan Dhingra, Ye Yuan, Junjie Hu,
William W Cohen, and Ruslan Salakhutdinov.
2016a. Words or characters? fine-grained gating for
reading comprehension. In Proceedings of the 2016
International Conference on Learning Representa-
tions.

Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng,
and Alex Smola. 2016b. Stacked attention networks
for image question answering. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition.

Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, and
Wei Xu. 2016. Video paragraph captioning using
hierarchical recurrent neural networks. In Proceed-
ings of the IEEE conference on computer vision and
pattern recognition.

Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-
Fei. 2016. Visual7w: Grounded question answering
in images. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition.


