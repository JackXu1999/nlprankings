



















































Graph Neural Networks with Generated Parameters for Relation Extraction


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1331–1339
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

1331

Graph Neural Networks with Generated Parameters for Relation
Extraction

Hao Zhu1 Yankai Lin1 Zhiyuan Liu1 Jie Fu2 Tat-seng Chua3 Maosong Sun1
1 State Key Lab on Intelligent Technology and Systems

Department of Computer Science and Technology
Institute for Artificial Intelligence, Tsinghua Univerisity, Beijing, China

2 National University of Singapore, Singapore
3 Université de Montréal, Montréal, Québec, Canada

{{zhuhao15,linyk14}@mails,{liuzy, sms}@}tsinghua.edu.cn
jie.fu@polymtl.ca,chuats@comp.nus.edu.sg

Abstract

In this paper, we propose a novel graph neu-
ral network with generated parameters (GP-
GNNs). The parameters in the propagation
module, i.e. the transition matrices used in
message passing procedure, are produced by
a generator taking natural language sentences
as inputs. We verify GP-GNNs in relation ex-
traction from text, both on bag- and instance-
settings. Experimental results on a human-
annotated dataset and two distantly supervised
datasets show that multi-hop reasoning mecha-
nism yields significant improvements. We also
perform a qualitative analysis to demonstrate
that our model could discover more accu-
rate relations by multi-hop relational reason-
ing. Codes and data are released at https:
//github.com/thunlp/gp-gnn.

1 Introduction

In recent years, graph neural networks (GNNs)
have been applied to various fields of ma-
chine learning, including node classification
(Kipf and Welling, 2016), relation classification
(Schlichtkrull et al., 2017), molecular property
prediction (Gilmer et al., 2017), few-shot learning
(Garcia and Bruna, 2018), and achieved promising
results on these tasks. These works have demon-
strated GNNs’ strong power to process relational
reasoning on graphs.

Relational reasoning aims to abstractly reason
about entities/objects and their relations, which
is an important part of human intelligence. Be-
sides graphs, relational reasoning is also of great
importance in many natural language processing

The original idea comes from several discussions be-
tween Hao Zhu and Jie Fu while Hao Zhu visiting NUS;
Hao Zhu designed research, prepared datasets, and conducted
experiments; Jie Fu, Yankai Lin, and Zhiyuan Liu also par-
ticipated in discussion while planning experiments; Zhiyuan
Liu, Tat-seng Chua and Maosong Sun proofread the paper.
Zhiyuan Liu serves as the corresponding author.

tasks such as question answering, relation extrac-
tion, summarization, etc. Consider the example
shown in Fig. 1, existing relation extraction mod-
els could easily extract the facts that Luc Besson
directed a film Léon: The Professional and that
the film is in English, but fail to infer the rela-
tionship between Luc Besson and English with-
out multi-hop relational reasoning. By consider-
ing the reasoning patterns, one can discover that
Luc Besson could speak English following a rea-
soning logic that Luc Besson directed Léon: The
Professional and this film is in English indicates
Luc Besson could speak English. However, most
existing GNNs can only process multi-hop rela-
tional reasoning on pre-defined graphs and cannot
be directly applied in natural language relational
reasoning. Enabling multi-hop relational reason-
ing in natural languages remains an open problem.

To address this issue, in this paper, we pro-
pose graph neural networks with generated pa-
rameters (GP-GNNs), to adapt graph neural net-
works to solve the natural language relational rea-
soning task. GP-GNNs first constructs a fully-
connected graph with the entities in the sequence
of text. After that, it employs three modules
to process relational reasoning: (1) an encoding
module which enables edges to encode rich in-
formation from natural languages, (2) a propaga-
tion module which propagates relational informa-
tion among various nodes, and (3) a classification
module which makes predictions with node rep-
resentations. As compared to traditional GNNs,
GP-GNNs could learn edge parameters from natu-
ral languages, extending it from performing infer-
ence on only non-relational graphs or graphs with
a limited number of edge types to unstructured in-
puts such as texts.

In the experiments, we apply GP-GNNs to a
classic natural language relational reasoning task:

https://github.com/thunlp/gp-gnn
https://github.com/thunlp/gp-gnn


1332

Léon: The Professional is a 1996 English-language French thriller film directed by Luc Besson.

LéonEnglish Luc Besson

Language Spoken

Language Cast member

Figure 1: An example of relation extraction from plain text. Given a sentence with several entities marked, we
model the interaction between these entities by generating the weights of graph neural networks. Modeling the
relationship between “Léon” and “English” as well as “Luc Besson” helps discover the relationship between “Luc
Besson” and “English”.

relation extraction from text. We carry out experi-
ments on Wikipedia corpus aligned with Wikidata
knowledge base (Vrandečić and Krötzsch, 2014)
and build a human annotated test set as well as
two distantly labeled test sets with different lev-
els of denseness.Experiment results show that our
model outperforms other models on relation ex-
traction task by considering multi-hop relational
reasoning. We also perform a qualitative analysis
which shows that our model could discover more
relations by reasoning more robustly as compared
to baseline models.

Our main contributions are in two-fold:
(1) We extend a novel graph neural network

model with generated parameters, to enable rela-
tional message-passing with rich text information,
which could be applied to process relational rea-
soning on unstructured inputs such as natural lan-
guage.

(2) We verify our GP-GNNs on the task of re-
lation extraction from text, which demonstrates
its ability on multi-hop relational reasoning as
compared to those models which extract relation-
ships separately. Moreover, we also present three
datasets, which could help future researchers com-
pare their models in different settings.

2 Related Work

2.1 Graph Neural Networks (GNNs)
GNNs were first proposed in (Scarselli et al.,
2009) and are trained via the Almeida-Pineda al-
gorithm (Almeida, 1987). Later the authors in
Li et al. (2016) replace the Almeida-Pineda algo-
rithm with the more generic backpropagation and
demonstrate its effectiveness empirically. Gilmer
et al. (2017) propose to apply GNNs to molecu-
lar property prediction tasks. Garcia and Bruna

(2018) shows how to use GNNs to learn clas-
sifiers on image datasets in a few-shot manner.
Gilmer et al. (2017) study the effectiveness of
message-passing in quantum chemistry. Dhingra
et al. (2017) apply message-passing on a graph
constructed by coreference links to answer rela-
tional questions. There are relatively fewer pa-
pers discussing how to adapt GNNs to natural
language tasks. For example, Marcheggiani and
Titov (2017) propose to apply GNNs to semantic
role labeling and Schlichtkrull et al. (2017) ap-
ply GNNs to knowledge base completion tasks.
Zhang et al. (2018) apply GNNs to relation extrac-
tion by encoding dependency trees, and De Cao
et al. (2018) apply GNNs to multi-hop ques-
tion answering by encoding co-occurence and co-
reference relationships. Although they also con-
sider applying GNNs to natural language process-
ing tasks, they still perform message-passing on
predefined graphs. Johnson (2017) introduces a
novel neural architecture to generate a graph based
on the textual input and dynamically update the
relationship during the learning process. In sharp
contrast, this paper focuses on extracting relations
from real-world relation datasets.

2.2 Relational Reasoning

Relational reasoning has been explored in various
fields. For example, Santoro et al. (2017) propose
a simple neural network to reason the relationship
of objects in a picture, Xu et al. (2017) build up a
scene graph according to an image, and Kipf et al.
(2018) model the interaction of physical objects.

In this paper, we focus on the relational rea-
soning in the natural language domain. Existing
works (Zeng et al., 2014, 2015; Lin et al., 2016)
have demonstrated that neural networks are capa-



1333

ble of capturing the pair-wise relationship between
entities in certain situations. For example, Zeng
et al. (2014) is one of the earliest works that ap-
plies a simple CNN to this task, and Zeng et al.
(2015) further extends it with piece-wise max-
pooling. Nguyen and Grishman (2015) propose a
multi-window version of CNN for relation extrac-
tion. Lin et al. (2016) study an attention mech-
anism for relation extraction tasks. Peng et al.
(2017) predict n-ary relations of entities in differ-
ent sentences with Graph LSTMs. Le and Titov
(2018) treat relations as latent variables which are
capable of inducing the relations without any su-
pervision signals. Zeng et al. (2017) show that the
relation path has an important role in relation ex-
traction. Miwa and Bansal (2016) show the effec-
tiveness of LSTMs (Hochreiter and Schmidhuber,
1997) in relation extraction. Christopoulou et al.
(2018) proposed a walk-based model to do rela-
tion extraction. The most related work is Sorokin
and Gurevych (2017), where the proposed model
incorporates contextual relations with an attention
mechanism when predicting the relation of a target
entity pair. The drawback of existing approaches
is that they could not make full use of the multi-
hop inference patterns among multiple entity pairs
and their relations within the sentence.

3 Graph Neural Network with
Generated Parameters (GP-GNNs)

We first define the task of natural language rela-
tional reasoning. Given a sequence of text with
m entities, it aims to reason on both the text and
entities and make a prediction of the labels of the
entities or entity pairs.

In this section, we will introduce the general
framework of GP-GNNs. GP-GNNs first build
a fully-connected graph G = (V, E), where V
is the set of entities, and each edge (vi, vj) ∈
E , vi, vj ∈ V corresponds to a sequence s =
xi,j0 , x

i,j
1 , . . . , x

i,j
l−1 extracted from the text. After

that, GP-GNNs employ three modules including
(1) encoding module, (2) propagation module and
(3) classification module to process relational rea-
soning, as shown in Fig. 2.

3.1 Encoding Module

The encoding module converts sequences into
transition matrices corresponding to edges, i.e. the

parameters of the propagation module, by

A(n)i,j = f(E(x
i,j
0 ), E(x

i,j
1 ), · · · , E(xi,jl−1); θne ),

(1)
where f(·) could be any model that could encode
sequential data, such as LSTMs, GRUs, CNNs,
E(·) indicates an embedding function, and θne de-
notes the parameters of the encoding module of
n-th layer.

3.2 Propagation Module

The propagation module learns representations for
nodes layer by layer. The initial embeddings of
nodes, i.e. the representations of layer 0, are
task-related, which could be embeddings that en-
code features of nodes or just one-hot embeddings.
Given representations of layer n, the representa-
tions of layer n+ 1 are calculated by

h
(n+1)
i =

∑
vj∈N (vi)

σ(A(n)i,j h
(n)
j ), (2)

whereN (vi) denotes the neighbours of node vi in
graph G and σ(·) denotes a non-linear activation
function.

3.3 Classification Module

Generally, the classification module takes node
representations as inputs and outputs predictions.
Therefore, the loss of GP-GNNs could be calcu-
lated as

L = g(h00:|V|−1,h10:|V|−1, . . . ,hK0:|V|−1, Y ; θc),
(3)

where θc denotes the parameters of the classifica-
tion module,K is the number of layers in propaga-
tion module and Y denotes the ground truth label.
The parameters in GP-GNNs are trained by gradi-
ent descent methods.

4 Relation Extraction with GP-GNNs

Relation extraction from text is a classic natu-
ral language relational reasoning task. Given a
sentence s = (x0, x1, . . . , xl−1), a set of re-
lations R and a set of entities in this sentence
Vs = {v1, v2, . . . , v|Vs|}, where each vi consists
of one or a sequence of tokens, relation extraction
from text is to identify the pairwise relationship
rvi,vj ∈ R between each entity pair (vi, vj).

In this section, we will introduce how to apply
GP-GNNs to relation extraction.



1334

Encoding Module

Propagation 
Module Classification Module

h
(n)
1

h
(n)
2

h
(n)
3

A(n)1,2

A(n)2,3

A(n)3,1

x1,23 x
1,2
4x

1,2
2x

1,2
1x

1,2
0

Figure 2: Overall architecture: an encoding module takes a sequence of vector representations as inputs, and output
a transition matrix as output; a propagation module propagates the hidden states from nodes to its neighbours
with the generated transition matrix; a classification module provides task-related predictions according to nodes
representations.

4.1 Encoding Module

To encode the context of entity pairs (or edges in
the graph), we first concatenate the position em-
beddings with word embeddings in the sentence:

E(xi,jt ) = [xt;p
i,j
t ], (4)

where xt denotes the word embedding of word xt
and pi,jt denotes the position embedding of word
position t relative to the entity pair’s position i, j
(Details of these two embeddings are introduced in
the next two paragraphs.) After that, we feed the
representations of entity pairs into encoder f(·)
which contains a bi-directional LSTM and a multi-
layer perceptron:

A(n)i,j = [MLPn(BiLSTMn((E(x
i,j
0 ), E(x

i,j
1 ), · · · , E(x

i,j
l−1))],

(5)

where n denotes the index of layer 1, [·] means
reshaping a vector as a matrix, BiLSTM encodes a
sequence by concatenating tail hidden states of the
forward LSTM and head hidden states of the back-
ward LSTM together and MLP denotes a multi-
layer perceptron with non-linear activation σ.

Word Representations We first map each to-
ken xt of sentence {x0, x1, . . . , xl−1} to a k-
dimensional embedding vector xt using a word
embedding matrix We ∈ R|V |×dw , where |V | is
the size of the vocabulary. Throughout this pa-
per, we stick to 50-dimensional GloVe embed-
dings pre-trained on a 6-billion-word corpus (Pen-
nington et al., 2014).

1Adding index to neural models means their parameters
are different among layers.

Position Embedding In this work, we consider
a simple entity marking scheme2: we mark each
token in the sentence as either belonging to the
first entity vi, the second entity vj or to neither
of those. Each position marker is also mapped to
a dp-dimensional vector by a position embedding
matrix P ∈ R3×dp . We use notation pi,jt to repre-
sent the position embedding for xt corresponding
to entity pair (vi, vj).

4.2 Propagation Module

Next, we use Eq. (2) to propagate information
among nodes where the initial embeddings of
nodes and number of layers are further specified
as follows.

The Initial Embeddings of Nodes Suppose we
are focusing on extracting the relationship be-
tween entity vi and entity vj , the initial embed-
dings of them are annotated as h(0)vi = asubject,
and h(0)vj = aobject, while the initial embeddings
of other entities are set to all zeros. We set spe-
cial values for the head and tail entity’s initial em-
beddings as a kind of “flag” messages which we
expect to be passed through propagation. Anno-
tators asubject and aobject could also carry the prior
knowledge about subject entity and object entity.
In our experiments, we generalize the idea of
Gated Graph Neural Networks (Li et al., 2016) by
setting asubject = [1;0]> and aobject = [0;1]>3.

2As pointed out by Sorokin and Gurevych (2017), other
position markers lead to no improvement in performance.

3The dimensions of 1 and 0 are the same. Hence, dr
should be positive even integers. The embedding of subject
and object could also carry the type information by changing
annotators. We leave this extension for future work.



1335

Number of Layers In general graphs, the num-
ber of layers K is chosen to be of the order of the
graph diameter so that all nodes obtain informa-
tion from the entire graph. In our context, how-
ever, since the graph is densely connected, the
depth is interpreted simply as giving the model
more expressive power. We treat K as a hyper-
parameter, the effectiveness of which will be dis-
cussed in detail (Sect. 5.4).

4.3 Classification Module

The output module takes the embeddings of the
target entity pair (vi, vj) as input, which are first
converted by:

rvi,vj = [[h
(1)
vi �h

(1)
vj ]
>; [h(2)vi �h

(2)
vj ]
>; . . . ; [h(K)vi �h

(K)
vj ]

>],
(6)

where � represents element-wise multiplication.
This could be used for classification:

P(rvi,vj |h, t, s) = softmax(MLP(rvi,vj )), (7)

where rvi,vj ∈ R, and MLP denotes a multi-layer
perceptron module.

We use cross entropy here as the classification
loss

L =
∑
s∈S

∑
i6=j

log P(rvi,vj |i, j, s), (8)

where rvi,vj denotes the relation label for entity
pair (vi, vj) and S denotes the whole corpus.

In practice, we stack the embeddings for every
target entity pairs together to infer the underlying
relationship between each pair of entities. We use
PyTorch (Paszke et al., 2017) to implement our
models. To make it more efficient, we avoid us-
ing loop-based, scalar-oriented code by matrix and
vector operations.

5 Experiments

Our experiments mainly aim at: (1) showing that
our best models could improve the performance
of relation extraction under a variety of settings;
(2) illustrating that how the number of layers af-
fect the performance of our model; and (3) per-
forming a qualitative investigation to highlight the
difference between our models and baseline mod-
els. In both part (1) and part (2), we do three sub-
parts of experiments: (i) we will first show that
our models could improve instance-level relation
extraction on a human annotated test set, and (ii)
then we will show that our models could also help

enhance the performance of bag-level relation ex-
traction on a distantly labeled test set 4, and (iii)
we also split a subset of distantly labeled test set,
where the number of entities and edges is large.

5.1 Experiment Settings

5.1.1 Datasets

Distantly labeled set Sorokin and Gurevych
(2017) have proposed a dataset with Wikipedia
corpora. There is a small difference between our
task and theirs: our task is to extract the relation-
ship between every pair of entities in the sentence,
whereas their task is to extract the relationship be-
tween the given entity pair and the context entity
pairs. Therefore, we need to modify their dataset:
(1) We added reversed edges if they are missing
from a given triple, e.g. if triple (Earth, part
of, Solar System) exists in the sentence, we add a
reversed label, (Solar System, has a member,
Earth), to it; (2) For all of the entity pairs with no
relations, we added “NA” labels to them.5 We use
the same training set for all of the experiments.

Human annotated test set Based on the test set
provided by (Sorokin and Gurevych, 2017), 5 an-
notators6 are asked to label the dataset. They are
asked to decide whether or not the distant super-
vision is right for every pair of entities. Only the
instances accepted by all 5 annotators are incorpo-
rated into the human annotated test set. There are
350 sentences and 1,230 triples in this test set.

Dense distantly labeled test set We further split
a dense test set from the distantly labeled test set.
Our criteria are: (1) the number of entities should
be strictly larger than 2; and (2) there must be at
least one circle (with at least three entities) in the
ground-truth label of the sentence 7. This test set
could be used to test our methods’ performance on
sentences with the complex interaction between
entities. There are 1,350 sentences and more than
17,915 triples and 7,906 relational facts in this test
set.

4Bag-level relation extraction is a widely accepted
scheme for relation extraction with distant supervision, which
means the relation of an entity pair is predicted by aggregat-
ing a bag of instances.

5We also resolve entities at the same position and remove
self-loops from the previous dataset. Furthermore, we limit
the number of entities in one sentence to 9, resulting in only
0.0007 data loss.

6They are all well-educated university students.
7Every edge in the circle has a non-“NA” label.



1336

5.1.2 Models for Comparison
We select the following models for comparison,
the first four of which are our baseline models.

Context-Aware RE, proposed by Sorokin and
Gurevych (2017). This model utilizes attention
mechanism to encode the context relations for pre-
dicting target relations. It was the state-of-the-art
models on Wikipedia dataset. This baseline is im-
plemented by ourselves based on authors’ public
repo8.

Multi-Window CNN. Zeng et al. (2014) uti-
lize convolutional neural networks to classify rela-
tions. Different from the original version of CNN
proposed in Zeng et al. (2014), our implementa-
tion, follows Nguyen and Grishman (2015), con-
catenates features extracted by three different win-
dow sizes: 3, 5, 7.

PCNN, proposed by Zeng et al. (2015). This
model divides the whole sentence into three pieces
and applies max-pooling after convolution layer
piece-wisely. For CNN and following PCNN, the
entity markers are the same as originally proposed
in Zeng et al. (2014, 2015).

LSTM or GP-GNN with K = 1 layer. Bi-
directional LSTM (Schuster and Paliwal, 1997)
could be seen as an 1-layer variant of our model.

GP-GNN with K = 2 or K = 3 layers. These
models are capable of performing 2-hop reasoning
and 3-hop reasoning, respectively.

5.1.3 Hyper-parameters
We select the best parameters for the validation
set. We select non-linear activation functions be-
tween relu and tanh, and select dn among
{2, 4, 8, 12, 16}9. We have also tried two forms
of adjacent matrices: tied-weights (set A(n) =
A(n+1)) and untied-weights. Table 1 shows our
best hyper-parameter settings, which are used in
all of our experiments.

5.2 Evaluation Details

So far, we have only talked about the way to imple-
ment sentence-level relation extraction. To evalu-
ate our models and baseline models in bag-level,
we utilize a bag of sentences with a given entity
pair to score the relations between them. Zeng
et al. (2015) formalize the bag-level relation ex-
traction as multi-instance learning. Here, we fol-

8https://github.com/UKPLab/
emnlp2017-relation-extraction

9We set all dns to be the same as we do not see improve-
ments using different dns

Hyper-parameters Value

learning rate 0.001
batch size 50
dropout ratio 0.5
hidden state size 256
non-linear activation σ relu
embedding size for #layers = 1 8
embedding size for #layers = 2 and 3 12
adjacent matrices untied

Table 1: Hyper-parameters settings.

low their idea and define the score function of an
entity pair and its corresponding relation r as a
max-one setting:

E(r|vi, vj , S) = max
s∈S

P(rvi,vj |i, j, s). (9)

Dataset Human Annotated Test Set
Metric Acc Macro F1
Multi-Window CNN 47.3 17.5
PCNN 30.8 3.2
Context-Aware RE 68.9 44.9
GP-GNN (#layers=1) 62.9 44.1
GP-GNN (#layers=2) 69.5 44.2
GP-GNN (#layers=3) 75.3 47.9

Table 2: Results on human annotated dataset

5.3 Effectiveness of Reasoning Mechanism

From Table 2 and 3, we can see that our best
models outperform all the baseline models signif-
icantly on all three test sets. These results indicate
our model could successfully conduct reasoning
on the fully-connected graph with generated pa-
rameters from natural language. These results also
indicate that our model not only performs well
on sentence-level relation extraction but also im-
proves on bag-level relation extraction. Note that
Context-Aware RE also incorporates context in-
formation to predict the relation of the target en-
tity pair, however, we argue that Context-Aware
RE only models the co-occurrence of various re-
lations, ignoring whether the context relation par-
ticipates in the reasoning process of relation ex-
traction of the target entity pair. Context-Aware
RE may introduce more noise, for it may mis-
takenly increase the probability of a relation with
the similar topic with the context relations. We
will give samples to illustrate this issue in Sect.
5.5. Another interesting observation is that our
#layers=1 version outperforms CNN and PCNN
in these three datasets. One probable reason is
that sentences from Wikipedia are often complex,

https://github.com/UKPLab/emnlp2017-relation-extraction
https://github.com/UKPLab/emnlp2017-relation-extraction


1337

Dataset Distantly Labeled Test Set Dense Distantly Labeled Test Set
Metric P@5% P@10% P@15% P@20% P@5% P@10% P@15% P@20%
Multi-Window CNN 78.9 78.4 76.2 72.9 86.2 83.4 81.4 79.1
PCNN 73.0 65.4 58.1 51.2 85.3 79.1 72.4 68.1
Context-Aware RE 90.8 89.9 88.5 87.2 93.5 93.0 93.8 93.0
GP-GNN (#layers=1) 90.5 89.9 88.2 87.2 97.4 93.5 92.4 91.9
GP-GNN (#layers=2) 92.5 92.0 89.3 87.1 95.0 94.6 95.2 94.2
GP-GNN (#layers=3) 94.2 92.0 89.7 88.3 98.5 97.4 96.6 96.1

Table 3: Results on distantly labeled test set

which may be hard to model for CNN and PCNN.
Similar conclusions are also reached by Zhang and
Wang (2015).

0.00 0.05 0.10 0.15 0.20 0.25
Recall

0.80

0.82

0.84

0.86

0.88

0.90

0.92

0.94

0.96

Pr
ec

isi
on

Ours(#layers=3)
Ours(#layers=2)
Ours(#layers=1)
Context Aware RE

0.00 0.05 0.10 0.15 0.20 0.25
Recall

0.90

0.92

0.94

0.96

0.98

1.00

Pr
ec

isi
on

Ours(#layers=3)
Ours(#layers=2)
Ours(#layers=1)
Context Aware RE

Figure 3: The aggregated precision-recall curves of our
models with different number of layers on distantly la-
beled test set (left) and dense distantly labeled test set
(right). We also add Context Aware RE for comparison.

5.4 The Effectiveness of the Number of
Layers

The number of layers represents the reasoning
ability of our models. A K-layer version has the
ability to infer K-hop relations. To demonstrate
the effects of the number of layers, we also com-
pare our models with different numbers of lay-

ers. From Table 2 and Table 3, we could see
that on all three datasets, 3-layer version achieves
the best. We could also see from Fig. 3 that
as the number of layers grows, the curves get
higher and higher precision, indicating consider-
ing more hops in reasoning leads to better perfor-
mance. However, the improvement of the third
layer is much smaller on the overall distantly su-
pervised test set than the one on the dense subset.
This observation reveals that the reasoning mecha-
nism could help us identify relations especially on
sentences where there are more entities. We could
also see that on the human annotated test set 3-
layer version to have a greater improvement over
2-layer version as compared with 2-layer version
over 1-layer version. It is probably due to the rea-
son that bag-level relation extraction is much eas-
ier. In real applications, different variants could be
selected for different kind of sentences or we can
also ensemble the prediction from different mod-
els. We leave these explorations for future work.

5.5 Qualitative Results: Case Study

Tab. 4 shows qualitative results that compare our
GP-GNN model and the baseline models. The re-
sults show that GP-GNN has the ability to infer the
relationship between two entities with reasoning.
In the first case, GP-GNN implicitly learns a logic

rule ∃y, x ∼cast-member−−−−−−−−→ y original language−−−−−−−−−→ z ⇒
x

language spoken−−−−−−−−−→ z to derive (Oozham, language
spoken, Malayalam) and in the second case
our model implicitly learns another logic rule

∃y, x owned-by−−−−−→ y located in−−−−−→ z ⇒ x located in−−−−−→ z
to find the fact (BankUnited Center, located
in, English). Note that (BankUnited Center,
located in, English) is even not in Wikidata,
but our model could identify this fact through rea-
soning. We also find that Context-Aware RE tends
to predict relations with similar topics. For ex-
ample, in the third case, share border with
and located in are both relations about ter-



1338

The association was 
organized in Enterprise (now 
known as Redbush) 
Johnson County, 
Kentucky in 1894 and was 
incorporated in 1955, after 
relocating to Gallipolis, 
Ohio.

Sentence GP-GNNs (#layers = 3)LSTMContext AwareRelation Extraction

Oozham ( or Uzham ) is an 
upcoming 2016 Malayalam 
drama film written and 
directed by Jeethu Joseph 
with Prithviraj Sukumaran 
in the lead role.

Ground Truth

The third annual of the 2006 
Premios Juventud (Youth 
Awards) edition will be held 
on July 13, 2006 at the 
BankUnited Center from 
the University of Miami in 
Coral Gables, Florida .

Oozham

MalayalamJeethu Joseph

Prithviraj Sukumaran

cast member

director original language

language spoken
Oozham

MalayalamJeethu Joseph

Prithviraj Sukumaran

cast member

director original language

language spoken
Oozham

MalayalamJeethu Joseph

Prithviraj Sukumaran

cast member

director original language

Oozham

MalayalamJeethu Joseph

Prithviraj Sukumaran

cast member

director original language

BankUnited Center

University of Miami

Coral Gables, Florida

located in the admini-
strative territorial entity BankUnited Center

University of Miami

Coral Gables, Florida

located in the admini-
strative territorial entity BankUnited Center

University of Miami

Coral Gables, Florida

owned by

located in the admini-
strative territorial entityBankUnited Center

University of Miami

Coral Gables, Florida

owned by

located in the admini-
strative territorial entity

located in the admini-
strative territorial entity

Redbush

Johnson County

KentuckyOhio

located in the admini-
strative territorial entity

located in the admini-
strative territorial entity

Redbush

Johnson County

KentuckyOhio

located in the admini-
strative territorial entity

located in the admini-
strative territorial entity

Redbush

Johnson County

KentuckyOhio

located in the admini-
strative territorial entity

located in the admini-
strative territorial entity

Redbush

Johnson County

KentuckyOhio

located in the admini-
strative territorial entity

located in the admini-
strative territorial entity

share 
border with

Table 4: Sample predictions from the baseline models and our GP-GNN model. Ground truth graphs are the
subgraph in Wikidata knowledge graph induced by the sets of entities in the sentences. The models take sentences
and entity markers as input and produce a graph containing entities (colored and bold) and relations between them.
Although “No Relation” is also be seen as a type of relation, we only show other relation types in the graphs.

ritory issues. Consequently, Context-Aware RE
makes a mistake by predicting (Kentucky, share
boarder with, Ohio). As we have discussed
before, this is due to its mechanism to model co-
occurrence of multiple relations. However, in our
model, since Ohio and Johnson County have no
relationship, this wrong relation is not predicted.

6 Conclusion and Future Work

We addressed the problem of utilizing GNNs
to perform relational reasoning with natural lan-
guages. Our proposed model, GP-GNN, solves the
relational message-passing task by encoding natu-
ral language as parameters and performing propa-
gation from layer to layer. Our model can also be
considered as a more generic framework for graph
generation problem with unstructured input other
than text, e.g. image, video, audio. In this work,
we demonstrate its effectiveness in predicting the
relationship between entities in natural language
and bag-level and show that by considering more
hops in reasoning the performance of relation ex-
traction could be significantly improved.

Acknowledgement

The authors thank the members of Tsinghua NLP
lab10 for their thoughtful suggestions. This work

10 http://thunlp.org

is jointly supported by the NSFC project under the
grant No. 61661146007 and the NExT++ project,
the National Research Foundation, Prime Minis-
ters Office, Singapore under its IRC@Singapore
Funding Initiative. Hao Zhu is supported by Ts-
inghua Initiative Research Program.

References

Luis B Almeida. 1987. A learning rule for asyn-
chronous perceptrons with feedback in a combinato-
rial environment. In Proceedings, 1st First Interna-
tional Conference on Neural Networks, pages 609–
618. IEEE.

Fenia Christopoulou, Makoto Miwa, and Sophia Ana-
niadou. 2018. A walk-based model on entity graphs
for relation extraction. In Proceedings of ACL, vol-
ume 2, pages 81–88.

Nicola De Cao, Wilker Aziz, and Ivan Titov. 2018.
Question answering by reasoning across documents
with graph convolutional networks. arXiv preprint
arXiv:1808.09920.

Bhuwan Dhingra, Zhilin Yang, William W Cohen, and
Ruslan Salakhutdinov. 2017. Linguistic knowledge
as memory for recurrent neural networks. arXiv
preprint arXiv:1703.02620.

JVictor Garcia and Joan Bruna. 2018. Few-shot learn-
ing with graph neural networks. In Proceedings of
ICLR.

http://thunlp.org


1339

Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley,
Oriol Vinyals, and George E. Dahl. 2017. Neural
message passing for quantum chemistry. In Pro-
ceedings of ICML.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation, pages
1735–1780.

Daniel D Johnson. 2017. Learning graphical state tran-
sitions. In Proceedings of ICLR.

Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max
Welling, and Richard Zemel. 2018. Neural rela-
tional inference for interacting systems. In Proceed-
ings of ICML.

Thomas N Kipf and Max Welling. 2016. Semi-
supervised classification with graph convolutional
networks. Proceedings of ICLR.

Phong Le and Ivan Titov. 2018. Improving entity link-
ing by modeling latent relations between mentions.

Yujia Li, Daniel Tarlow, Marc Brockschmidt, and
Richard Zemel. 2016. Gated graph sequence neu-
ral networks. Proceedings of ICLR.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. In Proceed-
ings of ACL, pages 2124–2133.

Diego Marcheggiani and Ivan Titov. 2017. Encoding
sentences with graph convolutional networks for se-
mantic role labeling. In Proceedings EMNLP.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
structures. In Proceedings of ACL, pages 1105–
1116.

Thien Huu Nguyen and Ralph Grishman. 2015. Rela-
tion extraction: Perspective from convolutional neu-
ral networks. In Proceedings of the 1st Workshop on
Vector Space Modeling for Natural Language Pro-
cessing, pages 39–48.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in pytorch.

Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina
Toutanova, and Wen-tau Yih. 2017. Cross-sentence
n-ary relation extraction with graph lstms. Transac-
tions of the Association for Computational Linguis-
tics, pages 101–115.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of EMNLP, pages
1532–1543.

Adam Santoro, David Raposo, David G Barrett, Ma-
teusz Malinowski, Razvan Pascanu, Peter Battaglia,
and Tim Lillicrap. 2017. A simple neural network
module for relational reasoning. In Proceedings of
NIPS, pages 4967–4976.

Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus
Hagenbuchner, and Gabriele Monfardini. 2009. The
graph neural network model. IEEE Transactions on
Neural Networks, pages 61–80.

Michael Schlichtkrull, Thomas N Kipf, Peter Bloem,
Rianne van den Berg, Ivan Titov, and Max Welling.
2017. Modeling relational data with graph convolu-
tional networks. arXiv preprint arXiv:1703.06103.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, pages 2673–2681.

Daniil Sorokin and Iryna Gurevych. 2017. Context-
aware representations for knowledge base relation
extraction. In Proceedings of EMNLP, pages 1784–
1789.

Denny Vrandečić and Markus Krötzsch. 2014. Wiki-
data: a free collaborative knowledgebase. Commu-
nications of the ACM.

Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-
Fei. 2017. Scene graph generation by iterative mes-
sage passing. In Proceedings of CVPR, volume 2.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
piecewise convolutional neural networks. In Pro-
ceedings of EMNLP, pages 1753–1762.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via con-
volutional deep neural network. In Proceedings of
COLING, pages 2335–2344.

Wenyuan Zeng, Yankai Lin, Zhiyuan Liu, and
Maosong Sun. 2017. Incorporating relation paths
in neural relation extraction. In Proceedings of
EMNLP.

Dongxu Zhang and Dong Wang. 2015. Relation classi-
fication via recurrent neural network. arXiv preprint
arXiv:1508.01006.

Yuhao Zhang, Peng Qi, and Christopher D. Manning.
2018. Graph convolution over pruned dependency
trees improves relation extraction. In Proceedings
of EMNLP.

http://arxiv.org/abs/arXiv:1804.10637
http://arxiv.org/abs/arXiv:1804.10637
https://nlp.stanford.edu/pubs/zhang2018graph.pdf
https://nlp.stanford.edu/pubs/zhang2018graph.pdf

