










































Linguistically Aware Coreference Evaluation Metrics


International Joint Conference on Natural Language Processing, pages 1366–1374,
Nagoya, Japan, 14-18 October 2013.

Linguistically Aware Coreference Evaluation Metrics

Chen Chen and Vincent Ng
Human Language Technology Research Institute

University of Texas at Dallas
Richardson, TX 75083-0688

{yzcchen,vince}@hlt.utdallas.edu

Abstract

Virtually all the commonly-used evalua-
tion metrics for entity coreference reso-
lution are linguistically agnostic, treating
the mentions to be clustered as generic
rather than linguistic objects. We argue
that the performance of an entity coref-
erence resolver cannot be accurately re-
flected when it is evaluated using linguis-
tically agnostic metrics. Consequently,
we propose a framework for incorporating
linguistic awareness into commonly-used
coreference evaluation metrics.

1 Introduction

Coreference resolution is the task of determin-
ing which mentions in a text or dialogue refer
to the same real-world entity. Designing appro-
priate evaluation metrics for coreference resolu-
tion is an important and challenging task. Since
there is no consensus on which existing corefer-
ence evaluation metric is the best, the organizers of
the CoNLL-2011 and CoNLL-2012 shared tasks
on unrestricted coreference (Pradhan et al., 2011,
2012) decided to take the average of the scores
computed by three coreference evaluation metrics,
MUC (Vilain et al., 1995), B3 (Bagga and Bald-
win, 1998), and CEAFe (Luo, 2005), as the official
score of a participating coreference resolver.

One weakness shared by virtually all exist-
ing coreference evaluation metrics is that they
are linguistically agnostic, treating the mentions
to be clustered as generic rather than linguistic
objects. In other words, while MUC, B3, and
CEAF were designed for evaluating coreference
resolvers, their linguistic agnosticity implies that
they can be used to evaluate any clustering task,
including those that are not linguistic in nature.1

1This statement is also true for BLANC (Recasens and
Hovy, 2011), a Rand Index-based coreference evaluation
metric we will not focus on in this paper.

To understand why linguistic agnosticity is a
potential weakness of existing scoring metrics,
consider a document in which there are three
coreferent mentions, Hillary Clinton, she, and she,
appearing in this order in the document. Assume
that two coreference resolvers, R1 and R2, are
applied to these three mentions, where R1 only
posits Hillary Clinton and she as coreferent, and
R2 only posits the two occurrences of she as coref-
erent. Being linguistically agnostic, existing scor-
ing metrics will assign the same score to both re-
solvers after seeing that both of them correctly as-
sign two of the three objects to the same cluster.
Intuitively, however, R1 should receive a higher
score than R2: R1 has facilitated automated text
understanding by successfully finding the referent
of one of the pronouns, whereas from R2’s output
we know nothing about the referent of the two pro-
nouns. Failure to rank R1 higher than R2 implies
that existing scoring metrics fail to adequately re-
flect the performance of a resolver.2

Our goal in this paper is to address the afore-
mentioned weakness by proposing a framework
for incorporating linguistic awareness into the
most commonly-used coreference scoring metrics,
including MUC, B3, and CEAF. Rather than mak-
ing different modifications to different metrics,
one of the contributions of our work lies in the
proposal of a unified framework that enables us to
employ the same set of modifications to create lin-
guistically aware versions of all these metrics.

2 Existing Evaluation Metrics

In this section, we review four scoring metrics,
MUC, B3, and the two versions of CEAF, namely,

2One may disagree that R1 should be ranked higher than
R2 by arguing that successful identification of two corefer-
ential pronouns is not necessarily easier than resolving an
anaphoric pronoun to a non-pronominal antecedent. Our ar-
gument, however, is based on the view traditionally adopted
in pronoun resolution research that resolving an anaphoric
pronoun entails finding a non-pronominal antecedent for it.

1366



CEAFm and CEAFe. As F-score is always com-
puted as the unweighted harmonic mean of recall
and precision, we will only show how recall and
precision are computed. Note that unlike previous
discussion of these metrics, we present them in a
way that reveals their common elements.

2.1 Notation and Terminology

In the rest of this paper, we use the terms corefer-
ence chains and coreference clusters interchange-
ably. For a coreference chain C , we define |C|
as the number of mentions in C . Key chains
and system chains refer to gold coreference chains
and system-generated coreference chains, respec-
tively. In addition, K(d) and S(d) refer to the
set of gold chains and the set of system-generated
chains in document d, respectively. Specifically,

K(d) = {Ki : i = 1, 2, · · · , |K(d)|},

S(d) = {Sj : j = 1, 2, · · · , |S(d)|},

where Ki is a chain in K(d) and Sj is a chain in
S(d). |K(d)| and |S(d)| are the number of chains
in K(d) and S(d), respectively.

2.2 MUC (Vilain et al., 1995)

MUC is a link-based metric. Given a document d,
recall is computed as the number of common links
between the key chains and the system chains in d
divided by the number of links in the key chains.
Precision is computed as the number of common
links divided by the number of links in the system
chains. Below we show how to compute (1) the
number of common links, (2) the number of key
links, and (3) the number of system links.

To compute the number of common links, a par-
tition P (Sj) is created for each system chain Sj
using the key chains. Specifically,

P (Sj) = {C
i
j : i = 1, 2, · · · , |K(d)|} (1)

Each subset Cij in P (Sj) is formed by intersect-
ing Sj with Ki. Note that |Cij | = 0 if Sj and
Ki have no mentions in common. Since there are
|K(d)|∗|S(d)| subsets in total, the number of com-
mon links is

c(K(d),S(d)) =

|S(d)|
∑

j=1

|K(d)|
∑

i=1

wc(C
i
j),

where wc(C
i
j) =

{

0 if |Cij| = 0;
|Cij| − 1 if |C

i
j| > 0.

(2)

Intuitively, wc(Cij) can be interpreted as the
“weight” of Cij . In MUC, the weight of a cluster is
defined as the minimum number of links needed
to create the cluster, so wc(Cij) = |C

i
j| − 1 if

|Cij| > 0.
The number of links in the key chains, K(d), is

calculated as:

k(K(d)) =

|K(d)|
∑

i=1

wk(Ki), (3)

where wk(Ki) = |Ki|−1. The number of links in
the system chains, s(S(d)), is calculated as:

s(S(d)) =

|S(d)|
∑

j=1

ws(Sj), (4)

where ws(Sj) = |Sj | − 1.

2.3 B3 (Bagga and Baldwin, 1998)

One of MUC’s shortcoming is that it fails to re-
ward successful identification of singleton clus-
ters. To address this weakness, B3 first computes
the recall and precision for each mention, and then
averages these per-mention values to obtain the
overall recall and precision.

Let mn be the nth mention in document d. Its
recall, R(mn), and precision, P (mn), are com-
puted as follows. Let Ki and Sj be the key chain
and the system chain that contain mn, respec-
tively, and let Cij be the set of mentions appearing
in both Sj and Ki.

R(mn) =
wc(C

i
j)

wk(Ki)
, P (mn) =

wc(C
i
j)

ws(Sj)
, (5)

where wc(Cij) = |C
i
j|, wk(Ki) = |Ki|, and

ws(Sj) = |Sj|.

2.4 CEAF (Luo, 2005)

While B3 addresses the shortcoming of MUC, Luo
presents counter-intuitive results produced by B3,
which it attributes to the fact that B3 may use
a key/system chain more than once when com-
puting recall and precision. To ensure that each
key/system chain will be used at most once in the
scoring process, his CEAF scoring metric scores
a coreference partition by finding an optimal one-
to-one mapping (or alignment) between the chains
in K(d) and those in S(d).

Since the mapping is one-to-one, not all key
chains and system chains will be involved in it. Let

1367



Kmin(d) and Smin(d) be the set of key chains and
the set of system chains involved in the alignment,
respectively. The alignment can be represented as
a one-to-one mapping function g, where

g(Ki) = Sj,Ki ∈ Kmin(d) and Sj ∈ Smin(d).

The score of g, Φ(g), is defined as

Φ(g) =
∑

Ki∈Kmin(D)

φ(Ki, g(Ki)),

where φ is a function that computes the similar-
ity between a gold chain and a system chain. The
optimal alignment, g∗, is the alignment whose Φ
value is the largest among all possible alignments,
and can be computed efficiently using the Kuhn-
Munkres algorithm (Kuhn, 1955).

Given g∗, the recall (R) and precision (P) of a
system partition can be computed as follows:

R =
Φ(g∗)

∑|K(d))|
i=1 φ(Ki,Ki)

, P =
Φ(g∗)

∑|S(d))|
j=1 φ(Sj , Sj)

.

As we can see, at the core of CEAF is the simi-
larity function φ. Luo defines two different φ func-
tions, φ3 and φ4:

φ3(Ki, Sj) = |Ki ∩ Sj| = wc(C
i
j) (6)

φ4(Ki, Sj) =
2|Ki ∩ Sj|

|Ki|+ |Sj|
=

2 ∗ wc(C
i
j)

wk(Ki) + ws(Sj)
(7)

φ3 and φ4 result in mention-based CEAF (a.k.a.
CEAFm) and entity-based CEAF (a.k.a. CEAFe),
respectively.

2.5 Common functions

Recall that the three weight functions, wc, wk, and
ws, are involved in all the scoring metrics we have
discussed so far. To summarize:

• wc(C
i
j) is the weight of the common subset

between Ki and Sj . For MUC, its value is 0
if Cij is empty and |C

i
j |−1 otherwise; for B

3,
CEAFm and CEAFe, its value is |Cij|.

• wk(Ki) is the weight of key chain Ki. For
MUC, its value is |Ki| − 1, while for B3,
CEAFm and CEAFe, its value is |Ki|.

• ws(Sj) is the weight of system chain Sj . For
MUC, its value is |Sj | − 1, while for B3,
CEAFm and CEAFe, its value is |Sj|.

Next, we will show that simply by redefin-
ing these three functions appropriately, we can
create linguistically aware versions of MUC, B3,
CEAFm, and CEAFe.3 For convenience, we will
refer to their linguistically aware counterparts as
LMUC, LB3, LCEAFm, and LCEAFe.4

3 Incorporating Linguistic Awareness

As mentioned in the introduction, one of the con-
tributions of our work lies in identifying the three
weight functions that are common to MUC, B3,
CEAFm, and CEAFe (see Section 2.5). To see
why these weight functions are important, note
that any interaction between a scoring metric and
a coreference chain is mediated by one of these
weight functions. In other words, if these weight
functions are linguistically agnostic (i.e., they treat
the mentions as generic rather than linguistic ob-
jects when assigning weights), the scoring metric
that employs them will be linguistically agnostic.
On the other hand, if these weight functions are
linguistically aware, the scoring metric that em-
ploys them will be linguistically aware.

This observation makes it possible for us to de-
sign a unified framework for incorporating lin-
guistic awareness into existing coreference scor-
ing metrics. Specifically, rather than making dif-
ferent modifications to different scoring metrics to
incorporate linguistic awareness, we can simply
incorporate linguistic awareness into these three
weight functions. So when they are being used
in different scoring metrics, we can handily obtain
the linguistically aware versions of these metrics.

In the rest of this section, we will suggest one
way of implementing linguistic awareness. This is
by no means the only way to implement linguis-
tic awareness, but we believe that this is a good
starting point, which hopefully will initiate further
discussions in the coreference community.

3.1 Formalizing Linguistic Awareness

Other than illustrating the notion of linguistic
awareness via a simple example in the introduc-
tion, we have thus far been vague about what ex-

3Note that for a given scoring metric, wc(C) = wk(C) =
ws(C) for any non-empty chain C. The reason why we de-
fine three weight functions as opposed to one is that they are
defined differently in the linguistically aware scoring metrics,
as we will see.

4Our implementation of the linguistically aware eval-
uation metrics is available from http://www.hlt.
utdallas.edu/˜yzcchen/coreference.

1368



actly it is. In this section, we will make this notion
more concrete.

Recall that the goal of (co)reference resolution
is to facilitate automated text understanding by
finding the referent for each referring expression
in a text. Hence, when resolving a mention, a
resolver should be rewarded more if the selected
antecedent allows the underlying entity to be in-
ferred than if it doesn’t, because the former con-
tributes more to understanding the corresponding
text than the latter. Note that the more informative
the selected antecedent is, the easier it will be for
the reader to infer the underlying entity. Here, we
adopt a simple notion of linguistic informativeness
based on the mention type: a name is more infor-
mative than a nominal, which in turn is more infor-
mative than a pronoun.5 Hence, a coreference link
involving a name should be given a higher weight
than one that doesn’t, and a coreference link in-
volving a nominal should be given a higher weight
than one that involves only pronouns.

We implement this observation by assigning to
each link el a weight of wl(el), where wl(el) is
defined using the first rule applicable to el below:
Rule 1: If el involves a name, wl(el) = wnam.
Rule 2: If el involves a nominal, wl(el) = wnom.
Rule 3: wl(el) = wpro.

There is a caveat, however. By assigning
weights to coreference links rather than mentions,
we will be unable to reward successful identi-
fication of singleton clusters, since they contain
no links (and hence they carry no weights). To
address this problem, we introduce a singleton
weight wsing, which will be assigned to any chain
that contains exactly one mention.

So far, we have introduced four weights, W =
(wnam, wnom, wpro, wsing), which encode our
(somewhat simplistic) notion of linguistic aware-
ness. Below we show how these four weights are
incorporated into the three weight functions, wc,
wk, and and ws, to create their linguistically aware
counterparts, wLc , w

L
k , and w

L
s .

3.2 Defining wLc
Recall that Cij represents the set of mentions com-
mon to key chain Ki and system chain Sj . To
define the linguistically aware weight function
wLc (C

i
j), there are three cases to consider:

5Different notions of linguistic informativeness might be
appropriate for different natural language applications. In our
framework, a different notion of linguistic informativeness
can be implemented simply by altering the weight functions.

Case 1: |Cij | ≥ 2
Recall that the linguistically agnostic wc function
returns a weight of |Cij| − 1. This makes sense,
because in a linguistically agnostic situation, all
the links have the same weight, and hence the
weight assigned to Cij will be the same regardless
of which |Cij | − 1 links in C

i
j are chosen. How-

ever, the same is no longer true in a linguistically
aware setting: since the links may not necessar-
ily have the same weight, the weight assigned to
Cij depends on which |C

i
j | − 1 links are chosen.

In this case, it makes sense for our linguistically
aware wLc function to find the |C

i
j | − 1 links that

have the largest weights and assign to wLc the sum
of these weights, since they reflect how well a re-
solver managed to find informative antecedents for
the mentions. Note that the sum of the |Cij | − 1
links that have the largest weights is equal the
weight of the maximum spanning tree defined over
the mentions in Cij .

Case 2: |Cij | = 0
In this case Cij is empty, meaning that Ki and Sj
do not have any mention in common. wLc simply
returns a weight of 0 when applied to Cij .

Case 3: |Cij | = 1
In this case, Ki and Sj have one mention in com-
mon. The question, then, is: can we simply re-
turn wsing, the weight associated with a single-
ton cluster? The answer is no: since wsing was
created to reward successful identification of sin-
gleton clusters, a resolver should be rewarded by
wsing only if it correctly identifies a singleton clus-
ter. In other words, wLc returns wsing if all of C

i
j ,

Ki and Sj contain exactly one mention (which im-
plies that the singleton cluster Cij is correctly iden-
tified); otherwise, wLc returns 0.

The definition of wLc is summarized as follows,
where E is the set of edges in the maximum span-
ning tree defined over the mentions in Cij .

wLc (C
i
j) =











∑

el∈E
wl(el) if |Cij| > 1;

wsing if |Cij|, |Ki|, |Sj | = 1;
0 otherwise.

(8)

3.3 Defining wLk

Recall that wLk aims to compute the weight of key
chain Ki. Given the definition of wLc , in order to
ensure that the maximum recall is 1, it is natural to
define wLK as follows, where E is the set of edges

1369



appearing in the maximum spanning tree defined
over the mentions in Ki.

wLk (Ki) =

{
∑

el∈E
wl(el) if |Ki| > 1;

wsing if |Ki| = 1.
(9)

3.4 Defining wLs
Finally, we define wLs , the function for computing
the weight of system chain Sj . To better under-
stand how we might want to define wLs , recall that
in MUC, B3, and both versions of CEAF, precision
and recall play a symmetric role. In other words,
precision is computed by reversing the roles of the
key partition K(d) and the system partition S(d)
used to compute recall for document d. If we
wanted precision and recall to also play a symmet-
ric role in the linguistically aware versions of these
scoring metrics, it would be natural to define wLs in
the same way as wLk , where E is the set of edges
appearing in the maximum spanning tree defined
over the mentions in Sj .

wLs (Sj) =

{
∑

el∈E
wl(el) if |Sj | > 1;

wsing if |Sj | = 1.
(10)

However, there is a reason why it is undesirable
for us to define wLs in this manner. Consider the
special case in which a system partition S(d) con-
tains only correct links, some of which are subop-
timal.6 Although S(d) contains only correct links,
the precision computed by any scoring metric that
employs wLs with the above definition will be less
than one simply because it contains suboptimal
links. In other words, if a scoring metric employs
wLs with the above definition, it will penalize a re-
solver for choosing suboptimal links twice, once
in recall and once in precision.

To avoid penalizing a resolver for the same mis-
take twice, wLs cannot be defined in the same way
as wLk .

7 In particular, only spurious links (i.e.,
links between two non-coreferent mentions), not
suboptimal links, should be counted as precision
errors. To avoid this problem, recall that P (Sj) is
defined as a partition of system chain Sj created
by intersecting Sj with all key chains in K(d).

P (Sj) = {C
i
j : i = 1, 2, · · · , |K(d)|}

6Suboptimal links are links that are correct but do not ap-
pear in a maximum spanning tree for any of its chains.

7This implies that precision and recall will no longer play
a symmetric role in our linguistically aware scoring metrics.

Note that a link is spurious if it links a men-
tion in Ci1j with a mention in C

i2
j , where 1 ≤ i1 6=

i2 ≤ K(d). Without loss of generality, assume that
there are nej non-empty clusters in P (Sj). Note
that we need nej−1 spurious links in order to con-
nect the nej non-empty clusters. To adequately
reflect the damage created by these spurious links,
among the different sets of nej−1 spurious links
that connect the nej non-empty clusters in P (Sj),
we choose the set where the sum of the weights of
the links is the largest and count the edges in it as
precision errors. We denote this set as Et(Sj).

Now we are ready to define wLs . There are two
cases to consider.
Case 1: |Sj | > 1
In this case, wLs (Sj) is computed as follows:

wLs (Sj) =
∑

Cij∈P (Sj)

wLc (C
i
j) +

∑

e∈Et(Sj)

wl(e).

(11)
Note that the second term corresponds to the pre-
cision errors discussed in the previous paragraph,
whereas the first term corresponds to the sum of
the values returned by wLc when applied to each
cluster in P (Sj). The first term guarantees that a
resolver is penalized for precision errors because
of spurious links, not suboptimal links.
Case 2: |Sj | = 1
In this case, Sj only contains one mention. We set
wLs (Sj) to wsing.

4 Evaluation

In this section, we design experiments to better un-
derstand our linguistically aware metrics (hence-
forth LMetrics). Specifically, our evaluation is
driven by two questions. First, given that the
LMetrics are parameterized by a vector of four
weights W , how do their behaviors change as we
alter W ? Second, how do the LMetrics differ from
the existing metrics (henceforth OMetrics)?

4.1 Experimental Setup

We use as our running example the paragraph
shown in Figure 1, which is adapted from the Bible
domain of the English portion of the OntoNotes
v5.0 corpus. There are 19 mentions in the para-
graph, each of which is enclosed in parentheses
and annotated as myx, where y is the ID of the
chain to which this mention belongs, and x is the
mention ID.

Figure 2 shows five system responses (a–e) for
our running example along with the key chains.

1370



(Jesus)1a came near (Jerusalem)
2

d. Looking at (the city)
2

e , (he)
1

b began to cry for (it)
2

f and said, (I)
1

c wish (you)
2

g knew what
would bring (you)2h (peace)

4

p. But it is hidden from (you)
2

i (now)
5

q . (A time)
6

r is coming when ((your)
2

j enemies)
3

n will hold
(you)2k in on (all sides)

7

s . (They)
3

o will destroy (you)
2

l and (all (your)
2

m people)
8

t .

Figure 1: A paragraph adapted from the Bible domain of the OntoNotes 5.0 corpus.

For conciseness, a mention is denoted by its men-
tion ID, and each connected sub-graph forms one
coreference chain. Moreover, the type of a men-
tion is denoted by its shape: a square denotes a
NAME mention; a triangle denotes a NOMINAL
mention, and a circle denotes a PRONOUN men-
tion. Note that Syou, the set of coreferent “you”
mentions consisting of {m2g,m

2
h, · · · ,m

2
m}, ap-

pears in all system responses.

Figure 2: Key and system coreference chains.

Let us begin by describing the five system re-
sponses. Response (a) is produced by a simple
and conservative resolver. Besides forming Syou,
this resolver also correctly links m1b with m

1
c . Re-

sponses (b), (c) and (d) each improves upon re-
sponse (a) by linking Syou to one of three pre-
ceding mentions, namely, one PRONOUN mention,
one NOMINAL mention, and one NAME mention
respectively. Response (e) is produced by an ag-
gressive resolver that tries to resolve all the pro-

nouns to a non-pronominal antecedent, but unfor-
tunately, it wrongly connects Syou to m1a, m

1
b and

m1c .
Next, we investigate the two questions posed

at the beginning of Section 4.1. To determine
how the LMetrics behave when used in com-
bination with different weight vectors W =
(wnam, wnom, wpro, wsing), we experiment with:

W1 = (1.0, 1.0, 1.0, 10
−20 );8

W2 = (1.0, 1.0, 1.0, 0.5);
W3 = (1.0, 1.0, 1.0, 1.0);
W4 = (1.0, 0.75, 0.5, 1.0);
W5 = (1.0, 0.5, 0.25, 1.0).

Note that W1, W2, and W3 differ only with respect
to wsing, so comparing the results obtained us-
ing these weight vectors will reveal the impact of
wsing on the LMetrics. On the other hand, W4 and
W5 differ with respect to the gap of the weights
associated with the three types of mentions. Ex-
amining the LMetrics when they are used in com-
bination with W4 and W5 will reveal the differ-
ence between having “relatively similar” weights
versus having “relatively different” weights on the
three mention types.

Figure 3 shows four graphs, one for each of the
four LMetrics. Each graph contains six curves,
five of which correspond to curves generated by
using the aforementioned five weight vectors, and
the remaining one corresponds to the OMetric
curve that we include for comparison purposes.
Each curve is plotted using five points that corre-
spond to the five system responses.

4.2 Impact of wsing

We first investigate the impact of wsing. We will
determine how the LMetrics behave in response to
W1, W2 and W3.

The first graph in Figure 3 shows the LMUC
and MUC F-scores. As we can see, the scores of
MUC and LMUC(W1) are almost the same. This
is understandable: the uniform edge weights and
a very small wsing in W1 imply that LMUC will

8We set wsing to a very small value other than 0, because
setting wsing to 0 may cause the denominator of the expres-
sions in (5) and (7) to be 0.

1371



 40

 50

 60

 70

 80

 90

 100

(a) (b) (c) (d) (e)

F%

System Response

LMUC

MUC,W1
W2
W3

W4
W5

 40

 50

 60

 70

 80

 90

 100

(a) (b) (c) (d) (e)

F%

System Response

LB3

B3
W1,2,3

W4
W5

 40

 50

 60

 70

 80

 90

 100

(a) (b) (c) (d) (e)

F%

System Response

LCEAFm

CEAFm
W1
W2

W3
W4
W5

 40

 50

 60

 70

 80

 90

 100

(a) (b) (c) (d) (e)

F%

System Response

LCEAFe

CEAFe
W1,2,3

W4
W5

Figure 3: Comparison of the LMetrics scores under different weight settings and the OMetrics scores.

essentially ignore correct identification of single
clusters and consider all errors to be equal, just
like MUC. When we replace W1 with W2 and
W3, the two weight vectors with a larger wsing
value, and rescore the five responses, we see that
the LMUC scores for responses (a), (b), (c) and
(d) decrease. This is because LMUC uses wsing
to penalize these four responses for identifying
wrong singleton clusters. On the other hand, the
LMUC score for response (e) is higher than the
corresponding MUC score, because LMUC addi-
tionally rewards response (e) for correctly classi-
fying all singleton clusters without introducing er-
roneous singleton clusters.

The second graph in Figure 3 shows the LB3

and B3 F-scores. Here, we see that the scores
for LB3(W1), LB3(W2) and LB3(W3) are iden-
tical. These results suggest that the value of wsing
does not affect the LB3 score, despite the fact
that LB3 does take into account singleton clusters
when scoring, a property that it inherits from B3.
The reason is that regardless of what wsing is, if
a mention m is correctly classified as a singleton
mention, both of R(m) and P (m) will be 1, other-
wise, both will be 0 (see formula (5)). Note, how-
ever, that there is a difference between LB3 and
B3: for an erroneously identified singleton cluster
containing mention m, LB3 sets P (m) to 0 while
B3 sets P (m) to 1. In other words, LB3 puts a
higher penalty on precision given erroneous sin-
gleton clusters. This difference causes LB3 and B3

to evaluate responses (a) and (e) differently. Recall
that responses (a) and (e) are quite different: re-
sponse (e) correctly finds informative antecedents
for m1b , m

1
c , m

2
e, m

2
f and m

3
o, whereas response (a)

contains many erroneous singleton clusters. De-
spite the large differences in these responses, B3

only gives 0.7% more points to response (e) than
response (a). On the other hand, LB3 assigns a
much lower score to response (a) owing to the nu-
merous erroneous singleton clusters it contains.

The third graph of Figure 3 shows the LCEAFm
and CEAFm F-scores. Since LCEAFm uses both
singleton and non-singleton clusters when com-
puting the optimal alignment, it should not be
surprising that as we increase wsing, the sin-
gleton clusters will play a more important role
in the LCEAFm score. Consider, for example,
LCEAFm(W1). Since wsing = 0, LCEAFm(W1)
ignores the correct identification of singleton clus-
ters. From the graph, we see that LCEAFm(W1)
gives a higher score to response (a) than response
(e). This is understandable: response (a) is not
penalized for the many erroneous singleton clus-
ters it contains; on the other hand, response (e)
is penalized for the erroneous coreference links
it introduces. Now, consider LCEAF(W3), where
wsing = 1. Here, response (e) is assigned a higher
score by LCEAF(W3) than response (a): response
(a) is heavily penalized because of the many erro-
neous clusters it contains.

The rightmost graph of Figure 3 shows the
LCEAFe and CEAFe F-scores. Like LB3,
LCEAFe returns the same score when it is used
in combination with W1, W2 and W3, because the
φ4 similarity function returns 0 or 1 when the key
cluster or the system cluster it is applied to is a
singleton cluster, regardless of the value of wsing.
In addition, we can see that LCEAFe penalizes er-
roneous singleton clusters more than CEAFe does

1372



ch- MUC LMUC B3 LB3 CEAFm LCEAFm CEAFe LCEAFe
ains R P F R P F R P F R P F R P F R P F R P F R P F
(a) 58.3 100 73.7 50.7 58.6 54.4 64.3 100 78.3 39.2 70.0 50.2 75.0 75.0 75.0 50.7 58.6 54.4 91.1 56.1 69.4 73.8 45.4 56.2
(b) 66.7 100 80.0 53.7 64.3 58.5 71.3 100 83.3 43.1 75.0 54.7 80.0 80.0 80.0 53.7 64.3 58.5 91.9 61.3 73.6 74.5 49.7 59.6
(c) 66.7 100 80.0 64.2 68.3 66.2 71.3 100 83.3 50.8 75.0 60.6 80.0 80.0 80.0 64.2 68.3 66.2 91.9 61.3 73.6 76.7 51.1 61.4
(d) 66.7 100 80.0 74.6 71.4 73.0 71.3 100 83.3 58.6 75.0 65.8 80.0 80.0 80.0 74.6 71.4 73.0 91.9 61.3 73.6 78.4 52.3 62.8
(e) 91.7 91.7 91.7 76.1 92.7 83.6 79.0 79.0 79.0 65.0 72.5 68.5 70.0 70.0 70.0 58.2 70.9 63.9 86.5 86.5 86.5 85.8 85.8 85.8

Table 1: Comparison of the LMetrics(W4) scores and the OMetrics scores.

for the same reason that LB3 penalizes erroneous
singleton clusters more than B3 does.

In sum, the value of wsing does not impact LB3

and LCEAFe. On the other hand, LMUC and
LCEAFm pay more attention to singleton clusters
as wsing increases.

4.3 Impact of wnam, wnom and wpro

When we were analyzing the LMetrics in the pre-
vious subsection, by setting wnam, wnom, and
wpro to the same value, we were not exploiting
their capability to be linguistically aware. In this
subsection, we investigate the impact of linguis-
tic awareness using W4 and W5, which employ
different values for the three weights.9 To better
understand the differences in recall and precision
scores for each of the five system responses, we
show these scores as computed by the LMetrics
when they are used in combination with W4.

First, consider response (a). As we can see from
Figure 3 and the first row of Table 1, the OMetrics
give decent scores to this output. Linguistically
speaking, however, the system should be penal-
ized more. The reason is that its output contributes
little to understanding the document: in response
(a), only the links between the PRONOUN men-
tions are established, and none of the PRONOUN
or NOMINAL mentions is linked to a more infor-
mative mention that would enable the underlying
entity to be inferred.

As expected, LMetrics(W4) and LMetrics(W5)
assign much lower scores to response (a) than the
OMetrics, owing to a relatively small value of
wpro. Also, we see that the LMetrics(W5) scores
are even lower than the LMetrics(W4) scores. This
suggests that the smaller the values of wpro and
wnom are, the more heavily a resolver will be pe-
nalized for its failure to link a mention to a more
informative coreferent mention.

Next, consider responses (b), (c) and (d). As the

9Like W3, we set wsing to 1 in W4 and W5, because this
assignment makes CEAFm(W3) rank response (e) above re-
sponse (a), which we think is reasonable.

OMetrics ignore the type of mentions while scor-
ing, they are unable to distinguish the differences
among these three system responses: the OMet-
rics results in Figure 3 and their results in rows 2, 3
and 4 of Table 1 show that the scores for responses
(b), (c) and (d) are identical. Linguistically speak-
ing, however, they should not be. Response (d)
contributes the most to document understanding,
because the presence of NAME mention m2d in its
output enables one to infer the entity (Jerusalem)
to which the mentions in Syou refer. In contrast,
although response (b) correctly links Syou to PRO-
NOUN mention m2f , one cannot infer the entity to
which the mentions in Syou refer. The contribution
of response (c) is in-between, because via m2e, we
at least know that the mentions in Syou point to
one city, although we do not know which city it
is. Such differences in responses (b), (c) and (d)
are captured by LMetrics(W4) and LMetrics(W5).
Specifically, the LMetrics scores for response (d)
are higher than those for response (c), which in
turn are higher than those for response (b).

It is worth noting that the performance gaps be-
tween responses (b) and (c) and between responses
(c) and (d) are larger under LMetrics(W5) than
under LMetrics(W4). This is because wnom and
wpro in W5 are comparatively smaller. These re-
sults enable us to conclude that as the difference
in the three edge weights becomes larger, the per-
formance gap between a less informative resolver
and a more informative resolver according to the
LMetrics widens.

5 Conclusion

We addressed the problem of linguistic agnos-
ticity in existing coreference evaluation metrics
by proposing a framework that enables linguistic
awareness to be incorporated into these metrics.
While our experiments were performed on gold
mentions, it is important to note that our linguisti-
cally aware metrics can be readily combined with,
for example, Cai and Strube’s (2010) method, so
that they can be applied to system mentions.

1373



Acknowledgments

We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier
draft of the paper. This work was supported in
part by NSF Grants IIS-1147644 and IIS-1219142.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views or of-
ficial policies, either expressed or implied, of NSF.

References

Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
the LREC Workshop on Linguistic Coreference, page
563–566.

Jie Cai and Michael Strube. 2010. Evaluation metrics
for end-to-end coreference resolution systems. In
Proceedings of the 11th Annual SIGDIAL meeting
on Discourse and Dialogue, pages 28–36.

Harold W. Kuhn. 1955. The Hungarian method for
the assignment problem. Naval Research Logistics
Quarterly, 2:83–97.

Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 25–32.

Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 shared task: Modeling
unrestricted coreference in OntoNotes. In Proceed-
ings of the Fifteenth Conference on Computational
Natural Language Learning: Shared Task, pages 1–
27.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
of 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning: Shared Task, pages 1–
40.

Marta Recasens and Eduard Hovy. 2011. BLANC:
Implementing the Rand Index for coreference reso-
lution. Natural Language Engineering, 17(4):485–
510.

Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the Sixth Message Understanding Confer-
ence, pages 45–52.

1374


