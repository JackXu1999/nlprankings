



















































Finding Arguing Expressions of Divergent Viewpoints in Online Debates


Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM) @ EACL 2014, pages 35–43,
Gothenburg, Sweden, April 26-30 2014. c©2014 Association for Computational Linguistics

Finding Arguing Expressions of Divergent Viewpoints in Online Debates

Amine Trabelsi
Department of Computing Science

University of Alberta
atrabels@ualberta.ca

Osmar R. Zaı̈ane
Department of Computing Science

University of Alberta
zaiane@ualberta.ca

Abstract

This work suggests a fine-grained min-
ing of contentious documents, specifically
online debates, towards a summarization
of contention issues. We propose a Joint
Topic Viewpoint model (JTV) for the un-
supervised identification and the cluster-
ing of arguing expressions according to
the latent topics they discuss and the im-
plicit viewpoints they voice. A set of ex-
periments is conducted on online debates
documents. Qualitative and quantitative
evaluations of the model’s output are per-
formed in context of different contention
issues. Analysis of experimental results
shows the effectiveness of the proposed
model to automatically and accurately de-
tect recurrent patterns of arguing expres-
sions in online debate texts.

1 Introduction

This paper addresses the issue of improving the
quality of opinion mining from online contentious
texts like the posts in debate sites. Mining and
summarizing these new resources is crucial, es-
pecially when the opinion is related to a subject
that stimulates divergent viewpoints within peo-
ple (e.g. Healthcare Reform, Same-Sex Marriage).
We refer to such subjects as issues of contentions.
A contentious issue is “likely to cause disagree-
ment between people” (cf. Oxford Dictionaries).
Documents such as debate sites’ posts may contain
multiple contrastive viewpoints regarding a partic-
ular issue of contention. Table 1 presents an exam-
ple of short-text documents expressing divergent
opinions where each is exclusively supporting or
opposing a healthcare legislation 1.

1extracted from a Gallup Inc. survey
http://www.gallup.com/poll/126521/favor-oppose-obama-
healthcare-plan.aspx

Opinion in contentious issues is often expressed
implicitly, not necessarily through the usage of
usual negative or positive opinion words, like
“bad” or “great”. This makes its extraction a chal-
lenging task. It is usually conveyed through the
arguing expression justifying the endorsement of
a particular point of view. The act of arguing is
“to give reasons why you think that something is
right/wrong, true/not true, etc, especially to per-
suade people that you are right” (cf. Oxford Dic-
tionaries). For example, the arguing expression
“many people do not have healthcare”, in Table 1,
implicitly explains that the reform is intended to
fix the problem of uninsured people, and thus, the
opinion is probably on the supporting side. On the
other hand, the arguing expression “it will produce
too much debt” denotes the negative consequence
that may result from passing the bill, making it on
the opposing side.

The automatic identification and clustering of
these kind of arguing expressions, according to
their topics and the viewpoints they convey, is en-
ticing for a variety of application domains. For in-
stance, it can save journalists a substantial amount
of work and provide them with drafting elements
(viewpoints and associated arguing expressions)
about controversial issues. In addition, it would
enhance the output quality of the opinion summa-
rization task in general.

The rest of this paper is organized as follows.
Section 2 covers the details of the problem state-
ment. Section 3 explains the key issues in the con-
text of recent related work. Section 4 provides
the technical details of our model, the Joint Topic
Viewpoint model (JTV) . Section 5 describes the
clustering task that might be used to obtain a fea-
sible solution. Section 6 provides a description of
the experimental set up. Section 7 assesses the ad-
equacy and the performance of our solution. Sec-
tion 8 concludes the paper.

35



Support Viewpoint Oppose Viewpoint
Many people do not have health care The government should not be involved
Provide health care for 30 million people It will produce too much debt
The government should help old people The bill would not help the people

Table 1: Excerpts of support and opposition opinion to a healthcare bill in the USA.

2 Problem Statement

This paper examines the task of mining the topics
and the viewpoints of arguing expressions towards
the summarization of contentious text. An exam-
ple of a human-made summary of arguing expres-
sions (Jones, 2010) on, what is commonly known
as, the Obama healthcare reform is presented in
Table 2. Ultimately, the target is to automatically
generate similar summaries given a corpus of con-
tentious documents. However, this paper tack-
les the sub-problem of identifying recurrent words
and phrases expressing arguing and cluster them
according to their topics and viewpoints. This
would help solve the general problem. We use
Table 2’s examples to define some key concepts
which can help us formulate this latter. Here, the
contentious issue yielding the divergent positions
is the Obama healthcare. The documents are peo-
ple’s verbatim responses to the question “Why do
you favor or oppose a healthcare legislation simi-
lar to President Obama’s ?”.

We define a contention question as a question
that can generate expressions of two or more di-
vergent viewpoints as a response.

While the previous question explicitly asks for
the reasons (“why”), we relax this constraint and
consider also usual opinion questions like “Is the
passing of Obamacare bad for Americans ?” or
“Do you favor or oppose Obamacare ?”.

A contentious document is a document that
contains expressions of one or more divergent
viewpoints in response to the contention question.
In the context of online debate, a post usually ex-
presses one viewpoint, although it can mention ar-
guing used to justify a different viewpoint.

Table 2 is split into two parts according to the
viewpoint: supporting or opposing the healthcare
bill. Each row contains one or more phrases, each
expressing a reason (or an explanation), e.g. “costs
are out of control” and “would help control costs”.
Though lexically different, these phrases share a
common hidden theme (or topic), e.g. insurance’s
cost, and implicitly convey the same hidden view-
point’s semantics, e.g. support the healthcare bill.

Thus, we define an arguing expression as the set
of reasons (words or phrases) sharing a common
topic and justifying the same viewpoint regarding
a contentious issue.

We assume that a viewpoint (e.g. a column of
Table 2) in a contentious document is a stance, in
response to a contention question, which is implic-
itly expressed by a set of arguing expressions (e.g.
rows of a column in Table 2).

Thus, the arguing expressions voicing the same
viewpoint differ in their topics, but agree in the
stance. For example, arguing expressions repre-
sented by “system is broken” and “costs are out
of control” discuss different topics, i.e. healthcare
system and insurance’s cost, but both support the
healthcare bill. On the other hand, arguing ex-
pressions of divergent viewpoints may have sim-
ilar topic or may not. For instance, “government
should help elderly” and “government should not
be involved” share the same topic, i.e. govern-
ment’s role, while conveying opposed viewpoints.

Our research problem and objectives in terms
of the newly introduced concepts are stated as
follows. Given a corpus of unlabeled con-
tentious documents {doc1, doc2, .., docD}, where
each document docd expresses one or more view-
points ~vd from a set of L possible viewpoints
{v1, v2, .., vL}, and each viewpoint vl can be con-
veyed using one or more arguing expressions ~φl
from a set of possible arguing expressions dis-
cussing K different topics {φ1l, φ2l, .., φKl}, the
objective is to perform the following two tasks:

1. automatically extracting coherent words and
phrases describing any distinct arguing ex-
pression φkl;

2. grouping extracted distinct arguing expres-
sions φkl for different topics, k = 1..K, into
their corresponding viewpoint vl.

This paper focuses on the first task while laying
the ground for solving the second one. In carry-
ing out the first task, we must meet the main chal-
lenge of recognizing arguing expressions having

36



Support Viewpoint Oppose Viewpoint
People need health insurance/too many uninsured Will raise cost of insurance/ less affordable
System is broken/needs to be fixed Does not address real problems
Costs are out of control/would help control costs Need more information on how it works
Moral responsibility to provide/Fair Against big government involvement (general)
Would make healthcare more affordable Government should not be involved in healthcare
Don’t trust insurance companies Cost the government too much

Table 2: Human-made summary of arguing expressions supporting and opposing Obamacare.

the same topic and viewpoint but which are lexi-
cally different, e.g. “provide health care for 30
million people ” and “ many people do not have
healthcare”. For this purpose we propose a Joint
Topic Viewpoint Model (JTV) to account for the
dependence structure of topics and viewpoints.

3 Related Work

3.1 Classifying Stances
An early body of work addresses the challenge of
classifying viewpoints in contentious or ideolog-
ical discourses using supervised techniques (Kim
and Hovy, 2007; Lin et al., 2006). Although the
models give good performances, they remain data-
dependent and costly to label, making the unsuper-
vised approach more appropriate for the existing
huge quantity of online data. A similar trend of
studies scrutinizes the discourse aspect of a docu-
ment in order to identify opposed stances (Thomas
et al., 2006; Park et al., 2011). However, these
methods utilize polarity lexicon to detect opinion-
ated text and do not look for arguing expression,
which is shown to be useful in recognizing op-
posed stances (Somasundaran and Wiebe, 2010).
Somasundaran and Wiebe (2010) classify ideolog-
ical stances in online debates using a generated ar-
guing clues from the Multi Perspective Question
Answering (MPQA) opinion corpus2. Our prob-
lem is not to classify documents, but to recognize
recurrent pattern of arguing phrases instead of ar-
guing clues. Moreover, our approach is indepen-
dent of any annotated corpora.

3.2 Topic Modeling in Reviews Data
Another emerging body of work applies proba-
bilistic topic models on reviews data to extract ap-
praisal aspects and the corresponding specific sen-
timent lexicon. These kinds of models are usually
referred to as joint sentiment/aspect topic models
(Jo and Oh, 2011; Titov and McDonald, 2008;

2http://mpqa.cs.pitt.edu/

Zhao et al., 2010). Lin and He (2009) propose the
Joint Sentiment Topic Model (JST) to model the
dependency between sentiment and topics. They
make the assumption that topics discussed on a re-
view are conditioned on sentiment polarity. Re-
versely, our JTV model assumes that a viewpoint
endorsement (e.g., oppose reform) is conditioned
on the discussed topic (e.g., government’s role)
and its application is different from that of JST.
Most of the joint aspect sentiment topic models are
either semi-supervised or weakly supervised using
sentiment polarity words (Paradigm lists) to boost
their efficiency. In our case, viewpoints are often
expressed implicitly and finding specific arguing
lexicon for different stances is a challenging task
in itself. Indeed, our model is enclosed in another
body of work that based on a probabilistic Topic
Model framework to mine divergent viewpoints.

3.3 Topic Modeling in Contentious Text

A recent study by Mukherjee and Liu (2012)
examines mining contention from discussion fo-
rums data where the interaction between differ-
ent authors is pivotal. It attempts to jointly
discover contention/agreement indicators (CA-
Expressions) and topics using three different Joint
Topic Expressions Models (JTE). The JTEs’ out-
put is used to discover points (topics) of con-
tention. The model supposes that people ex-
press agreement or disagreement through CA-
expressions. However, this is not often the case
when people express their viewpoint via other
channels than discussion forums like debate sites
or editorials. Moreover, agreement or disagree-
ment may also be conveyed implicitly through ar-
guing expressions rejecting or supporting another
opinion. JTEs do not model viewpoints and use
the supervised Maximum Entropy model to detect
CA-expressions.

Recently, Gottipati et al. (2013) propose a topic
model to infer human interpretable text in the do-

37



main of issues using Debatepedia3 as a corpus of
evidence. Debatepedia is an online authored en-
cyclopedia to summarize and organize the main
arguments of two possible positions. The model
takes advantage of the hierarchical structure of ar-
guments in Debatepedia. Our work aims to model
unstructured online data, with unrestricted num-
ber of positions, in order to, ultimately, output a
Debatepedia-like summary.

The closest work to ours is the one presented
by Paul et al. (2010). It introduces the problem
of contrastive summarization which is very simi-
lar to our stated problem in Section 2. They pro-
pose the Topic Aspect Model (TAM) and use the
output distributions to compute similarities’ scores
for sentences. Scored sentences are used in a mod-
ified Random Walk algorithm to generate the sum-
mary. The assumption of TAM is that any word
in the document can exclusively belong to a topic
(e.g., government), a viewpoint (e.g., good), both
(e.g., involvement) or neither (e.g., think). How-
ever, according to TAM’s generative model, an au-
thor would choose his viewpoint and the topic to
talk about independently. Our JTV encodes the
dependency between topics and viewpoints.

4 Joint Topic Viewpoint Model

Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) is one of the most popular topic models used
to mine large text data sets. It models a document
as a mixture of topics where each topic is a dis-
tribution over words. However, it fails to model
more complex structures of texts like contention
where viewpoints are hidden.

We augment LDA to model a contentious doc-
ument as a pair of dependent mixtures: a mixture
of arguing topics and a mixture of viewpoints for
each topic. The assumption is that a document dis-
cusses the topics in proportions, (e.g. 80% gov-
ernment’s role, 20% insurance’s cost). Moreover,
as explained in Section 2, each one of these top-
ics can be shared by divergent arguing expres-
sions conveying different viewpoints. We suppose
that for each discussed topic in the document, the
viewpoints are expressed in proportions. For in-
stance, 70% of the document’s text discussing the
government’s role expresses an opposing view-
point to the reform while 30% of it conveys a sup-
porting viewpoint. Thus, each term in a docu-
ment is assigned a pair topic-viewpoint label (e.g.

3http://dbp.idebate.org

Figure 1: The JTV’s graphical model (plate nota-
tion)

“government’s role-oppose reform”). A term is a
word or a phrase i.e. n-grams (n>1). For each
topic-viewpoint pair, the model generates a topic-
viewpoint probability distribution over terms. This
topic-viewpoint distribution would corresponds to
what we define as an arguing expression in Sec-
tion 2, i.e. a set of terms sharing a common topic
and justifying the same viewpoint regarding a con-
tentious issue. The Joint Topic Viewpoint (JTV),
is similar to the Joint Sentiment Topic model (JST)
(Lin and He, 2009), as it models documents as two
dependent mixtures. However, here we condition
viewpoints on topics instead of conditioning top-
ics on sentiment. Moreover, the application is dif-
ferent from that of JST which intend to model re-
views data.

Formally, assume that a corpus contains D doc-
uments d1..D, where each document is a term’s
vector ~wd of size Nd; each term wdn in a docu-
ment belongs to the corpus vocabulary of distinct
terms of size V . Let K be the total number of top-
ics andL be the total number of viewpoints. Let θd
denote the probabilities (proportions) of K topics
under a document d; ψdk be the probability distri-
butions (proportions) of L viewpoints for a topic
k in the document d (the number of viewpoints L
is the same for all topics); and φkl be the multino-
mial probability distribution over terms associated
with a topic k and a viewpoint l. The generative

38



process (see. the JTV graphical model in Figure
1) is the following:

• for each topic k and viewpoint l, draw a
multinomial distribution over the vocabulary
V : φkl ∼ Dir(β);
• for each document d,

draw a topic mixture θd ∼ Dir(α)
for each topic k, draw a viewpoint mixture
ψdk ∼ Dir(γ)
for each term wdn, sample a topic assignment
zdn ∼Mult(θd); sample a viewpoint assign-
ment vdn ∼Mult(ψdzdn); and sample a term
wdn ∼Mult(φzdnvdn).

We use fixed symmetric Dirichlet’s parameters γ,
β and α. They can be interpreted as the prior
counts of: terms assigned to viewpoint l and topic
k in a document; a particular term w assigned to
topic k and viewpoint l within the corpus; terms
assigned to a topic k in a document, respectively.

In order to learn the hidden JTV’s parameters
φkl, ψdk and θd, we draw on approximate in-
ference as exact inference is intractable (Blei et
al., 2003). We use the collapsed Gibbs Sampling
(Griffiths and Steyvers, 2004), a Markov Chain
Monte Carlo algorithm. The collapsed Gibbs sam-
pler integrate out all parameters φ, ψ and θ in the
joint distribution of the model and converge to a
stationary posterior distribution over viewpoints’
assignments ~v and all topics’ assignments ~z in the
corpus. It iterates on each current observed token
wi and samples each corresponding vi and zi given
all the previous sampled assignments in the model
~v¬i, ~z¬i and observed ~w¬i, where ~v = {vi, ~v¬i},
~z = {zi, ~z¬i}, and ~w = {wi, ~w¬i}. The derived
sampling equation is:

p(zi = k, vi = l|~z¬i, ~v¬i, wi = t, ~w¬i) ∝
n

(t)
kl,¬i + β

V∑
t=1

n
(t)
kl,¬i + V β

.
n

(l)
dk,¬i + γ

L∑
l=1

n
(l)
dk,¬i + Lγ

.n
(k)
d,¬i + α (1)

where n(t)kl,¬i is the number of times term t was as-
signed to topic k and the viewpoint l in the corpus;
n

(l)
dk,¬i is the number of times viewpoint l of topic k

was observed in document d; and n(k)d,¬i is the num-
ber of times topic k was observed in document d.
All these counts are computed excluding the cur-
rent token i, which is indicated by the symbol ¬i.

AW GM ObCare
View pt allow not illegal not bad not
#doc 213 136 44 54 129 54
tot.#toks 44482 10666 22733
avg.#toks.
doc.

127.45 108.83 124.22

Table 3: Statistics on the three used data sets

After the convergence of the Gibbs algorithm, the
parameters φ, ψ and θ are estimated using the last
obtained sample.

5 Clustering Arguing Expressions

Although we are not tackling the task of cluster-
ing arguing expressions according to their view-
points in this paper (Task 2 in Section 2), we ex-
plain how the structure of JTV lays the ground for
performing it. We mentioned in the previous Sec-
tion that an inferred topic-viewpoint distribution
φkl can be assimilated to an arguing expression.
For convenience, we will use “arguing expression”
and “topic-viewpoint” interchangeably to refer to
the topic-viewpoint distribution.

Indeed, two topic-viewpoint φkl and φk′l, hav-
ing different topics k and k′, do not necessarily
express the same viewpoint, despite the fact that
they both have the same index l. The reason stems
from the nested structure of the model, where the
generation of the viewpoint assignments for a par-
ticular topic k is completely independent from that
of topic k′. In other words, the model does not
trace and match the viewpoint labeling along dif-
ferent topics. Nevertheless, the JTV can still help
overcome this problem. According to the JTV’s
structure, a topic-viewpoint φkl, is more similar
in distribution to a divergent topic-viewpoint φkl′ ,
related to the same topic k, than to any other topic-
viewpoint φk′∗, corresponding to a different topic
k′. Therefore, we can formulate the problem of
clustering arguments as a constrained clustering
problem (Basu et al., 2008). The goal is to group
the similar topics-viewpoints φkls into L clusters
(number of viewpoints), given the constraint that
the φkls of the same topic k should not belong to
the same cluster. The similarity between the topic-
viewpoint distributions can be measured using the
Jensen-Shannon Divergence (Bishop, 2006).

39



6 Experimental Set up

In order to evaluate the performances of the JTV
model, we experiment with three different cor-
pora of contentious documents. Recall, we assume
that any input document to the JTV is answer-
ing a contentious question which makes it con-
tentious according to the definitions stated in Sec-
tion 2. Posts in online debate websites, like “creat-
edebate.com” or “debate.org”, match this require-
ment. They correspond to online users’ takes on
a clearly stated contention question making them
more adequate for our matter than debate forums’
posts. These latter contain online interactions be-
tween users where the objective is not necessar-
ily answering a contention question but rather dis-
cussing a contentious topic. Classifying a docu-
ment as contentious or not is not an issue consid-
ered in this paper but can be explored in our future
work. Table 3 describes the used data sets.

Assault Weapons (AW) 4: includes posts ex-
tracted from “debate.com”. The contention ques-
tion is “Should assault weapons be allowed in the
United States as means of allowing individuals to
defend themselves?”. The viewpoints are either
“should be allowed” or “should not be allowed”.

Gay Marriage (GM) 5: contains posts from
“debate.com” related to the contention question
“Should gay marriage be illegal?”. The posts’
stance are either “should be illegal” or “should be
legal”.

Obama Healthcare (ObCare) 6: includes posts
from “debate.org” responding to the contention
question “Is the passing of ObamaCare bad for the
American public?”. Stances are either “bad” or
“not bad”.

Paul et al. (2010) stress out the importance of
negation features in detecting contrastive view-
points. Thus, we performed a simple treatment
of merging any negation indicators, like “noth-
ing”, “no one”, “never”, etc., found in text with
the following occurring word to form a single to-
ken. Moreover, we merge the negation “not” with
any Auxiliary verb (e.g., is, was, could, will) pre-
ceding it. Then, we removed the stop-words.

4http://www.debate.org/opinions/should-assault-
weapons-be-allowed-in-the-united-states-as-means-of-
allowing-individuals-to-defend-themselves

5http://www.debate.org/opinions/should-gay-marriage-
be-illegal

6http://www.debate.org/opinions/is-the-passing-of-
obamacare-bad-for-the-american-public

Throughout the experiments below, the JTV’s
hyperparameters are set to fixed values. The γ is
set, according to Steyvers and Griffiths’s (Steyvers
and Griffiths, 2007) hyperparameters settings, to
50/L, where L is the number of viewpoints. β
and α are adjusted manually, to give reasonable
results, and are both set to 0.01. Along the exper-
iments, we try different number of topics K. The
number of viewpoints L is equal to 2. The TAM
model (Paul et al., 2010) (Section 3.3) is run as a
means of comparison during the evaluation proce-
dure. Its default parameters are used.

7 Model Evaluation

7.1 Qualitative Evaluation

Tables 4 and 5 present the inferred topic-
viewpoints words, i.e. arguing expressions, by
JTV for the Obama Healthcare and Gay Marriage
data sets, respectively. We set a number of topics
of K = 3 for the former and K = 2 for the lat-
ter. The number of viewpoints is L = 2 for both
data sets. For the Obamacare data set, we run the
model with balanced number of posts from “bad”
and “not bad” stances. Each topic-viewpoint pair
(e.g. Topic 1-view 1) is represented by the set of
top terms. The terms are sorted in descending or-
der according to their probabilities. Inferred prob-
abilities over topics, and over viewpoints for each
topic, are also reported. We try to qualitatively
observe the distinctiveness of each arguing (topic-
viewpoint) and assess the coherence in terms of
the topic discussed and the viewpoint conveyed
and its divergence with the corresponding pair-
element.

In both Tables 4 and 5, most of the topic-
viewpoint pairs, corresponding to a same topic,
are conveying opposite stances. For instance, tak-
ing a closer look to the original data suggests that
Topic3-view5 (Table 4) criticizes the healthcare
system and compares it to the other countries (e.g.
a sample from the original documents:“revise our
healthcare system with the most efficient systems
in the world”). On the other side, Topic 3-view
6 explains the negative consequence of obamacare
on middle class, e.g. “ObamaCare was supposed
to help the poor and the middle class. In the
end, the businesses fire all the people because of
the ObamaCare taxes and then IT IS THE MID-
DLE CLASS PEOPLE WHO SUFFER!”. Simi-
larly, Topic1-view1 advances the question of the
costs that the bill will cause at the level of people

40



Topic 1 0.328 Topic 2 0.334 Topic 3 0.337
view 1 0.64 view 2 0.36 view 3 0.59 view 4 0.41 view 5 0.63 view 6 0.37

pay universal people insurance healthcare obamacare
people care insurance health obamacare healthcare
make life good companies system government

money law free medicare americans class
costs act health doctors affordable taxes

government poor work plan country/world middle

Table 4: JTV’s generated topics-viewpoints (arguing expressions) from Obamacare data set

Topic 1 0.50 Topic 2 0.50
view 1 0.47 view 2 0.53 view 3 0.60 view 4 0.40

marriage marriage people gay
love man gay children
life woman religion people

couples god shouldnt sex
person bible wrong parents
legal illegal rights natural

married wrong government human
happy love marry population

samesex homosexual freedom opposite
illegal word argument race

Table 5: JTV’s generated topics-viewpoints (arguing expressions) from Gay Marriage data set

and government, e.g. “The government doesn’t
even have enough money to pay of a fraction of the
towering debt that we’ve accrued”, “forcing peo-
ple to buy insurance or pay an even higher tax will
make more families poverty stricken”. However,
Topic1-view2 stresses out the importance of hav-
ing a universal healthcare, e.g. “ObamaCare cer-
tainly has problems, but just like any law, we can
work on these problems and make the law better
(..). The fundamental goal is Universal Health-
care (...)”, “If you were poor and had a hernia
that needed surgery, you need money to pay for
it. Denying Obama’s Plan for a health care sys-
tem means you cannot pay for it which means you
will DIE.”. Similar pattern is observed in Topic 2.

The results on Gay Marriage 1 dataset (Table
5) encompass the notion of shared topic between
divergent arguing expressions (Section 2) more
clearly than the results obtained from Obamacare.
This may be related to the nature of the contention.
For instance, Topic 1 in Table 5 is “the concept of
marriage” and it is shared by both view 1 and view
2. However, the concept is perceived differently
according to the stance. The terms in view 1 (not
illegal) suggest that marriage is about love, hap-

piness and it wouldn’t disturb anyone’s life (as it
may be read from original data). The view 2 (il-
legal) may emphasize the notion of a marriage as
a union between man and woman and the sacred-
ness aspect of it (god, bible). Similarly, Topic 2
is about “people who are gay”. The terms in view
3 (not illegal) may advocate that religious argu-
ments from opposing stance do not make sense
and that gay people are free and have the same
rights as other people. Moreover, the government
should not interfere in this matter. View 4 (illegal)
suggests that gay people can not have children
which raises the problem of population decrease.
It also casts doubt on their ability to be parents.

7.2 Quantitative Evaluation

We assess the ability of the model to fit the online
debate data and generate distinct topic-viewpoint
pairs by comparing it with TAM which models
also the topic-viewpoint dimension.

7.2.1 Held-Out Perplexity
We use the perplexity criterion to measure the abil-
ity of the learned topic model to fit a new held-
out data. Perplexity assesses the generalization
performance and, subsequently, provides a com-

41



(a) AW (b) GM (c) ObCare

Figure 2: JVT and TAM’s perplexity plots for three different data sets

Figure 3: Average of overall topic-viewpoint di-
vergences of JTV and TAM

paring framework of learned topic models. The
lower the perplexity, the less “perplexed” is the
model by unseen data and the better the general-
ization. It algebraically corresponds to the inverse
geometrical mean of the test corpus’ terms likeli-
hoods given the learned model parameters (Hein-
rich, 2009). We compute the perplexity under es-
timated parameters of JTV and compare it to that
of TAM for our three unigrams data sets (Section
6).

Figure 2 exhibits, for each corpus, the perplex-
ity plot as function of the number of topics K
for JTV and TAM. Note that for each K, we run
the model 50 times. The drawn perplexity corre-
sponds to the average perplexity on the 50 runs
where each run compute one-fold perplexity from
a 10-fold cross-validation. The figures show evi-
dence that the JTV outperforms TAM for all data
sets, used in the experimentation.

7.2.2 Kullback-Leibler Divergence
Kullback-Leibler (KL) Divergence is used to mea-
sure the degree of separation between two proba-
bility distributions. We utilize it to assess the dis-

tinctiveness of generated topic-viewpoint by JTV
and TAM. This is an indicator of a good ag-
gregation of arguing expressions. We compute
an overall-divergence quantity, which is an av-
erage KL-Divergence between all pairs of topic-
viewpoint distributions, for JTV and TAM and
compare them. Figure 3 illustrates the results for
all datasets. Quantities are averages on 20 runs of
the models. Both models are run with a number
of topics K = 5. Comparing JTV and TAM, we
notice that the overall-divergence of JTV’s topic-
viewpoint is significantly (p − value < 0.01)
higher for all data sets. This result reveals a better
quality of our JTV extracting process of arguing
expressions (the first task stated in Section 2)

8 Conclusion

We suggested a fine grained probabilistic frame-
work for improving the quality of opinion min-
ing from online contention texts. We proposed
a Joint Topic Viewpoint model (JTV) for the un-
supervised detection of arguing expressions. Un-
like common approaches the proposed model fo-
cuses on arguing expressions that are implicitly
described in unstructured text according to the la-
tent topics they discuss and the implicit viewpoints
they voice. The qualitative and quantitative analy-
sis of the experimental results show the effective-
ness of our (JTV) model in generating informative
summaries of recurrent topics and viewpoints pat-
terns in online debates’ texts. Future study needs
to give more insights into the clustering of arguing
expressions according to their viewpoints, as well
as their automatic extractive summary.

References
Sugato Basu, Ian Davidson, and Kiri Wagstaff. 2008.

Constrained Clustering: Advances in Algorithms,

42



Theory, and Applications. Chapman & Hall/CRC,
1 edition.

Christopher M. Bishop. 2006. Pattern Recognition
and Machine Learning (Information Science and
Statistics). Springer-Verlag New York, Inc., Secau-
cus, NJ, USA.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022, March.

Yi Fang, Luo Si, Naveen Somasundaram, and Zhengtao
Yu. 2012. Mining contrastive opinions on political
texts using cross-perspective topic model. In Pro-
ceedings of the fifth ACM international conference
on Web search and data mining, WSDM ’12, pages
63–72, New York, NY, USA. ACM.

Swapna Gottipati, Minghui Qiu, Yanchuan Sim, Jing
Jiang, and Noah A. Smith. 2013. Learning topics
and positions from debatepedia. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’13.

Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228–5235.

Gregor Heinrich. 2009. Parameter estimation for
text analysis. Technical report, Fraunhofer IGD,
September.

Yohan Jo and Alice H. Oh. 2011. Aspect and sen-
timent unification model for online review analysis.
In Proceedings of the fourth ACM international con-
ference on Web search and data mining, WSDM ’11,
pages 815–824, New York, NY, USA. ACM.

Jeffrey M. Jones. 2010. In u.s., 45% favor, 48% op-
pose obama healthcare plan. Gallup, March.

Soo-Min Kim and Eduard H Hovy. 2007. Crystal: An-
alyzing predictive opinions on the web. In EMNLP-
CoNLL, pages 1056–1064.

Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceedings of the 18th ACM conference on Informa-
tion and knowledge management, CIKM ’09, pages
375–384, New York, NY, USA. ACM.

Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on?: identifying perspectives at the document and
sentence levels. In Proceedings of the Tenth Con-
ference on Computational Natural Language Learn-
ing, CoNLL-X ’06, pages 109–116, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Arjun Mukherjee and Bing Liu. 2012. Mining con-
tentions from discussions and debates. In Proceed-
ings of the 18th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,

KDD ’12, pages 841–849, New York, NY, USA.
ACM.

Souneil Park, KyungSoon Lee, and Junehwa Song.
2011. Contrasting opposing views of news articles
on contentious issues. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
- Volume 1, HLT ’11, pages 340–349, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ’10, pages 66–76, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Gener-
ation of Emotion in Text, CAAGET ’10, pages 116–
124, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of latent semantic analysis,
427(7):424–440.

Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: determining support or opposition from
congressional floor-debate transcripts. In Proceed-
ings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’06,
pages 327–335, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Ivan Titov and Ryan McDonald. 2008. Modeling
online reviews with multi-grain topic models. In
Proceedings of the 17th international conference on
World Wide Web, WWW ’08, pages 111–120, New
York, NY, USA. ACM.

Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a maxent-lda hybrid. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ’10, pages 56–
65, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

43


