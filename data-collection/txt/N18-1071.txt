



















































Collective Entity Disambiguation with Structured Gradient Tree Boosting


Proceedings of NAACL-HLT 2018, pages 777–786
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Collective Entity Disambiguation with Structured Gradient Tree Boosting

Yi Yang Ozan Irsoy Kazi Shefaet Rahman
Bloomberg LP

New York, NY 10022
{yyang464+oirsoy+krahman7}@bloomberg.net

Abstract

We present a gradient-tree-boosting-based
structured learning model for jointly disam-
biguating named entities in a document. Gra-
dient tree boosting is a widely used machine
learning algorithm that underlies many top-
performing natural language processing sys-
tems. Surprisingly, most works limit the use
of gradient tree boosting as a tool for regu-
lar classification or regression problems, de-
spite the structured nature of language. To the
best of our knowledge, our work is the first
one that employs the structured gradient tree
boosting (SGTB) algorithm for collective en-
tity disambiguation. By defining global fea-
tures over previous disambiguation decisions
and jointly modeling them with local features,
our system is able to produce globally opti-
mized entity assignments for mentions in a
document. Exact inference is prohibitively ex-
pensive for our globally normalized model. To
solve this problem, we propose Bidirectional
Beam Search with Gold path (BiBSG), an ap-
proximate inference algorithm that is a variant
of the standard beam search algorithm. BiBSG
makes use of global information from both
past and future to perform better local search.
Experiments on standard benchmark datasets
show that SGTB significantly improves upon
published results. Specifically, SGTB outper-
forms the previous state-of-the-art neural sys-
tem by near 1% absolute accuracy on the pop-
ular AIDA-CoNLL dataset.1

1 Introduction

Entity disambiguation (ED) refers to the process
of linking an entity mention in a document to its
corresponding entity record in a reference knowl-
edge base (e.g., Wikipedia or Freebase). As a core
information extraction task, ED plays an impor-
tant role in the language understanding pipeline,
underlying a variety of downstream applications

1When ready, the code will be published at https://
github.com/bloomberg/sgtb.

such as relation extraction (Mintz et al., 2009;
Riedel et al., 2010), knowledge base population (Ji
and Grishman, 2011; Dredze et al., 2010), and
question answering (Berant et al., 2013; Yih et al.,
2015). This task is challenging because of the
inherent ambiguity between mentions and the re-
ferred entities. Consider, for example, the men-
tion ‘Washington’, which can be linked to a city, a
state, a person, an university, or a lake (Fig. 1).

Fortunately, simple and effective features have
been proposed to capture the ambiguity that are
designed to model the similarity between a men-
tion (and its local context) and a candidate en-
tity, as well as the relatedness between entities
that co-occur in a single document. These are
typically statistical features estimated from entity-
linked corpora, and similarity features that are
pre-computed using distance metrics such as co-
sine. For example, a key feature for ED is the
prior probability of an entity given a specific men-
tion, which is estimated from mention-entity co-
occurrence statistics. This simple feature alone
can yield 70% to 80% accuracy on both news and
Twitter texts (Lazic et al., 2015; Guo et al., 2013).

To capture the non-linear relationships between
the low-dimensional dense features like statistical
features, sophisticated machine learning models
such as neural networks and gradient tree boost-
ing are preferred over linear models. In particu-
lar, gradient tree boosting has been shown to be
highly competitive for ED in recent work (Yang
and Chang, 2015; Yamada et al., 2016). How-
ever, although achieving appealing results, exist-
ing gradient-tree-boosting-based ED systems typ-
ically operate on each individual mention, with-
out attempting to jointly resolve entity mentions
in a document together. Joint entity disambigua-
tion has been shown to significantly boost perfor-
mance when used in conjunction with other ma-
chine learning techniques (Ratinov et al., 2011;
Hoffart et al., 2011). However, how to train a

777



global gradient tree boosting model that produces
coherent entity assignments for all the mentions in
a document is still an open question.

In this work, we present, to the best of our
knowledge, the first structured gradient tree boost-
ing (SGTB) model for collective entity disam-
biguation. Building on the general SGTB frame-
work introduced by Yang and Chang (2015), we
develop a globally normalized model for ED that
employs a conditional random field (CRF) ob-
jective (Lafferty et al., 2001). The model per-
mits the utilization of global features defined be-
tween the current entity candidate and the entire
decision history for previous entity assignments,
which enables the global optimization for all the
entity mentions in a document. As discussed in
prior work (Smith and Johnson, 2007; Andor et al.,
2016), globally normalized models are more ex-
pressive than locally normalized models.

As in many other global models, our SGTB
model suffers from the difficulty of computing the
partition function (normalization term) for training
and inference. We adopt beam search to address
this problem, in which we keep track of multiple
hypotheses and sum over the paths in the beam. In
particular, we propose Bidirectional Beam Search
with Gold path (BiBSG) technique that is specif-
ically designed for SGTB model training. Com-
pared to standard beam search strategies, BiBSG
reduces model variance and also enjoys the advan-
tage in its ability to consider both past and future
information when predicting an output.

Our contributions are:

• We propose a SGTB model for collectively
disambiguating entities in a document. By
jointly modeling local decisions and global
structure, SGTB is able to produce globally
optimal entity assignments for all the men-
tions.

• We present BiBSG, an efficient algorithm for
approximate bidirectional inference. The al-
gorithm is tailored to SGTB models, which
can reduce model variance by generating
more point-wise functional gradients for es-
timating the auxiliary regression models.

• SGTB achieves state-of-the-art (SOTA) re-
sults on various popular ED datasets, and
it outperforms the previous SOTA systems
by 1-2% absolute accuracy on the AIDA-
CoNLL (Hoffart et al., 2011) dataset.

2 Model

In this section, we present a SGTB model for col-
lective entity disambiguation. We first formally
define the task of ED, and then describe a struc-
tured learning formalization for producing glob-
ally coherent entity assignments for mentions in a
document. Finally, we show how to optimize the
model using functional gradient descent.

For an input document, assume that we are
given all the mentions of named entities within
it. Also assume that we are given a lexicon that
maps each mention to a set of entity candidates in
a given reference entity database (e.g., Wikipedia
or Freebase). The ED system maps each mention
in the document to an entry in the entity database.
Since a mention is often ambiguous on its own
(i.e., the lexicon maps the mention to multiple en-
tity candidates), the ED system needs to lever-
age two types of contextual information for dis-
ambiguation: local information based on the en-
tity mention and its surrounding words, and global
information that exploits the document-level co-
herence of the predicted entities. Note that model-
ing entity-entity coherence is very challenging, as
the long-range dependencies between entities cor-
respond to exponentially large search space.

We formalize this task as a structured learning
problem. Let x be a document with T target men-
tions, and y = {yt}Tt=1 be the entity assignments
of the mentions in the document. We use S(x,y)
to denote the joint scoring function between the
input document and the output structure. In tradi-
tional NLP tasks, such as part-of-speech tagging
and named entity recognition, we often rely on
low-order Markov assumptions to decompose the
global scoring function into a summation of lo-
cal functions. ED systems, however, are often re-
quired to model nonlocal phenomena, as any pair
of entities is potentially interdependent. There-
fore, we choose the following decomposition:

S(x,y) =
T∑

t=1

F (x, yt,y1:t−1), (1)

where F (x, yt,y1:t−1) is a factor scoring function.
Specifically, a local prediction yt depends on all
the previous decisions, y1:t−1 in our model, which
resembles recurrent neural network (RNN) mod-
els (Elman, 1990; Hochreiter and Schmidhuber,
1997).

We adopt a CRF loss objective, and define a

778



Figure 1: (a) Example document x with entity candidates for each mention (gold entities are in bold); (b) the m-th
SGTB update iteration: (i) conduct beam search to sample candidate entity sequences (§ 3), (ii) compute point-
wise functional gradients for each candidate sequence, (iii) fit a regression tree to the negative functional gradient
points with input features, φ, (iv) update the factor scoring function, F , by adding the trained regression tree.

distribution over possible output structures as fol-
lows:

p(y|x) = exp{
∑T

t=1 F (x, yt,y1:t−1)}
Z(x)

, (2)

where

Z(x) =
∑

y′∈Gen(x)
exp{

T∑

t=1

F (x, y′t,y
′
1:t−1)}

and Gen(x) is the set of all possible sequences
of entity assignments depending on the lexicon.
Z(x) is then a global normalization term. As
shown in previous work, globally normalized
models are very expressive, and also avoid the
label bias problem (Lafferty et al., 2001; Andor
et al., 2016). The inference problem is to find

argmax
y∈Gen(x)

p(y|x) = argmax
y∈Gen(x)

T∑

t=1

F (x, yt,y1:t−1).

(3)

2.1 Structured gradient tree boosting
An overview of our SGTB model is shown
in Fig. 1. The model minimizes the negative log-
likelihood of the data,

L(y∗, S(x,y)) = − log p(y∗|x)
= logZ(x)− S(x,y∗), (4)

where y∗ is the gold output structure.

In a standard CRF, the factor scoring func-
tion is typically assumed to have this form:
F (x, yt,y1:t−1) = θ>φ(x, yt,y1:t−1), where
φ(x, yt,y1:t−1) is the feature function and θ are
the model parameters. The key idea of SGTB is
that, instead of defining a parametric model and
optimizing its parameters, we can directly opti-
mize the factor scoring function F (·) iteratively
by performing gradient descent in function space.
In particular, suppose F (·) = Fm−1(·) in them-th
iteration, we will update F (·) as follows:

Fm(x, yt,y1:t−1) = Fm−1(x, yt,y1:t−1)

− ηmgm(x, yt,y1:t−1),
(5)

where

gm(x, yt,y1:t−1) =
∂L(y∗, S(x,y))
∂F (x, yt,y1:t−1))

= p(y1:t|x)− 1[y1:t = y∗1:t]
(6)

is the functional gradient, ηm is the learning rate,
and 1[·] represents an indicator function, which
returns 1 if the predicted sequence matches the
gold one, and 0 otherwise. We initialize F (·) to
0 (F0(·) = 0).

We can approximate the negative func-
tional gradient −gm(·) with a regression
tree model hm(·) by fitting the training data
{φ(x(i), y(i)t ,y

(i)
1:t−1)} to the point-wise negative

functional gradients (also known as residuals)
{−gm(x(i), y(i)t ,y

(i)
1:t−1)}. Then the factor scoring

779



function can be obtained by

F (x, yt,y1:t−1) =
M∑

m=1

ηmhm(x, yt,y1:t−1),

(7)
where hm(x, yt,y1:t−1) is called a basis function.
We set ηm = 1 in this work.

3 Training

Training the SGTB model requires computing
the point-wise functional gradients with respect
to training documents and candidate entity se-
quences. This is challenging, due to the exponen-
tial output structure search space. First, we are not
able to enumerate all possible candidate entity se-
quences. Second, computing the conditional prob-
abilities shown in Eq. 6 is intractable, as it is pro-
hibitively expensive to compute the partition func-
tionZ(x) in Eq. 2. Beam search can be used to ad-
dress these problems. We can compute point-wise
functional gradients for candidate entity sequences
in the beam, and approximately compute the parti-
tion function by summing over the elements in the
beam.

In this section, we present a bidirectional beam
search training algorithm that always keeps the
gold sequence in the beam. The algorithm is tai-
lored to SGTB, and improves standard training
methods in two aspects: (1) it reduces model vari-
ance by collecting more point-wise function gra-
dients to train a regression tree; (2) it leverages
information from both past and future to conduct
better local search.

3.1 Beam search with gold path
The early update (Collins and Roark, 2004) and
LaSO (Daumé III and Marcu, 2005; Xu and Fern,
2007) strategies are widely adopted with beam
search for updating model parameters in previous
work. Both methods keep track of the location of
the gold path in the beam while decoding a train-
ing sequence. A gradient update step will be taken
if the gold path falls out of the beam at a specific
time step t or after the last step T . Adapting the
strategies to SGTB training is straightforward. We
will compute point-wise functional gradients for
all candidate entity sequences after time step T or
when the gold sequence falls out the beam. Both
early update and LaSO are typically applied to on-
line learning scenarios, in which model parame-
ters are updated after passing one or a few training
sequences.

SGTB training, however, fits the batch learning
paradigm. In each training epoch, a SGTB model
will be updated only once using the regression
tree model fit on the point-wise negative functional
gradients. The gradients are calculated with re-
spect to the output sequences obtained from beam
search. We propose a simple training strategy that
computes and collects point-wise functional gra-
dients at every step of a training sequence. In
addition, instead of passively monitoring the gold
path, we always keep the gold path in the beam
to ensure that we have valid functional gradients
at each time step. The new beam search training
method, Beam Search with Gold path (BSG), gen-
erates much more point-wise functional gradients
than early update or LaSO, which can reduce the
variance of the auxiliary regression tree model. As
a result, SGTB trained with BSG consistently out-
performs early update or LaSO in our exploratory
experiments, and it also requires fewer training
epochs to converge.2

3.2 Bidirectional beam search
During beam search, if we consider a decision
made at time step t, the joint probability p(y|x)
can be factorized around t as follows:

p(y|x) = p(y1:t−1|x) · p(yt|y1:t−1,x)
·p(yt+1:T |yt,y1:t−1,x).

(8)

Traditional beam search performs inference in
a unidirectional (left-to-right) fashion. Since the
beam search at time step t considers only the beam
sequences that were committed to so far, {y1:t−1},
it effectively approximates the above probability
by assuming that all futures are equally likely, i.e.
p(yt+1:T |yt,y1:t−1,x) is uniform. Therefore, at
any given time, there is no information from the
future when incorporating the global structure.

In this work, we adopt a Bidirectional Beam
Search (BiBS) methodology that incorporates
multiple beams to take future information into ac-
count (Sun et al., 2017). It makes two simplify-
ing assumptions that better approximate the joint
probability above while remaining tractable: (1)
future predictions are independent of past predic-
tions given yt; (2) p(yt) is uniform. These yield
the following approximation:

p(yt+1:T |yt,y1:t−1,x) = p(yt+1:T |yt,x)
∝ p(yt|yt+1:T ,x) · p(yt+1:T |x).

(9)

2Early update and LaSO perform similarly, thus we only
report results for early update in § 5.

780



Substituting this back into Eq. 8 therefore yields:

p(y|x) ∝ p(y1:t−1|x) · p(yt|y1:t−1,x)
·p(yt|yt+1:t,x) · p(yt+1:T |x),

(10)

which decomposes into multiplication of a for-
ward probability and a backward probability.
In (Sun et al., 2017), these are retrieved from for-
ward and backward recurrent networks, whereas
in our work we use the joint scores (log proba-
bilities shown in Eq. 1) computed for partial se-
quences from forward and backward beams.

Algorithm 1: Bidirectional Beam Search with
Gold path (BiBSG)

Input : input document x, candidate sequences {y},
joint scoring function S(x,yt1:t2)

Output: beam sequence set C
C ← ∅
while not converged do

// forward beam search
for t = 1, · · · , T do

C(F ) ← top-By1:t [S(x,y1:t) + S(x,yT :t)]
// add gold subsequence
C(F ) ← C(F ) ∪ {y∗1:t}
C ← C ∪ C(F )

end
// backward beam search
for t = T, · · · , 1 do

C(B) ← top-ByT :t [S(x,yT :t) + S(x,y1:t)]
// add gold subsequence
C(B) ← C(B) ∪ {y∗T :t}
C ← C ∪ C(B)

end
end

The full inference algorithm, Bidirectional
Beam Search with Gold path (BiBSG), is pre-
sented in Alg. 1. When performing the forward
pass to update the forward beam, forward joint
scores, S(x,y1:t), are computed with respect to
current forward beam, and backward joint scores,
S(x,yT :t), are computed with respect to previous
backward beam. A similar procedure is used for
the backward pass. The search converges very
fast, and we use two rounds of bidirectional search
as a good approximation. Finally, SGTB-BiBSG
compares the conditional probabilities p(y(·)|x) of
the best scoring output sequences y(F) and y(B) ob-
tained from the forward and backward beams. The
final prediction is the sequence with the higher
conditional probability score.

4 Implementation

We provide implementation details of our SGTB
systems, including entity candidate generation,

adopted local and global features, and some efforts
to make training and inference faster.

4.1 Candidate selection

We use a mention prior p̂(y|x) to select en-
tity candidates for a mention x. Follow-
ing Ganea and Hofmann (2017), the prior is com-
puted by averaging mention prior probabilities
built from mention-entity hyperlink statistics from
Wikipedia3 and a large Web corpus (Spitkovsky
and Chang, 2012). Given a mention, we select the
top 30 entity candidates according to p̂(y|x).

We also use a simple heuristic proposed
by Ganea and Hofmann (2017) to improve candi-
date selection for persons: for a mention x, if there
are mentions of persons that contain x as a contin-
uous subsequence of words, then we consider the
candidate set obtained from the longest mention
for the mention x.

4.2 Features

The feature function φ(x, yt,y1:t−1) can be de-
composed into the summation of a local feature
function φL(x, yt) and a global feature function
φG(yt,y1:t−1).

Local features We consider standard local fea-
tures that have been used in prior work, includ-
ing mention priors p(y|x) obtained from differ-
ent resources; entity popularity features based on
Wikipedia page view count statistics;4 named en-
tity recognition (NER) type features given by an
in-house NER system trained on the CoNLL 2003
NER data (Tjong Kim Sang and De Meulder,
2003); entity type features based on Freebase type
information; and three textual similarity features
proposed by Yamada et al. (2016).5

Global features Three features are utilized to
characterize entity-entity relationships: entity-
entity co-occurrence counts obtained from
Wikipedia, and two cosine similarity scores
between entity vectors based on entity embed-
dings from (Ganea and Hofmann, 2017) and
Freebase entity embeddings released by Google6

3We use a Wikipedia snapshot as of Feb. 2017.
4We obtain the statistics of Feb. 2017 and Dec. 2011

from https://dumps.wikimedia.org/other/
pagecounts-ez/merged/ .

5We obtain embeddings jointly trained for words and en-
tities from (Ganea and Hofmann, 2017).

6https://code.google.com/archive/p/
word2vec/

781



respectively. We denote the entity-entity features
between entities yt and yt′ as φE(yt, yt′).

At step t of a training sequence, we quantify the
coherence of yt with respect to previous decisions
y1:y−1 by first extracting entity-entity features be-
tween yt and yt′ where 1 ≤ t′ ≤ t − 1, and then
aggregating the information to have a global fea-
ture vector φG(yt,y1:t−1) of a fixed length:

φG(yt,y1:t−1) =
t−1∑

t′=1

φE(yt, yt′)

t− 1

⊕ t−1max
t′=1

φE(yt, yt′),

where ⊕ denotes concatenation of vectors.

4.3 Efficiency

Global models are powerful and effective, but of-
ten at a cost of efficiency. We discuss ways to
speed up training and inference for SGTB models.

Many of the adopted features such as mention
priors and entity-entity co-occurrences can be ex-
tracted once and retrieved later with just a hash
map lookup. The most expensive features are the
cosine similarity features based on word and en-
tity embeddings. By normalizing the embeddings
to have a unit norm, we can obtain the similarity
features using dot products. We find this simple
preprocessing makes feature extraction faster by
two orders of magnitude.

SGTB training can be easily parallelized, as the
computation of functional gradients are indepen-
dent for different documents. During each train-
ing iteration, we randomly split training docu-
ments into different partitions, and then calculate
the point-wise functional gradients for documents
of different partitions in parallel.

5 Experiments

In this section, we evaluate SGTB on some of
the most popular datasets for ED. After describing
the experimental setup, we compare SGTB with
previous state-of-the-art (SOTA) ED systems and
present our main findings in § 5.3.

5.1 Data

We use six publicly available datasets to validate
the effectiveness of SGTB. AIDA-CoNLL (Hof-
fart et al., 2011) is a widely adopted dataset for
ED based on the CoNLL 2003 NER dataset (Tjong
Kim Sang and De Meulder, 2003). It is

Dataset # mention # doc # mentionper doc

AIDA-train 18,448 946 19.5
AIDA-dev 4,791 216 22.1
AIDA-test 4,485 231 19.4

AQUAINT 727 50 14.5
MSNBC 656 20 32.8
ACE 257 36 7.1

CWEB 11,154 320 34.8
WIKI 6,821 320 21.3

Table 1: Statistics of the ED datasets used in this work.

further split into training (AIDA-train), de-
velopment (AIDA-dev), and test (AIDA-test)
sets.7 AQUAINT (Milne and Witten, 2008),
MSNBC (Cucerzan, 2007), and ACE (Ratinov
et al., 2011) are three datasets for Wikification,
which also contain Wikipedia concepts beyond
named entities. These datasets were recently
cleaned and updated by Guo and Barbosa (2016).
WIKI and CWEB are automatically annotated
datasets built from the ClueWeb and Wikipedia
corpora by Guo and Barbosa (2016). The statis-
tics of these datasets are available in Table 1.

5.2 Experimental settings
Following previous work (Guo and Barbosa, 2016;
Ganea and Hofmann, 2017), we evaluate our mod-
els on both in-domain and cross-domain testing
settings. In particular, we train our models on
AIDA-train set, tune hyperparameters on AIDA-
dev set, and test on AIDA-test set (in-domain test-
ing) and all other datasets (cross-domain testing).
We follow prior work and report in-KB accuracies
for AIDA-test and Bag-of-Title (BoT) F1 scores
for the other test sets.

Two AIDA-CoNLL specific resources have
been widely used in previous work. In order to
have fair comparisons with these works, we also
adopt them only for the AIDA datasets. First, we
use a mention prior obtained from aliases to candi-
date entities released by Hoffart et al. (2011) along
with the two priors described in § 4.1. Second, we
also experiment with PPRforNED, an entity can-
didate selection system released by Pershina et al.
(2015). It is unclear how candidates were pruned,
but the entity candidates generated by this sys-
tem have high recall and low ambiguity, and they
contribute to some of the best results reported for
AIDA-test (Yamada et al., 2016; Sil et al., 2018).

7AIDA-dev and AIDA-test are also referred as AIDA-a
and AIDA-b datasets in previous work.

782



Competitive systems We implement four com-
petitive ED systems, and three of them are based
on variants of our proposed SGTB algorithm.8

Gradient tree boosting is a local model that em-
ploys only local features to make independent de-
cisions for every entity mention. Note that our lo-
cal model is different from that presented by Ya-
mada et al. (2016), where they treat ED as binary
classification for each mention-entity pair. SGTB-
BS is a Structured Gradient Tree Boosting model
trained with Beam Search with early update strat-
egy. SGTB-BSG uses Beam Search with Gold path
training strategy presented in § 3.1. Finally, SGTB-
BiBSG exploits Bidirectional Beam Search with
Gold path to leverage information from both past
and future for better local search.

In addition, we compare against best published
results on all the datasets. To ensure fair compar-
isons, we group results according to candidate se-
lection system that different ED systems adopted.

Parameter tuning We tune all the hyperparam-
eters on the AIDA-dev set. We use recommended
hyperparameter values from scikit-learn to train
regression trees, except for the maximum depth
of the tree, which we choose from {3, 5, 8}. Af-
ter a set of preliminary experiments, we select the
beam size from {3, 4, 5, 6}. The best values for
the two hyperparameters are 3 and 4 respectively.
As mentioned in § 2, the learning rate is set to 1.
We train SGTB for at most 500 epochs (i.e., fit at
most 500 regression trees). During training, we
check the performance on the development set ev-
ery 25 epochs to perform early stopping. Training
takes 3 hours for SGTB-BS and SGTB-BSG, and
takes 9 hours for SGTB-BiBSG on 16 threads.

5.3 Results

In-domain results In-domain evaluation results
are presented in Table 2. As shown, SGTB
achieves much better performance than all pre-
viously published results. Specifically, SGTB-
BiBSG outperforms the previous SOTA sys-
tem (Ganea and Hofmann, 2017) by 0.8% accu-
racy, and improves upon the best published results
when employing the PPRforNED candidate selec-
tion system by 1.9% accuracy. Global informa-
tion is clearly useful, as it helps to boost the per-
formance by 2-4 points of accuracy, depending on
the candidate generation system. In terms of beam

8Our implementations are based on the scikit-learn pack-
age (Pedregosa et al., 2011).

System PPRforNED In-KB acc.

Published results
Lazic et al. (2015) 86.4
Huang et al. (2015) 86.6
Chisholm and Hachey (2015) 88.7
Ganea et al. (2016) 87.6
Guo and Barbosa (2016) 89.0
Globerson et al. (2016) 91.0
Yamada et al. (2016) 91.5
Ganea and Hofmann (2017) 92.2

Our implementations
Gradient tree boosting 88.4
SGTB-BS 91.7
SGTB-BSG 92.4
SGTB-BiBSG 93.0

Published results
Pershina et al. (2015) X 91.8
Yamada et al. (2016) X 93.1
Sil et al. (2018) X 94.0
Our implementations
Gradient tree boosting X 93.1
SGTB-BS X 95.1
SGTB-BSG X 95.5
SGTB-BiBSG X 95.9

Table 2: In-domain evaluation: in-KB accuracy results
on the AIDA-test set. Checked PPRforNED indicates
that the system uses PPRforNED (Pershina et al., 2015)
to select candidate entities.The best results are in bold.

search training strategies, BiBSG consistently out-
performs BSG and beam search with early update.
By employing more point-wise functional gradi-
ents to train the regression trees and leveraging
global information from both past and future to
carry on local search, BiBSG is able to find bet-
ter global solutions than alternative training strate-
gies.

Cross-domain results As presented in Table 3,
cross-domain experimental results are a little more
mixed. SGTB-BS and SGTB-BSG perform quite
competitively compared with SGTB-BiBSG. In a
cross-domain evaluation setting, the test data is
drawn from a different distribution as the train-
ing data. Therefore, less expressive models may
be preferred as they may learn more abstract
representations that will generalize better to out-
of-domain data. Nevertheless, our SGTB mod-
els achieve better performance than best pub-
lished results on three of the five popular ED
datasets. Specifically, SGTB-BS outperforms the
prior SOTA system by absolute 4% F1 on the
CWEB dataset, and SGTB-BiBSG performs con-
sistently well across different datasets.

783



System AQUAINT MSNBC ACE CWEB WIKI

Published results
Fang et al. (2016) 88.8 81.2 85.3 - -
Ganea et al. (2016) 89.2 91.0 88.7 - -
Milne and Witten (2008) 85.0 78.0 81.0 64.1 81.7
Hoffart et al. (2011) 56.0 79.0 80.0 58.6 63.0
Ratinov et al. (2011) 83.0 75.0 82.0 56.2 67.2
Cheng and Roth (2013) 90.0 90.0 86.0 67.5 73.4
Guo and Barbosa (2016) 87.0 92.0 88.0 77.0 84.5
Ganea and Hofmann (2017) 88.5 93.7 88.5 77.9 77.5

Our implementations
Gradient tree boosting 90.3 91.1 89.2 78.8 75.0
SGTB-BS 90.5 92.4 88.9 81.7 76.4
SGTB-BSG 89.4 92.5 88.6 81.7 78.4
SGTB-BiBSG 89.9 92.6 88.5 81.8 79.2

Table 3: Cross-domain evaluation: Bag-of-Title (BoT) F1 results on ED datasets. The best results are in bold.

6 Related work

Entity disambiguation Most ED systems con-
sist of a local component that models relatedness
between a mention and a candidate entity, as well
as a global component that produces coherent en-
tity assignments for all mentions within a docu-
ment. Recent research has largely focused on joint
resolution of entities, which is usually performed
by maximizing the global topical coherence be-
tween entities. As discussed above, directly op-
timizing the coherence objective is computation-
ally intractable, and several heuristics and approx-
imations have been proposed to address the prob-
lem. Hoffart et al. (2011) use an iterative heuristic
to remove unpromising mention-entity edges. Ya-
mada et al. (2016) employ a two-stage approach,
in which global information is incorporated in
the second stage based on local decisions from
the first stage. Approximate inference techniques
have been widely adopted for ED. Cheng and Roth
(2013) use an integer linear program (ILP) solver.
Belief propagation (BP) and its variant loopy be-
lief propagation (LBP) have been used by Ganea
et al. (2016) and Ganea and Hofmann (2017) re-
spectively. We employ another standard approx-
imate inference algorithm, beam search, in this
work. To make beam search a better fit for SGTB
training, we propose BiBSG that improves beam
search training on stability and effectiveness.

Structured gradient tree boosting Gradient
tree boosting has been used in some of the most
accurate systems for a variety of classification and
regression problems (Babenko et al., 2011; Wu
et al., 2010; Yamada et al., 2016). However, gradi-
ent tree boosting is seldom studied in the context

of structured learning, with only a few exceptions.
Dietterich et al. (2004) propose TreeCRF that re-
places the linear scoring function of a CRF with
a scoring function given by a gradient tree boost-
ing model. TreeCRF achieves comparable or bet-
ter results than CRF on some linear chain struc-
tured prediction problems. Bagnell et al. (2007)
extend the Maximum Margin Planning (MMP;
Ratliff et al., 2006) algorithm to structured predic-
tion problems by learning new features using gra-
dient boosting machines. Yang and Chang (2015)
present a general SGTB framework that is flex-
ible in the choice of loss functions and specific
structures. They also apply SGTB to the task of
tweet entity linking with a special non-overlapping
structure. By decomposing the structures into lo-
cal substructures, exact inference is tractable in all
the aforementioned works. Our work shows that
we can train SGTB models efficiently and effec-
tively even with approximate inference. This ex-
tends the utility of SGTB models to a wider range
of interesting structured prediction problems.

7 Conclusion and future work

In this paper, we present a structured gradient tree
boosting model for entity disambiguation. Entity
coherence modeling is challenging, as exact in-
ference is prohibitively expensive due to the pair-
wise entity relatedness terms in the objective func-
tion. We propose an approximate inference al-
gorithm, BiBSG, that is designed specifically for
SGTB to solve this problem. Experiments on
benchmark ED datasets suggest that the expressive
SGTB models are extremely good at dealing with
the task of ED. SGTB significantly outperforms
all previous systems on the AIDA-CoNLL dataset,

784



and it also achieves SOTA results on many other
ED datasets even in the cross-domain evaluation
setting. SGTB is a family of structured learning
algorithms that can be potentially applied to other
core NLP tasks. In the future, we would like to in-
vestigate the effectiveness of SGTB on other infor-
mation extraction tasks, such as relation extraction
and coreference resolution.

8 Acknowledgments

We thank Prabhanjan Kambadur and other people
in the Bloomberg AI team for their valuable com-
ments on earlier version of this paper. We also
thank the NAACL reviewers for their helpful feed-
back. This work also benefitted from discussions
with Mark Dredze and Karl Stratos.

References
Daniel Andor, Chris Alberti, David Weiss, Aliaksei

Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
malized transition-based neural networks. In Pro-
ceedings of the Association for Computational Lin-
guistics (ACL).

Boris Babenko, Ming-Hsuan Yang, and Serge Be-
longie. 2011. Robust object tracking with online
multiple instance learning. Pattern Analysis and
Machine Intelligence, IEEE Transactions on .

JA Bagnell, Joel Chestnutt, David M Bradley, and
Nathan D Ratliff. 2007. Boosting structured predic-
tion for imitation learning. In Neural Information
Processing Systems (NIPS).

Jonathan Berant, Andrew Chou, Roy Frostig, and
Percy Liang. 2013. Semantic parsing on freebase
from question-answer pairs. In Proceedings of Em-
pirical Methods for Natural Language Processing
(EMNLP).

Xiao Cheng and Dan Roth. 2013. Relational inference
for wikification. In Proceedings of Empirical Meth-
ods for Natural Language Processing (EMNLP).

Andrew Chisholm and Ben Hachey. 2015. Entity dis-
ambiguation with web links. Transactions of the As-
sociation for Computational Linguistics 3.

Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the Association for Computational Linguis-
tics (ACL).

Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In Proceed-
ings of Empirical Methods for Natural Language
Processing (EMNLP).

Hal Daumé III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction. In Proceedings
of the International Conference on Machine Learn-
ing (ICML).

Thomas G Dietterich, Adam Ashenfelter, and Yaroslav
Bulatov. 2004. Training conditional random fields
via gradient tree boosting. In Proceedings of
the International Conference on Machine Learning
(ICML).

Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation for
knowledge base population. In Proceedings of the
Association for Computational Linguistics (ACL).

Jeffrey L Elman. 1990. Finding structure in time. Cog-
nitive science 14(2).

Wei Fang, Jianwen Zhang, Dilin Wang, Zheng Chen,
and Ming Li. 2016. Entity disambiguation by
knowledge and text jointly embedding. In Proceed-
ings of the Conference on Natural Language Learn-
ing (CoNLL).

Octavian-Eugen Ganea, Marina Ganea, Aurelien Luc-
chi, Carsten Eickhoff, and Thomas Hofmann. 2016.
Probabilistic bag-of-hyperlinks model for entity
linking. In Proceedings of the International Con-
ference on World Wide Web (WWW).

Octavian-Eugen Ganea and Thomas Hofmann. 2017.
Deep joint entity disambiguation with local neural
attention. In Proceedings of Empirical Methods for
Natural Language Processing (EMNLP).

Amir Globerson, Nevena Lazic, Soumen Chakrabarti,
Amarnag Subramanya, Michael Ringaard, and Fer-
nando Pereira. 2016. Collective entity resolution
with multi-focal attention. In Proceedings of the As-
sociation for Computational Linguistics (ACL).

Stephen Guo, Ming-Wei Chang, and Emre Kiciman.
2013. To link or not to link? a study on end-to-end
tweet entity linking. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).

Zhaochen Guo and Denilson Barbosa. 2016. Robust
named entity disambiguation with random walks.
Semantic Web .

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation 9(8).

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fürstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In Proceedings of Empirical Meth-
ods for Natural Language Processing (EMNLP).

Hongzhao Huang, Larry Heck, and Heng Ji. 2015.
Leveraging deep neural networks and knowledge
graphs for entity disambiguation. arXiv preprint
arXiv:1504.07678 .

785



Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges.
In Proceedings of the Association for Computational
Linguistics (ACL).

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning (ICML).

Nevena Lazic, Amarnag Subramanya, Michael Ring-
gaard, and Fernando Pereira. 2015. Plato: A selec-
tive context model for entity resolution. Transac-
tions of the Association for Computational Linguis-
tics .

David Milne and Ian H Witten. 2008. Learning to link
with wikipedia. In Proceedings of the ACM con-
ference on Information and knowledge management
(CIKM).

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the As-
sociation for Computational Linguistics (ACL).

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al. 2011. Scikit-learn:
Machine learning in python. Journal of Machine
Learning Research .

Maria Pershina, Yifan He, and Ralph Grishman. 2015.
Personalized page rank for named entity disam-
biguation. In Proceedings of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL).

Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to wikipedia. In Proceedings of the As-
sociation for Computational Linguistics (ACL).

Nathan D Ratliff, J Andrew Bagnell, and Martin A
Zinkevich. 2006. Maximum margin planning. In
Proceedings of the International Conference on Ma-
chine Learning (ICML).

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. Machine learning and knowledge
discovery in databases .

Avirup Sil, Gourab Kundu, Radu Florian, and Wael
Hamza. 2018. Neural cross-lingual entity linking.
In Proceedings of the National Conference on Arti-
ficial Intelligence (AAAI).

Noah A Smith and Mark Johnson. 2007. Weighted and
probabilistic context-free grammars are equally ex-
pressive. Computational Linguistics .

Valentin I Spitkovsky and Angel X Chang. 2012. A
cross-lingual dictionary for english wikipedia con-
cepts. In Proceedings of the Language Resources
and Evaluation Conference (LREC).

Qing Sun, Stefan Lee, and Dhruv Batra. 2017. Bidi-
rectional beam search: Forward-backward inference
in neural sequence models for fill-in-the-blank im-
age captioning. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition
(CVPR).

Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition.
In Proceedings of the North American Chapter
of the Association for Computational Linguistics
(NAACL).

Qiang Wu, Christopher JC Burges, Krysta M Svore,
and Jianfeng Gao. 2010. Adapting boosting for in-
formation retrieval measures. Information Retrieval
.

Yuehua Xu and Alan Fern. 2007. On learning linear
ranking functions for beam search. In Proceedings
of the International Conference on Machine Learn-
ing (ICML).

Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and
Yoshiyasu Takefuji. 2016. Joint learning of the em-
bedding of words and entities for named entity dis-
ambiguation. In Proceedings of the Conference on
Natural Language Learning (CoNLL).

Yi Yang and Ming-Wei Chang. 2015. S-mart: Novel
tree-based structured learning algorithms applied to
tweet entity linking. In Proceedings of the Associa-
tion for Computational Linguistics (ACL).

Scott Wen-tau Yih, Ming-Wei Chang, Xiaodong He,
and Jianfeng Gao. 2015. Semantic parsing via
staged query graph generation: Question answering
with knowledge base. In Proceedings of the Associ-
ation for Computational Linguistics (ACL).

786


