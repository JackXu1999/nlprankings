



















































What can we learn from Semantic Tagging?


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4881–4889
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

4881

What can we learn from Semantic Tagging?

Mostafa Abdou†?, Artur Kulmizev?, Vinit Ravishankar§,
Lasha Abzianidze?, and Johan Bos?

abdou@di.ku.dk, a.kulmizev@student.rug.nl, vinit.ravishankar@gmail.com,
{l.abzianidze, johan.bos}@rug.nl
CLCG, University of Groningen?

CoAStaL DIKU, University of Copenhagen†

LTG, University of Oslo§

Abstract
We investigate the effects of multi-task learn-
ing using the recently introduced task of se-
mantic tagging. We employ semantic tagging
as an auxiliary task for three different NLP
tasks: part-of-speech tagging, Universal De-
pendency parsing, and Natural Language In-
ference. We compare full neural network shar-
ing, partial neural network sharing, and what
we term the learning what to share setting
where negative transfer between tasks is less
likely. Our findings show considerable im-
provements for all tasks, particularly in the
learning what to share setting, which shows
consistent gains across all tasks.

1 Introduction

Multi-task learning (MTL) is a recently resurgent
approach to machine learning in which multiple
tasks are simultaneously learned. By optimising
the multiple loss functions of related tasks at once,
multi-task learning models can achieve superior
results compared to models trained on a single
task. The key principle is summarized by Caruana
(1998) as “MTL improves generalization by lever-
aging the domain-specific information contained
in the training signals of related tasks”. Neural
MTL has become an increasingly successful ap-
proach by exploiting similarities between Natural
Language Processing (NLP) tasks (Collobert and
Weston, 2008; Søgaard and Goldberg, 2016; Plank
et al., 2016). Our work builds upon Bjerva et al.
(2016), who demonstrate that employing seman-
tic tagging as an auxiliary task for Universal De-
pendency (McDonald et al., 2013) part-of-speech
tagging can lead to improved performance.

The objective of this paper is to investigate
whether learning to predict lexical semantic cat-
egories can be beneficial to other NLP tasks. To
achieve this we augment single-task models (ST)1

1We replicate models which perform at or close to the

with an additional classifier to predict semantic
tags and jointly optimize for both the original task
and the auxiliary semantic tagging task. Our hy-
pothesis is that learning to predict semantic tags
as an auxiliary task can improve performance of
single-task systems. We believe that this is, among
other factors, due to the following:

• Providing the main task’s model with a useful
inductive bias, encouraging it to prefer repre-
sentations that lead to semantically plausible
hypotheses over those that are not.

• Putting the focus of the main task model’s
attention on features that actually matter by
providing additional evidence for the rele-
vance or irrelevance of those features.

• Reducing the risk of overfitting by mini-
mizing the model’s Rademacher Complexity2

Representations which are learned for multi-
ple tasks have been shown to generalize bet-
ter (Baxter et al., 2000).

We test our hypothesis on three disparate NLP
tasks: (i) Universal Dependency part-of-speech
tagging (UPOS), (ii) Universal Dependency pars-
ing (UD DEP), a complex syntactic task; and (iii)
Natural Language Inference (NLI), a complex task
requiring deep natural language understanding.

2 Background and Related work

2.1 Semantic Tagging

Semantic tagging (Bjerva et al., 2016; Abzianidze
and Bos, 2017) is the task of assigning language-
neutral semantic categories to words. It is de-
signed to overcome a lack of semantic information
syntax-oriented part-of-speech tagsets, such as the

state-of-the-art. Our choice of models is based on replica-
bility.

2The ability to fit random noise.



4882

Figure 1: Our three multi-task learning settings: (A) fully shared networks, (B) partially shared networks,
and (C) Learning What to Share. Layers are mathematically denoted by vectors and the connections
between them, represented by arrows, are mathematically denoted by matrices of weights. S indicates a
shared layer, P a private layer, and X a layer with shared and private subspaces.

Penn Treebank tagset(Marcus et al., 1993), usu-
ally have. Such tagsets exclude important seman-
tic distinctions, such as negation and modals, types
of quantification, named entity types, and the con-
tribution of verbs to tense, aspect, or event.

The semantic tagset is language-neutral, ab-
stracts over part-of-speech and named-entity
classes, and includes fine-grained semantic in-
formation. The tagset consists of 80 seman-
tic tags grouped in 13 coarse-grained classes.
The tagset originated in the Parallel Meaning
Bank (PMB) project (Abzianidze et al., 2017),
where it contributes to compositional semantics
and cross-lingual projection of semantic represen-
tations. Recent work has highlighted the utility of
the tagset as a conduit for evaluating the seman-
tics captured by vector representations (Belinkov
et al., 2018), or employed it in an auxiliary tagging
task (Bjerva et al., 2016), as we do in this work.

2.2 Learning What to Share

Recently, there has been an increasing interest in
the development of models which are trained to
learn what to (and what not to) share between a
set of tasks, with the general aim of preventing
negative transfer when the tasks are not closely re-
lated (Meyerson and Miikkulainen, 2017; Ruder
et al., 2017; Lu et al., 2017; Misra et al., 2016).
Our Learning What to Share setting is based on
this idea and closely related to Liu et al. (2016)’s
shared layer architecture.

Specifically, a layer ~hX which is shared be-
tween the main task and the auxiliary task is split
into two subspaces: a shared subspace ~hXS and
a private subspace ~hXP . The interaction between

the shared subspaces is modulated via a sigmoidal
gating unit applied to a set of learned weights, as
seen in Equations (1) and (2) where ~hXS(main) and
~hXS(aux) are the main and auxiliary tasks’ shared
layers, Wa→m and Wm→a are learned weights,
and σ is a sigmoidal function.
~hXS(main) =

~hXS(main)σ(
~hXS(aux)Wa→m) (1)

~hXS(aux) =
~hXS(aux)σ(

~hXS(main)Wm→a) (2)

Unlike Liu et al. (2016)’s Shared-Layer Archi-
tecture, in our setup each task has its own shared
subspace rather than one common shared layer.
This enables the sharing of different parameters
in each direction (i.e., from main to auxiliary
task and from auxiliary to main task), allowing
each task to choose what to learn from the other,
rather than having “one shared layer to capture the
shared information for all the tasks” as in Liu et al.
(2016).

3 Multi-Task Learning Settings

We implement three neural MTL settings, shown
in Figure 1. They differ in the way the network’s
parameters are shared between the tasks:

• Fully shared network (FSN): All hidden
layers are entirely shared among the tasks,
each task has a separate output layer. The
transformation of our input vector ~x into a
shared hidden layer ~hS is described by Equa-
tion (3):

~hS = σ(~xW ) (3)

• Partially shared network (PSN): A subset
of hidden layers is shared among the tasks;



4883

each task has at least one private hidden
layer and a separate output layer. The trans-
formation of a shared hidden layer ~hS into
private hidden layers, denoted by ~hP(main)
and ~hP(aux) is described in Equations (4)
and (5).

~hP(main) = σ(
~hSW(main)) (4)

~hP(aux) = σ(
~hSW(aux)) (5)

• Learning What to Share (LWS): Each task
has a dedicated set of hidden layers. For
sharing, a hidden layer is split into a shared
subspace and a private subspace. A gating
unit modulates the transfer of information
between the shared subspaces as shown in
Equations (1) and (2).

4 Data

In the UPOS tagging experiments, we utilize the
UD 2.0 English corpus (Nivre et al., 2017) for the
POS tagging and the semantically tagged PMB
release 0.1.0 (sem-PMB)3 for the MTL settings.
Note that there is no overlap between the two
datasets. Conversely, for the UD DEP and NLI
experiments there is a complete overlap between
the datasets of main and auxiliary tasks, i.e., each
instance is labeled with both the main task’s labels
and semantic tags. We use the Stanford POS Tag-
ger (Toutanova et al., 2003) trained on sem-PMB
to tag the UD corpus and NLI datasets with seman-
tic tags, and then use those assigned tags for the
MTL settings of our dependency parsing and NLI
models. We find that this approach leads to better
results when the main task is only loosely related
to the auxiliary task. The UD DEP experiments
use the English UD 2.0 corpus, and the NLI ex-
periments use the SNLI (Bowman et al., 2015) and
SICK-E4 datasets (Marelli et al., 2014). The pro-
vided train, development, and test splits are used
for all datasets. For sem-PMB, the silver and gold
parts are used for training and testing respectively.

5 Experiments

We run four experiments for each of the four tasks
(UPOS, UD DEP, SNLI, SICK-E), one using the
ST model and one for each of the three MTL set-
tings. Each experiment is run five times, and the
average of the five runs is reported. We briefly de-
scribe the ST models and refer the reader to the

3http://pmb.let.rug.nl/data.php
4SICK-E refers to the entailment part of the SICK dataset.

original work for further details due to a lack of
space.5 For reproducibility, detailed diagrams of
the MTL models for each task and their hyperpa-
rameters can be found in Appendix A.

5.1 Universal Dependency POS Tagging

Our tagging model uses a basic contextual one-
layer bi-LSTM (Hochreiter and Schmidhuber,
1997) that takes in word embeddings and produces
a sequence of recurrent states which can be viewed
as contextualized representations. The recurrent
rn state from the bi-LSTM corresponding to each
time-step tn is passed through a dense layer with
a softmax activation to predict the token’s tag.

In each of the MTL settings a softmax classifier
is added to predict a token’s semantic tag and the
model is then jointly trained on the concatenation
of the sem-PMB and UPOS tagging data to min-
imize the sum of softmax cross-entropy losses of
both the main (UPOS tagging) and auxiliary (se-
mantic tagging) tasks.

5.2 Universal Dependency Parsing

We employ a parsing model that is based on Dozat
and Manning (2016). The model’s embeddings
layer is a concatenation of randomly initialized
word embeddings6 and character-based word rep-
resentations added to pre-trained word embed-
dings, which are passed through a 4-layer stacked
bi-LSTM. Unlike Dozat and Manning (2016), our
model jointly learns to perform UPOS tagging and
parsing, instead of treating them as separate tasks.
Therefore, instead of tag embeddings, we add a
softmax classifier to predict UPOS tags after the
first bi-LSTM layer. The outputs from that layer
and the UPOS softmax prediction vectors are both
concatenated to the original embedding layer and
passed to the second bi-LSTM layer. The output
of the last bi-LSTM is then used as input for four
dense layers with a ReLU activation, producing
four vector representations: a word as a depen-
dent seeking its head; a word as a head seeking
all its dependents; a word as a dependent decid-
ing on its label; a word as head deciding on the
labels of its dependents. These representations are
then passed to biaffine and affine softmax classi-
fiers to produce a fully-connected labeled prob-
abilistic dependency graph (Dozat and Manning,

5This applies to UD DEP and NLI only, as the POS tagger
is not based on any one particular work.

6This replaces the holistic word embeddings for frequent
words in Dozat and Manning (2016).

http://pmb.let.rug.nl/data.php


4884

2016). Finally, a non-projective maximum span-
ning tree parsing algorithm (Chu, 1965; Edmonds,
1967) is used to obtain a well-formed dependency
tree.7

Similarly to UPOS tagging, an additional soft-
max classifier is used to predict a token’s seman-
tic tag in each of the MTL settings, as both tasks
are jointly learned. In the FSN setting, the 4-layer
stacked bi-LSTM is entirely shared. In the PSN
setting the semantic tags are predicted from the
second layer’s hidden states, and the final two lay-
ers are devoted to the parsing task. In the LWS
setting, the first two layers of the bi-LSTM are
split into a private bi-LSTMprivate and a shared
bi-LSTMshared for each of the tasks with the inter-
action between the shared subspaces being modu-
lated via a gating unit. Then, two bi-LSTM layers
that are devoted to parsing only are stacked on top.

5.3 Natural Language Inference

We base our NLI model on Chen et al. (2017)’s
Enhanced Sequential Inference Model which uses
a bi-LSTM to encode the premise and hypothe-
sis, computes a soft-alignment between premise
and hypothesis’ representations using an attention
mechanism, and employs an inference composi-
tion bi-LSTM to compose local inference infor-
mation sequentially.8 The MTL settings are im-
plemented by adding a softmax classifier to pre-
dict semantic tags at the level of the encoding bi-
LSTM, with rest of the model unaltered. In the
FSN setting, the hidden states of the encoding bi-
LSTM are directly passed as input to the soft-
max classifier. In the PSN setting an earlier bi-
LSTM layer is used to predict the semantic tags
and the output from that is passed on to the en-
coding bi-LSTM which is stacked on top. This
follows Hashimoto et al. (2016)’s hierarchical ap-
proach. In the LWS setting, a bi-LSTM layer with
private and shared subspaces is used for semantic
tagging and for the ESIM model’s encoding layer.
In all MTL settings, the bi-LSTM used for seman-
tic tagging is pre-trained on the sem-PMB data.

6 Results and Discussion

Results for all tasks are shown in Table 1. In line
with Bjerva et al. (2016)’s findings, the FSN set-

7This is recommended but not implemented by Dozat
et al. (2017).

8We do not implement the additional tree-LSTM model
used in Chen et al. (2017) as we focus on the effect of MTL
with semantic tagging rather than on absolute performance.

ting leads to an improvement for UPOS tagging.
POS tagging, a sequence labeling task, can be seen
as the most closely related to semantic tagging,
therefore negative transfer is minimal and the full
sharing of parameters is beneficial. Surprisingly,
the FSN setting also leads improvements for UD
DEP. Indeed, for UD DEP, all of the MTL models
outperform the ST model by increasing margins.
For the NLI tasks, however, there is a clear degra-
dation in performance.

The PSN setting shows mixed results and does
not show a clear advantage over FSN for UPOS
and UD DEP. This suggests that adding task-
specific layers after fully-shared ones does not al-
ways enable sufficient task specialization. For
the NLI tasks however, PSN is clearly preferable
to FSN, especially for the small-sized SICK-E
dataset where the FSN model fails to adequately
learn.

Model SNLI SICK-E UPOS UD DEP

ST 87.01 81.30 92.12 80.24 / 84.87
FSN 84.96 56.69 92.95 81.03 / 85.54
PSN 87.08 77.92 92.34 80.92 / 85.81
LWS 87.51 84.57 95.54 81.39 / 86.00

Table 1: Results for single-task models (ST), fully-
shared networks (FSN), partially-shared networks
(PSN), and learning what to share (LWS). All
scores are reported as accuracy, except UD DEP
for which we report LAS/UAS F1 score.

As a sentence-level task, NLI is functionally
dissimilar to semantic tagging. However, it is a
task which requires deep understanding of natural
language semantics and can therefore conceivably
benefit from the signal provided by semantic tag-
ging. Our results demonstrate that it is possible to
leverage this signal given a selective sharing setup
where negative transfer can be minimized. In-
deed, for the NLI tasks, only the LWS setting leads
to improvements over the ST models.9 The im-
provement is larger for the SICK-E task which has
a much smaller training set and therefore stands
to learn more from the semantic tagging signal.
For all tasks, it can be observed that the LWS
models outperform the rest of the models. This
is in line with our expectations with the findings
from previous work (Ruder et al., 2017; Liu et al.,

9Demonstrative examples of the SNLI models’ outputs
can be found in Appendix B.



4885

(a) Single-task network (b) Partially shared network (c) Learning what to share

Figure 2: Normalized semantic tag frequencies for all six sets of sentences. X - Y denotes the set of
sentences correctly classified by model X but misclassified by model Y.

2016) that selective sharing outperforms full net-
work and partial network sharing.

7 Analysis

In addition to evaluating performance directly, we
attempt to qualify how semtags affect performance
with respect to each of the SNLI MTL settings10.

7.1 Qualitative analyses

The fact that NLI is a sentence-level task, while se-
mantic tags are word-level annotations presents a
difficulty in measuring the effect of semantic tags
on the systems’ performance, as there is no one-
to-one correspondence between a correct label and
a particular semantic tag. We therefore employ
the following method in order to assess the con-
tribution of semantic tags. Given the performance
ranking of all our systems — FSN < ST <
PSN < LWS — we make a pairwise compar-
ison between the output of a superior system Ssup
and an inferior system Sinf . This involves tak-
ing the pairs of sentences that every Ssup classi-
fies correctly, but some Sinf does not. Given that
FSN is the worst performing system and, as such,
has no ‘worse’ system for comparison, we are left
with six sets of sentences: ST-FSN, PSN-FSN,
PSN-ST, LWS-PSN, LWS-ST, and LWS-FSN.
To gain insight as to where a given system Ssup
performs better than a given Sinf , we then sort
each comparison sentence set by the frequency of
semtags predicted therein, which are normalized
by dividing by their frequency in the full SNLI test
set.

We notice interesting patterns, visible in Fig-
ure 2. Specifically, PSN appears markedly bet-
ter at sentences with named entities (ART, PER,

10We also provide a per-label report of the standard preci-
sion and recall metrics in Appendix B.

GEO, ORG) and temporal entities (DOM) than both
ST and the FSN. Marginal improvements are also
observed for sentences with negation and reflexive
pronouns. The LWS setting continues this pattern,
with additional improvements observable for sen-
tences with the HAP tag for names of events, SST
for subsective attributes, and the ROL tag for role
nouns.

7.2 Contribution of semantic tagging

To assess the contribution of the semantic tagging
auxiliary task independent of model architecture
and complexity we run three additional SNLI ex-
periments — one for each MTL setting — where
the model architectures are unchanged but the aux-
iliary tasks are assigned no weight (i.e. do not af-
fect the learning). The results confirm our previ-
ous findings that, for NLI, the semantic tagging
auxiliary task only improves performance in a se-
lective sharing setting, and hurts it otherwise: i)
the FSN system which had performed below ST
improves to equal it and ii) the PSN and LWS set-
tings both see a drop to ST-level performance.

8 Conclusions

We present a comprehensive evaluation of MTL
using a recently proposed task of semantic tag-
ging as an auxiliary task. Our experiments span
three types of NLP tasks and three MTL settings.
The results of the experiments show that employ-
ing semantic tagging as an auxiliary task leads to
improvements in performance for UPOS tagging
and UD DEP in all MTL settings. For the SNLI
tasks, requiring understanding of phrasal seman-
tics, the selective sharing setup we term Learning
What to Share holds a clear advantage. Our work
offers a generalizable framework for the evalua-
tion of the utility of an auxiliary task.



4886

References
Lasha Abzianidze, Johannes Bjerva, Kilian Evang,

Hessel Haagsma, Rik van Noord, Pierre Ludmann,
Duc-Duy Nguyen, and Johan Bos. 2017. The paral-
lel meaning bank: Towards a multilingual corpus of
translations annotated with compositional meaning
representations. In Proceedings of the 15th Confer-
ence of the European Chapter of the Association for
Computational Linguistics: Volume 2, Short Papers,
pages 242–247. Association for Computational Lin-
guistics.

Lasha Abzianidze and Johan Bos. 2017. Towards uni-
versal semantic tagging. In Proceedings of the 12th
International Conference on Computational Seman-
tics (IWCS 2017) – Short Papers, pages 1–6, Mont-
pellier, France.

Jonathan Baxter et al. 2000. A model of inductive bias
learning. J. Artif. Intell. Res.(JAIR), 12(149-198):3.

Yonatan Belinkov, Lluı́s Màrquez, Hassan Sajjad,
Nadir Durrani, Fahim Dalvi, and James Glass. 2018.
Evaluating layers of representation in neural ma-
chine translation on part-of-speech and semantic
tagging tasks. arXiv preprint arXiv:1801.07772.

Johannes Bjerva, Barbara Plank, and Johan Bos. 2016.
Semantic tagging with deep residual networks. In
Proceedings of COLING 2016, the 26th Interna-
tional Conference on Computational Linguistics:
Technical Papers, pages 3531–3541. The COLING
2016 Organizing Committee.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632–642. Association for Computational Linguis-
tics.

Rich Caruana. 1998. Multitask learning. In Learning
to learn, pages 95–133. Springer.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017. Enhanced lstm for
natural language inference. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 1657–1668.

Yoeng-Jin Chu. 1965. On the shortest arborescence of
a directed graph. Science Sinica, 14:1396–1400.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.

Timothy Dozat and Christopher D Manning. 2016.
Deep biaffine attention for neural dependency pars-
ing. arXiv preprint arXiv:1611.01734.

Timothy Dozat, Peng Qi, and Christopher D Manning.
2017. Stanford’s graph-based neural dependency
parser at the conll 2017 shared task. Proceedings
of the CoNLL 2017 Shared Task: Multilingual Pars-
ing from Raw Text to Universal Dependencies, pages
20–30.

Jack Edmonds. 1967. Optimum branchings. Journal
of Research of the national Bureau of Standards B,
71(4):233–240.

Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsu-
ruoka, and Richard Socher. 2016. A joint many-task
model: Growing a neural network for multiple nlp
tasks. arXiv preprint arXiv:1611.01587.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang.
2016. Recurrent neural network for text classi-
fication with multi-task learning. arXiv preprint
arXiv:1605.05101.

Yongxi Lu, Abhishek Kumar, Shuangfei Zhai,
Yu Cheng, Tara Javidi, and Rogerio Feris. 2017.
Fully-adaptive feature sharing in multi-task net-
works with applications in person attribute classi-
fication. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
5334–5343.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics, 19(2):313–330.

Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, Roberto Zamparelli,
et al. 2014. A sick cure for the evaluation of com-
positional distributional semantic models. In LREC,
pages 216–223.

Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Os-
car Täckström, et al. 2013. Universal dependency
annotation for multilingual parsing. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), volume 2, pages 92–97.

Elliot Meyerson and Risto Miikkulainen. 2017. Be-
yond shared hierarchies: Deep multitask learn-
ing through soft layer ordering. arXiv preprint
arXiv:1711.00108.

Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and
Martial Hebert. 2016. Cross-stitch networks for
multi-task learning. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 3994–4003.



4887

Joakim Nivre, Lars Ahrenberg Zeljko Agic, et al. 2017.
Universal dependencies 2.0. lindat/clarin digital li-
brary at the institute of formal and applied linguis-
tics, charles university, prague.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Barbara Plank, Anders Søgaard, and Yoav Goldberg.
2016. Multilingual part-of-speech tagging with
bidirectional long short-term memory models and
auxiliary loss. arXiv preprint arXiv:1604.05529.

Sebastian Ruder, Joachim Bingel, Isabelle Augenstein,
and Anders Søgaard. 2017. Sluice networks: Learn-
ing what to share between loosely related tasks.
arXiv preprint arXiv:1705.08142.

Anders Søgaard and Yoav Goldberg. 2016. Deep
multi-task learning with low level tasks supervised
at lower layers. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), volume 2, pages
231–235.

Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173–180. Association for Compu-
tational Linguistics.



4888

A MTL setting Diagrams, Preprocessing,
and Hyperparameters

UPOS Tagging

Figure 3a shows the three MTL models used for
UPOS. All hyperparameters were tuned with re-
spect to loss on the English UD 2.0 UPOS vali-
dation set. We trained for 20 epochs with a batch
size of 128 and optimized using Adam (Kingma
and Ba, 2014) with a learning rate of 0.0001. We
weight the auxiliary semantic tagging loss with
λ = 0.1. The pre-trained word embeddings we
used are GloVe embeddings (Pennington et al.,
2014) of dimension 100 trained on 6 billion tokens
of Wikipedia 2014 and Gigaword 5. We applied
dropout and recurrent dropout with a probability
of 0.3 to all bi-LSTMs.

UD DEP

Figure 3b shows the three MTL models for UD
DEP. We use the gold tokenization. All hyper-
parameters were tuned with respect to loss on the
English UD 2.0 UD validation set. We trained for
15 epochs with a batch size of 50 and optimized
using Adam with a learning rate of 2e − 3. We
weight the auxiliary semantic tagging loss with λ
= 0.5. The pre-trained word embeddings we use
are GloVe embeddings of dimension 100 trained
on 6 billion tokens of Wikipedia 2014 and Giga-
word 5. We applied dropout with a probability of
0.33 to all bi-LSTM, embedding layers, and non-
output dense layers.

(a) UPOS

(b) UD DEP

(c) NLI

Figure 3: The three MTL settings for each task. Layers dimensions are displayed in brackets.



4889

NLI
Figure 3c shows the three MTL models for NLI.
All hyperparameters were tuned with respect to
loss on the SNLI and SICK-E validation datasets
(separately). For the SNLI experiments, we
trained for 37 epochs with a batch size of 128. For
the SICK-E experiments, we trained for 20 epochs
with a batch size of 8. Note that the ESIM model
was designed for the SNLI dataset, therefore per-
formance is non-optimal for SICK-E. For both sets
of experiments: we optimized using Adam with a
learning rate of 0.00005; we weight the auxiliary
semantic tagging loss with λ = 0.1; the pre-trained
word embeddings we use are GloVe embeddings
of dimension 300 trained on 840 billion tokens of
Common Crawl; and we applied dropout and re-
current dropout with a probability of 0.3 to all bi-
LSTM, and non-output dense layers.

B SNLI model output analysis

Table 2 shows demonstrative examples from the
SNLI test set on which the Learning What to Share
(LWS) model outperforms the single-task (ST)
model. The examples cover all possible combi-
nations of entailment classes. Table 3 explains the
relevant part of the semantic tagset. Table 4 shows
the per-label precision and recall scores.

Tag category Semantic tag with examples

Anaphoric DEF: definite; the, loIT , derDE

HAS: possessive pronoun; my, her

Attribute COL: colour; red, crimson, light blue, chestnut brown
QUC: concrete quantity; two, six million, twice
IST: intersective; open, vegetarian, quickly
REL: relation; in, on, ’s, of, after

Unnamed entity CON: concept; dog, person

Logical ALT: alternative & repetitions; another, different, again
DIS: disjunction & exist. quantif.; a, some, any, or

Discourse SUB: subordinate relations; that, while, because

Events ENS: present simple; we walk, he walks
EPS: past simple; ate, went
EXG: untensed progessive; is running
EXS: untensed simple; to walk, is eaten, destruction

Tense & aspect NOW: present tense; is skiing, do ski, has skied, now

Table 3: The list of semantic tags found in Table 2.

Model Label
Entailment Contradiction Neutral

FSN 80.64/93.23 91.64/83.63 83.97/77.63
ST 84.86/91.54 90.10/88.04 84.74/79.71

PSN 84.08/92.70 91.17/88.63 85.96/79.15
LWS 84.45/92.87 91.74/88.91 85.95/79.65

Table 4: Per-label precision (left) and recall (right)
for all models.

Premise-hypothesis pairs ST LWS/GOLD

P: TheDEF gentlemanCON isNOW speakingEXS whileSUB theDEF othersALT areNOW listeningEXS N E
H: TheDEF manCON isNOW beingEXS givenEXS respectCON

P: MenCON wearingEXG hatsCON walkEXS onREL theDEF streetCON C E
H: TheDEF menCON havingEXS hatsCON onREL theirHAS headCON

P: ThreeQUC menCON inREL orangeIST suitsCON areNOW doingEXG streetCON repairsCON atREL nightCON N C
H: ThreeQUC menCON inREL orangeIST suitsCON escapedEPS fromREL prisonCON

P: ADIS toddlerCON sitsENS onREL aDIS stoneCON wallCON surroundedEXS byREL fallenEXS leavesCON E C
H: AnDIS childCON isNOW throwingEXG stonesCON atREL aDIS leafCON wallCON

P: AnDIS oldIST shoemakerCON inREL hisHAS factoryCON C N
H: TheDEF shoemakerCON isNOW wealthyIST

P: ADIS kidCON slidesCON downIST aDIS yellowCOL slideCON intoREL aDIS swimmingCON poolCON E N
H: TheDEF kidCON isNOW playingEXS atREL theDEF waterparkCON

Table 2: Examples of the entailment problems from SNLI which are incorrectly classified by the ST model
but correctly classified by the LWS model. Automatically assigned semantic tags are in superscript.


