



















































SimiHawk at SemEval-2016 Task 1: A Deep Ensemble System for Semantic Textual Similarity


Proceedings of SemEval-2016, pages 741–748,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

SimiHawk at SemEval-2016 Task 1: A Deep Ensemble System for Semantic
Textual Similarity

Peter Potash∗, William Boag∗, Alexey Romanov∗,
Vasili Ramanishka, Anna Rumshisky

Dept. of Computer Science
University of Massachusetts Lowell
198 Riverside St, Lowell, MA 01854

{ppotash,wboag,aromanov,vramanis,arum}@cs.uml.edu

Abstract

This paper describes the SimiHawk system
submission from UMass Lowell for the core
Semantic Textual Similarity task at SemEval-
2016. We built four systems: a small feature-
based system that leverages word alignment
and machine translation quality evaluation
metrics, two end-to-end LSTM-based sys-
tems, and an ensemble system. The LSTM-
based systems used either a simple LSTM
architecture or a Tree-LSTM structure. We
found that of the three base systems, the
feature-based model obtained the best results,
outperforming each LSTM-based model’s
correlation by .06. Ultimately, the ensemble
system was able to outperform the base sys-
tems substantially, obtaining a weighted Pear-
son correlation of 0.738, and placing 7th out of
115 participating systems. We find that the en-
semble system’s success comes largely from
its ability to form a consensus and eliminate
complementary noise from its base systems’
predictions.

1 Introduction

The task of semantic textual similarity (STS) has
been developed over the past several SemEval com-
petitions with the idea of capturing the degree of
similarity in the meaning conveyed by two snippets
of text (usually two sentences). In that respect, the
STS task can be seen as similar to such tasks as
paraphrase detection (Xu et al., 2015), recognizing
textual entailment (RTE) (Negri et al., 2012), and
semantic relatedness (Marelli et al., 2014a). The

∗These three authors contributed equally to this work.

STS task captures different gradations of similar-
ity, rather than a binary decision, and it is symmet-
ric, rather than directed, as is the case with RTE
(Agirre et al., 2012). It also aims to capture a
more general notion of semantic similarity that does
not focus solely on the semantic relatedness derived
through compositional processes. In this paper, we
describe the SimiHawk system submission for the
core Semantic Textual Similarity (STS) task at the
SemEval-2016 competition.

The STS task series (Agirre et al., 2012; Agirre et
al., 2013; Agirre et al., 2014; Agirrea et al., 2015)
has aggregated a sizable dataset of sentence pairs
annotated with numeric similarity scores. The pres-
ence of this dataset allows for a shift from earlier
work that mostly used unsupervised learning (Cor-
ley and Mihalcea, 2005; Mihalcea et al., 2006; Li et
al., 2006), to the supervised approaches that lever-
age the labeled data (Sultan et al., 2015; Han et al.,
2015; Hänig et al., 2015). The availability of labeled
data from different genres also allows for a clearer
evaluation and a better comparison across different
approaches.

Specifically, the task of semantic similarity is de-
fined as follows: given an input of two sentences,
output a real number in the range [0,5] where 0
means lowest and 5 means highest similarity. The
top-performing system from last year’s task (Sul-
tan et al., 2015) relied heavily on a hand-engineered
word alignment tool (Sultan et al., 2014) (achiev-
ing 5th place with the aligner alone), combined with
dense word embeddings (Baroni et al., 2014) to
create a two-feature regression model. Other top-
performing systems (Han et al., 2015; Hänig et al.,

741



2015) follow this trend of exploiting word alignment
and embedding similarity across textual pairs.

Recent work (Bowman et al., 2014; Bowman et
al., 2015b; Tai et al., 2015) has shown the effective-
ness of deep learning end-to-end architectures us-
ing recurrent and recursive neural networks on tasks
similar to semantic similarity, such as semantic re-
latedness (Tai et al., 2015), natural language infer-
ence (Bowman et al., 2015a), and textual entail-
ment (Marelli et al., 2014b), providing state-of-the-
art performance. Since all of these tasks evaluate
the relationship between the semantics of two input
sentences, it stands to reason that systems with such
architecture may perform well on the more general
task of semantic similarity.

In our submission, we are interested in comparing
an approach using hand-engineered features against
the deep RNN-based architectures with Long Short-
Term Memory (LSTM) cells. We therefore imple-
mented a feature-based system over a small num-
ber of heavily engineered features, and two LSTM-
based systems: one that uses a simple LSTM archi-
tecture and one that uses a Tree-LSTM architecture
that mirrors the syntactic dependencies of the input
in the LSTM model.

In the following section, we describe each system
in detail. We then report and analyze the outcome
of this bake-off. Not surprisingly, the ensemble sys-
tem performs the best, obtaining a weighted Pearson
correlation of 0.738. Of the three base systems, the
feature-based model obtained the best results, out-
performing the LSTM-based models by .06.

2 System Description

2.1 Feature-Based

Because of the impressive success of feature-based
methods in previous STS shared tasks, we wanted to
understand whether deep learning approaches were
even necessary. The winning system for the 2015
shared task computed two features:

1. alignment ratio As part of their 1st place sys-
tem, (Sultan et al., 2015) constructed a custom,
open tool to align the words within a sentence
pair1. The alignment ratio feature r is com-
puted as:

1https://github.com/ ma-sultan/monolingual-word-aligner

r =
a(s1) + a(s2)

t(s1) + t(s2)
(1)

where s1 and s2 are sentences 1 and 2 respec-
tively, a() is the number of aligned content
words in a sentence, and t() is the total num-
ber of content words in a sentence.

2. cosine of word2vec centroids Since word2vec
came out in 2013 (Mikolov et al., 2013), word
embeddings have gained massive popularity.
Though a naive form of compositionality, sim-
ple vector addition has been shown as a surpris-
ingly effective way to represent phrases. Using
the official word2vec skip-gram Google News
embeddings2, we compute the word2vec cosine
feature w as follows:

w = cos(
Σwi1
N1

,
Σwi2
N2

) (2)

where Σwis is the sum of the embeddings of
each word in sentence s and Ns is the sentence
length.

We used this system as a starting point, but we
added two additional feature classes. The two fea-
ture classes we added were:

1. cosine of one-hot bag-of-words Similar to
word2vec centroids, we also computed a fea-
ture b as the cosine of binary bag-of-words en-
codings of each sentence:

b = cos(Σei1,Σe
j
2) (3)

where for a vocabulary (consisting of the union
of the two sentences’ sets of tokens) of size V ,
the one-hot vector eis is a vector of size V con-
sisting of V − 1 zero entries and a 1 at word i’s
entry in the vocabulary. Note that subscripts 1
and 2 signify one-hot vectors representing to-
kens from sentences 1 and 2, respectively.

2. Machine Translation evaluation metrics In
2012, an ensemble of various MT metrics was
created to attain (at the time) state-of-the-art re-
sults in Paraphrase Identification (Madnani et

2https://code.google.com/archive/p/word2vec

742



al., 2012). We created features for each of the
following metrics to compute the similarity be-
tween sentences 1 and 2:

• BLEU (Papineni et al., 2002)3
• NIST (Doddington, 2002)4
• METEOR (Lavie and Agarwal, 2005)5
• BADGER (Parker, 2008)6
• TER (Snover et al., 2006)7
• TERp (Snover et al., 2009)8

We combine these four feature classes using a
fully-connected neural network with 2 layers of size
40. Similar to (Tai et al., 2015) (specifically equa-
tions (14) and (15)), the network produces a prob-
ability distribution over scores and is trained using
categorical cross-entropy loss. To build the network,
we use the Keras library (Chollet, 2015), a Python
neural network library written on top of Theano
(Bergstra et al., 2010; Bastien et al., 2012).

2.2 Deep End-to-End LSTM-Based Systems

Deep end-to-end systems are very enticing because
they learn the representation for a given input, as
opposed to manually constructing a feature space.
We built two LSTM-based systems for this task. Al-
though in principle these systems can be considered
end-to-end, both systems make use of pre-trained
word embeddings as input. We describe the two
systems below, assuming a familiarity with regular
LSTM cell structure. We begin by discussing Tree-
LSTM because it recently achieved state-of-the-art
performance on the semantic relatedness task (Tai et
al., 2015).

2.2.1 Tree-LSTM
The architecture of Tree-LSTM is a generaliza-

tion of LSTMs to tree-structured network topolo-
gies. The tree-structured LSTM composes its cur-
rent state from an input vector, as well as the hidden

3ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a-
20091001.tar.gz

4ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a-
20091001.tar.gz

5http://www.cs.cmu.edu/ alavie/METEOR/download/meteor-
1.5.tar.gz

6http://babblequest.org/badger
7http://www.cs.umd.edu/˜snover/tercom/tercom-0.7.25.tgz
8https://github.com/snover/terp

states of an arbitrary number of child units. As pre-
viously mentioned, the Tree-LSTM architecture has
been used for the semantic relatedness task. For this
task, the specific tree structure the authors use is a
dependency tree, obtained via the Stanford Neural
Network Dependency Parser (Chen and Manning,
2014). For the semantic relatedness task, the sys-
tem takes as input two pieces of text, and outputs a
real number in the range [1,5]. Therefore, one only
needs to modify the original Tree-LSTM system to
output a number in the range [0,5] in order to apply
it to the task of semantic similarity.

The specific architecture we use is referred to by
the original authors as Child-Sum Tree-LSTM (Tai
et al., 2015). Using the authors’ original notation,
and using C(j) to denote the set of children of node
j, the hidden state at a given node is computed as
follows:

h̃j =
∑

k∈C(j)
hk (4)

ij = σ(W
ixj + U

ih̃j + b
i) (5)

fjk = σ(W
fxj + U

fhk + b
f ) (6)

oj = σ(W
oxj + U

oh̃j + b
o) (7)

uj = tanh(W
uxj + U

uh̃j + b
u) (8)

cj = ij � uj +
∑

k∈C(j)
fjk � uk (9)

hj = oj � tanh(cj) (10)
where xj is the input of node j (typically a word em-
bedding vector), σ is the logistic sigmoid function,
and � is element-wise multiplication. Note here we
refer to nodes instead of timesteps, since the the tree
structure induced by the dependency parse alters the
standard temporal flow of a normal LSTM. Conse-
quently, the hidden state from the root node is then
used as a representation for a given text input.

Given two input texts, the authors use a Tree-
LSTM to generate a representation for each input,
which we call hL and hR. These are then used to
compute a final score, ŷ, as follows:

h× = hL � hR (11)
h+ = |hL − hR| (12)

hs = σ(W
×h× +W+h+ + bh) (13)

743



p̂θ = softmax(W
phs + b

p) (14)

ŷ = rT p̂θ (15)

Note how (14) creates a distribution over integer
scores, and (15) converts the distribution into a real
number in range [a,b] by setting r equal to a vector
of integer values [a ... b ]. For our system, we sim-
ply set r equal to the vector [0 1 2 3 4 5], as opposed
to the original values of [1 2 3 4 5] for the seman-
tic relatedness task. The inclusion of zero will better
facilitate the output of scores less than one. This
is because distributions that have their mass highly
concentrated in the first index will have little remain-
ing mass in the other indices that can then multiply
by the non-zero integers.

For our implementation we used the code from
the original authors9, which uses Glove embeddings
(Pennington et al., 2014) as input vectors xj , hid-
den states (hi) of size 200, and a similarity module
of size 50 (hs (13)). For training, we use a regular-
ization strength of 0.0004, a minibatch size of 25,
and a learning rate of 0.05. The model trained for
10 epochs. We also decided to disregard the use of a
validation set, since our intermediate results showed
that, if anything, the use of a validation set hurt per-
formance.

2.2.2 LSTM
Despite the potential of the Tree-LSTM model

for this task, (Bowman et al., 2015b) shows that
even for text that is highly syntactically dependent,
a standard LSTM architecture can perform as well
as a Tree-LSTM architecture given enough train-
ing data. The authors also show that an LSTM
can perform better on shorter sentences. More-
over, (Vinyals et al., 2015) successfully applied a
sequence-to-sequence LSTM model with attention
to the syntactic parsing task, achieving state-of-the-
art results. Motivated by these results, we also pro-
duced a model that uses standard LSTM architec-
ture.

The architecture of the LSTM model is repre-
sented in Figure 1. The model is very similar to
the Tree-LSTM, but instead of using tree-structured
LSTMs, it uses standard LSTM cells. The hidden
states of the LSTMs are combined by concatenation

9https://github.com/stanfordnlp/treelstm

of element-wise multiplication, summation and co-
sine similarity, and the resulting vector serves as the
input to the fully-connected layer. As in (Tai et al.,
2015), the network produces a probability distribu-
tion over scores, which is in turn transformed into a
real number over the specified range.

We used the Keras library to implement this
model, creating LSTMs with hidden states of size
300, and fully-connected layers of size 50. For train-
ing, we use a regularization strength of 0.0004, mini-
batch size of 25, and dropout of 0.1. The model was
trained for 15 epochs. We also trimmed sentences
to have a maximum length of 50 tokens. The LSTM
system uses Glove embeddings (Pennington et al.,
2014) as its pretrained word vectors. We chose to
use Glove instead of word2vec because Glove had
fewer out-of-vocabulary tokens for the dataset.

2.3 Ensemble
In order to hedge our bets, we created a ensem-
ble system, using Feature-Based, Tree-LSTM, and
LSTM systems as base systems. The ensemble
model can be seen as a feature-based, stacking sys-
tem because it uses the predictions of the three
base systems as the features for a supervised model.
Thus, the most basic stacking system has three fea-
tures, one for each base system.

In order to train the stacking model, we need base
predictions that are not ‘dirty’: the data used to train
base systems to generate predictions (which in turn
train our stacking system) must have an empty in-
tersection with the data we use to make predictions.
To overcome this, each base system performed 5-
fold cross-validation on the training data. By doing
this, we create predictions for each training example
from ‘clean’ data. Next, we used the predictions on
the held-out folds as the features to train the stack-
ing model. We made sure to shuffle the training
data prior to creating the folds to ensure that each
fold would be statistically similar in terms of which
domains are represented as well as the distributions
over gold standard scores. To generate test scores for
the stacking system, we allowed the base systems to
train on all the training data and then make predic-
tions on the test data, which become the test features
for the stacking system.

We hypothesize that in certain scenarios, the
stacking system should favor the scores from certain

744



Sentence 1

Sentence 2

LSTM 1

LSTM 2

+

×

cos

FC Layer 1 FC Layer 2
Distribution
of similarity

scores
Shared weights Sigmoid Softmax

Figure 1: The architecture of the LSTM model

System mean answer-answer headlines plagiarism postediting question-question
Ensemble 0.73774 0.59237 0.81419 0.80566 0.82179 0.65048
Feature-based 0.70647 0.44003 0.77109 0.81105 0.81600 0.71035
LSTM 0.64840 0.44177 0.75703 0.71737 0.72317 0.60691
TreeLSTM 0.64140 0.52277 0.74083 0.67628 0.70655 0.55265
# Examples - 254 249 230 244 209

Table 1: Results of our systems on the 2016 gold standard. The highest score from each column is in bold.

Features Weighted Mean
All 0.7168
-average pronoun count 0.7178
-average length 0.7211
-embeddings similarity 0.7237
-edit distance 0.7164
-align ratio 0.6952
-sentence length 0.7116

Table 2: Feature ablation study for ensemble system. ‘-’ de-
notes witholding a feature.

base systems. For example, (Bowman et al., 2015b)
has shown that LSTM models can have difficulty on
long sequences. Therefore, we performed a feature
ablation study where we combined the three base-
line predictions with several hand-picked features:
length of sentence 1, length of sentence 2 (these two
combined are called sentence length), alignment ra-
tio (1), string edit distance, embedding similarity
(2), average sentence length between the pair, and
average pronoun count. We included average pro-
noun count because we found certain domains con-
tain a larger frequency of pronouns. The ablation
experiment was performed on 2015 evaluation data,
and the results are shown in Table 2. It is visible in
the table that the only feature absence that generated
any significant drop-off is alignment ratio. There-
fore, we decided to include alignment ratio as the
fourth and final feature in our stacking system. For
the stacking implementation, we use the same neu-

ral network as the feature-based system. However,
we decided to use a three-layer network based on
experimental results.

3 Results

All systems were trained using the train-
ing/evaluation data from previous years’ tasks
(Agirre et al., 2012; Agirre et al., 2013; Agirre et
al., 2014; Agirrea et al., 2015). After filtering for
duplicate examples, our training set contains a total
of 13,061 examples.

Results of our systems on the 2016 gold standard
are shown in Table 1. We report the performance,
specifically Pearson correlation, across five different
domains: Answer-Answer, Headlines, Plagiarism,
Postediting, and Question-Question. We also report
the weighted mean across all five domains. The last
row of Table 1 also records the number of gold stan-
dard examples for each domain.

According to the official results, the overall per-
formance of our ensemble system was .738, exceed-
ing the performance of the base systems, although
it is not the highest performing system for every
domain. Specifically for the question-question do-
main, the feature-based system seems to outperform
it substantially. The overall ranking of the ensem-
ble system is 7 out of 115 submitted systems, while
feature-based ranks 37, LSTM ranks 73, and TreeL-
STM ranks 77.

745



System Fold 1 Fold 2 Fold 3 Fold 4 Fold 5 mean
LSTM 0.802 0.806 0.805 0.795 0.790 0.800
Feature-based 0.790 0.800 0.813 0.823 0.799 0.805
TreeLSTM 0.772 0.797 0.793 0.806 0.794 0.792

Table 3: Results of our base systems on cross-validation folds

System Ensemble Features LSTM TreeLSTM Reference
Ensemble 1.000 0.769 0.751 0.802 0.592
Feature-based 0.769 1.000 0.456 0.413 0.440
LSTM 0.751 0.456 1.000 0.608 0.442
TreeLSTM 0.802 0.413 0.608 1.000 0.523
Reference 0.592 0.440 0.442 0.523 1.000

Table 4: Pairwise correlation scores amongst systems’ predictions and gold labels (Reference) on the answer-answer domain.

We also report five-fold cross-validation results
for our three base systems (Table 3). These experi-
ments used the training data, drawn from several do-
mains. We shuffled the data for our cross-validation
experiments so that each fold has a similar distribu-
tion of domain examples as well as scores. These
results show that when the data is shuffled, and do-
mains and scores are equally distributed, the overall
correlation is much higher.

4 Discussion

The ensemble system’s success can be attributed to
its ability to form a consensus among the base sys-
tems and eliminate noise in the predictions. As we
can see in Table 4, which shows the correlation be-
tween systems for the answer-answer domain, al-
though the three base systems have low pairwise cor-
relations (.456, .413, .608), they all have high corre-
lations with the ensemble system (.769, .751, .802).
We can also see that the ensemble system has its
largest correlation with the highest-scoring base sys-
tem for answer-answer, TreeLSTM. Since each base
system had pairwise low correlation, they were cap-
turing different views of the data, and the ensemble
system was able to take the important parts of each
view, while canceling out the noise. This allowed
the ensemble system to outperform the base systems
in answer-answer by .12, on average.

To further illustrate this point, take for example
the following input pair: “There’s not a lot you can
do about that.” and “There’s not that much that you
can do with a sourdough starter.” with a gold la-
bel of 2. The baseline systems predict the follow-
ing scores: 3.96 from the feature-based system, 0.31
from the LSTM system, and 1.39 from the TreeL-

STM system. The TreeLSTM system provides the
closest prediction, while the feature-based system
provides the worst prediction, over-predicting the
gold label by almost 2. However, the ensemble sys-
tem is able to effectively leverage this higher pre-
diction, producing a prediction of 1.76, which is the
most accurate prediction out of all four systems.

In our experiments, we found that in three
domains (answer-answer, headlines, question-
question), the ensemble system had the highest
correlation with TreeLSTM. It is likely that TreeL-
STM’s low score on question-question (.553),
combined with a high correlation with the ensemble
(.900), were what brought the ensemble system’s
score down enough for the feature-based system
to outperform it. On the other two occasions (pla-
giarism and postediting), the base system with the
highest correlation with ensemble system was the
feature-based system. In both of these scenarios, the
two systems produced very similar scores, within
.01 of one another for both occasions.

Although the feature-based system was the high-
est scoring base system on 4 out of 5 domains,
there is only one occasion where its correlation with
another base system exceeds .79 (plagiarism with
LSTM). This suggests that the shallow methods for
feature extraction are not able to represent some im-
portant information that the deep LSTM-based mod-
els are able to capture. As a result, the ensemble sys-
tem was able to outperform all three base systems in
most cases, suggesting that none of the base systems
were able to capture the whole picture individually.

746



References
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor

Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main confer-
ence and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Eval-
uation, pages 385–393. Association for Computational
Linguistics.

Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. sem 2013 shared task:
Semantic textual similarity, including a pilot on typed-
similarity. In In* SEM 2013: The Second Joint Con-
ference on Lexical and Computational Semantics. As-
sociation for Computational Linguistics. Citeseer.

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,
Rada Mihalcea, German Rigau, and Janyce Wiebe.
2014. Semeval-2014 task 10: Multilingual semantic
textual similarity. In Proceedings of the 8th inter-
national workshop on semantic evaluation (SemEval
2014), pages 81–91.

Eneko Agirrea, Carmen Baneab, Claire Cardiec, Daniel
Cerd, Mona Diabe, Aitor Gonzalez-Agirrea, Wei-
wei Guof, Inigo Lopez-Gazpioa, Montse Maritxalara,
Rada Mihalceab, et al. 2015. Semeval-2015 task 2:
Semantic textual similarity, english, spanish and pilot
on interpretability. In Proceedings of the 9th inter-
national workshop on semantic evaluation (SemEval
2015), pages 252–263.

Marco Baroni, Georgiana Dinu, and Germán Kruszewski.
2014. Don’t count, predict! a systematic compari-
son of context-counting vs. context-predicting seman-
tic vectors. In ACL (1), pages 238–247.

Frédéric Bastien, Pascal Lamblin, Razvan Pascanu,
James Bergstra, Ian J. Goodfellow, Arnaud Berg-
eron, Nicolas Bouchard, and Yoshua Bengio. 2012.
Theano: new features and speed improvements. Deep
Learning and Unsupervised Feature Learning NIPS
2012 Workshop.

James Bergstra, Olivier Breuleux, Frédéric Bastien, Pas-
cal Lamblin, Razvan Pascanu, Guillaume Desjardins,
Joseph Turian, David Warde-Farley, and Yoshua Ben-
gio. 2010. Theano: a cpu and gpu math expression
compiler. In Proceedings of the Python for scien-
tific computing conference (SciPy), volume 4, page 3.
Austin, TX.

Samuel R Bowman, Christopher Potts, and Christopher D
Manning. 2014. Recursive neural networks can learn
logical semantics. arXiv preprint arXiv:1406.1827.

Samuel R Bowman, Gabor Angeli, Christopher Potts, and
Christopher D Manning. 2015a. A large annotated

corpus for learning natural language inference. arXiv
preprint arXiv:1508.05326.

Samuel R Bowman, Christopher D Manning, and
Christopher Potts. 2015b. Tree-structured composi-
tion in neural networks without tree-structured archi-
tectures. arXiv preprint arXiv:1506.04834.

Danqi Chen and Christopher D Manning. 2014. A
fast and accurate dependency parser using neural net-
works. In EMNLP, pages 740–750.

Franois Chollet. 2015. Keras.
https://github.com/fchollet/keras.

Courtney Corley and Rada Mihalcea. 2005. Measur-
ing the semantic similarity of texts. In Proceedings of
the ACL workshop on empirical modeling of semantic
equivalence and entailment, pages 13–18. Association
for Computational Linguistics.

G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proceedings of HLT, pages 138–145.

Lushan Han, Justin Martineau, Doreen Cheng, and
Christopher Thomas. 2015. Samsung: Align-and-
differentiate approach to semantic textual similarity.
SemEval-2015, page 172.

Christian Hänig, Robert Remus, and Xose De La Puente.
2015. Exb themis: Extensive feature extraction
from word alignments for semantic textual similarity.
SemEval-2015, page 264.

Alon Lavie and Abhaya Agarwal. 2005. Meteor: An
automatic metric for mt evaluation with improved cor-
relation with human judgments. In Proceedings of the
ACL 2005 Workshop on Intrinsic and Extrinsic Eval-
uation Measures for MT and/or Summarization, pages
65–72.

Yuhua Li, David McLean, Zuhair A Bandar, James D
O’shea, and Keeley Crockett. 2006. Sentence sim-
ilarity based on semantic nets and corpus statistics.
Knowledge and Data Engineering, IEEE Transactions
on, 18(8):1138–1150.

Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics for
paraphrase identification. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 182–190, Montréal,
Canada, June. Association for Computational Linguis-
tics.

Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zampar-
elli. 2014a. Semeval-2014 task 1: Evaluation of com-
positional distributional semantic models on full sen-
tences through semantic relatedness and textual entail-
ment. SemEval-2014.

747



Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zampar-
elli. 2014b. A sick cure for the evaluation of composi-
tional distributional semantic models. In LREC, pages
216–223.

Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In AAAI, volume 6, pages
775–780.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado,
and Jeffrey Dean. 2013. Distributed representations of
words and phrases and their compositionality. CoRR,
abs/1310.4546.

Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 task 8: cross-lingual textual entailment
for content synchronization. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main confer-
ence and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Eval-
uation, pages 399–407. Association for Computational
Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.

Steven Parker. 2008. Badger: A new machine translation
metric.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word rep-
resentation. In EMNLP, volume 14, pages 1532–1543.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In In
Proceedings of Association for Machine Translation in
the Americas, pages 223–231.

Matthew G. Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Ter-plus: Paraphrase, se-
mantic, and alignment enhancements to translation
edit rate.

Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014. Back to basics for monolingual align-
ment: Exploiting word similarity and contextual evi-
dence. Transactions of the Association for Computa-
tional Linguistics, 2:219–230.

Md Arafat Sultan, Steven Bethard, and Tamara Sumner.
2015. Dls@ cu: Sentence similarity from word align-
ment and semantic vector composition. In Proceed-
ings of the 9th International Workshop on Semantic
Evaluation, pages 148–153.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term memory
networks. arXiv preprint arXiv:1503.00075.

Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Grammar
as a foreign language. In Advances in Neural Informa-
tion Processing Systems, pages 2755–2763.

Wei Xu, Chris Callison-Burch, and William B Dolan.
2015. Semeval-2015 task 1: Paraphrase and seman-
tic similarity in twitter (pit). Proceedings of SemEval.

748


