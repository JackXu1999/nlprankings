















































Incremental Computation of Infix Probabilities for Probabilistic Finite Automata


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2732–2741
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

2732

Incremental Computation of Infix Probabilities for
Probabilistic Finite Automata

Marco Cognetta, Yo-Sub Han, and Soon Chan Kwon
Department of Computer Science

Yonsei University, Seoul, Republic of Korea
{mcognetta, emmous, soon--chan}@yonsei.ac.kr

Abstract

In natural language processing, a common task
is to compute the probability of a given phrase
appearing or to calculate the probability of
all phrases matching a given pattern. For in-
stance, one computes affix (prefix, suffix, in-
fix, etc.) probabilities of a string or a set of
strings with respect to a probability distribu-
tion of patterns.

The problem of computing infix probabili-
ties of strings when the pattern distribution is
given by a probabilistic context-free grammar
or by a probabilistic finite automaton is al-
ready solved, yet it was open to compute the
infix probabilities in an incremental manner.
The incremental computation is crucial when
a new query is built from a previous query.
We tackle this problem and suggest a method
that computes infix probabilities incrementally
for probabilistic finite automata by represent-
ing all the probabilities of matching strings as
a series of transition matrix calculations. We
show that the proposed approach is theoreti-
cally faster than the previous method and, us-
ing real world data, demonstrate that our ap-
proach has vastly better performance in prac-
tice.

1 Introduction

Probabilistic grammars and finite automata are
commonly used to model distributions in natural
language processing. Among language models,
probabilistic finite automata (PFAs) provide a sim-
ple, yet powerful and well-understood representa-
tion of many probabilistic language phenomena.
Numerous speech processing tasks rely on PFAs in
practice (Gyawali et al., 2013; Mohri et al., 2002;
Wilson and Raaijmakers, 2008; Ng et al., 2000).

An important problem regarding PFAs is to cal-
culate the probability of some affix (prefix, suffix,
infix, etc.) of a string with respect to a given dis-
tribution. That is, given a PFA P and a string w,

one might ask the probability of w appearing as
a prefix, suffix, or infix in the distribution mod-
eled by P—in other words, the sum of the prob-
abilities of all strings in the form of wx, xw, or
xwy with respect to P , for some strings x and
y. A more general problem is to compute the
sum of the probabilities of all strings in a regu-
lar language with respect to a PFA. Computing af-
fix probabilities in probabilistic models is an im-
portant problem in natural language processing.
For probabilistic context-free grammars (PCFGs)
and PFAs, the problem of calculating the prefix
or suffix probability of a string can be efficiently
solved (Fred, 2000; Corazza et al., 1991). How-
ever, calculating the infix probability of a string
or the weight of a regular language is not as
straightforward (Corazza et al., 1991). Addition-
ally, computing affix probabilities for more gen-
eral probabilistic language models has proven to
be quite difficult. Nevertheless, there are some ap-
proaches for computing the exact affix probabil-
ities over a variety of models. Lattice posterior
probabilities for n-grams, which are a restricted
form of PFA (Vidal et al., 2005b), have a vari-
ety of uses in speech processing and can be com-
puted efficiently (de Gispert et al., 2013; Can and
Narayanan, 2015). For several affixes, Corazza
et al. (1991) described algorithms to determine the
probability of a string appearing as that affix in
a PCFG. They made an important note that, un-
like computing prefix or suffix probabilities, infix
probability calculations are prone to double count-
ing in the event of the infix appearing multiple
times in a string. Then, they provided an algorithm
for computing the infix probability of a string in
restricted cases. Stolcke (1995) described a series
of recurrences that can be used to compute affix
probabilities and variations of the most probable
parse of a string for PCFGs. For the general class
of linear context-free rewriting systems, a method



2733

to compute prefix probabilities is known (Neder-
hof and Satta, 2011b). Fred (2000) also consid-
ered these problems and described a method to
compute the infix probabilities under the condi-
tion that the infix appears at most once in any non-
zero probability string when the language model is
a stochastic regular grammar, which is equivalent
in power to a PFA. These assumptions are rather
strict and led researchers to consider a more gen-
eral problem. Nederhof and Satta (2011a) solved
the general infix probability problem for PCFGs.
In fact, their method can be used to compute the
weight of any regular language with respect to a
PCFG. They also proposed an open problem of
incrementally computing the infix probability of
a string—using the numerical result of one infix
computation to speed up the evaluation of another.

We design a new method for solving the in-
fix probability problem for PFAs incrementally.
Unlike the previous methods involving recur-
rence calculations or intersection constructions,
our method is based on evaluating a series of ma-
trices formed from regular expressions. Addition-
ally, our method has no constraints on the input
string. We show that our method is both theoreti-
cally and practically more performant than the pre-
vious algorithms. Our experimental results show a
greater than 80% performance improvement. In
Section 2, we review PFAs and other necessary
formalisms. We recall how to obtain unambigu-
ous regular expressions in Section 3, and introduce
a matrix representation for computing the weight
of a regular language in Section 4. In Section 5,
we propose an algorithm to incrementally com-
pute the infix probability of a given string. We val-
idate the practical performance of the incremental
method using a test set of PFAs obtained from real
life data in Section 6 and conclude with a discus-
sion and some open problems in Section 7.

2 Preliminaries

2.1 Finite Automata and PFAs

Following standard notation in automata theory,
we let Σ be a set of characters and Σ∗ be the set of
all strings. For a string w = w1w2 . . . wn ∈ Σ∗,
we write |w| = n as its length. The empty string
is written as λ.

A deterministic finite automaton (DFA) is a 5-
tupleD = (Q,Σ, δ, s, F ), whereQ is a finite set of
states, Σ is a finite alphabet, δ : Q×Σ→ Q is the
transition function, s ∈ Q is the initial state, and

F ⊂ Q is the set of final states. A language is a
set of strings, andD recognizes a regular language
denoted by L(D). For a summary of automata
theory (including regular expressions and formal
language theory), we direct the reader to Hopcroft
and Ullman (1979).

A PFA is a weighted finite automaton that com-
putes a function P : Σ∗ → [0, 1]. We abuse
function notation and write P(w) or P(π) to de-
note the weight of a word or a path, respectively.
These values are defined later. A PFA P is spec-
ified by a 5-tuple P = (Q,Σ, δ, I, F ), where
Q is a finite set of states, Σ is a finite alphabet,
δ : Q × Σ × Q → [0, 1] is the transition func-
tion, I : Q → [0, 1] and F : Q → [0, 1] are the
initial and final functions, respectively. The tran-
sition function is assumed to have a default value
of 0, in other words, if a transition does not exist it
can be considered as having weight 0. A PFA has
three additional requirements:

1.
∑
q∈Q

I(q) = 1

2. ∀q ∈ Q, F (q) +
∑

q′∈Q,c∈Σ
δ(q, c, q′) = 1

3. All states are both accessible and co-
accessible1.

If these conditions hold, then for all strings w,
0 ≤ P(w) ≤ 1 and

∑
w∈Σ∗ P(w) = 1. A

PFA can be represented in the form of transition
matrices, which simplifies several computations.
We denote the matrix formulation of a PFA by
P = (Q,Σ, {M(c)}c∈Σ, I,F) where {M(c)}c∈Σ
is a set of |Q| × |Q| transition matrices with
M(c)i,j = δ(qi, c, qj). Likewise, I and F are
1 × |Q| and |Q| × 1 vectors with Ii = I(qi) and
Fj = F (qj).

Consider a string w = w1w2 · · ·wn ∈ Σ∗ and
a corresponding labeled path

π = (q0, w1, q1), (q1, w2, q2), . . . , (qn−1, wn, qn)

in P . Then the probability of a path π in P is

P(π) = I(q0)

(
n∏
i=1

δ(qi−1, wi, qi)

)
F (qn).

Let Φw be the set of all labeled paths corre-
sponding to w. The probability of w is now

1Accessible states are states reachable from a state with
non-zero initial weight and co-accessible states are those that
can reach a state with non-zero final weight.



2734

∑
π∈Φw P(π). There exist two equivalent dy-

namic programming methods—the forwards and
backwards algorithms—to compute the probabil-
ity of a given string (Vidal et al., 2005a). Using
the matrix formulation, the probability of a string
is given succinctly as

I
|w|∏
i=1

M(wi)F.

For brevity, we write M(Σ) =
∑

c∈Σ M(c) and
0 and 1 for the zero and identity matrices when
the dimensions are clear. Further, we compute

∞∑
i=0

M(Σ)i = (1−M(Σ))−1,

which we denote M(Σ∗). We can compute the pre-
fix and suffix probabilities of a string w as

P(wΣ∗) = I

 |w|∏
i=1

M(wi)

M(Σ∗)F
and

P(Σ∗w) = IM(Σ∗)

 |w|∏
i=1

M(wi)

F,
respectively. If an automaton M satisfies all
of the requirements for a PFA except that∑

q∈Q I(q) ≤ 1 or ∀ q ∈ Q, F (q) +∑
q′∈Q,c∈Σ δ(q, c, q

′) ≤ 1, then we call M a
sub-PFA. These machines have the property that
for any string w over Σ, 0 ≤ M(w) ≤ 1 and∑

w∈Σ∗M(w) ≤ 1. Since a sub-PFA M may
not describe a probability distribution over strings,
we call M(w) the weight of w instead of prob-
ability. A stochastic language over an alphabet
Σ is a set S ⊆ Σ∗ where each string in S has
an associated probability, 0 ≤ PrS(w) ≤ 1,
such that

∑
w∈Σ∗ PrS(w) = 1. Given a stochas-

tic language S, if there exists a PFA P such that
∀w ∈ Σ∗, P(w) = PrS(w), we call S a regular
stochastic language.

2.2 Unambiguous Regular Expressions
Regular expressions are a common representation
of regular languages. A string that can be matched
with a regular expression is said to be in the lan-
guage of the regular expression. A match occurs
when there is a valid assignment of symbols in the

a, 0.2 | b, 0.2

a
,0

.3
|b

,0
.1

0.8 | 0.3 0.1 | 0.2

0.0 | 0.30.0 | 0.5

0.1 | 0.6

a, 0.5 | b, 0.2

b, 0.7

b, 0
.2

a,
0.4

a, 0.1

b, 0.2

Figure 1: An example PFA over Σ = {a, b}. Each state
has an initial and final probability, separated by a bar.
The edges hold a character and the probability of the
corresponding transition.

regular expression to the queried string. We de-
note the set of all strings that match a regular ex-
pression as L(R). An unambiguous regular ex-
pression has only one valid assignment for any
string in the language. For example, consider the
regular expression (a ∪ b)∗aa(a ∪ b)∗, which ac-
cepts the set of strings over Σ = {a, b} containing
aa as an infix. We assign different subscripts to
symbols that appear in more than one position to
form (a1 ∪ b1)∗a2a3(a4 ∪ b2)∗. Given the string
baaa, we find that b1a1a2a3 and b1a2a3a4 are both
valid assignments, hence baaa is in the language.
However, since there are two valid assignments,
we say that this regular expression is ambigu-
ous (Book et al., 1971). An unambiguous expres-
sion for the same language is b∗a(bb∗a)∗a(a∪b)∗.

2.3 Intersecting a DFA and a PFA
The standard method to compute the weight of a
regular language with respect to a PFA is to in-
tersect its DFA representation with the PFA. This
construction was already discussed in (Vidal et al.,
2005a). Nederhof and Satta (2011a) considered
a similar construction for DFAs and PCFGs and
solved the infix problem for that class of machine.
The construction creates a new sub-PFA [D ∩ P]
such that:

[D ∩ P](w) =

{
P(w), w ∈ L(D);
0, otherwise.

It follows that∑
w∈Σ∗

[D ∩ P](w) =
∑

w∈L(D)

P(w),

as desired.



2735

The intersection algorithm is similar to that
of the cross product of two automata from clas-
sical automata theory. Given a DFA D and a
PFAP , we construct a new machineW with states
QW = QD × QP . For two states (x, y), (x′, y′)
and a character c ∈ Σ, δW((x, y), c, (x′, y′)) =
δP(y, c, y

′) if δD(x, c) = x′ and 0 otherwise.
Likewise, IW((p, q)) = IP(q) if p is the initial
state of D and FW((p′, q′)) = FP(q′) if p′ is a
final state of D and are 0 otherwise.

Using Algorithm 1, we can now efficiently com-
pute the sum of the weight of all strings in L(D)
by evaluating

I[D ∩ P](1−M[D ∩ P](Σ))−1F[D ∩ P].

Algorithm 1 DFA/PFA intersection
1: procedure INTERSECT(DFA D, PFA P)
2: Q′ = QD ×QP
3: for (d, p) ∈ Q′ do
4: if d is q0 then
5: I ′((d, p)) = I(p)
6: else
7: I ′((d, p)) = 0
8: end if
9: if d ∈ FD then

10: F ′((d, p)) = F (p)
11: else
12: F ′((d, p)) = 0
13: end if
14: for c ∈ Σ, (d′, p′) ∈ Q′ do
15: if δD(d, c) = d′ then
16: δ′((d, p), c, (d′, p′)) = δP(p, c, p

′)
17: else
18: δ′((d, p), c, (d′, p′)) = 0
19: end if
20: end for
21: end for
22: return [D ∩ P] = (Q′,Σ, δ′, I ′, F ′)
23: end procedure

To find the infix probability of a given string
w = w1w2 · · ·wn, we take D to be a DFA that
recognizes the language of all strings containing
w as an infix. The Knuth-Morris-Pratt (KMP) al-
gorithm produces such a DFA (with O(n) states)
in O(n) time (Knuth et al., 1977). Thus, comput-
ing the infix probabilities of each of w’s prefixes
takes O(n(n|QP |)m) time, where m is the matrix

multiplication constant2, as at each step one needs
to rebuild the entire intersection automaton for the
current prefix and compute an inverse on its tran-
sition matrices.

Nederhof and Satta (2011a) demonstrated that a
similar method can be used to compute the infix
probability of a given string in PCFGs.

3 Generating Unambiguous Regular
Expressions

Book et al. (1971) showed that, given a regular
language (in the form of an automaton or regular
expression), one can find an equivalent unambigu-
ous regular expression by constructing an equiva-
lent DFA and using state elimination on the result-
ing DFA.

Let D = (Q,Σ, δ, q1, F ) be a DFA with |Q| =
n and the states being ordered from 1 to n. We
add two new states, q0 and qn+1 such that q0 is the
new start state and qn+1 is the only final state, and
add the following transitions: δ(q0, λ) = q1 and
∀q ∈ F, δ(q, λ) = qn+1. We then dynamically
eliminate states using the following recurrence:

αki,j = α
k−1
i,j + α

k−1
i,k (α

k−1
k,k )

∗αk−1k,j

with the base cases:

α0i,j =


λ, i = 0, j = 1;

λ, qi ∈ F ∧ j = n+ 1;
{c | δ(qi, c) = qj}, otherwise.

The equations follow the general concatenation,
union, and Kleene star rules for regular expres-
sions. In addition, we have:

• ∅+ c = c+ ∅ = c, for c ∈ Σ

• ∅c = c∅ = ∅, for c ∈ Σ

• λc = cλ = c, for c ∈ Σ

• ∅∗ = λ.

The term αk−1i,j corresponds to the set of strings
for which, starting from state qi, describe a path
to qj where all intermediate states are of the form
q`, where ` < k (the terminal state in the path has
no such restriction). Similarly, αk−1i,k (α

k−1
k,k )

∗αk−1k,j
corresponds to all of the strings which, beginning

2The matrix multiplication constant, m, is the order of the
polynomial for the runtime of multiplying two n×n matrices
together, i.e. a function in O(nm). In practice, Strassen’s
algorithm is often used, yielding m ≈ 2.81 (Strassen, 1969).



2736

at state qi, end at qj going through qk, where
all intermediate states have label at most k (Mc-
Naughton and Yamada, 1960).

We extract αn0,n+1, which is an unambiguous
regular expression for the language recognized by
D (Book et al., 1971). Note that the requirement
of an input automaton being deterministic is not
strict. In fact, state elimination generates unam-
biguous regular expressions from any unambigu-
ous automaton.

From now on, we only consider unambiguous
regular expressions for any regular languages.

4 The Weight of a Regular Language

In Section 2.3, we reviewed the classical way of
computing the weight of a regular language with
respect to a PFA using an intersection construc-
tion. Here, we present a new method based on
unambiguous regular expressions. We describe a
simple transformation to convert regular expres-
sions into operations on transition matrices. Con-
sider the following mapping from regular expres-
sions to matrices:

• ∅ → 0

• λ→ 1

• c ∈ Σ→M(c).

Now, let R and S be regular expressions with
M(R) and M(S) being their corresponding ma-
trices:

• R ∪ S →M(R) + M(S)

• RS →M(R)M(S)

• R∗ → (1−M(R))−1.

Using these definitions, we can build a ma-
trix calculation out of a given regular expression.
We then obtain the weight of a regular expres-
sion R with respect to some PFA P by evaluat-
ing IPMP(R)FP . However, the straightforward
application of this method is prone to overcount-
ing when there are many ways for a string to be
matched to the expression.

We present a simple example where an am-
biguous expression overcounts whereas an unam-
biguous expression returns the correct result when
transformed into matrix calculations:

Let R = b∗a(bb∗a)∗a(a ∪ b)∗ and S = (a ∪
b)∗aa(a ∪ b)∗, which are both regular expressions

for all strings containing aa as an infix. Using the
PFA in Figure 1, we have:

IM(b∗a(bb∗a)∗a(a ∪ b)∗)F ≈ 0.153,

IM((a ∪ b)∗aa(a ∪ b)∗)F ≈ 0.198.

This gap can be made arbitrarily large by
adding ambiguity to the regular expression with-
out changing the described language.

We now show that, given a PFAP and a DFAD,
we can compute the weight of L(D) with respect
to P .
Lemma 1. Let P be a PFA, D be a DFA
and R be an unambiguous regular expression
for L(D) generated by state elimination. Then
IPMP(R)FP =

∑
w∈L(D) P(w).

Proof. Let D have n states, labeled q1 to qn. We
proceed as in (Book et al., 1971) by adding two
new states, q0 and qn+1. We fill out the base case
table α0. We then construct a new table, β where
βki,j = MP(αki,j). In other words, α holds the
regular expressions generated during state elimi-
nation while β holds the corresponding matrices.
Since α0 contains only unambiguous regular ex-
pressions, MP(α0i,j) is the matrix corresponding
to the sum of MP (w) for all w that, starting from
state qi travel to state qj without passing through
any states with label greater than 0. We continue
the elimination process until we reach αn and βn.
The regular expression in αn0,n+1 corresponds to
the unambiguous regular expression containing all
strings accepted by D. Thus, the matrix stored in
βn0,n+1 is the matrix such that

IPMP(αn0,n+1)FP = IPβn0,n+1FP =
∑

w∈L(D)

P(w).

5 Incremental Infix Calculation

We now tackle the open problem of incrementally
computing the infix probability of a string with re-
spect to a PFA. Suppose we have computed the in-
fix probability of a stringw. We want to use the re-
sult of that computation to compute the infix prob-
ability of wa without simply starting the computa-
tion again from scratch. Such a calculation is rel-
atively easy for other affixes. For the prefix prob-
ability of a string represented by the unambiguous
regular expression wΣ∗, one can simply compute
IM(w)M(Σ∗)F and save the vector IM(w). When



2737

the prefix is extended to wa, the saved vector can
be multiplied by M(a)M(Σ∗)F and obtain the re-
sult (we can save the vector IM(w)M(a) to use
in future incremental calculations). A similar pro-
cess works for the incremental suffix probability
of a string (Σ∗w to Σ∗aw). These incremental ap-
proaches are due to the inherent unambiguity of
regular expressions for strings appearing as a pre-
fix or suffix.

Unfortunately, the analogous method for infix
probabilities is not as straightforward. We cannot
simply append (or prepend) the desired character
to a precomputed regular expression because the
resulting expression may not be unambiguous or
may represent the a different language. To tackle
this problem, we first define the language F(w) to
be the set of strings that end in the first occurrence
of w. In other words,

F(w) = {x | w appears only as a suffix of x}.

It follows that F(w) · Σ∗ is exactly the set of
strings containing w as an infix. Thus, given an
unambiguous regular expression forF(w), we can
build an unambiguous regular expression for the
infix of w by concatenating with Σ∗.

Next, we find a regular language L such that
F(wa) = F(w) · L for a character a ∈ Σ, which
gives rise to an incremental computation using the
previous result F(w). Given two languages R and
S, we define the left quotient R\S to be:

R\S = {y | ∃x ∈ R such that xy ∈ S}.

It is known that regular languages are closed under
the left quotient operation (Hopcroft and Ullman,
1979).

Corollary 2 follows from the definition of the
left quotient and describes the desired L.
Corollary 2. Given F(w) and F(w)\F(wa),
F(wa) = F(w) · F(w)\F(wa).

We use this characteristic and compute
F(w)\F(wa) without explicitly computing
F(wa) based on state elimination—the procedure
is detailed later in this section. Figure 2 is
an annotated example of unambiguous regular
expressions for F of a, aa, and aab.

Let D be the DFA for F(w1w2 · · ·wn) gener-
ated by the KMP algorithm (Knuth et al., 1977).
Two examples are depicted in Figure 3. D has
n + 1 states, with state qn+1 being final and hav-
ing no outgoing transitions. Furthermore, each

F(a) = b∗a
F(aa) = b∗a · (bb∗a)∗a

F(aab) = b∗a(bb∗a)∗a · a∗b
F(a) F(a)\F(aa)

F(aa) F(aa)\F(aab)

Figure 2: An example of F for a, aa, aab. We anno-
tate them to show how F can be built up incrementally
using previously computed regular expressions.

state qi with i < n + 1 has exactly one outgo-
ing transition to state qi+1 and all other transitions
are to states with labels at most i. Thus, removing
state qn+1 and all incoming and outgoing transi-
tions and making qn the only final state results in
the DFA for F(w1w2 · · ·wn−1). This process can
be repeated until the empty string is reached. This
leads to the observation that, when constructing an
unambiguous expression for F(w1w2 · · ·wn), we
can recover the expression for F(w1w2 · · ·wn−1)
extracting the regular expression at αn−10,n . Simi-
larly, the expression for F(w1w2 · · ·wk) is stored
at αk0,k+1.

At stage k of the state elimination procedure on
D, state q0 is only connected to states up to label
k − 1, thus αk−10,k+1 = ∅. Since

αk0,k+1 = α
k−1
0,k+1 + α

k−1
0,k (α

k−1
k,k )

∗αk−1k,k+1,

we can simplify the expression as

αk0,k+1 = α
k−1
0,k (α

k−1
k,k )

∗αk−1k,k+1.

In addition, we recall

αk−10,k = F(w1w2 · · ·wk−1).

Therefore,

F(w1 · · ·wk) = F(w1 · · ·wk−1)(αk−1k,k )
∗αk−1k,k+1

and

(αk−1k,k )
∗αk−1k,k+1 = F(w1 · · ·wk−1)\F(w1 · · ·wk).

Algorithm 2 is for the offline setting—we know
the entire string ahead of time and compute the in-
fix probabilities of each of its prefixes incremen-
tally. In Algorithm 2, following Section 3 and
the relationship between the tables α and β in
the proof of Lemma 1, we run the matrix eval-
uations of each regular expression instead of the



2738

b

a a b

a

b
1 2 3 4

b

a a b

a

b
1 2 3 4 50 λλ

b

a a b

a

b
1 2 3 4 5b

a
b

a a b

a

b
1 2 3 4 5b

a

0 6λ λ

A) B)

Figure 3: DFAs generated by the KMP algorithm. DFAA) acceptsF(aab) andB) acceptsF(aabb). The automata
below are the new finite automata after adding the new initial and final state for the state elimination procedure, as
described in Section 3.

regular expressions directly. At any step k, we
only consider tables αk and αk−1 and, thus, we
can employ a standard sliding window technique
for dynamic programming to reduce the required
space complexity. In this scheme, instead of hold-
ing all tables up to αk, we simply hold the two
most previous ones, reducing the space complex-
ity required by a factor of O(k). We call our two
tables T and T ′ and, to simplify the pseudocode,
make elements of these two tables behave as both
matrices and regular expressions. For example,
(Ti,j)

∗ simultaneously corresponds to the regular
expression (αi,j)∗ and the matrix M((αi,j)∗) =
(1−M(αi,j))−1.

Algorithm 2 Offline Incremental Infix
1: procedure INFIX(w = w1w2 · · ·wn ∈ Σ∗)
2: D ← DFA accepting F(w)
3: T ← (n+ 3)× (n+ 3) table
4: T0,1 ← 1
5: Tn+1,n+2 ← 1
6: for i ∈ [1, n+ 2]; j ∈ [1, n+ 2]; c ∈ Σ do
7: if δ(qi, c) = qj then
8: Ti,j ← Ti,j + M(c)
9: end if

10: end for
11: V← I
12: for k ∈ [0, n+ 1] do
13: V← V(Tk,k)∗Tk,k+1
14: yield VM(Σ∗)F
15: T ′ ← (n+ 3)× (n+ 3) table
16: for i ∈ [0, n+ 2]; j ∈ [0, n+ 2] do
17: T ′i,j ← Ti,j + Ti,k(Tk,k)∗Tk,j
18: end for
19: T ← T ′
20: end for
21: end procedure

Algorithm 2 begins by constructing DFA for

F(w), which takes linear time in the length of
w (Knuth et al., 1977) on Line 2. This DFA is
modified to contain the new start and end state
as described in Section 3 for a total of n + 3
states. We then begin to step through the state
elimination algorithm. After constructing the ini-
tial table based on the base cases described in Sec-
tion 3 from Line 3 to 10, we prepare our vector
V that will record results from the previous in-
fix calculation. At the beginning, V should hold
IM(F(λ)) = I. In the first step, we eliminate state
q1 of the automaton. Since state q0 leads to state
q1 with a λ-transition, F at this step corresponds
to F(λ) and F(λ) · Σ∗ = Σ∗, which trivially has
infix probability 1. At stage k of the for loop,
we compute F(w1w2 · · ·wk−1)\F(w1w2 · · ·wk)
on Line 13. We multiply this by V, which
holds F(w1w2 · · ·wk−1), which makes V now
store F(w1w2 · · ·wk). On Line 14, we emit
the infix probability of w1w2 · · ·wk by evaluat-
ing VMP(Σ∗)FP . From Line 15 to 18, we up-
date our state elimination table. On Line 19, we
use the sliding window technique and copy our
updated table into T which allows for T ′ to be
safely overwritten in the next iteration. This pro-
cess repeats until we have removed all states qi for
1 ≤ i ≤ n + 1, and therefore have computed the
infix probability of each prefix of w.

The loop starting on Line 12 executes O(|w|)
times since the automaton has |w| + 3 states. At
each iteration, we recompute the state-elimination
table, which has size O(|w|2). For each ele-
ment of the table we must perform one matrix
addition, two matrix multiplications, and one ma-
trix inverse on the matrices from the PFA, which
has overall time complexity O(|Q|m). Thus, the
runtime of Algorithm 2 is O(|w|(|w|2|Q|m)) =
O(|w|3|Q|m), where m is the matrix multi-
plication constant. Throughout the computa-



2739

States |Q| = 614 |Q| = 1028 |Q| = 1455
Infix

Length
Incremental Intersection Incremental Intersection Incremental Intersection

1 0.226 0.147 0.857 0.468 2.383 1.079
2 0.272 0.316 1.072 1.235 3.000 3.112
3 0.334 0.637 1.327 2.634 3.693 6.997
4 0.399 1.133 1.586 4.864 4.442 13.250
5 0.465 1.934 1.855 8.104 5.124 22.357
6 0.527 3.375 2.088 12.562 5.815 35.065
7 0.584 4.129 2.347 18.414 6.593 51.709
8 0.649 5.791 2.591 25.614 7.224 72.512
9 0.711 7.879 2.851 34.959 7.950 99.347

Total 4.169 25.342 16.574 108.853 46.224 305.428

Table 1: Experimental results (in seconds) for the incremental regular expression method and the intersection
method. The average of 10 runs is reported. For the incremental test, we measure the time for the step of building
the state elimination table and outputting the current infix probability.

tion, we generate a series of tables of size
O(|w| × |w|) with elements of size O(|QP | ×
|QP |). However, since we use the sliding win-
dow technique, we only save O(1) of these ta-
bles, leading to an overall space complexity of
O((|w||QP |)2). Note that the space complex-
ity matches that of the intersection method, but
the time complexity of the incremental method is
asymptotically faster; O(|w|3|QP |m) compared to
O(|w|(|w||QP |)m) = O(|w|m+1|QP |m).

6 Experimental Results

We verify the effectiveness of our method using
real-world data. We present 3 n-grams3 (two 2-
grams and one 3-gram) and their 3 PFAs with 614,
1028, and 1455 states extracted from the Brown
Corpus tagged with the Penn Tree Bank tagset us-
ing the NLTK library (Bird and Loper, 2004). As
a preprocessing step, we convert all punctuation to
the PUNC tag for a total of 35 tags. The n-grams
are built by computing the probability of reading
a given tag under the condition that we have just
seen a specific sequence of n tags. We then ran-
domly select a sequence of 9 tags as our string
across all experiments. For each of the n-grams,
we calculate the infix probability of each prefix of
our input string using the incremental regular ex-
pression method and the intersection method. We
record the average of 10 runs per infix calcula-
tion. All experiments were written in Python 3.5
and the automata and matrices were implemented
using NumPy. The experiments were run on an
AMD Ryzen 7 1700 (3.0 GHz) 8-Core Processor

3We have a similar performance improvement for all other
test cases that we extract from the dataset.

with 16GB of RAM.
In the worst-case (the case when |Q| = 1028 in

Table 1), the cumulative time for the incremental
shows an 556.77% speed-up. The largest individ-
ual infix probability calculation is in the experi-
ment with |Q| = 1455 when calculating the infix
of length 9, where the incremental method obtains
a 560.76% speed-up. These results show that the
incremental method is not only theoretically faster,
but also much faster in practice when compared to
the intersection method.

The runtime gap is quite large compared to what
is expected from the theoretical analysis of the
two algorithms. Additionally, in theory, the time
for each step of the incremental calculation should
remain essentially constant, but the experimental
results show that it grows at a slow rate. These
are most likely implementation issues stemming
from the large memory requirements involved in
the calculations.

The experimental results show that the incre-
mental infix method vastly outperforms the naive
intersection method. The ability to memoize
previous calculations allows the incremental ap-
proach to calculate the next infix probability much
faster than simply restarting the entire calculation.
This empirically verifies the asymptotic analysis
of the two approaches.

7 Conclusions

We have presented a new method for solving the
open problem of incrementally computing the in-
fix probabilities of a given string with respect to
a PFA. Our method utilizes unambiguous regular
expressions and is distinguished from the previous



2740

methods in that it does not alter the structure of
the PFA during the evaluation. Similarly to the al-
gorithm presented in (Nederhof and Satta, 2011a),
this method imposes no restrictions on the input
string.

We have showed that our method is asymp-
totically faster than the previously best-known
method. Furthermore, we have experimentally
evaluated the performance of our algorithm on a
real life dataset and have observed that the pro-
posed algorithm performs significantly better than
the intersection method in all cases.

Future directions of this line of research are to
determine a method for two sided incremental in-
fix computation—that is, given a computation for
the infix of w, compute the infix of wa or aw at
will. Currently, it is only possible to do one sided
incremental infix calculations. Computing the in-
fix incrementally in an online fashion—in which
we do not know the entire string ahead of time and
receive new characters in a stream—would be an-
other improvement. We believe that the current
method can be modified to work in an online set-
ting, possibly with an increase in runtime. Fur-
thermore, finding new classes of problems that can
benefit from a similar incremental calculation is
also interesting. Finally, extending this method to
work for PCFGs and more complex probabilistic
models is an important open problem.

Acknowledgements

The authors thank the reviewers for their detailed
and constructive comments.

This work was supported by the Institute for
Information & Communications Technology Pro-
motion (IITP) grant funded by the Korea govern-
ment (MSIP) (2018-0-00247, 2018-0-00276).

References
Steven Bird and Edward Loper. 2004. NLTK: The nat-

ural language toolkit. In Proceedings of the ACL
2004 on Interactive Poster and Demonstration Ses-
sions.

Ronald Book, Shimon Even, Sheila Greiback, and
Gene Ott. 1971. Ambiguity in graphs and expres-
sions. IEEE Transactions on Computers, 20:149–
153.

Dogan Can and Shrikanth Narayanan. 2015. A dy-
namic programming algorithm for computing n-
gram posteriors from lattices. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, EMNLP, pages 2388–2397.

Anna Corazza, Renato De Mori, Roberto Gretter, and
Giorgio Satta. 1991. Computation of probabili-
ties for an island-driven parser. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
13(9):936–950.

Ana L. N. Fred. 2000. Computation of substring proba-
bilities in stochastic grammars. In Grammatical In-
ference: Algorithms and Applications, pages 103–
114.

Adrià de Gispert, Graeme Blackwood, Gonzalo Igle-
sias, and William Byrne. 2013. N-gram posterior
probability confidence measures for statistical ma-
chine translation: an empirical study. Machine
Translation, 2:85–114.

Binod Gyawali, Gabriela Ramı́rez-de-la-Rosa, and
Thamar Solorio. 2013. Native language identifi-
cation: a simple n-gram based approach. In Pro-
ceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
BEA@NAACL-HLT, pages 224–231.

John E. Hopcroft and Jeff D. Ullman. 1979. Introduc-
tion to Automata Theory, Languages, and Computa-
tion. Addison-Wesley Publishing Company.

Donald E. Knuth, Jr. James H. Morris, and Vaughan R.
Pratt. 1977. Fast pattern matching in strings. SIAM
Journal on Computing, 6:323–350.

Robert McNaughton and Hisao Yamada. 1960. Regu-
lar expressions and state graphs for automata. IRE
Transactions on Electronic Computers, 9:39–47.

Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer Speech & Language,
16(1):69–88.

Mark-Jan Nederhof and Giorgio Satta. 2011a. Compu-
tation of infix probabilities for probabilistic context-
free grammars. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP, pages 1213–1221.

Mark-Jan Nederhof and Giorgio Satta. 2011b. Pre-
fix probabilities for linear context-free rewriting sys-
tems. In Proceedings of the 12th International Con-
ference on Parsing, pages 151–162.

Corinna Ng, Ross Wilkinson, and Justin Zobel. 2000.
Experiments in spoken document retrieval using
phoneme n-grams. Speech Communication, 32(1-
2):61–77.

Andreas Stolcke. 1995. An efficient probabilis-
tic context-free parsing algorithm that computes
prefix probabilities. Computational Linguistics,
21(2):165–201.

Volker Strassen. 1969. Gaussian elimination is not op-
timal. Numer. Math., 13:354–356.



2741

Enrique Vidal, Franck Thollard, Colin de la Higuera,
Francisco Casacuberta, and Rafael C. Carrasco.
2005a. Probabilistic finite-state machines–part I.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 27:1013–1025.

Enrique Vidal, Franck Thollard, Colin de la Higuera,
Francisco Casacuberta, and Rafael C. Carrasco.
2005b. Probabilistic finite-state machines–part II.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 27:1026–1039.

Theresa Wilson and Stephan Raaijmakers. 2008. Com-
paring word, character, and phoneme n-grams for
subjective utterance recognition. In INTERSPEECH
2008, 9th Annual Conference of the International
Speech Communication Association, pages 1614–
1617.


