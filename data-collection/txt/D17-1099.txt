



















































Zero-Shot Activity Recognition with Verb Attribute Induction


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 946–958
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Zero-Shot Activity Recognition with Verb Attribute Induction

Rowan Zellers and Yejin Choi
Paul G. Allen School of Computer Science & Engineering

University of Washington
Seattle, WA 98195, USA

{rowanz,yejin}@cs.washington.edu

Abstract

In this paper, we investigate large-scale
zero-shot activity recognition by modeling
the visual and linguistic attributes of ac-
tion verbs. For example, the verb “salute”
has several properties, such as being a light
movement, a social act, and short in dura-
tion. We use these attributes as the internal
mapping between visual and textual rep-
resentations to reason about a previously
unseen action. In contrast to much prior
work that assumes access to gold standard
attributes for zero-shot classes and focuses
primarily on object attributes, our model
uniquely learns to infer action attributes
from dictionary definitions and distributed
word representations. Experimental re-
sults confirm that action attributes inferred
from language can provide a predictive
signal for zero-shot prediction of previ-
ously unseen activities.

1 Introduction

We study the problem of inferring action verb at-
tributes based on how the word is defined and used
in context. For example, given a verb such as
“swig” shown in Figure 1, we want to infer var-
ious properties of actions such as motion dynam-
ics (moderate movement), social dynamics (soli-
tary act), body parts involved (face, arms, hands),
and duration (less than 1 minute) that are generally
true for the range of actions that can be denoted by
the verb “swig”.

Our ultimate goal is to improve zero-shot learn-
ing of activities in computer vision: predicting
a previously unseen activity by integrating back-
ground knowledge about the conceptual properties
of actions. For example, a computer vision system
may have seen images of “drink” activities during

Dictionary 
definitions

drool

drink

chug

sip

swig To drink liquid in great gulps

to drink a large 
amount 

especially of beer

To drink in small 
quantities

To let run from 
the mouth

To take into the 
mouth and 

swallow a liquid

medium 
motion

more 
solitary

N/A

activity more solitary

more 
solitary

medium 
motion

achieve
ment

solitary 
or 

social
activity

achieve
ment

activity

solitary 
or 

social

no 
motion

low 
motion

low 
motion

Word 
embeddings

zero-shot image

…
sip: 10%
chug: 25%
swig: 65%

Distribution over
Zero-shot labels

attribute-embedding space

Verb attribute induction from languageA)

B) Zero-shot activity recognition
training images

aspect
m

otion   

uses head

has effect 

on object

social

uncork

drool

lick

drink

Figure 1: An overview of our task. Our goal is
twofold. A: we seek to use use distributed word
embeddings in tandem with dictionary definitions
to obtain a high level understanding of verbs. B:
we seek to use these predicted attributes to allow
a classifier to recognize a broader set of activities
than what was seen in training time.

training, but not “swig”. Ideally, the system should
infer the likely visual characteristics of “swig” us-
ing world knowledge implicitly available in dictio-
nary definitions and word embeddings.

However, most existing literature on zero-shot
learning has focused on object recognition with
only a few notable exceptions (see Related Work

946



in Section 8). There are two critical reasons: ob-
ject attributes, such as color, shape, and texture,
are conceptually straightforward to enumerate. In
addition, they have distinct visual patterns which
are robust for current vision systems to recognize.
In contrast, activity attributes are more difficult
to conceptualize as they involve varying levels of
abstractness, which are also more challenging for
computer vision as they have less distinct visual
patterns. Noting this difficulty, Antol et al. (2014)
instead employ cartoon illustrations as intermedi-
ate mappings for zero-shot dyadic activity recog-
nition. We present a complementary approach:
that of tackling the abstractness of verb attributes
directly. We develop and use a corpus of verb at-
tributes, using linguistic theories on verb seman-
tics (e.g., aspectual verb classes of Vendler (1957))
and also drawing inspiration from studies on lin-
guistic categorization of verbs and their properties
(Friedrich and Palmer, 2014; Siegel and McKe-
own, 2000).

In sum, we present the first study aiming to re-
cover general action attributes for a diverse collec-
tion of verbs, and probe their predictive power for
zero-shot activity recognition on the recently in-
troduced imSitu dataset (Yatskar et al., 2016). Em-
pirical results show that action attributes inferred
from language can help classifying previously un-
seen activities and suggest several avenues for fu-
ture research on this challenging task. We publicly
share our dataset and code for future research.1

2 Action Verb Attributes

We consider seven different groups of action verb
attributes. They are motivated in part by poten-
tial relevance for visual zero-shot inference, and
in part by classical literature on linguistic theories
on verb semantics. The attribute groups are sum-
marized below.2 Each attribute group consists of
a set of attributes, which sums to K = 24 distinct
attributes annotated over the verbs.3

[1] Aspectual Classes We include the aspectual
verb classes of Vendler (1957):

• state: a verb that does not describe a chang-
ing situation (e.g. “have”, “be”)

1Available at http://github.com/rowanz/verb-attributes
2The full list is available in the supplemental section.
3Several of our attributes are categorical; if converted to

binary attributes, we would have 40 attributes in total.

• achievement: a verb that can be completed in
a short period of time (e.g. “open”, “jump”)

• accomplishment: a verb with a sense of com-
pletion over a longer period of time (e.g.
“climb”)

• activity: a verb without a clear sense of com-
pletion (e.g. “swim”, “walk”, “talk”)

[2] Temporal Duration This attribute group re-
lates to the aspectual classes above, but provides
additional estimation of typical time duration with
four categories. We categorize verbs by best-
matching temporal units: seconds, minutes, hours,
or days, with an additional option for verbs with
unclear duration (e.g., “provide”).

[3] Motion Dynamics This attribute group fo-
cuses on the energy level of motion dynamics in
four categories: no motion (“sleep”), low motion
(“smile”), medium (“walk”), or high (“run”). We
add an additional option for verbs whose motion
level depends highly on context, such as ‘finish.’

[4] Social Dynamics This attribute group fo-
cuses on the likely social dynamics, in particular,
whether the action is usually performed as a soli-
tary act, a social act, or either. This is graded on a
5-part scale from least social (−2) to either (+0)
to most social (+2)

[5] Transitivity This attribute group focuses on
whether the verb can take an object, or be used
without. This gives the model a sense of the im-
plied action dynamics of the verb between the
agent and the world. We record three variables:
whether or not the verb is naturally transitive on a
person (“I hug her” is natural), on a thing (“I eat
it”), and whether the verb is intransitive (“I run”).

[6] Effects on Arguments This attribute group
focuses on the effects of actions on agents and
other arguments. For each of the possible tran-
sitivities of the verb, we annotate whether or not it
involves location change (“travel”), world change
(“spill”), agent or object change (“cry”) , or no
visible change (“ponder”).

[7] Body Involvements This attribute group
specifies prominent body parts involved in carry-
ing out the action. For example, “open” typically
involves “hands” and “arms” when used in a phys-
ical sense. We use five categories: head, arms,
torso, legs, and other body parts.

947



Action Attributes and Contextual Variations
In general, contextual variations of action at-
tributes are common, especially for frequently
used verbs that describe everyday physical activi-
ties. For example, while “open” typically involves
“hands”, there are exceptions, e.g. “open one’s
eyes”. In this work, we focus on stereotypical or
prominent characteristics across a range of actions
that can be denoted using the same verb. Thus,
three investigation points of our work include: (1)
crowd-sourcing experiments to estimate the dis-
tribution of human judgments on the prominent
characteristics of everyday physical action verbs,
(2) the feasibility of learning models for inferring
the prominent characteristics of the everyday ac-
tion verbs despite the potential noise in the human
annotation, and (3) their predictive power in zero-
shot action recognition despite the potential noise
from contextual variations of action attributes. As
we will see in Section 7, our study confirms the
usefulness of studying action attributes and moti-
vates the future study in this direction.

Relevance to Linguistic Theories The key idea
in our work that action verbs project certain expec-
tations about their influence on their arguments,
their pre- and post-conditions, and their implica-
tions on social dynamics, etc., relates to the orig-
inal Frame theories of Baker et al. (1998a). The
study of action verb attributes are also closely
related to formal studies on verb categorization
based on the characteristics of the actions or states
that a verb typically associates to (Levin, 1993),
and cognitive linguistics literature that focus on
causal structure and force dynamics of verb mean-
ings (Croft, 2012).

3 Learning Verb Attributes from
Language

In this section we present our models for learn-
ing verb attributes from language. We consider
two complementary types of language-based in-
put: dictionary definitions and word embeddings.
The approach based on dictionary definitions re-
sembles how people acquire the meaning of a new
word from a dictionary lookup, while the approach
based on word embeddings resembles how people
acquire the meaning of words in context.

Overview This task follows the standard super-
vised learning approach where the goal is to pre-
dict K attributes per word in the vocabulary V .

Let xv ∈ X represent the input representation of a
word v ∈ V . For instance, xv could denote a word
embedding, or a definition looked up from a dic-
tionary (modeled as a list of tokens). Our goal is to
produce a model f : X → Rd that maps the input
to a representation of dimension d. Modeling op-
tions include using pretrained word embeddings,
as in Section 3.1, or using a sequential model to
encode a dictionary, as in Section 3.2.

Then, the estimated probability distribution
over attribute k is given by:

ŷv,k = σ(W(k)f(xv)). (1)

If the attribute is binary, then W(k) is a vector of
dimension d and σ is the sigmoid function. Other-
wise, W(k) is of shape dk × d, where dk is the di-
mension of attribute k, and σ is the softmax func-
tion. Let the vocabulary V be partitioned into sets
Vtrain and Vtest; then, we train by minimizing the
cross-entropy loss over Vtrain and report attribute-
level accuracy over words in Vtest.
Connection to Learning Object Attributes
This problem has been studied before for zero-shot
object recognition, but there are several key dif-
ferences. Al-Halah et al. (2016) build the ‘Class-
Attribute Association Prediction’ model (CAAP)
that classifies the attributes of an object class from
its name. They apply it on the Animals with At-
tributes dataset, a dataset containing 50 animal
classes, each described by 85 attributes (Lampert
et al., 2014). Importantly, these attributes are con-
crete details with semantically meaningful names
such as “has horns” and “is furry”. The CAAP
model takes advantage of this, consisting of a ten-
sor factorization model initialized by the word
embeddings of the object class names as well as
the attribute names. On the other hand, verb at-
tributes such as the ones we outline in Section 2,
are technical linguistic terms. Since word embed-
dings principally capture common word senses,
they are unsuited for verb attributes. Thus, we
evaluate two versions of CAAP as a baseline:
one where the model is preinitialized with GloVe
embeddings (Pennington et al., 2014) for the at-
tribute names (CAAP-pretrained), and one where
the model is learned from random initialization
(CAAP-learned).

3.1 Learning from Distributed Embeddings
One way of producing attributes is from dis-
tributed word embeddings such as word2vec

948



(Mikolov et al., 2013). Intuitively, we expect sim-
ilar verbs to have similar distributions of nearby
nouns and adverbs, which can greatly help us in
zero-shot prediction. In our experiments, we use
300-dimensional GloVe vectors trained on 840B
tokens of web data (Pennington et al., 2014). We
use logistic regression to predict each attribute, as
we found that extra hidden layers did not improve
performance. Thus, we let femb(xv) = wv, the
GloVe embedding of v, and use Equation 1 to get
the distribution over labels.

We additionally experiment with retrofitted em-
beddings, in which embeddings are mapped in
accordance with a lexical resource. Follow-
ing the approach of Faruqui et al. (2015), we
retrofit embeddings using WordNet (Miller, 1995),
Paraphrase-DB (Ganitkevitch et al., 2013), and
FrameNet (Baker et al., 1998b).

3.2 Learning from Dictionary Definitions
We additionally propose a model that learns the
attribute-grounded meaning of verbs through dic-
tionary definitions. This is similar in spirit to the
task of using a dictionary to predict word embed-
dings (Hill et al., 2016).

BGRU encoder Our first model involves a Bidi-
rectional Gated Recurrent Unit (BGRU) encoder
(Cho et al., 2014). Let xv,1:T be a definition for
verb v, with T tokens. To encode the input, we
pass it through the GRU equation:

~ht = GRU(xv,t, ~ht−1). (2)

Let ~h1 denote the output of a GRU applied on the
reversed input xv,T :1. Then, the BGRU encoder is
the concatenation f bgru = ~hT ‖ ~h1.
Bag-of-words encoder Additionally, we try two
common flavors of a Bag-of-Words model. In the
standard case, we first construct a vocabulary of
5000 words by frequency on the dictionary def-
initions. Then, f bow(xv) represents the one-hot
encoding f bow(xv)i = [i ∈ xv], in other words,
whether word i appears in definiton xv for verb v.

Additionally, we try out a Neural Bag-of-Words
model where the word embeddings in a defini-
tion are averaged (Iyyer et al., 2015). This is
fnbow(xv,1:T ) = 1|T |

∑T
t=1 f

emb(xv,t).

Dealing with multiple definitions per verb
One potential pitfall with using dictionary defini-
tions is that there are often many defnitions asso-
ciated with each verb. This creates a dataset bias

To drink in large draughts

swig

Attributes

concat
W(k)

femb

~h1 ~h2 ~h3 ~h4 ~h5

~h1~h2~h3~h4~h5

Figure 2: Overview of our combined dictionary +
embedding to attribute model. Our encoding is the
concatenation of a Bidirectional GRU of a defini-
tion and the word embedding for that word. The
encoding is then mapped to the space of attributes
using parameters W(k).

since polysemic verbs are seen more often. Ad-
ditionally, dictionary definitions tend to be sorted
by relevance, thus lowering the quality of the data
if all definitions are weighted equally during train-
ing. To counteract this, we randomly oversample
the definitions at training time so that each verb
has the same number of definitions.4 At test time,
we use the first-occurring (and thus generally most
relevant) definition per verb.

3.3 Combining Dictionary and Embedding
representations

We hypothesize that the two modalities of the dic-
tionary and distributional embeddings are comple-
mentary. Therefore, we propose an early fusion
(concatenation) of both categories. Figure 2 de-
scribes the GRU + embedding model – in other
words, fBGRU+emb = fBGRU‖femb. This can
likewise be done with any choice of definition en-
coder and word embedding.

4 Zero-Shot Activity Recognition

4.1 Verb Attributes as Latent Mappings

Given learned attributes for a collection of activi-
ties, we would like to evaluate their performance at
describing these activities from real world images
in a zero-shot setting. Thus, we consider several
models that classify an image’s label by pivoting
through an attribute representation.

4For the (neural) bag of words models, we also tried con-
catenating the definitions together per verb and then doing the
encoding. However, we found that this gave worse results.

949



Overview A formal description of the task is at
follows. Let the space of labels be V , partitioned
into Vtrain and Vtest. Let zv ∈ Z represent an
image with label v ∈ V; our goal is to correctly
predict this label amongst verbs v ∈ Vtest at test
time, despite never seeing any images with labels
in Vtest during training.

Generalization will be done through a lookup
table A, with known attributes for each v ∈ V .
Formally, for each attribute k we define it as:

A(k)v′,i =

{
1 if attribute k for verb v′ is i
−1 otherwise (3)

For binary attributes, we need only one entry per
verb, making A(k) a single column vector. Let our
image encoder be represented by the map g : Z →
Rd. We then use the linear map in Equation 1 to
produce the log-probability distribution over each
attribute k. The distribution over labels is then:

P (·|zv) = softmax
v′

(∑
k

A(k)W(k)g(zv)

)
(4)

where W(k) is a learned parameter that maps the
image encoder to the attribute representation. We
then train our model by minimizing the cross-
entropy loss over the training verbs Vtrain.
Convolutional Neural Network (CNN) Encoder
Our image encoder is a CNN with the Resnet-152
architecture (He et al., 2016). We use weights pre-
trained on ImageNet (Deng et al., 2009) and per-
form additional pretraining on ImSitu using the
classes Vtrain. After this, we remove the top layer
and set g(xv) to be the 2048-dimensional image
representation from the network.

4.1.1 Connection to other attribute models
Our model is similar to those of Akata et al. (2013)
and Romera-Paredes and Torr (2015) in that we
predict the attributes indirectly and train the model
through the class labels.5 It differs from sev-
eral other zeroshot models, such as Lampert et al.
(2014)’s Direct Attribute Prediction (DAP) model,
in that DAP is trained by maximizing the probabil-
ity of predicting each attribute and then multiplies
the probabilities at test time. Our use of joint train-
ing allows the recognition model to directly op-
timize class-discrimination rather than attribute-
level accuracy.

5Unlike these models, however, we utilize (some) cate-
gorical attributes and optimize using cross-entropy.

CNN

embedding 
prediction

attribute 
prediction

attribute 
lookup

embedding 
lookup

Wemb

A(k)

W(k)

g

Aemb

+
label prediction

zv

Figure 3: Our proposed model for combin-
ing attribute-based zero-shot learning and word-
embedding based transfer learning. The embed-
ding and attribute lookup layers are used to predict
a distribution of labels over Vtrain during training
and Vtest during testing.

4.2 Verb Embeddings as Latent Mappings
An additional method of doing zero-shot image
classification is by using word embeddings di-
rectly. Frome et al. (2013) build DeVISE, a model
for zero-shot learning on ImageNet object recog-
nition where the objective is for the image model
to predict a class’s word embedding directly. De-
VISE is trained by minimizing∑
v′∈Vtrain\{v}

max{0, .1+(wv′−wv)Wembg(zv)}

for each image zv. We compare against a version
of this model with fixed GloVe embeddings w.

Additionally, we employ a variant of our model
using only word embeddings. The equation is the
same as Equation 4, except using the matrix Aemb

as a matrix of word embeddings: i.e., for each la-
bel v we consider, we have Aembv = wv.

4.3 Joint prediction from Attributes and
Embeddings

To combine the representation power of the at-
tribute and embedding models, we build an en-
semble combining both models. This is done by
adding the logits before the softmax is applied:

softmax
v′

(∑
k

A(k)W(k)g(zv) + AembWembg(zv)

)
A diagram is shown in Figure 3. We find that
during optimization, this model can easily over-
fit, presumably by excessive coadaption of the em-
bedding and attribute components. To solve this,

950



we train the model to minimize the cross entropy
of three sources independently: the attributes only,
the embeddings only, and the sum, weighting each
equally.

Incorporating predicted and gold attributes
We additionally experiment with an ensemble
of our model, combining predicted and gold at-
tributes of Vtest. This allows the model to hedge
against cases where a verb attribute might have
several possible correct answers. A single model
is trained; at test time, we multiply the class level
probabilities P (·|zv) of each together to get the fi-
nal predictions.

5 Actions and Attributes Dataset

To evaluate our hypotheses on action attributes and
zero-shot learning, we constructed a dataset using
crowd-sourcing experiments. The Actions and At-
tributes dataset consists of annotations for 1710
verb templates, each consisting of a verb and an
optional particle (e.g. “put” or “put up”).

We selected all verbs from the ImSitu cor-
pus, consisting of images representing verbs
from many categories (Yatskar et al., 2016),
then extended the set using the MPII movie vi-
sual description dataset and ScriptBase datasets,
(Rohrbach et al., 2015; Gorinski and Lapata,
2015). We used the spaCy dependency parser
(Honnibal and Johnson, 2015) to extract the verb
template for each sentence, and collected annota-
tions on Mechanical Turk to filter out nonliteral
and abstract verbs. Turkers annotated this filtered
set of templates using the attributes described in
Section 2. In total, 1203 distinct verbs are in-
cluded. The templates are split randomly by verb;
out of 1710 total templates, we save 1313 for train-
ing, 81 for validation, and 316 for testing.

To provide signal for classifying these verbs, we
collected dictionary definitions for each verb us-
ing the Wordnik API,6 including only senses that
are explicitly labeled “verb.” This leaves us with
23,636 definitions, an average of 13.8 per verb.

6 Experimental Setup

BGRU pretaining We pretrain the BGRU
model on the Dictionary Challenge, a collection
of 800,000 word-definition pairs obtained from

6Available at http://developer.wordnik.com/
with access to American Heriatge Dictionary, the Century
Dictionary, the GNU Collaborative International Dictionary,
Wordnet, and Wiktionary.

Wordnik and Wikipedia articles (Hill et al., 2016);
the objective is to obtain a word’s embedding
given one of its definitions. For the BGRU model,
we use an internal dimension of 300, and embed
the words to a size 300 representation. The vocab-
ulary size is set to 30,000 (including all verbs for
which we have definitions). During pretraining,
we keep the architecture the same, except a differ-
ent 300-dimensional final layer is used to predict
the GloVe embeddings.

Following Hill et al. (2016), we use a ranking
loss. Let ŵ = Wembf(x) be the predicted word
embeddings for each definition x of a word in the
dictionary (not necessarily a verb). Let w be the
word’s embedding, and w̃ be the embedding of a
random dictionary word. The loss is then given
by:

L = max{0, .1− cos(w, ŵ) + cos(w, w̃)}

After pretraining the model on the Dictionary
Challenge, we fine-tune the attribute weights
W(k) using the cross-entropy over Equation 1.

Zero-shot with the imSitu dataset We build
our image-to-verb model on the newly introduced
imSitu dataset, which contains a diverse collection
of images depicting one of 504 verbs. The images
represent a variety of different semantic role labels
(Yatskar et al., 2016). Figure 4 shows examples
from the dataset. We apply our attribute split to
the dataset and are left with 379 training classes,
29 validation classes, and 96 test classes.

Zero-shot activity recognition baselines We
compare against several additional baseline mod-
els for learning from attributes and embeddings.
Romera-Paredes and Torr (2015) propose “Em-
barassingly Simple Zero-shot Learning” (ESZL),
a linear model that directly predicts class labels
through attributes and incorporates several types
of regularization. We compare against a variant
of Lampert et al. (2014)’s DAP model discussed
in Section 4.1.1. We additionally compare against
DeVISE (Frome et al., 2013), as mentioned in Sec-
tion 4.2. We use a Resnet-152 CNN finetuned on
the imSitu Vtrain classes as the visual features for
these baselines (the same as discussed in Section
4.1).

Additional implementation details are pro-
vided in the Appendix.

951



acc-macro acc-micro Body Duration Aspect Motion Social Effect Transi.
most frequent class 61.33 75.45 76.84 76.58 43.67 35.13 42.41 84.97 69.73

E
m

b
CAAP-pretrained 64.96 78.06 84.81 72.15 50.95 46.84 43.67 85.21 71.10
CAAP-learned 68.15 81.00 86.33 76.27 52.85 52.53 45.57 88.29 75.21
GloVe 66.60 79.69 85.76 75.00 50.32 48.73 43.99 86.52 75.84
GloVe + framenet 67.42 80.79 86.27 76.58 49.68 50.32 44.94 88.19 75.95
GloVe + ppdb 67.52 80.75 85.89 76.58 51.27 50.95 43.99 88.21 75.74
GloVe + wordnet 68.04 81.13 86.58 76.90 54.11 50.95 43.04 88.34 76.37

D
ic

t BGRU 66.05 79.44 85.70 76.90 51.27 48.42 40.51 86.92 72.68
BoW 62.53 77.61 83.54 76.58 48.42 35.76 36.39 86.31 70.68
NBoW 65.41 78.96 85.00 76.58 52.85 42.41 43.35 86.87 70.78

D
+E

NBoW + GloVe 67.52 80.76 86.84 75.63 53.48 51.90 41.77 88.03 75.00
BoW + GloVe 63.15 77.89 84.11 77.22 49.68 34.81 38.61 86.18 71.41
BGRU + GloVe 68.43 81.18 86.52 76.58 56.65 53.48 41.14 88.24 76.37

Table 1: Results on the text-to-attributes task. All values reported are accuracies (in %). For attributes
where multiple labels can be selected, the accuracy is averaged over all instances (e.g., the accuracy of
“Body” is given by the average of accuracies from correctly predicting Head, Torso, etc.). As such, we
report two ways of averaging the results: microaveraging (where the accuracy is the average of accuracies
on the underlying labels) and macroaveraging (where the accuracy is averaged together from the groups).

7 Experimental Results

7.1 Predicting Action Attributes from Text

Our results for action attribute prediction from text
are given in Table 1. Several examples are given
in the supplemental section in Table 3. Our results
on the text-to-attributes challenge confirm that it is
a challenging task for two reasons. First, there is
noise associated with the attributes: many verb at-
tributes are hard to annotate given that verb mean-
ings can change in context.7 Second, there is a
lack of training data inherent to the problem: there
are not many common verbs in English, and it
can be difficult to crowdsource annotations for rare
ones. Third, any system must compete with strong
frequency-based baselines, as attributes are gener-
ally sparse. Moreover, we suspect that were more
attributes collected (so as to cover more obscure
patterns), the sparsity would only increase.

Despite this, we report strong baseline results
on this problem, particularly with our embedding
based models. The performance gap between
embedding- and definition-based models can pos-
sibly be explained by the fact that the word em-
beddings are trained on a very large corpus of real-
world examples of the verb, while the definition is
only a single high-level representation meant to be
understood by someone who already speaks that
language. For instance, it is likely difficult for the
definition-based model to infer whether a verb is
transitive or not (Transi.), since definitions might
assume commonsense knowledge about the under-

7 As such, our attributes have a median Krippendorff Al-
pha of α = .359.

lying concepts the verb represents. The strong
performance of embedding models is further en-
hanced by using retrofitted word embeddings, sug-
gesting an avenue for improvement on language
grounding through better representation of linguis-
tic corpora.

We additionally see that both joint dictionary-
embedding models outperform the dictionary-only
models overall. In particular, the BGRU+GloVe
model performs especially well at determining the
aspect and motion attributes of verbs, particularly
relative to the baseline. The strong performance
of the BGRU+GloVe model indicates that there is
some signal that is missing from the distributional
embeddings that can be recovered from the dictio-
nary definition. We thus use the predictions of this
model for zero-shot image recognition.

Based on error analysis, we found that one com-
mon mode of failure is where contextual knowl-
edge is required. To give an example, the embed-
ding based model labels “shop” as a likely soli-
tary action. This is possibly because there are a
lack of similar verbs in Vtrain; by random chance,
“buy” is also in the test set. We see that this can be
partially mitigated by the dictionary, as evidenced
by the fact that the dictionary-based models label
“shop” as in between social and solitary. Still, it
is a difficult task to infer that people like to “visit
stores in search of merchandise” together.

7.2 Zero-shot Action Recognition

Our results for verb prediction from images are
given in Table 2. Despite the difficulty of pre-
dicting the correct label over 96 unseen choices,

952



Attributes used v ∈ Vtest
Model atts(P) atts(G) GloVe top-1 top-5

Random 1.04 5.20
DeVISE X 16.50 37.56

ESZL
X 3.60 14.81

X 3.27 13.21

DAP
X 3.35 16.69

X 4.33 17.56

Ours

X 4.79 19.98
X 7.04 22.19
X X 7.71 24.90

X 17.60 39.29
X X 18.10 41.46

X X 16.75 40.44
X X X 18.15 42.17

Table 2: Results on the image-to-verb task.
atts(P) refers to attributes predicted from the
BGRU+GloVe model described in Section 3,
atts(G) to gold attributes, and GloVe to GloVe vec-
tors. The accuracies reported are amongst the 96
unseen labels of Vtest.

our models show predictive power. Although our
attribute models do not outperform our embed-
ding models and DeVISE alone, we note that our
joint attribute and embedding model scores the
best overall, reaching 18.10% in top-1 and 41.46%
in top-5 accuracy when using gold attribute anno-
tations for the zero-shot verbs. This result is possi-
bly surprising given the small number of attributes
(K = 24) in total, of which most tend to be sparse
(as can be seen from the baseline performance in
Table 1). We thus hypothesize that collecting more
activity attributes would further improve perfor-
mance.

We also note the success in performing zero-
shot learning with predicted attributes. Perhaps
paradoxically, our attribute-only models (along
with DAP) perform better in both accuracy met-
rics when given predicted attributes at test time, as
opposed to gold attributes. Further, we get an ex-
tra boost by ensembling predictions of our model
when given two sets of attributes at test time, giv-
ing us the best results overall at 18.15% top-1 ac-
curacy and 42.17% top-5. Interestingly, better per-
formance with predicted attributes is also reported
by Al-Halah et al. (2016): predicting the attributes
with their CAAP model and then running the DAP
model on these predicted attributes outperforms
the use of gold attributes at test time. It is some-

what unclear why this is the case - possibly, there
is some bias in the attribute labeling, which the at-
tribute predictor can correct for.

In addition to quantitative results, we show
some zero-shot examples in Figure 4. The ex-
amples show inherent difficulty of zero-shot ac-
tion recognition. Incorrect predictions are of-
ten reasonably related to the situation (“rub” vs
“dye”) but picking the correct target verb based
on attribute-based inference is still a challenging
task.

Although our results appear promising, we ar-
gue that our model still fails to represent much of
the semantic information about each image class.
In particular, our model is prone to hubness: the
overprediction of a limited set of labels at test
time: those that closely match signatures of ex-
amples in the training set. This problem has pre-
viously been observed with the use of word em-
beddings for zero-shot learning (Marco and Geor-
giana, 2015) and can be seen in our examples (for
instance, the over-prediction of “buy”). Unfortu-
nately, we were unable to mitigate this problem
in a way that also led to better quantitative results
(for instance, by using a ranking loss as in DeVISE
(Frome et al., 2013)). We thus leave resolving the
hubness problem in zero-shot activity recognition
as a question for future work.

8 Related Work

Learning attributes from embeddings
Rubinstein et al. (2015) seek to predict McRae
et al. (2005)’s feature norms from word embed-
dings of concrete nouns. Likewise, the CAAP
model of Al-Halah et al. (2016) predicts the object
attributes of concrete nouns for use in zero-shot
learning. In contrast, we predict verb attributes. A
related task is that of improving word embeddings
using multimodal data and linguistic resources
(Faruqui et al., 2015; Silberer et al., 2013; Ven-
drov et al., 2016). Our work runs orthogonal to
this, as we focus on word attributes as a tool for a
zero-shot activity recognition pipeline.

Zero-shot learning with objects Though dis-
tinct, our work is related to zero-shot learn-
ing of objects in computer vision. There are
several datasets (Nilsback and Zisserman, 2008;
Welinder et al., 2010) and models developed on
this task (Romera-Paredes and Torr (2015); Lam-
pert et al. (2014); Mukherjee and Hospedales
(2016); Farhadi et al. (2010)). In addition,

953



shopsquint dye

performcough fling ignite

mourn

Figure 4: Predictions on unseen classes from our attribute+embedding model with gold attributes. The
top and bottom rows show successful and failure cases respectively. The bars to the right of each image
represent a probability distribution, showing the ground truth class and the top 5 scoring incorrect classes.

Ba et al. (2015) augment existing datasets with de-
scriptive Wikipedia articles so as to learn novel ob-
jects from descriptive text. As illustrated in Sec-
tion 1, action attributes pose unique challenges
compared to object attributes, thus models devel-
oped for zero-shot object recognition are not as
effective for zero-shot action recognition, as has
been empirically shown in Section 7.

Zero-shot activity recognition In prior work,
zero-shot activity recognition has been studied on
video datasets, each containing a selection of con-
crete physical actions. The MIXED action dataset,
itself a combination of three action recognition
datasets, has 2910 labeled videos with 21 actions,
each described by 34 action attributes (Liu et al.,
2011). These action attributes are concrete bi-
nary attributes corresponding to low-level physical
movements, for instance, “arm only motion,” “leg:
up-forward motion.” By using word embeddings
instead of attributes, Xu et al. (2017) study video
activity recognition on a variety of action datasets,
albeit in the transductive setting wherein access to
the test labels is provided during training. In com-
parison with our work on imSitu (Yatskar et al.,
2016), these video datasets lack broad coverage
of verb-level classes (and for some, sufficient data
points per class).

The abstractness of broad-coverage activity la-
bels makes the problem much more difficult to
study with attributes. To get around this, Antol
et al. (2014) present a synthetic dataset of car-
toon characters performing dyadic actions, and use
these cartoon illustrations as internal mappings for
zero-shot recognition of dyadic actions in real-
world images. We investigate an alternative ap-
proach by using linguistically informed verb at-

tributes for activity recognition.

9 Future work / Conclusion

Several possibilities remain open for future work.
First, more attributes could be collected and evalu-
ated, possibly integrating other linguistic theories
about verbs, with more accurate modeling of con-
text. Second, while our experiments use attributes
as a pivot between language and vision domains,
the effects of this could be explored more in future
work. In particular, since our experiments show
that unsupervised word embeddings significantly
help performance, it might be desirable to learn
data-driven attributes in an end-to-end fashion di-
rectly from a large corpus or from dictionary defi-
nitions. Third, future research on action attributes
should ideally include videos to better capture at-
tributes that require temporal signals.

Overall, however, our work presents a strong
early step towards zero-shot activity recognition,
a relatively less studied task that poses several
unique challenges over zero-shot object recogni-
tion. We introduce new action attributes motivated
by linguistic theories and demonstrate their empir-
ical use for reasoning about previously unseen ac-
tivities.

Acknowledgements We thank the anonymous
reviewers, Mark Yatskar, Luke Zettlemoyer, and
Yonatan Bisk, for their helpful feedback. We also
thank the Mechanical Turk workers and members
of the XLab, who helped with the annotation pro-
cess. This work is supported by the National Sci-
ence Foundataion Graduate Research Fellowship
(DGE-1256082), the NSF grant (IIS-1524371),
DARPA CwC program through ARO (W911NF-
15-1-0543), and gifts by Google and Facebook.

954



References
Zeynep Akata, Florent Perronnin, Zaid Harchaoui,

and Cordelia Schmid. 2013. Label-embedding for
attribute-based classification. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition, pages 819–826.

Ziad Al-Halah, Makarand Tapaswi, and Rainer Stiefel-
hagen. 2016. Recovering the missing link: Pre-
dicting class-attribute associations for unsupervised
zero-shot learning. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recogni-
tion, pages 5975–5984.

Stanislaw Antol, C. Lawrence Zitnick, and Devi
Parikh. 2014. Zero-Shot Learning via Visual Ab-
straction. In ECCV.

Jimmy Ba, Kevin Swersky, Sanja Fidler, and Rus-
lan Salakhutdinov. 2015. Predicting deep zero-shot
convolutional neural networks using textual descrip-
tions. In ICCV.

Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998a. The Berkeley FrameNet project. In
COLING-ACL ’98: Proceedings of the Conference,
pages 86–90, Montreal, Canada.

Collin F Baker, Charles J Fillmore, and John B Lowe.
1998b. The berkeley framenet project. In Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics-
Volume 1, pages 86–90. Association for Computa-
tional Linguistics.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734, Doha, Qatar. Association for Computational
Linguistics.

William Croft. 2012. Verbs: Aspect and causal struc-
ture. OUP Oxford. Pg. 16.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hi-
erarchical image database. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Con-
ference on, pages 248–255. IEEE.

Ali Farhadi, Ian Endres, and Derek Hoiem. 2010.
Attribute-centric recognition for cross-category gen-
eralization. In Computer Vision and Pattern Recog-
nition (CVPR), 2010 IEEE Conference on, pages
2352–2359. IEEE.

Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,
Chris Dyer, Eduard Hovy, and Noah A Smith. 2015.
Retrofitting Word Vectors to Semantic Lexicons.
Association for Computational Linguistics.

Annemarie Friedrich and Alexis Palmer. 2014. Auto-
matic prediction of aspectual class of verbs in con-
text. In ACL (2), pages 517–523.

Andrea Frome, Greg S Corrado, Jon Shlens, Samy
Bengio, Jeff Dean, Marco Aurelio Ranzato, and
Tomas Mikolov. 2013. DeViSE: A Deep Visual-
Semantic Embedding Model. In C. J. C. Burges,
L. Bottou, M. Welling, Z. Ghahramani, and K. Q.
Weinberger, editors, Advances in Neural Informa-
tion Processing Systems 26, pages 2121–2129. Cur-
ran Associates, Inc.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. Ppdb: The paraphrase
database. In HLT-NAACL, pages 758–764.

Philip John Gorinski and Mirella Lapata. 2015. Movie
script summarization as graph-based scene extrac-
tion. In Proceedings of the 2015 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1066–1076, Denver, Colorado. As-
sociation for Computational Linguistics.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770–
778.

Felix Hill, KyungHyun Cho, Anna Korhonen, and
Yoshua Bengio. 2016. Learning to Understand
Phrases by Embedding the Dictionary. Transac-
tions of the Association for Computational Linguis-
tics, 4(0):17–30.

Matthew Honnibal and Mark Johnson. 2015. An im-
proved non-monotonic transition system for depen-
dency parsing. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1373–1378, Lisbon, Portugal. As-
sociation for Computational Linguistics.

Mohit Iyyer, Varun Manjunatha, Jordan L Boyd-
Graber, and Hal Daumé III. 2015. Deep unordered
composition rivals syntactic methods for text classi-
fication. In ACL (1), pages 1681–1691.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Christoph H Lampert, Hannes Nickisch, and Stefan
Harmeling. 2014. Attribute-based classification for
zero-shot visual object categorization. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, 36(3):453–465.

Beth Levin. 1993. English verb classes and alterna-
tions : a preliminary investigation.

Jingen Liu, Benjamin Kuipers, and Silvio Savarese.
2011. Recognizing human actions by attributes. In
Computer Vision and Pattern Recognition (CVPR),
2011 IEEE Conference on, pages 3337–3344. IEEE.

955



Angeliki Lazaridou Marco, Baroni and Dinu Geor-
giana. 2015. Hubness and pollution: Delving into
cross-space mapping for zero-shot learning. ACL.

Ken McRae, George S Cree, Mark S Seidenberg, and
Chris McNorgan. 2005. Semantic feature produc-
tion norms for a large set of living and nonliving
things. Behav Res Methods, 37(4):547–559.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39–
41.

Tanmoy Mukherjee and Timothy Hospedales. 2016.
Gaussian visual-linguistic embedding for zero-shot
recognition. EMNLP.

Maria-Elena Nilsback and Andrew Zisserman. 2008.
Automated flower classification over a large number
of classes. In Computer Vision, Graphics & Image
Processing, 2008. ICVGIP’08. Sixth Indian Confer-
ence on, pages 722–729. IEEE.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al. 2011. Scikit-learn:
Machine learning in python. Journal of Machine
Learning Research, 12(Oct):2825–2830.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP, volume 14, pages 1532–
1543.

Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and
Bernt Schiele. 2015. A Dataset for Movie Descrip-
tion. arXiv:1501.02530 [cs]. ArXiv: 1501.02530.

Bernardino Romera-Paredes and Philip HS Torr. 2015.
An embarrassingly simple approach to zero-shot
learning. In ICML, pages 2152–2161.

Dana Rubinstein, Effi Levi, Roy Schwartz, and Ari
Rappoport. 2015. How well do distributional mod-
els capture different types of semantic knowledge?
In Proceedings of the 53nd Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers).

Eric V Siegel and Kathleen R McKeown. 2000.
Learning methods to combine linguistic indica-
tors: Improving aspectual classification and reveal-
ing linguistic insights. Computational Linguistics,
26(4):595–628.

Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of semantic representation with vi-
sual attributes. In Proceedings of the 51st Annual

Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 572–582,
Sofia, Bulgaria. Association for Computational Lin-
guistics.

Zeno Vendler. 1957. Verbs and Times. The Philosoph-
ical Review, 66(2):143–160.

Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel
Urtasun. 2016. Order-embeddings of images and
language. In ICLR.

Peter Welinder, Steve Branson, Takeshi Mita, Cather-
ine Wah, Florian Schroff, Serge Belongie, and Pietro
Perona. 2010. Caltech-ucsd birds 200.

Xun Xu, Timothy Hospedales, and Shaogang Gong.
2017. Transductive zero-shot action recognition by
word-vector embedding. International Journal of
Computer Vision, pages 1–25.

Mark Yatskar, Luke Zettlemoyer, and Ali Farhadi.
2016. Situation recognition: Visual semantic role
labeling for image understanding. In Proceedings of
the IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 5534–5542.

A Supplemental

Implementation details

Our CNN and BGRU models are built in Py-
Torch8. All of our one-layer neural network mod-
els are built in Scikit-learn (Pedregosa et al., 2011)
using the provided LogisticRegression class (us-
ing one-versus-rest if appropriate). Our neural
models use the Adam optimizer (Kingma and Ba,
2014), though we weak the default hyperparame-
ters somewhat.

Recall that our dictionary definition model is
a bidirectional GRU with a hidden size of 300,
with a vocabulary size of 30,000. After pretraining
on the Dictionary Challenge, we freeze the word
embeddings and apply a dropout rate of 50% be-
fore the final hidden layer. We found that such an
aggressive dropout rate was necessary due to the
small size of the training set. During pretraining,
we used a learning rate of 10−4, a batch size of
64, and set the Adam parameter � to the default
1e−8. During finetuning, we set � = 1.0 and the
batch size to 32. In general, we found that setting
too low of an � during finetuning caused our zero-
shot models to update parameters too aggressively
during the first couple of updates, leading to poor
results.

For our CNN models, we pretrained the Resnet
152 (initialized with imagenet weights) on the

8pytorch.org

956



training classes of the imSitu dataset, using a
learning rate of 10−4 and � = 10−8. During fine-
tuning, we dropped the learning rate to 10−5 and
set � = 10−1. We also froze all parameters except
for the final resnet block, and the linear attribute
and embedding weights. We also found L2 regu-
larization quite important in reducing overfitting,
and we applied regularization at a weight of 10−4

to all trainable parameters.

Full list of attributes
The following is a full list of the attributes. In
addition to the attributes presented here, we also
crowdsourced attributes for the emotion content of
each verb (e.g., happiness, sadness, anger, and sur-
prise). However, we found these annotations to be
skewed towards “no emotion”, since most verbs
do not strongly associate with a specific emotion.
Thus, we omit them in our experiments.

(1) Aspectual Classes: one attribute with 5 values:

(a) State
(b) Achievement
(c) Accomplishment
(d) Activity
(e) Unclear without context

(2) Temporal Duration: one attribute with 5 values:

(a) Atemporal
(b) On the order of seconds
(c) On the order of hours
(d) On the order of days

(3) Motion Dynamics: One attribute with 5 values:

(a) unclear without context
(b) No motion
(c) Low motion
(d) Medium motion
(e) High motion

(4) Social Dynamics: One attribute with 5 values:

(a) solitary
(b) likely solitary
(c) solitary or social
(d) likely social
(e) social

(5) Transitivity: Three binary attributes:

(a) Intransitive: 1 if the verb can be used intransitively,
0 otherwise

(b) Transitive (person): 1 if the verb can be used in the
form “<someone>”, 0 otherwise

(c) Transitive (object): 1 if the verb can be used in the
form “<verb> something”, 0 otherwise

(6) Effects on Arguments: 12 binary attributes

(a) Intransitive 1: 1 if the verb is intransitive and the
subject moves somewhere

(b) Intransitive 2: 1 if the verb is intransitive and the
external world changes

(c) Intransitive 3: 1 if the verb is intransitive, and the
subject’s state changes

(d) Intransitive 4: 1 if the verb is intransitive, and noth-
ing changes

(e) Transitive (obj) 1: 1 if the verb is transitive for ob-
jects and the object moves somewhere

(f) Transitive (obj) 2: 1 if the verb is transitive for ob-
jects and the external world changes

(g) Transitive (obj) 3: 1 if the verb is transitive for ob-
jects and the object’s state changes

(h) Transitive (obj) 4: 1 if the verb is transitive for ob-
jects and nothing changes

(i) Transitive (person) 1: 1 if the verb is transitive for
people and the object is a person that moves some-
where

(j) ‘Transitive (person) 2: 1 if the verb is transitive for
people and the external world changes

(k) Transitive (person) 3: 1 if the verb is transitive for
people and if the object is a person whose state
changes

(l) Transitive (person) 4: 1 if the verb is transitive for
people and nothing changes

(7) Body Involements: 5 binary attributes

(a) Arms: 1 if arms are used
(b) Head: 1 if head is used
(c) Legs: 1 if legs are used
(d) Torso: 1 if torso is used
(e) Other: 1 if another body part is used

957



Model Definition Social Aspect Energy Time Body part

sh
op

GT To visit stores in
search of mer-
chandise or bar-
gains

likely social accomplish. high hours arms,head
embed likely solitary activity medium minutes
BGRU solitary or social activity medium minutes
BGRU+ solitary or social activity medium minutes

m
as

h

GT
To convert malt
or grain into
mash

likely solitary activity high seconds arms
embed likely solitary activity medium seconds arms
BGRU solitary or social achievement medium seconds arms
BGRU+ likely solitary activity high seconds arms

ph
ot

og
ra

ph GT
To take a photo-
graph of

solitary or social achievement low seconds arms,head
embed solitary or social accomplish. medium minutes arms
BGRU solitary or social achievement medium seconds arms
BGRU+ solitary or social unclear low seconds arms

sp
ew

ou
t GT eject or send

out in large
quantities also
metaphorical

solitary or social achievement high seconds head
embed likely solitary achievement medium seconds
BGRU solitary or social achievement high seconds arms
BGRU+ likely solitary achievement medium seconds

te
ar

GT
To pull apart or
into pieces by
force rend

likely solitary achievement low seconds arms
embed solitary or social achievement medium seconds arms
BGRU solitary or social achievement high seconds arms
BGRU+ solitary or social achievement high seconds arms

sq
ui

nt

GT To look with
the eyes partly
closed as in
bright sunlight

likely solitary achievement low seconds head
embed likely solitary achievement low seconds head
BGRU likely solitary achievement low seconds head
BGRU+ likely solitary achievement low seconds head

sh
ak

e

GT To cause to
move to and
fro with jerky
movements

solitary or social activity medium seconds
embed likely solitary achievement medium seconds arms
BGRU likely solitary activity medium seconds
BGRU+ likely solitary activity medium seconds

do
ze

GT
To sleep lightly
and intermit-
tently

likely solitary state none minutes head
embed likely solitary achievement medium seconds
BGRU likely solitary achievement low seconds
BGRU+ likely solitary activity low seconds

w
ri

th
e

GT
To twist as in
pain struggle or
embarrassment

solitary or social activity high seconds arms,torso
embed likely solitary activity medium seconds
BGRU likely solitary activity medium seconds arms
BGRU+ likely solitary activity medium seconds

Table 3: Example sentences and predicted attributes. Due to space constraints, we only list a few repre-
sentative attributes and verbs.

958


