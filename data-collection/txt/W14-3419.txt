



















































FFTM: A Fuzzy Feature Transformation Method for Medical Documents


Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 128–133,
Baltimore, Maryland USA, June 26-27 2014. c©2014 Association for Computational Linguistics

FFTM: A Fuzzy Feature Transformation Method for Medical Documents

Amir Karami, Aryya Gangopadhyay
Information Systems Department

University of Maryland Baltimore County
Baltimore, MD, 21250

amir3@umbc.edu,gangopad@umbc.edu

Abstract

The vast array of medical text data repre-
sents a valuable resource that can be an-
alyzed to advance the state of the art in
medicine. Currently, text mining meth-
ods are being used to analyze medical re-
search and clinical text data. Some of the
main challenges in text analysis are high
dimensionality and noisy data. There is a
need to develop novel feature transforma-
tion methods that help reduce the dimen-
sionality of data and improve the perfor-
mance of machine learning algorithms. In
this paper we present a feature transfor-
mation method named FFTM. We illus-
trate the efficacy of our method using lo-
cal term weighting, global term weighting,
and Fuzzy clustering methods and show
that the quality of text analysis in medical
text documents can be improved. We com-
pare FFTM with Latent Dirichlet Alloca-
tion (LDA) by using two different datasets
and statistical tests show that FFTM out-
performs LDA.

1 Introduction

The exponential growth of medical text data
makes it difficult to extract useful information in a
structured format. Some important features of text
data are sparsity and high dimensionality. This
means that while there may be a large number
of terms in most of the documents in a corpus,
any one document may contain a small percentage
of those terms (Aggarwal and Zhai, 2012). This
characteristic of medical text data makes feature
transformation an important step in text analysis.
Feature transformation is a pre-processing step in
many machine-learning methods that is used to
characterize text data in terms of a different num-
ber of attributes in lower dimensions. This tech-
nique has a direct impact on the quality of text

mining methods. Topic models such as LDA has
been used as one of popular feature transforma-
tion techniques (Ramage et al., 2010). However,
fuzzy clustering methods, particularly in combina-
tion with term weighting methods, have not been
explored much in medical text mining.

In this research, we propose a new method
called FFTM to extract features from free-text
data. The rest of the paper is organized in the fol-
lowing sections. In the section 2, we review re-
lated work. Section 3 contains details about our
method. Section 4 describes our experiments, per-
formance evaluation, and discussions of our re-
sults. Finally we present a summary, limitations,
and future work in the last section.

2 Related Work

Text analysis is an important topic in medical in-
formatics that is challenging due to high sparse
dimensionality data. Big dimension and diver-
sity of text datasets have been motivated medi-
cal researchers to use more feature transforma-
tion methods. Feature transformation methods en-
capsulate a text corpus in smaller dimensions by
merging the initial features. Topic model is one of
popular feature transformation methods. Among
topic models, LDA (Blei et al., 2003) has been
considered more due to its better performance
(Ghassemi et al., 2012; Lee et al., 2010).

One of methods that has not been fully con-
sidered in medical text mining is Fuzzy cluster-
ing. Although most of Fuzzy Clusterings work
in medical literature is based on image analysis
(Saha and Maulik, 2014; Cui et al., 2013; Beevi
and Sathik, 2012), a few work have been done
in medical text mining (Ben-Arieh and Gullipalli,
2012; Fenza et al., 2012) by using fuzzy cluster-
ing. The main difference between our method and
other document fuzzy clustering such as (Singh et
al., 2011) is that our method use fuzzy clustering
and word weighting as a pre-processing step for

128



feature transformation before implementing any
classification and clustering algorithms; however,
other methods use fuzzy clustering as a final step
to cluster the documents. Our main contribution
is to improve the quality of input data to improve
the output of fuzzy clustering. Among fuzzy clus-
tering methods, Fuzzy C-means (Bezdek, 1981)
is the most popular one (Bataineh et al., 2011).
In this research, we propose a novel method that
combines local term weighting and global term
weighting with fuzzy clustering.

3 Method

In this section, we detail our Fuzzy Feature Trans-
formation Method (FFTM) and describe the steps.
We begin with a brief review of LDA.

LDA is a topic model that can extract hidden
topics from a collection of documents. It assumes
that each document is a mixture of topics. The out-
put of LDA are the topic distributions over docu-
ments and the word distributions over topics. In
this research, we use the topics distributions over
documents. LDA uses term frequency for local
term weighting.

Now we introduce FFTM concepts and nota-
tions. This model has three main steps includ-
ing Local Term Weighting (LTW), Global Term
Weighting (GTM), and Fuzzy Clustering (Algo-
rithm 1). In this algorithm, each step is the out
put of each step will be the input of the next step.

Step 1: The first step is to calculate LTW.
Among different LTW methods we use term fre-
quency as a popular method. Symbol fij defines
the number of times term i happens in document
j.We have n documents and m words.Let

b(fij) =

{
1 fij > 0
0 fij = 0

(1)

pij =
fij∑
j fij

(2)

The outputs of this step are b(fij), fij , and pij .
We use them as inputs for the second step.

Step 2: The next step is to calculate GTW. We
explore four GTW methods in this paper includ-
ing Entropy, Inverse Document Frequency (IDF),
Probabilistic Inverse Document Frequency (Pro-
bIDF), and Normal(Table 1).

IDF assigns higher weights to rare terms and
lower weights to common terms (Papineni, 2001).
ProbIDF is similar to IDF and assigns very low

Algorithm 1 FFTM algorithm
Functions:E():Entropy;I():IDF;PI():ProbIDF;
NO():Normal; FC():Fuzzy Clustering.
Input: Document Term Matrix
Output: Clustering membership value (µij)
for all documents and clusters.

1: Remove stop words
Step 1: Calculate LTW

2: fori = 1 to ndo
3: forj = 1 to mdo
4: Calculate fij , b(fij), pij
5: endfor
6: endfor

Step 2: Calculate GTW
7: fori = 1 to ndo
8: forj = 1 to mdo
9: Execute E(pij ,n),I(fij ,n),PI(b(fij),n),

NO(fij ,n)
10: endfor
11: endfor

Step 3: Perform Fuzzy Clustering
12: Execute FC(E),FC(I),FC(PI),FC(NO)

Table1: GTW Methods

Name Formula

Entropy 1 +
∑

j
pij log2(pij)

log2 n

IDF log2
n∑
j

fij

ProbIDF log2
n−

∑
j

b(fij)∑
j

b(fij)

Normal 1√∑
j

f2ij

negative weight for the terms happen in every doc-
ument (Kolda, 1998). In Entropy, it gives higher
weight for the terms happen less in few documents
(Dumais, 1992). Finally, Normal is used to correct
discrepancies in document lengths and also nor-
malize the document vectors. The outputs of this
step are the inputs of the last step.

Step 3: Fuzzy clustering is a soft clustering
technique that finds the degree of membership for
each data point in each cluster, as opposed to
assigning a data point only one cluster. Fuzzy
clustering is a synthesis between clustering and
fuzzy set theory. Among fuzzy clustering meth-
ods, Fuzzy C-means (FCM) is the most popular
one and its goal is to minimize an objective func-

129



tion by considering constraints:

Min Jq(µ, V,X) =
c∑

i=1

n∑
j=1

(µij)qD2ij (3)

subject to:
0 ≤ µij ≤ 1; (4)

i ∈ {1, .., c} and j ∈ {1, ..., n} (5)
c∑

i=1

µij = 1 (6)

0 <
n∑

j=1

µij < n; (7)

Where:

n= number of data
c= number of clusters
µij= membership value
q= fuzzifier, 1 < q ≤ ∞
V = cluster center vector

Dij = d(xj , vi)= distance between xj and vi

By optimizing eq.3:

µij =
1∑c

k=1(
Dij
Dkj

)
2

q−1
(8)

vi =
∑n

j=1(µij)
qxj∑n

j=1(µij)q
(9)

The iterations in the clustering algorithms con-
tinue till the the maximum changes in µij becomes
less than or equal to a pre-specified threshold. The
computational time complexity is O(n). We use
µij as the degree of clusters’ membership for each
document.

4 Experimental Results

In this section, we evaluate FFTM against LDA
using two measures: document clustering inter-
nal metrics and document classification evalua-
tion metrics by using one available text datasets.
We use Weka1for classification evaluation, MAL-
LET2package with its default setting for imple-
menting LDA, Matlab fcm package3for imple-
menting FCM clustering, and CVAP Matlab pack-
age4for clustering validation.

1http://www.cs.waikato.ac.nz/ml/weka/
2http://mallet.cs.umass.edu/
3http://tinyurl.com/kl33w67
4http://tinyurl.com/kb5bwnm

4.1 Datasets
We leverage two available datasets in this re-
search. Our first test dataset called Deidentified
Medical Text5 is an unlabeled corpus of 2434
nursing notes with 12,877 terms after removing
stop words. The second dataset 6 is a labeled cor-
pus of English scientific medical abstracts from
Springer website. It is included 41 medical jour-
nals ranging from Neurology to Radiology. In this
research, we use the first 10 journals including:
Arthroscopy, Federal health standard sheet, The
anesthetist, The surgeon, The gynecologist, The
dermatologist, The internist, The neurologist, The
Ophthalmology, The orthopedist, and The pathol-
ogist. In our experiments we select three subsets
from the above journals, the first two with 4012
terms and 171 documents, first five with 14189
terms and 1527 documents, and then all ten re-
spectively with 23870 terms and 3764 documents
to track the performance of FFTM and LDA by
increasing the number of documents and labels.

4.2 Document Clustering
The first evaluation comparing FFTM with LDA is
document clustering by using the first dataset. In-
ternal and external validation are two major meth-
ods for clustering validation; however, compari-
son between these two major methods shows that
internal validation is more more precise (Rendón
et al., 2011). We evaluate different number of fea-
tures (topics) and clusters by using two internal
clustering validation methods including Silhouette
index and Calinski-Harabasz index using K-means
with 500 iterations. Silhouette index shows that
how closely related are objects in a cluster and
how distinct a cluster from other other clusters.
The higher value means the better result.The Sil-
houette index (S) is defined as:

S(i) =
(b(i)− a(i))

Max{a(i), b(i)} (10)

Where a(i) is the average dissimilarity of sam-
ple i with the same data in a cluster and b(i) is the
minimum average dissimilarity of sample i with
other data that are not in the same cluster.

Calinski-Harabasz index (CH) valuates the
cluster validity based on the average between- and
within-cluster sum of squares.It is defined as:

5http://tinyurl.com/kfz2hm4
6http://tinyurl.com/m2c8se6

130



2 3 4 5 6 7 8
0

0.2

0.4

0.6

0.8

# Clusters

Si
lh

ou
et

te
In

de
x

(a) 50 Features

2 3 4 5 6 7 8
0

0.2

0.4

0.6

0.8

# Clusters

Si
lh

ou
et

te
In

de
x

(b) 100 Features

2 3 4 5 6 7 8

0

0.2

0.4

0.6

0.8

# Clusters

Si
lh

ou
et

te
In

de
x

(c) 150 Features

Figure1: Clustering Validation with Silhouette Index

2 3 4 5 6 7 8

0

0.5

1

1.5

·104

# Clusters

C
al

in
sk

i-
H

ar
ab

as
z

In
de

x

(a) 50 Features

2 3 4 5 6 7 8

0

0.5

1

1.5

·104

# Clusters

C
al

in
sk

i-
H

ar
ab

as
z

In
de

x

(b) 100 Features

2 3 4 5 6 7 8

0

0.5

1

1.5

·104

# Clusters

C
al

in
sk

i-
H

ar
ab

as
z

In
de

x

(c) 150 Features
FFTM(Entropy) FFTM(IDF ) FFTM(ProbIDF ) FFTM(Normal) LDA

Figure2: Clustering Validation with Calinski-Harabasz Index

CH =
trace(SB)
trace(SW )

.
np − 1
np − k (11)

Where (SB) is the between-cluster scatter ma-
trix, (SW ) the internal scatter matrix, np is the
number of clustered samples, and k is the number
of clusters. Higher value indicates a better clus-
tering. We track the performance of both FFTM
and LDA using different number of clusters rang-
ing from 2 to 8 with different number of features
including 50, 100, and 150. Both Silhouette in-
dex and Calinski-Harabasz index show that FFTM
is the best method with all ranges of features and
clusters (Figures 1 and 2). The gap between FFTM
and LDA does not change a lot by using different
number of features and clusters. LDA has the low-
est performance and Normal has the best perfor-
mance among GTW methods in different ranges of
features and clusters. According to the paired dif-
ference test, the improvement of FFTM over LDA
is statistically significant with a p− value < 0.05
using the two internal clustering validation meth-
ods.

4.3 Document Classification

The second evaluation measure is document clas-
sification by using the second datasest. We evalu-
ate different number of classes and features (top-
ics) with accuracy, F-measure, and ROC using
Random Forest. Accuracy is the portion of true re-
sults in a dataset. F-measure is another measure of
classification evaluation that considers both preci-
sion and recall. ROC curves plot False Positive on
the X axis vs. True Positive on the Y axis to find
the trade off between them; therefore, the closer to
the upper left indicates better performance. We
assume more documents and classes have more
topics;therefore, we choose 100 features for two
classes, 150 features for five classes, and 200 fea-
tures for ten classes. In addition, we use 10 cross
validation as test option.

This experiment shows that FFTM has the best
performance in different number of features and
labels (Table 2). LDA has the lowest performance
and the average performance of ProbIDF has the
best among GTW methods in all ranges of features
and clusters. According to the paired difference
test, the improvement of FFTM over LDA is sta-
tistically significant with a p− value < 0.05.

131



Table2: The Second Dataset Classification Performance

Method #Features # Labels Acc % F-Measure ROC
FFTM(Entropy) 100 2 96.49 0.959 0.982
FFTM(IDF) 100 2 98.24 0.982 0.996
FFTM(ProIDF) 100 2 97.66 0.977 0.987
FFTM(Normal) 100 2 92.39 0.912 0.971
LDA 100 2 90.06 0.9 0.969
FFTM(Entropy) 150 5 71.84 0.694 0.874
FFTM(IDF) 150 5 70.79 0.686 0.859
FFTM(ProIDF) 150 5 70.39 0.674 0.859
FFTM(Normal) 150 5 68.11 0.649 0.851
LDA 150 5 66.27 0.637 0.815
FFTM(Entropy) 200 10 51.06 0.501 0.828
FFTM(IDF) 200 10 51.73 0.506 0.826
FFTM(ProIDF) 200 10 53.72 0.525 0.836
FFTM(Normal) 200 10 50.05 0.485 0.815
LDA 200 10 47.68 0.459 0.792

5 Conclusion

The explosive growth of medical text data makes
text analysis as a key requirement to find patterns
in datasets;however, the typical high dimensional-
ity of such features motivates researchers to utilize
dimension reduction techniques such as LDA. Al-
though LDA has been considered more recently in
medical text analysis (Jimeno-Yepes et al., 2011),
fuzzy clustering methods such as FCM has not
been used in medical text clustering, but rather in
image processing. In the current study, we pro-
pose a method called FFTM to combine LTW and
GTM with Fuzzy clustering, and compare its per-
formance with that of LDA. We use different sets
of data including different number of features, dif-
ferent number of clusters, and different number of
classes.The findings of this study show that com-
bining FCM with LTW and GTW methods can
significantly improve medical documents analysis.
We conclude that different factors including num-
ber of features, number of clusters, and classes
can affect the outputs of machine learning algo-
rithms. In addition, the performance of FFTM is
improved by using GTW methods. This method
proposed in this paper may be applied to other
medical documents to improve text analysis out-
puts. One limitation of this paper is that we use
one clustering method, one classification method,
and two internal clustering validation methods for
evaluation. Our future direction is to explore more
machine learning algorithms and clustering vali-
dation methods for evaluation and also other fuzzy
clustering algorithms for feature transformation.
The main goal of future research is to present an
efficient and effective medical topic model using

fuzzy set theory.

References
CharuC Aggarwal and ChengXiang Zhai. 2012. An

introduction to text mining. In Mining Text Data,
pages 1–10. Springer.

KMBataineh, MNaji, and MSaqer. 2011. A compar-
ison study between various fuzzy clustering algo-
rithms. Jordan Journal of Mechanical & Industrial
Engineering, 5(4).

Zulaikha Beevi and Mohamed Sathik. 2012. A ro-
bust segmentation approach for noisy medical im-
ages using fuzzy clustering with spatial probability.
International Arab Journal of Information Technol-
ogy (IAJIT), 9(1).

David Ben-Arieh and DeepKumar Gullipalli. 2012.
Data envelopment analysis of clinics with sparse
data: Fuzzy clustering approach. Computers & In-
dustrial Engineering, 63(1):13–21.

JamesC Bezdek. 1981. Pattern recognition with fuzzy
objective function algorithms. Kluwer Academic
Publishers.

DavidM Blei, AndrewY Ng, and MichaelI Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.

Wenchao Cui, YiWang, Yangyu Fan, Yan Feng, and
Tao Lei. 2013. Global and local fuzzy clustering
with spatial information for medical image segmen-
tation. In Signal and Information Processing (Chi-
naSIP), 2013 IEEE China Summit & International
Conference on, pages 533–537. IEEE.

Susan Dumais. 1992. Enhancing performance in latent
semantic indexing (lsi) retrieval.

Giuseppe Fenza, Domenico Furno, and Vincenzo Loia.
2012. Hybrid approach for context-aware service
discovery in healthcare domain. Journal of Com-
puter and System Sciences, 78(4):1232–1247.

132



Marzyeh Ghassemi, Tristan Naumann, Rohit Joshi, and
Anna Rumshisky. 2012. Topic models for mortality
modeling in intensive care units. In ICML Machine
Learning for Clinical Data Analysis Workshop.

Antonio Jimeno-Yepes, Bartłomiej Wilkowski,
JamesG Mork, Elizabeth VanLenten, DinaDemner
Fushman, and AlanR Aronson. 2011. A bottom-up
approach to medline indexing recommendations.
In AMIA Annual Symposium Proceedings, volume
2011, page 1583. American Medical Informatics
Association.

TamaraG Kolda. 1998. Limited-memory matrix meth-
ods with applications.

Sangno Lee, Jeff Baker, Jaeki Song, and JamesC
Wetherbe. 2010. An empirical comparison of
four text mining methods. In System Sciences
(HICSS), 2010 43rd Hawaii International Confer-
ence on, pages 1–10. IEEE.

Kishore Papineni. 2001. Why inverse document fre-
quency? In Proceedings of the second meeting of
the North American Chapter of the Association for
Computational Linguistics on Language technolo-
gies, pages 1–8. Association for Computational Lin-
guistics.

Daniel Ramage, SusanT Dumais, and DanielJ Liebling.
2010. Characterizing microblogs with topic models.
In ICWSM.

Eréndira Rendón, Itzel Abundez, Alejandra Arizmendi,
and ElviaM Quiroz. 2011. Internal versus external
cluster validation indexes. International Journal of
computers and communications, 5(1):27–34.

Indrajit Saha and Ujjwal Maulik. 2014. Multiobjective
differential evolution-based fuzzy clustering for mr
brain image segmentation image segmentation. In
Advanced Computational Approaches to Biomedical
Engineering, pages 71–86. Springer.

VivekKumar Singh, Nisha Tiwari, and Shekhar Garg.
2011. Document clustering using k-means, heuris-
tic k-means and fuzzy c-means. In Computational
Intelligence and Communication Networks (CICN),
2011 International Conference on, pages 297–301.
IEEE.

133


