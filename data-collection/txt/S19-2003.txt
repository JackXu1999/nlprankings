



















































SemEval-2019 Task 2: Unsupervised Lexical Frame Induction


Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 16–30
Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics

16

SemEval-2019 Task 2: Unsupervised Lexical Frame Induction

Behrang QasemiZadeh
SFB991, Germany
zadeh@phil.hhu.de

Miriam R. L. Petruck
ICSI, US

miriamp@icsi.berkeley.edu

Regina Stodden
HHUD, Germany

stodden@phil.hhu.de

Laura Kallmeyer
HHUD, SFB991, Germany
kallmeyer@phil.hhu.de

Marie Candito
Paris Diderot University - CNRS, France

marie.candito@linguist.univ-paris-diderot.fr

Abstract

This paper presents Unsupervised Lexical
Frame Induction, Task 2 of the International
Workshop on Semantic Evaluation in 2019.
Given a set of prespecified syntactic forms in
context, the task requires that verbs and their
arguments be clustered to resemble semantic
frame structures. Results are useful in iden-
tifying polysemous words, i.e., those whose
frame structures are not easily distinguished,
as well as discerning semantic relations of the
arguments. Evaluation of unsupervised frame
induction methods fell into two tracks: Task
A) Verb Clustering based on FrameNet 1.7;
and B) Argument Clustering, with B.1) based
on FrameNet’s core frame elements, and B.2)
on VerbNet 3.2 semantic roles. The shared
task attracted nine teams, of whom three re-
ported promising results. This paper describes
the task and its data, reports on methods and
resources that these systems used, and offers a
comparison to human annotation.

1 Introduction

SemEval 2019 Task 2 focused on the unsupervised
semantic labeling of a set of prespecified (seman-
tically) unlabeled structures (Figure 1). Unsuper-
vised learning methods analyze these structures
(Figure 1a) to augment them with semantic labels
(Figure 1b). The shape of the manually labeled in-
put frames is constrained to an acyclic connected
tree of lexical items (words and multi-word units)
of maximum depth 1, where just one root gov-
erns several arguments. The task used Berkeley
FrameNet (FN) (Ruppenhofer et al., 2016) and Q.
Zadeh and Petruck (2019), guidelines for this task,
to determine the arguments and label them with
semantic information.

We compared the proposed system results for
unsupervised semantic tagging with that of human
annotated (or, gold-standard) data in three differ-
ent subtasks (Figure 2). To evaluate the systems,
we computed distributional similarities between

Exxon Mobil sell

skyscraper

company

nsubj dobj

nmod:to

(a) Input: subcategorization frames.

Exxon Mobil
sell

COMMERCE SELL

skyscraper

company

nsubj

Seller

Agent

dobj

Goods

Theme

nmod:to

Buyer

Recipient

(b) Output: Semantic Frame Tagging using labels
learned by Unsupervised methods.

Figure 1: Given semantically unlabeled structures
(1a), annotate the input with semantic information
learned via unsupervised methods (1b).

their generated unsupervised labeled data and hu-
man annotated reference data. For computing sim-
ilarities we used general purpose numeral methods
of text clustering, in particular BCUBED F-SCORE
(Bagga and Baldwin, 1998) as the single figure of
merit to rank the systems.

The most important result of the shared task is
the creation of a benchmark for a future complex
task. This benchmark includes a moderately sized,
manually annotated set of frames, where only the
verbs of each were included, along with their core
frame elements (which uniquely define a frame
as Ruppenhofer et al. describe). To complement
FN’s core frame elements that have highly specific
meanings, the benchmark also includes the anno-
tated argument structures of the verbs based on the
generic semantic roles proposed for verb classes
in VerbNet 3.2 (Kipper et al., 2000; Palmer et al.,
2017). The benchmark comes with simplified an-
notation guidelines and a modular annotation sys-



17

tem with browsing and editing capabilities.1 Com-
plementing the benchmarking are several state-of-
the-art competing baselines, from the participants,
that serve as a point of departure for improvements
in the future.2

The rest of this paper is organized as follows:
Section 2 contextualizes this task; Section 3 offers
a detailed task-description; Section 4 describes
the data; Section 5 introduces the evaluation met-
rics and baselines; Section 6 characterizes the par-
ticipating systems and unsupervised methods that
participants used; Section 7 provides evaluation
scores and additional insight about the data; and
Section 8 presents concluding remarks.

2 Background

Frame Semantics (Fillmore, 1976) and other the-
ories (Gamerschlag et al., 2014) that adopt typed
feature structures for representing knowledge and
linguistic structures have developed in parallel
over several decades in theoretical linguistic stud-
ies about the syntax–semantics interface, as well
as in empirical corpus-driven applications in natu-
ral language processing. Building repositories of
(lexical) semantic frames is a core component in
all of these efforts. In formal studies, lexical se-
mantic frame knowledge bases instantiate foun-
dational theories with tangible examples, e.g., to
provide supporting evidence for the theory. Prac-
tically, frame semantic repositories play a pivotal
role in natural language understanding and seman-
tic parsing, both as inspiration for a representation
format and for training data-driven machine learn-
ing systems, which is required for tasks such as
information extraction, question-answering, text
summarization, among others.

However, manually developing frame semantic
databases and annotating corpus-derived illustra-
tive examples to support analyses of frames are
resource-intensive tasks. The most well-known
frame semantic (lexical) resource is FrameNet
(Ruppenhofer et al., 2016), which only covers a
(relatively) small set of the vocabulary of con-
temporary English. While NLP research has inte-
grated FrameNet data into semantic parsing, e.g.,
Swayamdipta et al. (2018), these methods can-
not extend beyond previously seen training labels,
tagging out-of-domain semantics as unknown at

1http://sfa.phil.hhu.de/.
2See https://competitions.codalab.org/

competitions/19159 for accessing the task’s language
resources, tools, and further technical details.

best. This limitation does not hinder unsupervised
methods, which will port and extend the coverage
of semantic parsers, a common challenge in se-
mantic parsing (Hartmann et al., 2017).

Unsupervised frame induction methods can
serve as an assistive semantic analytic tool, to
build language resources and facilitate linguis-
tic studies. Since the focus is usually to build
language resources, most systems (Pennacchiotti
et al. (2008); Green et al. (2004)) have used a lexi-
cal semantic resource like WordNet (Miller, 1995)
to extend coverage of a resource like FrameNet.
Some methods, e.g., Modi et al. (2012) and
Kallmeyer et al. (2018), tried to extract FrameNet-
like resources automatically without additional se-
mantic information. Others (Ustalov et al. (2018);
Materna (2012)) addressed frame induction only
for verbs with two arguments.

Lastly, unsupervised frame induction methods
can also facilitate linguistic investigations by cap-
turing information about the reciprocal relation-
ships between statistical features and linguistic or
extra-linguistic observations (e.g., Reisinger et al.
(2015)). This task aimed to benchmark a class of
such unsupervised frame induction methods.

3 Task Description

Exxon Mobil
sell

COMMERCE SELL
skyscraper

company

nsubj dobj

nmod:to

(a) Task A - Identifying Semantic Frames: Unsupervised
learned labels evaluated against FN’s lexical units

Exxon Mobil
sell

COMMERCE SELL skyscraper

company

nsubj

Seller
dobj

Goods

nmod:to

Buyer

(b) Task B.1 - Full Frame Semantic Tagging: Unsupervised
labels evaluated against FN’s frames

Exxon Mobil
sell

skyscraper

company

nsubj

Agent

dobj

Theme

nmod:to

Recipient

(c) Task B.2 – Case Role Labeling: Unsupervised labels eval-
uated against generic semantic roles (VerbNet)

Figure 2: Subtasks of SemEval 2019 Task 2.

http://sfa.phil.hhu.de/
https://competitions.codalab.org/competitions/19159
https://competitions.codalab.org/competitions/19159


18

The ambitious goal of this task was the unsuper-
vised induction of frame semantic structures from
tokenized and morphosyntacally labeled text cor-
pora. We sought to achieve this goal by building
an evaluation benchmark for three tasks. Task A
dealt with unsupervised labeling of verb lemmas
with their frame meaning. Task B involved unsu-
pervised argument role labeling, where B.1 bench-
marked unsupervised labeling of frame-specific
frame elements (FEs) based on FN, and B.2
benchmarked unsupervised role labeling of argu-
ments in Case Grammar terms (Fillmore, 1968)
and against a set of generic semantic roles, taken
primarily from VerbNet.

The task was unsupervised in that it forbade the
use of any explicit semantic annotation (only per-
mitting morphosyntactic annotation). Instead, we
encouraged the use of unsupervised representation
learning methods (e.g., word embeddings, brown
clusters) to obtain semantic information. Hence,
systems learn and assign semantic labels to test
records without appealing to any explicit training
labels. For development purposes, developers re-
ceived a small labeled development set.

3.1 Task A: Clustering Verbs

The goal of this task was to identify verbs that
evoke the same frame. The task involved labeling
verb uses in context to resemble their categoriza-
tion based on Frame Semantics (Figure 2a). Here,
we used FN 1.7 as the reference for frame defini-
tions. Hence, the task constituted the unsupervised
induction of FN’s lexical units, where a lexical
unit (LU) is a pairing of a lemma and a frame. For
example, we expected that the LUs auction.v, re-
tail.v, sell.v, etc., which evoke the typed situation
of COMMERCE SELL, be labeled with the same un-
supervised tag.3

The task resembles word sense induction in that
it assigns a class (or sense) label to a verb. In
word sense induction (WSI), labels are determined
and evaluated on word forms (lemma + part-of-
speech e.g., sell.v or auction.n). WSI evaluations
assume that the inventory of senses (set Sis) for
different word forms f is devised independently.
For instance, assuming f1 is labeled with the set
of senses S1 and f2 with S2, then S1 ∩ S2 6= φ
only if f1 = f2; and, if f1 6= f2 then S1 ∩ S2 =
φ (as in other SemEval benchmarks, including
Agirre and Soroa (2007); Manandhar et al. (2010);

3Dark red small caps indicate FN frames.

Jurgens and Klapaftis (2013); Navigli and Van-
nella (2013)). For instance, in WSI evaluations
based on OntoNotes (Hovy et al., 2006), six dif-
ferent labels from Ssell are assigned to the lemma
sell.v, and one label s′ is assigned to auction.v,
knowing that s′ /∈ Ssell. Typically, lexical se-
mantic relationships among members of Sis (e.g.,
synonymy, antonymy) are then analyzed indepen-
dently of WSI (e.g., Lenci and Benotto (2012);
Girju et al. (2007); McCarthy and Navigli (2007)).
In contrast, this task assumes that the sense inven-
tory is defined independent of word forms.

This task involves uncovering mapping between
word forms f and members of S such that differ-
ent word forms (i.e., fi 6= fj) can be mapped to
the same meaning (label), and the same meaning
(label) can be mapped to several word forms. We
defined S with respect to FrameNet and assumed
that its typed-situation frames are units of mean-
ing. So, COMMERCE SELL captures the meaning
associated with both sell.v and auction.v., as well
as other selling-related words. Hence, in some
sense, Task A goes beyond the ordinary WSI task
as it also demands identifying (unspecified) lexical
semantic relationships between verbs.

3.2 Task B.1: Unsupervised Frame Semantic
Argument Labeling

Taking the frames as primary and defining roles
relative to each frame, the aim of Task B.1 was to
cluster prespecified verb-headed argument struc-
tures according to the principles of Frame Se-
mantics, where FrameNet served as the reference
for evaluation. This task amounted to unsuper-
vised labeling of frames and core FEs (Figure 2b).
Because FrameNet defines FEs frame-specifically,
Task B.1 entails Task A.

Given a set of semantically-unlabelled argu-
ments as input (e.g., Figure 1a), the root nodes
(i.e., verbs) are clustered and assigned to a set of
unsupervised frame labels πi (1 ≤ i ≤ n, where
n is the number of latent frames). Then, the argu-
ments are labeled with semantic role labels (FEs)
interpreted locally given the frame. That is, for
any pair of πx and πy, the set of assigned roles Rx
to arguments under πx are assumed to be indepen-
dent from Ry labels for πy (Rx ∩Ry = φ).

3.3 Task B.2: Unsupervised Case Role
Labeling

We defined Subtask B.2 in parallel to Subtask B.1
and involved an idea from Case Grammar. The ar-



19

guments of a verb in a set of prespecified subcat-
egorization frames were clustered according to a
common set of generic semantic roles (Figure 2c).
Here, the task assumed that semantic roles are uni-
versal and generic (e.g., Agent, Patient). Their
configuration determines the argument structure of
verb-headed phrases. We evaluated this unsuper-
vised labeling of arguments with semantic roles
independently of the class, sense, and word form
of a verb. We compared the role labels against a set
of semantic roles from VerbNet 3.2 (Kipper et al.,
2000). Given a verb instance, no guarantee ex-
ists that input argument structures for B.2 and B.1
would be the same.

4 Evaluation Dataset

The dataset consists of manual annotations for
verb-headed frame structures anchored in tok-
enized sentences. These frame structures were
manually annotated using the guidelines for this
task (Q. Zadeh and Petruck, 2019). For example,
as already illustrated, the verb come from.v is an-
notated in terms of FN’s ORIGIN frame and its core
FEs, as Example 1 shows.

(1) Criticism of futures COMES FROM Wall Street.

Criticism come from Wall Street

ORIGIN
ENTITY ORIGIN

Also, using the set of 32 generic semantic role la-
bels in VerbNet 3.2 and two additional roles, COG-
NIZER and CONTENT, we annotated arguments of
the verb as the following graphic shows.

Criticism come from Wall Street

THEME SOURCE

We assumed unique identifiers for sentences,
e.g., #s1 for Example 1. The evaluation record for
come from.v (Task A) appears below, where #s1
4 5 specifies the position of the verb in the sen-
tence (Example 1).

A [ #s1 4 5 come from.ORIGIN]

Similarly, for Task B.1 and Task B.2, respectively,
the evaluation records are as follows here.

B.1 [#s1 4 5 come from.ORIGIN Criticism-:-1-:-
ENTITY Wall Street-:-6 7-:-ORIGIN]

B.2 [#s1 4 5 come from.NA Criticism-:-1-:-
THEME Wall Street-:-6 7-:-SOURCE]

We stripped off the manually asserted labels from
the records and passed them to systems for assign-
ing unsupervised labels. Evidently, later a scorer
program (Section 5) compared system-generated
labels with the manually assigned labels.

4.1 Data Sampling
We sampled data from the Wall Street Journal
(WSJ) corpus of the Penn Treebank. Kallmeyer
et al. (2018) provided frame annotations similar
to those in this task for a portion of WSJ sen-
tences, using SemLink (Bonial et al., 2013) and
EngVallex (Cinková et al., 2014) to generate frame
semantic annotations semi-automatically. That
work was based on FrameNet and the Prague
Dependency Treebank (PSD) (Hajič et al., 2012)
from the Broad-coverage Semantic Dependency
resource (Oepen et al., 2016). We started by anno-
tating a portion of the records in Kallmeyer et al.
(2018), and later deviated from this subset to cre-
ate a more representative sample of the overall di-
versity and distribution of verbs in the WSJ corpus
using a stratified random sampling method.

4.2 Guidelines
The annotation guidelines for this task were
slightly different from those of FrameNet and var-
ious semantic dependency treebanks. In contrast
to FN, which annotates a full span of text as an ar-
gument filler, or PropBank, which annotates syn-
tactic constituents of arguments of verbs (Palmer
et al., 2005), we identified the text spans and
only annotated a single word or a multi-word unit
(MWU), i.e., the semantic head of the span, like
annotations in Oepen et al. (2016) and Abstract
Meaning Representation (Banarescu et al., 2013).
To illustrate, in Example 1, FN would annotate
Criticism of futures as filling the FE ENTITY.
We only annotated Criticism, understanding it as
the LU that evokes JUDGMENT COMMUNICATION,
which in turn represents the meaning of the whole
text span. Thus, we assumed that another frame fa
fills an argument of a frame. We annotated only
the main content word(s) that evoke(s) fa; these
main words are the semantic heads.4

Multi-word unit semantic heads (e.g., named
entities, word form combinations) are annotated as
if a single word form, such as Wall Street (# 1), ex-
cluding modifiers. In contrast to semantic depen-

4The annotation guidelines (Q. Zadeh and Petruck, 2019)
discuss decisions about marking semantic heads and the com-
plex situations resulting from it for argument annotation.



20

dency structures (e.g., DELPH-IN MRS-Derived
Semantic Dependencies, Enju PredicateArgument
Structures, and Tectogramatical Representation in
PSD (Oepen et al., 2016)), we did not commit to
the underlying syntactic structure of the sentence
since we were not obliged to relabel only syntac-
tic structures. Rather, we annotated words and
MWUs if the frame analysis permitted doing so.5

4.3 Annotation Procedure

We annotated the data in a modular manner and
in a semi-controlled environment using an annota-
tion system developed for this purpose. The proce-
dure consisted of four steps: 1) Reading and Com-
prehension; 2) Choosing a Frame; 3) Annotating
Arguments; and 4) Rating, Commenting, or Re-
vising. We tracked and logged all changes in the
data as well as annotator interaction with the anno-
tation system upon starting to annotate. The tool
measured the time that annotators spent on each
record and each annotation step, as well as how
annotators moved between steps.

In Step 1, annotators viewed a sentence with
one highlighted verb, as in Example 2.

(2) Criticism of futures COMES from Wall Street.

The goal of this step was understanding the
meaning of the verb and its semantic function, and
identifying semantic heads of arguments and their
associated words or MWUs. To continue, an anno-
tator must confirm the understanding of the verb’s
meaning of the verb, and can identify its seman-
tic arguments. Without confirmation, an annotator
would terminate the annotation process for that in-
put sentence and go to the next one.

If confirmed, Step 2 required the annotator to
choose the frame that the verb evoked. This
step may have included annotating multi-word
phrasal verbs, e.g., COMES+FROM (Example 2).
The annotation system assisted by providing a
list of likely frames for the verb, including a LU
lookup function (as in FN), an extended set of
LUs derived via statistical methods, and previ-
ously logged annotations. After reviewing the def-
initions of the proposed frames, annotators chose
one, or annotated the verb form with a different
existing FN frame. Otherwise, the annotator ter-
minated the process and the record moved to the
list of “skipped items”.

The annotation of arguments, Step 3, required

5Q. Zadeh and Petruck describe the issues in detail.

that annotators label the core FEs of the cho-
sen frame by first identifying their semantic head,
which first may have required marking MWUs,
e.g., Wall+Street in Example 3, below.

(3) Criticism of futures comes from Wall Street.
The tool lists the core FEs and their definitions,

and checks the integrity of record annotations to
ensure that each core FE is annotated only once.
In parallel, annotators add the verb’s subcatego-
rization frame and its semantic role. We did not
annotate null instantiated FEs (but FN does). Dur-
ing step 3, annotators could go back to the previ-
ous step and change their choice of frame type.

For Step 4, annotators rated their annotation,
stating their opinion on how well the annotated in-
stance fit FrameNet’s definition and how it com-
pared to other annotated instances. In a sense, an-
notators measured their confidence in the assigned
labels. They did so by selecting a number on a
scale from 1 to 5, with 1 not confident at all and 5
the most confident, i.e., the annotation fit perfectly
to the chosen FrameNet frame, its definition, and
examples. Annotators had the option to add free
text comments on each record.

The annotation procedure was rarely straight-
forward. Given the interdependence of Steps 2
and 3, annotators usually moved back and forth
between them. In Step 2 an annotator might be-
lieve that a target verb did not belong in any ex-
isting FN frame. Likewise, annotators could ter-
minate the annotation process even upon reaching
the last step.

4.3.1 Quality Control
At least two annotators verified all annotation used
in the evaluation. A main annotator annotated all
records in the dataset; two other annotators veri-
fied or disputed those annotations. If annotators
could not reach an agreement, we removed the
record from the SemEval dataset.

A full analysis of annotator disagreement goes
beyond the scope of this work. While the source
of annotator disagreement may seem trivial and
simple (e.g., only one annotator understood the
sentence correctly), we believe that some sen-
tences may have more than one interpretation, all
of which are plausible. Like the disagreement re-
sulting from incorrect frame assignment, decid-
ing what frame a verb evokes may be challeng-
ing; and resolving the dilemma is not always sim-
ple. Choosing between two related frames (e.g.,



21

BUILDING vs. INTENTIONALLY CREATE, related via
Inheritance in FN), or identifying metaphorical
and non-metaphorical uses of a verb requires sub-
tle and sophisticated understanding of the seman-
tics of the language, and of Frame Semantics. At
times, disagreements pointed to more complex lin-
guistic issues that remain in debate, e.g., choosing
the semantic head of a syntactically complex argu-
ment, treating quantifiers, conjunctions, etc.

4.4 Summary statistics

Table 1 shows a statistical summary of the annota-
tion task. The SemEval column reports the statis-
tics for the final set of records, i.e., gold records
with double-agreement between annotators, and
which we used to evaluate the systems. Total re-
ports the statistics of all analyzed records, from
which we chose our SemEval data. Skipped and
InProg show the statistics for discarded records
and records without a final decision, respectively.
Dev shows the statistics for the development set.

Each of the rows reports a value of a compo-
nent of the data or annotator interaction with the
data. Records indicates the number of annotated
verbs and their arguments. Sentences and Tokens
indicate the size of the sub-corpus of the anno-
tated records. VF is the number of distinct verb
lemmas (273), mapped to the number of distinct
frames that the Frames-Type row shows (149)
(Figure 3 in Appendix A.1 plots their frequency
distribution.) FElements reports the number of
annotated FEs categorized under the number of
FE types shown in the FE-Type row. Sem-Arg
shows the number of annotated verb arguments
with VerbNet-like semantic roles, classified into
32 of 41 possible semantic role categories. Multi-
word lists the number of annotated MWUs

SemEval Total Skipped InProg Dev
Records 4,620 5,637 301 716 594
Sentences 3,346 3,803 294 675 582
Tokens 90,460 102,067 8,329 19,151 15198
Verb-Forms 273 373 93 210 35
Frame-Type 149 234 75 185 37
#FEs 9,510 11,269 373 1,386 1,128
FE-Type 198 270 64 197 62
Sem-Arg 9,466 11,215 370 1,379 1,079
Multi-word 2,366 2,773 61 346 368
Confidence 3.30 3.2 2.41 2.5 3.34
Time 539h 742h 25h 177h 19h
Total-Move 68,784 83,753 1,903 13,066 4,406

Table 1: Annotation and Data Statistical Summary

Confidence reports the average of annotator-
assigned confidence scores for annotations per

record. Although interpreting this measure de-
mands more work, the averages appear to be as
expected. Specifically, SemEval is higher in value
than both InProg and Skipped, facts that we as-
sociate with double agreement and the choice re-
viewing process. Still, many records with high
confidence scores remained as InProg given the
lack of double agreement. Table 5 (Appendix A.1)
lists the top 10 frames annotated with their respec-
tive highest and lowest confidence ratings aver-
aged by their frequency in SemEval.

The last two rows of Table 1 are meta-data on
the annotation process. Time reports the total time
annotators spent in active annotation, engaged in
the steps described above (742 hours), excluding
the reviewing process (Section 4.3.1) and includ-
ing the time to annotate MWUs. Total-Move is
the total number of logical moves for frame anno-
tation between annotators and the annotation sys-
tem, i.e., logged changes in the process of frame
and core FE annotation. This number excludes
annotation of verb subcategorization with generic
semantic roles.6

In SemEval, annotated frames had an average
of 2.15 arguments, requiring a minimum of five
logical moves to annotate (MWU-less sentences).
However, on average, each SemEval record re-
quired 14.8 moves. This number is even higher
for InProg (18.2); we believe that it indicates the
complexity of the annotation task. Table 4 (Ap-
pendix A.1) further details annotator activity, with
time spent and moves per annotation step. As ex-
pected, frame annotation of verbs (Step 2), was the
most time consuming part of the task.
4.5 Development Dataset
Shared task participants received a development
set consisting of 600 records from a total of 4,620
records, where Table 4 shows the statistics. The
development set contained gold annotations for all
three subtasks.

5 Evaluation Metrics
For all subtasks, as figure of merit, here we re-
port the performance of participating systems with
measures for evaluating text clustering techniques,
including the classic measures of Purity (PU),
inverse-Purity (IPU), and their harmonic mean
(PIF) (Steinbach et al., 2000), as well as the har-
monic mean for BCubed precision and recall (i.e.,

6With the exception of a few verbs, annotators rarely
changed the annotation system’s rule-based suggestions of
VerbNet semantic roles.



22

BCP, BCR, and BCF, respectively) (Bagga and
Baldwin, 1998).

To compute these measures for the pairing of
reference-labeled data and unsupervised-labeled
data (with each having an exact set of annotated
items), we built a contingency table T with rows
for gold labels and columns for unsupervised sys-
tem labels. We filled the table with the number
of intersecting items, as done in cross-tabulation
of results in classification tasks to compute preci-
sion and recall. For Task A (Section 3), T tracks
the unsupervised system labels and the gold refer-
ence labels assigned to verbs. For Task B.1, we
labeled the rows and columns of T with tuples
(lv, la), where lv labels the frame evoking verb and
la labels the FE filler. For Task B.2, the rows and
columns in T track the unsupervised system la-
bels and the gold reference labels (generic seman-
tic roles) assigned to arguments.

These performance measures reflect a notion
of similarity between the distribution of unsuper-
vised labels and that of the gold reference labels,
given certain criteria. Specifically, they define the
notions of consistency and completeness of au-
tomatically generated clusters based on the eval-
uation data. Each method measures consistency
and completeness in its own way, and alone may
lack sufficient information for a clear understand-
ing and analysis of system performance (Amigó
et al., 2009). But, as the single metric for system
ranking, we used the BCF measure, given its satis-
factory behavior in certain situations. Note that we
modeled the task and its evaluation as hard cluster-
ing, where a record receives only one label, with-
out overlap in any generated category of items.

5.1 Baselines

Similar to other clustering tasks, we use base-
lines of random, all-in-one-cluster (AIN1), and
one-cluster-per-instance (1CPI). Additionally, we
adapted the baseline of the most frequent sense
in WSI for these tasks by introducing the
one-cluster-per-head (1CPH) baseline in Task
A, and one-cluster-per-syntactic-category (1CPG)
for verb argument clustering in Task B.2.7 For
Task B.1, we built a baseline, 1CPGH for label-
ing verbs with their lemmas (as in 1CPH) and
FEs with grammatical relation to their heads (as in
1CPG). We included two more labels lcmpx and

7We use syntactic dependencies of the Enhanced Univer-
sal Dependencies formalism (Schuster and Manning, 2016).

rcmpx for frame fillers with no direct syntactic re-
lation to the head verb, if occurring left of or right
of the verb, respectively.

Both 1CPH and 1CPG (and their combination
for Task B.1) are hard to beat because of the long-
tailed distribution of the frequency of our test data.
E.g., most verbs frequently instantiate one par-
ticular frame and rarely other ones. Similarly, a
particular role (FE) frequently is filled by words
that have a particular grammatical relation to its
governing verb; e.g., most subjects of most verb
forms receive the agent label in their subcatego-
rization frame (or, an agent-like element in their
Frame Semantics representations). Evidently the
chosen labels for grammatical relations influences
1CPG and 1CPHG scores. Values reported later
(specifically, Tables 6 and 2) could be improved
by employing heuristics, e.g., relabeling enhanced
dependencies using a few rules.

We also employed one unsupervised and a sec-
ond supervised system baselines. For the unsuper-
vised one, we trained the system with data from
Kallmeyer et al. (2018). For the supervised one,
we used OPEN-SESAME, a state-of-the-art su-
pervised FrameNet tagger (Swayamdipta et al.,
2018). After converting its output to the format of
the present task, we evaluated it similar to other
systems. Both systems were trained out-of-the-
box with no additional tuning.

6 System Descriptions

We received submissions from nine teams (13 par-
ticipants). Only three chose to submit system de-
scription papers. Arefyev et al. (2019) provided a
solution for Task A and Task B.2, using both sets
of these results to address Task B.1. Task A used
language models and Hearst-like patterns to tune
and obtain contextualized vector representations
for the verbs in the test set. A hierarchical agglom-
erative clustering method followed, where hyper-
parameters were set with labeled and unlabeled
records from the development and test sets. Task
B.2 employed a logistic regression trained over the
development set to identify only the most frequent
labels. The classifier was based on features ob-
tained from a language model and hand-crafted
rules. Using logistic regression and training this
algorithm with the development set remains an is-
sue of concern, given the intended unsupervised
scenario. While we objected to using the devel-
opment set to train a supervised system for this



23

subtask, we still report its scores. The differences
between its results and those of the other systems
may be informative. Still, we considered Arefyev
et al.’s results for Task B only complementarily,
not to rank the systems.

Anwar et al. (2019) proposed a method that was
similar to that of Arefyev et al. (2019). Arefyev
et al. used contextualized word embeddings from
the BERT language modeling tool Devlin et al.
(2018), whereas Anwar et al. used pre-trained em-
beddings. They merged the outputs of Tasks A
and B.2 for Task B.1. Task A used agglomerative
clustering of vectors with concatenated verb rep-
resentation vectors and vectors that represent us-
age context. Task B.2 employed hand crafted fea-
tures, a method to encode syntactic information,
and again an agglomerative clustering method.

Ribeiro et al. (2019) also reported results for
all subtasks using similar techniques to those re-
ported in the other two submitted papers. Ribeiro
et al. (2019) used the bidirectional neural language
model BERT, which Arefyev et al. (2019) also
used. Task A employed contextualized word rep-
resentations proposed in (Ustalov et al., 2018), and
Biemann’s clustering algorithm (Biemann, 2006).
Compared to the two other systems, Ribeiro et al.
(2019) exploited input structures, weighted them,
and used them elegantly in its algorithm. With
the same method but different hyper-parameters
for B.2 along with combining results from Task
A, Ribeiro et al. (2019) offered a solution to B.1.

7 Results and Data Analysis

Table 2 reports the BCF scores for system submis-
sions along with a baseline for each task.8 As the
table shows, each system performs best only in
one of the tasks. We report Arefyev et al.’s sub-
mission for Tasks B.1 and B.2 only to show the
benefit of using a small amount of training data
and a supervised method together with a cluster-
ing algorithm, provided that such training data is
available. As readers know, finding the optimal
(actual) number of clusters is an open research
area. Participants knew the number of clusters:
whereas Arefyev et al. and Anwar et al. used this
information, Ribeiro et al. opted for a statistical
method tuned with data that we provided.

The baseline systems, the unsupervised method
of Kallmeyer et al. (2018) performed the worst

8The full list of baselines and performance measures ap-
pear in Table 6 of the Appendix.

System BCF
Arefyev et al. 70.70
Anwar et al. 68.10
Ribeiro et al. 65.32
BASELINE 65.35

Task A

BCF
63.12
49.49
42.75
45.79

B.1

BCF
64.09
42.1

45.65

39.03

B.2

Table 2: Summary of Results. The BASELINE for
Task A is 1CPH, and for B.1 and B.2 is 1CPHG.
Best results appear in bold face; discarded results
are crossed out. Table 6 lists all other baselines.

of all systems regarding BCF. This result is not
surprising since that work did not effectively han-
dle MWUs in the test, where only the head of the
MWU was kept. However, the output of Open-
SESAME, and its low BCF was indeed surprising.

We fed Open-SESAME the sentences from the
test set; it identified approximately 5k frames.
However, the overlap with the test set was only
1,216 records (identification problem in Open-
SESAME). These 1,216 records exhibit a mis-
match between 536 of the arguments and their re-
spective target verbs. We ignored the system’s
extra or incorrectly generated arguments, and re-
placed the missing items with those of the 1CPHG
baseline records. We then used the resulting
records for evaluation against the task’s gold data
as did the task’s participants. As Table 3 shows,
the unsupervised method outperforms the super-
vised system for all tasks by a wide margin, i.e.,the
unsupervised label set can carry more information
than does the supervised label set.

BCP BCR BCF
Task A 84.52 44.67 58.45
Task B.1 81.04 31.6 45.47
Task B.2 34.26 36.56 35.37

Table 3: Open-SESAME Performance

We compared results for confidence measure
that annotators assigned to records. First, we split
the evaluation records according to their assigned
confidence value into five subsets Ei, 1 ≤ i ≤ 5,
such that subset E1 contained only records with
confidence value 1, E2 contained only record with
confidence value 2, etc.. Then we evaluated sys-
tem outputs on each subset Ei and logged that
BCF. Later, we performed this evaluation cumula-
tively using subsetsE′is by adding records from all
Ejs to Ei where i < j. Interpreting the obtained
values requires careful attention (e.g., changes in
the prior probabilities of gold clusters and their



24

cardinality must be taken into account), overall,
we observed a similar trend for all systems: as ex-
pected, namely a positive correlation between the
confidence value and BCF. Thus, what human an-
notators usually found hard to annotate, automatic
systems also found hard to cluster. (The reverse re-
lation does not hold). Or, pessimistically, the level
of noise in annotation increases as their associated
confidence decreases. (Table 7 in Appendix A.2
details the results.)

Finally, we wanted to identify the frames that
machines found difficult to cluster. To estimate
difficulty we used the differences in BCF under
the following conditions. We repeated the evalu-
ation process 1 ≤ i ≤ n times (where n is the
number of gold labels for a task) for each sys-
tem. In each iteration i, we removed all data
items of a gold category i. We measured and
noted the resulting BCF in the given iteration; we
deduced the score from the system performance
over the entire gold set. To cancel frequency ef-
fects, we normalized the differences by the num-
ber of gold data instances. We removed all records
annotated as COMMERCE SELL from the evalua-
tion set E to form E′. We computed the BCF
of the systems over E′ (E′ ⊂ E), and measured
d = EBCF − E

′
BCF. We interpreted a positive

difference as an easy to cluster gold category i,
and a negative difference as a hard to cluster gold
category i.

The heat maps in Table 8 and Table 9 show a
summary of the results for Task A and Task B.2,
respectively. All systems performed similarly for
approximately 30% of the gold classes. Compar-
ing differences across systems and the baselines
of 1CPH and 1CPG reveals (possibly) interest-
ing information. Thus, for example, in Task A,
most systems found COMMERCE SELL hard and
COMMERCE BUY easy to cluster. Interestingly, a
set of six verbs evokes each frame: buy, pur-
chase, buy back, buy up, buy out, buy into for
COMMERCE BUY; and sell, retail, auction, place,
deal, resell for COMMERCE SELL. From these two
sets of verbs, three are polysemous: buy in the for-
mer, and place and deal in the latter. Does the mor-
phology of the verbs (e.g., buy-back, resell) make
one easy to cluster? Alternatively, are other fac-
tors at play, such as the number of verb instances?
How these factors might influence the proposed
naive BCF-difference model is an open question.

8 Concluding Remarks

We have presented the SemEval 2019 task on un-
supervised lexical frame induction. We described
the task in detail, provided a summary of methods
that participants developed, and compared the re-
sults. Although much room for improvement of
the task remains, we consider it a step forward.
It employed a well-motivated typology of lexi-
cal frames to distinguish lexical frame induction
tasks. The evaluation data derived from annota-
tions of a well-known resource, namely a portion
of WSJ sentences, perhaps the most annotated cor-
pus of English. These features provide opportuni-
ties for future investigation, in particular in stud-
ies related to reciprocal relations between syntac-
tic and lexical semantic frame structures.

One reason to promote using unsupervised
methods is their inherent flexibility to embrace un-
known data. These methods have a high margin of
tolerance for noise, and perform better than super-
vised method with insufficient training data. For
unsupervised data, obtaining or generating train-
ing data is easier than doing so with supervised
methods because they simply do not require an-
notation. For example, all participant systems
could collect similar unlabeled training data from
only syntactically annotated corpora to generate
more unlabeled records. Ultimately, such methods
can achieve respectable performance, and produce
clusters which are both more informative than the
unlabeled input and supervised categories (under
certain situations). As shown, unsupervised meth-
ods can even outperform a state-of-the-art Frame
Semantics parser by a wide margin (Section 7),
while a very large gap remains for improvements
in future work.

Acknowledgements

This research was funded by DFG - SFB991. We
thank Timm Lichte, Rainer Oswald, Curt Ander-
son, and Kurt Erbach. We also thank the LDC for
its generous support, and the NVIDIA Corpora-
tion for the Titan Xp GPU used in this work.

References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007

task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the 4th In-
ternational Workshop on Semantic Evaluation, Se-
mEval ’07, pages 7–12, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.

http://dl.acm.org/citation.cfm?id=1621474.1621476
http://dl.acm.org/citation.cfm?id=1621474.1621476
http://dl.acm.org/citation.cfm?id=1621474.1621476


25

Enrique Amigó, Julio Gonzalo, Javier Artiles, and
Felisa Verdejo. 2009. A comparison of extrinsic
clustering evaluation metrics based on formal con-
straints. Inf. Retr., 12(4):461–486.

Saba Anwar, Dmitry Ustalov, Nikolay Arefyev, Si-
mone Paolo Ponzetto, Chris Biemann, and Alexan-
der Panchenko. 2019. Hm2 at semeval 2019 task
2: Unsupervised frame induction using contextu-
alized and uncontextualized word embeddings. In
Proceedings of The 13th International Workshop on
Semantic Evaluation.

Nikolay Arefyev, Boris Sheludko, Adis Davletov,
Dmitry Kharchev, Alex Nevidomsky, , and Alexan-
der Panchenko. 2019. Neural granny at semeval
2019 task 2: A combined approach for better model-
ing of semantic relationships in semantic frame in-
duction. In Proceedings of The 13th International
Workshop on Semantic Evaluation.

Amit Bagga and Breck Baldwin. 1998. Entity-
based cross-document coreferencing using the vec-
tor space model. In Proceedings of the 17th Inter-
national Conference on Computational Linguistics -
Volume 1, COLING ’98, pages 79–85, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse, pages 178–186. Association for Compu-
tational Linguistics.

Chris Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In Proceedings
of TextGraphs: the First Workshop on Graph Based
Methods for Natural Language Processing, pages
73–80. Association for Computational Linguistics.

Claire Bonial, Kevin Stowe, and Martha Palmer. 2013.
Renewing and revising semlink. In Proceedings of
the 2nd Workshop on Linked Data in Linguistics
(LDL-2013): Representing and linking lexicons, ter-
minologies and other language data, pages 9 – 17,
Pisa, Italy. Association for Computational Linguis-
tics.

Silvie Cinková, Eva Fučı́ková, Jana Šindlerová, and
Jan Hajič. 2014. EngVallex - English valency lex-
icon. LINDAT/CLARIN digital library at the In-
stitute of Formal and Applied Linguistics, Charles
University.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

C. J. Fillmore. 1976. Frame Semantics and the Na-
ture of Language. Annals of the New York Academy
of Sciences, 280(Origins and Evolution of Language
and Speech):20–32.

Charles J. Fillmore. 1968. The case for case. In Uni-
versals in Linguistic Theory, pages 1–88. Holt Rine-
hart and Winston, New York.

Thomas Gamerschlag, Doris Gerland, Rainer Osswald,
and Wiebke Petersen, editors. 2014. General Intro-
duction. Springer International Publishing, Cham.

Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic
relations between nominals. In Proceedings of the
Fourth International Workshop on Semantic Evalu-
ation (SemEval-2007), pages 13–18. Association for
Computational Linguistics.

Rebecca Green, Bonnie J. Dorr, and Philip Resnik.
2004. Inducing frame semantic verb classes from
WordNet and LDOCE. In Proceedings of the 42Nd
Annual Meeting on Association for Computational
Linguistics, ACL ’04, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.

Jan Hajič, Eva Hajičová, Jarmila Panevová, Petr
Sgall, Ondřej Bojar, Silvie Cinková, Eva Fučı́ková,
Marie Mikulová, Petr Pajas, Jan Popelka, Jiřı́ Se-
mecký, Jana Šindlerová, Jan Štěpánek, Josef Toman,
Zdeňka Urešová, and Zdeněk Žabokrtský. 2012.
Announcing prague czech-english dependency tree-
bank 2.0. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation
(LREC-2012). European Language Resources Asso-
ciation (ELRA).

Silvana Hartmann, Ilia Kuznetsov, Teresa Martin, and
Iryna Gurevych. 2017. Out-of-domain framenet se-
mantic role labeling. In Proceedings of the 15th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics: Volume 1, Long
Papers, volume 1, pages 471–482.

Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% solution. In Proceedings of the Human
Language Technology Conference of the NAACL,
Companion Volume: Short Papers, NAACL-Short
’06, pages 57–60, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.

David Jurgens and Ioannis Klapaftis. 2013. Semeval-
2013 task 13: Word sense induction for graded and
non-graded senses. In Second Joint Conference on
Lexical and Computational Semantics (* SEM), Vol-
ume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
volume 2, pages 290–299.

Laura Kallmeyer, Behrang QasemiZadeh, and Jackie
Chi Kit Cheung. 2018. Coarse lexical frame acquisi-
tion at the syntax–semantics interface using a latent-
variable pcfg model. In Proceedings of the Seventh

https://doi.org/10.1007/s10791-008-9066-8
https://doi.org/10.1007/s10791-008-9066-8
https://doi.org/10.1007/s10791-008-9066-8
https://doi.org/10.3115/980451.980859
https://doi.org/10.3115/980451.980859
https://doi.org/10.3115/980451.980859
http://www.aclweb.org/anthology/W13-2322
http://www.aclweb.org/anthology/W13-2322
http://aclweb.org/anthology/W06-3812
http://aclweb.org/anthology/W06-3812
http://aclweb.org/anthology/W06-3812
http://www.aclweb.org/anthology/W13-5503
http://hdl.handle.net/11858/00-097C-0000-0023-4337-2
http://hdl.handle.net/11858/00-097C-0000-0023-4337-2
https://doi.org/10.1111/j.1749-6632.1976.tb25467.x
https://doi.org/10.1111/j.1749-6632.1976.tb25467.x
https://doi.org/10.1007/978-3-319-01541-5_1
https://doi.org/10.1007/978-3-319-01541-5_1
http://aclweb.org/anthology/S07-1003
http://aclweb.org/anthology/S07-1003
https://doi.org/10.3115/1218955.1219003
https://doi.org/10.3115/1218955.1219003
http://www.lrec-conf.org/proceedings/lrec2012/pdf/510_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2012/pdf/510_Paper.pdf
http://dl.acm.org/citation.cfm?id=1614049.1614064
http://dl.acm.org/citation.cfm?id=1614049.1614064
http://www.aclweb.org/anthology/S18-2016
http://www.aclweb.org/anthology/S18-2016
http://www.aclweb.org/anthology/S18-2016


26

Joint Conference on Lexical and Computational Se-
mantics, pages 130–141, New Orleans, Louisiana.
Association for Computational Linguistics.

Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon.
In Proceedings of the Seventeenth National Confer-
ence on Artificial Intelligence and Twelfth Confer-
ence on Innovative Applications of Artificial Intelli-
gence, pages 691–696. AAAI Press.

Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
SemEval 2012, pages 75–79. Association for Com-
putational Linguistics.

Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,
and Sameer Pradhan. 2010. Semeval-2010 task 14:
Word sense induction & disambiguation. In Pro-
ceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 63–68, Uppsala, Sweden.
Association for Computational Linguistics.

Jiřı́ Materna. 2012. Lda-frames: An unsupervised ap-
proach togenerating semantic frames. In Compu-
tational Linguistics and Intelligent Text Processing,
pages 376–387, Berlin, Heidelberg. Springer Berlin
Heidelberg.

Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of the Fourth International Workshop
on Semantic Evaluation (SemEval-2007), pages 48–
53. Association for Computational Linguistics.

George A. Miller. 1995. WordNet: A lexical database
for English. Commun. ACM, 38(11):39–41.

Ashutosh Modi, Ivan Titov, and Alexandre Klementiev.
2012. Unsupervised induction of frame-semantic
representations. In Proceedings of the NAACL-HLT
Workshop on the Induction of Linguistic Structure,
pages 1–7, Montréal, Canada. Association for Com-
putational Linguistics.

Roberto Navigli and Daniele Vannella. 2013. Semeval-
2013 task 11: Word sense induction and disam-
biguation within an end-user application. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 193–201, Atlanta, Geor-
gia, USA. Association for Computational Linguis-
tics.

Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Silvie Cinkova, Dan Flickinger,
Jan Hajic, Angelina Ivanova, and Zdenka Uresova.
2016. Towards comparability of linguistic graph
banks for semantic parsing. In LREC 2016, Paris,
France. ELRA.

Martha Palmer, Claire Bonial, and Jena Hwang. 2017.
Verbnet: Verbnet: Capturing english verb behavior,
meaning, and usage. In The Oxford Handbook of
Cognitive Science. Oxford Press.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus
of semantic roles. Comput. Linguist., 31(1):71–106.

Marco Pennacchiotti, Diego De Cao, Roberto Basili,
Danilo Croce, and Michael Roth. 2008. Automatic
induction of framenet lexical units. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ’08, pages 457–
465, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Behrang Q. Zadeh and Miriam R. L. Petruck. 2019.
Guidelines for the semantic frame annotation sys-
tem. corpus annotation guidelines TR.9.2018,
SFB991 - ICSI.

Drew Reisinger, Rachel Rudinger, Francis Ferraro,
Craig Harman, Kyle Rawlins, and Benjamin Van
Durme. 2015. Semantic proto-roles. Transactions
of the Association for Computational Linguistics,
3:475–488.

Eugénio Ribeiro, Vânia Mendonça, Ricardo Ribeiro,
David Martins de Matos, Alberto Sardinha,
Ana Lúcia Santos, and Luı́sa Coheur. 2019.
L2F/INESC-ID at SemEval-2019 Task 2: Un-
supervised Lexical Semantic Frame Induction
using Contextualized Word Representations. In
Proceedings of The 13th International Workshop on
Semantic Evaluation.

Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, Collin F. Baker,
and Jan Scheffczyk. 2016. FrameNet II: Extended
Theory and Practice. ICSI, Berkeley.

Sebastian Schuster and Christopher D. Manning. 2016.
Enhanced English Universal Dependencies: An im-
proved representation for natural language under-
standing tasks. In Proceedings of the Tenth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2016), Paris, France. European Lan-
guage Resources Association (ELRA).

M. Steinbach, G. Karypis, and V. Kumar. 2000. A com-
parison of document clustering techniques. In KDD
Workshop on Text Mining.

Swabha Swayamdipta, Sam Thomson, Kenton Lee,
Luke Zettlemoyer, Chris Dyer, and Noah A. Smith.
2018. Syntactic scaffolds for semantic structures.
In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing,
pages 3772–3782, Brussels, Belgium. Association
for Computational Linguistics.

Dmitry Ustalov, Alexander Panchenko, Andrey Kutu-
zov, Chris Biemann, and Simone Paolo Ponzetto.
2018. Unsupervised semantic frame induction us-
ing triclustering. In ACL, pages 55–62, Melbourne,
Australia. ACL.

http://dl.acm.org/citation.cfm?id=647288.721573
http://aclweb.org/anthology/S12-1012
http://aclweb.org/anthology/S12-1012
http://www.aclweb.org/anthology/S10-1011
http://www.aclweb.org/anthology/S10-1011
https://doi.org/10.1007/978-3-642-28604-9_31
https://doi.org/10.1007/978-3-642-28604-9_31
http://aclweb.org/anthology/S07-1009
http://aclweb.org/anthology/S07-1009
https://doi.org/10.1145/219717.219748
https://doi.org/10.1145/219717.219748
http://www.aclweb.org/anthology/W12-1901
http://www.aclweb.org/anthology/W12-1901
http://www.aclweb.org/anthology/S13-2035
http://www.aclweb.org/anthology/S13-2035
http://www.aclweb.org/anthology/S13-2035
https://doi.org/10.1093/oxfordhb/9780199842193.013.15
https://doi.org/10.1093/oxfordhb/9780199842193.013.15
https://doi.org/10.1162/0891201053630264
https://doi.org/10.1162/0891201053630264
http://dl.acm.org/citation.cfm?id=1613715.1613773
http://dl.acm.org/citation.cfm?id=1613715.1613773
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/674
http://citeseer.ist.psu.edu/steinbach00comparison.html
http://citeseer.ist.psu.edu/steinbach00comparison.html
http://www.aclweb.org/anthology/D18-1412
http://www.aclweb.org/anthology/P18-2010
http://www.aclweb.org/anthology/P18-2010


27

A Appendices

A.1 Appendix I: Annotation Process
A.1.1 Time and Moves per Annotation Step
Table 4 shows the amount of effort to develop
the SemEval dataset in terms of time and moves
that the annotation system recorded. (See Sec-
tions 4.3, 4.4).

Annotator Activity Time Moves
Reading and Comprehension 78 4,795
Choosing a Frame 177 9,737
Annotating Arguments 81 19,510
Rating, Revising, Commenting 115 25,793
Multi-word Unit Annotation 89 8,949
Total 539 68,784

Table 4: Total hours and number of moves for each
annotation step for the 4,620 record dataset.

A.1.2 Plot of frequency of annotated frames
Figure 3 plots the frequency distribution of the an-
notated frames in the gold data (SemEval).

641 Commerce buy

32
1
Ch

an
ge

po
sit

ion
on

a
sc

ale

28
0
Ac

tiv
ity

sta
rt

64
1

C
om

m
er

ce
bu

y

91
O

pi
ni

on

65
P

ro
ce

ss
st

ar
t

43
P

ro
hi

bi
tin

g
or

lic
en

si
ng

32
M

an
uf

ac
tu

rin
g

25
Po

ss
es

si
on

19
In

te
nt

io
na

lly
cr

ea
te

14
Fi

lli
ng

13
C

om
in

g
to

be
lie

ve
9

P
ur

po
se

6
A

w
ar

en
es

s
5

B
ei

ng
in

co
nt

ro
l

4
Im

po
si

ng
ob

lig
at

io
n

3
A

im
in

g

3
C

om
m

un
ic

at
e

ca
te

go
riz

at
io

n

2
S

el
f

m
ot

io
n

2
A

ffi
rm

or
de

ny

Figure 3: Frequency Distribution of Annotated
Frames

A.1.3 Some Frames and their Averaged
Confidence

Table 5 lists FN frames annotated with the high-
est and lowest confidence. Table 4 details hours
spent to derive the evaluation data set. Section 4.3
discusses both tables. The full list of annotations

in human readable form is available to browse
and comment on at http://corpora.phil.
hhu.de/fi/frames.html.

A.2 Appendix II: Statistical Summary of
Evaluation and System Submissions

A.2.1 Unabridged Results Table
Table 6 extends Table 2. Section 5 defines the ab-
breviations. A horizontal line separates participat-
ing systems and the baselines.

A.2.2 Confidence Measures and BCF
Performance

Table 7 shows system BCF scores for confidence.
The table shows changes in the BCF of systems
when altering the evaluation set based on the as-
signed confidence for an annotated record. (See
Section 7 for an explanation).

Frame Type #VF #Rec Conf
DECIDING 1 13 4.31
AGREE OR REFUSE TO ACT 1 15 4.13
TAKE PLACE OF 1 11 4
BEING EMPLOYED 1 6 4
STATEMENT 8 149 3.97
TAKING SIDES 3 16 3.88
ACTIVITY STOP 4 16 3.88
COMMERCE SELL 6 168 3.82
BRINGING 1 5 3.8
GIVE IMPRESSION 4 39 3.79

(a) Frames with Highest Average Confidence

Frame Type #VF #Rec Conf
BEING IN CONTROL 2 5 1.6
COMING TO BE 2 5 1.8
OPERATING A SYSTEM 2 10 1.8
AWARENESS 1 6 1.83
REMOVING 3 8 1.88
INTENTIONALLY CREATE 6 19 1.95
CERTAINTY 1 68 2.03
OPINION 2 91 2.1
THWARTING 2 22 2.32
FIRST RANK 1 21 2.38

(b) Frames with Lowest Average Confidence

Table 5: Frame types with the highest (5a) and
the lowest (5b) confidence (Conf) by number
of records (#Rec) with double annotator agree-
ment. #VF reports the number of distinct verb
forms that evoke a frame.

http://corpora.phil.hhu.de/fi/frames.html
http://corpora.phil.hhu.de/fi/frames.html


28

System #C PU IPU PIF BCP BCR BCF
Arefyev et al. 272 78.68 77.62 78.15 70.86 70.54 70.7
Anwar et al. 150 72.4 81.49 76.68 62.17 75.27 68.1
Ribeiro et al. 222 72.84 77.84 75.25 61.25 69.96 65.32
Kallmeyer et al. 218 73.77 72.86 73.31 64.62 65.48 65.05
1CPI 4620 100 3.23 6.25 100 3.23 6.25
AIN1 1 13.87 100 24.37 3.78 100 7.28
1CPH 273 82.16 66.95 73.78 75.98 57.33 65.35
RANDOM 149 15.11 5.78 8.36 6.76 3.85 4.9

Task A

System #C PU IPU PIF BCP BCR BCF
Arefyev et al. 776 72.47 72.16 72.31 62.73 63.51 63.12
Anwar et al. 338 55.74 67.79 61.18 43.22 57.9 49.49
Ribeiro et al. 518 52.29 57.56 54.8 39.43 46.69 42.75
Kallmeyer et al. 1023 72.24 49.12 58.48 62.71 37.51 46.94
1CPI 9510 100 4.58 8.77 100 4.58 8.77
AIN1 1 6.55 100 12.3 1.56 100 3.08
1CPHG 1203 78.46 45.99 57.99 71.11 33.77 45.79
RANDOM 436 11.34 6.04 7.88 6.03 4.81 5.35

Task B.1

System #C PU IPU PIF BCP BCR BCF
Arefyev et al. 14 73.94 81.4 77.49 56.25 74.46 64.09
Anwar et al. 2 50.43 80.47 62.00 29.58 73.00 42.1
Ribeiro et al. 7 58.25 71.4 64.16 36.88 59.91 45.65
Kallmeyer et al. 37 61.44 51.53 56.05 40.89 37.33 39.03
1CPG 37 61.44 51.53 56.05 40.89 37.33 39.03
1CPI 9466 100 0.34 0.67 100 0.34 0.67
AIN1 1 34.34 100 51.13 21.66 100 35.6
RANDOM 32 34.65 4.75 8.36 21.89 3.45 5.96

Task B.2

Table 6: Complete System Results and Baselines



29

Cnf #I Arefyev Anwar Ribeiro
1 4620 70.7 68.10 65.32
2 4334 71.87 69.28 66.57
3 3657 74.64 72.22 70.17
4 2542 76.46 73.82 73.43
5 84 86.14 84.65 85.13

Task A

Cnf #I Arefyev Anwar Ribeiro
1 9,510 63.12 49.52 42.75
2 9017 64.20 50.44 43.61
3 7,606 67.18 53.40 46.42
4 5,356 68.70 55.99 49.20
5 169 85.16 81.85 65.60

Task B.1

Cnf #I Arefyev Anwar Ribeiro
1 9,466 64.09 42.12 45.65
2 8,911 64.98 42.32 46.27
3 7,528 66.47 42.67 47.52
4 5,292 65.71 40.67 46.95
5 167 77.19 55.18 56.58

Task B.2

Cumulative

Cnf #I Arefyev Anwar Ribeiro
1 286 73.79 70.57 67.70
2 677 66.45 63.80 60.46
3 1,115 76.71 75.98 70.01
4 2,458 76.65 74.05 73.45
5 84 86.14 84.65 85.13

Task A
Cnf #I Arefyev Anwar Ribeiro
1 493 68.57 55.37 51.84
2 1,411 59.86 49.08 42.16
3 2,250 70.67 57.97 47.60
4 5,187 68.70 56.01 49.24
5 169 85.16 81.85 65.60

Task B.1
Cnf #I Arefyev Anwar Ribeiro
1 553 52.69 39.82 38.21
2 1,385 58.36 40.99 41.55
3 2,236 69.01 48.07 49.4
4 5,125 65.44 40.37 46.72
5 167 77.19 55.18 56.58

Task B.2

Stratified

Table 7: Changes in BCF score of systems relative to changes in evaluation records based on assigned
confidence measure.



30

A.3 Examining Clusters by Removing One
Gold Cluster at a Time

#R
m

vd

Frame

A
re

fy
ev

A
nw

ar
R

ib
ei

ro
K

al
lm

ey
er

1C
P

H

168 Commerce sell
6 Awareness
89 Assessing
54 Seeking to achieve
30 Activity ongoing
19 Intention. creat.
2 Storing
3 Cause ... progress
27 Process continue
14 Filling
121 Causation
59 Hiring
89 Choosing
6 Being employed
7 Criminal investig.
65 Process start
4 Notifi... charges
28 Assistance
21 First rank
20 Giving
2 Sentencing
16 Activity stop
641 Commerce buy
42 Have associated
13 Request
149 Statement
67 Avoiding
2 Performer... roles
2 Attack
92 Cause change...
9 Purpose
14 Collaboration
13 Coming to believe

Table 8: Task A – Part of a heat map from results
(Section 7), with cases that exhibit a range of dif-
ference values. Red denotes a positive and blue a
negative difference; white means no change (zero
difference). Differences (normalized by cluster
size) are in domain 0.01 to −0.01.

#R
m

vd

Frame

A
re

fy
ev

A
nw

ar
R

ib
ei

ro
K

al
lm

ey
er

1C
P

H

2 Attending
38 Getting
16 Activity stop
63 Becoming a member
641 Commerce buy
6 Retaining
7 Criminal investigation
67 Avoiding
46 Scrutiny
280 Activity start
5 Bringing
2 Change event time
89 Choosing
8 Removing
13 Coming to believe
14 Inspecting
3 Cause to end
3 Communicate categorization
13 Deciding
2 Attack
2 Creating
19 Intentionally act
34 Cause to amalgamate
19 Intentionally create
5 Usefulness
16 Taking sides
4 Notification of charges
3 Aiming
121 Causation
3 Process end
2 Transfer
5 Coming to be
10 Cotheme
7 Hearsay
16 Transition to state
92 Cause change of position on a scale
7 Inclusion
7 Simultaneity
4 Imposing obligation
3 Cause change
3 Distributed position
39 Give impression
4 Supporting

Table 9: Heat map that visualizes Task B.2 data


