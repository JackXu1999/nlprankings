



















































Beyond Weight Tying: Learning Joint Input-Output Embeddings for Neural Machine Translation


Proceedings of the Third Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 73–83
Belgium, Brussels, October 31 - Novermber 1, 2018. c©2018 Association for Computational Linguistics

https://doi.org/10.18653/v1/W18-64008

Beyond Weight Tying: Learning Joint Input-Output Embeddings
for Neural Machine Translation

Nikolaos Pappas† Lesly Miculicich Werlen†♦ James Henderson†
†Idiap Research Institute, Martigny, Switzerland

♦École Polytechnique Fédérale de Lausanne (EPFL), Switzerland
{npappas,lmiculicich,jhenderson}@idiap.ch

Abstract

Tying the weights of the target word em-
beddings with the target word classifiers of
neural machine translation models leads to
faster training and often to better translation
quality. Given the success of this parameter
sharing, we investigate other forms of shar-
ing in between no sharing and hard equal-
ity of parameters. In particular, we pro-
pose a structure-aware output layer which
captures the semantic structure of the output
space of words within a joint input-output em-
bedding. The model is a generalized form
of weight tying which shares parameters but
allows learning a more flexible relationship
with input word embeddings and allows the
effective capacity of the output layer to be
controlled. In addition, the model shares
weights across output classifiers and transla-
tion contexts which allows it to better lever-
age prior knowledge about them. Our eval-
uation on English-to-Finnish and English-to-
German datasets shows the effectiveness of the
method against strong encoder-decoder base-
lines trained with or without weight tying.

1 Introduction

Neural machine translation (NMT) predicts the
target sentence one word at a time, and thus mod-
els the task as a sequence classification problem
where the classes correspond to words. Typi-
cally, words are treated as categorical variables
which lack description and semantics. This makes
training speed and parametrization dependent on
the size of the target vocabulary (Mikolov et al.,
2013). Previous studies overcome this problem
by truncating the vocabulary to limit its size and
mapping out-of-vocabulary words to a single “un-
known” token. Other approaches attempt to use a
limited number of frequent words plus sub-word
units (Sennrich et al., 2016), the combination of
which can cover the full vocabulary, or to perform

character-level modeling (Chung et al., 2016; Lee
et al., 2017; Costa-jussà and Fonollosa, 2016; Ling
et al., 2015); with the former being the most effec-
tive between the two. The idea behind these al-
ternatives is to overcome the vocabulary size issue
by modeling the morphology of rare words. One
limitation, however, is that semantic information
of words or sub-word units learned by the input
embedding are not considered when learning to
predict output words. Hence, they rely on a large
amount of examples per class to learn proper word
or sub-word unit output classifiers.

One way to consider information learned by in-
put embeddings, albeit restrictively, is with weight
tying i.e. sharing the parameters of the input em-
beddings with those of the output classifiers (Press
and Wolf, 2017; Inan et al., 2016) which is effec-
tive for language modeling and machine transla-
tion (Sennrich et al., 2017; Klein et al., 2017). De-
spite its usefulness, we find that weight tying has
three limitations: (a) It biases all the words with
similar input embeddings to have a similar chance
to be generated, which may not always be the case
(see Table 1 for examples). Ideally, it would be
better to learn distinct relationships useful for en-
coding and decoding without forcing any general
bias. (b) The relationship between outputs is only
implicitly captured by weight tying because there
is no parameter sharing across output classifiers.
(c) It requires that the size of the translation con-
text vector and the input embeddings are the same,
which in practice makes it difficult to control the
output layer capacity.

In this study, we propose a structure-aware out-
put layer which overcomes the limitations of pre-
vious output layers of NMT models. To achieve
this, we treat words and subwords as units with
textual descriptions and semantics. The model
consists of a joint input-output embedding which
learns what to share between input embeddings

73

https://doi.org/10.18653/v1/W18-64008


NMT NMT-tied NMT-joint
Query Input Output Input/Output Input Output
visited attacked visiting visits visiting attended

(Verb past tense) conquered attended attended attended witnessed
contacted visit visiting visits discussed
occupied visits frequented visit recognized
consulted discovered visit frequented demonstrated

generous modest spacious generosity spacious friendly
(Adjective) extensive generosity spacious generosity flexible

substantial generously generously flexible brilliant
ambitious massive lavish generously fantastic
sumptuous huge massive massive massive

friend wife friends colleague colleague colleague
(Noun) husband colleague friends friends fellow

colleague Fri@@ neighbour neighbour supporter
friends fellow girlfriend girlfriend partner
painter friendship companion husband manager

Table 1: Top-5 most similar input and output representations to two query words based on cosine sim-
ilarity for an NMT trained without (NMT) or with weight tying (NMT-tied) and our structure-aware
output layer (NMT-joint) on De-En (|V| ≈ 32K). Our model learns representations useful for en-
coding and generation which are more consistent to the dominant semantic and syntactic relations of the
query such as verbs in past tense, adjectives and nouns (inconsistent words are marked in red).

and output classifiers, but also shares parameters
across output classifiers and translation contexts
to better capture the similarity structure of the out-
put space and leverage prior knowledge about this
similarity. This flexible sharing allows it to distin-
guish between features of words which are useful
for encoding, generating, or both. Figure 1 shows
examples of the proposed model’s input and out-
put representations, compared to those of a soft-
max linear unit with or without weight tying.

This proposal is inspired by joint input-output
models for zero-shot text classification (Yazdani
and Henderson, 2015; Nam et al., 2016a), but in-
novates in three important directions, namely in
learning complex non-linear relationships, con-
trolling the effective capacity of the output layer
and handling structured prediction problems.

Our contributions are summarized as follows:

• We identify key theoretical and practical lim-
itations of existing output layer parametriza-
tions such as softmax linear units with or
without weight tying and relate the latter to
joint input-output models.

• We propose a novel structure-aware output
layer which has flexible parametrization for
neural MT and demonstrate that its mathe-

matical form is a generalization of existing
output layer parametrizations.

• We provide empirical evidence of the superi-
ority of the proposed structure-aware output
layer on morphologically simple and com-
plex languages as targets, including under
challenging conditions, namely varying vo-
cabulary sizes, architecture depth, and output
frequency.

The evaluation is performed on 4 translation pairs,
namely English-German and English-Finnish in
both directions using BPE (Sennrich et al., 2016)
of varying operations to investigate the effect of
the vocabulary size to each model. The main
baseline is a strong LSTM encoder-decoder model
with 2 layers on each side (4 layers) trained with
or without weight tying on the target side, but we
also experiment with deeper models with up to 4
layers on each side (8 layers). To improve effi-
ciency on large vocabulary sizes we make use of
negative sampling as in (Mikolov et al., 2013) and
show that the proposed model is the most robust to
such approximate training among the alternatives.

2 Background: Neural MT

The translation objective is to maximize the con-
ditional probability of emitting a sentence in a

74



target language Y = {y1, ..., yn} given a sen-
tence in a source language X = {x1, ..., xm},
noted pΘ(Y |X), where Θ are the model param-
eters learned from a parallel corpus of length N :

max
Θ

1

N

N∑

i=1

log(pΘ(Y
(i)|X(i))). (1)

By applying the chain rule, the output sequence
can be generated one word at a time by calculating
the following conditional distribution:

p(yt|yt−11 , X) ≈ fΘ(yt−11 , X). (2)

where fΘ returns a column vector with an element
for each yt. Different models have been proposed
to approximate the function fΘ (Kalchbrenner and
Blunsom, 2013; Sutskever et al., 2014; Bahdanau
et al., 2015; Cho et al., 2014; Gehring et al., 2017;
Vaswani et al., 2017). Without loss of generality,
we focus here on LSTM-based encoder-decoder
model with attention Luong et al. (2015).

2.1 Output Layer parametrizations
2.1.1 Softmax Linear Unit
The most common output layer (Figure 3a), con-
sists of a linear unit with a weight matrix W ∈
IRdh×|V| and a bias vector b ∈ IR|V| followed by
a softmax activation function, where V is the vo-
cabulary, noted as NMT. For brevity, we focus our
analysis specifically on the nominator of the nor-
malized exponential which characterizes softmax.
Given the decoder’s hidden representation ht with
dimension size dh, the output probability distribu-
tion at a given time, yt, conditioned on the input
sentence X and the previously predicted outputs
yt−11 can be written as follows:

p(yt|yt−11 , X) ∝ exp(W Tht + b)
∝ exp(W T Iht + b), (3)

where I is the identity function. From the sec-
ond line of the above equation, we observe that
there is no explicit output space structure learned
by the model because there is no parameter shar-
ing across outputs; the parameters for output class
i, W Ti , are independent from parameters for any
other output class j, W Tj .

2.1.2 Softmax Linear Unit with Weight Tying
The parameters of the output embedding W can
be tied with the parameters of the input embed-
ding E ∈ IR|V|×d by setting W = ET , noted as

NMT-tied. This can happen only when the in-
put dimension of W is restricted to be the same as
that of the input embedding (d = dh). This cre-
ates practical limitations because the optimal di-
mensions of the input embedding and translation
context may actually be when dh 6= d.

With tied embeddings, the parametrization of
the conditional output probability distribution
from Eq. 3 can be re-written as:

p(yt|yt−11 , X) ∝ exp((ET )Tht + b)
∝ exp(Eht + b). (4)

As above, this model does not capture any explicit
output space structure. However, previous stud-
ies have shown that the input embedding learns
linear relationships between words similar to dis-
tributional methods (Mikolov et al., 2013). The
hard equality of parameters imposed by W = ET

forces the model to re-use this implicit structure in
the output layer and increases the modeling bur-
den of the decoder itself by requiring it to match
this structure through ht. Assuming that the la-
tent linear structure which E learns is of the form
E ≈ ElW where El ∈ IR|V|×k and W ∈ IRk×d
and d = dh, then Eq. 4 becomes:

p(yt|yt−11 , X) ∝ exp(ElWht + b) �. (5)

The above form, excluding bias b, shows that
weight tying learns a similar linear structure, albeit
implicitly, to joint input-output embedding mod-
els with a bilinear form for zero-shot classifica-
tion (Yazdani and Henderson, 2015; Nam et al.,
2016a).1 This may explain why weight tying is
more sample efficient than the baseline softmax
linear unit, but also motivates the learning of ex-
plicit structure through joint input-output models.

2.2 Challenges

We identify two key challenges of the existing
parametrizations of the output layer: (a) their dif-
ficulty in learning complex structure of the output
space due to their bilinear form and (b) their rigid-
ness in controlling the output layer capacity due
to their strict equality of the dimensionality of the
translation context and the input embedding.

1The capturing of implicit structure could also apply for
the output embedding W in Eq. 3, however that model would
not match the bilinear input-output model form because it is
based on the input embedding E.

75



E W

 dh x |V|

ht

 |V| x d 

ct

yt-1

.

d=dh

yt

W

SoftmaxDecoder

(a) Typical output layer which is a softmax linear unit
without or with weight tying (W = ET ).

E

htct

yt-1 . yt
E'

Softmax

V 
Decoder

E

ht'

U 
Joint

Embedding

 dh x dj

 d x dj
 |V| x d 

(b) The structure-aware output layer is a joint embedding
between translation contexts and word classifiers.

Figure 1: Schematic of existing output layers and the proposed output layer for the decoder of the NMT
model with source context vector ct, previous word yt−1 ∈ IRd, and decoder hidden states, ht ∈ IRdh .

2.2.1 Learning Complex Structure
The existing joint input-output embedding models
(Yazdani and Henderson, 2015; Nam et al., 2016a)
have the following bilinear form:

E W︸︷︷︸
Structure

ht (6)

where W ∈ IRd×dh . We can observe that the
above formula can only capture linear relation-
ships between encoded text (ht) and input embed-
ding (E) through W . We argue that for struc-
tured prediction, the relationships between differ-
ent outputs are more complex due to complex in-
teractions of the semantic and syntactic relations
across outputs but also between outputs and dif-
ferent contexts. A more appropriate form for this
purpose would include a non-linear transformation
σ(·), for instance with either:

(a) σ(EW)︸ ︷︷ ︸
Output structure

ht or (b) E σ(Wht)︸ ︷︷ ︸
Context structure

. (7)

2.2.2 Controlling Effective Capacity
Given the above definitions we now turn our focus
to a more practical challenge, which is the capac-
ity of the output layer. Let Θbase, Θtied, Θbilinear
be the parameters associated with a softmax lin-
ear unit without and with weight tying and with
a joint bilinear input-output embedding, respec-
tively. The capacity of the output layer in terms of
effective number of parameters can be expressed
as:

Cbase ≈ |Θbase| = |V| × dh + |V| (8)
Ctied ≈ |Θtied| ≤ |V| × dh + |V| (9)

Cbilinear ≈ |Θbilinear| = d× dh + |V|. (10)

But since the parameters of Θtied are tied to the
parameters of the input embedding, the effective

number of parameters dedicated to the output layer
is only |Θtied| = |V|.

The capacities above depend on external fac-
tors, that is |V|, d and dh, which affect not only
the output layer parameters but also those of other
parts of the network. In practice, for Θbase the
capacity dh can be controlled with an additional
linear projection on top of ht (e.g. as in the Open-
NMT implementation), but even in this case the
parametrization would still be heavily dependent
on |V|. Thus, the following inequality for the
effective capacity of these models holds true for
fixed |V |, d, dh:

Ctied < Cbilinear < Cbase. (11)

This creates in practice difficulty in choosing
the optimal capacity of the output layer which
scales to large vocabularies and avoids under-
parametrization or overparametrization (left and
right side of Eq. 11 respectively). Ideally, we
would like to be able to choose the effective capac-
ity of the output layer more flexibly moving freely
in between Cbilinear and Cbase in Eq. 11.

3 Structure-aware Output Layer for
Neural Machine Translation

The proposed structure-aware output layer for
neural machine translation, noted as NMT-
joint, aims to learn the structure of the out-
put space by learning a joint embedding between
translation contexts and output classifiers, as well
as, by learning what to share with input embed-
dings (Figure 1b). In this section, we describe the
model in detail, showing how it can be trained effi-
ciently for arbitrarily high number of effective pa-
rameters and how it is related to weight tying.

76



3.1 Joint Input-Output Embedding
Let ginp(ht) and gout(ej) be two non-linear pro-
jections of dj dimensions of any translation con-
text ht and any embedded output ej , where ej is
the jth row vector from the input embedding ma-
trix E, which have the following form:

e′j = gout(ej) = σ(Ue
T
j + bu) (12)

h′t = ginp(ht) = σ(V ht + bv), (13)

where the matrix U ∈ IRdj×d and bias bu ∈ IRdj is
the linear projection of the translation context and
the matrix V ∈ IRdj×dh and bias bv ∈ IRdj is the
linear projection of the outputs, and σ is a non-
linear activation function (here we use Tanh).
Note that the projections could be high-rank or
low-rank for h′t and e

′
j depending on their initial

dimensions and the target joint space dimension.
With E′ ∈ IR|V|×dj being the matrix result-

ing from projecting all the outputs ej to the joint
space, i.e. gout(E), and a vector b ∈ IR|V| which
captures the bias for each output, the conditional
output probability distribution of Eq 3 can be re-
written as follows:

p(yt|yt−11 , X) (14)
∝ exp

(
E′h′t + b

)

∝ exp
(
gout(E)ginp(ht) + b

)

∝ exp
(
σ(UET + bu)σ(V ht + bv) + b

)
.

3.1.1 What Kind of Structure is Captured?
From the above formula we can derive the general
form of the joint space which is similar to Eq. 7
with the difference that it incorporates both com-
ponents for learning output and context structure:

σ(EWo)︸ ︷︷ ︸
Output structure

σ(Wcht)︸ ︷︷ ︸
Context structure

, (15)

where Wo ∈ IRd×dj and Wc ∈ IRdj×dh are the
dedicated projections for learning output and con-
text structure respectively (which correspond to
U and V projections in Eq. 14). We argue that
both nonlinear components are essential and vali-
date this hypothesis empirically in our evaluation
by performing an ablation analysis (Section 4.4).

3.1.2 How to Control the Effective Capacity?
The capacity of the model in terms of effective
number of parameters (Θjoint) is:

Cjoint ≈ |Θjoint| = d× dj + dj × dh + |V|.
(16)

By increasing the joint space dimension dj above,
we can now move freely between Cbilinear and
Cbase in Eq .11 without depending anymore on the
external factors (d, dh, |V |) as follows:

Ctied < Cbilinear ≤ Cjoint ≤ Cbase. (17)

However, for very large number of dj the com-
putational complexity increases prohibitively be-
cause the projection requires a large matrix multi-
plication between U and E which depends on |V|.
In such cases, we resort to sampling-based train-
ing, as explained in the next subsection.

3.2 Sampling-based Training

To scale up to large output sets we adopt the
negative sampling approach from (Mikolov et al.,
2013). The goal is to utilize only a sub-set V ′
of the vocabulary instead of the whole vocabu-
lary V for computing the softmax. The sub-set V ′
includes all positive classes whereas the negative
classes are randomly sampled. During back prop-
agation only the weights corresponding to the sub-
set V ′ are updated. This can be trivially extended
to mini-batch stochastic optimization methods by
including all positive classes from the examples
in the batch and sampling negative examples ran-
domly from the rest of the vocabulary.

Given that the joint space models generalize
well on seen or unseen outputs (Yazdani and Hen-
derson, 2015; Nam et al., 2016b), we hypothesize
that the proposed joint space will be more sample
efficient than the baseline NMT with or without
weight tying, which we empirically validate with a
sampling-based experiment in Section 4.5 (Table
2, last three rows with |V| ≈ 128K).

3.3 Relation to Weight Tying

The proposed joint input-output space can be seen
as a generalization of weight tying (W = ET ,
Eq. 3), because its degenerate form is equivalent
to weight tying. In particular, this can be simply
derived if we set the non-linear projection func-
tions in the second line of Eq. 14 to be the identity
function, ginp(·) = gout(·) = I , as follows:

p(yt|yt−11 , X) ∝ exp
(
(IE) (Iht) + b

)

∝ exp
(
Eht + b

)
�. (18)

Overall, this new parametrization of the output
layer generalizes over previous ones and addresses
their aforementioned challenges in Section 2.2.

77



En→ Fi Fi→ En En→ De De→ En
Model |Θ| BLEU (∆) |Θ| BLEU (∆) |Θ| BLEU (∆) |Θ| BLEU (∆)

32
K

NMT 60.0M 12.68 (–) 59.8M 9.42 (–) 61.3M 18.46 (–) 65.0M 15.85 (–)

NMT-tied 43.3M 12.58 (−0.10) 43.3M 9.59 (+0.17) 44.9M 18.48 (+0.0) 46.7M 16.51 (+0.66)†
NMT-joint 47.5M 13.03 (+0.35)‡ 47.5M 10.19 (+0.77)‡ 47.0M 19.79 (+1.3)‡ 48.8M 18.11 (+2.26)‡

64
K

NMT 108.0M 13.32 (–) 106.7M 12.29 (–) 113.9M 20.70 (–) 114.0M 20.01 (–)

NMT-tied 75.0M 13.59 (+0.27) 75.0M 11.74 (−0.55)‡ 79.4M 20.85 (+0.15) 79.4M 19.19 (−0.82)†
NMT-joint 75.5M 13.84 (+0.52)‡ 75.5M 12.08 (−0.21) 79.9M 21.62 (+0.92)‡ 79.9M 20.61 (+0.60)†

12
8K

(∼
) NMT 201.1M 13.52 (–) 163.1M 11.64 (–) 211.3M 22.48 (–) 178.3M 19.12 (–)

NMT-tied 135.6M 13.90 (+0.38)∗ 103.2M 11.97 (+0.33)∗ 144.2M 21.43 (−0.0) 111.6M 19.43 (+0.30)
NMT-joint 137.7M 13.93 (+0.41)† 103.7M 12.07 (+0.43)† 146.3M 22.73 (+0.25)† 115.8M 20.60 (+1.48)‡

Table 2: Model performance and number of parameters (|Θ|) with varying BPE operations (32K, 64K,
128K) on the English-Finish and English-German language pairs. The significance of the difference
against the NMT baseline with p-values <.05, <.01 and <.001 are marked with ∗, † and ‡ respectively.

4 Evaluation
We compare the NMT-joint model to two
strong NMT baselines trained with and without
weight tying over four large parallel corpora which
include morphologically rich languages as targets
(Finnish and German), but also morphologically
less rich languages as targets (English) from WMT
2017 (Bojar et al., 2017)2. We examine the be-
havior of the proposed model under challenging
conditions, namely varying vocabulary sizes, ar-
chitecture depth, and output frequency.

4.1 Datasets and Metrics
The English-Finnish corpus contains 2.5M sen-
tence pairs for training, 1.3K for develop-
ment (Newstest2015), and 3K for testing (New-
stest2016), and the English-German corpus 5.8M
for training, 3K for development (Newstest2014),
and 3K for testing (Newstest2015). We pre-
process the texts using the BPE algorithm (Sen-
nrich et al., 2016) with 32K, 64K and 128K op-
erations. Following the standard evaluation prac-
tices in the field (Bojar et al., 2017), the trans-
lation quality is measured using BLEU score
(Papineni et al., 2002) (multi-blue) on tokenized
text and the significance is measured with the
paired bootstrap re-sampling method proposed by
(Koehn et al., 2007).3 The quality on infrequent
words is measured with METEOR (Denkowski
and Lavie, 2014) which has originally been pro-
posed to measure performance on function words.

2http://www.statmt.org/wmt17/
3multi-bleu.perl and bootstrap-hypothe-

sis-difference-significance.pl scripts.

To adapt it for our purposes on English-German
pairs (|V| ≈ 32K), we set as function words dif-
ferent sets of words grouped according to three
frequency bins, each of them containing |V|3 words
of high, medium and low frequency respectively
and set its parameters to {0.85, 0.2, 0.6, 0.} and
{0.95, 1.0, 0.55, 0.} when evaluating on English
and German respectively.

4.2 Model Configurations
The baseline is an encoder-decoder with 2 stacked
LSTM layers on each side from OpenNMT (Klein
et al., 2017), but we also experiment with varying
depth in the range {1, 2, 4, 8} for German-English.
The hyperparameters are set according to vali-
dation accuracy as follows: maximum sentence
length of 50, 512-dimensional word embeddings
and LSTM hidden states, dropout with a probabil-
ity of 0.3 after each layer, and Adam (Kingma and
Ba, 2014) optimizer with initial learning rate of
0.001. The size of the joint space is also selected
on validation data in the range {512, 2048, 4096}.
For efficiency, all models on corpora with V ≈
128K (∼) and all structure-aware models with
dj ≥ 2048 on corpora with V ≤ 64K are trained
with 25% negative sampling.4

4.3 Translation Performance
Table 2 displays the results on four translation
sets from English-German and English-Finish lan-
guage pairs when varying the number of BPE op-
erations. The NMT-tied model outperforms the

4Training the models with a full 128K vocabulary without
sampling runs out of memory on our machines.

78



Model Layer form BLEU |Θ|
NMT W Tht 15.85 65.0M

NMT-tied Eht 16.51 46.7M

N
M

T-
j
o
i
n
t

Eq. 6 EWht 16.23 47.0M
Eq. 7 a σ(EW)ht 16.01 47.0M
Eq. 7 b Eσ(Wht) 17.52 47.0M
Eq. 15 (512) σ(EWo)σ(Wcht) 17.54 47.2M
Eq. 15 (2048) σ(EWo)σ(Wcht) 18.11 48.8M

Table 3: BLEU scores on De → En (|V| ≈ 32K)
for the ablation analysis of NMT-joint.

NMT baseline in many cases, but the differences
are not consistent and it even scores significantly
lower than NMT baseline in two cases, namely on
Fi→ En and De→ En with V ≈ 64K. This vali-
dates our claim that the parametrization of the out-
put space of the original NMT is not fully redun-
dant, otherwise the NMT-tied would be able to
match its BLEU in all cases. In contrast, the NMT-
jointmodel outperforms consistently both base-
lines with a difference up to +2.2 and +1.6 BLEU
points respectively,5 showing that the NMT-tied
model has a more effective parametrization and
retains the advantages of both baselines, namely
sharing weights with the input embeddings, and
dedicating enough parameters for generation.

Overall, the highest scores correlate with a high
number of BPE operations, namely 128K, 64K,
128K and 64k respectively. This suggests that the
larger the vocabulary the better the performance,
especially for the morphologically rich target lan-
guages, namely En → Fi and En → De. Lastly,
the NMT baseline seems to be the least robust to
sampling since its BLEU decreases in two cases.
The other two models are more robust to sampling,
however the difference of NMT-tied with the
NMT is less significant than that of NMT-joint.

4.4 Ablation Analysis

To demonstrate whether all the components of
the proposed joint input-output model are useful
and to which extend they contribute to the perfor-
mance, we performed an ablation analysis; the re-
sults are displayed in Table 3. Overall, all the vari-
ants of the NMT-joint outperform the baseline
with varying degrees of significance. The NMT-
joint with a bilinear form (Eq. 6) as in (Yaz-

5Except in the case of Fi→ En with |V| ≈ 64K, where
the NMT baseline performed the best.

Joint space dimension (    )

Fi → En

En→ De De → En

En → Fi

Figure 2: BLEU scores for the NMT-jointmodel
when varying its dimension (dj) with |V| ≈ 32K.

dani and Henderson, 2015; Nam et al., 2016b) is
slightly behind the NMT-tied and outperforms
the NMT baseline; this supports our theoretical
analysis in Section 2.1.2 which demonstrated that
weight tying is learning an implicit linear structure
similar to bilinear joint input-output models.

The NMT-joint model without learning ex-
plicit translation context structure (Eq. 7 a) per-
forms similar to the bilinear model and the NMT-
tied model, while the NMT-joint model with-
out learning explicit output structure (Eq. 7 b)
outperforms all the previous ones. When keep-
ing same capacity (with dj=512), our full model,
which learns both output and translation con-
text structure, performs similarly to the latter
model and outperforms all the other baselines, in-
cluding joint input-output models with a bilinear
form (Yazdani and Henderson, 2015; Nam et al.,
2016b). But when the capacity is allowed to in-
crease (with dj=2048), it outperforms all the other
models. Since both nonlinearities are necessary
to allow us to control the effective capacity of the
joint space, these results show that both types of
structure induction are important for reaching the
top performance with NMT-joint.

4.5 Effect of Embedding Size

Performance Figure 2 displays the BLEU scores
of the proposed model when varying the size of the
joint embedding, namely dj ∈ {512, 2048, 4096},
against the two baselines. For English-Finish
pairs, the increase in embedding size leads to a
consistent increase in BLEU in favor of the NMT-
joint model. For the English-German pairs, the
difference with the baselines is much more evident

79



(a) Results on En→ De (|V| ≈ 32K). (b) Results on De→ En (|V| ≈ 32K).
Figure 3: METEOR scores (%) on both directions of German-English language pair for all the models
when focusing the evaluation on different frequency outputs grouped into three bins (high, medium, low).

Sampling
Model dj 50% 25% 5%
NMT - 4.3K 5.7K 7.1K
NMT-tied - 5.2K 6.0K 7.8K
NMT-joint 512 4.9K 5.9K 7.2K
NMT-joint 2048 2.8K 4.2K 7.0K
NMT-joint 4096 1.7K 2.9K 6.0K

Table 4: Target tokens processed per second during
training with negative sampling on En→ De pair
with a large BPE vocabulary |V| ≈ 128K.

and the optimal size is observed around 2048 for
De→ En and around 512 on En→De. The results
validate our hypothesis that there is parameter re-
dundancy in the typical output layer. However
the ideal parametrization is data dependent and
is achievable systematically only with the joint
output layer which is capacity-wise in between the
typical output layer and the tied output layer.

Training speed Table 4 displays the target to-
kens processed per second by the models on En
→ DE with |V| ≈ 128K using different levels of
negative sampling, namely 50%, 25%, and 5%.
In terms of training speed, the 512-dimensional
NMT-joint model is as fast as the baselines, as
we can observe in all cases. For higher dimensions
of the joint space, namely 2048 and 4096 there is
a notable decrease in speed which is remidiated by
reducing the percentage of the negative samples.

4.6 Effect of Output Frequency and
Architecture Depth

Figure 3 displays the performance in terms of ME-
TEOR on both directions of German-English lan-
guage pair when evaluating on outputs of differ-
ent frequency levels (high, medium, low) for all

the competing models. The results on De → EN
show that the improvements brought by the NMT-
joint model against baselines are present con-
sistently for all frequency levels including the low-
frequency ones. Nevertheless, the improvement is
most prominent for high-frequency outputs, which
is reasonable given that no sentence filtering was
performed and hence frequent words have higher
impact in the absolute value of METEOR. Sim-
ilarly, for En → De we can observe that NMT-
joint outperforms the others on high-frequency
and low-frequency labels while it reaches parity
with them on the medium-frequency ones.

We also evaluated our model in another chal-
lenging condition in which we examine the ef-
fect of the NMT architecture depth in the perfor-
mance of the proposed model. The results are dis-
played in Table 5. The results show that the NMT-
joint outperforms the other two models consis-
tently when varying the architecture depth of the
encoder-decoder architecture. The NMT-joint
overall is much more robust than NMT-tied and
it outperforms it consistently in all settings. Com-
pared to the NMT which is overparametrized the
improvement even though consistent it is smaller
for layer depth 3 and 4. This happens because
NMT has a much higher number of parameters
than NMT-joint with dj=512.

Increasing the number of dimensions dj of the
joint space should lead to further improvements,
as shown in Fig. 2. In fact, our NMT-joint with
dj = 2048 reaches 18.11 score with a 2-layer deep
model, hence it outperforms all other NMT and
NMT-tied models even with a deeper architec-
ture (3-layer and 4-layer) regardless of the fact that
it utilizes fewer parameters than them (48.8M vs
69.2-73.4M and 50.9-55.1M respectively).

80



Model dj 1-layer |Θ| 2-layer |Θ| 3-layer |Θ| 4-layer |Θ|
NMT - 16.49 60.8M 15.85 65.0M 17.71 69.2M 17.74 73.4M

NMT-tied - 15.93 42.5M 16.51 46.7M 17.72 50.9M 17.60 55.1M
NMT-joint 512 16.93 43.0M 17.54 47.2M 17.83 51.4M 18.13 55.6M

Table 5: BLEU scores on De→ En (|V| ≈ 32K) for the NMT-joint with dj = 512 against baselines
when varying the depth of both the encoder and the decoder of the NMT model.

5 Related Work

Several studies focus on learning joint input-
output representations grounded to word seman-
tics for zero-shot image classification (Weston
et al., 2011; Socher et al., 2013; Zhang et al.,
2016), but there are fewer such studies for NLP
tasks. (Yazdani and Henderson, 2015) proposed
a zero-shot spoken language understanding model
based on a bilinear joint space trained with hinge
loss, and (Nam et al., 2016b), proposed a similar
joint space trained with a WARP loss for zero-shot
biomedical semantic indexing. In addition, there
exist studies which aim to learn output represen-
tations directly from data such as (Srikumar and
Manning, 2014; Yeh et al., 2018; Augenstein et al.,
2018); their lack of semantic grounding to the
input embeddings and the vocabulary-dependent
parametrization, however, makes them data hun-
gry and less scalable on large label sets. All these
models, exhibit similar theoretical limitations as
the softmax linear unit with weight tying which
were described in Sections 2.2.

To our knowledge, there is no existing study
which has considered the use of such joint input-
output labels for neural machine translation. Com-
pared to previous joint input-label models our
model is more flexible and not restricted to lin-
ear mappings, which have limited expressivity,
but uses non-linear mappings modeled similar
to energy-based learning networks (Belanger and
McCallum, 2016). Perhaps, the most similar em-
bedding model to ours is the one by (Pappas and
Henderson, 2018), except for the linear scaling
unit which is specific to sigmoidal linear units de-
signed for multi-label classification problems and
not for structured prediction, as here.

6 Conclusion and Perspectives

We proposed a re-parametrization of the output
layer for the decoder of NMT models which is
more general and robust than a softmax linear
unit with or without weight tying with the input

word embeddings. Our evaluation shows that the
structure-aware output layer outperforms weight
tying in all cases and maintains a significant dif-
ference with the typical output layer without com-
promising much the training speed. Furthermore,
it can successfully benefit from training corpora
with large BPE vocabularies using negative sam-
pling. The ablation analysis demonstrated that
both types of structure captured by our model
are essential and complementary, as well as, that
their combination outperforms all previous out-
put layers including those of bilinear input-output
embedding models. Our further investigation re-
vealed the robustness of the model to sampling-
based training, translating infrequent outputs and
to varying architecture depth.

As future work, the structure-aware output
layer could be further improved along the fol-
lowing directions. The computational complex-
ity of the model becomes prohibitive for a large
joint projection because it requires a large matrix
multiplication which depends on |V|; hence, we
have to resort to sampling based training relatively
quickly when gradually increasing dj (e.g. for
dj >= 2048). A more scalable way of increas-
ing the output layer capacity could address this
issue, for instance, by considering multiple con-
secutive additive transformations with small dj .
Another useful direction would be to use more
advanced output encoders and additional exter-
nal knowledge (contextualized or generically de-
fined) for both words and sub-words. Finally,
to encourage progress in joint input-output em-
bedding learning for NMT, our code is available
on Github: http://github.com/idiap/
joint-embedding-nmt.

Acknowledgments

We are grateful for the support from the European
Union through its Horizon 2020 program in the
SUMMA project n. 688139, see http://www.
summa-project.eu and for the valuable feed-
back from the anonymous reviewers.

81



References
Isabelle Augenstein, Sebastian Ruder, and Anders

Sgaard. 2018. Multi-task learning of pairwise
sequence classification tasks over disparate label
spaces. In Proceedings of the 2018 Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
1896–1906, New Orleans, Louisiana. Association
for Computational Linguistics.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the International Conference on Learning Represen-
tations (ICLR), San Diego, USA.

David Belanger and Andrew McCallum. 2016. Struc-
tured prediction energy networks. In Proceedings
of The 33rd International Conference on Machine
Learning, volume 48 of Proceedings of Machine
Learning Research, pages 983–992, New York, New
York, USA. PMLR.

Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Shujian Huang,
Matthias Huck, Philipp Koehn, Qun Liu, Varvara
Logacheva, Christof Monz, Matteo Negri, Matt
Post, Raphael Rubino, Lucia Specia, and Marco
Turchi. 2017. Findings of the 2017 conference
on machine translation (wmt17). In Proceedings
of the Second Conference on Machine Translation,
Volume 2: Shared Task Papers, pages 169–214,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734, Doha, Qatar. Association for Computational
Linguistics.

Junyoung Chung, Kyunghyun Cho, and Yoshua Ben-
gio. 2016. A character-level decoder without ex-
plicit segmentation for neural machine translation.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1693–1703, Berlin, Germany.
Association for Computational Linguistics.

Marta R. Costa-jussà and José A. R. Fonollosa. 2016.
Character-based neural machine translation. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 357–361, Berlin, Germany. Associa-
tion for Computational Linguistics.

Michael Denkowski and Alon Lavie. 2014. Meteor
universal: Language specific translation evaluation
for any target language. In Proceedings of the EACL
2014 Workshop on Statistical Machine Translation.

Jonas Gehring, Michael Auli, David Grangier, De-
nis Yarats, and Yann N Dauphin. 2017. Convolu-
tional sequence to sequence learning. arXiv preprint
arXiv:1705.03122.

Hakan Inan, Khashayar Khosravi, and Richard Socher.
2016. Tying word vectors and word classifiers:
A loss framework for language modeling. arXiv
preprint arXiv:1611.01462.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1700–1709, Seattle,
Washington, USA. Association for Computational
Linguistics.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander Rush. 2017. Opennmt:
Open-source toolkit for neural machine translation.
In Proceedings of ACL 2017, System Demonstra-
tions, pages 67–72, Vancouver, Canada. Association
for Computational Linguistics.

Philipp Koehn et al. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of the 45th Annual Meeting of the ACL (Demo and
Poster Sessions), pages 177–180, Prague, Czech Re-
public. Association for Computational Linguistics.

Jason Lee, Kyunghyun Cho, and Thomas Hofmann.
2017. Fully character-level neural machine trans-
lation without explicit segmentation. Transactions
of the Association for Computational Linguistics,
5:365–378.

Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W
Black. 2015. Character-based neural machine trans-
lation. arXiv preprint arXiv:1511.04586.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1412–1421, Lis-
bon, Portugal. Association for Computational Lin-
guistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Jinseok Nam, Eneldo Loza Mencı́a, and Johannes
Fürnkranz. 2016a. All-in text: learning document,
label, and word representations jointly. In Thirtieth
AAAI Conference on Artificial Intelligence.

Jinseok Nam, Eneldo Loza Mencı́a, and Johannes
Fürnkranz. 2016b. All-in text: Learning document,

82



label, and word representations jointly. In Proceed-
ings of the 30th AAAI Conference on Artificial Intel-
ligence, pages 1948–1954, Phoenix, AR, USA.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.

Nikolaos Pappas and James Henderson. 2018. Joint
input-label embedding for neural text classification.
arXiv pre-print arXiv:1806.06219.

Ofir Press and Lior Wolf. 2017. Using the output em-
bedding to improve language models. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics:
Volume 2, Short Papers, pages 157–163, Valencia,
Spain. Association for Computational Linguistics.

Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexan-
dra Birch, Barry Haddow, Julian Hitschler, Marcin
Junczys-Dowmunt, Samuel Läubli, Antonio Valerio
Miceli Barone, Jozef Mokry, and Maria Nadejde.
2017. Nematus: a toolkit for neural machine trans-
lation. In Proceedings of the Software Demonstra-
tions of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 65–68, Valencia, Spain. Association for Com-
putational Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the ACL (Vol. 1: Long Papers), pages
1715–1725, Berlin, Germany. Association for Com-
putational Linguistics.

Richard Socher, Milind Ganjoo, Christopher D. Man-
ning, and Andrew Y. Ng. 2013. Zero-shot learning
through cross-modal transfer. In Proceedings of the
26th International Conference on Neural Informa-
tion Processing Systems, NIPS’13, pages 935–943,
Lake Tahoe, Nevada.

Vivek Srikumar and Christopher D. Manning. 2014.
Learning distributed representations for structured
output prediction. In Proceedings of the 27th In-
ternational Conference on Neural Information Pro-
cessing Systems - Volume 2, NIPS’14, pages 3266–
3274, Cambridge, MA, USA. MIT Press.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of the 27th International
Conference on Neural Information Processing Sys-
tems, NIPS’14, pages 3104–3112, Cambridge, MA,
USA. MIT Press.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. arXiv preprint arXiv:1706.03762.

Jason Weston, Samy Bengio, and Nicolas Usunier.
2011. WSABIE: Scaling up to large vocabulary im-
age annotation. In Proceedings of the 22nd Inter-
national Joint Conference on Artificial Intelligence -
Volume Volume Three, IJCAI’11, pages 2764–2770.
AAAI Press.

Majid Yazdani and James Henderson. 2015. A model
of zero-shot learning of spoken language under-
standing. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, pages 244–249.

Chih-Kuan Yeh, Wei-Chieh Wu, Wei-Jen Ko, and Yu-
Chiang Frank Wang. 2018. Learning deep latent
spaces for multi-label classification. In In Proceed-
ings of the 32nd AAAI Conference on Artificial In-
telligence, New Orleans, USA.

Yang Zhang, Boqing Gong, and Mubarak Shah. 2016.
Fast zero-shot image tagging. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition, Las Vegas, USA.

83


