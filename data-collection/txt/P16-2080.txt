



















































Cross-Lingual Word Representations via Spectral Graph Embeddings


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 493–498,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Cross-Lingual Word Representations via Spectral Graph Embeddings

Takamasa Oshikiri, Kazuki Fukui, Hidetoshi Shimodaira
Division of Mathematical Science, Graduate School of Engineering Science

Osaka University, Japan
1-3 Machikaneyama-cho, Toyonaka, Osaka

{oshikiri, fukui, shimo}@sigmath.es.osaka-u.ac.jp

Abstract

Cross-lingual word embeddings are used
for cross-lingual information retrieval or
domain adaptations. In this paper, we
extend Eigenwords, spectral monolin-
gual word embeddings based on canoni-
cal correlation analysis (CCA), to cross-
lingual settings with sentence-alignment.
For incorporating cross-lingual informa-
tion, CCA is replaced with its general-
ization based on the spectral graph em-
beddings. The proposed method, which
we refer to as Cross-Lingual Eigenwords
(CL-Eigenwords), is fast and scalable for
computing distributed representations of
words via eigenvalue decomposition. Nu-
merical experiments of English-Spanish
word translation tasks show that CL-
Eigenwords is competitive with state-
of-the-art cross-lingual word embedding
methods.

1 Introduction

There have been many methods proposed for word
embeddings. Neural network based models are
popular, and one of the most major approaches
is the skip-gram model (Mikolov et al., 2013b),
and some extended methods have also been devel-
oped (Levy and Goldberg, 2014a; Lazaridou et al.,
2015). The skip-gram model has many interest-
ing syntactic and semantic properties, and it can
be seen as the factorization of a word-context ma-
trix whose elements represent pointwise mutual
information (Levy and Goldberg, 2014b). How-
ever, word embeddings based on neural networks
(without neat implementation) can be very slow
in general, and it is sometimes difficult to under-
stand how they work. Recently, a simple spectral
method, called Eigenwords, for word embeddings

+ +

+

+

+

+

+

+

+

+

+

+

+ +

+

+

+

+

+

+

+

+

+

+

−10 −5 0 5 10

−6
−4

−2
0

2
4

6
8

PC1

P
C
2

italia

suecia

grecia

austria

francia

finlandia

italy

sweden

greece

austria

france

finland

roma

estocolmoatenasvienaparís

helsinki

rome

stockholmathensviennaparis

helsinki

Figure 1: PCA projections (PC1 and PC2) of CL-
Eigenwords of countries (bold) and its capitals
(italic) in Spanish (red) and English (blue). Word
vectors of the two languages match quite well,
although they are computed using sentence-level
alignment without knowing word-level alignment.
100-dim word representations are used for PCA
computation.

is proposed (Dhillon et al., 2012; Dhillon et al.,
2015). It is based on canonical correlation anal-
ysis (CCA) for computing word vectors by maxi-
mizing correlations between words and their con-
texts. Eigenword algorithms are fast and scalable,
yet giving good performance comparable to neural
network approaches for capturing the meaning of
words from their context.

The skip-gram model, originally proposed for
monolingual corpora, has been extended to cross-
lingual settings. Given two vector representa-
tions of two languages, a linear transformation be-
tween the two spaces is trained from a set of word
pairs for translation task (Mikolov et al., 2013a),
while other researchers use CCA for learning lin-
ear projections to a common vector space where

493



translation pairs are strongly correlated (Faruqui
and Dyer, 2014). These methods require word-
alignment in the training data, while some multi-
lingual corpora have only coarse information such
as a set of sentence pairs or paragraph pairs. Re-
cently, extensions of the skip-gram model requir-
ing only sentence-alignment have been developed
by introducing cross-lingual losses in the objective
of the original models (Gouws et al., 2015; Coul-
mance et al., 2015; Shi et al., 2015).

In this paper, instead of the skip-gram model,
we extend Eigenwords (Dhillon et al., 2015)
to cross-lingual settings with sentence-alignment.
Our main idea is to replace CCA, which is applica-
ble to only two different kinds of data, with a gen-
eralized method (Nori et al., 2012; Shimodaira,
2016) based on spectral graph embeddings (Yan
et al., 2007) so that the Eigenwords can deal
with two or more languages for cross-lingual word
embeddings. Our proposed method, referred to
as Cross-Lingual Eigenwords (CL-Eigenwords),
requires only sentence-alignment for capturing
cross-lingual relationships. The method is very
simple in mathematics as well as computation; it
involves a generalized eigenvalue problem, which
can be solved by fast and scalable algorithms
such as the randomized eigenvalue decomposi-
tion (Halko et al., 2011).

Fig. 1 shows an illustrative example of cross-
lingual word vectors obtained by CL-Eigenwords.
Although only sentence-alignment is available in
the corpus, word-level translation is automatically
captured in the vector representations; the same
words (countries and capitals) in the two lan-
guages are placed in close proximity to each other;
greece is close to grecia and rome is close to roma.
In addition, the same kinds of relationships be-
tween word pairs share similar directions in the
vector space; the direction from sweden to stock-
holm is nearly parallel to the direction from finland
to helsinki.

We evaluate the word vectors obtained by
our method on the English-Spanish cross-lingual
translation task and compare the results with those
of state-of-the-art methods, showing that our pro-
posed method is competitive with those existing
methods. We use Europarl corpus for learning the
vector representation of words. Although the ex-
periments in this paper are conducted using bilin-
gual corpus, our method can be easily applied to
three or more languages.

2 Eigenwords (One Step CCA)

CCA (Hotelling, 1936) is a multivariate analysis
method for finding optimal projections of two sets
of data vectors by maximizing the correlations.
Applying CCA to pairs of raw word vector and
raw context vector, Eigenword algorithms attempt
to find low dimensional vector representations of
words (Dhillon et al., 2012). Here we explain the
simplest version of Eigenwords called One Step
CCA (OSCCA).

We have monolingual corpus consisting of T to-
kens; (ti)i=1,...,T , and the vocabulary consisting
of V word types; {vi}i=1,...,V . Each token ti is
drawn from this vocabulary. We define word ma-
trix V ∈ {0, 1}T×V whose i-th row encodes token
ti by 1-of-V representation; the j-th element is 1
if the word type of ti is vj , 0 otherwise.

Let h be the size of context window. We de-
fine context matrix C ∈ {0, 1}T×2hV whose i-th
row represents the surrounding context of token
ti with concatenated 1-of-V encoded vectors of
(ti−h, . . . , ti−1, ti+1, . . . , ti+h).

We apply CCA to T pairs of row vectors of V
and C. The objective function of CCA is con-
structed using V⊤V, V⊤C, C⊤C which rep-
resent occurrence and co-occurrence counts of
words and contexts. In Eigenwords, however, we
use CV V ∈ RV×V+ , CV C ∈ RV×2hV+ , CCC ∈
R2hV×2hV+ with the following preprocessing of
these matrices before constructing the objective
function. First, centering-process of V and C is
omitted, and off-diagonal elements of C⊤C are
ignored for simplifying the computation of in-
verse matrices. Second, we take the square root
of the elements of these matrices for “squash-
ing” the heavy-tailed word count distributions. Fi-
nally, we obtain vector representations of words
as C−1/2V V (u1, . . . , uK), where u1, . . . , uK ∈ RV
are left singular vectors of C−1/2V V CV C C−1/2CC cor-
responding to the K largest singular values. The
computation of SVD is fast and scalable using
recent idea of random projections (Halko et al.,
2011).

3 Cross-Lingual Eigenwords

In this section, we introduce Cross-Lingual
Eigenwords (CL-Eigenwords), a novel method
for cross-lingual word embeddings. Suppose
that we have parallel corpora that contain L lan-
guages. Schematic diagrams of Eigenwords and

494



Figure 2: Eigenwords are CCA-based spectral
monolingual word embeddings. CL-Eigenwords
are CDMCA-based spectral cross-lingual word
embeddings, where the two (or more) languages
are linked by sentence-alignment.

CL-Eigenwords (with L = 2) are shown in Fig. 2.
In the same way as the monolingual Eigen-

words, we denote the word matrix and the context
matrix for ℓ-th language by V(ℓ) ∈ RT (ℓ)×V (ℓ)+ and
C(ℓ) ∈ RT (ℓ)×2h(ℓ)V (ℓ)+ respectively, where V (ℓ) is
the size of vocabulary, T (ℓ) is the number of to-
kens, and h(ℓ) is the size of context window. There
are D sentences (or paragraphs) in the multilin-
gual corpora, and each token is included in one
of the sentences. The sentence-alignment is repre-
sented in the matrix J(ℓ) ∈ RT (ℓ)×D+ whose (i, j)-
element J (ℓ)i,j is set to 1 if the i-th token t

(ℓ)
i of ℓ-th

language corpus comes from the j-th sentence or
0 otherwise. We also define document matrix D
whose j-th row encodes j-th sentence by 1-of-D
representation; D = ID, where ID represents D-
dimensional identity matrix.

The goal of CL-Eigenwords is to construct vec-
tor representations of words of two (or more)
languages from multilingual corpora at the same
time. This problem is formulated as an example
of Cross-Domain Matching Correlation Analysis
(CDMCA) (Shimodaira, 2016), which deals with
many-to-many relationships between data vectors
from multiple sources. CDMCA is based on
the spectral graph embeddings (Yan et al., 2007),
and attempts to find optimal linear projections of
data vectors so that associated transformed vec-
tors are placed in close proximity to each other.
The strength of association between two vectors
is specified by a nonnegative real value called
matching weight. Since CDMCA includes CCA

and a variant of Latent Semantic Indexing (LSI)
(Deerwester et al., 1990) as special cases, CL-
Eigenwords can be interpreted as LSI-equipped
Eigenwords (See Appendix).

In CL-Eigenwords, the data vectors are given
as v(ℓ)i , c

(ℓ)
i ,di, namely, the i-th row vectors of

V(ℓ),C(ℓ),D, respectively. The matching weights
between row vectors of V(ℓ) and C(ℓ) are speci-
fied by the identity matrix IT (ℓ) because the data
vectors are in one-to-one correspondence. On the
other hand, the matching weights between row
vectors of V(ℓ) and D as well as those between
C(ℓ) and D are specified by J̃

(ℓ)
= b(ℓ)J(ℓ), the

sentence-alignment matrix multiplied by a con-
stant b(ℓ). Then we will find linear transformation
matrices A(ℓ)V ,A

(ℓ)
C ,AD, (ℓ = 1, 2, . . . , L) to K-

dimensional vector space by minimizing the ob-
jective function

L∑
ℓ=1

T (ℓ)∑
i=1

∥v(ℓ)i A(ℓ)V − c(ℓ)i A(ℓ)C ∥22

+
L∑

ℓ=1

T (ℓ)∑
i=1

D∑
j=1

J̃
(ℓ)
i,j ∥v(ℓ)i A(ℓ)V − djAD∥22

+
L∑

ℓ=1

T (ℓ)∑
i=1

D∑
j=1

J̃
(ℓ)
i,j ∥c(ℓ)i A(ℓ)C − djAD∥22 (1)

with a scale constraint for projection matrices.
Note that the first term in (1) is equivalent to that
of CCA between words and contexts, namely the
objective of monolingual Eigenwords, and there-
fore word vectors of two languages are obtained
as row vectors of A(ℓ)V (ℓ = 1, 2, . . . , L).

Hereafter, we assume L = 2 for notational sim-
plicity. A generalization to the case L > 2 is
straightforward; redefine X, W, A below by re-
peating the submatrices, such as V(ℓ) and C(ℓ), for
L times. For solving the optimization problem, we
define

X =


V(1) O O O O

O C(1) O O O

O O V(2) O O

O O O C(2) O
O O O O D

 ,

W =


O IT (1) O O J̃

(1)

IT (1) O O O J̃
(1)

O O O IT (2) J̃
(2)

O O IT (2) O J̃
(2)

J̃
(1)⊤

J̃
(1)⊤

J̃
(2)⊤

J̃
(2)⊤

O

 ,
A⊤ = (A(1)⊤V ,A

(1)⊤
C ,A

(2)⊤
V ,A

(2)⊤
C ,A

⊤
D).

495



1 – 1000 1 – 1000 5001 – 6000 5001 – 6000
es→ en en→ es es→ en en→ es

Method Time [min] P@1 P@5 P@1 P@5 P@1 P@5 P@1 P@5
Edit distance - 29.1 37.8 20.6 34.4 28.5 40.0 26.4 33.5
BilBOWA (40 dim.) ∗ 4.6 46.7 59.6 43.6 56.4 44.6 53.6 49.4 58.7
BilBOWA (100 dim.) ∗ 7.5 43.3 55.9 36.8 49.0 43.6 53.3 48.6 57.9
BilBOWA (200 dim.) ∗ 11.6 38.8 52.2 29.7 43.2 43.3 52.0 47.3 57.2
CL-LSI (40 dim.) 1.4 45.9 54.8 46.9 55.8 31.6 38.5 40.7 45.1
CL-LSI (100 dim.) 2.4 51.7 62.9 48.5 61.8 41.6 49.8 42.8 49.1
CL-LSI (200 dim.) 5.1 55.2 66.5 50.7 65.5 45.5 54.7 45.6 51.9
CL-Eigenwords (40 dim.) 9.5 54.7 66.2 53.3 65.7 40.3 49.2 44.7 50.0
CL-Eigenwords (100 dim.) 19.6 57.7 71.3 54.9 70.3 47.9 59.0 49.3 54.6
CL-Eigenwords (200 dim.) 37.5 58.7 72.4 56.2 72.2 51.6 62.4 50.6 55.7

Table 1: Computational times (in minutes) and word translation accuracies (in percent, higher is better)
evaluated by Precision@n using the 1,000 test words (the 1st to 1,000th most frequent words or the
5,001st to 6,000th most frequent words). Shown are for Spanish (es) to English (en) translation and
for English (en) to Spanish (es) translation. ∗ BilBOWA is executed on 3 threads, while CL-LSI and
CL-Eigenwords are executed on a single thread.

Also define H = X⊤WX, G = X⊤MX, M =
diag(W1). Then the optimization problem (1)
is equivalent to maximizing Tr(A⊤HA) with a
scale constraint A⊤GA = IK . Following the
Eigenwords implementation (Dhillon et al., 2015),
we replace H,G with H, G by ignoring the non-
diagonal elements of G and taking the square root
of elements in H,G. The optimization problem
is solved as a generalized eigenvalue problem, and
the word representations, as well as those for con-
texts and sentences, are obtained as row vectors
of Â = G−1/2(u1, . . . , uK), where u1, . . . , uK
are eigenvectors of (G−1/2)⊤HG−1/2 for the K
largest eigenvalues. We choose K so that all the
K eigenvalues are positive. As in the case of
monolingual Eigenwords, we can exploit fast im-
plementations such as the randomized eigenvalue
decomposition (Halko et al., 2011); our compu-
tation in the experiments is only approximation
based on the low-rank factorization with rank 2K.

For measuring similarities between two word
vectors x, y ∈ RK , we use the weighted cosine
similarity

sim(x,y) = (∥x∥2 · ∥y∥2)−1
K∑

i=1

λixiyi,

where λi is the i-th largest eigenvalue.

4 Experiments

The implementation of our method is available on
GitHub1. Following the previous works (Mikolov
et al., 2013a; Gouws et al., 2015), we use only

1https://github.com/shimo-lab/kadingir

the first 500K lines of English-Spanish sentence-
aligned parallel corpus of Europarl (Koehn, 2005)
for numerical experiments.

4.1 Word Translation Tasks
Experiments are performed in similar settings
as the previous works based on the skip-gram
model (Mikolov et al., 2013a; Gouws et al., 2015).
We extract 1,000 test words with frequency rank
1–1000 or 5001–6000 from the source language,
and translate these words to the target language
using Google Translate, assuming they are the cor-
rect translations. Then, we evaluate the transla-
tion accuracies of each method with precision@n
as the fraction of correct translations for the test
words being in the top-n words of the target lan-
guage returned by each method.

4.2 Baseline Systems
We compare CL-Eigenwords with the following
three methods.

Edit distance Finding the nearest words mea-
sured by Levenshtein distance.

CL-LSI Cross-Language LSI (CL-LSI) (Littman
et al., 1998) is not originally for word embed-
dings. However, since this method can be used for
cross-lingual information retrieval, we select it as
one of our baselines. For each language, we con-
struct the term-document matrix of size V (ℓ) ×D
whose (i, j)-element represents the frequency of
i-th word in j-th sentence. Then LSI is applied to
the concatenated matrix of size (V (1)+V (2))×D.
BilBOWA BilBOWA (Gouws et al., 2015) is one
of the state-of-the-art methods for cross-lingual

496



word embeddings based on the skip-gram model.
We obtain vector representations of words using
publicly available implementation.2

4.3 Results

In CL-Eigenwords, vocabulary size V (1) =
V (2) = 104, window size h(1) = h(2) = 2, the
constant b(1) = b(2) = 103. The dimensional-
ity of vector representations is K = 40, 100, or
200. Similarities of two vector representations
are measured by the unweighted cosine similar-
ity in CL-LSI and BilBOWA. Our experiments
were performed on a CentOS 7.2 server with In-
tel Xeon E5-2680 v3 CPU, 256GB of RAM and
gcc 4.8.5. The computation times and the result
accuracies of word translation tasks are shown in
Table 1. We observe that CL-Eigenwords is com-
petitive with BilBOWA and CL-LSI. In particu-
lar, CL-Eigenwords performed very well for the
most frequent words (ranks 1–1000) in this par-
ticular parameter setting. Furthermore, the com-
putation times of CL-Eigenwords are as short as
those of BilBOWA for achieving similar accura-
cies. Preliminary experiments also suggest that
CL-Eigenwords works well for semi-supervised
learning where sentence-alignment is specified
only partially; the word translation accuracies are
maintained well with aligned 240K lines and un-
aligned 260K lines.

5 Conclusion

We proposed CL-Eigenwords for incorporating
cross-lingual information into the monolingual
Eigenwords. Although our method is simple, ex-
perimental results of English-Spanish word trans-
lation tasks show that the proposed method is com-
petitive with other state-of-the-art cross-lingual
methods.

Acknowledgments

This work was partially supported by grants from
Japan Society for the Promotion of Science KAK-
ENHI (24300106, 16H01547 and 16H02789) to
HS.

Appendix

In this Appendix, we discuss the relationships be-
tween CL-LSI and CL-Eigenwords.

2https://github.com/gouwsmeister/
bilbowa

Figure 3: Cross-Language Latent Semantic Index-
ing (CL-LSI) does not use the context information.

Let V(1),V(2),D,J(1),J(2) be those defined in
Section 3. In CL-LSI, we consider the truncated
singular value decomposition of a word-document
matrix

B =
(
V(1)⊤J(1)

V(2)⊤J(2)

)
≈ AV ΛKA⊤D

using the largest K singular values. Then row vec-
tors of AV are the vector representations of words
of CL-LSI.

CL-LSI can also be interpreted as an eigenvalue
decomposition of H = X⊤WX where

X =

(
V(1) O O

O V(2) O
O O D

)
,

W =

(
O O J(1)

O O J(2)

J(1)⊤ J(2)⊤ O

)

are redefined from those in Section 3 by remov-
ing submatrices related to contexts. The structure
of X and W is illustrated in Fig. 3. Similarly to
CL-Eigenwords of Section 3, but ignoring G, we
define A = (u1, . . . , uK) with the eigenvectors
of H for the largest K eigenvalues λ1, . . . , λK . It
then follows from

H =
(

O B
B⊤ O

)
that A⊤ = 2−1/2(A⊤V ,A

⊤
D) with the same AV

and AD obtained by the truncated singular value
decomposition. The eigenvalues are the same as
the singular values: diag(λ1, . . . , λK) = ΛK .
Therefore CL-LSI is interpreted as a variant of
CL-Eigenwords without the context information.

References
Jocelyn Coulmance, Jean-Marc Marty, Guillaume

Wenzek, and Amine Benhalloum. 2015. Trans-
gram, fast cross-lingual word-embeddings. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1109–
1113, Lisbon, Portugal, September. Association for
Computational Linguistics.

497



Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American society for information science,
41(6):391.

Paramveer S. Dhillon, Jordan Rodu, Dean P. Foster,
and Lyle H. Ungar. 2012. Two step cca: A
new spectral method for estimating vector models
of words. In John Langford and Joelle Pineau, ed-
itors, Proceedings of the 29th International Confer-
ence on Machine Learning (ICML-12), ICML ’12,
pages 1551–1558, New York, NY, USA, July. Om-
nipress.

Paramveer S. Dhillon, Dean P. Foster, and Lyle H. Un-
gar. 2015. Eigenwords: Spectral word embeddings.
Journal of Machine Learning Research, 16:3035–
3078.

Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilingual
correlation. In Proceedings of the 14th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 462–471, Gothenburg,
Sweden, April. Association for Computational Lin-
guistics.

Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2015. Bilbowa: Fast bilingual distributed represen-
tations without word alignments. In Proceedings
of the 32nd International Conference on Machine
Learning, ICML 2015, Lille, France, 6-11 July 2015,
pages 748–756.

Nathan Halko, Per-Gunnar Martinsson, and Joel A.
Tropp. 2011. Finding structure with random-
ness: Probabilistic algorithms for constructing ap-
proximate matrix decompositions. SIAM review,
53(2):217–288.

Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321–377.

Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Sum-
mit, pages 79–86, Phuket, Thailand. AAMT.

Angeliki Lazaridou, The Nghia Pham, and Marco Ba-
roni. 2015. Combining language and vision with a
multimodal skip-gram model. In Proceedings of the
2015 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 153–163. Asso-
ciation for Computational Linguistics.

Omer Levy and Yoav Goldberg. 2014a. Dependency-
based word embeddings. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
302–308. Association for Computational Linguis-
tics.

Omer Levy and Yoav Goldberg. 2014b. Neural word
embedding as implicit matrix factorization. In Ad-
vances in Neural Information Processing Systems,
pages 2177–2185.

Michael L. Littman, Susan T. Dumais, and Thomas K.
Landauer. 1998. Automatic cross-language infor-
mation retrieval using latent semantic indexing. In
Cross-language information retrieval, pages 51–62.
Springer.

Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013a. Exploiting similarities among lan-
guages for machine translation. arXiv preprint
arXiv:1309.4168.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeff Dean. 2013b. Distributed represen-
tations of words and phrases and their composition-
ality. In Advances in neural information processing
systems, pages 3111–3119.

Nozomi Nori, Danushka Bollegala, and Hisashi
Kashima. 2012. Multinomial relation prediction
in social data: A dimension reduction approach. In
AAAI, volume 12, pages 115–121.

Tianze Shi, Zhiyuan Liu, Yang Liu, and Maosong
Sun. 2015. Learning cross-lingual word embed-
dings via matrix co-factorization. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 2: Short Papers), pages 567–572, Beijing,
China, July. Association for Computational Linguis-
tics.

Hidetoshi Shimodaira. 2016. Cross-validation of
matching correlation analysis by resampling match-
ing weights. Neural Networks, 75:126–140.

Shuicheng Yan, Dong Xu, Benyu Zhang, Hong-Jiang
Zhang, Qiang Yang, and Stephen Lin. 2007. Graph
embedding and extensions: A general framework for
dimensionality reduction. Pattern Analysis and Ma-
chine Intelligence, IEEE Transactions on, 29(1):40–
51, Jan.

498


