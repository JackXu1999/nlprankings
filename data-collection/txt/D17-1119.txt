



















































Human Centered NLP with User-Factor Adaptation


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1146–1155
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Human Centered NLP with User-Factor Adaptation

Veronica E. Lynn, Youngseo Son, Vivek Kulkarni
Niranjan Balasubramanian and H. Andrew Schwartz

Stony Brook University
Stony Brook, NY

{velynn, yson, vvkulkarni, niranjan, has}@cs.stonybrook.edu

Abstract

We pose the general task of user-factor
adaptation — adapting supervised learn-
ing models to real-valued user factors in-
ferred from a background of their lan-
guage, reflecting the idea that a piece of
text should be understood within the con-
text of the user that wrote it. We intro-
duce a continuous adaptation technique,
suited for real-valued user factors that are
common in social science and bringing us
closer to personalized NLP, adapting to
each user uniquely. We apply this tech-
nique with known user factors including
age, gender, and personality traits, as well
as latent factors, evaluating over five tasks:
POS tagging, PP-attachment, sentiment
analysis, sarcasm detection, and stance de-
tection. Adaptation provides statistically
significant benefits for 3 of the 5 tasks:
up to +1.2 points for PP-attachment, +3.4
points for sarcasm, and +3.0 points for
stance.

1 Introduction

Language use is personal. Knowing who wrote
a piece of text can help to better understand it.
For instance, knowing the age and gender groups
of authors has been shown to improve document
classification (Hovy, 2015) and sentiment ana-
lysis (Volkova et al., 2013).

However, putting people into discrete groups
(e.g. age groups, binary gender) often relies on
arbitrary boundaries which may not correspond to
meaningful changes in language use. A wealth of
psychological research suggests people should not
be characterized as discrete types (or domains) but
rather as mixtures of continuous factors (McCrae

and Costa Jr., 1989; Ruscio and Ruscio, 2000;
Widiger and Samuel, 2005).

Here, we ask how one can adapt NLP models
to real-valued human factors – continuous valued
attributes that capture fine-grained differences be-
tween users (e.g. real-valued age, continuous gen-
der scores). We refer to this problem as user-
factor adaptation, and investigate a solution to it
in the context of social media, a genre where lan-
guage is generated by a particularly diverse set
of users (Duggan and Smith, 2013). Importantly,
user-factor adaptation brings us closer to person-
alized NLP in that with real-valued factors we can
now adapt uniquely for each user.

Our approach composes user factor information
with the linguistic features, similar to feature aug-
mentation (Daumé III, 2007), a widely used do-
main adaptation technique which allows for easy
integration with most feature-based learning mod-
els. Since relevant user information often is not
explicitly available, we use a background of tweets
from the user to infer user factors. We evaluate
our approach over five tasks — POS tagging, PP-
attachment, sentiment analysis, sarcasm detection,
and stance detection — and with a variety of in-
ferred user factors including (a) known factors:
age, gender, and personality traits, as well as (b)
latent factors derived from past user tweets.

Contributions. The main contributions of this
work include (a) adaptation based simply on back-
ground language (e.g. past tweets; no required a
priori user knowledge or “domain”), (b) a method
for adapting models based on continuous vari-
ables, (c) adaptation to other user attributes be-
yond age and gender (personality and latent fac-
tors), and (d) empirical evidence that standard
NLP models can often be improved by user-factor
adaptation with a range of inferred factors.

1146



2 User-Factor Adaptation

User-factor adaptation is especially critical for so-
cial media, where content is generated by a di-
verse user base (Duggan and Smith, 2013). Adap-
tation requires two components: 1) a user factor
representation that captures salient traits indicative
of language differences between users, and 2) an
adaptation technique that uses this representation
to modify learning appropriately.

User factors, even simple ones such as age and
gender, may not always be readily available. The
messages posted by users, however, are often pub-
lic and can be used to infer many known linguisti-
cally relevant user traits including personality, as
well as latent language factors (described next).
Given this background information about users,
the user-factor adaptation problem is to learn a
single model that is sensitive to both the varia-
tions and commonalities in language across differ-
ent users.

3 User Factors

The first step in our adaptation approach is to cre-
ate a representation of users that relates to their
language use. To this end, we explore two sets of
factors: 1) inferred demographics and personality
traits, and 2) latent language factors that directly
capture language use variations among users.

Different from prior work, we model these hu-
man attributes as real-valued factors, as is com-
mon in psychology literature. Although they may
refer to discrete classes such as cluster member-
ship, a factor representation is able to capture
more nuanced differences and characteristics that
are best understood as a continuum (McCrae and
Costa Jr., 1989; Ruscio and Ruscio, 2000; Widiger
and Samuel, 2005). This is critical for our goal
of moving beyond group-level adaptation toward
personalization.

3.1 Demographic and Personality Factors

Many studies have linked language variations with
demographic (Argamon et al., 2007; Cheshire,
2005), occupational (Preoţiuc-Pietro et al., 2016)
and other psychosocial variables such as person-
ality (Schwartz et al., 2013). We investigate the
relevance of a subset of these social variables as
user factors for adaptation.

However, we may not have direct access to such
information. Unlike the tweets posted by a user,

their demographic and personality traits are not al-
ways publicly available. We use automatic classi-
fication models for obtaining real-valued age and
gender estimates (Sap et al., 2014) and personality
traits (Park et al., 2015). In addition to being rea-
sonably accurate (e.g. age prediction has a Pear-
son r of .83 with true age), language based estima-
tion of factor scores may capture linguistic prefer-
ences more clearly. For instance, Bamman et al.
(2014b) found that perceived gender was strongly
linked to the gender makeup of a user’s social net-
work, and may be a better descriptor of linguistic
preferences than self-reported gender.

3.2 Latent Language Factors
We also explore methods to derive latent factors
that capture language use similarities and varia-
tions across users. The main idea is to derive a la-
tent d-dimensional representation of each user us-
ing their background tweets. While there are many
choices here, we explore a factorization technique
(generative factor analysis), a clustering technique
(k-means with TF-IDF), and a hybrid (word2vec
with k-means).

Generative Factor Analysis. Factorization
methods allow us to build latent representations
of users by finding low-rank approximations of
the original high-dimensional representations of
their text. We use a general method called factor
analysis (FA) (Lawley and Maxwell, 1971). Intu-
itively, FA seeks to capture the variability across
correlated variables as a weighted linear combi-
nation of a given number of latent dimensions,
thus allowing a low-dimensional representation of
words1.

Formally, let M|U|×|V| denote the user-term ma-
trix, whose entries Mij indicate the number of
times word j is used by user i. FA factorizes this
high-dimensional representation into two matrices
F and L as follows: M = FL + E where E is an
error matrix consisting of residual errors not cap-
tured by FL and where the residual noise is as-
sumed to be Gaussian distributed with zero mean.

Clustering. We also explore commonly used
text clustering-based methods to derive latent fac-
tors from the users’ tweets. The idea is to cluster
the users based on their tweets. In one case we use
TF-IDF based representations, and in the other we
use word2vec embeddings (Mikolov et al., 2013).
1In this sense FA is a more flexible method than singular
value decomposition in that it allows factors to be correlated.

1147



We produce a k-means clustering on this re-
duced dimensional space to create clusters of users
who have similar language use. We derive real-
valued factors from these clusters using the dis-
tance of the user to the centers of each cluster.
Cluster membership yields the discrete represen-
tation. Refer to section 5.1 for implementation de-
tails.

4 Adaptation Models

Given a factor representation of each user, the
adaptation task is to learn a model that is sen-
sitive to both the differences and commonalities
across all users. This is similar to the objective
for domain adaptation tasks, where the task data
is drawn from one or more underlying domains
and learning needs to account for both the simi-
larities and differences in the domains. We formu-
late user-factor adaptation as a domain adaptation
technique based on feature augmentation (Daumé
III, 2007) but rather than force users into discrete
domains, we develop a continuous formulation
that allows us to make good use of the real-valued
user factors.

Here we first describe a direct discrete formula-
tion of feature augmentation and then describe our
proposed continuous formulation.

4.1 Discrete Adaptation

Feature augmentation uses domain information to
transform instances into a new augmented space
such that instances from the same domain have
higher similarity in the augmented space com-
pared to instances from different domains. A
learner operating over this augmented space can
now learn to model both domain-specific and
domain-general influences of the features.

The discrete adaptation method is a direct ap-
plication of this idea, where the training and test
instances are mapped into domains based on some
grouping that we induce from the user factors. For
example, the user factor age induces three discrete
domains: low (age < 24), middle (24 < age <
28), and high (age > 28).

Given the instance domain mapping, feature
augmentation transforms the instances based on
their domain. Suppose the original instances have
n features and suppose there are d discrete fac-
tor classes (F1, · · · , Fd) i.e., d domains. Given
an instance which is mapped to a factor class Fi,
augmentation creates a new feature vector that has

User Factor Augmented Instance
Classes Φ(x, u)

User 1 F1 〈x,x,0,0,· · · , 0〉
User 2 F2 〈x,0,x,0,· · · , 0〉
User 3 F1, F3 〈x,x,0,x,· · · , 0〉
User 4 Fk 〈x,0,0,· · · , 0, x〉

Table 1: Discrete Factor Adaptation: Augmen-
tations of an original instance vector x under dif-
ferent factor class mappings. With k domains the
augmented feature vector is of length n(k + 1).

d + 1 feature sets of length n each. The origi-
nal features are copied over to the first feature set
for all instances regardless of their domain. For
instances from domain i, the original features are
copied over to feature set i + 1. The other feature
sets are zeroes. Table 1 shows some examples of
this augmentation strategy for a single instance, x,
under different factor class mappings.

These augmented instances are used for train-
ing and testing without any further modifications
to the original learning formulation.

4.2 Continuous Adaptation

Discrete adaptation ignores the continuous nature
of user factors. Unlike the commonly considered
domains, people don’t fit neatly into discrete bins.
Many psychological studies have shown the in-
effectiveness of treating user factors as discrete
types (McCrae and Costa Jr., 1989); we expect
an adaptation method which does so to be sim-
ilarly ineffective. For most factors the bound-
aries for determining classes is unclear, and such
arbitrarily-drawn boundaries may not correspond
to big changes in language use.

Figure 1 illustrates the advantage of continuous
adaptation for a single feature — whether the cur-
rent instance contains an intensifier — using sar-
casm detection as an example. The colored shapes
show the feature values for instances from four
users, with green squares representing “sarcastic”
tweets and yellow circles representing “not sar-
castic” ones. The model is unable to distinguish
between sarcastic and non-sarcastic tweets in the
no adaptation and discrete adaptation case. While
discrete adaptation could induce some separabil-
ity, in this case it fails to account for the variations
between differently-aged over 30 users. On the
other hand, if we use features values that are pro-
portional to the actual age, it can result in a better

1148



Figure 1: Comparison of feature augmentation under discrete and continuous adaptations. Each shape
represents a particular observation (e.g. a tweet) to be classified, each from a different user. The x-
axis represents a particular boolean feature: whether the tweet has an intensifier. The y-axis represents
how the feature is augmented by the users’ age using both discrete adaptation (middle) and continuous
adaptation (right). Continuous adaptation allows us to distinguish observations where discrete may not.

separation as shown in the figure.
A compositional function c combines d user

factor scores fu,d with original feature values x:

Φ(x, u) = 〈x, c(fu,1,x), c(fu,2,x), · · · , c(fu,d,x)〉
Thus, a version of each feature exists with and
without the factor information integrated. We
will explore a simple multiplicative compositional
function (i.e., c(fu,d,x) = fu,d · x) but others can
be imagined (e.g. additive, multiplicative with ker-
nel functions).

Multiplicative composition has the property of
reducing Φ to discrete adaptation when the factors
are binary i.e., c(fu,d,x) = x when u ∈ Fd and
c(fu,d,x) = 0 otherwise. As with discrete adapta-
tion, learning then proceeds unmodified with these
augmented instances.

The augmented training data (trainaug) is thus
associated with the features x of the tweet, the task
labels y, and the user information u. Following the
feature augmentation formulation, any supervised
learning task of finding a parametrized function hθ
over the original labeled training data can now be
specified in terms of the augmented training data
along with the transformed instances:

arg min
θ

∑
(x,y,u)∈trainaug

loss (hθ (Φ(x, u), y))

For test instances we apply the same transforma-
tion function Φ before prediction.

5 Evaluation

We apply user-factor adaptation to five popular
NLP tasks: part-of-speech tagging, prepositional-

phrase attachment, sentiment analysis, sarcasm
detection, and stance detection. These represent
both syntactic and semantic tasks; include some
of the key steps in an NLP application pipeline;
and use different types of learning formulations
including logistic regression, conditional random
fields, and support vector machines.

We demonstrate the value of user-factor adap-
tation on strong baselines for each task. Table 2
provides the specific details for each task includ-
ing the systems used and their configurations.

5.1 Implementation Details

We learn factors from a user’s background lan-
guage, or past tweets2. To do so, we collect up
to 200 tweets per user; users with fewer than 20
tweets were excluded. Retweets were not included
and all tweets were tokenized using the Happier
Fun Tokenizer3.

Demographics and Personality. We derive
real-valued demographics and personality scores
using the models introduced in section 3.1. For
demographics, our model predicts continuous age
and a gender score where higher values imply
more “femaleness”. For personality, these scores
represent the Big Five personality traits: open-
ness to experience, conscientiousness, extraver-
sion, agreeableness, and neuroticism (Goldberg,
1990; McCrae and Costa Jr., 1997). Age, gender,
and the five personality dimensions are each a sin-
gle factor.

2Factor inference code is available at:
https://stonybrooknlp.github.io/user-factor-adaptation/

3https://github.com/dlatk/happierfuntokenizing

1149



Task POS Tagging PP-Attachment Sentiment Sarcasm Stance

Output POS tags ranked attachments positive, neutral,negative
sarcastic, not

sarcastic for, against, neutral

System Owoputi et al.(2013)
variant on Belinkov

et al. (2014)
Mohammad et al.

(2013)
Bamman and
Smith (2015)

Mohammad et al.
(2016)

Features Brown clusters,lexical features
n-grams, Treebank,

WordNet
word/char n-grams,

lexicon features all tweet features word/char n-grams

Learning
Alg.

conditional
random field

SVM-Rank
(Joachims, 2006) linear-SVM

logistic
regression SVM

Dataset Owoputi et al.(2013)
Kong et al. (2014)
+ 986 new tweets

SemEval 2013
(Nakov et al., 2013)

Bamman and
Smith (2015)

SemEval 2016
(Mohammad et al.,

2016)

Eval Train/Test,Accuracy
Cross-validation,

Accuracy Train/Test, F1
Cross-validation,

F1 Train/Test, F1

Tweets 1544 1319 10339 17084 3021
Users 1541 1319 9917 10966 2349

Instances 22723 2365 10339 17084 3021

Table 2: Overview of the experimental setup for all tasks. Choices were dictated primarily by the
literature on top performing systems for each task.

Latent Language Factors. We use three meth-
ods to derive latent factors: (1) tf-idf: The TF-IDF
approach uses unigrams, bigrams, and trigrams
occurring in more than 20% but fewer than 80%
of documents. (2) word2vec: The skip-grams al-
gorithm (Mikolov et al., 2013) was used to pro-
duce 50-dimensional word embeddings. (3) user-
embed: d-dimensional user embeddings from
generative factor analysis (Child, 1990) over rel-
ative frequencies of n-grams per user-background.
The TF-IDF and word2vec representations are
then clustered to produce a low-dimensional repre-
sentation of the users. Each dimension is a single
factor. We primarily report results for d=5 for all
latent factors, although we explore alternate values
in Section 5.3.

Discrete Adaptation. Each user is mapped to a
single “domain” per factor. For inferred age, we
select three equally-sized domains: age < 24,
24 < age < 28, and age > 28. TF-IDF and
word2vec define their domains based on cluster
membership. Gender, personality, and user em-
beddings have two domains, above and below the
mean, which is done on a per-dimension basis.

Continuous Adaptation. We apply transforma-
tions to the raw factor scores before using them
for adaptation. For demographic and personality
factors, we apply a min-max transformation. Be-
cause language often does not vary linearly with
age (Pennebaker and Stone, 2003), we addition-
ally use the square root of the predicted age. For
the cluster based latent factors, we use the inverse
of the Euclidean distance of the user-background

from the cluster centroid, amplifying the power of
those users who are closest to each cluster. User
embeddings from factor analysis are used with-
out any transforms since they naturally produce a
Gaussian distribution.

5.2 Results

Table 3 presents the main adaptation results. We
compare the performance of adaptation techniques
against two baselines: no inclusion of additional
factors or adaptation, and models with factors ran-
domly drawn from a Gaussian distribution – a sit-
uation requiring learning the same number of pa-
rameters as our most augmented models. For the
random factor baseline, we take the average per-
formance across five iterations for both discrete
and continuous adaptation. To establish signifi-
cance of difference in error between adaptation re-
sults and the no-adaptation baseline, we use per-
mutation testing for stance detection and McNe-
mar’s test for the others. Our findings follow.
While these conclusions were drawn from our
own experiments, we encourage future researchers
to see what works best on their own tasks and
datasets.
(i) Adaptation improves over unadapted baselines:
The results show significant gains with adaptation
for PP-attachment (+1.0), sentiment (+1.0),
sarcasm (+3.4), and stance (+3.0). Adaptation
yields better results for sarcasm and stance, se-
mantic tasks where we’d expect user preferences
to be an important factor. While prior studies
have shown POS variations across demographic
factors (Pennebaker and Stone, 2003; Schwartz

1150



pos tagging pp-attachment sentiment sarcasm stance
eval measure acc. acc. F1 F1 F1
adaptation factors disc cont disc cont disc cont disc cont disc cont
baselines
no adaptation 91.7 91.7 71.0 71.0 60.6 60.6 73.9 73.9 64.9 64.9
random factors 91.4 91.7 71.0 70.7 59.1 61.1 73.4 74.0 65.5 65.3
user-factor adaptation — known factors
age 91.5 91.7 69.6 70.8 60.0 61.4 74.9† 74.8† 66.3 64.9
gender 91.6 91.9 69.7 70.7 61.0 61.0 75.0† 75.1† 66.2 65.1
personality 91.1 91.2 71.3 70.2 58.6 61.2 74.3 75.6† 67.7† 66.3
user-factor adaptation — latent factors
user embed (d=5) 91.2 90.9 70.7 70.8 59.8 60.7 73.9 77.3† 64.6 67.9†
tf-idf (d=5) 91.4 91.5 70.5 72.0† 58.7 61.6 73.8 74.7† 66.8 64.9
word2vec (d=5) 91.6 90.7 70.3 71.1 56.30 60.5 76.4† 76.9† 67.0 66.2

Table 3: Results of user-factor adaptation across all tasks. Adaptation results are shown in comparison
with baseline performance (1) without adaptation and (2) with adaptation using randomly-assigned fac-
tors. disc denotes discrete adaptation results, and cont denotes continuous adaptation results. † indicates
statistically significant results at 0.05 level, in comparison to the no-adaptation baseline.

et al., 2013), we hypothesize that the ambiguity in
POS reduces greatly when conditioning on local
context compared to demographic preferences.
This coupled with the ceiling effect in a strong
baseline may explain the lack of improvements.

(ii) Continuous is better than discrete: For PP-
attachment, sarcasm, and sentiment, continuous
adaptation is better than discrete in all but three
of the eighteen configurations. Binning people
into discrete groups is hard and lossy; using
continuous weights helps avoid this issue. Stance,
however, is the one task where discrete works
better for most factors. As we show in Section 5.4,
demographics and personality scores are them-
selves highly predictive of stances on issues; we
believe this makes the simpler binning approach
more reliable than the continuous model.

(iii) Both known and latent factors are helpful:
Sarcasm benefits from age, gender and personality
based adaptations, while stance benefits from
personality. The demographic and personality
factors do not help PP-attachment. Language
factors help all tasks except POS tagging.

(iv) Latent factors provide best gains: The latent
language factors provide the best observed gains
for all of the tasks where we saw a significant
improvement: PP-attachment, sentiment, sarcasm,
and stance. The language factors model users di-
rectly in terms of the similarities (and differences)
in their entire language use, whereas the inferred
demographic and personality factors focus more
on a subset of their language as it relates to the
particular attribute.

(v) Expanding feature space alone is not enough:

One possible explanation for the gains with
factors are that the expanded feature space could
somehow provide more capacity for learners to
generalize. However, adapting to random factors
typically lowered results, suggesting that models
using more features but not more information
naturally take a hit.

5.3 Background Size and Number of Factors

The amount of background available directly af-
fects the factor measurement, which in turn may
impact adaptation effectiveness. Figure 2a shows
how varying the background size affects adap-
tation effectiveness for sarcasm. In general,
larger backgrounds lead to bigger gains as ex-
pected. Even with small amounts of background
(50 tweets) adaptation can provide gains, suggest-
ing that even with imperfect predictions of the user
attributes, there is still some benefit to adaptation.

Figure 2b compares how performance varies
with the number of latent factors for sarcasm. We
see gains for all d sizes we explored. Perfor-
mance improves with d first and then tapers off; its
best is +3.4 at d=5 and 7. As the number of fac-
tors increases, there is greater potential for a fine-
grained characterization of language use differ-
ences. However, this is offset by the increased risk
of overwhelming the learner with too many param-
eters to learn during adaptation. We also find that
the impact of number of factors also varies with
the type of task (e.g., for PP-attachment we find
d=3 gives the best performance of 72.2, a +1.2
gain over the baseline).

1151



(a) Background size effects for cases with large adaptation
gains: sarcasm when using personality and user-embedding
factors.

(b) Gains over unadapted baseline for sarcasm using TF-IDF,
user embeddings, and word2vec with varying number of fac-
tors.

Figure 2: Adaptation performance compared against background size and number of factors.

5.4 Factors as Direct Features
One way to use the factors is to add them as di-
rect features to the instances, without adaptation.
Table 4 compares how the most beneficial known
factor, personality, performs when added directly
as a feature to the two tasks where it had the high-
est impact.

task base direct adapt best
sarcasm 73.9 75.6† 75.6† 77.3†
stance 64.9 65.5 67.7† 67.9†

Table 4: Effects of including personality scores as
direct features, rather than doing adaptation. Other
tasks had no benefit from direct features. Best col-
umn shows best result achieved with adaptation
using any factor.

For sarcasm, adding personality as a direct fea-
ture itself leads to a strong improvement on par
with using it for adaptation. For stance, however,
we see that while there is an improvement over
the baseline, it is not as large as that from adap-
tation. We observed little-to-no improvement for
POS tagging, sentiment or PP-attachment when
using personality as direct features. This reflects
the relative complexity of the relationships be-
tween user factors and class labels for each task.
Note that while direct features provide benefits,
the overall possible gain with adaptation using any
factor (shown in best column) is larger.

Including user factors as direct features is ben-
eficial when there is a linear relationship with
the class label, such as with gender and sarcasm
use. In contrast, user-factor adaptation can capture
more complex relationships between user groups
and their language expression. Figure 3, for in-

Figure 3: There is a positive correlation between
gender and adjective use for sarcastic tweets, and
a negative correlation for non-sarcastic tweets.
Higher gender scores indicate a greater degree
of “femaleness”, whereas lower scores represent
more “maleness” according to the gender predic-
tion model.

stance, shows a three-way interaction between
gender scores, adjective use and sarcasm. Increase
in the number of adjectives is a positive indicator
of sarcasm for women (high gender scores) but is
a negative indicator for men (low gender scores).
We observe similar trends for age: phrases such as
“thanks” and “love it” tend to be meant sarcasti-
cally by younger users and sincerely by older ones.
User-factor adaptation can model these interaction
relationships when direct features alone may not.

5.5 Comparison of Latent Representations

To understand the advantage of continuous latent
adaptation, we look at how well discrete and con-
tinuous factor representations capture meaning-
ful information about users. To do so, we select
two dimensions from the TF-IDF latent factors for
stance detection and examine the extent to which
they differentiate users based on their attributes
(i.e. age) and posting behavior (i.e. URL use).
This is shown in Figure 4. The top row gives the

1152



Figure 4: Kernel densities (top) and scatter plots
(bottom) of users’ age and use of URLs broken
down by TF-IDF latent dimension. Colors rep-
resent each dimension and are consistent across
plots. Lines in scatter plots represent best-fit linear
regression. Shaded regions indicate standard er-
ror. On the left, discrete latent factors do not seem
to distinguish by age (top) but come apart when
treated continuously (bottom). On the other hand,
discrete and continuous seem to partially capture a
dimension of how often someone mentions URLs.

discrete representation: kernel density plots show
age and URL use distributions for users binned
into the two factor dimensions, shown here in red
and blue. The bottom gives the continuous repre-
sentation: scatter plots show the relationship be-
tween age and URL use and the factor score for
each dimension.

In the discrete view, age distributions are similar
for both factors; there is no apparent relationship
between factor membership and age. However, in
the continuous view there is a clear negative corre-
lation for age with the factor score for blue and a
positive one for red. This indicates that the fac-
tors are capturing meaningful information about
user age: those with a high factor score for blue
tend to be younger, whereas those with a low fac-
tor score are older. The reverse is true for red. The
URL use shows some difference between the two
dimensions in the discrete view, and again we see
strong and differing linear relationships with the
continuous view.

Overall, the latent factors appear to capture both
user attributes and posting behavior, with the con-
tinuous version providing additional benefits in
characterizing these relationships. The lack of a
clear differentiation in the discrete case hints at the
difficulty in capturing linguistic variance by split-
ting users into discrete groups.

6 Related Work

Modeling users has a long history of successful
applications in providing personalized informa-
tion access (Dou et al., 2007; Teevan et al., 2005)
and recommendations (Guy et al., 2009; Li et al.,
2010; Morales et al., 2012). In contrast, this work
models users to better understand their content via
language processing tasks following ideas from
demographics-aware and domain adaptation.

User-level language variance affects lexical
choices (Preoţiuc-Pietro et al., 2016) and
even syntactic aspects of language (Johannsen
et al., 2015). Such variations can result in
demographics-based bias in low-level tasks such
as POS tagging (Hovy and Søgaard, 2015) and can
also impact high-level applications such as senti-
ment analysis (Volkova et al., 2013) and machine
translation (Mirkin et al., 2015), motivating demo-
graphics and personality-based adaptations.

Consequently, recent works have explored
demographics-aware NLP (Volkova et al., 2013;
Bamman et al., 2014a; Kulkarni et al., 2016;
Hovy, 2015; Yang and Eisenstein, 2015). Volkova
et al. (2013) propose a gender-aware model and
demonstrate superior performance over a gender-
agnostic model on the task of sentiment ana-
lysis. Bamman et al. (2014a) and Kulkarni et al.
(2016) analyze regional linguistic variation using
region-specific word embeddings on online so-
cial media. Hovy (2015) advances this line of
research further and learns group-specific word
embeddings, showing improvements over general
embeddings on three types of text classification
tasks. When author demographics are not avail-
able, Yang and Eisenstein (2015) show that learn-
ing community-specific embeddings using social
networks can help improve sentiment analysis. A
similar approach with a social theory-based opti-
mization also showed improvements for sentiment
analysis (Hu et al., 2013). For sarcasm detection,
historical information about the author and their
past context (e.g. entities they discuss) have been
shown to be helpful (Bamman and Smith, 2015;
Khattri et al., 2015; Rajadesingan et al., 2015).

Our work builds on these ideas and explores
the general task of user-factor adaptation. Com-
pared to past work, our method (a) is more gen-
eral – working with both continuous and discrete
factors, (b) uses factors beyond demographics –
characteristics like personality are known to in-
fluence language beyond demographics (Schwartz

1153



et al., 2013), and (c) only requires a background of
language – by using inferred factors from a back-
ground of language, we require no a priori knowl-
edge of user traits.

7 Conclusion

Language on social media reflects the diversity in
its user base and motivates the need for robust
models that can handle the resulting variations by
user attributes. We have introduced user-factor
adaptation, a method to adapt typical supervised
language classifiers based on factors of the user
authoring the language. Our approach requires
nothing more than a background of language by
the user and only needs access to the features used
by the supervised learner. Since it requires no
other modifications to the learner, our approach
can be easily applied to many NLP tasks.

To the best of our knowledge, this represents
the first work to use the idea of continuous-
valued variables for language processing adapta-
tion. Continuous adaptation to a variety of user
factors brings us closer to personalized NLP and
outperforms discrete adaptation over four different
tasks: part-of-speech tagging, preposition-phrase
attachment, sentiment analysis, and sarcasm de-
tection. Adaptations with data-driven latent fac-
tors produced the largest gains. We see this work
as part of a growing trend to put language not just
within its document-wide context, but also within
the context of the human being that wrote it.

Acknowledgments

This publication was made possible, in part,
through the support of a grant from the Temple-
ton Religion Trust – TRT0048. We wish to thank
the following colleagues for their annotation help
for the PP-attachment task: Chetan Naik, Heey-
oung Kwon, Ibrahim Hammoud, Jun Kang, Ma-
soud Rouhizadeh, Mohammadzaman Zamani, and
Samuel Louvan.

References
Shlomo Argamon, Moshe Koppel, James W. Pen-

nebaker, and Jonathan Schler. 2007. Mining the
blogosphere: Age, gender and the varieties of self-
expression. First Monday, 12(9).

David Bamman, Chris Dyer, and Noah A. Smith.
2014a. Distributed representations of geographi-
cally situated language. Proceedings of ACL, pages
828–834.

David Bamman, Jacob Eisenstein, and Tyler Schnoe-
belen. 2014b. Gender identity and lexical varia-
tion in social media. Journal of Sociolinguistics,
18(2):135–160.

David Bamman and Noah A Smith. 2015. Contextual-
ized sarcasm detection on Twitter. In Ninth Interna-
tional AAAI Conference on Web and Social Media.

Yonatan Belinkov, Tao Lei, Regina Barzilay, and Amir
Globerson. 2014. Exploring compositional architec-
tures and word vector representations for preposi-
tional phrase attachment. TACL, 2:561–572.

Jenny Cheshire. 2005. Syntactic variation and be-
yond: Gender and social class variation in the use
of discourse-new markers. Journal of Sociolinguis-
tics, 9(4):479–508.

Dennis Child. 1990. The Essentials of Factor Analysis.
Cassell Educational.

Hal Daumé III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL.

Zhicheng Dou, Ruihua Song, and Ji-Rong Wen. 2007.
A large-scale evaluation and analysis of personal-
ized search strategies. In Proceedings of WWW.

Maeve Duggan and Aaron Smith. 2013. Demographics
of key social networking platforms. Pew Research
on Social Media.

Lewis R. Goldberg. 1990. An alternative “descrip-
tion of personality”: The Big-Five factor struc-
ture. Journal of Personality and Social Psychology,
59(6):1216.

Ido Guy, Naama Zwerdling, David Carmel, Inbal Ro-
nen, Erel Uziel, Sivan Yogev, and Shila Ofek-
Koifman. 2009. Personalized recommendation of
social software items based on social relations. In
Proceedings of RecSys.

Dirk Hovy. 2015. Demographic factors improve clas-
sification performance. In Proceedings of ACL.

Dirk Hovy and Anders Søgaard. 2015. Tagging perfor-
mance correlates with author age. In Proceedings of
ACL.

Xia Hu, Lei Tang, Jiliang Tang, and Huan Liu. 2013.
Exploiting social relations for sentiment analysis in
microblogging. In Proceedings of WSDM.

Thorsten Joachims. 2006. Training linear SVMs in lin-
ear time. In Proceedings of KDD, pages 217–226.
ACM.

Anders Johannsen, Dirk Hovy, and Anders Søgaard.
2015. Cross-lingual syntactic variation over age and
gender. In Proceedings of CONLL.

Anupam Khattri, Aditya Joshi, Pushpak Bhat-
tacharyya, and Mark James Carman. 2015. Your
sentiment precedes you: Using an author’s histor-
ical tweets to predict sarcasm. In Sixth Workshop

1154



on Computational Approaches to Subjectivity, Senti-
ment and Social Media Analysis (WASSA), page 25.

Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A. Smith. 2014. A dependency parser for
tweets. In Proceedings of EMNLP.

Vivek Kulkarni, Bryan Perozzi, and Steven Skiena.
2016. Freshman or fresher? Quantifying the geo-
graphic variation of language in online social media.
In Tenth International AAAI Conference on Web and
Social Media.

Derrick Norman Lawley and Albert Ernest Maxwell.
1971. Factor analysis as a statistical method, vol-
ume 18. JSTOR.

Lihong Li, Wei Chu, John Langford, and Robert E.
Schapire. 2010. A contextual-bandit approach to
personalized news article recommendation. In Pro-
ceedings of WWW.

Robert R. McCrae and Paul T. Costa Jr. 1989. Rein-
terpreting the Myers-Briggs type indicator from the
perspective of the five-factor model of personality.
Journal of Personality, 57(1):17–40.

Robert R. McCrae and Paul T. Costa Jr. 1997. Person-
ality trait structure as a human universal. American
Psychologist, 52(5):509.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proceedings of NIPS.

Shachar Mirkin, Scott Nowson, Caroline Brun, and
Julien Perez. 2015. Motivating personality-aware
machine translation. In Proceedings of EMNLP.

Saif M. Mohammad, Svetlana Kiritchenko, Parinaz
Sobhani, Xiaodan Zhu, and Colin Cherry. 2016.
SemEval-2016 task 6: Detecting stance in tweets.
In Proceedings of SemEval-2016, volume 16.

Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the state-of-
the-art in sentiment analysis of tweets. In Proceed-
ings of SemEval-2013.

Gianmarco De Francisci Morales, Aristides Gionis,
and Claudio Lucchese. 2012. From chatter to head-
lines: Harnessing the real-time web for personalized
news recommendation. In Proceedings of WSDM.

Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara
Rosenthal, Veselin Stoyanov, and Theresa Wilson.
2013. SemEval-2013 task 2: Sentiment analysis in
Twitter.

Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT.

Gregory Park, H. Andrew Schwartz, Johannes C. Eich-
staedt, Margaret L. Kern, Michal Kosinski, David J.
Stillwell, Lyle H. Ungar, and Martin E. P. Seligman.
2015. Automatic personality assessment through
social media language. Journal of Personality and
Social Psychology, 108(6):934.

James W. Pennebaker and Lori D. Stone. 2003. Words
of wisdom: Language use over the life span. Jour-
nal of Personality and Social Psychology, 85(2):291.

Daniel Preoţiuc-Pietro, Wei Xu, and Lyle Ungar. 2016.
Discovering user attribute stylistic differences via
paraphrasing. In Proceedings of AAAI.

Ashwin Rajadesingan, Reza Zafarani, and Huan Liu.
2015. Sarcasm detection on twitter: A behavioral
modeling approach. In Proceedings of WSDM.

John Ruscio and Ayelet Meron Ruscio. 2000. Inform-
ing the continuity controversy: A taxometric ana-
lysis of depression. Journal of Abnormal Psychol-
ogy, 109(3):473–487.

Maarten Sap, Gregory J. Park, Johannes C. Eichstaedt,
Margaret L. Kern, David Stillwell, Michal Kosinski,
Lyle H. Ungar, and Hansen Andrew Schwartz. 2014.
Developing age and gender predictive lexica over so-
cial media. In Proceedings of EMNLP.

H. Andrew Schwartz, Johannes C. Eichstaedt, Mar-
garet L. Kern, Lukasz Dziurzynski, Stephanie M.
Ramones, Megha Agrawal, Achal Shah, Michal
Kosinski, David Stillwell, Martin E. P. Seligman,
and Lyle H. Ungar. 2013. Personality, gender, and
age in the language of social media: The open-
vocabulary approach. PloS ONE, 8(9):e73791.

Jaime Teevan, Susan T. Dumais, and Eric Horvitz.
2005. Personalizing search via automated analysis
of interests and activities. In SIGIR.

Svitlana Volkova, Theresa Wilson, and David
Yarowsky. 2013. Exploring demographic lan-
guage variations to improve multilingual sentiment
analysis in social media. In Proceedings of EMNLP.

Thomas A. Widiger and Douglas B. Samuel. 2005.
Diagnostic categories or dimensions? A question
for the Diagnostic and Statistical Manual of Men-
tal Disorders—Fifth Edition. Journal of Abnormal
Psychology, 114(4):494.

Yi Yang and Jacob Eisenstein. 2015. Putting
things in context: Community-specific embed-
ding projections for sentiment analysis. CoRR,
abs/1511.06052.

1155


