



















































A Type-Driven Tensor-Based Semantics for CCG


Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 46–54,
Gothenburg, Sweden, April 26-30 2014. c©2014 Association for Computational Linguistics

A Type-Driven Tensor-Based Semantics for CCG

Jean Maillard
University of Cambridge

Computer Laboratory
jm864@cam.ac.uk

Stephen Clark
University of Cambridge

Computer Laboratory
sc609@cam.ac.uk

Edward Grefenstette
University of Oxford

Department of Computer Science
edward.grefenstette@cs.ox.ac.uk

Abstract

This paper shows how the tensor-based se-
mantic framework of Coecke et al. can
be seamlessly integrated with Combina-
tory Categorial Grammar (CCG). The inte-
gration follows from the observation that
tensors are linear maps, and hence can
be manipulated using the combinators of
CCG, including type-raising and compo-
sition. Given the existence of robust,
wide-coverage CCG parsers, this opens up
the possibility of a practical, type-driven
compositional semantics based on distri-
butional representations.

1 Intoduction

In this paper we show how tensor-based distribu-
tional semantics can be seamlessly integrated with
Combinatory Categorial Grammar (CCG, Steed-
man (2000)), building on the theoretical discus-
sion in Grefenstette (2013). Tensor-based distribu-
tional semantics represents the meanings of words
with particular syntactic types as tensors whose se-
mantic type matches that of the syntactic type (Co-
ecke et al., 2010). For example, the meaning of a
transitive verb with syntactic type (S\NP)/NP is
a 3rd-order tensor from the tensor product space
N ⊗ S ⊗ N . The seamless integration with CCG
arises from the (somewhat trivial) observation that
tensors are linear maps — a particular kind of
function — and hence can be manipulated using
CCG’s combinatory rules.

Tensor-based semantics arises from the desire to
enhance distributional semantics with some com-
positional structure, in order to make distribu-
tional semantics more of a complete semantic the-
ory, and to increase its utility in NLP applica-
tions. There are a number of suggestions for how
to add compositionality to a distributional seman-
tics (Clarke, 2012; Pulman, 2013; Erk, 2012).

One approach is to assume that the meanings of
all words are represented by context vectors, and
then combine those vectors using some operation,
such as vector addition, element-wise multiplica-
tion, or tensor product (Clark and Pulman, 2007;
Mitchell and Lapata, 2008). A more sophisticated
approach, which is the subject of this paper, is to
adapt the compositional process from formal se-
mantics (Dowty et al., 1981) and attempt to build
a distributional representation in step with the syn-
tactic derivation (Coecke et al., 2010; Baroni et al.,
2013). Finally, there is a third approach using neu-
ral networks, which perhaps lies in between the
two described above (Socher et al., 2010; Socher
et al., 2012). Here compositional distributed rep-
resentations are built using matrices operating on
vectors, with all parameters learnt through a su-
pervised learning procedure intended to optimise
performance on some NLP task, such as syntac-
tic parsing or sentiment analysis. The approach
of Hermann and Blunsom (2013) conditions the
vector combination operation on the syntactic type
of the combinands, moving it a little closer to the
more formal semantics-inspired approaches.

The remainder of the Introduction gives a short
summary of distributional semantics. The rest of
the paper introduces some mathematical notation
from multi-linear algebra, including Einstein nota-
tion, and then shows how the combinatory rules of
CCG, including type-raising and composition, can
be applied directly to tensor-based semantic rep-
resentations. As well as describing a tensor-based
semantics for CCG, a further goal of this paper is to
present the compositional framework of Coecke et
al. (2010), which is based on category theory, to a
computational linguistics audience using only the
mathematics of multi-linear algebra.

1.1 Distributional Semantics

We assume a basic knowledge of distributional se-
mantics (Grefenstette, 1994; Schütze, 1998). Re-

46



cent inroductions to the topic include Turney and
Pantel (2010) and Clark (2014).

A potentially useful distinction for this paper,
and one not commonly made, is between distri-
butional and distributed representations. Distri-
butional representations are inherently contextual,
and rely on the frequently quoted dictum from
Firth that “you shall know a word from the com-
pany it keeps” (Firth, 1957; Pulman, 2013). This
leads to the so-called distributional hypothesis that
words that occur in similar contexts tend to have
similar meanings, and to various proposals for
how to implement this hypothesis (Curran, 2004),
including alternative definitions of context; alter-
native weighting schemes which emphasize the
importance of some contexts over others; alterna-
tive similarity measures; and various dimension-
ality reduction schemes such as the well-known
LSA technique (Landauer and Dumais, 1997). An
interesting conceptual question is whether a sim-
ilar distributional hypothesis can be applied to
phrases and larger units: is it the case that sen-
tences, for example, have similar meanings if they
occur in similar contexts? Work which does ex-
tend the distributional hypothesis to larger units
includes Baroni and Zamparelli (2010), Clarke
(2012), and Baroni et al. (2013).

Distributed representations, on the other hand,
can be thought of simply as vectors (or possibly
higher-order tensors) of real numbers, where there
is no a priori interpretation of the basis vectors.
Neural networks can perhaps be categorised in this
way, since the resulting vector representations are
simply sequences of real numbers resulting from
the optimisation of some training criterion on a
training set (Collobert and Weston, 2008; Socher
et al., 2010). Whether these distributed represen-
tations can be given a contextual interpretation de-
pends on how they are trained.

One important point for this paper is that the
tensor-based compositional process makes no as-
sumptions about the interpretation of the tensors.
Hence in the remainder of the paper we make no
reference to how noun vectors or verb tensors,
for example, can be acquired (which, for the case
of the higher-order tensors, is a wide open re-
search question). However, in order to help the
reader who would prefer a more grounded dis-
cussion, one possibility is to obtain the noun vec-
tors using standard distributional techniques (Cur-
ran, 2004), and learn the higher-order tensors us-

ing recent techniques from “recursive” neural net-
works (Socher et al., 2010). Another possibility
is suggested by Grefenstette et al. (2013), extend-
ing the learning technique based on linear regres-
sion from Baroni and Zamparelli (2010) in which
“gold-standard” distributional representations are
assumed to be available for some phrases and
larger units.

2 Mathematical Preliminaries

The tensor-based compositional process relies on
taking dot (or inner) products between vectors and
higher-order tensors. Dot products, and a number
of other operations on vectors and tensors, can be
conveniently written using Einstein notation (also
referred to as the Einstein summation convention).
In the rest of the paper we assume that the vector
spaces are over the field of real numbers.

2.1 Einstein Notation
The squared amplitude of a vector v ∈ Rn is given
by:

|v|2 =
n∑

i=1

vivi

Similarly, the dot product of two vectors v,w ∈
Rn is given by:

v ·w =
n∑

i=1

viwi

Denote the components of an m×n real matrix
A by Aij for 1 ≤ i ≤ m and 1 ≤ j ≤ n. Then
the matrix-vector product of A and v ∈ Rn gives
a vector Av ∈ Rm with components:

(Av)i =
n∑

j=1

Aijvj

We can also multiply an n×mmatrix A and an
m × o matrix B to produce an n × o matrix AB
with components:

(AB)ij =
m∑

k=1

AikBkj

The previous examples are some of the most
common operations in linear algebra, and they all
involve sums over repeated indices. They can be
simplified by introducing the Einstein summation
convention: summation over the relevant range
is implied on every component index that occurs

47



twice. Pairs of indices that are summed over are
known as contracted, while the remaining indices
are known as free. Using this convention, the
above operations can be written as:

|v|2 = vivi
v ·w = viwi
(Av)i = Aijvj , i.e. the contraction of v with
the second index of A

(AB)ij = AikBkj , i.e. the contraction of the
second index of A with the first of B

Note how the number of free indices is always
conserved between the left- and right-hand sides in
these examples. For instance, while the last equa-
tion has two indices on the left and four on the
right, the two extra indices on the right are con-
tracted. Hence counting the number of free indices
can be a quick way of determining what type of
object is given by a certain mathematical expres-
sion in Einstein notation: no free indices means
that an operation yields a scalar number, one free
index means a vector, two a matrix, and so on.

2.2 Tensors
Linear Functionals Given a finite-dimensional
vector space Rn over R, a linear functional is a
linear map a : Rn → R.

Let a vector v have components vi in a fixed ba-
sis. Then the result of applying a linear functional
a to v can be written as:

a(v) = a1v1+· · ·+anvn =
(
a1 · · · an

) v1...
vn


The numbers ai are the components of the lin-

ear functional, which can also be pictured as a row
vector. Since there is a one-to-one correspondence
between row and column vectors, the above equa-
tion is equivalent to:

v(a) = a1v1+· · ·+anvn =
(
v1 · · · vn

) a1...
an


Using Einstein convention, the equations above

can be written as:

a(v) = viai = v(a)

Thus every finite-dimensional vector is a linear
functional, and vice versa. Row and column vec-
tors are examples of first-order tensors.

Definition 1 (First-order tensor). Given a vector
space V over the field R, a first-order tensor T
can be defined as:

• an element of the vector space V ,
• a linear map T : V → R,
• a |V |-dimensional array of numbers Ti, for

1 ≤ i ≤ |V |.
These three definitions are all equivalent. Given

a first-order tensor described using one of these
definitions, it is trivial to find the two other de-
scriptions.

Matrices An n×mmatrix A over R can be rep-
resented by a two-dimensional array of real num-
bers Aij , for 1 ≤ i ≤ n and 1 ≤ j ≤ m.

Via matrix-vector multiplication, the matrix A
can be seen as a linear map A : Rm → Rn. It
maps a vector v ∈ Rm to a vectorA11 · · · A1m... . . . ...

An1 · · · Anm


 v1...
vm

 ,
with components

A(v)i = Aijvj .

We can also contract a vector with the first index
of the matrix, which gives us a map A : Rn →
Rm. This corresponds to the operation

(
w1 · · · wn

)A11 · · · A1m... . . . ...
An1 · · · Anm

 ,
resulting in a vector with components

(wTA)i = Ajiwj .

We can combine the two operations and see a
matrix as a map A : Rn × Rm → R, defined by:

wTAv =
(
w1 · · · wn

)A11 · · · A1m... . . . ...
An1 · · · Anm


 v1...
vm


In Einstein notation, this operation can be writ-

ten as
wiAijvj ,

48



which yields a scalar (constant) value, consistent
with the fact that all the indices are contracted.

Finally, matrices can also be characterised in
terms of Kronecker products. Given two vectors
v ∈ Rn and w ∈ Rm, their Kronecker product
v ⊗w is a matrix

v ⊗w =

v1w1 · · · v1wm... . . . ...
vnw1 · · · vnwm

 ,
with components

(v ⊗w)ij = viwj .

It is a general result in linear algebra that any
n × m matrix can be written as a finite sum of
Kronecker products

∑
k x

(k) ⊗ y(k) of a set of
vectors x(k) and y(k). Note that the sum over k
is written explicitly as it would not be implied by
Einstein notation: this is because the index k does
not range over vector/matrix/tensor components,
but over a set of vectors, and hence that index ap-
pears in brackets.

An n × m matrix is an element of the tensor
space Rn ⊗Rm, and it can also be seen as a linear
map A : Rn ⊗ Rm → R. This is because, given
a matrix B with decomposition

∑
k x

(k) ⊗ y(k),
the matrix A can act as follows:

A(B) = Aij
∑

k

x
(k)
i y

(k)
j

=
∑

k

(
x

(k)
1 · · · x(k)n

)A11 · · · A1m... . . . ...
An1 · · · Anm


y

(k)

...
y
(k)
m


= AijBij .

Again, counting the number of free indices in the
last line tells us that this operation yields a scalar.

Matrices are examples of second-order tensors.

Definition 2 (Second-order tensor). Given vector
spaces V,W over the field R, a second-order ten-
sor T can be defined as:

• an element of the vector space V ⊗W ,
• a |V | × |W |-dimensional array of numbers
Tij , for 1 ≤ i ≤ |V | and 1 ≤ j ≤ |W |,
• a (multi-) linear map:

– T : V →W ,
– T : W → V ,

– T : V ×W → R or T : V ⊗W → R.

Again, these definitions are all equivalent. Most
importantly, the four types of maps given in the
definition are isomorphic. Therefore specifying
one map is enough to specify all the others.

Tensors We can generalise these definitions to
the more general concept of tensor.

Definition 3 (Tensor). Given vector spaces
V1, . . . , Vk over the field R, a kth-order tensor T
is defined as:

• an element of the vector space V1⊗ · · ·⊗Vk,

• a |V1| × · · · × |Vk|, kth-dimensional array of
numbers Ti1···ik , for 1 ≤ ij ≤ |Vj |,

• a multi-linear map T : V1 × · · · × Vk → R.

3 Tensor-Based CCG Semantics

In this section we show how CCG’s syntactic types
can be given tensor-based meaning spaces, and
how the combinator’s employed by CCG to com-
bine syntactic categories carry over to those mean-
ing spaces, maintaining what is often described
as CCG’s “transparent interface” between syntax
and semantics. Here are some example syntactic
types, and the corresponding tensor spaces con-
taining the meanings of the words with those types
(using the notation syntactic type : semantic type).

We first assume that all atomic types have
meanings living in distinct vector spaces:

• noun phrases, NP : N
• sentences, S : S

The recipe for determining the meaning space
of a complex syntactic type is to replace each
atomic type with its corresponding vector space
and the slashes with tensor product operators:

• Intransitive verb, S\NP : S⊗ N
• Transitive verb, (S\NP)/NP : S⊗ N⊗ N
• Ditransitive verb, ((S\NP)/NP)/NP :

S⊗ N⊗ N⊗ N
• Adverbial modifier, (S\NP)\(S\NP) :

S⊗ N⊗ S⊗ N
• Preposition modifying NP , (NP\NP)/NP :

N⊗ N⊗ N

49



Hence the meaning of an intransitive verb, for
example, is a matrix in the tensor product space
S ⊗ N. The meaning of a transitive verb is a
“cuboid”, or 3rd-order tensor, in the tensor product
space S⊗N⊗N. In the same way that the syntac-
tic type of an intransitive verb can be thought of as
a function — taking an NP and returning an S —
the meaning of an intransitive verb is also a func-
tion (linear map) — taking a vector in N and re-
turning a vector in S. Another way to think of this
function is that each element of the matrix spec-
ifies, for a pair of basis vectors (one from N and
one from S), what the result is on the S basis vec-
tor given a value on the N basis vector.

Now we describe how the combinatory rules
carry over to the meaning spaces.

3.1 Application
The function application rules of CCG are forward
(>) and backward (<) application:

X/Y Y =⇒ X (>)
Y X\Y =⇒ X (<)

In a traditional semantics for CCG, if function
application is applied in the syntax, then function
application applies also in the semantics (Steed-
man, 2000). This is also true of the tensor-based
semantics. For example, the meaning of a subject
NP combines with the meaning of an intransitive
verb via matrix multiplication, which is equivalent
to applying the linear map corresponding to the
matrix to the vector representing the meaning of
the NP . Applying (multi-)linear maps in (multi-
)linear algebra is equivalent to applying tensor
contraction to the combining tensors. Here is the
case for an intransitive verb:

Pat walks
NP S\NP
N S⊗ N

Let Pat be assigned a vector P ∈ N and walks
be assigned a second-order tensor W ∈ S ⊗ N.
Using the backward application combinator cor-
responds to feeding P , an element of N, into W ,
seen as a function N→ S. In terms of tensor con-
traction, this is the following operation:

WijPj .

Here we use the convention that the indices
maintain the same order as the syntactic type.
Therefore, in the tensor of an object of type X/Y ,

the first index corresponds to the type X and the
second to the type Y . That is why, when perform-
ing the contraction corresponding to Pat walks,
P ∈ N is contracted with the second index of
W ∈ S ⊗ N, and not the first.1 The first index
of W is then the only free index, telling us that the
above operation yields a first-order tensor (vector).
Since this index corresponds to S, we know that
applying backward application to Pat walks yields
a meaning vector in S.

Forward application is performed in the same
manner. Consider the following example:

Pat kisses Sandy
NP (S\NP)/NP NP
N S⊗ N⊗ N N

with corresponding tensors P ∈ N for Pat, K ∈
S⊗ N⊗ N for kisses and Y ∈ N for Sandy.

The forward application deriving the type of
kisses Sandy corresponds to

KijkYk,

where Y is contracted with the third index of K
because we have maintained the order defined by
the type (S\NP)/NP : the third index then corre-
sponds to an argument NP coming from the right.

Counting the number of free indices in the
above expression tells us that it yields a second-
order tensor. Looking at the types corresponding
to the free indices tells us that this second-order
tensor is of type S⊗N, which is the semantic type
of a verb phrase (or intransitive verb), as we have
already seen in the walks example.

3.2 Composition
The forward (>B) and backward (<B) composi-
tion rules are:

X/Y Y/Z =⇒ X/Z (>B)
Y \Z X\Y =⇒ X\Z (<B)

Composition in the semantics also reduces to a
form of tensor contraction. Consider the following
example, in which might can combine with kiss
using forward composition:

Pat might kiss Sandy
NP (S\NP)/(S\NP) (S\NP)/NP NP
N S⊗ N⊗ S⊗ N S⊗ N⊗ N N

1The particular order of the indices is not important, as
long as a convention such as this one is decided upon and
consistently applied to all types (so that tensor contraction
contracts the relevant tensors from each side when a combi-
nator is used).

50



with tensors M ∈ S ⊗ N ⊗ S ⊗ N for might and
K ∈ S⊗N⊗N for kiss. Combining the meanings
of might and kiss corresponds to the following op-
eration:

MijklKklm,

yielding a tensor in S ⊗ N ⊗ N, which is the
correct semantic type for a phrase with syntactic
type (S\NP)/NP . Backward composition is per-
formed analogously.

3.3 Backward-Crossed Composition
English also requires the use of backward-crossed
composition (Steedman, 2000):

X/Y Z\X =⇒ Z/Y (<B×)
In tensor terms, this is the same as forward com-

position; we just need to make sure that the con-
traction matches up the correct parts of each ten-
sor correctly. Consider the following backward-
crossed composition:

(S\NP)/NP (S\NP)\(S\NP) ⇒<B× (S\NP)/NP
Let the two items on the left-hand side be rep-
resented by tensors A ∈ S ⊗ N ⊗ N and B ∈
S ⊗ N ⊗ S ⊗ N. Then, combining them with
backward-crossed composition in tensor terms is

BijklAklm,

resulting in a tensor in S ⊗ N ⊗ N (correspond-
ing to the indices i, j and m). Note that we have
reversed the order of tensors in the contraction to
make the matching of the indices more transpar-
ent; however, tensor contraction is commutative
(since it corresponds to a sum over products) so
the order of the tensors does not affect the result.

3.4 Type-raising
The forward (>T) and backward (<T) type-
raising rules are:

X =⇒ T/(T\X) (>T)
X =⇒ T\(T/X) (<T)

where T is a variable ranging over categories.
Suppose we are given an item of atomic type Y ,

with corresponding vector A ∈ Y. If we apply
forward type-raising to it, we get a new tensor of
type A′ ∈ T ⊗ T ⊗ Y. Now suppose the item of
type Y is followed by another item of type X\Y ,
with tensor B ∈ X ⊗ Y. A phrase consisting of
two words with types Y and X\Y can be parsed
in two different ways:

• Y X\Y ⇒ X , by backward application;
• Y X\Y ⇒T X/(X\Y ) X\Y , by forward

type-raising, and X/(X\Y ) X\Y ⇒ X , by
forward application.

Both ways of parsing this sentence yield an item
of type X , and crucially the meaning of the result-
ing item should be the same in both cases.2 This
property of type-raising provides an avenue into
determining what the tensor representation for the
type-raised category should be, since the tensor
representations must also be the same:

AjBij = A′ijkBjk.

Moreover, this equation must hold for all items,
B. As a concrete example, the requirement says
that a subject NP combining with a verb phrase
S\NP must produce the same meaning for the
two alternative derivations, irrespective of the verb
phrase. This is equivalent to the requirement that

AjBij = A′ijkBjk, ∀B ∈ X⊗ Y.

So to arrive at the tensor representation, we sim-
ply have to solve the tensor equation above. We
start by renaming the dummy index j on the left-
hand side:

AkBik = A′ijkBjk.

We then insert a Kronecker delta (δij = 1 if i = j
and 0 otherwise):

AkBjkδij = A′ijkBjk.

Since the equation holds for allB, we are left with

A′ijk = δijAk,

which gives us a recipe for performing type-
raising in a tensor-based model. The recipe is par-
ticularly simple and elegant: it corresponds to in-
serting the vector being type-raised into the 3rd-
order tensor at all places where the first two in-
dices are equal (with the rest of the elements in
the 3rd-order tensor being zero). For example, to
type-raise a subject NP , its meaning vector in N is
placed in the 3rd-order tensor S⊗S⊗N at all places
where the indices of the two S dimensions are the
same. Visually, the 3rd-order tensor correspond-
ing to the meaning of the type-raised category is

2This property of CCG resulting from the use of type-
raising and composition is sometimes referred to as “spurious
ambiguity”.

51



a cubiod in which the noun vector is repeated a
number of times (once for each sentence index),
resulting in a series of “steps” progressing diag-
onally from the bottom of the cuboid to the top
(assuming a particular orientation).

The discussion so far has been somewhat ab-
stract, so to finish this section we include some
more examples with CCG categories, and show
that the tensor contraction operation has an intu-
itive similarity with the “cancellation law” of cat-
egorial grammar which applies in the syntax.

First consider the example of a subject NP
with meaning A, combining with a verb phrase
S\NP with meaning B, resulting in a sentence
with meaning C. In the syntax, the two NPs can-
cel. In the semantics, for each basis of the sentence
space S we perform an inner product between two
vectors in N:

Ci = AjBij

Hence, inner products in the tensor space corre-
spond to cancellation in the syntax.

This correspondence extends to complex argu-
ments, and also to composition. Consider the sub-
ject type-raising case, in which a subject NP with
meaning A in S ⊗ S ⊗ N combines with a verb
phrase S\NP with meaning B, resulting in a sen-
tence with meaning C. Again we perform inner
product operations, but this time the inner product
is between two matrices:3

Ci = AijkBjk

Note that two matrices are “cancelled” for each
basis vector of the sentence space (i.e. for each
index i in Ci).

As a final example, consider the forward com-
position from earlier, in which a modal verb with
meaningA in S⊗N⊗S⊗N combines with a tran-
sitive verb with meaning B in S ⊗ N ⊗ N to give
a transitive verb with meaning C in S ⊗ N ⊗ N.
Again the cancellation in the syntax corresponds
to inner products between matrices, but this time
we need an inner product for each combination of
3 indices:

Cijk = AijlmBlmk
3To be more precise, the two matrices can be thought of

as vectors in the tensor space S ⊗ N and the inner product is
between these vectors. Another way to think of this opera-
tion is to “linearize” the two matrices into vectors and then
perform the inner product on these vectors.

For each i, j, k, two matrices — corresponding to
the l,m indices above — are “cancelled”.

This intuitive explanation extends to arguments
with any number of slashes. For example, a
composition where the cancelling categories are
(N /N )/(N /N ) would require inner products be-
tween 4th-order tensors in N⊗ N⊗ N⊗ N.

4 Related Work

The tensor-based semantics presented in this pa-
per is effectively an extension of the Coecke et al.
(2010) framework to CCG, re-expressing in Ein-
stein notation the existing categorical CCG exten-
sion in Grefenstette (2013), which itself builds
on an earlier Lambek Grammar extension to the
framework by Coecke et al. (2013).

This work also bears some similarity to the
treatment of categorial grammars presented by Ba-
roni et al. (2013), which it effectively encompasses
by expressing the tensor contractions described by
Baroni et al. as Einstein summations. However,
this paper also covers CCG-specific operations not
discussed by Baroni et al., such as type-raising and
composition.

One difference between this paper and the orig-
inal work by Coecke et al. (2010) is that they use
pregroups as the syntactic formalism (Lambek,
2008), a context-free variant of categorial gram-
mar. In pregroups, cancellation in the syntax is
always between two atomic categories (or more
precisely, between an atomic category and its “ad-
joint”), whereas in CCG the arguments in complex
categories can be complex categories themselves.
To what extent this difference is significant re-
mains to be seen. For example, one area where this
may have an impact is when non-linearities are
added after contractions. Since the CCG contrac-
tions with complex arguments happen “in one go”,
whereas the corresponding pregroup cancellation
in the semantics would be a series of contractions,
many more non-linearities would be added in the
pregroup case.

Krishnamurthy and Mitchell (2013) is based on
a similar insight to this paper – that CCG provides
combinators which can manipulate functions op-
erating over vectors. Krishnamurthy and Mitchell
consider the function application case, whereas we
have shown how the type-raising and composition
operators apply naturally in this setting also.

52



5 Conclusion

This paper provides a theoretical framework for
the development of a compositional distributional
semantics for CCG. Given the existence of ro-
bust, wide-coverage CCG parsers (Clark and Cur-
ran, 2007; Hockenmaier and Steedman, 2002),
together with various techniques for learning the
tensors, the opportunity exists for a practical im-
plementation. However, there are significant engi-
neering difficulties which need to be overcome.

Consider adapting the neural-network learning
techniques of Socher et al. (2012) to this prob-
lem.4 In terms of the number of tensors, the lexi-
con would need to contain a tensor for every word-
category pair; this is at least an order of magnitude
more tensors then the number of matrices learnt in
existing work (Socher et al., 2012; Hermann and
Blunsom, 2013). Furthermore, the order of the
tensors is now higher. Syntactic categories such as
((N /N )/(N /N ))/((N /N )/(N /N )) are not un-
common in the wide-coverage grammar of Hock-
enmaier and Steedman (2007), which in this case
would require an 8th-order tensor. This combina-
tion of many word-category pairs and higher-order
tensors results in a huge number of parameters.

As a solution to this problem, we are investigat-
ing ways to reduce the number of parameters, for
example using tensor decomposition techniques
(Kolda and Bader, 2009). It may also be possi-
ble to reduce the size of some of the complex cat-
egories in the grammar. Many challenges remain
before a type-driven compositional distributional
semantics can be realised, similar to the work of
Bos for the model-theoretic case (Bos et al., 2004;
Bos, 2005), but in this paper we have set out the
theoretical framework for such an implementation.

Finally, we repeat a comment made earlier that
the compositional framework makes no assump-
tions about the underlying vector spaces, or how
they are to be interpreted. On the one hand, this
flexibility is welcome, since it means the frame-
work can encompass many techniques for building
word vectors (and tensors). On the other hand, it
means that a description of the framework is nec-
essarily abstract, and it leaves open the question

4Non-linear transformations are inherent to neural net-
works, whereas the framework in this paper is entirely linear.
However, as hinted at earlier in the paper, non-linear transfor-
mations can be applied to the output of each tensor, turning
the linear networks in this paper into extensions of those in
Socher et al. (2012) (extensions in the sense that the tensors
in Socher et al. (2012) do not extend beyond matrices).

of what the meaning spaces represent. The lat-
ter question is particularly pressing in the case of
the sentence space, and providing an interpretation
of such spaces remains a challenge for the distri-
butional semantics community, as well as relating
distributional semantics to more traditional topics
in semantics such as quantification and inference.

Acknowledgments

Jean Maillard is supported by an EPSRC MPhil
studentship. Stephen Clark is supported by ERC
Starting Grant DisCoTex (306920) and EPSRC
grant EP/I037512/1. Edward Grefenstette is sup-
ported by EPSRC grant EP/I037512/1. We would
like to thank Tamara Polajnar, Laura Rimell, Nal
Kalchbrenner and Karl Moritz Hermann for useful
discussion.

References
M. Baroni and R. Zamparelli. 2010. Nouns

are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-10), Cambridge, MA.

M. Baroni, R. Bernardi, and R. Zamparelli. 2013.
Frege in space: A program for compositional dis-
tributional semantics (to appear). Linguistic Issues
in Language Technologies.

Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of COLING-04, pages 1240–
1246, Geneva, Switzerland.

Johan Bos. 2005. Towards wide-coverage seman-
tic interpretation. In Proceedings of the Sixth In-
ternational Workshop on Computational Semantics
(IWCS-6), pages 42–53, Tilburg, The Netherlands.

Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493–552.

Stephen Clark and Stephen Pulman. 2007. Combining
symbolic and distributional models of meaning. In
Proceedings of AAAI Spring Symposium on Quan-
tum Interaction, Stanford, CA. AAAI Press.

Stephen Clark. 2014. Vector space models of lexical
meaning (to appear). In Shalom Lappin and Chris
Fox, editors, Handbook of Contemporary Semantics
second edition. Wiley-Blackwell.

Daoud Clarke. 2012. A context-theoretic frame-
work for compositionality in distributional seman-
tics. Computational Linguistics, 38(1):41–71.

53



B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Math-
ematical foundations for a compositional distribu-
tional model of meaning. In J. van Bentham,
M. Moortgat, and W. Buszkowski, editors, Linguis-
tic Analysis (Lambek Festschrift), volume 36, pages
345–384.

Bob Coecke, Edward Grefenstette, and Mehrnoosh
Sadrzadeh. 2013. Lambek vs. Lambek: Functorial
vector space semantics and string diagrams for Lam-
bek calculus. Annals of Pure and Applied Logic.

R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In International
Conference on Machine Learning, ICML, Helsinki,
Finland.

James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh.

D.R. Dowty, R.E. Wall, and S. Peters. 1981. Introduc-
tion to Montague Semantics. Dordrecht.

Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: a survey. Language and
Linguistics Compass, 6(10):635–653.

J. R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. In Studies in Linguistic Analysis, pages 1–32.
Oxford: Philological Society.

Edward Grefenstette, Georgiana Dinu, YaoZhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multistep regression learning for composi-
tional distributional semantics. In Proceedings of
the 10th International Conference on Computational
Semantics (IWCS-13), Potsdam, Germany.

Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Kluwer.

Edward Grefenstette. 2013. Category-Theoretic
Quantitative Compositional Distributional Models
of Natural Language Semantics. Ph.D. thesis, Uni-
versity of Oxford.

Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of composi-
tional semantics. Proceedings of ACL, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.

Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with Combi-
natory Categorial Grammar. In Proceedings of the
40th Meeting of the ACL, pages 335–342, Philadel-
phia, PA.

Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355–396.

T. G. Kolda and B. W. Bader. 2009. Tensor decompo-
sitions and applications. SIAM Review, 51(3):455–
500.

Jayant Krishnamurthy and Tom M. Mitchell. 2013.
Vector space semantic parsing: A framework for
compositional vector space models. In Proceed-
ings of the 2013 ACL Workshop on Continuous Vec-
tor Space Models and their Compositionality, Sofia,
Bulgaria.

Joachim Lambek. 2008. From Word to Sentence.
A Computational Algebraic Approach to Grammar.
Polimetrica.

T. K. Landauer and S. T. Dumais. 1997. A solu-
tion to Plato’s problem: the latent semantic analysis
theory of acquisition, induction and representation
of knowledge. Psychological Review, 104(2):211–
240.

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08, pages 236–244, Columbus, OH.

Stephen Pulman. 2013. Distributional semantic mod-
els. In Sadrzadeh Heunen and Grefenstette, editors,
Compositional Methods in Physics and Linguistics.
Oxford University Press.

Hinrich Schütze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
124.

Richard Socher, Christopher D. Manning, and An-
drew Y. Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS Deep
Learning and Unsupervised Feature Learning Work-
shop, Vancouver, Canada.

Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1201–
1211, Jeju, Korea.

Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, MA.

Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.

54


