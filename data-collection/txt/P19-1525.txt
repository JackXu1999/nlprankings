



















































Span-Level Model for Relation Extraction


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5308–5314
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

5308

Span-Level Model for Relation Extraction

Kalpit Dixit
Amazon AWS AI, USA

kddixit@amazon.com

Yaser Al-Onaizan
Amazon AWS AI, USA

onaizan@amazon.com

Abstract

Relation Extraction is the task of identifying
entity mention spans in raw text and then iden-
tifying relations between pairs of the entity
mentions. Recent approaches for this span-
level task have been token-level models which
have inherent limitations. They cannot eas-
ily define and implement span-level features,
cannot model overlapping entity mentions and
have cascading errors due to the use of sequen-
tial decoding. To address these concerns, we
present a model which directly models all pos-
sible spans and performs joint entity mention
detection and relation extraction. We report a
new state-of-the-art performance of 62.83 F1
(prev best was 60.49) on the ACE2005 dataset.

1 Introduction

Many NLP tasks follow the pattern of taking raw
text as input and then: detecting relevant spans
and classifying the relations between those spans.
Examples of this include Relation Extraction (Li
and Ji, 2014), Coreference Resolution (Ng, 2010)
and Semantic Role Labeling (Gildea and Juraf-
sky, 2002). This class of NLP problems are in-
herently span-level tasks. This paper focuses on
Relation Extraction (RE), which is the task of en-
tity mention detection and classifying the relations
between each pair of those mentions. We report
a new state-of-the-art performance of 62.83 F1
(prev best was 60.49) on the ACE2005 dataset.

Here is a simple example of Relation Extrac-
tion for the sentence, ”Washington, D.C. is the
capital of the USA”. Step 1, Entity Mention De-
tection will detect the spans ”Washington, D.C.”
and ”USA” as LOCATIONS. Step 2, Relation
Extraction will classify all directed pairs of de-
tected entity mentions. It will classify the directed
pair (”Washington, D.C.”, ”USA”) as having the
relation IS CAPITAL OF. But the directed pair
(”USA”, ”Washington, D.C.”) will be classified

as having no relation (NONE). In more complex
cases, each entity could participate in multiple dif-
ferent relations.

Since (Li and Ji, 2014), work on RE has re-
volved around end-to-end systems: single models
which first perform entity mention detection and
then relation extraction. These recent works (Bek-
oulis et al., 2018; Katiyar and Cardie, 2017; Miwa
and Bansal, 2016; Li and Ji, 2014) have used se-
quential token-level methods for both the steps.
Token-level models are primarily constrained by
the fact that each token has a single fixed repre-
sentation while each token is a part of many dif-
ferent spans. To model and extract spans, these
token-level models have to resort to approximate
span-level features which are increasingly indirect
and expensive: Tree-LSTMs (Miwa and Bansal,
2016), CRFs (Bekoulis et al., 2018), Beam Search
(Li and Ji, 2014) and Pointer Networks (Katiyar
and Cardie, 2017). Their usage of the BILOU
(Ratinov and Roth, 2009; Florian et al., 2006)
token-tagging scheme makes modelling overlap-
ping entities impossible. In general, these token-
level models are sequential in nature and hence
have cascading errors.

Another end-to-end approach for RE is to use
a simple span-level model. A model which cre-
ates explicit representations for all possible spans,
uses them for the entity mention detection step and
then explicitly compares ordered pairs of spans for
the relation extraction step. Such a model is not
constrained like the token-level models because it
can define direct span-specific features for each
span inexpensively. Since each possible span is
separately considered, selecting overlapping entity
mentions is possible. Predicting one span as an
entity no longer blocks another span from being
predicted as an entity. This approach models each
possible span independently and in parallel i.e. it
is not sequential and does not suffer from cascad-



5309

ing errors. Such models have recently found suc-
cess in similar NLP tasks like Coreference Resolu-
tion (Lee et al., 2017) and Semantic Role Labeling
(Ouchi et al., 2018). In this paper, we present such
a span-level model for Relation Extraction.

We propose a simple bi-LSTM based model
which generates span representations for each pos-
sible span. The span representations are used to
perform entity mention detection on all spans in
parallel. The same span representations are then
used to perform relation extraction on all pairs of
detected entity mentions. We evaluated the per-
formance of our model on the ACE2005 dataset
(Doddington et al., 2004) and report a new start-
of-the-art F1 score of 62.83 for Relation Extrac-
tion.

2 Related Work

Given text input, Relation Extraction involves two
steps: span detection and classification of the rela-
tion between pairs of detected spans. In the RE
literature, these are more commonly called En-
tity Mention Detection and Relation Extraction re-
spectively. An earlier line of research has focused
on only the second step, assuming that the argu-
ments of the relations are given by some other sys-
tem/oracle (Bunescu and Mooney, 2005; Socher
et al., 2012; dos Santos et al., 2015).

The more interesting problem is joint Entity
Mention Detection and Relation Extraction. More
interesting because it simultaneously addresses
both steps, enriches embeddings from losses re-
lated to both sub-tasks and only requires using a
single model during test. Past approaches include
Integer Linear Programming (Yang and Cardie,
2013) and Probabilistic Graphical Models (Singh
et al., 2013). Li and Ji (2014) modeled this joint
task as a Structured Prediction problem and since
then most work on RE has revolved around end-
to-end systems which do the joint task (Miwa and
Bansal, 2016; Katiyar and Cardie, 2017; Bekoulis
et al., 2018).

A common theme in current end-to-end mod-
els is the use of token-level models. For the en-
tity mention detection step, recent works (Miwa
and Bansal, 2016; Katiyar and Cardie, 2017; Bek-
oulis et al., 2018) have used the BILOU (Rati-
nov and Roth, 2009; Florian et al., 2006) token-
tagging scheme. For the relation extraction step
there have been a variety of methods tried like
Tree-LSTMs (Miwa and Bansal, 2016), sequence

labeling (Katiyar and Cardie, 2017) and multi-
head selection (Bekoulis et al., 2018). Li and Ji
(2014) used semi-Markov chains and the Viterbi
algorithm, which is also a sequential token-level
approach. This token-level modeling approach has
several limitations as highlighted in Section 1.

Recent work using span-level end-to-end mod-
els have seen success in NLP tasks following the
same pattern as RE (Coreference Resolution (Lee
et al., 2017) and Semantic Role Labeling (Ouchi
et al., 2018)). In this paper, we adapt (Lee et al.,
2017) to create a span-level end-to-end model for
RE.

3 Model

Our model consists of three steps which we ex-
plain in detail in the next subsections:

1. Span Representation Generation
Use task-agnostic raw token embeddings to
create task-specific token embeddings for
each token. The task-specific token embed-
dings are used to generate span embeddings
for each possible span.

2. Entity Mention Detection (EMD)
The span embeddings are used to obtain a
vector of entity type scores for each span.
Each span is assigned the entity type corre-
sponding to its highest entity type score. The
spans that are assigned an entity type other
than NONE are selected for Step 3.

3. Relation Extraction (RE)
For each ordered span-pair (i, j), we obtain
a representation by concatenating the respec-
tive span embeddings. This representation is
defined in an order-sensitive way in Section
3.3 i.e. the span-pair representation of spans
(i, j) is different from that of spans (j, i). For
each ordered span-pair, its representation is
used to obtain a vector of relation type scores.
Each ordered span-pair is assigned the rela-
tion type of its highest relation type score.

3.1 Step 1: Span Representation Generation
The architecture we use to generate span represen-
tations closely follows (Lee et al., 2017).

Given a document D with T tokens, there are
N = T (T+1)2 possible spans. span i is defined by
all the tokens from START(i) to END(i) inclusive,
for 1 ≤ i ≤ N . The aim is to obtain a span repre-
sentation gi for each span i.



5310

Raw Token Embeddings We use xt to represent
the raw token embeddings of token t with 1 ≤ t ≤
T . xt is a concatenation of the following:

1. Fixed Contextual Word Embeddings

2. Fixed Word Embeddings

3. Trained from scratch Character Embeddings

We use fixed ELMo (Peters et al., 2018) for
Contextual Word Embeddings, fixed Senna (Col-
lobert et al., 2011) for Word Embeddings and train
Character Embeddings from scratch. The Con-
textual Word Embeddings for each sentence were
computed separately.

In terms of number of free parameters; Con-
textual Word Embeddings use the most (100’s of
millions), followed by Word Embeddings (10’s of
millions) and finally Character Embeddings use by
far the least (10’s of thousands). The decision to
train only the Character Embeddings was based
on overfitting concerns given our relatively small
dataset.

Bi-LSTM Layers The pretrained Contextual
Embeddings we use in xt above are obtained
by unsupervised task-agnostic training. To ob-
tain task-specific contextualization we use stacked
bidirectional LSTMs (Hochreiter and Schmidhu-
ber, 1997) on the raw token embeddings xt to ob-
tain x∗t ,

x∗t = [
−→
ht,
←−
ht]

where
−→
ht and

←−
ht are the hidden states of the last

layer of the forward and backward LSTMs respec-
tively. x∗t is the concatenation of

−→
ht and

←−
ht. The

bi-LSTMs were run separately on each sentence
as that gave better performance.

Span Representation Syntactic heads obtained
from general syntactic parsers are used in many
NLP systems. Here we don’t use general syntactic
parsers but instead use attention (Bahdanau et al.,
2015) to create a task-specific span-head feature.
This feature vector is computed for each span:

αt = MLPα(x∗t)

βi,t =
exp(αt)

END(i)∑
k=START(i)

exp(αk)

x̂i =

END(i)∑
k=START(i)

βi,txt

where MLPα is a Multi Layer Perceptron (aka
Feed Forward Network). x̂i is a weighted sum of
fixed word vectors for the tokens in span i. We did
experiment with using the weighted sum of the bi-
LSTM output (x∗t) or of the ELMo (Peters et al.,
2018) fixed contextual word embeddings but got
better results with using fixed word embeddings.

For each span i, its span representation gi was
defined as:

gi = [x
∗

START(i),x
∗

END(i), x̂i, φ(i)]

where φ(i) encodes the size of span i in number
of tokens. Each component of gi is a span-specific
feature that would be difficult to define and use in
token-level models.

3.2 Step 2: Entity Mention Detection (EMD)
In this step, we predict the entity type for each
span. This prediction is done identically and par-
allelly for each span. For each span we compute a
vector of entity type scores. The number of entity
type scores computed is the number of entity types
(including the NONE entity type). For each span,
the softmax function is applied to its entity type
scores to get a distribution over the entity types.
For span i,

scoreneri = MLPner(gi)

pneri = softmax(score
ner
i ) (1)

The output size of MLPner and hence the size of
pneri is equal to the number of NER classes.

The predicted entity type for each span i is the
entity type corresponding to span i’s highest en-
tity type score i.e. max (scoreneri ). Only spans
whose predicted entity type is not NONE are se-
lected for Step 3. Unlike token-level models, over-
lapping spans can be selected here as each span’s
selection decision is independent of other spans.

3.3 Step 3: Relation Extraction (RE)
In this paper, we only consider ordered binary
relations, the most common setting of RE i.e.
only relations between exactly two arguments and
where the two pairs (span i, span j) and (span j,
span i) are considered different. We consider ev-
ery ordered pair of selected spans (from Step 2)
such that both spans are from the same sentence.
For each such pair (span i, span j), we first com-
pute an ordered pair embedding r(i,j):

ri,j = [gi,gj,gi ◦ gj]



5311

where gi and gj are the span embeddings of the
1st and 2nd arguments respectively (from Step 1).
gi ◦ gj refers to their element-wise product.

We use the ordered pair embedding ri,j to com-
pute a vector of relation type scores. The number
of relation type scores is the number of relation
types (including the NONE relation type). For each
ordered pair of spans, the softmax function is ap-
plied to its relation type scores to get a distribution
over the relation types. For pair (span i, span j),

scorerei,j = MLPre(ri,j)

prei,j = softmax(score
re
i,j) (2)

The output size of MLPre and hence the size of
prei,j is equal to the number of RE classes.

3.4 Loss
Two learning signals are provided to train the
model: entity type information per span and rela-
tion type information per ordered (selected) span
pair. Both are provided via CrossEntropy Loss
on Equations 1 and 2 respectively. We use ŷneri
to represent the correct entity type for span i and
ŷrei,j to represent the correct relation type for the
ordered pair of spans, (span i, span j). S repre-
sents the set of all spans and S ′ represents the set
of all selected spans (Section 3.2). Then the final
training loss is,

loss =
∑
i∈S

pneri (ŷ
ner
i ) +

∑
i∈S′

∑
j∈S′,j 6=i

prei,j(ŷ
re
i,j)

where the first term is a sum over all spans of the
entity mention detection loss (eqn 1) and the sec-
ond term is a sum over all ordered pairs of selected
spans of the relation extraction loss (eqn 2).

4 Experiments

Dataset We use the ACE2005 dataset (Dodding-
ton et al., 2004). It has 351 documents for train,
80 for validation and 80 for test. There are seven
span-level entity types and six ordered span rela-
tion types.

Character Embeddings The learned character
embeddings are of size 8. 1-dimensional convo-
lutions of window size 3,4,5 are applied per-token
with 50 filters of each window size. This is fol-
lowed by ReLU activation (Nair and Hinton, 2010)
and max-pooling over each filter.

Model Size Our stacked bi-LSTMs (Section 3.1)
has 3 layers with 200-dimensional hidden states

and highway connections. All Multi Layer Per-
ceptrons (MLP) has two hidden layers with 500
dimensions, each followed by ReLU activation.

Feature Encoding Each span gets a span width
feature which is a learned 20-dimensional vector
representing the number of tokens in that span.

Span Pruning A high number of spans un-
der consideration can lead to memory and speed
issues. We only consider spans that are entirely
within a sentence and limit spans to a max length
of L = 10. This choice was based on our Train
Set, see Section 5) for a discussion about it. Per-
formance is not affected significantly as very few
entity mentions have more than 10 tokens.

Regularization Dropout (Srivastava et al., 2014)
is applied with dropout rate 0.2 to all hidden layers
of all MLPs and feature encodings, with dropout
rate 0.5 to all word and character embeddings and
with dropout rate 0.4 to all LSTM layer outputs.

Learning Learning is done with Adam (Kingma
and Ba, 2015) with default parameters. The learn-
ing rate is annealed by 1% every 100 iterations.
Minibatch Size is 1. Early Stopping of 20 evalua-
tions on the dev set is used.

5 Model Complexity

Section 3.1 describes our span generation process
and Section 4 describes our algorithmic span prun-
ing process. The algorithmic span pruning process
limits our model spans which are entirely within a
single sentence and have a max length of L = 10
tokens. While our model creates representations
for spans (instead of just tokens), it achieves the
dual goals of being memory efficient and captur-
ing most (more than 99.95%) entities and relations
in the space of the spans considered.

Table 2 shows the model complexity and en-
tity/relation coverage for different policies of span
generation on the Train Set of ACE2005. It shows
numbers for policies ranging from one which con-
siders all spans across the doc, to a policy that con-
siders only single token spans. It shows that our
chosen span generation policy (in bold) is far more
memory efficient than a naive search over all pos-
sible spans in the input document. Yet our policy
still considers more than 99.95% of all entities and
relations. Our policy is linear in the document’s
(sentence) length, not quadratic; because we limit
our model to spans that are wholly in a single sen-
tence and have a max length of L = 10 tokens.



5312

Entity Mention Detection Relation Extraction
System P R F1 P R F1

(Li and Ji, 2014) 85.2 76.9 80.8 68.9 41.9 52.1
(Miwa and Bansal, 2016) 82.9 83.9 83.4 57 54.0 55.6

(Katiyar and Cardie, 2017) 84.0 81.3 82.6 57.9 54.0 55.9
(Sanh et al., 2018) EMD + RE 86.54 85.49 86.02 68.66 54.05 60.49
(Sanh et al., 2018) multi-task * 85.68 85.69 85.69 68.53 54.48 61.30

our model 85.85 86.10 85.98 68.02 58.38 62.83

Table 1: EMD and RE results on the ACE2005 Test dataset. Our model reports a new state-of-the-art RE perfor-
mance. Sanh et al. (2018) present several results in their multi-task paper. Results marked with (*) are not fair
comparisons here because they use additional signals beyond EMD and RE. Included here for completeness.

Permitted Spans # Spans % Entities Covered % Relations Covered
all spans across doc 45,836,252 100.00 100.00
only spans within single sentence 1,894,256 100.00 100.00

+ max length L = 10 1,079,150 99.99 99.96
+ max length L = 5 632,477 99.92 99.60
+ max length L = 2 279,515 98.02 94.92
+ max length L = 1 144,783 89.13 78.87

Table 2: Numbers are for the Train Set (351 docs) of ACE2005, where each Relation is between exactly two
Entities. Dev and Test Sets follow the same trends. Each row is a different policy for span generation and our
chosen policy is bolded. ”# Spans” is the number of spans considered by the policy. ”% Entities Covered” is the
percentage of entities in the dataset that are considered by that policy. ”% Relations Covered” is the same thing
for Relations (i.e. a Relation is covered if both entities of the Relation are covered). Note how our chosen policy
is more than 40x more memory efficient than a policy which considers all spans in the doc. And yet, our method
covers 99.99% and 99.96% of all Entities and Relations respectively in the Train Set of ACE2005.

6 Results

Table 1 shows the results for RE. For the joint
task, we compare entity mention detection perfor-
mance and relation extraction performance. Our
proposed model achieves a new SOTA on RE with
a F1 of 62.83, more than 2.3 F1 above the previ-
ous SOTA. Our proposed model also beats a multi-
task model Sanh et al. (2018) which uses signals
from additional tasks by more than 1.5 F1 points.

For both tasks, our model’s Precision is close
to and Recall is significantly higher than previ-
ous works. The Recall gains for RE (4.3 absolute
points) are much higher than for EMD (0.6 abso-
lute points). The gains in EMD Recall highlights
the effectiveness of our span representations (Sec-
tion 3.1). The disproportionate gains in RE Recall
cannot be fully explained by the relatively lower
gains in EMD Recall. Thus, our large gains in RE
Recall (and F1) showcase the effectiveness of our
simple modeling of ordered span pairs for relation
extraction (Section 3.3).

7 Conclusions

We present a neural span-level end-to-end model
for joint entity mention detection and relation ex-
traction. In contrast with existing token-level
models: our model is able to use span-specific fea-
tures, allows for overlapping entity mentions and
does not use sequential decoding. Our proposed
model achieves a new state-of-the-art RE perfor-
mance on the ACE2005 dataset. The gains are
driven by improvements in Recall for both tasks.

Acknowledgements

We would like to thank He He for her guidance
in writing this paper and comments on multiple
manuscripts. We would also like to thank the
anonymous reviewers of ACL 2019, their com-
ments and suggestions have helped us focus more
on the important aspects of this paper.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua

Bengio. 2015. Neural machine translation by



5313

jointly learning to align and translate. CoRR,
abs/1409.0473.

Giannis Bekoulis, Johannes Deleu, Thomas Demeester,
and Chris Develder. 2018. Joint entity recogni-
tion and relation extraction as a multi-head selection
problem. Expert Syst. Appl., 114:34–45.

Razvan C. Bunescu and Raymond J. Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of the Conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, HLT ’05, pages
724–731, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537.

George R Doddington, Alexis Mitchell, Mark A Przy-
bocki, Lance A Ramshaw, Stephanie Strassel, and
Ralph M Weischedel. 2004. The automatic content
extraction (ace) program-tasks, data, and evaluation.
In LREC, volume 2, page 1.

Radu Florian, Hongyan Jing, Nanda Kambhatla, and
Imed Zitouni. 2006. Factorizing complex models:
A case study in mention detection. In ACL 2006,
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics, Proceedings of
the Conference, Sydney, Australia, 17-21 July 2006.

Daniel Gildea and Daniel Jurafsky. 2002. Auto-
matic labeling of semantic roles. Comput. Linguist.,
28(3):245–288.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780.

Arzoo Katiyar and Claire Cardie. 2017. Going out on
a limb: Joint extraction of entity mentions and re-
lations without dependency trees. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 917–928. Association for Computational Lin-
guistics.

Diederik P. Kingma and Jimmy Ba. 2015. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle-
moyer. 2017. End-to-end neural coreference reso-
lution. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 188–197. Association for Computational
Linguistics.

Qi Li and Heng Ji. 2014. Incremental joint extrac-
tion of entity mentions and relations. In Proceed-
ings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long

Papers), pages 402–412. Association for Computa-
tional Linguistics.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
structures. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1105–1116. Asso-
ciation for Computational Linguistics.

Vinod Nair and Geoffrey E. Hinton. 2010. Rectified
linear units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference
on International Conference on Machine Learning,
ICML’10, pages 807–814, USA. Omnipress.

Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ’10, pages 1396–1411,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Hiroki Ouchi, Hiroyuki Shindo, and Yuji Matsumoto.
2018. A span selection model for semantic role la-
beling. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1630–1642. Association for Computa-
tional Linguistics.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), pages 2227–
2237. Association for Computational Linguistics.

Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, CoNLL ’09,
pages 147–155, Stroudsburg, PA, USA. Association
for Computational Linguistics.

V. Sanh, T. Wolf, and S. Ruder. 2018. A hierarchical
multi-task approach for learning embeddings from
semantic tasks.

Cicero dos Santos, Bing Xiang, and Bowen Zhou.
2015. Classifying relations by ranking with con-
volutional neural networks. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 626–634. Association
for Computational Linguistics.

Sameer Singh, Sebastian Riedel, Brian Martin, Jiap-
ing Zheng, and Andrew McCallum. 2013. Joint in-
ference of entities, relations, and coreference. In
Proceedings of the 2013 Workshop on Automated
Knowledge Base Construction, AKBC ’13, pages 1–
6, New York, NY, USA. ACM.

https://doi.org/10.3115/1220575.1220666
https://doi.org/10.3115/1220575.1220666
http://dl.acm.org/citation.cfm?id=1953048.2078186
http://dl.acm.org/citation.cfm?id=1953048.2078186
http://aclweb.org/anthology/P06-1060
http://aclweb.org/anthology/P06-1060
https://doi.org/10.1162/089120102760275983
https://doi.org/10.1162/089120102760275983
https://doi.org/10.1162/neco.1997.9.8.1735
https://doi.org/10.1162/neco.1997.9.8.1735
https://doi.org/10.18653/v1/P17-1085
https://doi.org/10.18653/v1/P17-1085
https://doi.org/10.18653/v1/P17-1085
https://doi.org/10.18653/v1/D17-1018
https://doi.org/10.18653/v1/D17-1018
https://doi.org/10.3115/v1/P14-1038
https://doi.org/10.3115/v1/P14-1038
https://doi.org/10.18653/v1/P16-1105
https://doi.org/10.18653/v1/P16-1105
https://doi.org/10.18653/v1/P16-1105
http://dl.acm.org/citation.cfm?id=3104322.3104425
http://dl.acm.org/citation.cfm?id=3104322.3104425
http://dl.acm.org/citation.cfm?id=1858681.1858823
http://dl.acm.org/citation.cfm?id=1858681.1858823
http://aclweb.org/anthology/D18-1191
http://aclweb.org/anthology/D18-1191
https://doi.org/10.18653/v1/N18-1202
https://doi.org/10.18653/v1/N18-1202
http://dl.acm.org/citation.cfm?id=1596374.1596399
http://dl.acm.org/citation.cfm?id=1596374.1596399
http://arxiv.org/abs/arXiv:1811.06031
http://arxiv.org/abs/arXiv:1811.06031
http://arxiv.org/abs/arXiv:1811.06031
https://doi.org/10.3115/v1/P15-1061
https://doi.org/10.3115/v1/P15-1061
https://doi.org/10.1145/2509558.2509559
https://doi.org/10.1145/2509558.2509559


5314

Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic compo-
sitionality through recursive matrix-vector spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ’12, pages 1201–1211, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. J. Mach. Learn. Res., 15(1):1929–
1958.

Bishan Yang and Claire Cardie. 2013. Joint infer-
ence for fine-grained opinion extraction. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1640–1649. Association for Compu-
tational Linguistics.

http://dl.acm.org/citation.cfm?id=2390948.2391084
http://dl.acm.org/citation.cfm?id=2390948.2391084
http://dl.acm.org/citation.cfm?id=2627435.2670313
http://dl.acm.org/citation.cfm?id=2627435.2670313
http://aclweb.org/anthology/P13-1161
http://aclweb.org/anthology/P13-1161

