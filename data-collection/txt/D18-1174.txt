



















































Disambiguated skip-gram model


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1445–1454
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1445

Disambiguated skip-gram model

Karol Grzegorczyk1,2 and Marcin Kurdziel1
1Department of Computer Science,

AGH University of Science and Technology, Kraków, Poland
2Allegro, Poznań, Poland

{kgr,kurdziel}@agh.edu.pl

Abstract

We present disambiguated skip-gram: a
neural-probabilistic model for learning multi-
sense distributed representations of words.
Disambiguated skip-gram jointly estimates a
skip-gram-like context word prediction model
and a word sense disambiguation model. Un-
like previous probabilistic models for learning
multi-sense word embeddings, disambiguated
skip-gram is end-to-end differentiable and can
be interpreted as a simple feed-forward neu-
ral network. We also introduce an effective
pruning strategy for the embeddings learned
by disambiguated skip-gram. This allows us
to control the granularity of representations
learned by our model. In experimental evalua-
tion disambiguated skip-gram improves state-
of-the are results in several word sense induc-
tion benchmarks.

1 Introduction

Distributed representations of words find applica-
tions in a broad range of tasks, from natural lan-
guage parsing (Socher et al., 2013) to image cap-
tioning (Karpathy and Fei-Fei, 2015). Their use-
fulness led to a renewed interest in word embed-
ding algorithms. The most popular algorithms
of this kind learn word vectors in an unsuper-
vised manner, e.g., from word contexts (Mikolov
et al., 2013a) or from statistics of word co-
occurrence (Pennington et al., 2014). Unsuper-
vised learning of word embeddings has a clear
advantage: both general and domain-specific text
corpora are available for a number of languages,
which greatly reduces the cost of training. That
said, unsupervised learning of word embeddings
comes with its own challenges. One of the most
important is word ambiguity: words in a natural
language often have more than one meaning. The
word mouse, for example, may mean a pointing
device or an animal. Word embedding algorithms

often do not recognize this language feature and
estimate only one vector representation per word.
This may lead to suboptimal word representations.

The main contribution of this work is disam-
biguated skip-gram: a neural-probabilistic model
for learning distributed representations of words
that capture word ambiguity. Disambiguated skip-
gram builds upon the skip-gram model introduced
by Mikolov et al. (2013a,b). Skip-gram constructs
word embeddings via an auxiliary prediction task:
given a word in a sentence, skip-gram attempts to
predict the surrounding words. To this end, skip-
gram defines a simple softmax model for the con-
ditional probability of observing a context word c
given the center word w:

p (c | w) = e
vTwuc∑

c′∈D e
vTwuc′

, (1)

where D is the vocabulary. This log-bilinear
model assigns two embedding vectors to every
word w ∈ D: an input embedding vector vw and
an output embedding vector uw. Skip-gram de-
fines the training objective for a single example as
the log-probability: log p (c | w). By maximizing
this objective, skip-gram estimates input and out-
put vectors that reflect semantic relations between
words that occur in similar contexts. The input
vectors are then used as word embeddings.

The main idea behind disambiguated skip-gram
is to jointly learn to disambiguate words and pre-
dict their contexts. We therefore extend skip-
gram with a parametric word sense disambigua-
tion model. This allows us to discover word senses
in an unsupervised manner, while preserving the
simplicity of the skip-gram approach. In particu-
lar, unlike previous probabilistic models for multi-
sense word embeddings, disambiguated skip-gram
can be seen as a simple feed-forward neural net-
work amendable to end-to-end training with back-



1446

propagation. Furthermore, disambiguated skip-
gram admits an effective pruning strategy for the
learned word sense embeddings. In particular,
we control the granularity of the learned repre-
sentations by penalizing the entropy of the prob-
ability distributions learned by the disambiguation
model. We then marginalize word sense probabil-
ities over the training examples and prune embed-
dings with low marginal probability.

We have carried out an extensive experimental
evaluation of disambiguated skip-gram. Our re-
sults demonstrate that the multi-sense word em-
beddings learned by disambiguated skip-gram im-
prove state-of-the-art results in the word sense in-
duction task.

2 Disambiguated skip-gram model

Let X = [w1, . . . , wn], where each wi ∈ D, be a
sequence of words from a vocabulary D. By Cwi
we denote the context of the word wi in X . For
example, Cwi can be a set of words that are no fur-
ther than l positions from wi and are in the same
sentence as wi. To simplify notation we will usu-
ally omit the sequence index i and write w ∈ X
for an element of the input sequence X and Cw
for its context. We will also use notation yw for
a vector y (from some set of vectors indexed by
the vocabulary words) corresponding to the word
w. Note that in this case we disregard the position
of w in X and use just the word as the index. In
particular, if some word w occurs multiple times
in X , all occurrences share a single vector yw.

Similarly to skip-gram, the disambiguated skip-
gram model constructs word embeddings by learn-
ing to predict context words c ∈ Cw given the
center word w ∈ X . However, disambiguated
skip-gram explicitly accounts for word ambiguity.
To this end, we represent each word d ∈ D by
a set of k sense embedding vectors vdz , indexed
by z ∈ {1, . . . k}, and an output embedding vec-
tor ud. We then parametrize the conditional prob-
ability p (c | w, z = j) of observing a word c in
the context of the word w in its j-th sense with
a softmax model similar to the original skip-gram
parametrization:

p (c | w, z = j) = e
vTwjuc∑

c′∈D e
vTwjuc′

. (2)

Furthermore, in this work we assume that a sense
of the word w ∈ X can be guessed from its con-

text Cw1, i.e. that:

zw ∼ p (z = j | w, Cw) , j = 1, . . . , k, (3)

where zw is an index of the sense of the word
w ∈ X . Given this assumption, we parametrize
the probability distribution for zw using a softmax
model similar to Eq. 2. That is, for each word
d ∈ D we introduce k sense disambiguation vec-
tors qds, s ∈ {1, . . . , k}, and a context embed-
ding vector rd. The conditional probability that
the word w ∈ X occurs in its j-th sense is then
modelled as:

p (z = j | w, Cw) =
eq

T
wj r̄w∑

s=1,...,k e
qTwsr̄w

, (4)

where r̄w is a vector representation of Cw. We rep-
resent Cw by an average of context embedding vec-
tors:

r̄w =
1

#Cw

∑
c∈Cw

rc. (5)

We can now define the training objective for a sin-
gle word w ∈ X as the expected negative log-
likelihood of observing the context Cw under the
distribution of senses of the center word w:

L (φ, w) =Ezw∼p(z=j|w,Cw)[
− log

∏
c∈Cw

p (c | w, z = zw)

]
,

(6)

with the parameters: φ = {vdz,ud,qdz, rd|d ∈
D, z = 1, . . . , k}.

The objective in Eq. 6 is inconvenient for
gradient-based optimization, because the expecta-
tion is taken with respect to a probability distri-
bution that is a function of model parameters. In
principle, we can estimate the gradient of this ob-
jective with the score function estimator (Glynn,
1990; Williams, 1992). Unfortunately, the score
function estimator suffers from a high variance,
even when used with a control variate. One can
also derive a low-variance unbiased gradient esti-
mator for certain probability distributions, by ex-
pressing the samples as a differentiable function
of model parameters and a random variable from
some independent fixed distribution (Kingma and
Welling, 2013). This approach is not directly ap-
plicable to our case, because categorical distribu-
tion do not admit reparametrization with a differ-
entiable function. That said, a simple and effective

1This assumption follows from an observation that two
different meanings of a given word will often have vastly dif-
ferent contexts.



1447

biased gradient estimator for an expectation with
respect to a categorical distribution was recently
proposed by Jang et al. (2016) and Maddison
et al. (2016). The basic idea is to reparametrize
the samples from the categorical distribution with
the Gumbel-Max trick (Gumbel, 1954) and then
approximate the non-differentiable max operator
with a softmax function with temperature hyper-
parameter. This can be seen as a reparametriza-
tion trick for a continuous relaxation to discrete
samples from the categorical distribution.

In our case, the samples from p (z = j | w, Cw)
take the form:

zwj =


1

if j = arg max
s

(ξs+

log p (z = s | w, Cw)),
0 otherwise,

where ξs are i.i.d. samples from the standard
Gumbel distribution f (0, 1). Note that the sam-
ples zw = [zw1, . . . , zwk] are now one-hot en-
coded. The continuous relaxation to zw is:

z̃w = [z̃wj (ξ1, . . . , ξk, Cw) | j = 1 . . . k] ,
z̃wj (ξ1, . . . , ξk, Cw) =

e[ξj+log p(z=j|w,Cw)]/τ∑
s=1,...,k e

[ξs+log p(z=s|w,Cw)]/τ
,

(7)

where τ is the temperature hyper-parameter.
When τ → 0, we recover the samples from
p (z = j | w, Cw). However, for τ > 0 the sam-
ples z̃w are no longer discrete. In this case we
consider a relaxed sense embedding vector:

ṽw =
∑

j=1,...,k

z̃wj (ξ1, . . . , ξk, Cw)vwj (8)

and model the conditional probability of observing
a word c in the context of the word w as:

p (c | w, ṽw) =
eṽ

T
wuc∑

c′∈D e
ṽTwuc′

. (9)

Our training objective for a single word w ∈ X
then takes the form:

L (φ, w) =Eξ1,...,ξk∼f(0,1)[
− log

∏
c∈Cw

p (c | w, ṽw)

]
.

(10)

The relaxed objective in Eq. 10 is tractable and
differentiable with respect to the parameters φ.

context
embedding

lookup

...

co
nt

ex
t 

w
or

ds
ce

nt
er

 w
or

d 
ID

...

sense
embedding

lookup

co
nt

ex
t 

w
or

d

relaxed
sense vector

prediction

...

vw1

vwk

rc1

rcm

r

disambiguation
embedding

lookup

...

center word ID

qw1 qwk

ξ1 ξk

zw~

vw~

U

Elementwise multiplication

Figure 1: Disambiguated skip-gram model.

When τ → 0, it becomes equivalent to the ob-
jective in Eq. 6. In practice, we approximate the
expectation in Eq. 10 with a one-sample Monte
Carlo estimator. In these settings disambiguated
skip-gram can be seen as a simple feed-forward
neural network pictured in Fig. 1. During training,
the network jointly estimates a sense disambigua-
tion model (Eq. 4) and a context word prediction
model (Eq. 2), which we use to construct multi-
sense word embeddings.

2.1 Pruning word senses

The disambiguated skip-gram model is paramet-
ric, i.e. it allocates a fixed number of sense em-
bedding vectors to each word, even though dif-
ferent words have different number of discernible
senses. That said, we can prune the sense embed-
ding vectors by considering their probabilities ac-
cording to the learned disambiguation model. In
particular, after training we estimate the marginal
probability:

p (d, j) =
1

md

∑
wi∈X,
wi=d

p (z = j | wi, Cwi) (11)

for each word d ∈ D and each sense index j ∈
{1, . . . , k}. We then prune sense embedding vec-
tors with low marginal probability, e.g., p (d, j) <
0.05. The normalizing factor md in Eq. 11 is the
number of occurrences of the word d in X .



1448

The above pruning technique can be extended
to allow for an explicit control over the granular-
ity of the learned sense representations. To this
end, we use an entropy regularization term sim-
ilar to the one studied by Pereyra et al. (2017)
in classification networks. In disambiguated
skip-gram the granularity of the learned repre-
sentations is controlled by the disambiguation
model (Eq. 4). Therefore, we extend the objective
of our model (Eq. 10) by adding to it an entropy S
of the probability distribution p (z = j | w, Cw):

Lr (φ, w) = L (φ, w) + γS (φ, w) , (12)

where:

S (φ, w) =

−
k∑
j=1

p (z = j | w, Cw) log p (z = j | w, Cw) .

The hyper-parameter γ, which we further call en-
tropy cost, controls the strength of the regulariza-
tion and, in turn, the granularity of the learned
sense representations. In particular, γ > 0 encour-
ages the model to learn more coarse-grained sense
representations, whereas γ < 0 increases the gran-
ularity of the learned senses.

3 Related work

Algorithms for learning distributed multi-sense
representations of words have been a focus of sev-
eral recent works. Initial approaches to this task
relied on clustering word contexts. One of the first
algorithms of this kind was proposed by Huang
et al. (2012). They learn multi-sense word repre-
sentations in three steps. First, they estimate vec-
tor representations of words using a feed-forward
neural language model. Next, they calculate av-
erage word vector for each context in the training
corpus, cluster these context representations and
relabel each word in the corpus to a word sense
represented by the nearest cluster. Finally, they
train the language model on the relabelled corpus
and obtain vector representations for word senses.

Neelakantan et al. (2014) proposed the Multi-
Sense Skip-gram (MSSG) model, that jointly
learns context cluster prototypes and word sense
embeddings. Their model extends skip-gram by
maintaining context clusters for every word in the
vocabulary. Given a training example with a cen-
ter word w and its context representation c, they
infer the word sense for w by a hard assignment of

the context representation c to the cluster with the
nearest centroid. Afterwards, they perform a skip-
gram-like update on the vector representation of
the selected word sense and the output vectors of
the context words. Neelakantan et al. also pro-
posed a non-parametric version of MSSG (NP-
MSSG), in which the number of clusters, and in
turn the number of word senses, increases during
training. They use a simple heuristic to determine
the number of word senses: NP-MSSG allocates a
new sense for the center word w when the similar-
ity between the context representation c and the
nearest cluster centroid falls below some prede-
fined threshold. Neelakantan et al. demonstrated
that MSSG and NP-MSSG outperform the Huang
et al. algorithm on a contextual word similarity
task.

A disadvantage of the Huang et al. and Nee-
lakantan et al. algorithms is that they do not fol-
low a principled statistical approach, but instead
rely on hard clustering of context vectors. This
has been addressed in more recent algorithms,
which learn multi-sense word representations in a
probabilistic framework. Concurrent to Neelakan-
tan et al. work, Tian et al. (2014) proposed a
probabilistic Multi-Prototype Skip-Gram (MPSG)
model. MPSG extends the skip-gram model by
adding to each position in the input text a latent
variable that encodes the index of the word sense
at that position. Furthermore, for each word in
the vocabulary MPSG maintains a fixed number of
sense embedding vectors and a single output vec-
tor. These parameters define a softmax model for
the conditional probability of observing a context
word given the center word and the latent sense in-
dex. Finally, MPSG models the conditional prob-
ability of observing a context word given the cen-
ter word with a mixture model whose components
correspond to the senses of the center word. Tian
et al. derived an expectation maximization algo-
rithm for estimation of softmax parameters and
prior sense probabilities in their model. MPSG
was evaluated in a contextual word similarity task,
where its performance was similar to that of the
Huang et al. algorithm.

Bartunov et al. (2016) proposed the AdaGram
model, which can be seen as a non-parametric ver-
sion of MPSG. Similarly to MPSG, AdaGram in-
troduces latent variables for word sense indexes in
the input text. However, unlike MPSG, AdaGram
does not assume a fixed number of word senses.



1449

Instead, it defines the prior over word senses via
a Dirichlet process. As a result, AdaGram au-
tomatically learns the number of senses for all
words in the vocabulary. Unfortunately, defining
the prior over word senses via a Dirichlet pro-
cess gives an intractable model likelihood. Bar-
tunov et al. therefore optimize variational lower
bound of the AdaGram model likelihood using a
stochastic variational inference algorithm. Bar-
tunov et al. evaluated AdaGram performance on
several word sense induction benchmarks. They
demonstrated that AdaGram consistently outper-
forms MSSG, NP-MSSG and MPSG models in
these benchmarks. AdaGram has been recently
extended to handle parallel multilingual text cor-
pora (Upadhyay et al., 2017).

For the prediction of context words (Eq. 2) dis-
ambiguated skip-gram adopts the softmax model
used in MPSG. However, in contrast to the previ-
ous works, disambiguated skip-gram learns a para-
metric model for the conditional probability dis-
tribution over senses of the center word given the
context words (Eq. 4). This allows us to define the
training objective for disambiguated skip-gram as
the expected negative log-likelihood of observing
the context words under the distribution of senses
of the center word. We use a biased low-variance
gradient estimator for this objective, which en-
ables stable end-to-end training with backpropa-
gation.

The main goal of the AdaGram model is to auto-
matically discover the number of word senses for
the vocabulary words. This does not mean, how-
ever, that the number of senses learned by Ada-
Gram is independent of model hyper-parameters.
On the contrary, the number of senses learned
by AdaGram is directly controlled by the hy-
per parameter α in the Dirichlet process used to
define the prior over word meanings (Bartunov
et al., 2016). Disambiguated skip-gram controls
the number of learned senses by penalizing the en-
tropy of the conditional probability distribution in
the sense disambiguation model (Eq. 12). The en-
tropy cost γ in this approach performs a function
similar to the hyper-parameter α in AdaGram.

In addition to the works discussed above, word
ambiguity was also modelled using topic mod-
els (Liu et al., 2015), large bi-directional language
models (Peters et al., 2018) or subword informa-
tion (Athiwaratkun et al., 2018). Also, Li and Ju-
rafsky (2015) evaluated multi-sense embeddings

in several downstream tasks. They found that
multi-sense embeddings improve performance in
tasks such as POS tagging or identification of se-
mantic relations. They also identify downstream
tasks which do not benefit from sense disambigua-
tion. In sentiment analysis, for example, word
sentiment usually does not depend on the inferred
sense.

4 Experiments

We conducted a number of experiments to eval-
uate the quality of multi-sense word embeddings
learned by disambiguated skip-gram. This section
reports results from our evaluation. First, we re-
port qualitative results from our model for several
polysemous words. We then compare the perfor-
mance of disambiguated skip-gram with several
competing algorithms on a set of word sense in-
duction tasks. Finally, we evaluate the effect of
the entropy cost on the learned representations.

It is worth noting that the quality of multi-
sense word embeddings was formerly assessed
in contextual word similarity experiments. How-
ever, Bartunov et al. (2016) demonstrated that con-
textual word similarity experiments do not reflect
the quality of multi-sense representations. In par-
ticular, the best performance in contextual word
similarity task is often achieved by the baseline
skip-gram model, which does not recognize word
senses. This can be attributed to the fact that
skip-gram objective directly optimizes similarity
of vector representations of words that appear
in similar contexts. Multi-sense models, on the
other hand, solve a harder task: they disambiguate
words in contexts and then model the similarities
between the discovered senses. Bartunov et al. fo-
cus, therefore, on the performance of multi-sense
embeddings in the word sense induction task. We
adopt their evaluation methodology in this work.

We trained our disambiguated skip-gram mod-
els on the Westbury Lab Wikipedia corpus (Shaoul
and Westbury, 2010). We optimized the models
using mini-batch stochastic gradient descend with
momentum.

4.1 Qualitative results

We begin our evaluation by presenting senses
discovered by disambiguated skip-gram for sev-
eral ambiguous words. For the demonstration we
trained four 300-dimensional models with three
sense embedding vectors allocated to each word



1450

Word γ = 0.0 γ = 0.25 γ = 0.5
p Nearest neighbors p Nearest neighbors p Nearest neighbors

fox

0.52 nbc cbs network 0.60 nbc cbs abc 0.68 cbs nbc abc
syndication espn syndication network cable colmes

0.25 miller allen plummer 0.22 miller allen terry 0.18 allen russell miller
crowe buck russell soper turner berry

0.24 badger wolf coyote 0.18 badger squirrel weasel 0.14 badger marten raccoon
weasel marten raccoon marten beaver mink

net

0.41 ebitda earnings annualized 0.43 ebitda annualized jpy 0.77 ebitda deadweight isk
taxable depreciation deadweight gni annualized deducting

0.32 trawl streamline maximis- 0.33 trawl minimises maximis- 0.23 crossbar puck lob
ing minimises counteracts ing streamlines stickiness header dribbled

0.26 crossbar puck lob 0.24 crossbar puck lob 0
sliothar offside offside dribbled

rock

0.41 band indie punk 0.7 alternative glam progre- 0.72 alternative punk indie
alternative supergroup ssive indie psychedelic glam progressive

0.34 punk rockabilly pop 0.17 boulder basalt outcrop 0.17 basalt boulders quart-
psychedelia funk quartzite cliffs zite cliff outcropping

0.26 boulder quartzite 0.13 granite bluff pine 0.11 pine bluff eagle
granite sandstone basalt pigeon ledge pigeon turtle

plant

0.45 flowering perennial 0.46 flowering grasses shrub 0.48 flowering shrub grasses
shrub grass fungus fungus herbaceous herbaceous fungus

0.38 refinery smelter petroche- 0.45 refinery factory megawatt 0.48 refinery factory smelter
mical processing factory smelter cogeneration megawatt sellafield

0.18 factory botanical labo- 0.08 weed planted shed 0.04
ratory farm nurseryman grinder laboratory

mouse

0.47 mickey rabbit goofy 0.5 rat mice rodent 0.51 mice rodent rat
cat porky mus elegans mus elegans

0.35 cursor joystick trackball 0.49 rabbit goofy cat 0.49 rabbit goofy porky
touchpad touchscreen porky tigger tigger tweety

0.19 rodent vole shrew 0.01 0
pygmy rat

apple

0.46 macintosh imac iigs 0.64 macintosh iigs imac 0.95 macintosh blackberry
iie iic iie iic iigs imac apricot

0.28 wozniak macworld 0.25 wozniak blackberry 0.04
macintosh ipod sculley tomato potato popcorn

0.26 strawberry peach 0.11 peach pecan persimmon 0.01
raspberry blueberry plum prune blueberry

table

0.40 sortable column lookup 0.48 sortable column lookup 0.66 sortable tray column
hashed tray tray hashed chairs buckets

0.38 foosball carom lang- 0.36 foosball ept languishing 0.31 standings ept foosball
uishing pool slipping pool leaderboard leaderboard ittf

0.22 sortable list alphabe- 0.16 sortable descending list 0.03
tical descending brackets alphabetically please

Table 1: Nearest neighbors and marginal probabilities p of word sense embedding vectors discovered
by the disambiguated skip-gram model for several ambiguous words. Sense embedding vectors with a
marginal probability p < 0.05 are pruned from the learned model.

and the entropy cost γ ranging from 0.0 to 0.5.
For each of the evaluated words sense embeddings
we calculated the cosine similarity to the remain-
ing words and selected 5 nearest neighbours. The
results are reported in Tab. 1.

Disambiguated skip-gram discovered main
meanings of our test words. For example, the
meanings discovered for the word fox correspond
to a broadcasting company, an animal and a fam-

ily name. The meanings discovered for the word
mouse correspond to a cartoon character, a com-
puter mouse and an animal.

Results in Tab. 1 also demonstrate that disam-
biguated skip-gram will often expose an internal
structure in a word meaning, if that meaning ap-
pears in different contexts. For example, disam-
biguated skip-gram learned two embeddings for
the word plant corresponding to its factory mean-



1451

ing: one related to heavy industry and one related
to a farm or a plant nursery. This is a consequence
of the fact that disambiguated skip-gram discovers
word senses using only the information about the
contexts in which these words occur. In particular,
it does not employ any supervision from an exter-
nal knowledge base. Bartunov et al. (2016) refer
to a related phenomenon in the AdaGram embed-
dings as the semantic resolution of the model.

4.2 Word-sense induction experiments
To compare disambiguated skip-gram with state-
of-the-art competing algorithms we assessed its
performance in a set of word sense induction
tasks. In this evaluation we follow the experi-
mental setup from (Bartunov et al., 2016), allow-
ing for direct comparison with the results reported
therein. In particular, we evaluated disambiguated
skip-gram on the datasets from the SemEval-2007
Task 2 competition (SE-2007), SemEval-2010
Task 14 competition (SE-2010), SemEval-2013
Task 13 competition (SE-2013) and the Wikipedia
Word-sense Induction (WWSI) dataset introduced
by Bartunov et al. The test datasets consist of
between 4664 and 36354 examples. Each ex-
ample provides a ground truth sense of a cen-
ter word and the context in which this sense ap-
peared. The goal is to recognize the sense of the
center word given the context. We use the prepro-
cessed versions of SE-2007, SE-2010 and WWSI
datasets made available by Bartunov et al. For the
SemEval-2013 Task 13 we use the original com-
petition dataset (Jurgens and Klapaftis, 2013) and
follow the preprocessing steps reported in (Bar-
tunov et al., 2016).

We use a simple procedure for resolving word
senses. That is, we average all sense embedding
vectors of all context words and select the sense
zw of the center word w whose embedding vector
is most similar to the average vector:

zw = arg max
j

cos (vwj , c̄w) , (13)

where:

c̄w = (k ·#Cw)−1
∑
c∈Cw

k∑
s=1

vcs. (14)

The intuition behind this procedure is that we ex-
pect the average to preserve a shared component
in the embedding vectors, namely embeddings for
senses related to the sense of the center word, and

cancel out embeddings of unrelated senses. In ad-
dition to averaging sense embedding vectors we
also experimented with averaging output vectors
of context words. However, this approach usually
gave slightly worse results.

Following Bartunov et al. (2016), we use ad-
justed rand index (Hubert and Arabie, 1985) to
compare ground truth senses for a given word with
the senses inferred from disambiguated skip-gram
embeddings. The final performance on a bench-
mark task is the average of adjusted rand index
values over all test words in the task.

For this evaluation we trained a 300-
dimensional disambiguated skip-gram model
with 5 sense embedding vectors allocated to
each word and no entropy cost (γ = 0.0). The
comparison between our model and MSSG, NP-
MSSG, MPSG and AdaGram is reported in Tab. 2.
The results for MSSG, NP-MSSG, MPSG and
AdaGram are taken from Bartunov et al. (2016).
Multi-sense embedding vectors learned by
disambiguated skip-gram outperform baseline
methods on the SE-2007, SE-2010 and WWSI
benchmarks, and achieve the second best result on
the SE-2013 benchmark.

SE-2007 SE-2010 SE-2013 WWSI
MSSG 0.048 0.085 0.033 0.194

NP-MSSG 0.033 0.044 0.033 0.110
MPSG 0.044 0.077 0.014 0.160

AdaGram 0.069 0.097 0.061 0.286
Disamb. 0.077 0.117 0.045 0.292

skip-gram

Table 2: Performance of multi-sense word embed-
ding methods in word sense induction tasks. The
reported performance metric is the adjusted rand
index averaged over all test words in the bench-
mark task. Results for all models except the dis-
ambiguated skip-gram (Disamb. skip-gram) are
taken from (Bartunov et al., 2016).

4.3 Effect of the entropy cost

Results reported in Tab. 1 demonstrate that the en-
tropy cost γ (Eq. 12) indeed allows for pruning
senses learned by disambiguated skip-gram. In
particular, when the entropy cost increases, disam-
biguated skip-gram allocates more of the marginal
probability mass (Eq. 11) to the frequent mean-
ings of the modelled words and, in effect, learns
coarser representations.

For a quantitative evaluation of the effect of



1452

entropy cost on the learned representations we
trained 50-dimensional disambiguated skip-gram
models with γ ranging from 0.0 to 1.0. All models
allocate 5 sense embedding vectors to every word
in the vocabulary. In Tab. 3 we report an average
number of senses per word with marginal prob-
ability p ≥ 0.05, depending on the value of the
entropy cost. In Fig. 2 we also report histograms
of marginal probabilities for selected entropy cost
values.

Entropy cost 0.0 0.1 0.25 0.5 0.75 1.0
Avg. sense num. 4.7 4.3 3.7 3.2 2.8 2.5

Table 3: Average number of senses per word with
marginal probability p ≥ 0.05, learned by disam-
biguated skip-gram models with different values
of the entropy cost.

0.0 0.2 0.4 0.6 0.8 1.0

Marginal probability

0

1

2

3

C
o
u
n
t

×104

γ=0. 0

γ=0. 25

γ=0. 5

γ=0. 75

Figure 2: Histograms of marginal probabilities of
word senses learned by disambiguated skip-gram
models with different values of the entropy cost.

Histograms in Fig. 2 confirm our observation
from the qualitative evaluation: when the en-
tropy cost increases, disambiguated skip-gram
learns more peaked distributions for the condi-
tional sense probability p (z = j | w, Cw). This
translates to coarser sense representations. In par-
ticular, the model with no entropy cost learned
an average of 4.7 senses per word with marginal
probability p ≥ 0.05 (Tab. 3). This number de-
creases with an increasing entropy cost, reaching
an average of 2.5 senses per word for γ = 1.0.

We also evaluated 50- and 300-dimensional
models with different entropy costs in the word
sense induction tasks. In each case we pruned
senses with marginal probability p < 0.05. Re-

Dim. Entropy SE SE SE WWSI
Cost 2007 2010 2013

50
No 0.064 0.107 0.040 0.304

0.25 0.083 0.116 0.043 0.244
0.5 0.064 0.091 0.045 0.182

300
No 0.077 0.117 0.045 0.292

0.25 0.079 0.113 0.045 0.259
0.5 0.065 0.091 0.049 0.183

Table 4: Performance of disambiguated skip-gram
models with different entropy costs in the word
sense induction tasks. The reported performance
metric is the adjusted rand index averaged over all
test words in the benchmark task.

sults from this evaluation (Tab. 4) indicate that
the desired granularity of the learned sense rep-
resentations depends on the underlying task. In
the WWSI benchmark the best performing models
had no entropy cost, while in the SemEval tasks
small entropy cost usually improved results. The
results agree for both model dimensionalities.

5 Conclusions

In this work we developed disambiguated skip-
gram: a novel neural-probabilistic model for
learning multi-sense distributed representations
of words. Unlike previous probabilistic models
for multi-sense word embeddings, disambiguated
skip-gram is a simple feed-forward neural network
and can be trained end-to-end with backpropaga-
tion. In experimental evaluation disambiguated
skip-gram improved over the state-of-the-art re-
sults in three out of four benchmark datasets and
ranked second the fourth.

Disambiguated skip-gram optimizes expected
log-likelihood of the context prediction model un-
der the distribution of word senses parametrized
by the disambiguation model. We choose to opti-
mize this objective with a biased but low-variance
gradient estimator. However, parallel to this work
there has been a significant progress in gradient-
based training of models with discrete latent vari-
ables. Specifically, Tucker et al. (2017) pro-
posed an unbiased low-variance gradient estima-
tor, called REBAR, that is applicable to models
with categorical latent variables. REBAR may
allow to efficiently optimize the original disam-
biguated skip-gram objective (Eq. 6), instead of
the relaxed objective (Eq. 10). This may further
improve the quality of embeddings learned with
our approach.



1453

Acknowledgments

This research was supported by National Science
Centre, Poland grant no. 2013/09/B/ST6/01549
“Interactive Visual Text Analytics (IVTA): De-
velopment of novel, user-driven text mining and
visualization methods for large text corpora ex-
ploration”. The paper was partially financed by
AGH University of Science and Technology Statu-
tory Fund. The paper was also partially financed
by Allegro. Experiments for this work were
supported by the PL-Grid infrastructure and by
the “HPC Infrastructure for Grand Challenges of
Science and Engineering” project co-financed by
the European Regional Development Fund under
the Innovative Economy Operational Programme.
Last but not least, we would like to thank Professor
Witold Dzwinel for overall guidance and support.

References
Ben Athiwaratkun, Andrew Wilson, and Anima

Anandkumar. 2018. Probabilistic FastText for
multi-sense word embeddings. In Proceedings of
the 56th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 1–11. Association for Computational Linguis-
tics.

Sergey Bartunov, Dmitry Kondrashkin, Anton Osokin,
and Dmitry Vetrov. 2016. Breaking sticks and ambi-
guities with adaptive skip-gram. In Proceedings of
the 19th International Conference on Artificial Intel-
ligence and Statistics, pages 130–138.

Peter W Glynn. 1990. Likelihood ratio gradient esti-
mation for stochastic systems. Communications of
the ACM, 33(10):75–84.

Emil Julius Gumbel. 1954. Statistical theory of ex-
treme values and some practical applications: a se-
ries of lectures. 33. US Govt. Print. Office.

Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873–882. Asso-
ciation for Computational Linguistics.

Lawrence Hubert and Phipps Arabie. 1985. Compar-
ing partitions. Journal of classification, 2(1):193–
218.

Eric Jang, Shixiang Gu, and Ben Poole. 2016. Cat-
egorical reparameterization with Gumbel-Softmax.
arXiv preprint arXiv:1611.01144.

David Jurgens and Ioannis P Klapaftis. 2013. Semeval-
2013 task 13: Word sense induction for graded and

non-graded senses. In Proceedings of the 7th in-
ternational workshop on semantic evaluation, pages
290–299. Association for Computational Linguis-
tics.

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In Proceedings of the IEEE conference on
computer vision and pattern recognition (CVPR),
pages 3128–3137.

Diederik P Kingma and Max Welling. 2013. Auto-
encoding variational bayes. arXiv preprint
arXiv:1312.6114.

Jiwei Li and Dan Jurafsky. 2015. Do multi-sense
embeddings improve natural language understand-
ing? In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1722–1732. Association for Com-
putational Linguistics.

Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong
Sun. 2015. Topical word embeddings. In Proceed-
ings of the Twenty-Ninth AAAI Conference on Artifi-
cial Intelligence, pages 2418–2424.

Chris J Maddison, Andriy Mnih, and Yee Whye Teh.
2016. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv
preprint arXiv:1611.00712.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1059–1069. As-
sociation for Computational Linguistics.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), volume 14, pages 1532–1543.
Association for Computational Linguistics.

Gabriel Pereyra, George Tucker, Jan Chorowski,
Łukasz Kaiser, and Geoffrey Hinton. 2017. Regular-
izing neural networks by penalizing confident output
distributions. arXiv preprint arXiv:1701.06548.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke



1454

Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), pages 2227–
2237. Association for Computational Linguistics.

Cyrus Shaoul and Chris Westbury. 2010. The Westbury
lab Wikipedia corpus.

Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
tional vector grammars. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 455–465.

Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,
Enhong Chen, and Tie-Yan Liu. 2014. A probabilis-
tic model for learning multi-prototype word embed-
dings. In Proceedings of COLING 2014, the 25th In-
ternational Conference on Computational Linguis-
tics: Technical Papers, pages 151–160. Dublin City
University and Association for Computational Lin-
guistics.

George Tucker, Andriy Mnih, Chris J Maddison, John
Lawson, and Jascha Sohl-Dickstein. 2017. REBAR:
Low-variance, unbiased gradient estimates for dis-
crete latent variable models. In Advances in Neu-
ral Information Processing Systems 30, pages 2624–
2633.

Shyam Upadhyay, Kai-Wei Chang, Matt Taddy, Adam
Kalai, and James Zou. 2017. Beyond bilingual:
Multi-sense word embeddings using multilingual
context. In Proceedings of the 2nd Workshop on
Representation Learning for NLP, pages 101–110.
Association for Computational Linguistics.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.


