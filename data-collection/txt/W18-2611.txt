



















































Adaptations of ROUGE and BLEU to Better Evaluate Machine Reading Comprehension Task


Proceedings of the Workshop on Machine Reading for Question Answering, pages 98–104
Melbourne, Australia, July 19, 2018. c©2018 Association for Computational Linguistics

98

Adaptations of ROUGE and BLEU to Better Evaluate Machine Reading
Comprehension Task

An Yang1 2*, Kai Liu2, Jing Liu2, Yajuan Lyu2 and Sujian Li1

1Key Laboratory of Computational Linguistics, Peking University, MOE, China
2Baidu Inc., Beijing, China

{yangan, lisujian}@pku.edu.cn
{liukai20, liujing46, lvyajuan}@baidu.com

Abstract

Current evaluation metrics to question an-
swering based machine reading compre-
hension (MRC) systems generally focus
on the lexical overlap between candidate
and reference answers, such as ROUGE
and BLEU. However, bias may appear
when these metrics are used for specific
question types, especially questions in-
quiring yes-no opinions and entity lists.
In this paper, we make adaptations on the
metrics to better correlate n-gram overlap
with the human judgment for answers to
these two question types. Statistical anal-
ysis proves the effectiveness of our ap-
proach. Our adaptations may provide pos-
itive guidance for the development of real-
scene MRC systems.

1 Introduction

The goal of current MRC tasks is to develop agents
which are able to comprehend passages automati-
cally and answer open-domain questions correctly.
With the release of several large-scale datasets like
SQuAD (Rajpurkar et al., 2016), MS-MARCO
(Nguyen et al., 2016) and DuReader (He et al.,
2017), many MRC models have been proposed in
previous works (Wang and Jiang, 2016; Seo et al.,
2016; Wang et al., 2017). Although MRC model
architectures have been intensively studied, the
evaluation metrics for them are rarely discussed.
For early cloze-style and multiple choice datasets
(Richardson et al., 2013; Hermann et al., 2015),
this may not be problematic. However, consider-
ing the trend that the model is required to generate
answers and question type is becoming more vari-
able and closer to real cases, we believe the design

*This work was done while the first author was doing in-
ternship at Baidu Inc.

of evaluation metric is indeed an issue to be fo-
cused on.

Currently, the criterion for comparing generated
and gold answers is mostly based on lexical over-
lap. For example, SQuAD uses exact-match ra-
tio and word-level F1-score, while MS-MARCO
and DuReader employ ROUGE-L (Lin, 2004) and
BLEU (Papineni et al., 2002) which measure n-
gram consistency or longest common sequence
(LCS) length. For some question types, we no-
tice these metrics may not correlate with seman-
tic correspondence well in some cases. In this
paper, we mainly tackle the issue of yes-no and
entity questions. For yes-no questions, overlap-
based metrics may ignore the yes-or-no opinion
which is more crucial in determining agreement
between answers. Answers with contrary opinions
may have high lexical overlap, such as “The radi-
ation of wireless routers has an impact on peo-
ple” and “The radiation of wireless routers has
no impact on people”. Similarly, for entity ques-
tions, we think the agreement should be more re-
flected by the correctness of entity listing. An-
swers which lack or mispredict entities should be
in distinction from correct answers, but the mis-
takes actually affect little in BLEU and ROUGE,
especially when the entity is a number. These two
question types are quite common in MRC datasets
and real scenario. As is shown in He et al. (2017),
36.2% queries in DuReader and 47.5% in Baidu
real search data are classified into these two cate-
gories. For the reasons above, developing an au-
tomatic evaluation system which takes considera-
tion of the inherent characteristics of these ques-
tion types is of great necessity.

In previous work, Dang et al. (2007) employed
type-specific metrics for evaluating candidate an-
swers in TREC 2007 QA track. Setting the accu-
racy of yes-no opinion type and F1-score of en-
tity list as extra metrics may solve the problem



99

to some extent. However, from the perspective of
simplicity and scalability to growing question type
category, we hope to design a unified and end-to-
end evaluation metric which is calculated automat-
ically. We propose some adaptations for ROUGE
and BLEU which provide them awareness to yes-
no opinion and entity agreement. Compared with
original metrics, our modified ROUGE and BLEU
achieve higher correlation to human judgment on
DuReader samples in both type-specific and over-
all analysis. Our work is a preliminary exploration
of better automatic evaluation systems for MRC
model in the real application.

In the remainder of this paper, related work is
discussed in section 2. Then we give details about
our adaptation on ROUGE and BLEU in section 3.
Statistical analysis is given in section 4. In section
5, we conclude the paper.

2 Related Work

MRC Task Recent years have witnessed grow-
ing research interest in machine reading compre-
hension. Annotation of large-scale datasets is a
strong driving force for the recent progress of
MRC systems. The paradigm of such datasets
ranges from cloze test (Hermann et al., 2015; Hill
et al., 2015), multiple choice (Lai et al., 2017),
span extraction (Rajpurkar et al., 2016) and an-
swer generation (Nguyen et al., 2016; He et al.,
2017). The last paradigm with multi-passages and
manually annotated answers for each question is
more close to real application. Based on these
resources, end-to-end neural MRC model archi-
tectures are implemented, including match-LSTM
(Wang and Jiang, 2016), BiDAF (Seo et al., 2016),
DCN (Xiong et al., 2016) and r-net (Wang et al.,
2017). With the objective of lexical overlap based
evaluation metrics, these models focus more on
text matching to references, which has bias to hu-
man demand. Instead, conceiving opinion and
entity-aware metrics will encourage future MRC
systems to look more into real application cases.

QA Evaluation Metrics In the past competi-
tions on question answering, various evaluation
metrics were proposed to make comparisons be-
tween participating systems. Early tasks includ-
ing TREC-8 and TREC-9 QA tracks (Voorhees
et al., 1999; Voorhees and Tice, 2000) only con-
sist of factoid questions. The ordered candidate
answers are evaluated manually to give binary cor-
rectness judgment and summarized by mean re-

ciprocal rank (MRR). With the addition of com-
plex non-factoid questions such as definition ques-
tions in TREC 2003 (Voorhees, 2003) and “other”
questions in TREC 2007 (Dang et al., 2007), man-
ual assessment becomes more difficult. “Nugget
pyramids” (Nenkova et al., 2007) are employed for
scoring, which prefer answers coveraging more
key points (nuggets). The nuggets are annotated
and weighted by human assessors, which is labor-
intensive. Breck et al. (2000) proposed to use
word recall against the stemmed gold answer as an
automatic evaluation metric. Following this idea,
metrics evaluating n-gram overlap and LCS length
between candidate and gold answers are designed
and become prevalent, among which BLEU (Pap-
ineni et al., 2002) and ROUGE (Dang et al., 2007)
are most widely-used. In general, BLEU focuses
more on n-gram precision and ROUGE is recall-
oriented. Later work has made adaptations on
these metrics from different perspectives (Baner-
jee and Lavie, 2005; Liu and Liu, 2008). In this
paper, our adaptations are aimed at increasing their
correlation to real human judgment on yes-no and
entity question answering, which are proved to be
practical.

3 Methodology

The brief idea of our adaptations is to add addi-
tional lexical overlap items which can reflect opin-
ion and entity agreement as the bonus. In the of-
ficial evaluation of MS-MARCO and DuReader,
ROUGE-L and BLEU are employed as metrics
at the same time, with the former as the primary
criterion for ranking participating systems. Their
modifications will be elaborated separately.

3.1 Adaptations on BLEU

For one question sample with single candidate and
several gold answers, Papineni et al. (2002) define
cumulative BLEU-n with uniform n-gram weight
as follows:

BLEUcum = BP ·
(

n∏
i=1

Pi

) 1
n

(1)

In the equation, Pi is the precision of i-gram in
the candidate answer

Pi =

∑
i–gram∈C

Countclip (i–gram)∑
i–gram′∈C

Count (i–gram′)
(2)



100

where C is i-gram set of the candidate answer,
Count(x) calculates the number of times that i-
gram x appear in candidate andCountclip(x) clips
Count(x) to the maximum times that x appears in
references.
BP stands for brevity penalty item, given refer-

ence length r and candidate length c

BP = emin (1−
r
c
,0) (3)

For cases with mutiple reference answers, we
choose the reference length which is closest to c.

For yes-no questions, we add an additional term
into both the numerator and denominator of (2) to
measure yes-no opinion agreement

bonusyn = α
∑

i–gram∈C
Countclip–s (i–gram)

(4)
whereCountclip–s(x) clipsCount(x) to the max-
imum times that x appears in reference answers
sharing the same yes or no opinion with the candi-
date and α stands for bonus weight. If the partici-
pant correctly judges the opinion type, its adapted
BLEU score will increase due to the introduced
bonus. However, the BLEU score still never ex-
ceed 1.0.

Calculating bonusyn requires the opinion labels
of both candidate and gold answers. For refer-
ences, it does not consume much labor to annotate
opinion labels in the construction of datasets. We
notice that recent DuReader dataset already satis-
fies this requirement, with each yes-no reference
answer labeled “Yes”, “No” or “Depends”. For
candidate answers, we think it should be the trend
to encourage participating systems to provide ex-
plicit predicted opinion labels apart from the an-
swers. The following example gives a simple il-
lustration of how to compute Pi with consideration
of bonusyn.

Example 1 (Adapted P2 for yes-no answer)
Question: Is skipping rope an aerobic exercise?
Predicted answer: [Yes] Skipping rope is an aerobic exer-
cise.
Gold answer 1: [Yes] Skipping rope is a kind of aerobic ex-
ercise with low intensity.
Gold answer 2: [Depends] Skipping rope can be regarded as
an aerobic exercise only when skipping for a long time.
Number of predicted bigrams1: 6
Number of hit predicted bigrams: 4
Bigram count for bonus: 3 (hit gold answer 1)
Vanilla P2: 4 / 6 = 0.67
Adapted P2 (α = 1.0): (4 + 3) / (6 + 3) = 0.78

1Include period symbol and omit lemmatization.

Similarly, we add another term to the numerator
and denominator of (2) for bonusing correct entity
answers

bonusent = β
∑

i–gram∈C
Countclip–e (i–gram)

(5)
where the reference answers provide a gold en-
tity list and Countclip–e(x) clips Count(x) to the
maximum times that x appears in the entity strings
in the list. β stands for the weight of entity bonus.
As a result, the score of answer containing more
right entities will increase, as is shown in exam-
ple 2.

Example 2 (Adapted P2 for entity answer)
Question: How long did it take for Qin Dynasty to unify
China?
Predicted answer: Qin unified China in 221 BC after the war
against other kingdoms which lasted ten years.
Gold answer: Qin unified China in ten years, from 230 BC
to 221 BC.
Gold entity set: ten years, 230 BC, 221 BC
Number of predicted bigrams: 16
Number of hit predicted bigrams: 5
Vanilla P2: 5 / 16 = 0.31
Adapted P2 (β = 1.0): (5 + 2) / (16 + 2) = 0.39

To calculate BLEU score over entire dataset us-
ing (1), we follow the common approach to com-
pute overall Pi, which separately sums the numer-
ator and denominator of (2) with bonus terms over
all the samples and finally get them divided. The
r and c for BP are also the sum across whole
dataset.

3.2 Adaptations on ROUGE-L
As mentioned in Lin (2004), the principle of calcu-
lating ROUGE-L is to examine the precision and
recall between candidate and reference answers
considering longest common subsequences. For
single sample, ROUGE-L is computed as

ROUGE–L =

(
1 + γ2

)
RLCSPLCS

RLCS + γ2PLCS
(6)

RLCS is the ratio of LCS length to reference
answer length, namely recall

RLCS =
LCS (c, r)

|r|
(7)

where c and r represent the candidate and refer-
ence answer.
PLCS is the ratio of LCS length to candidate

answer length, namely precision.

PLCS =
LCS (c, r)

|c|
(8)



101

For multiple gold answers, the maximumRLCS
and PLCS are selected to compute ROUGE-L.
Overall ROUGE-L on the dataset is defined as the
average ROUGE value of each sample.

Like our adaptations on BLEU, we integrate ad-
ditional bonus items into RLCS and PLCS . For
yes-no answers, if r and c have the same opinion
label, we add αLCS(r, c) to the numerator and
denominator of RLCS and PLCS . If participant
judges opinions and the judgement is correct, the
precision and recall will both increase, as is shown
in example 3.

Example 3 (Adapted ROUGE-L for yes-no answer)
Question: Is skipping rope an aerobic exercise?
Predicted answer: [Yes] Skipping rope is an aerobic exer-
cise.
Gold answer 1: [Yes] Skipping rope is a kind of aerobic ex-
ercise with low intensity.
Gold answer 2: [Depends] Skipping rope can be regarded as
an aerobic exercise only when skipping for a long time.
LCS length: 6
LCS length for bonus: 6 (LCS to gold answer 1)
Adapted PLCS (α = 1.0): (6 + 6) / (7 + 6) = 0.92
Adapted RLCS (α = 1.0): (6 + 6) / (12 + 6) = 0.67
Vanilla ROUGE-L2: 0.59
Adapted ROUGE-L: 0.78

For entity answers, the bonus attached to the nu-
merator and denominator of RLCS and PLCS is
given as β

∑
e∈entities length(e) ∗ I(e ⊆ c), indi-

cating the length sum of gold entities appearing in
candidate answer. An example is given below.

Example 4 (Adapted ROUGE-L for entity answer)
Question: How long did it take for Qin Dynasty to unify
China?
Predicted answer: Qin unified China in 221 BC after the war
against other kingdoms which lasted ten years.
Gold answer: Qin unified China in ten years, from 230 BC
to 221 BC.
Gold entity set: ten years, 230 BC, 221 BC
LCS length: 7
Entity length sum for bonus: 4
Adapted PLCS (α = 1.0): (7 + 4) / (17 + 4) = 0.52
Adapted RLCS (α = 1.0): (7 + 4) / (14 + 4) = 0.61
Vanilla ROUGE-L: 0.45
Adapted ROUGE-L: 0.56

With the help of bonus items, our adapted met-
rics give more preference to correct yes-no and
entity answers. For the yes-no question in exam-
ple 1 & 3, a trivial extracted answer may occur as
“exercise with low intensity” which does not con-
tain yes-no opinion. The adapted ROUGE-L can
better distinguish it from the correct one we give.
With α set to 1.0, the right answer can achieve
0.28 higher point over the trivial one. When using

2For simplicity, in this section we compute harmonic av-
erage to get ROUGE-L.

vanilla ROUGE-L, the advantage narrows to only
0.09. For the entity question in example 2 & 4,
we consider a shorter candidate answer “Qin uni-
fied China in 221 BC after the war against other
kingdoms”. This answer lacks key information
and should be assigned a lower score. However,
this answer is preferred under vanilla ROUGE-L
compared with the longer candidate in example 4
(0.53 vs 0.45). This problem will be rectified if the
adapted ROUGE-L is employed with β > 2.6.

4 Statistical Analysis

To demonstrate the effectiveness of our adapta-
tions, we measure the correlation of our metrics
with human judgment quantitatively in compari-
son with original ROUGE-L and BLEU. 500 ques-
tions are sampled from DuReader, which cover
yes-no, entity and description question types. We
collect predicted answers to these questions from
the submissions of 5 different MRC systems in
MRC2018 Challenge3. Generated opinion labels
are attached with yes-no candidate answers, which
is the common case in DuReader evaluation. The
human judgment of these candidate answers is ob-
tained by assigning 2 annotators to give the 1-
5 score on each candidate. The overall human
judgment score is defined as the average of scores
across the questions. The correlation is analysed
on both single question type and all the types.
Meanwhile, the performances of these metrics
are compared on both single question and overall
score levels. The details of statistical analysis are
given below.

4.1 Human Judgment
The samples we select include 201 yes-no, 201
entity and 98 description questions, with a total
of 2500 candidate answers. The criterion of man-
ual scoring is mainly based on whether the answer
satisfies the demand of question, the coverage of
key-points and answer conciseness. In detail, an-
notators give 1-5 scores according to the following
guideline:

• 5-score: perfectly answer the question with
little redundant information
• 4-score: sufficiently answer the question

with unvital missing or some redundancy
• 3-score: the answer is a little insufficient,

such as only giving opinion without support-
ing context in yes-no answer

3http://mrc2018.cipsc.org.cn/

http://mrc2018.cipsc.org.cn/


102

• 2-score: vital missing or error exists
• 1-score: totally irrelevant

We follow the notion of Dang et al. (2007),
which emphasizes the coverage of vital key-point
in the answer. Annotators are asked to treat yes-
or-no opinion and important supporting informa-
tion for yes-no questions and gold entities for en-
tity questions as nuggets and put more weight on
them for scoring.

To ensure the quality and credibility of the hu-
man judgment, we measure the argeement be-
tween the 2 annotators. Table 1 shows the Pearson
correlation coefficients for each question type and
on overall.

Yes-No Entity Description Overall
PCC4 0.878 0.906 0.870 0.891

Table 1: Pearson correlation coefficients (PCC)
between annotators.

We can see the annotators achieve high agree-
ment on candidate judgment, which indicates the
practicability of our scoring criterion and the reli-
ability of the human annotation.

4.2 Effectiveness of Adaptations

The correlation between automatic and manual
evaluation metrics is calculated on both single
question and overall score levels. On single ques-
tion level, each candidate answer is taken as a
sample to be scored by the two metrics and score
pairs are collected across all the samples to com-
pute PCC. On the overall level, predicted answers
to 30 sampled questions for an MRC system are
scored together and the resulting automatic and
human overall score pairs are utilized for the cal-
culation of PCCs. The sampling is performed 100
times and 5 systems are sampled the same ques-
tions each time. Hence each overall level PCC is
computed on 500 samples.

In practice, we use cumulative BLEU-4 as the
implementation of BLEU, which follows the offi-
cial benchmark of DuReader. For ROUGE-L, γ
is set to 1.2 since we think the precision and re-
call are both of importance. The mean score given
by 2 annotators are used to represent human judg-
ment. For our adapted metrics, we set the weight
of yes-no bonus α to 2.0 and that of entity bonus
β to 1.0.

4All the PCCs are significant in t-test with p-value< 0.05.

On single question level, the pearson correla-
tion coefficients between automatic metrics and
human judgment are given in Table 2. The adapted
ROUGE-L achieves best performance on correla-
tion to human judgment, both on single yes-no or
entity question type and on overall.

Yes-No Entity Overall
Adapted ROUGE-L 0.540 0.620 0.570

ROUGE-L 0.493 0.491 0.504
Adapted BLEU-4 0.478 0.469 0.481

BLEU-4 0.459 0.397 0.450

Table 2: PCCs between various automatic metrics
and human judgment for different question types
on single question level.

Our adaptations bring substantial gain on PCCs
for both ROUGE-L and BLEU-4 on single ques-
tion level. To check the significance of these re-
sults, we follow the paired bootstrap resampling
test mentioned in Koehn (2004). For a pair of
metrics, samples are bootstrapped 100 times and
in each time the PCCs are recomputed and com-
pared. For both ROUGE-L and BLEU-4, the
paired test between original and adapted versions
are performed on yes-no, entity and overall sets.
In all the 6 tests, the adapted metric shows signifi-
cant better performance than the original one.

We also calculate PCCs between automatic and
human metrics on overall score level. The results
are shown in Table 3. Similar to single question
level, adapted ROUGE-L still gains the highest
correlation to human overall judgment. In this
task, we notice that ROUGE is much more effec-
tive than BLEU, which may reflect the importance
of recall in MRC evaluation. For the compari-
son between adapted and vanilla metrics, adapted
ROUGE-L performs better than vanilla version on
every question type. However, our adapted BLEU-
4 only works better on evaluating entity answers,
which is different from the result on single ques-
tion level. We think it may be due to the peculiar
way BLEU employs to get overall score for mul-
tiple questions, which was discussed as the “de-
composability” problem of BLEU in Chiang et al.
(2008). This issue will be explored in our future
work.

4.3 Impacts of Bonus Weights

We further inspect the impact of bonus weights
on metric performance. In Figure 1, the value
of yes-no bonus weight α is changed with entity



103

Yes-No Entity Overall
Adapted ROUGE-L 0.702 0.884 0.792

ROUGE-L 0.664 0.839 0.760
Adapted BLEU-4 0.536 0.686 0.646

BLEU-4 0.571 0.668 0.681

Table 3: PCCs between various automatic metrics
and human judgment for different question types
on overall score level.

0.475

0.500

0.525

0.550

0 1 2 3 4

Alpha

P
C

C
 o

n
 Y

e
s−

N
o

 A
n

sw
e

rs Metrics

Adapted
ROUGE−L

Adapted
BLEU−4

Figure 1: The PCC on yes-no answers w.r.t the
variation of α.

bonus weight β fixed to 1.0. The single-question
level PCCs of adapted BLEU-4 and ROUGE-L
on yes-no answers are plotted w.r.t the variation
of α. We can see that the introduction of yes-
no bonus brings positive effect for BLEU and
ROUGE. Meanwhile, the PCCs of these metrics
increase with α monotonically.

Similarly, Figure 2 shows the single-question
level PCCs of adapted BLEU-4 and ROUGE-L on
entity answers w.r.t the variation of β, in which α
is set to 2.0. The effect of entity bonus is also pos-
itive and increases with β monotonically. In future
work, we will further look into the issue of select-
ing proper bonus weights.

5 Conclusion

For question answering MRC tasks, automatic
evaluation metrics are commonly based on mea-
suring lexical overlap, such as BLEU and
ROUGE. However, in some cases, we notice that
these automatic evaluation metrics may be biased
from human judgment, especially for yes-no and
entity questions. We think it may mislead the de-
velopment of real scene MRC systems.

In this paper, we propose some adaptations to
ROUGE and BLEU metrics for better evaluat-

0.4

0.5

0.6

0.7

0 1 2 3

Beta

P
C

C
 o

n
 E

n
tit

y 
A

n
sw

e
rs

Metrics

Adapted
ROUGE−L

Adapted
BLEU−4

Figure 2: The PCC on entity answers w.r.t the vari-
ation of β.

ing yes-no and entity answers. Two bonus terms
are introduced into the computation of original
metrics. These terms are also based on lexical
overlap. The statistical analysis shows that our
adaptations achieve higher correlation to human
judgment compared with original ROUGE-L and
BLEU, proving the effectiveness of our methodol-
ogy. In the future, our work will cover more ques-
tion types and more MRC datasets. We hope our
exploration can bring more research attention to
the design of MRC evaluation metrics.

6 Acknowledgement

We thank the anonymous reviewers for their in-
sightful comments on this paper. This work
was partially supported by National Natural Sci-
ence Foundation of China (61572049) and Baidu-
Peking University Joint Project.

References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An

automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings
of the acl workshop on intrinsic and extrinsic evalu-
ation measures for machine translation and/or sum-
marization. pages 65–72.

Eric Breck, John D Burger, Lisa Ferro, Lynette
Hirschman, David House, Marc Light, and Inderjeet
Mani. 2000. How to evaluate your question answer-
ing system every day and still get real work done.
arXiv preprint cs/0004008 .

David Chiang, Steve DeNeefe, Yee Seng Chan, and
Hwee Tou Ng. 2008. Decomposability of translation
metrics for improved evaluation and efficient algo-
rithms. In Proceedings of the Conference on Empir-



104

ical Methods in Natural Language Processing. As-
sociation for Computational Linguistics, pages 610–
619.

Hoa Trang Dang, Diane Kelly, and Jimmy J Lin. 2007.
Overview of the trec 2007 question answering track.
In Trec. volume 7, page 63.

Wei He, Kai Liu, Yajuan Lyu, Shiqi Zhao, Xinyan
Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiao-
qiao She, Xuan Liu, et al. 2017. Dureader:
a chinese machine reading comprehension dataset
from real-world applications. arXiv preprint
arXiv:1711.05073 .

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems. pages 1693–
1701.

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. 2015. The goldilocks principle: Reading
children’s books with explicit memory representa-
tions. arXiv preprint arXiv:1511.02301 .

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
the 2004 conference on empirical methods in natural
language processing.

Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard Hovy. 2017. Race: Large-scale reading
comprehension dataset from examinations. arXiv
preprint arXiv:1704.04683 .

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. Text Summarization
Branches Out .

Feifan Liu and Yang Liu. 2008. Correlation between
rouge and human evaluation of extractive meeting
summaries. In Proceedings of the 46th annual meet-
ing of the association for computational linguistics
on human language technologies: Short papers. As-
sociation for Computational Linguistics, pages 201–
204.

Ani Nenkova, Rebecca Passonneau, and Kathleen
McKeown. 2007. The pyramid method: Incorpo-
rating human content selection variation in summa-
rization evaluation. ACM Transactions on Speech
and Language Processing (TSLP) 4(2):4.

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine
reading comprehension dataset. arXiv preprint
arXiv:1611.09268 .

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics. Association for Computational
Linguistics, pages 311–318.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250 .

Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. Mctest: A challenge dataset for
the open-domain machine comprehension of text.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing. pages
193–203.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2016. Bidirectional attention
flow for machine comprehension. arXiv preprint
arXiv:1611.01603 .

Ellen M Voorhees. 2003. Overview of trec 2003. In
Trec. pages 1–13.

Ellen M Voorhees and DM Tice. 2000. Overview of
the trec-9 question answering track. In TREC.

Ellen M Voorhees et al. 1999. The trec-8 question an-
swering track report. In Trec. volume 99, pages 77–
82.

Shuohang Wang and Jing Jiang. 2016. Machine com-
prehension using match-lstm and answer pointer.
arXiv preprint arXiv:1608.07905 .

Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang,
and Ming Zhou. 2017. Gated self-matching net-
works for reading comprehension and question an-
swering. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers). volume 1, pages 189–198.

Caiming Xiong, Victor Zhong, and Richard Socher.
2016. Dynamic coattention networks for question
answering. arXiv preprint arXiv:1611.01604 .


