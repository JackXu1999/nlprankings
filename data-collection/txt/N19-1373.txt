



















































Dialogue Act Classification with Context-Aware Self-Attention


Proceedings of NAACL-HLT 2019, pages 3727–3733
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

3727

Dialogue Act Classification with Context-Aware Self-Attention

Vipul Raheja Joel Tetreault
Grammarly

firstname.lastname@grammarly.com

Abstract

Recent work in Dialogue Act classification
has treated the task as a sequence labeling
problem using hierarchical deep neural net-
works. We build on this prior work by lever-
aging the effectiveness of a context-aware self-
attention mechanism coupled with a hierarchi-
cal recurrent neural network. We conduct ex-
tensive evaluations on standard Dialogue Act
classification datasets and show significant im-
provement over state-of-the-art results on the
Switchboard Dialogue Act (SwDA) Corpus.
We also investigate the impact of different
utterance-level representation learning meth-
ods and show that our method is effective at
capturing utterance-level semantic text repre-
sentations while maintaining high accuracy.

1 Introduction

Dialogue Acts (DAs) are the functions of ut-
terances in dialogue-based interaction (Austin,
1975). A DA represents the meaning of an utter-
ance at the level of illocutionary force, and hence,
constitutes the basic unit of linguistic communica-
tion (Searle, 1969). DA classification is an impor-
tant task in Natural Language Understanding, with
applications in question answering, conversational
agents, speech recognition, etc. Examples of DAs
can be found in Table 1. Here we have a conver-
sation of 7 utterances between two speakers. Each
utterance has a corresponding label such as Ques-
tion or Backchannel.

Early work in this field made use of statis-
tical machine learning methods and approached
the task as either a structured prediction or text
classification problem (Stolcke et al., 2000; Ang
et al., 2005; Zimmermann, 2009; Surendran and
Levow, 2006). Many recent studies have pro-
posed deep learning models for the DA classifi-
cation task with promising results (Lee and Der-
noncourt, 2016; Khanpour et al., 2016; Ortega and

Speaker Utterance DA label
A Okay. Other
A Um, what did you do this

weekend? Question
B Well, uh, pretty much spent

most of my time in the yard. Statement
B [Throat Clearing] Non Verbal
A Uh-Huh. Backchannel
A What do you have planned

for your yard? Question
B Well, we’re in the process

of, revitalizing it. Statement

Table 1: A snippet of a conversation sample from the
SwDA Corpus. Each utterance has a corresponding di-
alogue act label.

Vu, 2017). However, most of these approaches
treat the task as a text classification problem, treat-
ing each utterance in isolation, rendering them un-
able to leverage the conversation-level contextual
dependence among utterances. Knowing the text
and/or the DA labels of the previous utterances can
assist in predicting the current DA state. For in-
stance, in Table 1, the Answer or Statement dialog
acts often follow Question type utterances.

This work draws from recent advances in NLP
such as self-attention, hierarchical deep learning
models, and contextual dependencies to produce a
dialogue act classification model that is effective
across multiple domains. Specifically, we propose
a hierarchical deep neural network to model dif-
ferent levels of utterance and dialogue act seman-
tics, achieving state-of-the-art performance on the
Switchboard Dialogue Act Corpus. We demon-
strate how performance can improve by leverag-
ing context at different levels of the model: previ-
ous labels for sequence prediction (using a CRF),
conversation-level context with self-attention for
utterance representation learning, and character
embeddings at the word-level. Finally, we explore
different ways to learn effective utterance repre-



3728

sentations, which serve as the building blocks of
our hierarchical architecture for DA classification.

2 Related Work

A full review of all DA classification methods is
outside the scope of the paper, thus we focus on
two main classes of approaches which have domi-
nated recent research: those that treat DA classifi-
cation as a text classification problem, where each
utterance is classified in isolation, and those that
treat it as a sequence labeling problem.
Text Classification: Lee and Dernoncourt (2016)
build a vector representation for each utterance,
using either a CNN or RNN, and use the preceding
utterance(s) as context to classify it. Their model
was extended by Khanpour et al. (2016) and Or-
tega and Vu (2017). Shen and Lee (2016) used a
variant of the attention-based encoder for the task.
Ji et al. (2016) use a hybrid architecture, combin-
ing an RNN language model with a latent variable
model.
Sequence Labeling: Kalchbrenner and Blunsom
(2013) used a mixture of sentence-level CNNs and
discourse-level RNNS to achieve state-of-the-art
results on the task. Recent works (Li and Wu,
2016; Liu et al., 2017) have increasingly employed
hierarchical architectures to learn and model mul-
tiple levels of utterance and DA dependencies.
Kumar et al. (2018), Chen et al. (2018) and Tran
et al. (2017) used RNN-based hierarchical neural
networks, using different combinations of tech-
niques like last-pooling or attention mechanism
to encode sentences, coupled with CRF decoders.
Chen et al. (2018) achieved the highest perfor-
mance to date on the two datasets for this task.

Our work extends these hierarchical models and
leverages a combination of techniques proposed
across these prior works (CRF decoding, contex-
tual attention, and character-level word embed-
dings) with self-attentive representation learning,
and is able to achieve state-of-the-art performance.

3 Model

The task of DA classification takes a conversa-
tion C as input, which is a varying length se-
quence of utterances U = {u1, u2, ...uL}. Each
utterance ui ∈ U , in turn, is a sequence of vary-
ing lengths of words {w1i , w2i , ..., w

Ni
i }, and has

a corresponding target label yi ∈ Y . Hence,
each conversation (i.e. a sequence of utterances)
is mapped to a corresponding sequence of target

                

21

⃗21

⃖21

⃗22

⃖22

⃗23

⃖23

21

21 22 23

⃗1

⃖1

⃗2

⃖2

⃗3

⃖3

1 2 3

1

 1   2   3  ...   ...             n

22 23

how

Words

Embeddings

Bi-Directional 
RNN

Concatenate

2 3

Context-aware 
Self-Attention

Fully 
Connected

Bi-Directional 
RNN

Concatenate

CRF

21 3

21 22

are
22 23

you
23

Figure 1: Model Architecture

labels Y = {y1, y2, ..., yL}, which represents the
DAs associated with the corresponding utterances.

Figure 1 shows the overall architecture of
our model, which involves three main compo-
nents: (1) an utterance-level RNN that encodes
the information within the utterances at the word
and character-level; (2) a context-aware self-
attention mechanism that aggregates word repre-
sentations into utterance representations; and (3) a
conversation-level RNN that operates on the utter-
ance encoding output of the attention mechanism,
followed by a CRF layer to predict utterance la-
bels. We describe them in detail below.

3.1 Utterance-level RNN

For each word in an utterance, we combine two
different word embeddings: GloVe (Pennington
et al., 2014) and pre-trained ELMo representations
(Peters et al., 2018) with fine-tuned task-specific
parameters, which have shown superior perfor-
mance in a wide range of tasks. The word embed-
ding is then concatenated with its CNN-based 50-
D character-level embedding (Chiu and Nichols,
2016; Ma and Hovy, 2016) to get the complete
word-level representations. The motivation behind
incorporating subword-level information is to in-
fer the lexical features of utterances and named
entities better.



3729

The word representation layer is followed by a
bidirectional GRU (Bi-GRU) layer. Concatenating
the forward and backward outputs of the Bi-GRU
generates the utterance embedding that serves as
input to the utterance-level context-aware self-
attention mechanism which learns the final utter-
ance representation.

3.2 Context-aware Self-attention
Self-attentive representations encode a variable-
length sequence into a fixed size, using an at-
tention mechanism that considers different posi-
tions within the sequence. Inspired by Tran et al.
(2017), we use the previous hidden state from the
conversation-level RNN (Section 3.3), which pro-
vides the context of the conversation so far, and
combine it with the hidden states of all the con-
stituent words in an utterance, into a self-attentive
encoder (Lin et al., 2017), which computes a 2D
representation of each input utterance. We fol-
low the notation originally presented in Lin et al.
(2017) to explain our modification of their self-
attentive sentence representation below.

An utterance ui, which is a sequence of n
words {w1i , w2i , ...wni }, is mapped into an embed-
ding layer, resulting in a d-dimensional word em-
bedding for every word. It is then fed into a
bidirectional-GRU layer, whose hidden state out-
puts are concatenated at every time step.

−→
hji =

−−−→
GRU(wji ,

−−→
hj−1i ) (1)

←−
hji =

←−−−
GRU(wji ,

←−−
hj+1i ) (2)

hji = concat(
−→
hji ,
←−
hji ) (3)

Hi = {h1i ,h2i , ...hni } (4)

Hi represents the n GRU outputs of size 2u (u
is the number of hidden units in a unidirectional
GRU).

The contextual self-attention scores are then
computed as follows:

Si = Ws2tanh(Ws1H
T
i +Ws3

−−→gi−1 + b) (5)

Here, Ws1 is a weight matrix with a shape of
da × 2u, Ws2 is a matrix of parameters of shape
r × da, where r and da are hyperparameters we
can set arbitrarily, and Ws3 is a parameter ma-
trix of shape da×k for the conversational context,
where k is another hyperparameter that is the size
of a hidden state in the conversation-level RNN
(size of −−→gi−1), and b is a vector representing bias.

Equation 5 can then be treated as a 2-layer MLP
with bias, with da hidden units, Ws1,Ws2 and Ws3
as weight parameters. The scores Si are mapped
into a probability matrix Ai by means of a softmax
function:

Ai = softmax(Si) (6)

which is then used to obtain a 2-d representation
Mi of the input utterance, using the GRU hidden
states Hi according to the attention weights pro-
vided by Ai as follows:

Mi = AiHi (7)

This 2-d representation is then projected to a
1-d embedding (denoted as hi), using a fully-
connected layer. The conversation-level GRU then
operates over this 1-d utterance embedding, and
hence, we can represent gi as:

−→gi =
−−−→
GRU(hi,

−−→gi−1) (8)

←−gi =
−−−→
GRU(hi,

−−→gi+1) (9)

gi = concat(
−→gi ,←−gi ) (10)

gi then provides the conversation-level context
used to learn the attention scores and 2-d repre-
sentation (Mi+1) for the next utterance in the con-
versation (hi+1).

3.3 Conversation-level RNN

The utterance representation hi from the previous
step is passed on to the conversation-level RNN,
which is another bidirectional GRU layer used to
encode utterances across a conversation. The hid-
den states −→gi and ←−gi (Figure 1) are then concate-
nated to get the final representation gi of each ut-
terance, which is further propagated to a linear
chain CRF layer. The CRF layer considers the
correlations between labels in context and jointly
decodes the optimal sequence of utterance labels
for a given conversation, instead of decoding each
label independently.

4 Data

We evaluate the classification accuracy of our
model on the two standard datasets used for DA
classification: the Switchboard Dialogue Act Cor-
pus (SwDA) (Jurafsky et al., 1997) consisting of
43 classes, and the 5-class version of the ICSI
Meeting Recorder Dialogue Act Corpus (MRDA)
introduced in (Ang et al., 2005). For both datasets,



3730

Dataset |C| |V| Train Validation Test
MRDA 5 12k 78k 16k 15k
SwDA 43 20k 193k 23k 5k

Table 2: Number of utterances by dataset. |C| denotes
number of classes and |V| is the vocabulary size.

we use the train, validation and test splits as de-
fined in Lee and Dernoncourt (2016).

Table 2 shows the statistics for both
datasets. They are highly imbalanced in
terms of class distribution, with the DA
classes Statement-non-opinion and
Acknowledge/Backchannel in SwDA and
Statement in MRDA making up over 50% of
the labels in each set.

5 Results

5.1 Dialogue Act Classification

We compare the classification accuracy of our
model against several other recent methods (Ta-
ble 3).1 Four approaches (Chen et al., 2018; Tran
et al., 2017; Ortega and Vu, 2017; Shen and Lee,
2016) use attention in some form to model the con-
versations, but none of them have explored self-
attention for the task. The last three use CRFs
in the final layer of sequence labeling. Only one
other method (Chen et al., 2018) uses character-
level word embeddings. All models and their vari-
ants were trained ten times and we report the av-
erage test performance. Our model outperforms
state-of-the-art methods by 1.6% on SwDA, the
primary dataset for this task, and comes within
0.6% on MRDA. It also beats a TF-IDF GloVe
baseline (described in Section 5.2) by 16.4% and
12.2%, respectively.

The improvements that the model is able to
make over the other methods are significant, how-
ever, the gains on MRDA still fall short of the
state-of-the-art by 0.6%. This can mostly be at-
tributed to the conversation/context lengths and la-
bel noise at the conversation level. Conversations
in MRDA (1493 utterances on average) are signifi-
cantly longer than in SwDA (271 utterances on av-
erage). In spite of having nearly 12% the number

1Contemporaneous to this submission, (Li et al., 2018;
Wan et al., 2018; Ravi and Kozareva, 2018) proposed differ-
ent approaches for the task. We do not focus on them here per
NAACL 2019 guidelines, however note that our system out-
performs the first two. (Ravi and Kozareva, 2018) bypasses
the need for complex networks with huge parameters but its
overall accuracy is 4.2% behind our system, despite being
0.2% higher on SwDA.

Model SwDA MRDA
TF-IDF GloVe 66.5 78.7
Kalchbrenner and Blunsom (2013) 73.9 -
Lee and Dernoncourt (2016) 73.9 84.6
Khanpour et al. (2016) 75.8 86.8
Ji et al. (2016) 77.0 -
Shen and Lee (2016) 72.6 -
Li and Wu (2016) 79.4 -
Ortega and Vu (2017) 73.8 84.3
Tran et al. (2017) 74.5 -
Kumar et al. (2018) 79.2 90.9
Chen et al. (2018) 81.3 91.7
Our Method 82.9 91.1
Human Agreement 84.0 -

Table 3: DA Classification Accuracy

of labels (5 vs 43) compared to SwDA, MRDA has
6 times the normalized label entropy in its data.
Consequently, due to the noise in label depen-
dencies, and hence, in the inherent conversational
structure, the model is not able to yield as big of
a gain on the MRDA as it does on the SwDA.
Consequently, learning long-range dependencies
is a challenge because of noisier and longer path
lengths in the network. This is illustrated in Fig-
ures 2 and 3, which show for every class, the vari-
ation between the entropy of the previous label
in a conversation, and the accuracy of that class.
MRDA was found to have a high negative cor-
relation2 (-0.68) between previous label entropy
and accuracy, indicating the impact of label noise,
which was compounded by longer conversations.
On the other hand, SwDA was found to have a low
positive correlation (+0.22), which could be com-
pensated by significantly shorter conversations.

5.2 Utterance Representation Learning
One of the primary motivations for this work was
to investigate whether one can improve perfor-
mance by learning better representations for utter-
ances. To address this, we retrained our model
by replacing the utterance representation learn-
ing (utterance-level RNN + context-aware self-
attention) component with various sentence rep-
resentation learning methods (either pre-training
them or learning jointly), and feeding them into
the conversation-level recurrent layers in the hier-
archical model, so that the performance is indica-
tive of the quality of utterance representations.

There are three main categories of utterance
representation learning approaches: (i) the base-
line which uses a TF-IDF weighted sum of
GloVe word embeddings; (ii) pre-trained on cor-

2Pearson’s r



3731

Method SwDA MRDA
Baseline
TF-IDF GloVe 66.5 78.7
Pre-trained on Corpus
Skip Thought Vectors 72.6 82.8
Paragraph vectors 72.5 82.6
Joint Learning
RNN-Encoder 74.8 85.7
Bi-RNN-LastState 76.2 85.4
Bi-RNN-MaxPool 77.6 86.7
CNN 76.9 84.5
Bi-RNN + Attention 80.1 87.7

+ Context 81.8 89.2
Bi-RNN + Self-attention 81.1 88.6

+ Context 82.9 91.1

Table 4: Performance of utterance representation
methods when integrated with the hierarchical model

pus, where we first learn utterance representa-
tions on the corpus using Skip-Thought Vectors
(Kiros et al., 2015) and Paragraph Vectors (Le and
Mikolov, 2014), and then use them with the rest of
the model; (iii) jointly learned with the DA classi-
fication task. Table 4 describes the performance of
different utterance representation learning meth-
ods when combined with the overall architecture
on both datasets.

Introducing the word-level attention mecha-
nism (Yang et al., 2016) enables the model
to learn better representations by attending to
more informative words in an utterance, result-
ing in better performance (Bi-RNN + Attention).
The self-attention mechanism (Bi-RNN + Self-
attention) leads to even greater overall improve-
ments. Adding context information (previous re-
current state of the conversation) boosts perfor-
mance significantly.

A notable aspect of our model is how contex-
tual information is leveraged at different levels of
the sequence modeling task. The combination of
conversation-level contextual states for utterance-
representation learning (+ Context) and a CRF at
the conversation level to further inform conversa-
tion sequence modeling, leads to a collective per-
formance improvement. This is particularly pro-
nounced on the SwDA dataset: the two variants
of the context-aware attention models (Bi-RNN +
Attention + Context and Bi-RNN + Self-attention
+ Context) have significant performance gains.

6 Conclusion

We developed a model for DA classification with
context-aware self-attention, which significantly
outperforms earlier models on the commonly-used

Figure 2: Previous Label Entropy vs. Accuracy on the
SwDA Dataset

Figure 3: Previous Label Entropy vs. Accuracy on the
MRDA Dataset

SwDA dataset and is very close to state-of-the-art
on MRDA. We experimented with different utter-
ance representation learning methods and showed
that utterance representations learned at the lower
levels can impact the classification performance at
the higher level. Employing self-attention, which
has not previously been applied to this task, en-
ables the model to learn richer, more effective ut-
terance representations for the task.

As future work, we would like to experiment
with other attention mechanisms such as multi-
head attention (Vaswani et al., 2017), directional
self-attention (Shen et al., 2018a), block self-
attention (Shen et al., 2018b), or hierarchical at-
tention (Yang et al., 2016), since they have been
shown to address the limitations of vanilla atten-
tion and self-attention by either incorporating in-
formation from different representation subspaces
at different positions to capture both local and
long-range context dependencies, encoding tem-
poral order information, or by attending to context
dependencies at different levels of granularity.

Acknowledgements

The authors would like to thank Dimitris Alikanio-
tis, Maria Nadejde and Courtney Napoles for their
insightful discussions, and the anonymous review-
ers for their helpful comments.



3732

References
Jeremy Ang, Yang Liu, and Elizabeth Shriberg. 2005.

Automatic dialog act segmentation and classifica-
tion in multiparty meetings. In 2005 IEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing, ICASSP ’05, pages 1061–1064.

John Langshaw Austin. 1975. How to do things with
words. Oxford university press.

Zheqian Chen, Rongqin Yang, Zhou Zhao, Deng Cai,
and Xiaofei He. 2018. Dialogue act recognition via
crf-attentive structured network. In The 41st In-
ternational ACM SIGIR Conference on Research &
Development in Information Retrieval, SIGIR ’18,
pages 225–234. ACM.

Jason Chiu and Eric Nichols. 2016. Named entity
recognition with bidirectional lstm-cnns. Transac-
tions of the Association for Computational Linguis-
tics, 4:357–370.

Yangfeng Ji, Gholamreza Haffari, and Jacob Eisen-
stein. 2016. A latent variable recurrent neural net-
work for discourse-driven language models. In Pro-
ceedings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
332–342. Association for Computational Linguis-
tics.

Dan Jurafsky, Liz Shriberg, and Debra Biasca. 1997.
Switchboard SWBD-DAMSL shallow-discourse-
function annotation coders manual, draft 13. Tech-
nical report, University of Colorado at Boulder
Technical Report 97-02.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse com-
positionality. In Proceedings of the Workshop on
Continuous Vector Space Models and their Compo-
sitionality, pages 119–126. Association for Compu-
tational Linguistics.

Hamed Khanpour, Nishitha Guntakandla, and Rod-
ney Nielsen. 2016. Dialogue act classification in
domain-independent conversations using a deep re-
current neural network. In Proceedings of COLING
2016, the 26th International Conference on Compu-
tational Linguistics: Technical Papers, pages 2012–
2021. The COLING 2016 Organizing Committee.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
Advances in Neural Information Processing Systems
28, pages 3294–3302.

Harshit Kumar, Arvind Agarwal, Riddhiman Dasgupta,
and Sachindra Joshi. 2018. Dialogue act sequence
labeling using hierarchical encoder with CRF. In
Proceedings of the Thirty-Second AAAI Conference
on Artificial Intelligence, (AAAI-18), pages 3440–
3447.

Quoc V. Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Pro-
ceedings of the 31th International Conference on
Machine Learning, ICML 2014, pages 1188–1196.

Ji Young Lee and Franck Dernoncourt. 2016. Sequen-
tial short-text classification with recurrent and con-
volutional neural networks. In Proceedings of the
2016 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 515–520. Asso-
ciation for Computational Linguistics.

Ruizhe Li, Chenghua Lin, Matthew Collinson, Xiao Li,
and Guanyi Chen. 2018. A dual-attention hierarchi-
cal recurrent neural network for dialogue act classi-
fication. CoRR, abs/1810.09154.

Wei Li and Yunfang Wu. 2016. Multi-level gated re-
current neural network for dialog act classification.
In Proceedings of COLING 2016, the 26th Inter-
national Conference on Computational Linguistics:
Technical Papers, pages 1970–1979. The COLING
2016 Organizing Committee.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. In International Conference on Learn-
ing Representations 2017 (Conference Track).

Yang Liu, Kun Han, Zhao Tan, and Yun Lei. 2017. Us-
ing context information for dialog act classification
in dnn framework. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2170–2178. Association for Com-
putational Linguistics.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1064–1074. Association for
Computational Linguistics.

Daniel Ortega and Ngoc Thang Vu. 2017. Neural-
based context representation learning for dialog act
classification. In Proceedings of the 18th Annual
SIGdial Meeting on Discourse and Dialogue, pages
247–252. Association for Computational Linguis-
tics.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543. Associa-
tion for Computational Linguistics.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language

https://doi.org/10.1109/ICASSP.2005.1415300
https://doi.org/10.1109/ICASSP.2005.1415300
https://doi.org/10.1145/3209978.3209997
https://doi.org/10.1145/3209978.3209997
http://aclweb.org/anthology/Q16-1026
http://aclweb.org/anthology/Q16-1026
https://doi.org/10.18653/v1/N16-1037
https://doi.org/10.18653/v1/N16-1037
http://www.stanford.edu/~jurafsky/ws97/manual.august1.html
http://www.stanford.edu/~jurafsky/ws97/manual.august1.html
http://aclweb.org/anthology/W13-3214
http://aclweb.org/anthology/W13-3214
http://aclweb.org/anthology/W13-3214
http://aclweb.org/anthology/C16-1189
http://aclweb.org/anthology/C16-1189
http://aclweb.org/anthology/C16-1189
http://papers.nips.cc/paper/5950-skip-thought-vectors.pdf
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16706
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16706
http://jmlr.org/proceedings/papers/v32/le14.html
http://jmlr.org/proceedings/papers/v32/le14.html
https://doi.org/10.18653/v1/N16-1062
https://doi.org/10.18653/v1/N16-1062
https://doi.org/10.18653/v1/N16-1062
http://arxiv.org/abs/1810.09154
http://arxiv.org/abs/1810.09154
http://arxiv.org/abs/1810.09154
http://aclweb.org/anthology/C16-1185
http://aclweb.org/anthology/C16-1185
https://openreview.net/forum?id=BJC_jUqxe
https://openreview.net/forum?id=BJC_jUqxe
https://doi.org/10.18653/v1/D17-1231
https://doi.org/10.18653/v1/D17-1231
https://doi.org/10.18653/v1/D17-1231
https://doi.org/10.18653/v1/P16-1101
https://doi.org/10.18653/v1/P16-1101
https://doi.org/10.18653/v1/W17-5530
https://doi.org/10.18653/v1/W17-5530
https://doi.org/10.18653/v1/W17-5530
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
https://doi.org/10.18653/v1/N18-1202
https://doi.org/10.18653/v1/N18-1202


3733

Technologies, Volume 1 (Long Papers), pages 2227–
2237. Association for Computational Linguistics.

Sujith Ravi and Zornitsa Kozareva. 2018. Self-
governing neural networks for on-device short text
classification. In Proceedings of the 2018 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 887–893. Association for Com-
putational Linguistics.

John R Searle. 1969. Speech acts: An essay in the phi-
losophy of language, volume 626. Cambridge uni-
versity press.

Sheng-syun Shen and Hung-yi Lee. 2016. Neural at-
tention models for sequence classification: Analysis
and application to key term extraction and dialogue
act detection. In INTERSPEECH 2016, 17th Annual
Conference of the International Speech Communi-
cation Association, pages 2716–2720.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2018a. Disan: Di-
rectional self-attention network for rnn/cnn-free lan-
guage understanding. In Proceedings of the Thirty-
Second AAAI Conference on Artificial Intelligence,
(AAAI-18), the 30th innovative Applications of Arti-
ficial Intelligence (IAAI-18), and the 8th AAAI Sym-
posium on Educational Advances in Artificial Intel-
ligence (EAAI-18), pages 5446–5455.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, and
Chengqi Zhang. 2018b. Bi-directional block self-
attention for fast and memory-efficient sequence
modeling. In International Conference on Learning
Representations.

Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul
Taylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-
beth Shriberg, Daniel Jurafsky, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3).

Dinoj Surendran and Gina-Anne Levow. 2006. Dialog
act tagging with support vector machines and hid-
den markov models. In INTERSPEECH 2006 - IC-
SLP, Ninth International Conference on Spoken Lan-
guage Processing.

Quan Hung Tran, Ingrid Zukerman, and Gholamreza
Haffari. 2017. A hierarchical neural model for learn-
ing sequences of dialogue acts. In Proceedings of
the 15th Conference of the European Chapter of
the Association for Computational Linguistics: Vol-
ume 1, Long Papers, pages 428–437. Association for
Computational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30, pages 5998–6008. Curran Asso-
ciates, Inc.

Yao Wan, Wenqiang Yan, Jianwei Gao, Zhou Zhao,
Jian Wu, and Philip S. Yu. 2018. Improved dynamic
memory network for dialogue act classification with
adversarial training. CoRR, abs/1811.05021.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, pages 1480–1489. Association for Computa-
tional Linguistics.

Matthias Zimmermann. 2009. Joint segmentation and
classification of dialog acts using conditional ran-
dom fields. In INTERSPEECH 2009, 10th Annual
Conference of the International Speech Communi-
cation Association, pages 864–867.

A Supplementary Material

A.1 Training & Hyperparameters
All hyperparameters were selected by tuning one
hyperparameter at a time while keeping the oth-
ers fixed. Validation splits were used for the tun-
ing process. The final set of hyperparameters were
then used to train two different models, one each
on SwDA and MRDA training splits. Table 5 lists
the range of values for each parameter that we ex-
perimented with, and the final value that was cho-
sen. Dropout was applied to the utterance embed-
dings. Early stopping was used on the validation
set with a patience of 15 epochs.

Hyperparams Range of values Final value
Word GloVe 100D GloVe 300D +
Embeddings GloVe 200D ELMo 1024D

GloVe 300D

Word2vec 300D
Word2vec 200D

ELMo 1024D

GloVe 300D +
ELMo 1024D

Word2Vec 300D +
ELMo 1024D

Sentence GRU 20 - 300 50
Size (u)
Utterance GRU 20 - 600 100
Size (k)
Learning Rate 0.01 - 2.0 0.015
Dropout 0.1 - 0.8 0.3
Optimizer SGD, Adam

RMSProp,
Adam

Table 5: Hyperparameter space and tuned values

http://aclweb.org/anthology/D18-1105
http://aclweb.org/anthology/D18-1105
http://aclweb.org/anthology/D18-1105
https://doi.org/10.21437/Interspeech.2016-1359
https://doi.org/10.21437/Interspeech.2016-1359
https://doi.org/10.21437/Interspeech.2016-1359
https://doi.org/10.21437/Interspeech.2016-1359
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16126
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16126
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16126
https://openreview.net/forum?id=H1cWzoxA-
https://openreview.net/forum?id=H1cWzoxA-
https://openreview.net/forum?id=H1cWzoxA-
http://aclweb.org/anthology/J00-3003
http://aclweb.org/anthology/J00-3003
http://aclweb.org/anthology/J00-3003
http://www.isca-speech.org/archive/interspeech_2006/i06_1831.html
http://www.isca-speech.org/archive/interspeech_2006/i06_1831.html
http://www.isca-speech.org/archive/interspeech_2006/i06_1831.html
http://aclweb.org/anthology/E17-1041
http://aclweb.org/anthology/E17-1041
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://arxiv.org/abs/1811.05021
http://arxiv.org/abs/1811.05021
http://arxiv.org/abs/1811.05021
https://doi.org/10.18653/v1/N16-1174
https://doi.org/10.18653/v1/N16-1174
http://www.isca-speech.org/archive/interspeech_2009/i09_0864.html
http://www.isca-speech.org/archive/interspeech_2009/i09_0864.html
http://www.isca-speech.org/archive/interspeech_2009/i09_0864.html

