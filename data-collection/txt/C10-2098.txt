851

Coling 2010: Poster Volume, pages 851–859,

Beijing, August 2010

Imbalanced Classiﬁcation Using Dictionary-based Prototypes and

Hierarchical Decision Rules for Entity Sense Disambiguation

Tingting Mu

National Centre for Text Mining

University of Manchester

Xinglong Wang

National Centre for Text Mining

University of Manchester

tingting.mu@man.ac.uk

xinglong.wang@man.ac.uk

Jun’ichi Tsujii

Department of Computer Science

University of Tokyo

Sophia Ananiadou

National Centre for Text Mining

University of Manchester

tsujii@is.s.u-tokyo.ac.jp

Sophia.Ananiadou@man.ac.uk

Abstract

Entity sense disambiguation becomes dif-
ﬁcult with few or even zero training in-
stances available, which is known as im-
balanced learning problem in machine
learning. To overcome the problem, we
create a new set of reliable training in-
stances from dictionary, called dictionary-
based prototypes. A hierarchical classiﬁ-
cation system with a tree-like structure is
designed to learn from both the prototypes
and training instances, and three different
types of classiﬁers are employed. In addi-
tion, supervised dimensionality reduction
is conducted in a similarity-based space.
Experimental results show our system out-
performs three baseline systems by at least
8.3% as measured by macro F1 score.
Introduction

1
Ambiguities in terms and named entities are a
challenge for automatic information extraction
(IE) systems. The problem is particularly acute
for IE systems targeting the biomedical domain,
where unambigiously identifying terms is of fun-
damental importance. In biomedical text, a term
(or its abbreviation (Okazaki et al., 2010)) may
belong to a wide variety of semantic categories
(e.g., gene, disease, etc.). For example, ER may
denote protein estrogen receptor in one context,
but cell subunit endoplasmic reticulum in another,

not to mention it can also mean emergency room.
In addition, same terms (e.g., protein) may be-
long to many model organisms, due to the nomen-
clature of gene and gene products, where genes
in model organisms other than human are given,
whenever possible, the same names as their hu-
man orthologs (Wain et al., 2002). On the other
hand, public biological databases keep species-
speciﬁc records for the same protein or gene,
making species disambiguation an inevitable step
for assigning unique database identiﬁers to entity
names in text (Hakenberg et al., 2008; Krallinger
et al., 2008).

One way to entity disambiguation is classify-
ing an entity into pre-deﬁned semantic categories,
based on its context (e.g., (Bunescu and Pas¸ca,
2006)). Existing classiﬁers, such as maximum
entropy model, achieved satisfactory results on
the “majority” classes with abundant training in-
stances, but failed on the “minority” ones with few
or even zero training instances, i.e., the knowl-
edge acquisition bottleneck (Agirre and Martinez,
2004). Furthermore, it is often infeasible to cre-
ate enough training data for all existing semantic
classes. In addition, too many training instances
for certain majority classes lead to increased com-
putational complexity for training, and a biased
system ignoring the minority ones. These corre-
spond to two previously addressed difﬁculties in
imbalanced learning: “... either (i) you have far
more data than your algorithms can deal with,

852

and you have to select a sample, or (ii) you have
no data at all and you have to go through an in-
volved process to create them” (Provost, 2000).
Given an entity disambiguation task with imbal-
anced data, this paper explores how to create more
informative training instances for minority classes
and how to improve the large-scale training for
majority classes.

Previous research has shown that words denot-
ing class information in the surrounding context of
an entity can be an informative indicator for dis-
ambiguation (Krallinger et al., 2008; Wang et al.,
2010). Such words are refered to as “cue words”
throughout this paper. For example, to disam-
biguate the type of an entity, that is, whether it
is a protein, gene, or RNA, looking at words such
as protein, gene and RNA are very helpful (Hatzi-
vassiloglou et al., 2001). Similarly, for the task
of species disambiguation (Wang et al., 2010),
the occurrence of mouse p53 strongly suggests
that p53 is a mouse protein. In many cases, cue
words are readily available in dictionaries. Thus,
for the minority classes, instead of creating arti-
ﬁcial training instances by commonly used sam-
pling methods (Haibo and Garcia, 2009), we pro-
pose to create a new set of real training instances
by modelling cue words from a dictionary, called
dictionary-based prototypes. To learn from both
the original training instances and the dictionary-
based prototypes, a hierarchical classiﬁcation sys-
tem with a tree-like structure is designed. Further-
more, to cope with the large number of features
representing each instance, supervised orthogo-
nal locality preserving projection (SOLPP) is con-
ducted for dimensionality reduction, by simulta-
neously preserving the intrinsic structures con-
structed from both the features and labels. A new
set of lower-dimensional embeddings with better
discriminating power is obtained and used as in-
put to the classiﬁer. To cope with the large num-
ber of training instances in some majority classes,
we propose a committee machine scheme to ac-
celerate training speed without sacriﬁcing classi-
ﬁcation accuracy. The proposed method is evalu-
ated on a species disambiguation task, and the em-
pirical results are encouraging, showing at least
8.3% improvement over three different baseline
systems.

2 Related Work

Construction of a classiﬁcation model using su-
pervised learning algorithms is popular for entity
disambiguation. A number of researchers have
tackled entity disambiguation in general text us-
ing wikipedia as a resource to learn classiﬁca-
tion models (Bunescu and Pas¸ca, 2006). Hatzi-
vassiloglou et al. (2001) studied disambiguating
proteins, genes, and RNA in text by training var-
ious classiﬁers using entities with class informa-
tion provided by adjacent cue words. Wang et
al. (2010) proposed a “hybird” system for species
disambiguation, which heuristically combines re-
sults obtained from classifying the context, and
those from modeling relations between cue words
and entities. Although satisfactory performance
was reported, their system incurs higher computa-
tional cost due to syntactic parsing and the binary
relation classiﬁer.

Many imbalanced learning techniques, as re-
viewed by Haibo and Garcia (2009), can also be
used to achieve the same purpose. However, to
our knowledge, there is little research in apply-
ing these machine learning (ML) techniques to en-
tity disambiguation. It is worth mentioning that
although these ML techniques can improve the
learning performance to some extent, they only
consider the information contained in the origi-
nal training instances. The created instances do
not add new information, but instead utilize the
original training information in a more sophisti-
cated way. This motivates us to pursue a differ-
ent method of creating new training instances by
using information from a related and easily ob-
tained source (e.g., a dictionary), similar to trans-
fer learning (Pan and Yang, 2009).

3 Task and Corpus

In this work, we develop an entity disambiguation
technique with the use of cue words, as well as a
general ML algorithm for imbalanced classiﬁca-
tion using a set of newly created dictionary-based
prototypes. These prototypes are represented with
different features from those used by the original
training instances. The proposed method is eval-
uated on a species disambiguation task: given a
text, in which mentions of biomedical named en-

853

tities are annotated, we assign a species identi-
ﬁer to every entity mention. The types of entities
studied in this work are genes and gene products
(e.g., proteins), and we use the NCBI Taxonomy1
(taxon) IDs as species tags and to build the proto-
types. Note that this paper focuses on the task of
species disambiguation and makes the assumption
that the named entities are already recognised.

Consider the following sentence as an exam-
ple: if one searches the proteins (i.e., the under-
lined term) in a protein database, he/she will ﬁnd
they belong to many model organisms. However,
in this particular context, CD200R-CD4d3+4 is
human and mouse protein, while rCD4d3+4 is
a rat one.2 We call such a task of assigning
species identiﬁers to entities, according to context,
as species disambiguation.

and

The amounts of human and mouse
CD200R-CD4d3+4
rCD4d3+4
protein on the microarray spots were
similar as visualized by the red ﬂuo-
rescence of OX68 mAb recognising
the CD4 tag present
in each of the
recombinant proteins.

The informative cue words (e.g., mouse) used
to help species disambiguation are called species
words. In this work, species words are deﬁned as
any word that indicates a model organism and also
appears in the organism dictionaries we use. They
may have various parts-of-speech, and may also
contain multiple tokens (despite the name species
word). For example, “human”, “mice”, “bovine”
and “E. Coli” are all species words. We detect
these words by automatic dictionary lookup: a
word is annotated as a species word if it matches
an entry in a list of organism names. Each entry in
the list contains a species word and its correspond-
ing taxon ID, and the list is merged from two dic-
tionaries:
the NCBI Taxonomy and the UniProt
controlled vocabulary of species.3 The NCBI por-
tion is a ﬂattened NCBI Taxonomy (i.e., without
hierarchy) including only the identiﬁers of genus
and species ranks. In total, the merged list con-

1http://www.ncbi.nlm.nih.gov/sites/entrez?db=

taxon-

omy

2Preﬁx ‘r’ in “rCD4d3+4” indicates that it is a rat protein.
3http://www.expasy.ch/cgi-bin/speclist

tains 356,387 unique species words and 272,991
unique species IDs. The ambiguity in species
words is low: 3.86% of species words map to mul-
tiple IDs, and on average each word maps to 1.043
IDs.

The proposed method was evaluated on the
corpus developed in (Wang et al., 2010), con-
taining 6, 223 genes and gene products, each of
which was manually assigned with either a taxon
ID or an “Other” tag, with human being the
most frequent at 50.30%. With the extracted
features and the species ID tagged by domain
experts, each occurrence of named entities can
be represented as a d-dimensional vector with
a label. Species disambiguation can be mod-
elled as a multi-classiﬁcation task: Given n train-
ing instances {xi}n
i=1, their n × d feature ma-
trix X = [xij] and n-dimensional label vector
y = [y1, y2, . . . , yn]T are used to train a clas-
siﬁer C(·), where xi = [xi1, xi2, . . . , xid]T , yi ∈
{1, 2, . . . , c}, and c denotes the number of ex-
isting species in total. Given m different query
instances { ˆxi}m
i=1, their m × d feature matrix
ˆX = [ˆxij] are used as the input to the trained
classiﬁer, so that their labels can be predicted by
i=1.
{C( ˆxi)}m

We used relatively simple contextual features
because this work was focused on developing a
ML framework. In more detail, we used the fol-
lowing features: 1) 200 words surrounding the en-
tity in question; 2) two nouns and two adjectives
at the entity’s left and right; 3) 5 species words
at the entity’s left and right. In addition, function
words and words that consist of only digits and
punctuations are ﬁltered out. The ﬁnal numeri-
cal dataset consists of 6,227 instances, each rep-
resented by 16,851 binary features and belonging
to one of the 13 classes. The dataset is highly im-
balanced: among the 13 classes, the numbers of
instances in the four majority classes vary from
449 to 3,220, while no more than 20 instances are
contained in the eight minority classes (see Table
1).

854

4 Proposed Method
4.1 Dictionary-based Prototypes
For each existing species, we create a b-
dimensional binary vector, given as pi =
[pi1, pi2, . . . , pib]T , using b different species
words listed in the dictionary as features, which
is called dictionary-based prototype. The binary
value pij denotes whether the jth species word
belongs to the ith species in the dictionary. This
leads to a c × b feature matrix P = [pij] for c
species.
Considering that the species words preceding
and appearing in the same sentence as an en-
tity can be informative indicators for the possible
species of this entity, we create two more m×b bi-
nary feature matrices for the query instances with
the same b species words as features: ˆX1 = [ˆx(1)
ij ]
and ˆX2 = [ˆx(2)
ij denotes whether the
jth species word is the preceding word of the ith
entity, and ˆx(2)
ij denotes whether the jth species
word appears in the same sentence as the ith en-
tity but is not preceding word. Thus, the similar-
ity between each query entity and existing species
can be simply evaluated by calculating the inner-
product between the entity instance and the cor-
responding prototype. This leads to the following
m × c similarity matrix ˆS = [ˆsij]:

ij ], where ˆx(1)

ˆS = θ ˆX1PT + (1 − θ) ˆX2PT ,

(1)

where 0 ≤ θ ≤ 1 is a user-deﬁned parameter con-
trolling the degree of indicating reliability of the
preceding word and the same-sentence word. The
n×c similarity matrix S = [sij] between the train-
ing instances and the species can be constructed in
exactly the same way. Based on empirical expe-
rience, the preceding word indicates the entity’s
species more accurately than the same-sentence
word. Thus, θ is preferred to be set as greater
than 0.5. The obtained similarity matrix will be
used in the nearest neighbour classiﬁer (see Sec-
tion 4.2.1).

Both the original training instances X and the
newly created prototypes P are used to train the
proposed hierarchical classiﬁcation system. Sub-
ject to the nature of the classiﬁer employed, it is
convenient to construct one single feature matrix

Figure 1: Structure of the proposed hierarchical
classiﬁcation system

instead of using X and P individually. Aiming at
keeping the same similarity values between each
entity instance and the species prototype, we con-
struct the following (n+c)×(d+b) feature matrix
for both the training instances and prototypes:

F =(cid:20) X θX1 + (1 − θ)X2

P

0

(cid:21) ,

(2)

where X1 and X2 are constructed in the same way
as ˆX1 and ˆX2 but for training instances. Their cor-
responding label vector is l = [yT , 1, 2, . . . , c]T .

4.2 Hierarchical Classiﬁcation
Multi-stage or hierarchical classiﬁcation (Giusti
et al., 2002; Podolak, 2007; Kurzy´nski, 1988)
is widely used in many complex multi-category
classiﬁcation tasks. Existing research shows such
techniques can potentially achieve right trade-off
between accuracy and resource allocation (Giusti
et al., 2002; Podolak, 2007). Our proposed hier-
archical system has a tree-like structure with three
different types of classiﬁer at nodes (see Figure 1).
Different classes are organized in a hierarchical
order to be classiﬁed based on the corresponding
numbers of available training instances. Letting
ni denote the number of training instances avail-
able in the ithe class excluding the created proto-
types, we categorize the classes as follows:

• Minority Classes (MI): Classes with less
training instances than the threshold: MI =
{i : ni

n < σ1, i ∈ {1, 2 . . . , c}}.

Nearest 
Neighbor 
Classifier

Yes

(IT1)

No

Minority 
Classes

Majority 
Classes

Output: Instances 
with predicted labels 
belonging to MI

Output: 
Instances 
with 
predicted 
labels 
belonging 
to SMA

Small-
scale 

Majority 
Classes

SOLPP-
FLDA 
Classifier

(IT2)

Yes

No

Large-
scale 

Majority 
Classes

Committee 
Classifier

(END)

Note: Definition of
the minority,
majority, small-scale majority, large-
scale majority classes, as well as the
IF-THEN rule 1 (IT1) and IF-THEN rule
2 (IT2) are provided in the paper.

Output: Instances 
with predicted labels 
belonging to LMA

855

n ≥ σ1, i ∈ {1, 2 . . . , c}}.

• Majority Classes (MA): Classes with more
training instances than the threshold: MA =
{i : ni
• Small-scale Majority Classes (SMA): Ma-
jority Classes with less training instances
than the threshold: SMA = {i : ni
n <
σ2, i ∈ MA}.
• Large-scale Majority Classes (LMA): Ma-
jority Classes with more training instances
than the threshold: LMA = {i : ni
n ≥
σ2, i ∈ MA}.

Here, 0 < σ1 < 1 and 0 < σ2 < 1 are size
thresholds set by users. We have MI ∩ MA = ∅,
SMA ∩ LMA = ∅, and SMA ∪ LMA = MA.
The tree-like hierarchical structure of our sys-
tem is determined by MI, MA, SMA, and LMA.
We propose two IF-THEN rules to control the sys-
tem: Given a query instance ˆxi, the level 1 clas-
siﬁer C1 is used to predict whether ˆxi belongs to
MA or a speciﬁc class in MI, which wer call IF-
THEN rule 1 (IT1). If ˆxi belongs to MA, the level
2 classiﬁer C2 is used to predict whether ˆxi be-
longs to LMA or a speciﬁc class in SMA, called
IF-THEN rule 2 (IT2). If ˆxi belongs to LMA, the
level 3 classiﬁer C3 ﬁnally predicts the speciﬁc
class in LMA ˆxi belongs to. We explain in the
following sections how the classiﬁers C1, C2, and
C3 work in detail.
4.2.1 Nearest Neighbour Classiﬁer

The goal of the nearest neighbour classiﬁer, de-
noted by C1, is to decide whether the nearest-
neighbour prototype of the query instance be-
longs to MI. The only used training instances are
our created dictionary-based prototypes {pi}c
i=1
with the label vector [1, 2, . . . , c]T . The nearest-
neighbour prototype of the query instance ˆxi pos-
sesses the maximum similarity to ˆxi:

N N ( ˆxi) = arg max

j=1, 2, ..., c

ˆsij,

(3)

where ˆsij is obtained by Eq. (1). Consequently,
the output of the classiﬁer C1 is given as

C1( ˆxi) =(cid:26) N N ( ˆxi),

0,

If N N ( ˆxi) ∈ MI,
Otherwise.

(4)

The IF-THEN rule 1 can then be expressed as

Action(IT1) =(cid:26) Go to C2,

Stop,

If C1( ˆxi) = 0,
Otherwise.

4.2.2 SOLPP-FLDA Classiﬁer

The goal of the SOLPP-FLDA classiﬁer, de-
noted by C2, is to predict whether the query in-
stance belongs to LMA or a speciﬁc class in SMA.
In this classiﬁer, the used training instances are
the original training entities and the dictionary-
based prototypes, both belonging to MA. The fea-
ture matrix F and the label vector l deﬁned in Sec-
tion 4.1 are used, but with instances from MI re-
moved (we use ˜n to denote the number of remain-
ing training instances, and the same symbol F for
feature matrix). The used label vector ˜l to train C2
should be re-deﬁned as ˜li = li if li ∈ SMA, and 0
otherwise.
First, we propose to implement orthog-
onal
locality preserving projection (OLPP)
(Kokiopoulou and Saad, 2007) in a supervised
manner, leading to SOLPP, to obtain a smaller set
of more powerful features for classiﬁcation. Also,
we conduct SOLPP in a similarity-based feature
space computed from (d + 2b) original features
by employing dot-product based similarity, given
by FFT . As explained later,
to compute the
new features from FFT instead of the original
features F achieves reduced computational cost.
An ˜n× k projection matrix V = [vij] is optimized
in this n-dimensional similarity-based feature
space. The optimal projections are obtained by
minimizing the weighted distances between the
lower-dimensional embeddings so that “similar”
instances are mapped together in the projected
feature space. Mathematically, this leads to the
following constrained optimization problem:
tr[VT FT F(D − W)FFT V],

min

(5)

V∈R˜n×k,
VT V=Ik×k

where W = [wij] denotes the n × n weight ma-
trix with wij deﬁning the degree of “closeness” or
“similarity” between the ith and jth instances, D
is a diagonal matrix with {di =P˜n
i=1 as

Usually, the weight matrix W is deﬁned by
an adjacency graph constructed from the original

the diagonal elements.

j=1 wij}˜n

856

data, e.g. for OLPP. One common way to deﬁne
the adjacency is by including the K-nearest neigh-
bors (KNN) of a given node to its adjacency list,
which is also called the KNN-graph (Kokiopoulou
and Saad, 2007). There are two common ways to
deﬁne the weight matrix: constant value, where
wij = 1 if the ith and jth samples are adjacent,
while wij = 0 otherwise, and Gaussian kernel.
We will denote in the rest of the paper such a
weight matrix computed only from the features
as WX.
Ideally, if the features can accurately
describe all the discriminating characteristics, the
samples that are close or similar enough to each
other should have the same label vectors. How-
ever, when processing real dataset, what may hap-
pen is that, in the d-dimensional feature space,
the data points that are close to each other may
belong to different categories, while on the con-
trary, the data points that are in a distant to each
other may belong to the same category. In the k-
dimensional projected feature space obtained by
OLPP, one may have the same problem. Because
OLPP solves the constrained optimization prob-
lem in Eq.
if two instances are
close or similar to each other in the original fea-
ture space, they will be the same close or simi-
lar to each other in the projected space. To solve
this problem, we decide to modify the “closeness”
or “similarity” between instances in the projected
feature space by considering the label informa-
tion. The following computation of a supervised
weight matrix is used for our SOLPP:
W = (1 − α)WX + αLLT ,

(6)
where 0 ≤ α ≤ 1 is a user-deﬁned parameter
controlling the tradeoff between the label-based
and feature-based neighborhood structures, and
L = [lij] is an ˜n × c binary label matrix with
lij = 1 if the ith instance belongs to the jth class,
and lij = 0 otherwise.

(5) using WX:

The optimal solution of Eq.

(5) is the top
(k + 1)th eigenvectors of the ˜n × ˜n symmetric
matrix FT F(D − W)FFT , corresponding to the
k + 1 smallest eigenvalues, but with the top one
eigenvector removed, denoted by V∗. It is worth
to mention that if the original feature matrix F is
used as the input of SOLPP, one needs to com-
pute the eigen-decomposition of the (d + b) ×

(d + b) symmetric matrix FT (D − W)F. The cor-
responding computation complexity increases in
O((d + b)3), which is unacceptable in practical
when d + b (cid:29) ˜n. The projected features for the
training instances are computed by

Z = FFT V∗.

(7)

Given a different set of m query instances with an
m × (d + b) feature matrix,

ˆF = [ ˆX, θ ˆX1 + (1 − θ) ˆX2],

their embeddings can be easily obtained by

ˆZ = ˆFˆFT V∗.

(8)

(9)

Then, the projected feature matrix Z and label
vector ˜l are used to train a multi-class classiﬁer.
By employing the one-against-all scheme, differ-
ent binary classiﬁers {C(2)
i }i∈SMA∪{0} with label
space {+1, −1} are trained. For the ith class
(i ∈ SMA∪{0}), the training instances belonging
to it are labeled as positive, otherwise negative. In
each binary classiﬁer C(2)
, a separating function

i

f (2)
i

(x) = xT w(2)

i + b(2)

i

(10)

i

i

and bias b(2)

is constructed, of which the optimal values of the
weight vector w(2)
are computed us-
ing Fisher’s linear discriminant analysis (FLDA)
(Fisher, 1936; Mu, 2008). Finally, the output of
the classiﬁer C2 can be obtained by assigning the
most conﬁdent class label to the query instance ˆxi,
with the conﬁdence value indicated by the value of
separating function:

C2( ˆxi) = arg max

j∈SMA∪{0}

f (2)
j

( ˆxi).

(11)

The IF-THEN rule 2 can then be expressed as

Action(IT2) =(cid:26) Go to C3,

Stop,

4.2.3 Committee Classiﬁer

If C2( ˆxi) = 0,
Otherwise.

The goal of the committee classiﬁer, denoted
by C3, is to predict the speciﬁc class in LMA
the query instance belongs to. The used training

857

instances are entities and dictionary-based proto-
types only belonging to LMA. With the same one-
against-all scheme, there are large number of pos-
itive and negative training instances to train a bi-
nary classiﬁer for a class in LMA. To accelerate
the training procedure without sacriﬁcing the ac-
curacy, the following scheme is designed.

Letting ne denote the number of experts in
committee, all the training instances are averagely
divided into ne + 1 groups each containing similar
numbers of training instances from the same class.
The instances in the ith and the (i+1)th groups are
used to train the ith expert classiﬁer. This achieves
overlapped training instances between expert clas-
siﬁers. The output value of C(3)
is not the class in-
dex as used in C2, but the value of the separating
function of the most conﬁdent class, denoted by
f (3)
. Different from the commonly used majority
i
voting rule, we only trust the most conﬁdent ex-
pert. Thus, the output of C3 for a query instance
ˆxi can be obtained by

i

C3( ˆxi) = arg max

j=1, 2, ..., ne

f (3)
j

( ˆxi).

(12)

By using C3, different expert classiﬁers can be
trained in parallel. The total training time is equal
to that of the slowest expert classiﬁer. The more
expert classiﬁers are used, the faster the system is,
however, the less accurate the system may become
due to the decrease of used training instances for
each expert, especially the positive instances in
the case of imbalanced classiﬁcation. This is also
the reason we do not apply the committee scheme
to SMA classes.

5 Experiments
5.1 System Evaluation and Baseline
We evaluate the proposed method using 5-fold
cross validation, with around 4,980 instances for
training, and 1,245 instances for test in each trial.
We compute the F1 score for each species, and
employ macro- and micro- average scheme to
compute performance for all species. Three base-
lines for comparison include:

• Baseline 1 (B2) : Combination of B1 and
the species dictionary using rules employed
in Wang et al. (2010).

• Baseline 2 (B3): The “hybrid” system com-
bining B1,
the dictionary and a relation
model 4 using rules (Wang et al., 2010).

Our hierarchical classiﬁcation system were imple-
mented in two ways:

• HC: Only the training data on its own is used
to train the system.

• HC/D: Both the training data and the
dictionary-based prototypes are used to train
the system.

5.2 Results and Analysis
The proposed system was implemented with θ =
0.8, α = 0.8, ne = 4, and k = 1000. The species
9606, 10090, 7227, and 4932 were categorized as
LMA, the species 10116 as SMA, and the rest sep-
cies as MI. To compute the supervised weight ma-
trix, the percentage of the used KNN in the KNN-
graph was 0.6. Parameters were not ﬁne tuned, but
set based on our empirical experience on previous
classiﬁcation research. As shown in Table 1: HC
and B1 were trained with the same instances and
features, and HC outperformed B1 in both macro
and micro F1. Both HC and B1 obtained zero F1
scores for most minority species, showing that it is
nearly impossible to correctly label the query in-
stances of minority classes, due to lack of training
data. By learning from a related resource, HC/D,
B2, and B3 yielded better macro performance. In
particular, while HC/D and B2 learned from the
same dictionary and training data, HC/D outper-
formed B2 by 19.1% in macro and 2.5% in mi-
cro F1. B3 aimed at improving the macro perfor-
mance by employing computationally expensive
syntactic parsers and also by training an extra re-
lation classiﬁer. With the same goal, HC/D inte-
grated the cue word information into the ML clas-
siﬁer in a more general way, and yielded an 8.3%
improvement over B3, as measured by macro-F1.

• Baseline 1 (B1) : A maximum entropy
model trained with training data only.

4This is an SVM model predicting relations between en-
tities and nearby species words with positive output indicates
species words bear the semantic label of entities.

858

No.
Cat.
Species Name
LMA 3220
Homo sapiens (9606)
LMA 1709
Mus musculus (10090)
Drosophila melanogaster (7227)
LMA 641
Saccharomyces cerevisiae (4932) LMA 499
50
Rattus norvegicus (10116)
18
Escherichia coli K-12 (83333)
Xenopus tropicalis (8364)
8
7
Caenorhabditis elegans (6239)
3
Oryctolagus cuniculus (9986)
3
Bos taurus (9913)
Arabidopsis thaliana (3702)
2
1
Arthropoda (6656)
Martes zibellina (36722)
1
Micro-average
Macro-average

HC
87.39
79.99
86.62
90.24
55.07
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
N/A 85.03
N/A 30.72

SMA
MI
MI
MI
MI
MI
MI
MI
MI
N/A
N/A

HC/D
87.48
79.98
86.35
90.24
69.23
0.00
40.00
22.22
0.00
50.00
0.00
100.00
50.00
85.13
51.96

B1
86.06
79.59
87.96
83.35
48.42
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
83.59
29.42

B2
85.43
80.00
87.02
81.64
64.41
0.00
41.67
28.57
20.00
0.00
0.00
50.00
28.57
83.04
43.64

B3
86.48
80.41
87.37
84.64
59.41
0.00
36.36
22.22
0.00
100.00
66.67
0.00
0.00
83.80
47.97

Table 1: Performance is compared in F1 (%), where “No.” denotes the number of training instances
and “Cat.” denotes the category of species class as deﬁned in Section 4.2.

6 Conclusions and Future Work

Disambiguating bio-entities presents a challenge
for traditional supervised learning methods, due
to the high number of semantic classes and lack of
training instances for some classes. We have pro-
posed a hierarchical framework for imbalanced
learning, and evaluated it on the species disam-
biguation task. Our method automatically builds
training instances for the minority or missing
classes from a cue word dictionary, under the as-
sumption that cue words in the surrounding con-
text of an entity strongly indicate its semantic cat-
egory. Compared with previous work (Wang
et al., 2010; Hatzivassiloglou et al., 2001), our
method provides a more general way to integrate
the cue word information into a ML framework
without using deep linguistic information.

Although the species disambiguation task is
speciﬁc to bio-text, the difﬁculties caused by im-
balanced frequency of different senses are com-
mon in real application of sense disambiguation.
The proposed technique can also be applied to
other domains, providing the availability of a cue
word dictionary that encodes semantic informa-
tion regarding the target semantic classes. Build-
ing such a dictionary from scratch can be chal-
lenging, but may be easier compared to manual

annotation. In addition, such dictionaries may al-
ready exist in specialised domains.

Acknowledgment
The authors would like to thank the biologists who
annotated the species corpus, and National Cen-
tre for Text Mining. Funding: Pﬁzer Ltd.; Joint
Information Systems Committee (to UK National
Centre for Text Mining)

References
Agirre, E. and D. Martinez. 2004. Unsupervised WSD
based on automatically retrieved examples: The im-
portance of bias. In Proceedings of EMNLP.

Bunescu, R. and M. Pas¸ca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
Proceedings of EACL.

Fisher, R. A. 1936. The use of multiple measure-
ments in taxonomic problems. Annals of Eugenics,
7(2):179–188.

Giusti, N., F. Masulli, and A. Sperduti. 2002. Theoret-
ical and experimental analysis of a two-stage system
for classiﬁcation. IEEE Trans. on Pattern Analysis
and Machine Intelligence, 24(7):893–904.

Haibo, H. and E. A. Garcia. 2009. Learning from
IEEE Trans. on Knowledge and

imbalanced data.
Data Engineering, 21(9):1263–1284.

859

Hakenberg, J., C. Plake, R. Leaman, M. Schroeder, and
G. Gonzalez. 2008. Inter-species normalization of
gene mentions with GNAT. Bioinformatics, 24(16).

Hatzivassiloglou, V., PA Dubou´e, and A. Rzhetsky.
2001. Disambiguating proteins, genes, and RNA in
text: a machine learning approach. Bioinformatics,
17(Suppl 1).

Kokiopoulou, E. and Y. Saad.

2007. Orthogonal
neighborhood preserving projections: A projection-
based dimensionality reduction technique.
IEEE
Trans. on Pattern Analysis and Machine Intelli-
gence, 29(12):2143–2156.

Krallinger, M., A. Morgan, L. Smith, F. Leitner,
L. Tanabe, J. Wilbur, L. Hirschman, and A. Valen-
cia. 2008. Evaluation of text-mining systems for
biology: overview of the second biocreative com-
munity challenge. Genome Biology, 9(Suppl 2).

Kurzy´nski, M. W. 1988. On the multistage bayes clas-

siﬁer. Pattern Recognition, 21(4):355–365.

Mu, T. 2008. Design of machine learning algorithms
with applications to breast cancer detection. Ph.D.
thesis, University of Liverpool.

Okazaki, N., S. Ananiadou, and J. Tsujii.

2010.
Building a high quality sense inventory for im-
proved abbreviation disambiguation. Bioinformat-
ics, doi:10.1093/bioinformatics/btq129.

Pan, S. J. and Q. Yang. 2009. A survey on transfer
learning. IEEE Trans. on Knowledge and Data En-
gineering.

Podolak, I. T. 2007. Hierarchical rules for a hierarchi-
cal classiﬁer. Lecture Notes in Computer Science,
4431:749–757.

Provost, F. 2000. Machine learning from imbalanced
data sets 101. In Proc. of Learning from Imbalanced
Data Sets: Papers from the Am. Assoc. for Artiﬁcial
Intelligence Workshop. (Technical Report WS-00-
05).

Wain, H., E. Bruford, R. Lovering, M. Lush,
M. Wright, and S. Povey. 2002. Guidelines for
human gene nomenclature. Genomics, 79(4):464–
470.

Wang, X., J. Tsujii, and S. Ananiadou. 2010. Dis-
ambiguating the species of biomedical named enti-
ties using natural language parsers. Bioinformatics,
26(5):661667.

