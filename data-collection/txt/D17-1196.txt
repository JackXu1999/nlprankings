



















































Towards Quantum Language Models


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1840–1849
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Towards Quantum Language Models

Ivano Basile
Scuola Normale Superiore, Pisa, Italy

ivano.basile@sns.it

Fabio Tamburini
FICLIT - University of Bologna, Italy
fabio.tamburini@unibo.it

Abstract

This paper presents a new approach
for building Language Models using the
Quantum Probability Theory, a Quantum
Language Model (QLM). It mainly shows
that relying on this probability calculus it
is possible to build stochastic models able
to benefit from quantum correlations due
to interference and entanglement. We ex-
tensively tested our approach showing its
superior performances, both in terms of
model perplexity and inserting it into an
automatic speech recognition evaluation
setting, when compared with state-of-the-
art language modelling techniques.

1 Introduction

Quantum Mechanics Theory (QMT) is one of the
most successful theories in modern science. De-
spite its effectiveness in the physics realm, the at-
tempts to apply it in other domains remain quite
limited, excluding, of course, the large quantity of
studies regarding Quantum Information Process-
ing on quantum computers.

Only in recent years some scholars tried to em-
body principles derived from QMT into their spe-
cific fields, for example, by the Information Re-
trieval community (Zuccon et al., 2009; Melucci
and van Rijsbergen, 2011; González and Caicedo,
2011; Melucci, 2015) and in the domain of cog-
nitive sciences and decision making (Khrennikov,
2010; Busemeyer and Bruza, 2012; Aerts et al.,
2013). In the machine learning field (Arjovsky
et al., 2016; Wisdom et al., 2016; Jing et al., 2017)
have used unitary evolution matrices for building
deep neural networks obtaining interesting results,
but we have to observe that their works do not ad-
here to QMT and use unitary evolution operators
in a way not allowed by QMT. In recent years, also

the Natural Language Processing (NLP) commu-
nity started to look at QMT with interest and some
studies using it have already been presented (Bla-
coe et al., 2013; Liu et al., 2013; Tamburini, 2014;
Kartsaklis et al., 2016).

Language models (LM) are basic tools in NLP
used in various applications, such as Automatic
Speech Recognition (ASR), machine translation,
part-of-speech tagging, etc., and were traditionally
modeled by using N-grams and various smoothing
techniques. Among the dozen of tools for comput-
ing N-gram LM, we will refer to CMU-SLM (with
Good-Turing smoothing) (Clarkson and Rosen-
feld, 1997) and IRSTLM (with Linear Witten-Bell
smoothing) (Federico et al., 2008); the latter is the
tool used in Kaldi (Povey et al., 2011b), one of the
most powerful and used open-source ASR pack-
age that we will use for some of the experiments
presented in the following sections.

In recent years new techniques from the Neural
Networks (NN) domain have been introduced in
order to enhance the performances of such models.
Elman recurrent NN, as used in the RNNLM tool
(Mikolov et al., 2010, 2011), or Long Short-Term
Memory NN, as in the tool LSTMLM (Soutner
and Müller, 2015), produce state-of-the-art perfor-
mances for current language models.

This paper presents a different approach for
building LM based on quantum probability the-
ory. Actually, we present a QLM applicable only
to problems defined on a small set of different to-
kens. This is a “proof-of-concept” study and our
main aim is to show the potentialities of such ap-
proach rather than building a complete application
for solving this problem for any setting.

The paper is organized as follows: we provide
background on Quantum Probability Theory in
Section 2 followed by the description of our pro-
posed Quantum Language Model in Section 3. We
then discuss some numerical issues mainly related

1840



to the optimisation procedure in Section 4, and in
Section 5 we present the experiments we did to
validate our approach. In Section 6 we discuss our
results and draw some provisional conclusions.

2 Quantum Probability Theory

In QMT the state of a system is usually described,
in the most general case, by using density matrices
over an Hilbert spaceH. More specifically, a den-
sity matrix ρ is a positive semidefinite Hermitian
matrix of unit trace, namely ρ† = ρ, Tr(ρ) = 1,
and it is able to encode all the information about
the state of a quantum system1.

The measurable quantities, or observables, of
the quantum system are associated to Hermitian
matrices O defined on H. The axioms of QMT
specify how one can make predictions about the
outcome of a measurement using a density matrix:

• the possible outcomes of a projective mea-
surement of an observable O are its eigenval-
ues {λj};
• the probability that the outcome of the mea-

surement is λj is P (λj) = Tr(ρΠλj ) =
Tr(Πλjρ), where Πλj is the projector on the
eigenspace of O associated to λj . Note that
in the following we will use some proper-
ties of these kind of measurements, namely
Π†λj = Πλj and Π

2
λj

= Πλj ;

• after the measurement the system state col-
lapses in the following fashion: if the out-
come of the measurement was λj , the col-
lapse is

ρ′ =
ΠλjρΠλj

Tr(ΠλjρΠλj )

where the denominator is needed for trace
normalization;

• time evolution of states using a fixed time
step is described by a unitary matrix U over
H, i.e. U †U = I, where I is the identity ma-
trix. Given a state ρt, at a specific time t,
the system evolution without measurements
modifies the state as:

ρt+1 = UρtU †.

See for example (Nielsen and Chuang, 2010)
or (Vedral, 2007) for a complete introduction on
QPT.

1† marks the conjugate transpose of a vector/matrix and
Tr(·) is the trace of a matrix.

3 Quantum Language Models

In this section we describe our approach to build
QLM that can compute probabilities for the oc-
currence of a sequence w = (w1, w2, ..., wn) of
length n, composed using N different symbols,
the vocabulary containing all the words in the
model, i.e. for every symbol w in the sequence
w ∈ {0, ..., N − 1}. We define a set of orthogonal
N -dimensional vectors {ew : w ∈ {0, ..., N−1}},
spanning the complex spaceH = CN ; to measure
the probability of a symbol w, collapsing the state
over the space spanned by ew, we use the projec-
tor Πw = ewe

†
w. Note that all the words in the

vocabulary have been encoded as numbers corre-
sponding to the N dimensions of the vector space
H.

Our method is sequential, from QMT point of
view, in the sense that we use a quantum system
that produces a single symbol upon measurement.

The basic idea is that the probabilistic informa-
tion for a given sequence w = (w1, w2, ..., wn) is
encoded in the density matrix that results from the
following process:

• Inititalisation
Cond.Prob.: P (w1; ρ0, U) = Tr(ρ0Πw1)

Projection: ρ′1 =
Πw1ρ0Πw1

Tr(Πw1ρ0Πw1 )

Evolution: ρ1 = Uρ′1U †

• Recurrence (i = 2, .., n)
Cond.Prob.: P (wi|w1, ..., wi−1; ρ0, U) =

Tr(ρi−1Πwi)

Projection: ρ′i =
Πwiρi−1Πwi

Tr(Πwiρi−1Πwi )

Evolution: ρi = Uρ′iU
†

• Termination
P (w|ρ0, U) = P (w1; ρ0, U) ·

n∏
i=2

P (wi|w1, ..., wi−1; ρ0, U)

The total probability P (w|ρ0, U) for the given
sequence is thus obtained, in the termination
step, by multiplying the conditional probability
P (wi|w1, ..., wi−1; ρ0, U) for each word in the se-
quence.

We then use the initial density matrix ρ0 and the
time evolution unitary matrix U as parameters to
optimise the perplexity Γ, evaluated on a training

1841



corpus of sequences S,

Γ(ρ0, U) = exp
(
− 1
C

∑
w∈S

logP (w|ρ0, U)
)

which quantifies the uncertainty of the model. C
is the number of tokens in the corpus.

Minimising Γ is equivalent of learning a model
by fixing all the model parameters, a typical pro-
cedure in the machine learning domain.

3.1 Ancillary system
The problem with this setup is that the ‘quan-
tum effects’ are completely washed out by the
measurements on the system by using projec-
tors. The resulting expression for the probability
P (w|ρ0, U) for a sequence w is identical to that
obtained using a classical Markov model.

To solve this issue, our approach is to avoid the
complete collapse of the state after each symbol
measurement using a common technique in QMT:
we introduce an ancillary system described by a
fictitiousD-dimensional Hilbert space,Hancilla =
CD, and we couple the original system to the an-
cillary system. The resulting DN -dimensional
Hilbert space is

H2 ≡ Hancilla ⊗H = CDN

where ⊗ denotes the Kronecker product for matri-
ces and D can be seen as a free hyper-parameter
of the model. On this new space the projectors are
now given by Π(2)w = ID ⊗ Πw, where ID is the
D-dimensional identity matrix.

The advantage of using this method is that the
time evolution for the coupled system creates non-
trivial correlations between the two entangled sys-
tems such that measuring and collapsing the sym-
bol state keeps some information about the whole
sequence stored in the ancillary part of the state.
This information is then reshuffled into the symbol
state via time evolution, resulting in a ‘memory ef-
fect’ that takes the whole sequence of symbols into
account, thereby extending the idea behind the N-
grams approach. Larger D values will results in
more memory of this system and, of course, in a
larger number of parameters to learn.

3.2 System evolution
We need to specify the system evolution for our
coupled system. The simplest approach is to use a
unitary DN × DN matrix U that acts on the en-
tangled Hilbert space as shown before; it can be

specified by (DN)2 real parameters with a suit-
able parametrization (Spengler et al., 2010) that
ensures the unitarity of U . However, in our pre-
liminary experiments this approach resulted in an
insufficient ‘memory’ capability for the QLM and
in a very complex and slow minimisation proce-
dure.

A different approach could be introduced by us-
ing a specific unitary matrix for each word, but this
would lead to an enormous amount of parameters
to learn with the optimization procedure.

There are a lot of techniques in NLP to repre-
sent single words with dense vectors (see for ex-
ample (Mikolov et al., 2013) for the so called word
embeddings). Following this idea, we can repre-
sent every symbol in our system with a specific p-
dimensional vector trained using one of the avail-
able techniques w 7→ (α1(w), ..., αp(w)) or fixed
randomly.

We then work with a set of p DN×DN unitary
matrices U = (U1, ..., Up), one for each compo-
nent of the word vector, that are used to dynami-
cally build a different system evolution matrix for
each word in this way:

V (w) ≡
p∏
i=1

U
αi(w)
i

This results in p(DN)2 complex or 2p(DN)2 real
parameters to be learned.

Essentially, we treat the words in our problem
in different ways: the evolution operator for each
word V (w) is build by using a combination of the
operators U defined for each word-vector compo-
nent, while, considering the system projection, we
treat each word as one basis vector for the space
H.

Note that the choice to use a set {V (w)} of
operators, one for each word w, does not violate
the linearity of quantum mechanics: let K be the
quantum operation

K(ρ) =
∑
w

V (w)Π(2)w ρ Π
(2)
w V

†(w)

defined using projectors and evolution matrices.
Then K is a valid (i.e. a Completely Positive
Trace-preserving) evolution map that exactly re-
produces our results in the sequence of evolutions
and collapses.

The number of evolutionary operators is a trade-
off: as we said before, defining only one op-
erator U resulted in a poor performance of the

1842



proposed method in all the relevant experiments,
while defining an operator for each word would
produce too many parameters to be learned. The
trade-off that we chose is to use one operator for
each word-vector component, and build the set
{V (w)} from them as described above while pre-
serving unitarity.

With regard to the initial density matrix ρ0, we
have to define it combining the initial density ma-
trix of our system, ρs0, and the initial density ma-
trix of the ancilla, ρa0. We defined ρ

s
0 as a diagonal

N ×N matrix containing the classical Maximum
Likelihood probability Estimation to have a spe-
cific symbol at the first sequence position:

ρs0 =
1
|S|

∑
w∈S

Πw1

where S is again the set of all sequences in the
training set and w1 is the first word in each se-
quence w. With regard to the ancilla system we
do not know anything about it and thus we have to
define ρa0 as the D ×D diagonal matrix

ρa0 =
ID

Tr(ID)
.

Consequently we can define ρ0 as

ρ0 = ρa0 ⊗ ρs0 .

3.3 The final model
Putting all the ingredients together, we can fi-
nally write down the formula for the probability
P (w|ρ0,U) for a sequence w in the QLM speci-
fied by ρ0 and U. The product of conditional prob-
abilities simplifies because of the normalising de-
nominators added at each collapse and time evolu-
tion step. The result is:

P (w|ρ0,U) = Tr(Π(2)wn ...V †(w2)Π(2)w2V †(w1)
Π(2)w1ρΠ

(2)
w1V (w1)Π

(2)
w2V (w2)...Π

(2)
wn)

(1)

Using the fact that projectors have many zero en-
tries one can also re-express this trace of the prod-
uct of DN ×DN matrices in terms of the trace of
the product of D × D matrices. The formula for
P (w|ρ0,U) then simplifies to our final result

P (w|ρ0,U) = Tr(T †RT ) (2)

where the matricesR and T are defined as follows:

• in terms of entries Ri,j with indices i, j =
0, ..., D − 1, the matrix R is given by

Ri,j = [ρ0]Ni+w1,Nj+w1 .

Note that only the value of first symbol in the
sequence, w1, enters in the expression. This
is to be expected sinceR derives from the ini-
tial density matrix ρ0;

• analogously, the matrix T that encodes
the chain of combined collapses and time
evolutions is given by the product T =
T (2)T (3)...T (n), where the matrices T (k) are
given in entries, with indices i, j = 0, ..., D−
1, by

T
(k)
i,j = [V (wk−1)]Ni+wk−1,Nj+wk .

These matrices can be pre-calculated for ev-
ery pair of the involved symbols, so that
the calculation of P (w|ρ0,U) for all the se-
quences will be very fast.

The detailed calculation for obtaining the equation
(2) can be found in the supplementary material.

4 Optimisation and Numerical Issues

In order to optimise the parameters U we numer-
ically minimise the perplexity Γ computed on a
given training corpus of sequences S. This re-
quires that the matrices U remain strictly unitary
at every step of the minimisation procedure and it
can be accomplished in various ways.

The most straightforward way is to employ
an explicit parametrization for unitary matrices,
as was done in (Spengler et al., 2010). Due
to the transcendental functions employed in this
parametrisation, this approach resulted in a func-
tional form for Γ that has proven to be very chal-
lenging to minimise efficiently in our experiments.

A more elegant and efficient approach is to con-
sider the entries of U as parameters (thereby en-
suring a polynomial functional form for Γ) and
to employ techniques of differential geometry to
keep the parameters from leaving the unitary sub-
space at each minimisation step. This can be done
using a modification of the approach outlined in
(Tagare, 2011) that considers the unitary matri-
ces subspace as a manifold, the Stiefel manifold
U(DN). It is then possible to project the gradient
∇f of a generic function f(M) of the matrix vari-
able M on the tangent space of the Stiefel mani-
fold and build a line search algorithm that sweeps

1843



out curves on this manifold so that at each point
the parameters are guaranteed to form a unitary
matrix.

In our case we have multiple unitary matrices
U = (U1, ..., Up). This simply results having
curves defined on U(DN)p, parametrised by a p-
dimensional vector ofDN×DN unitary matrices.
4.1 Formula for the gradient
To implement the curvilinear search method de-
scribed in (Tagare, 2011) one needs an expression
for the gradient G = (G1, ..., Gp) of the proba-
bility function. This gradient is organised in a p-
dimensional vector of DN × DN matrices, such
that the component Gj is obtained by computing
the matrix derivative of P (w|ρ0,U) with respect
to Uj either analytically or by applying some nu-
merical estimate of the gradients, for example by
using finite differences. The latter method, when
working with thousands or millions of variables
can be very time consuming and, usually, an ex-
plicit analytic formula for the gradient accelerates
considerably all the required processing.

A lengthy analytic computation results in an ex-
plicit result. Firstly, we introduce the following
objects:

• The spectral decomposition of Uj , given by
Uj = SjDjS

†
j , guaranteed to exist by the

spectral theorem. Sj is unitary and the di-
agonal matrix Dj contains the eigenvalues
(uj1, ..., ujDN ) of Uj , j = 1, ..., p.

• The DN × DN matrices Cj(α) defined, in
entries, by

[Cj(α)]ab =
uja

α − ujbα
uja − ujb if uja 6= ujb

[Cj(α)]ab = αujaα−1 if uja = ujb

where u is the complex conjugate of u.

• TheD×DN matricesQk given in entries by

(Qk)jA = δNj+wk,A

where j = 0, ..., D− 1, A = 0, ..., DN − 1.
• The lesser and greater products associated to

the construction of system evolution matrices

V <j(w) =
j−1∏
i=1

U
αi(w)
i

V >j(w) =
n∏

i=j+1

U
αi(w)
i .

With these ingredients, the resulting formula for
the components Gj of the gradient is

Gj = 2Sj
n∑
k=2

{[
S†j
(
V <j(wk−1)†QTk−1

( k−1∏
l=2

T (l)
)†
RT
( n∏
l=k+1

T (l)
)†

QkV
>j(wk−1)†

)
Sj

]
· Cj(αj(wk−1))

}
S†j
(3)

where · denotes the element-wise matrix product.
Again, all the detailed calculations for obtaining
the analytic expression (3) for the gradient Gj can
be found in the supplementary material.

Using Tagare’s method we can project the gra-
dient onto the Stiefel manifold and build a curvi-
linear search algorithm for the minimisation.

To achieve this aim, Tagare proposed an
Armijo-Wolfe line search inserted into a simple
gradient descent procedure. We developed an ex-
tension of this algorithm combining the minimiza-
tion over the Steifel manifold technique with a
Moré-Thuente (1994) line search and a Conju-
gate Gradient minimisation algorithm that uses the
Polak-Ribière method for the combination of gra-
dients and search directions (Nocedal and Wright,
2006). All the experiments presented in the next
section were performed using these methods.

The minimisation uses random mini-batches
that increase their size during the training: they
start with approximately one tenth of the training
set dimension and increase to include all the in-
stances using a parametrised logistic function. As
stopping criterion we used the minimum of the
perplexity function over the validation set as sug-
gested in (Bengio, 2012; Prechelt, 2012) for other
machine learning techniques.

5 Experiments and Results

5.1 Data

The TIMIT corpus is a read speech corpus
designed to provide speech data for acoustic-
phonetic studies and for the development and eval-
uation of automatic speech recognition systems
(Garofolo et al., 1990). It contains broadband
recordings of 630 speakers of eight major dialects

1844



of American English and includes time-aligned or-
thographic, phonetic and word transcriptions as
well as a 16-bit, 16 kHz speech waveform file for
each utterance.

In the speech community, the TIMIT corpus
is the base for a standard phone-recognition task
with specific evaluation procedures described in
detail in (Lopes and Perdigao, 2011). We stick
completely to this evaluation to test the effective-
ness of our proposed model adopting, among the
other procedures, the same splitting between the
different data sets: the training set contains 3696
utterances (140225 phones), the validation set 400
utterances (15057 phones) and the test set 192 ut-
terances (7215 phones).

5.2 Evaluation Results

We tested the proposed model by setting up two
different evaluations: the first is an intrinsic evalu-
ation of LM performances in terms of global per-
plexity on the TIMIT testset; the second is an
extrinsic evaluation in which we replace the LM
tools provided with the Kaldi ASR toolkit (Povey
et al., 2011b) with our model in order to check the
final system performances in a phone-recognition
task and comparing them with the other state-of-
the-art LM techniques briefly introduced in Sec-
tion 1.

5.2.1 Intrinsic evaluation
The first experiment consisted in an evaluation of
models perplexity (PPL) on the TIMIT testset. We
compared the QLM model with two N-gram im-
plementations, namely CMU-SLM (Clarkson and
Rosenfeld, 1997) and IRSTLM (Federico et al.,
2008), and two recurrent NN models able to
produce state-of-the-art results in language mod-
elling, the RNNLM (Mikolov et al., 2010, 2011)
and the LSTMLM (Soutner and Müller, 2015)
packages.

Table 1 shows the results of the intrinsic evalu-
ation. With regard to RNNLM and LSTMLM re-
sults, only the best hyper-parameters combination
after a lot of experiments, optimizing them on the
validation set, has been inserted into the Table.

With regard to QLM, all the presented ex-
periments are based on artificial word vectors
produced randomly using values from the set
{−1, 0, 1} instead of real word embeddings. Ev-
ery word vector is different from the others and we
decided not to use real embeddings in order to test
the core QMT method without adding the contex-

Model Parameters PPL
CMU-SLM 2-gram 15.49
(Good-Turing 3-gram 14.28
smoothing) 4-gram 15.62

5-gram 17.33
IRSTLM 2-gram 15.47
(linear Witten- 3-gram 14.07
Bell smoothing) 4-gram 15.55

5-gram 17.53
RNNLM 280 neurons 13.32
LSTMLM 25 neurons, 1 layer 13.17
QLM N=48, p=4, D=10 13.44

N=48, p=4, D=20 13.15
N=48, p=4, D=30 13.10
N=48, p=4, D=40 12.99

Table 1: Perplexity (PPL) of the tested language-
modelling techniques on the TIMIT testset. All
the QLM results in bold face are better than the
other systems we tested.

tual information, contained in word embeddings,
that could have helped our approach to obtain bet-
ter performances, at least in principle.

5.2.2 Extrinsic evaluation
The “TIMIT recipe” contained in the Kaldi dis-
tribution2 reproduces exactly the same evalua-
tion settings described in (Lopes and Perdigao,
2011) for a phone recognition task based on this
corpus. Moreover, Kaldi provides some n-best
rescoring scripts that apply RNNLM hypothesis
rescoring and interpolate the results with the stan-
dard N-gram model results used in the evaluation.
We slightly modified these scripts to work with
LSTMLM and QLM in order to test different mod-
els using the same setting. This allowed us to re-
place the LM used in Kaldi and experiment with
all the systems evaluated in the previous section.

Table 2 outlines the results we obtained replac-
ing the LM technique into Kaldi ASR package
w.r.t. the different ASR systems that the TIMIT
recipe implements. These systems are built on top
of MFCC, LDA, MLLT, fMLLR with CMN3 fea-
tures (see (Povey et al., 2011b; Rath et al., 2013)
for all acronyms references and a complete feature

2https://github.com/kaldi-asr/kaldi
3MFCC: Mel-Frequency Cepstral Coefficients; LDA:

Linear Discriminant Analysis; MLTT: Maximum Likelihood
Linear Transform; fMLLR: feature space Maximum Likeli-
hood Linear Regression; SAT: Speaker Adapted Training, i.e.
train on fMLLR-adapted features; CMN: Cepstral Mean Nor-
malization.

1845



or recipe descriptions).
For this extrinsic evaluation we used the best

models we obtained in the previous experiments
interpolating their log-probability results for each
utterance with the original bigram (or trigram)
log-probability using a linear model with a ratio
0.25/0.75 between the original N-gram LM and
the tested one as suggested in the standard Kaldi
rescoring script. For this test we rescored the
10,000-best hypothesis.

We have to say that in this experiment we were
not trying to build the best possible phone recog-
niser, but simply to compare the relative perfor-
mances of the analysed LM techniques showing
the effectiveness of QLM when used in a real ap-
plication. Thus absolute Phone Error Rate is not
so important here and it can be certainly possible
to devise recognisers with better performances by
applying more sophisticated techniques. For ex-
ample (Peddinti et al., 2015) presented a method
for lattice rescoring in Kaldi that exhibits better
performances than the n-best rescoring we used to
interpolate between n-grams and the tested mod-
els, but modifying it in order to test LSTMLM
and QLM presented a lot of problems and thus
we decided to use the simpler n-best approach.
For completeness, the last column of Table 2 out-
lines the results obtained using this lattice rescor-
ing method with RNNLM as described in (Ped-
dinti et al., 2015).

6 Discussion and conclusions

We presented a new technique for building LM
based on QMT, and its probability calculus, test-
ing it extensively both with intrinsic and extrinsic
evaluation methods.

The PPL results for the intrinsic evaluation,
outlined in Table 1, show a clear superiority
of the proposed method when compared with
state-of-the-art techniques such as RNNLM and
LSTMLM. It is interesting to note that even using
D = 20, that means a system containing a quar-
ter of parameters, therefore much less ‘memory’,
w.r.t. the system with D = 40, we obtain a PPL
performance better than the other methods.

With regard to the second experiment we made,
an extrinsic evaluation where we replaced the LM
of an ASR system with the LM produced by all the
tested methods (see Table 2), QLM consistently
exhibits the best performances for all the tested
ASR systems from the Kaldi “TIMIT recipe”. De-

spite using a n-best technique in this evaluation
for hypothesis rescoring, that is known to perform
worse than the lattice rescoring method proposed
in (Peddinti et al., 2015), the QLM performances
are even better than this method.

The approach we have presented in this paper
is not without problems: the number of different
word types in the considered language has to be
small in order to keep the model computationally
tractable. Even if the code we used in the evalu-
ations is analytically highly optimised, the train-
ing of this model is rather slow and requires rele-
vant computational resources even for small prob-
lems. On the contrary, inference is very quick,
faster than the RNNLM and LSTMLM packages
we tested.

The main research question that drove this work
was to verify if the distinguishing properties of
quantum probability theory, namely interference
and system entaglement that could allow the an-
cilla to have a “potentially infinite” memory, were
enough to build stochastic systems more power-
ful than those built using classical probabilities
or those built using recurrent NN. Our main aim
was not to build a complete model to handle all
possible LM scenarios, but to present a “proof-of-
concept” study to test the potentialities of this ap-
proach. For this reason we tried to keep the model
as simple as possible using orthogonal projectors:
for measuring probabilities, projecting the system
state, each word is mapped onto a single basis vec-
tor and the dimension of the system Hilbert space,
N , is equal to the number of different words.
Given the matrix dimensions that we have to man-
age when we add the ancilla, DN ×DN , this set-
ting does not scale to real LM problems (e.g. the
Brown corpus), even though the calculations are
performed using D ×D submatrices, but allowed
us to successfully verify the research question. For
the same reason out-of-vocabulary words cannot
be handled in this model because there are no ba-
sis vectors assigned to them.

In order to overcome these limitations, this
work can be extended by using generalized quan-
tum measurements projectors (POVM) and by us-
ing a different structure for the system Hilbert
space: instead of mapping each word onto a sin-
gle basis vector we can span this space using as
basis the same p-basis vectors used to define the
V matrices. In this way we will project the system
state on a generic word vector built as a superposi-

1846



Kaldi IRSTLM N-Best rescoring Lattice
ASR 2-gram IRSTLM 2-gram LM interp. with: rescoring
System RNNLM LSTMLM QLM RNNLM
tri1 26.32 25.74 25.09 24.59 25.70
tri2 24.14 23.34 23.23 23.05 23.17
tri3 21.55 21.07 21.22 20.35 20.85
SGMM2 19.15 18.99 18.52 18.23 18.75
Dan NN 22.27 22.20 22.26 21.80 22.05
Kaldi IRSTLM N-Best rescoring Lattice
ASR 3-gram IRSTLM 3-gram LM interp. with: rescoring
System RNNLM LSTMLM QLM RNNLM
tri1 25.64 25.39 24.86 24.59 25.42
tri2 23.16 23.13 22.90 22.65 22.97
tri3 20.80 20.57 20.68 20.04 20.68
SGMM2 18.64 18.41 18.48 18.23 18.27
Dan NN 21.72 21.90 21.95 21.34 21.48

Table 2: Phone-recognition performances, in terms of Phone Error Rate, for the TIMIT dataset and
the different Kaldi ASR models, rescoring the 10,000-best solutions with the tested LM techniques in-
terpolated with the IRSTLM bigrams and trigrams LM (the standard LM used in Kaldi). In boldface
the best performing system and in italics the second best. Kaldi ASR systems descriptions: tri1 = a
triphone model using 13 dim. MFCC+∆+∆∆; tri2 = tri1+LDA+MLLT; tri3 = tri2+SAT; SGMM2 =
Semi-supervised Gaussian Mixture Model (Huang and Hasegawa-Johnson, 2010; Povey et al., 2011a);
Dan NN = DNN model by (Zhang et al., 2014; Povey et al., 2015).

tion on the p-basis. Such improvement would re-
duce dramatically the dimensions of the matrices
to Dp × Dp potentially mitigating the computa-
tional issue. Moreover, this would solve also the
problem of out-of-vocabulary words allowing for
a proper management of the large set of different
words typical of real applications.

We are still working on these improvements and
we will hope to get a complete model soon.

With this contribution we would like to raise
also some interest in the community to analyse
and develop more effective techniques, both on
the modelling and minimisation/learning sides, to
allow to build real world application based on
this framework. QMT and its probability calculus
seem to be promising methodologies to enhance
the performances of our systems in NLP and cer-
tainly deserve further investigations.

Acknowledgments

We acknowledge the CINECA4 award no.
HP10C7XVUO under the ISCRA initiative, for
the availability of HPC resources and support.

4https://www.cineca.it/en

References
Diederik Aerts, Jan Broekaert, Liane Gabora, and

Sandro Sozzo. 2013. Quantum structure and hu-
man thought. Behavioral and Brain Sciences,
36(3):274276.

Martin Arjovsky, Amar Shah, and Yoshua Bengio.
2016. Unitary evolution recurrent neural networks.
In Proceedings of the 33rd International Conference
on International Conference on Machine Learning -
ICML’16, pages 1120–1128.

Yoshua Bengio. 2012. Practical recommendations for
gradient-based training of deep architectures. In
Grégoire Montavon, Geneviève B. Orr, and Klaus-
Robert Müller, editors, Neural Networks: Tricks of
the Trade: Second Edition, pages 437–478. Springer
Berlin Heidelberg, Berlin, Heidelberg.

William Blacoe, Elham Kashefi, and Mirella Lapata.
2013. A quantum-theoretic approach to distribu-
tional semantics. In Proceedings of Human Lan-
guage Technologies: Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, Atlanta, Georgia, pages 847–857.

Jerome R. Busemeyer and Peter D. Bruza. 2012. Quan-
tum Models of Cognition and Decision. Cambridge
University Press, New York, NY.

Philip Clarkson and Ronald Rosenfeld. 1997. Statis-
tical language modeling using the cmu-cambridge

1847



toolkit. In Proceedings of EUROSPEECH ’97,
pages 2707–2710. ISCA.

Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In INTER-
SPEECH 2008, 9th Annual Conference of the Inter-
national Speech Communication Association, Bris-
bane, Australia, pages 1618–1621.

John Garofolo, Lori Lamel, William Fisher, Jonathan
Fiscus, David Pallett, Nancy Dahlgren, and Vic-
tor Zue. 1990. Darpa timit acoustic-phonetic con-
tinuous speech corpus cd-rom. DARPA, TIMIT
Acoustic-Phonetic Continuous Speech Corpus CD-
ROM.

Fabio A. González and Juan C. Caicedo. 2011. Quan-
tum latent semantic analysis. In Advances in Infor-
mation Retrieval Theory, LNCS, 6931, pages 52–63.

Jui Ting Huang and Mark Hasegawa-Johnson. 2010.
Semi-supervised training of gaussian mixture mod-
els by conditional entropy minimization. In Pro-
ceedings of the 11th Annual Conference of the In-
ternational Speech Communication Association, IN-
TERSPEECH 2010, pages 1353–1356.

Li Jing, Yichen Shen, Tena Dubcek, John Peurifoy,
Scott A. Skirlo, Max Tegmark, and Marin Soljacic.
2017. Tunable efficient unitary neural networks
(EUNN) and their application to RNN. In Thirty-
fourth International Conference on Machine Learn-
ing - ICML2017.

Dimitrios Kartsaklis, Martha Lewis, and Laura Rimell.
2016. Proceedings of the 2016 Workshop on Seman-
tic Spaces at the Intersection of NLP, Physics and
Cognitive Science, volume 221. Electronic Proceed-
ings in Theoretical Computer Science.

Andrei Y. Khrennikov. 2010. Ubiquitous Quantum
Structure: From Psychology to Finance. Springer-
Verlag Berlin Heidelberg.

Ding Liu, Xiaofang Yang, and Minghu Jiang. 2013. A
novel classifier based on quantum computation. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics, Sofia, Bul-
garia, pages 484–488.

Carla Lopes and Fernando Perdigao. 2011. Phoneme
recognition on the timit database. In Ivo Ipsic, edi-
tor, Speech Technologies. InTech, Rijeka.

Massimo Melucci. 2015. Introduction to Information
Retrieval and Quantum Mechanics. The Informa-
tion Retrieval Series 35. Springer-Verlag Berlin Hei-
delberg.

Massimo Melucci and Keith van Rijsbergen. 2011.
Quantum mechanics and information retrieval. In
Massimo Melucci and Ricardo Baeza-Yates, editors,
Advanced Topics in Information Retrieval, pages
125–155. Springer Berlin Heidelberg, Berlin, Hei-
delberg.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Repre-
sentations in Vector Space. In Proc. of Workshop at
ICLR.

Tomas Mikolov, Martin Karafiát, Lukás Burget, Jan
Cernocký, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH 2010, 11th Annual Conference of the
International Speech Communication Association,
Makuhari, Chiba, Japan, pages 1045–1048.

Tomáš Mikolov, Stefan Kombrink, Anoop Deoras,
Lukáš Burget, and Jan Černocký. 2011. Rnnlm -
recurrent neural network language modeling toolkit.
In Proceedings of ASRU 2011, pages 1–4.

Jorge J. Moré and David J. Thuente. 1994. Line
search algorithms with guaranteed sufficient de-
crease. ACM Trans. Math. Softw., 20(3):286–307.

Michael A. Nielsen and Isaac L. Chuang. 2010. Quan-
tum Computation and Quantum Information: 10th
Anniversary Edition. Cambridge University Press.

J. Nocedal and S. J. Wright. 2006. Numerical Opti-
mization, 2nd edition. Springer, New York.

Vijayaditya Peddinti, Guoguo Chen, Vimal Manohar,
Tom Ko, Daniel Povey, and Sanjeev Khudanpur.
2015. JHU aspire system: Robust LVCSR with
tdnns, ivector adaptation and RNN-LMS. In 2015
IEEE Workshop on Automatic Speech Recognition
and Understanding, ASRU 2015, Scottsdale, AZ,
USA, pages 539–546.

Daniel Povey, Lukáš Burget, Mohit Agarwal, Pinar
Akyazi, Feng Kai, Arnab Ghoshal, Ondřej Glem-
bek, Nagendra Goel, Martin Karafiát, Ariya Ras-
trow, Richard C. Rose, Petr Schwarz, and Samuel
Thomas. 2011a. The subspace gaussian mixture
model-a structured model for speech recognition.
Comput. Speech Lang., 25(2):404–439.

Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas
Burget, Ondrej Glembek, Nagendra Goel, Mirko
Hannemann, Petr Motlicek, Yanmin Qian, Petr
Schwarz, Jan Silovsky, Georg Stemmer, and Karel
Vesely. 2011b. The kaldi speech recognition toolkit.
In IEEE 2011 Workshop on Automatic Speech
Recognition and Understanding. IEEE Signal Pro-
cessing Society.

Daniel Povey, Xiaohui Zhang, and Sanjeev Khudanpur.
2015. Parallel training of dnns with natural gradient
and parameter averaging. In International Confer-
ence on Learning Representations - ICLR2015.

Lutz Prechelt. 2012. Early stopping — but when? In
Grégoire Montavon, Geneviève B. Orr, and Klaus-
Robert Müller, editors, Neural Networks: Tricks of
the Trade: Second Edition, pages 53–67. Springer
Berlin Heidelberg, Berlin, Heidelberg.

1848



Shakti P. Rath, Daniel Povey, Karel Veselý, and
Jan Cernocký. 2013. Improved feature processing
for deep neural networks. In Proceedings of the
14th Annual Conference of the International Speech
Communication Association - INTERSPEECH2013,
Lyon, France, pages 109–113.

Daniel Soutner and Luděk Müller. 2015. In Adrian-
Horia Dediu, Carlos Martı́n-Vide, and Klára Vicsi,
editors, Statistical Language and Speech Process-
ing: Third International Conference, SLSP 2015,
Budapest, Hungary, November 24-26, 2015, Pro-
ceedings, pages 267–274. Springer International
Publishing.

Christoph Spengler, Marcus Huber, and Beatrix C
Hiesmayr. 2010. A composite parameterization
of unitary groups, density matrices and subspaces.
Journal of Physics A: Mathematical and Theoreti-
cal, 43(38):385306.

H.D. Tagare. 2011. Notes on optimization on Stiefel
manifolds. Technical report, Technical report, Yale
University.

Fabio Tamburini. 2014. Are quantum classifiers
promising? In Proceedings of the First Italian Con-
ference on Computational Linguistics CLiC-it 2014,
Pisa, Pisa University Press, pages 360–364.

Vlatko Vedral. 2007. Introduction to Quantum Infor-
mation Science. Oxford University Press, USA.

Scott Wisdom, Thomas Powers, John Hershey,
Jonathan Le Roux, and Les Atlas. 2016. Full-
capacity unitary recurrent neural networks. In D. D.
Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and
R. Garnett, editors, Advances in Neural Information
Processing Systems 29, pages 4880–4888. Curran
Associates, Inc.

Xiaohui Zhang, Jan Trmal, Daniel Povey, and San-
jeev Khudanpur. 2014. Improving deep neural net-
work acoustic models using generalized maxout net-
works. In Acoustics, Speech and Signal Processing
(ICASSP), 2014 IEEE International Conference on,
pages 215–219. IEEE.

Guido Zuccon, Leif A. Azzopardi, and Keith van Ri-
jsbergen. 2009. The quantum probability ranking
principle for information retrieval. In Leif et al. Az-
zopardi, editor, Advances in Information Retrieval
Theory, LNCS, 5766, pages 232–240.

1849


