



















































Level-Up: Learning to Improve Proficiency Level of Essays


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 207–212
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

207

Level-Up: Learning to Improve Proficiency Level of Essays

Wen-Bin Han, Jhih-Jie Chen, Ching-Yu Yang, Jason S. Chang
Department of Computer Science

National Tsing Hua University
{vincent.han, jjc, chingyu, jason}@nlplab.cc

Abstract

We introduce a method for generating sug-
gestions on a given sentence for improving
the proficiency level. In our approach, the
sentence is transformed into a sequence of
grammatical elements aimed at providing sug-
gestions of more advanced grammar elements
based on originals. The method involves pars-
ing the sentence, identifying grammatical el-
ements, and ranking related elements to rec-
ommend a higher level of grammatical ele-
ment. We present a prototype coaching sys-
tem, Level-Up, that applies the method to En-
glish learners’ essays in order to assist them in
writing and reading. Evaluation on a set of es-
says shows that our method does assist user in
writing.

1 Introduction

Many essays (e.g., “Amazingly, the child is
so fashionable and creative that he makes the
ugly house modern.”) are submitted to tutor-
ing services by English learners on the Web
every day, and an increasing number of ser-
vices on the Web specifically target learn-
ers’ essays. For example, LanguageToolPlus
(languagetoolplus.com) uses rule-based
model with n-grams extracted from the nu-
merous data to inspect essays for grammat-
ical errors, while Grammarly (grammarly.
com), 1checker (1checker.com) and Ginger
(gingersoftware.com) use proprietary neu-
ral network approaches to proofread texts, check
grammar, review style, and enrich vocabulary.

Tutoring services such as Write&Improve
(writeandimprove.com) and WhiteSmoke
(whitesmoke.com) typically correct and grade
essays as a whole. However, very few systems
provide focused suggestions on how to raise the
level of proficiency. Learners could raise their
level of grammatical proficiency, if a system can

identify grammatical elements and suggest the
learner to use related elements with a higher level.

Consider the essay “Amazingly, the child is so
fashionable and creative that he makes the ugly
house modern.”. The useful suggestion for this
sentence is definitely not just a level for the whole
essay, which is pointless for learners, but the lev-
els of grammatical elements with explanation and
level-up grammatical elements. A helpful sugges-
tion for an essay should not only contain the pairs
of level and explanation such as: “B1 - make the
ugly house modern - Can use adjectives as ob-
ject complement after ‘make’.” but also suggest
improvement: “B2 - Can use a limited range of
degree adjectives (‘real’, ‘absolute’, ‘complete’)
before a noun to express intensity.”. These gram-
matical elements can be retrieved from English
Grammar Profile (EGP) with more than a thou-
sand grammatical elements with levels stipulated
by Common European Framework of Reference
(CEFR). Intuitively, by categorizing grammatical
elements, we can provide more informative in-
struction for learners to improve their essays.

We present a new system, Level-Up, that parses
essays into trees expected to recommend related
advanced grammatical elements. An example
Level-Up recommending for the essay “Amaz-
ingly, the child is so fashionable and creative that
he made the ugly house modern.” is shown in Fig-
ure 1. Level-Up has identified several grammat-
ical elements (e.g., {make NOUN ADJ}) for the
given essay. Level-Up detects these grammatical
elements by matching patterns against the parse
trees. We describe the Level-Up model in more
detail in Section 3.

At run-time, Level-Up starts with a given es-
say submitted by the learner (e.g., “Amazingly,
the child is so fashionable and creative that he
made the ugly house modern.”), which is first con-
verted into a set of grammatical elements. Then,

languagetoolplus.com
grammarly.com
grammarly.com
1checker.com
gingersoftware.com
writeandimprove.com
whitesmoke.com


208

Figure 1: The screenshot of Level-Up

Level-Up ranks categorized elements and retrieves
the related elements with a higher level as sugges-
tions. In our prototype, Level-Up returns detected
grammatical elements and recommendations of
higher level elements to English learners directly
(see Figure 1); alternatively, the elements and lev-
els returned by Level-Up can be used as input to
an essay scoring system.

The rest of the paper is organized as follows.
We review the related work in the next section.
Then we present our method for detecting gram-
matical elements in learners’ essays expected to
suggest more advanced elements. In our eval-
uation, Level-Up can provide useful collocations
with levels for learners during writing.

2 Related work

English Language Teaching (ELT) has been an
area of active research in Applied Linguistics and
Computational Linguistics. Recently, the state-
of-the-art research in ELT has been represented
in the 13th Workshop on Innovative Use of NLP
for Building Educational Applications (Tetreault
et al., 2018) in the Association for Computational
Linguistics (ACL) community. The workshop in-
volves developing applications based on NLP ap-
proaches for teachers and learners of English as
a Second Language (ESL) in educational settings.
For example, Bryant and Briscoe (2018) build a
competitive system only requiring minimal anno-
tated data by using a simple Language Model ap-

proach. In our work, we address an aspect of En-
glish Language Teaching which is not the focus
of correcting errors. Instead, we concentrate on
how to analyze grammatical elements and suggest
more advanced elements for learners to level up
their essays.

More specifically, we focus on grammatical
analysis for assisting learners in writing En-
glish, namely, suggesting grammatical elements at
higher proficiency level based on identified gram-
matical elements in learner’s writing. Grammat-
icality improvement for learners has been the fo-
cus of ELT research with much works concentrat-
ing on Grammar Error Correction (GEC). In gen-
eral, GEC systems are aimed at correcting errors
in learners’ essays without considering the levels
of grammatical elements used in the essays. In
contrast, we will analyze the levels of grammat-
ical elements in a given essay and provide more
grammatically and lexically advanced elements to
inform learners of how to refine and level up their
essays.

The most commonly used criteria for measuring
the proficiency levels is the Common European
Framework of Reference for Languages (CEFR)
(Council of Europe, 2001) with six proficiency
levels: the basic level (A1 and A2), independent
level (B1 and B2) and proficient level (C1 and
C2). As an aid to defining levels for learning,
teaching and assessment, CEFR describes what
language learners can do (“can-do” statement) at
different learning stages. (e.g., level A1 - Can use



209

Regex Level Statement
JJR and JJR B1 Can use ’and’ to join a limited range of comparative adjectives.
too JJ TO VB B1 Can use ’too’ before adjectives followed by ’to’-infinitive.
very JJ A1 Can use ’very’ to modify common gradable adjectives.

Table 1: Example regular expressions

commas and “and” to join more than two adjec-
tives, after “be”.) Moreover, Cambridge Univer-
sity Press organizes a wealth of information re-
lated to CEFR, including English Grammar Pro-
file (EGP) and English Vocabulary Profile (EVP).
EGP grades learners’ ability in terms of grammat-
ical form and CEFR levels, while EVP defines
words and phrases of different CEFR levels.

Previous works targeting CEFR level detection
of learners’ essays include Hancke and Meurers
(2013) for German and Vajjala (2014) for Estonian
based on annotated learner data. To cope with high
cost of collecting learners’ data, Pilán et al. (2016)
investigated the benefits of using texts from lan-
guage learning coursebooks to train classifiers for
predicting proficiency levels of learners’ texts. Va-
jjala (2017) and Tack et al. (2017) present methods
for identifying the linguistic variables that are in-
dicative of writing quality to evaluate a learner’s
proficiency. Their researches and Bartning et al.
(2019) all use CEFR to assess proficiency levels.
We also utilize CEFR criteria in our research to
evaluate essays, but focusing more on grammati-
cal elements laid out in Cambridge EGP.

In a study more closely related to our work,
Write&Improve1 (Andersen et al., 2013; Yan-
nakoudakis et al., 2018) supports self-assessment
and learning by correcting common errors and re-
turning an overall score for an essay. Furthermore,
it also indicates potentially worst sentences. In
contrast, we focus on providing specific informa-
tion on raising the proficiency level as the learner
writes.

Researches have pointed out that supplying sug-
gestions while writing is more helpful than sug-
gesting after the fact (Hearst, 2015). Grammarly
tries to correct grammatical errors and provides
the explanation while the user is writing. WriteA-
head2 (Yen et al., 2015) provides real-time writ-
ing suggestions on what to write next in the form
of grammar patterns and example sentences. Sim-
ilarly, ColloCaid3 (Lew et al., 2018) checks if the

1www.writeandimprove.com
2www.writeahead.nlpweb.org
3www.collocaid.uk

collocation is used correctly and provides frequent
collocates so that writers can choose words that go
well together.

In contrast to the previous research in En-
glish Language Teaching and Grammatical Error
Correction, we present a tutoring system, Level-
Up, that provides writing assistance, focusing on
analyzing grammatical elements and suggesting
higher level elements during writing.

3 The Level-Up System

To improve learners’ essays, grammatical error
correction (GEC) is not sufficient. Unfortunately,
very few Language tools go beyond GEC and
provide suggestions on proficiency level improve-
ment for learners. In this section, we address such
a problem. Level-Up displays a set of suggestions
based on leveled grammatical elements for im-
proving an unfinished sentence or complete sen-
tences in essays. We transform criteria (e.g., EGP)
into a pattern-matching program to identify gram-
matical elements and example n-grams from the
corpus. We describe the process of our solution to
this problem in the subsections that follow.

3.1 Extracting Grammatical Elements

Due to the lack of annotated data on grammati-
cal elements, we attempt to extract grammatical
elements using rules representing grammatical el-
ements. The method involves using regular ex-
pressions, a lexical dictionary, and a parser, since
regular expression is straightforward for matching
patterns. Table 1 shows the examples of regular
expression corresponding to EGP elements.

After converting these elements into regular ex-
pressions, we first parse the given corpus, and
then retrieve grammatical elements and example
n-grams for recommendation later at run-time.
Since regular expressions have some limitations
on flexibility, we solve this problem by using a de-
pendency parser. Therefore, we take advantage of
dependency tree to generate all phrasal elements
from the parse tree layer by layer, and then match
all the rules with these element candidates. We



210

(1) Parse the sentence into POS tags and keywords.
S: Actually, the child is very nice and friendly.

“ADV , DET NOUN be ADV ADJ and ADJ .”

(2) Generate all element candidates layer by layer.
S: Actually, the child is very nice and friendly.

“is”, “Actually , child is nice”, “Actually , the child is very nice and friendly .”
“be”, “ADV , NOUN be ADJ”, “ADV , DET NOUN be ADV ADJ and ADJ .”

(3) Match candidates against all regular expressions.
(a) “ADV”
(b) “be ADJ”
(c) “ADJ and ADJ”

(4) Extract these matches with the corresponding n-grams.
(a) Actually, “ADV”
(b) is nice, “be ADJ”
(c) nice and friendly, “ADJ and ADJ”

Figure 2: Outline of the process used to identify elements in an example sentence

(1) Obtain n-grams belonging to the given element.
(2) Remove the n-grams not containing the last word

in the unfinished sentence.
(3) Calculate Language Model of the unfinished sen-

tence with each n-gram.
(4) Retain top 10 highest n-grams of Language

Model.
(5) Calculate the n-grams of the average word level.
(6) Select the highest n-gram and its corresponding

grammatical elements.

Figure 3: Outline of the process used to select top 1
n-gram to exemplify a given suggested grammatical el-
ement.

then record every matching phrase and sentence
for generating suggestions at run-time. Figure 2
shows the process of identifying grammatical ele-
ments in an example sentence.

3.2 Automated Writing Suggestion for
Leveling up

Once the grammatical elements and n-grams are
automatically extracted and counted from the
given corpus, they are stored as suggestion can-
didates. Level-Up constantly returns suggestions
based on the last word the user types in the writ-
ing area. With the last word as a query, Level-
Up retrieves and displays n-grams, ranked by Lan-
guage Model and the level of words. Furthermore,
each n-gram exemplifies different grammatical el-
ements and is accompanied with three example

(1) Obtain grammatical elements in the same subcat-
egory.

(2) Retain elements with higher level than the identi-
fied element.

(3) Select the first one element for recommendation

Figure 4: Outline of the process of recommending
level-up elements for identified grammatical elements.

sentences. The process of selecting n-grams is
shown in Figure 3.

3.3 Analyzing Elements and Ranking
Suggestions

Level-Up also analyzes essays after users finish
writing. The process of analyzing is the same as
described in Subsection 3.1. However, we do not
display all the matches to the user. Instead, if
the grammatical element is completely overlapped
by the other element, we only retain the one with
higher level.

Our system not only identifies grammatical el-
ements but also suggests level-up elements. EGP
contains the broad categories of the grammatical
elements, including adjectives and adverbs. Fur-
thermore, every category includes several subcate-
gories (e.g., adjectives - comparatives and superla-
tives). We group those elements by the categories
and then select the most related level-up element
in the same group. The process of selecting level-
up elements is shown in Figure 4.



211

4 Experiments and Results

In this section, we describe the details of our ex-
periments and the results. First, we introduce how
we preprocess the corpus and extracted grammati-
cal elements. Then, we explain the analysis of vo-
cabularies in Level-Up. Finally, we describe the
program architecture and the toolkits used, and
show the evaluation results of our system.

4.1 English Grammar Profile
CEFR lists total 1,222 grammatical elements in
EGP. In our prototype, we experimented with two
categories, adjectives and adverbs, as a pilot study
to prove that our approach is effective. Some
specific types of words, such as degree adverbs,
are enumerated from Sinclair (2005). For prepro-
cessing, we used British National Corpus (BNC)
(Corpus, 2001), containing over four million sen-
tences, to collect example n-grams for grammati-
cal elements. To be more specific, we parsed all
the sentences and generated grammatical element
candidates by using SpaCy parser (Honnibal and
Montani, 2017). Then, we matched all the candi-
dates against all the rules. Every detected match
of grammar pattern is stored with its n-gram and
sentence.

4.2 English Vocabulary Profile
In addition to EGP, we also utilize EVP in our sys-
tem. Level-Up not only analyzes the levels of vo-
cabularies defined by EVP in learner’s essays but
also provides similar vocabularies at higher level.
For example, Level-Up can suggest “strive” (C2
level) for the verb “try” (A2 level). However, an-
alyzing vocabulary in our study is not described
in Section 3 due to our focus of EGP. First of all,
we obtained the full six-level vocabularies from
EVP4, which covers levels A1-C2 of CEFR. How-
ever, disambiguating the meanings of polysemy is
still an open problem. Therefore, we use the low-
est level of a word directly. In other words, after
tokenizing sentences with SpaCy, we match these
tokens against the lookup table, EVP, to obtain the
lower level.

For word suggestions, we use the pre-trained
300 dimension Word2Vec (Mikolov et al., 2013)
trained on Google News to generate top 100 sim-
ilar words as candidates using Gensim (Řehůřek
and Sojka, 2010). Then, we filter out those can-
didates that are not in EVP, at a lower level, or

4vocabulary.englishprofile.org/staticfiles/about.html

Suggestion Count Percent Precision
1st suggestions 22 0.53 0.44
2nd suggestions 13 0.32 0.26
3rd suggestions 6 0.15 0.12
Top 3 suggestions 41 - 0.82
Not in Top 3 9 - 0.18

Table 2: Human evaluation of Level-Up

different POS tags. Finally, we choose the top 10
candidates to show to learners.

4.3 Technical Architecture
Leve-Up was implemented in Python with the
Flask Web framework. We stored the suggestions
in JSON format and read the content into mem-
ory for fast access. Level-Up server obtains client
input from a popular browser (Safari, Chrome, or
Firefox) dynamically with AJAX techniques.

4.4 Evaluating Level-Up
To evaluate the performance of Level-Up, we
randomly sampled sentences from learner’s cor-
pus. For simplicity, we tested if learners can ac-
quire suitable n-grams with advanced grammat-
ical structures from using Level-Up and evalu-
ate the performance. We randomly selected 50
sentences with adjectives or adverbs from EF-
Cambridge Open Language Database (EFCAM-
DAT) as test data and segmented each sentence
from start to adjective or adverb word for recom-
mended n-grams. After typing it in Level-Up, we
considered n-grams from first one to third one, and
counted the position of good suggestions. We as-
sumed that learners can fit the n-grams to the in-
put with less tolerance of edit. Finally, we manu-
ally determined the appropriateness of suggestions
based on the precision of the Top-3 suggestions.
Table 2 shows the performance of Level-Up.

5 Future Work and Conclusion

Many avenues exist for future research and im-
provement of our system, Level-Up. For exam-
ple, other categories in EGP could be handled.
The method of ranking n-grams could be im-
proved by considering the relevancy of n-gram
to learner’s sentence more precisely. NLP and
Machine Learning techniques could be applied
to identify and rank grammatical elements. Ad-
ditionally, an interesting direction to explore is
recommending well-spoken words and phrases to
level up learner’s essays lexically. For example,



212

we could suggest “the better part of a week” to
level up “almost a week”. Similarly, we could
suggest “in the making” for “happening.” Yet an-
other direction of research is evaluating an essay
as a whole based on the detected grammatical el-
ements. In other words, teachers can assess stu-
dents’ essays more efficiently using Level-Up.

In summary, we have proposed a method for
analyzing grammatical elements and suggesting
level-up elements while a user is writing. The ap-
proach involves extracting, retrieving, and ranking
grammatical elements and examples. We have im-
plemented and evaluated the proposed approach as
applied to a large corpus with promising results.

References
Øistein E. Andersen, Helen Yannakoudakis, Fiona

Barker, and Tim Parish. 2013. Developing and
testing a self-assessment and tutoring system. In
BEA@NAACL-HLT.

Inge Bartning, M Martin, and Ineke Vedder. 2019.
Communicative proficiency and linguistic develop-
ment: intersections between sla and language testing
research.

Christopher Bryant and Ted Briscoe. 2018. Language
model based grammatical error correction without
annotated training data. In BEA@NAACL-HLT.

British National Corpus. 2001.

Council of Europe. 2001. Common European Frame-
work of Reference for Languages: Learning, Teach-
ing, Assessment. Common European Framework of
Reference for Languages: Learning, Teaching, As-
sessment. Cambridge University Press.

Julia Hancke and Detmar Meurers. 2013. Exploring
cefr classification for german based on rich linguis-
tic modeling. pages 54–56.

Marti A. Hearst. 2015. Can natural language process-
ing become natural language coaching? In ACL.

Matthew Honnibal and Ines Montani. 2017. spacy 2:
Natural language understanding with bloom embed-
dings, convolutional neural networks and incremen-
tal parsing. To appear.

Robert Lew, Ana Frankenberg-Garcia, Geraint Paul
Rees, Jonathan C Roberts, and Nirwan Sharma.
2018. Collocaid: A real-time tool to help academic
writers with english collocations. In The XVIII EU-
RALEX International Congress, page 165.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proceedings of the 26th International Con-
ference on Neural Information Processing Systems -

Volume 2, NIPS’13, pages 3111–3119, USA. Curran
Associates Inc.

Ildikó Pilán, Elena Volodina, and Torsten Zesch. 2016.
Predicting proficiency levels in learner writings by
transferring a linguistic complexity model from
expert-written coursebooks. In COLING.

Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks, pages 45–50, Val-
letta, Malta. ELRA. http://is.muni.cz/
publication/884893/en.

J. Sinclair. 2005. Collins Cobuild English grammar.
Collins Cobuild. HarperCollins.

Anaı̈s Tack, Thomas Franois, Sophie Roekhaut, and
Cédrick Fairon. 2017. Human and automated cefr-
based grading of short answers. In BEA@EMNLP.

Joel Tetreault, Jill Burstein, Ekaterina Kochmar, Clau-
dia Leacock, and Helen Yannakoudakis. 2018. Pro-
ceedings of the thirteenth workshop on innovative
use of nlp for building educational applications. In
Proceedings of the Thirteenth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions. Association for Computational Linguistics.

Sowmya Vajjala. 2014. Automatic cefr level prediction
for estonian learner text.

Sowmya Vajjala. 2017. Automated assessment of non-
native learner essays: Investigating the role of lin-
guistic features. International Journal of Artificial
Intelligence in Education, 28:79–105.

Helen Yannakoudakis, Øistein E. Andersen, Ardeshir
Geranpayeh, Ted Briscoe, and Diane Nicholls. 2018.
Developing an automated writing placement system
for esl learners.

Tzu-Hsi Yen, Jian-Cheng Wu, Jim Chang, Joanne Bois-
son, and Jason Chang. 2015. Writeahead: Min-
ing grammar patterns in corpora for assisted writing.
Proceedings of ACL-IJCNLP 2015 System Demon-
strations, pages 139–144.

http://www.natcorp.ox.ac.uk
https://books.google.com.tw/books?id=PygQ8Gk4k4YC
https://books.google.com.tw/books?id=PygQ8Gk4k4YC
https://books.google.com.tw/books?id=PygQ8Gk4k4YC
http://dl.acm.org/citation.cfm?id=2999792.2999959
http://dl.acm.org/citation.cfm?id=2999792.2999959
http://dl.acm.org/citation.cfm?id=2999792.2999959
http://is.muni.cz/publication/884893/en
http://is.muni.cz/publication/884893/en
https://books.google.com.tw/books?id=Kcl4AAAAIAAJ
http://aclweb.org/anthology/W18-0500
http://aclweb.org/anthology/W18-0500
http://aclweb.org/anthology/W18-0500

