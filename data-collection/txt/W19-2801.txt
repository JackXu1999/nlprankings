



















































Evaluation of named entity coreference


Proceedings of the 2nd Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2019), pages 1–7,
Minneapolis, USA, June 7, 2019. c©2019 Association for Computational Linguistics

1

Evaluation of Named Entity Coreference

Oshin Agarwal ∗
University of Pennsylvania

oagarwal@seas.upenn.edu

Sanjay Subramanian ∗
University of Pennsylvania
subs@seas.upenn.edu

Ani Nenkova
University of Pennsylvania

nenkova@seas.upenn.edu

Dan Roth
University of Pennsylvania

danroth@seas.upenn.edu

Abstract

In many NLP applications like search and in-
formation extraction for named entities, it is
necessary to find all the mentions of a named
entity, some of which appear as pronouns (she,
his, etc.) or nominals (the professor, the Ger-
man chancellor, etc.). It is therefore important
that coreference resolution systems are able to
link these different types of mentions to the
correct entity name. We evaluate state-of-the-
art coreference resolution systems for the task
of resolving all mentions to named entities.
Our analysis reveals that standard coreference
metrics do not reflect adequately the require-
ments in this task: they do not penalize sys-
tems for not identifying any mentions by name
to an entity and they reward systems even if
systems find correctly mentions to the same
entity but fail to link these to a proper name
(she–the student–no name). We introduce new
metrics for evaluating named entity corefer-
ence that address these discrepancies and show
that for the comparisons of competitive sys-
tems, standard coreference evaluations could
give misleading results for this task. We are,
however, able to confirm that the state-of-the
art system according to traditional evaluations
also performs vastly better than other systems
on the named entity coreference task.

1 Introduction

Coreference resolution is the task of identifying
all expressions in text that refer to the same en-
tity. In this paper we set out to provide an in-depth
analysis of the task specifically for named entities:
finding all references—either by name, pronoun or
nominal—to a named entity in the text.

Many language technology tasks focus on enti-
ties and our work is oriented towards practical uses
of the results of coreference resolution in down-
stream tasks. Named entities are often targets for

∗equal contribution

information extraction (Ji and Grishman, 2011),
biography summarization (Zhou et al., 2004) and
knowledge base completion tasks (West et al.,
2014). More relevant information can be ex-
tracted for these tasks if we also know which pro-
nouns and nominals refer to the entity. Similarly,
creation of proper noun ontologies (Mann, 2002)
can use patterns other than (proper noun–common
noun) if other references to the entity are known.

Recent work (Webster et al., 2018) has shown
that standard coreference datasets are biased and
high performance on these need not mean high
performance in downstream tasks. We argue that
the standard coreference metrics are not suitable
either from the perspective of downstream appli-
cations. Since applications require information
about entities and entities are usually identified by
their names, the evaluation metrics should focus
on the resolution of mentions to the correct name.
If all the pronouns referring to an entity are re-
solved correctly to each other but are not linked to
any name or are linked to a wrong name, the re-
sults would not be useful for downstream tasks.
Standard coreference metrics do not incorporate
these aspects and hence give high performance for
results unsuitable for further use. We also show
that the existing metrics are not sensitive to finding
any mention to an entity at all. They give higher
performance for systems that do not find a large
number of entities but do good coreference reso-
lution on the subset of entities they find.

This problem of coreference chains without any
named mentions being unsuitable has previously
been discussed in (Chen and Ng, 2013). The
authors argued that a name is more informative
than a nominal, which is more informative than
a pronoun so they assign different weights to
co-reference links (mention-antecedent pairs) in
a chain depending on the type of mentions the
link contains. They assign a higher weight to



2

a link having a name than one that doesn’t and
also higher weight to a link having a nominal
than a link that contains just pronouns. Simi-
larly, (Martschat and Strube, 2014) perform an er-
ror analysis for co-reference by choosing an an-
tecedent that is a name or a nominal in this or-
der because they are more informative than a pro-
noun. However, we argue that we should view the
coreference chains as a whole instead of individ-
ual links when evaluating systems for downstream
application. If a chain contains even one named
mention, it should be sufficient for using it in ap-
plications and we need not consider the mention
type in each link within the chain.

We introduce metrics focused on Named Entity
Coreference (NEC) which separate the identifica-
tion of entities and resolution of different mention
types, thus tackling the above issue and transpar-
ently tracking areas of system improvement.

2 Coreference Evaluation

Shared tasks on coreference (at CoNLL-2011 and
2012 (Pradhan et al., 2014) ) use the average of
three F1 scores as their official evaluation: MUC
(Vilain et al., 1995), B3 (Bagga and Baldwin,
1998) and CEAFE (Luo, 2005). Prior work
(Moosavi and Strube, 2016) discussed shortcom-
ing of these metrics and introduced the improved
link entity aware (LEA) score. Below we describe
each score in the context of downstream tasks. Let
K be the set of key (gold) clusters, and let R be
the set of response clusters.

MUC The recall for an entity is the minimum
number of links that would have to be added in
the predicted clusters containing any mention of
this entity, to make them connected and part of the
same cluster. Precision is computed by reversing
the role of gold and predicted clusters.

Recall =

∑
ki∈K |ki| − |p(ki)|∑

ki∈K(|ki| − 1)

where p(ki) is the partition of ki generated by in-
tersecting ki with the response entities.

Gold: {JohnDoe, he1, he2, he3} {RichardRoe, he4, he5}
Solution 1: {JohnDoe, he1, he2} {RichardRoe, he4}
Solution 2: {he1, he2, he3}{he4, he5}

Table 1: Hypothetical Solution 2 has no practical value.

B-cubed B3 works on the mention level. It iter-
ates over all gold-standard mentions of an entity,
averaging the recall of its gold cluster in its pre-
dicted cluster. It computes precision by reversing
the role of gold and predicted clusters.

Recall =

∑
ki∈K

∑
rj∈R

|ki∩rj |2
|ki|∑

ki∈K |ki|

CEAF CEAF first maps each gold cluster to a
predicted cluster. It then computes recall as the
number of similar mentions shared by the gold and
predicted clusters divided by the number of men-
tions in the gold cluster. Precision is equal to the
number of similar mentions shared by the gold and
predicted, divided by the number of mentions in
the predicted cluster. Numbers are reported either
per mention (CEAFm), or per entity (CEAFe).

Recall =

∑
ki∈K∗ φ(ki, g

∗(ki))∑
ki∈K φ(ki, ki)

where K∗ is the set of key entities in the optimal
one-to-one mapping and φ(·, ·) is a similarity mea-
sure for a pair of entities. In CEAFm, φ(ki, rj) =
|ki ∩ rj |, and in CEAFe, φ(ki, rj) = 2|ki∩rj ||ki|+|rj | .

LEA Recall is computed as the fraction of cor-
rectly resolved links between mentions. Results
for each entity are weighted by its number of men-
tions, so that resolving correctly an entity with
more mentions contributes more to the overall
score. Precision is computed by reversing the role
of gold and predicted clusters.

Recall =

∑
ki∈K

[
|ki| ×

∑
rj∈R

link(ki∩rj)
link(ki)

]
∑

ki∈K |ki|

where for any set S, link(S) denotes the number
of links between elements of S (so link(S) = |S|·
(|S| − 1)/2).

Solution 1 Solution 2
R P F1 R P F1

MUC 0.60 1 0.74 0.60 1 0.74
B-cub 0.51 1 0.67 0.51 1 0.67

CEAFm 0.71 1 0.83 0.71 1 0.83
CEAFe 0.82 0.82 0.82 0.82 0.82 0.82

LEA 0.42 1 0.60 0.42 1 0.60
NEC 0.71 1 0.83 0 1 0

Table 2: Evaluation of the hypothetical examples in Ta-
ble 1. NEC is the new metric introduced in Section 3.



3

The goal of NEC is to link all mentions refer-
ring to a named entity to the correct name. Con-
sider the example in Table 1. There are two enti-
ties, each with one named mention and a few pro-
nouns. Both solutions find the same number of
correct mentions pairs. However, solution 1 has
a named mention in each cluster but solution 2
has only pronouns. Standard evaluations have the
same values for both solutions (see Table 2) be-
cause they do not consider the types of mentions.

3 NEC Evaluation Metrics

The above example highlights the potential defi-
ciencies of standard coreference evaluations when
applied to NEC. Here we introduce a set of task-
specific criteria for the evaluation of NEC.1

3.1 NEC F1

In the gold-standard, all mentions to named en-
tities are grouped into chains. We wish to find
a chain corresponding to each entity in the sys-
tem output also. To map chains between the gold-
standard and the system output, we select for each
gold-standard chain, the predicted chain that has
the highest F1 score with respect to its mentions.
The NEC F1 score is the average of these highest
per entity scores.2

To compute the intersection between a gold-
standard and a system chain, we first augment each
gold-standard chain with a list of all variations of
the entity’s name. We rely on the gold-standard
named entity annotation and intersect this with the
membership in a coreference chain. This provides
lists of the full name, last name, occasionally nick-
names, i.e. {Frank Curzio, Francis X. Curzio,
Curzio}, {Dwayne Dog Chapman, Dog Chapman,
Chapman}. We consider a predicted chain to be a
candidate match for a gold chain only if it con-
tains at least one of the name variants. We do not
use exact mention match to find candidate chains
as the presence of the name can indicate which en-
tity the cluster is about. If the gold mention is ‘Mr
Joe from Boston’ and the system finds ’Mr Joe’,
we still consider the chain containing this mention
to be a candidate chain as the name can be deter-

1See supplementary material for examples of the errors.
2Although the task appears similar to Entity Linking

(EL) (Mihalcea and Csomai, 2007; Ratinov et al., 2011), it
does not involve linking an entity to a knowledge base (KB).
Not all entities even need to be in a KB. Also, EL typically
focuses on names and other nouns whereas coreference in-
cludes pronouns as well.

mined and other mentions may have been resolved
correctly.

For each named entity ki ∈ K, let Ni be the set
of response mentions that contain the full name of
ki. For a key named entity ki and a response entity
rj , the precision is defined to be p(ki, rj) =

|rj∩ki|
|rj |

and the recall is defined to be r(ki, rj) =
|rj∩ki|
|ki| .

The F1 for this pair of key entity and response en-
tity is then given by f(ki, rj) =

2p(ki,rj)r(ki,rj)
p(ki,rj)+r(ki,rj)

=
2|rj∩ki|
|rj |+|ki| . Then F1 for the key named entity ki is

F1i = max
rj∈R:rj∩Ni 6=∅

f(ki, rj)

We use an exact span matching between gold
and predicted mentions to calculate F1 to be con-
sistent with the existing scorers.

If a gold-standard chain does not get paired with
any system chain, the F1 for that chain is taken
to be zero. We find the overall F1 of the sys-
tem as the average of the F1 for each gold chain,
1
|K|

∑
ki∈K F1i.

3.2 Entity not Found

The NEC F1 gives a sense of overall performance
but mixes true purity of the system-discovered en-
tities and the ability to discover entities at all. “En-
tity not found” is the error when no NEC system
output overlaps with a gold standard chain. These
contribute a score of 0 for the average F1.3

3.3 Pronoun Resolution Accuracy

We also track the NEC F1 when only mentions of
given syntactic type are preserved in the chain—
name, pronoun and nominal. Of special interest
is to track performance when resolving pronouns.
Many of the errors on pronouns arise due to the
need for common-sense knowledge and reasoning.

3.4 Over-Splitting/Combination of Entities

We tracked the over-splitting (systems produce
multiple clusters for the same name) and the over-
combination of entities as well (placing mentions
to different named entities in the same cluster.
This error usually occurs when different people
have the same last name but also occasionally
when the names are completely different but the
roles of the people are similar. However, overall

3We consider only chains containing a named mention.
Chains that do not contain any named mention are filtered
out. More details on filtering in section 4.



4

PER ORG GPE
Chains

not found
NEC

F1
Coref

F1
Chains

not found
NEC

F1
Coref

F1
Chains

not found
NEC

F1
Coref

F1
(Raghunathan et al., 2010) 16% 0.55 0.50 34% 0.42 0.41 14% 0.67 0.56
(Clark and Manning, 2015) 36% 0.46 0.56 40% 0.39 0.46 21% 0.61 0.61
(Clark and Manning, 2016a,b) 21% 0.61 0.67 29% 0.50 0.52 17% 0.68 0.65
(Lee et al., 2017) 28% 0.58 0.69 26% 0.58 0.56 12% 0.76 0.68
(Lee et al., 2018) 7.5% 0.80 0.77 15% 0.69 0.61 8% 0.81 0.69

Table 3: Performance of systems. Chains not found and NEC F1 refer to the new named entity focused metrics.
Coref F1 refers to the evaluation combining MUC, B3 and CEAFE, on test data.

PER ORG GPE
Name Pronoun Nominal Name Pronoun Nominal Name Pronoun Nominal

(Raghunathan et al., 2010) 0.55 0.45 0.23 0.47 0.35 0.11 0.73 0.44 0.19
(Clark and Manning, 2015) 0.50 0.34 0.10 0.46 0.34 0.15 0.65 0.57 0.22
(Clark and Manning, 2016a,b) 0.66 0.49 0.15 0.54 0.47 0.33 0.72 0.59 0.41
(Lee et al., 2017) 0.64 0.41 0.15 0.65 0.48 0.39 0.80 0.70 0.47
(Lee et al., 2018) 0.85 0.58 0.26 0.76 0.64 0.47 0.85 0.77 0.51

Table 4: NEC F1 by type of mention. The errors on names are high, though it is possible to resolve these with
NER and string matching or similarity. Pronoun errors are high as expected.

such errors were quite small and similar for all sys-
tems and have thus not been included in the later
tables with results.

4 Evaluation of Systems

We make use of the relevant part of OntoNotes
coreference corpus (Pradhan et al., 2007) and
gold-standard annotations for named entities on
the same data to quantify the patterns in corefer-
ence of different named entity types (see the table
in the supplementary material) and to evaluate sys-
tems on the newswire, broadcast news and maga-
zine documents for PER, ORG and GPE entities.

Patterns in Coreference Named people, or-
ganizations and locations make up 38% of all
coreference clusters in OntoNotes (Pradhan et al.,
2007), yet 54% of all mentions that require coref-
erence resolution are mentions of these types. All
named entities are on average much less likely to
be singletons than a typical entity, mentioned only
once in the text and not requiring coreference res-
olution (De Marneffe et al., 2015). People, orga-
nizations and locations are most likely to be men-
tioned repeatedly: 68% of people, 51% of organi-
zations and 52% of locations named in text have at
least one other coreferent mention to them.

Named entities have a large portion of refer-
ences that are not by name. Nominals account
for less than 5% of the mentions in all genres for
PER, while the remaining mentions are split al-
most equally between names and pronouns. For
ORG, roughly half of the mentions are named, the

remaining are equally split between pronouns and
nominals. For GPE, roughly 70% of the mentions
are named and others are mostly pronouns.

Systems We evaluate the Stanford coreference
system, with its deterministic (Raghunathan et al.,
2010; Lee et al., 2011; Recasens et al., 2013),
statistical (Clark and Manning, 2015) and neural
(Clark and Manning, 2016a,b) versions, and the
neural end-to-end systems of (Lee et al., 2017) and
(Lee et al., 2018) on traditional and NEC metrics.

These general coreference systems find corefer-
ring expressions of any type and produce corefer-
ence chains for all mentioned entities. In NEC,
the goal is to find all mentions to an entity that has
been referred to by name at least once in the docu-
ment. The output of off-the-shelf coreference sys-
tems has to be filtered to keep only chains that con-
tain at least one mention noun phrase with a syn-
tactic head that is a entity’s name.4 For our evalua-
tion, we use the spaCy dependency parsing system
(Honnibal and Johnson, 2015) to detect whether a
name is the head of a mention, by checking that
no other word in the mention is an ancestor of the
name in the dependency parse tree. In evaluation,
we use gold NER tags to determine if the head is a
name. Note that the dependency parsing and gold
NER are not given to the systems but are used to
process their output.

Many system NEC chains did not have any

4Less strict filtering, such as the presence of an appropri-
ate pronoun could also indicate that it a specific type of entity.
For NEC, we insist on having at least one named mention.



5

named mentions. (Lee et al., 2017) does not have
a named mention in about 30% of the corefer-
ence chains on PER that do contain a personal or
possessive third person pronoun. This number is
about 20% for the CoreNLP neural system.

Table 3 shows the standard and NEC F1 on all
the systems. For PER, there are three notable
leaps of improvement according to the standard
coref evaluation: between the statistical and rule-
based CoreNLP systems, between their statistical
and neural systems and between the two versions
of the AllenNLP systems. Some of these improve-
ments contradict actual performance on NEC, no-
tably for the difference between the rule-based and
statistical systems. The other two improvements in
Coref F1 translate to improvements in NEC met-
rics. The difference between the statistical and
rule-based system is also falsely reflected in stan-
dard F1 for ORG and ORG entities. As expected,
(Lee et al., 2018) outperforms all the systems, with
(Lee et al., 2017) as a close second. Both per-
form much better than (Raghunathan et al., 2010)
and (Clark and Manning, 2015). (Clark and Man-
ning, 2016a,b) does slightly better than (Lee et al.,
2017) on PER entities. Notably, (Lee et al., 2018)
misses less than 10% of the chains for all entity
types compared to 20-40% by other systems.

Note that the performance varies considerably
across entity types. A top NER system such as
(Ratinov and Roth, 2009) that focus on PER, ORG
and GPE does not find a single named entity in
just 4.67%, 5.7% and 1.1% of chains respectively.
However, the percentage of chains not found is
much higher. It is possible that the non-named
mentions were resolved to each other but not to
any names so such chains got filtered out for the
NEC task. Future work involves developing coref-
erence systems driven by NER and producing re-
sults more suitable for downstream tasks.

We also separate the performance of the sys-
tems by mention type. The second panel of Ta-
ble 3 reveals that (Lee et al., 2018) outperforms all
the systems on each mention type for all the three
types of entities. Detection of named mentions
can be done with high accuracy by named entity
recognition systems (Stoyanov et al., 2009) and
the matching of names can also be done accurately
via string matching (Wacholder et al., 1997; Wick
et al., 2009). In spite of this, most systems do not
perform well on names. The mistakes on pronouns
and nominals are much higher as expected.

While (Lee et al., 2018) gets a better F1 on the
standard coreference metrics used as well, it im-
proves on many aspects of performance. It finds
more chains and even performs better resolution
of each mention type, making it more suitable for
downstream tasks.

5 Conclusion

We presented the task of Named Entity Corefer-
ence (NEC) and argued that the standard corefer-
ence metrics are not suitable for the evaluation of
this task. We introduced evaluation metrics that
tackle the shortcomings of the standard metrics for
the task and track the different errors made by sys-
tems. We showed that many off-the-shelf systems
do not perform well on these metrics. They out-
put many clusters without a link to any name or
a link to the incorrect name, making results un-
suitable for downstream applications. Our metrics
track different aspects of system performance and
help identify such issues.

Acknowledgments

This work was supported in part by Contract
HR0011-15-2-0025 with the US Defense Ad-
vanced Research Projects Agency (DARPA). The
views expressed are those of the authors and do
not reflect the official policy or position of the De-
partment of Defense or the U.S. Government.

References
Amit Bagga and Breck Baldwin. 1998. Entity–

based cross–document coreferencing using the vec-
tor space model. In 36th Annual Meeting of the As-
sociation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguis-
tics, COLING-ACL ’98, Université de Montréal,
Montréal, Quebec, Canada. Proceedings of the Con-
ference, pages 79–85.

Chen Chen and Vincent Ng. 2013. Linguistically
aware coreference evaluation metrics. In Proceed-
ings of the Sixth International Joint Conference on
Natural Language Processing, pages 1366–1374.

Kevin Clark and Christopher D Manning. 2015. Entity-
centric coreference resolution with model stacking.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
1405–1415.

Kevin Clark and Christopher D Manning. 2016a. Deep
reinforcement learning for mention-ranking coref-

http://aclweb.org/anthology/P/P98/P98-1012.pdf
http://aclweb.org/anthology/P/P98/P98-1012.pdf
http://aclweb.org/anthology/P/P98/P98-1012.pdf


6

erence models. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2256–2262.

Kevin Clark and Christopher D Manning. 2016b. Im-
proving coreference resolution by learning entity-
level distributed representations. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 643–653.

Marie-Catherine De Marneffe, Marta Recasens, and
Christopher Potts. 2015. Modeling the lifespan of
discourse entities with application to coreference
resolution. J. Artif. Int. Res., 52(1):445–475.

Matthew Honnibal and Mark Johnson. 2015. An im-
proved non-monotonic transition system for depen-
dency parsing. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1373–1378, Lisbon, Portugal. As-
sociation for Computational Linguistics.

Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies – Volume 1, HLT ’11, pages
1148–1158.

Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford’s multi-pass sieve corefer-
ence resolution system at the conll-2011 shared task.
In Proceedings of the 15th conference on compu-
tational natural language learning: Shared task,
pages 28–34. Association for Computational Lin-
guistics.

Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle-
moyer. 2017. End-to-end neural coreference reso-
lution. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 188–197.

Kenton Lee, Luheng He, and Luke Zettlemoyer. 2018.
Higher–order coreference resolution with coarse–
to–fine inference. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Volume 2 (Short Papers), pages
687–692.

Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In HLT/EMNLP 2005, Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Pro-
cessing, Proceedings of the Conference, Vancouver,
British Columbia, Canada, pages 25–32.

Gideon S Mann. 2002. Fine–grained proper noun on-
tologies for question answering. In Proceedings of
the 2002 workshop on Building and using seman-
tic networks–Volume 11, pages 1–7. Association for
Computational Linguistics.

Sebastian Martschat and Michael Strube. 2014. Recall
error analysis for coreference resolution. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 2070–2081.

Rada Mihalcea and Andras Csomai. 2007. Wikify!:
Linking documents to encyclopedic knowledge.

Nafise Sadat Moosavi and Michael Strube. 2016.
Which coreference evaluation metric do you trust?
a proposal for a link-based entity aware metric. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 632–642.

Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-
uard Hovy, Vincent Ng, and Michael Strube. 2014.
Scoring coreference partitions of predicted men-
tions: A reference implementation. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 30–35, Baltimore, Maryland. Associa-
tion for Computational Linguistics.

Sameer S Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. Ontonotes: A unified relational
semantic representation. International Journal of
Semantic Computing, 1(04):405–419.

Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 492–501.
Association for Computational Linguistics.

L. Ratinov and D. Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL, pages 147–155.

Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to wikipedia. In Proc. of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).

Marta Recasens, Marie-Catherine de Marneffe, and
Christopher Potts. 2013. The life and death of dis-
course entities: Identifying singleton mentions. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 627–633.

Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state–
of–the–art. In ACL 2009, Proceedings of the 47th
Annual Meeting of the Association for Computa-
tional Linguistics and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, Singapore, pages 656–664.

http://dl.acm.org/citation.cfm?id=2831407.2831417
http://dl.acm.org/citation.cfm?id=2831407.2831417
http://dl.acm.org/citation.cfm?id=2831407.2831417
https://aclweb.org/anthology/D/D15/D15-1162
https://aclweb.org/anthology/D/D15/D15-1162
https://aclweb.org/anthology/D/D15/D15-1162
http://dl.acm.org/citation.cfm
http://dl.acm.org/citation.cfm
http://aclweb.org/anthology/H/H05/H05-1004.pdf
http://aclweb.org/anthology/H/H05/H05-1004.pdf
http://www.aclweb.org/anthology/P14-2006
http://www.aclweb.org/anthology/P14-2006
http://cogcomp.cs.illinois.edu/papers/RatinovRo09.pdf
http://cogcomp.cs.illinois.edu/papers/RatinovRo09.pdf
http://cogcomp.org/papers/RRDA11.pdf
http://cogcomp.org/papers/RRDA11.pdf
http://www.aclweb.org/anthology/P09-1074
http://www.aclweb.org/anthology/P09-1074
http://www.aclweb.org/anthology/P09-1074


7

Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model–
theoretic coreference scoring scheme. In Proceed-
ings of the 6th Conference on Message Understand-
ing, MUC6 ’95, pages 45–52.

Nina Wacholder, Yael Ravin, and Misook Choi. 1997.
Disambiguation of proper names in text. In Pro-
ceedings of the Fifth Conference on Applied Natural
Language Processing, ANLC ’97, pages 202–208.

Kellie Webster, Marta Recasens, Vera Axelrod, and Ja-
son Baldridge. 2018. Mind the gap: A balanced
corpus of gendered ambiguous pronouns. Transac-
tions of the Association for Computational Linguis-
tics, 6:605–617.

Robert West, Evgeniy Gabrilovich, Kevin Murphy,
Shaohua Sun, Rahul Gupta, and Dekang Lin. 2014.
Knowledge base completion via search-based ques-
tion answering. In Proceedings of the 23rd inter-
national conference on World wide web, pages 515–
526. ACM.

Michael Wick, Aron Culotta, Khashayar Rohani-
manesh, and Andrew McCallum. 2009. An entity
based model for coreference resolution. In Proceed-
ings of the 2009 SIAM International Conference on
Data Mining, pages 365–376. SIAM.

Liang Zhou, Miruna Ticrea, and Eduard Hovy. 2004.
Multi–document biography summarization. In Pro-
ceedings of the 2004 Conference on Empirical Meth-
ods in Natural Language Processing.

https://doi.org/10.3115/1072399.1072405
https://doi.org/10.3115/1072399.1072405
https://doi.org/10.3115/974557.974587

