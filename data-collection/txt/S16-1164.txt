



















































Pomona at SemEval-2016 Task 11: Predicting Word Complexity Based on Corpus Frequency


Proceedings of SemEval-2016, pages 1047–1051,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

Pomona at SemEval-2016 Task 11:
Predicting Word Complexity Based on Corpus Frequency

David Kauchak
Computer Science Department

Pomona College
Claremont, CA

david.kauchak@pomona.edu

Abstract

We introduce a word frequency-based classi-
fier for the SemEval 2016 complex word iden-
tification task (#11). Words with lower fre-
quency are predicted as complex based on a
threshold optimized for G-score. We exam-
ine three different corpora for calculating fre-
quencies and find English Wikipedia to per-
form best (ranked 13th on the SemEval task),
followed by the Google Web Corpus and lastly
Simple English Wikipedia. Bagging is also
shown to slightly improve the performance of
the classifier. Overall, we find word frequency
to be a strong predictor of complexity. On the
SemEval “test” set, a frequency classifier that
uses the optimal frequency threshold performs
on-par with the best submitted system and a
system trained using only 500 labeled exam-
ples split from the test set achieves results that
are only slightly below the best submitted sys-
tem.

1 Introduction

Text simplification aims to transform text into
more accessible versions while retaining the origi-
nal meaning. A frequent subproblem of the general
simplification problem is complex word identifica-
tion: identify words in a text that are difficult to un-
derstand for the reader. Complex word identification
is critical in lexical simplification algorithms where
the simplification process is done a word at a time;
frequently, simplification is broken into two steps,
first identifying the complex words that need sim-
plifying and, second, determining substitutions for
these words (Shardlow, 2014). Even for simplifi-

cation systems that make sentence-level transforma-
tions (Siddharthan, 2014) complex word identifica-
tion can be used as an additional feature function in
the model and as a development tool to help measure
progress. Additionally, in some domains such as
health and medicine, accuracy is critical and semi-
automated simplification tools are common (Leroy
et al., 2012). In these domains, complex word iden-
tification is useful to help guide the simplification
process by both identifying which words need to be
simplified and filtering/ranking possible candidate
substitutions (Leroy et al., 2013).

In this paper, we explore the use of word fre-
quency as a predictor of the complexity of that word.
Corpus studies have shown that simpler texts contain
more frequent words than more complicated texts
(Breland, 1996; Pitler and Nenkova, 2008; Leroy
et al., 2012). User studies have also shown a cor-
relation between word frequency and whether users
know the definition of a word (Leroy and Kauchak,
2013). In semi-automated text simplification ap-
proaches, replacing less frequent words with higher
frequency synonyms has been shown to produce text
that people view as simpler and is easier to under-
stand (Leroy et al., 2013).

2 Bagged Frequency Classifier

Given a sentence S = s1s2...sm and a word in that
sentence, sj , the complex word identification task is
to predict whether that word is complex (1) or not
(0). Labeled examples are triples consisting of the
sentence, the word and the label, i.e. 〈S, sj , {0, 1}〉,
and unlabeled examples consist only of the sentence
and word 〈S, sj〉. We view the problem as a super-

1047



vised classification problem: given a collection of
training examples, the goal is to learn a classifier to
predict the label of unlabeled examples. See the Se-
mEval Task 11 description for more details (Paet-
zold and Specia, 2016).

We utilize bagging (bootstrap resampling) to learn
and combine multiple basic classifiers that predict
by thresholding the frequency of occurrence of the
word in question (sj) in an external corpus. Classi-
fication is then done by majority vote of these clas-
sifiers. The subsections below provide more details.

2.1 Basic frequency classifier
The basic frequency classifier predicts the word
complexity using only a single feature, the fre-
quency of the word in question (sj) in a corpus.
Given an unlabeled example, the classifier predicts
based on a learned threshold, α:

predict(〈S, sj〉) =
{
1 if freq(sj) < α
0 otherwise

with the assumption that words that occur less fre-
quently in a corpus are more complex.

To train the basic classifier, we select α in an ex-
haustive fashion by considering all possible frequen-
cies of seen in the training examples as candidate
thresholds. Specifically, for each training example
〈S, sj , {0, 1}〉, we consider using α = freq(sj).
We select the α that maximizes the G-score on the
training set, where the G-score is defined as:

2 ∗ accuracy ∗ recall
accuracy + recall

We chose to optimize the G-score since it was the
metric used for evaluation in the SemEval task (Paet-
zold and Specia, 2016), though other metrics could
be used instead.

2.2 Word frequencies
Word frequencies can be pre-calculated from any
corpus. For this paper we examined three corpora:
articles from Simple English Wikipedia1, articles
from English Wikipedia2 and the Google Web Cor-
pus (Brants and Franz, 2006). For the Wikipedia ar-
ticles we used the document aligned data set created

1https://simple.wikipedia.org/
2https://en.wikipedia.org/

by Kauchak (2013) consisting of approximately 60K
articles on the same topics from each Wikipedia.

To collect word frequencies for the two Wikipedia
variants, tokenization was first performed using the
Stanford CoreNLP PTBTokenizer (Manning et al.,
2014) and then frequencies were calculated. For the
Google Web Corpus, we used the unigram counts.
In all corpora, all capitalization variants were aggre-
gated, e.g. occurrences of “natural” and “Natural”
would both be counted towards the same word form.

2.3 Bagging

To improve classifier performance and reduce vari-
ance we investigated the use of bagging (Breiman,
1996), also referred to as bootstrap resampling. An
ensemble classifier is learned by training multiple
basic frequency classifiers. Specifically, let train
be a training set consisting of size(train) labeled
examples and b be the number of basic classifiers to
be learned and combined. The bagged classifier is
trained by repeatedly

1) generating a new training sample S ⊆ train
containing size(train) labeled examples by
randomly sampling with replacement from
train and then

2) training a new basic frequency classifier on S.

These two steps are repeated b times resulting in b
different classifiers. To classify a new, unlabeled ex-
ample, each of the b classifiers makes a prediction
and the final label is the label with the majority vote,
with ties going to not complex (0), since this was the
more frequent class.

3 Experiments

We submitted two systems to the SemEval Com-
plex Word Identification challenge (Task 11), which
used the same parameter settings and only differed
in where the corpus frequencies were collected, En-
glish Wikipedia (NormalBag) and the Google Web
Corpus (GoogleBag). Both systems used b = 10
rounds of bagging, which was shown experimentally
to have the best scoring value, using repeated rounds
of 10-fold validation. We also discuss results here
for a system which used Simple English Wikipedia
word frequencies, though we did not submit it to the
challenge (for consistency, we denote it SimpleBag).

1048



Test Train
System G-score Accuracy Recall G-score Accuracy Recall
NormalBag 0.714 0.603 0.872 0.684 0.665 0.705
GoogleBag 0.691 0.568 0.881 0.675 0.648 0.704
SimpleBag 0.674 0.542 0.891 0.674 0.630 0.725

Table 1: Results for the bagged systems (b = 10) with word frequencies calculated on the three different corpora. Results are
shown for the test and training sets for systems trained on the training data.

The task consisted of a training data set
(N = 2,237), which was available during develop-
ment, and a test data set (N = 88,221) on which the
competition was scored and the labels were only re-
leased after the competition (Paetzold and Specia,
2016). We use both data sets here to analyze the
performance of the classifiers.

3.1 The impact of corpus choice

Table 1 shows the results for the classifiers trained
using bagging with word frequencies calculated
from the three different corpora. English Wikipedia
performed the best, followed by Google Web Corpus
and finally Simple English Wikipedia. The top two
entries were entered into the SemEval competition
and ranked 13th and 16th respectively out of 51 sys-
tems (42 team submitted systems and 9 baseline sys-
tems). We hypothesize that English Wikipedia rep-
resents a good compromise between size/coverage
and corpus quality; even though NormalBag had
slightly lower recall than the other two, it was able
to achieve that recall with a significantly higher ac-
curacy.

To verify that the differences in performance be-
tween the three systems were significant, we used
bootstrap resampling with a paired sample t-test
(Koehn, 2004). Based on 100 random samples, all
differences between all metrics and all systems were
significant (p < 0.0001, with Bonferroni correc-
tion to correct for testing multiple different compar-
isons).

Overall, relative to other systems that were sub-
mitted for the SemEval task, these frequency-based
classifiers biased towards recall, e.g. the Google fre-
quency and English Wikipedia frequency systems
ranked 3rd and 5th with respect to recall (of the 42
team submitted systems).

Train
Corpus basic G-score bagged G-score
Normal 0.677 0.680
Google 0.668 0.667
Simple 0.665 0.669

Table 2: Comparison of the basic frequency classifiers and their
bagged counterparts with b = 10 on the training data averaged

over 100 random 10-fold samples. Systems that were signifi-

cantly different between the basic and bagged are in bold.

3.2 The impact of bagging

To measure the impact of bagging on the prediction
performance of the systems, for each corpus source,
we compared the basic frequency classifier to the
bagged variant. We generated 100 random 10-fold
partitions of the training data and performed 10-fold
cross validation on each for each system variant. We
averaged the results across the each 10-fold set re-
sulting in 100 scores for each of the systems.

Table 2 shows the averages over these 100 scores.
For both of the Wikipedia variants (Normal and Sim-
ple) bagging provided a small increase in perfor-
mance (p < 0.0001 based on a paired t-test). For
the Google frequencies the performance actually de-
creased, though this decrease was not significant.

To understand the effect that b (the number of
bootstrap samples) has on the performance of the
classifier, we compared the performance of the clas-
sifier with b = 1, 2, ..., 100. Figure 1 shows a plot of
the G-score versus the number of bags used by the
classifier for the NormalBag classifier on the train-
ing set. As with the previous experiment, to partially
mitigate noise, we generated 100 randomly 10-fold
sets and averaged the results across all of these to
generate the data.

For small b, increasing the number of classifiers
voting does increase the performance of the classi-
fier. However, after around b = 10 adding more

1049



 0.6775

 0.678

 0.6785

 0.679

 0.6795

 0.68

 0.6805

 0.681

 0  10  20  30  40  50  60  70  80  90  100

G
-s

co
re

b = number of bootstrap samples

Figure 1: G-score for the NormalBag classifier with varying
number of boostrap samples. Results are averages over 100 ran-

dom 10-fold samples of the training data.

classifiers degrades the performance with the classi-
fier. Although the difference is small for b = 1 vs.
b = 10 (0.677 vs. 0.680), the difference is statisti-
cally significant.

3.3 Limits of frequency-based classification

Using English Wikipedia frequencies and the opti-
mal frequency threshold (i.e. α), the basic thresh-
old classifier achieves a G-score of 0.779 on the test
data set. This is slightly higher than the best scor-
ing SemEval system, which achieved 0.774. Clearly
frequency provides a strong signal for word com-
plexity.

The previous experiment assumes an unreason-
able scenario where we know the labels and can
pick the optimal value. To better understand the im-
pact of frequency, we split the test data into 10-folds
and performed 10-fold cross-validation analysis us-
ing the basic threshold classifier, training the thresh-
old on 90% of the SemEval “test” data and then
testing on the remaining 10%. In this scenario, the
threshold classifier still achieves a G-score of 0.764,
only slightly less than the score achieved using the
optimal threshold.

0.764 is still significantly higher than the score
achieved by the system when trained on the SemEval
“training” set. Two possible differences exist be-
tween the training and testing data. First, the test
data is two orders of magnitude larger than the orig-
inal training data. This additional data could result
in a more reliable classifier. Alternatively, train and

 0.5

 0.55

 0.6

 0.65

 0.7

 0.75

 0.8

 0  100  200  300  400  500  600  700  800  900  1000

G
-s

co
re

training size

Figure 2: G-score for the basic threshold classifier using En-
glish Wikipedia frequencies for increasing training data size.

Here the training data is a subset of the SemEval “test” set.

test were generated in different ways and could have
different characteristics.

To investigate this, we held out 10% of the “test”
data set as testing data and trained the basic thresh-
old classifier on increasing amounts of the remain-
ing 90%. Figure 2 shows the G-score for training
sizes up to 1,000 (the G-score mostly stabilized be-
yond 1,000 with only minor variation). Even with
only 250 training examples, the classifier already
achieves a G-score of 0.748 and with 500 training
examples, it achieves 0.752, only a little less than
the final score using all of the training data of 0.760.
For the frequency classifier, more the data domain,
and less the size, accounts for the differences in per-
formance seen.

4 Conclusion

In this paper, we described our entry for the complex
word identification SeEval 2016 task (#11). We uti-
lize word frequency to classify complexity, with less
frequent words being classified as complex. As has
been seen in previous corpus studies, frequency is a
very strong predictor of the complexity of a word.
However, the corpus where those frequencies are
measured does play a role in performance. We found
that English Wikipedia performed best for this par-
ticular task. Future research is needed to investigate
this phenomena more broadly.

1050



References

Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1. Linguistic Data Consortium, Philadelphia.

Leo Breiman. 1996. Bagging predictors. Machine learn-
ing.

Hunter M Breland. 1996. Word frequency and word dif-
ficulty: A comparison of counts in four corpora. Psy-
chological Science.

William Coster and David Kauchak. 2011. Simple En-
glish Wikipedia: A new text simplification task. In
Proceedings of ACL.

David Kauchak. 2013. Improving text simplification lan-
guage modeling using unsimplified text data. In Pro-
ceedings of ACL.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP.

Gondy Leroy and David Kauchak. 2013. The effect of
word familiarity on actual and perceived text difficulty.
Journal of American Medical Informatics Association.

Gondy Leroy, James Endicott, Obay Mouradi, David
Kauchak, and Melissa Just. 2012. Improving per-
ceived and actual text difficulty for health information
consumers using semi-automated methods. In Amer-
ican Medical Informatics Association (AMIA) Fall
Symposium.

Gondy Leroy, James E. Endicott, David Kauchak, Obay
Mouradi, and Melissa Just. 2013. User evaluation of
the effects of a text simplification algorithm using term
familiarity on perception, understanding, learning, and
information retention. Journal of Medical Internet Re-
search (JMIR).

Christopher D Manning, Mihai Surdeanu, John Bauer,
Jenny Rose Finkel, Steven Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural language
processing toolkit. In ACL (System Demonstrations).

Courtney Napoles and Mark Dredze. 2010. Learn-
ing simple Wikipedia: A cogitation in ascertaining
abecedarian language. In Proceedings of HLT/NAACL
Workshop on Computation Linguistics and Writing.

Gustavo H. Paetzold and Lucia Specia. 2016. Semeval
2016 task 11: Complex word identification. In Pro-
ceedings of the 10th International Workshop on Se-
mantic Evaluation (SemEval 2016).

Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In Proceedings of the conference on empirical
methods in natural language processing. Association
for Computational Linguistics.

Matthew Shardlow. 2014. A survey of automated text
simplification. International Journal of Advanced
Computer Science and Applications.

Advaith Siddharthan. 2014. A survey of research on text
simplification. ITL-International Journal of Applied
Linguistics.

1051


