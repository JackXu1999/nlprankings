


































Alternative measures of word relatedness in distributional semantics

Alina Maria Ciobanu
Faculty of Mathematics
and Computer Science
University of Bucharest

alina.ciobanu@my.fmi.unibuc.ro

Anca Dinu
Faculty of Foreign Languages

and Literatures
University of Bucharest

anca d dinu@yahoo.com

Abstract
This paper presents an alternative method
to measuring word-word semantic relat-
edness in distributional semantics frame-
work. The main idea is to represent tar-
get words as rankings of all co-occurring
words in a text corpus, ordered by their
tf – idf weight and use a metric between
rankings (such as Jaro distance or Rank
distance) to compute semantic relatedness.
This method has several advantages over
the standard approach that uses cosine
measure in a vector space, mainly in that
it is computationally less expensive (i.e.
does not require working in a high dimen-
sional space, employing only rankings and
a distance which is linear in the rank’s
length) and presumably more robust. We
tested this method on the standard WS-
353 Test, obtaining the co-occurrence fre-
quency from the Wacky corpus. The re-
sults are comparable to the methods which
use vector space models; and, most impor-
tantly, the method can be extended to the
very challenging task of measuring phrase
semantic relatedness.

1 Introduction
This paper presents a method of measuring word-
word semantic relatedness in the distributional se-
mantics (DS) framework.

DS relies on a usage-based perspective on
meaning, assuming that the statistical distribution
of words in context plays a key role in characteriz-
ing their semantic behavior. The idea that word co-
occurrence statistics extracted from text corpora
can provide a basis for semantic representations
can be traced back at least to Firth (1957): ”You
shall know a word by the company it keeps” and
Harris (1954): ”words that occur in similar con-
texts tend to have similar meanings”. This view is

complementary to the formal semantics perspec-
tive, focusing on the meaning of content words,
(such as nouns, adjectives, verbs or adverbs) and
not on grammatical words (prepositions, auxiliary
verbs, pronouns, quantifiers, coordination, nega-
tion), which are the focus of formal semantics.
Since many semantic issues come from the lexi-
con of content words and not from grammatical
terms, DS offers semantical insight into problems
that cannot be addressed by formal semantics.

Moreover, DS Models can be induced fully au-
tomatically on a large scale, from corpus data.
Thus, a word may be represented by a vector
in which the elements are derived from the oc-
currences of the word in various contexts, such
as windows of words (Lund and Burgess, 1996),
grammatical dependencies (Lin, 1998; Padó and
Lapata, 2007), and richer contexts consisting of
dependency links and selectional preferences on
the argument positions (Erk and Padó, 2008).

The task of measuring word-word relatedness
was previously performed in DS by using vector
space models (see (Turney and Pantel, 2010) for
an excellent survey of vector-space models), that is
employing high dimensional matrices to store co-
occurrence frequency of target words and some set
of dimension words, usually highly frequent (but
not grammatical) words. The relatedness of two
target words was typically given by the cosine of
the angle between their vectors. Instead of using
vector space models, we propose to represent the
target words only by rankings (vectors) of words
in their decreasing order of co-occurrence fre-
quency or their tf – idf weight. The tf – idf weight
increases with the number of co-occurrences and
with the ”selectiveness” of the term - the fewer dis-
tinct words it occurs with, the higher the weight.

This proposal has some advantages, as dis-
cussed in Approach section. We can measure
the semantic relatedness between two target words
by computing the distance between the two cor-

80



responding rankings, using distances defined on
rankings.

In the remaining of the paper we will present
our approach, describe the data we have used,
compare the results and draw the conclusions.

2 Approach

The method we propose is meant to measure word
- word semantic relatedness, in a bag of words
model, using 4 different distances (Rank distance,
MeanRank distance, CosRank distance and Jaro
distance) between rankings. To do so, instead of
representing words in vector spaces, we represent
them as rankings of co-occurring words ordered
after their semantic contribution, i.e. arranged in
their raw co-occurrence frequency and, separately,
in their tf – idf weight. We thus take into consider-
ation all words that co-occurred with a target word,
not just a predefined set of dimension words.

We define the Rank distance (variants) and the
Jaro distance, as it follows.

A ranking is an ordered list and is the result of
applying an ordering criterion to a set of objects.
Formally (Dinu, 2005), we have:

Let U = {1, 2, ...,#U} be a finite set of objects,
named universe (we write #U for the cardinality
of U ). A ranking over U is an ordered list: τ =
(x1 > x2 > ... > xd), where xi ∈ U for all
1 ≤ i ≤ d, xi #= xj for all 1 ≤ i #= j ≤ d, and > is
a strict ordering relation on the set {x1, x2, ..., xd}.

A ranking defines a partial function on U where
for each object i ∈ U , τ(i) represents the position
of the object i in the ranking τ .

The order of an object x ∈ U in a ranking σ of
length d is defined by ord(σ, x) = |d+1− σ(x)|.
By convention, if x ∈ U \ σ, then ord(σ, x) = 0.

Given two partial rankings σ and τ over the
same universe U , the Rank distance between them
is defined as:

∆(σ, τ) =
∑

x∈σ∪τ
|ord(σ, x)− ord(τ, x)|.

MeanRank distance is the average value of
Rank distance computed when elements are
ranked top-down and Rank distance computed
when elements are ranked bottom-up.

Given two full rankigs σ and τ over the same
universe U with #U = n, CosRank distance
(Dinu and Ionescu, 2012) is defined as follows:

∆(σ, τ) =
< σ, τ >
||σ|| · ||τ || =

∑
x∈U ord(σ, x)× ord(τ, x)

12 + 22 + ...+ n2

Jaro distance (Jaro, 1989) is a measure which
accounts for the number and position of com-
mon characters between strings. Given two strings
wi = (wi1, ..., wim) and wj = (wj1, ..., wjn), the
number of common characters for wi and wj is the
number of characters wik in wi which satisfy the
condition:

∃wj l in wj : wik = wj l and |k − l| ≤
max(m,n)

2
− 1

Let c be the number of common characters in
wi and wj and t the number of character trans-
positions (i.e. the number of common characters
in wi and wj in different positions, divided by 2).
Jaro distance is defined as follows:

∆(wi, wj) =
1
3
∗
(

c
m

+
c
n
+

c− t
c

)

We computed straightforwardly the distances
between pairs of target words in the Word Simi-
larity 353 Test. WS-353 Test is a semantic relat-
edness test set consisting of 353 word pairs and a
gold standard defined as the mean value of seman-
tic relatedness scores, assigned by up to 17 human
judges. Finally, we used Spearman’s correlation
to compare the obtained distances to the gold stan-
dard.

One advantage of this technique over the stan-
dard application of the cosine measure in vecto-
rial space is that it doesn’t have to deal with high
dimensional matrices, and thus no techniques of
reducing dimensionality of the vector space are
required. Rank distance only uses rankings (or-
dered vectors) of semantically relevant words for
each target word. It does not even need that
these rankings contain the same words or have the
same length (number of words). Computing the
four distances between the rankings of two target
words is linear in the length of the rankings. Thus,
the method is much less computationally expen-
sive than standard vector space models used in dis-
tributional semantics for the task of word-word se-
mantic relatedness.

Also, we expect the method to be more robust
compared to traditional vector space models, since
rankings of features tend to vary less then the raw
frequency with the choice of corpus.

But most importantly, it opens the perspective
of experimenting with new methods of composing
(distributional) meaning by aggregating rankings
(Dinu, 2005), instead of combining (adding, mul-
tiplying) vectors.

81



2.1 The data
We used the publicly available Wacky corpus (Ba-
roni et al., 2009). The corpus is lemmatized and
pos tagged. As it is usual in distributional seman-
tics, we only targeted content words and not gram-
matical words. Here is the list with the pos tags we
have employed:

• JJ adjective, e.g. green
• JJR adjective, comparative, e.g. greener
• JJS adjective, superlative, e.g. greenest
• NN noun, singular or mass, e.g. table
• NNS noun plural, e.g. tables
• NPS proper noun, plural, e.g. Vikings
• RB adverb, e.g. however, usually, naturally,

here, good
• VV verb, base form, e.g. take
• VVD verb, past tense, e.g. took
• VVG verb, gerund/present participle, e.g.

taking
• VVN verb, past participle, e.g. taken
• VVP verb, sing. present,non-3d, e.g. take
• VVZ verb, 3rd person sing. present, e.g.

takes

Accordingly, we have extracted from Wacky
corpus the 10 words window co-occurrence vec-
tors for the words in WS-353 Test (Finkelstein et
al., 2002). WS-353 Test is a semantic relatedness
test set consisting of 353 word pairs and a gold
standard defined as the mean value of evaluations
by up to 17 human judges. The value scale for the
test is from 0 to 10: completely unrelated words
were assigned a value of 0, while identical words
a value of 10. Although this test suite contains
some controversial word pairs, and there are other
test suits such as in (Miller and Charles, 1991) and
(Rubenstein and Goodenough, 1965), it has been
widely used in the literature and has become the
de facto standard for semantic relatedness measure
evaluation. For all the 437 target-words in WS-
353 Test, we computed the raw co-occurrence fre-
quency tft,d of terms t (base-word) and d (target-
word), defined as the number of times that t and d
co-occurred. We preprocessed the data, as it fol-
lows:

• we deleted all non-English words;
• we separated hyphenated words and recom-

puted the weights accordingly;
• we eliminated all other words containing

non-letter characters;

Then we standardly processed the raw co-
occurrence frequencies, transforming it into the tf
– idf weight: wt,d = (1+lgtft,d)∗lgN/dft, where
N = 437 (the total number of words we are com-
puting vectors for) and dft is the number of tar-
get words t co-occurs with. The tf – idf weight
increases with the number of co-occurrences of
t and d (co-occurrence frequency) and increases
with the ”selectiveness” of the term - the fewer dis-
tinct words it occurs with, the higher the weight.

We then computed the distances between pairs
of target words both for raw frequencies and for tf
– idf weights, for different lengths of the rankings,
starting with a length of only 10 and adding 10 at
a time until 2000.

3 Results

We summarize our results in Figure 1: one graphic
for experiments with raw frequencies and one for
experiments with tf – idf weight. On the OX axis
we represent the length of the rankings (up to the
first 2000 words) and on the OY axis the value of
human/machine correlation. We only represent the
best 3 performing distances, namely Rank, Cos-
Rank and Jaro, along with the standard Cosine dis-
tance (for comparison).

Method Source Spearman Correlation
Hughes and Ramage (2007) WordNet 0.55

Finkelstein et al. (2002) LSA, Combination 0.56
Gabrilovich and Markovitch (2007) ODP 0.65

Agirre et al. (2009) Web Corpus 0.65
Agirre et al. (2010) WordNet 0.69

Gabrilovich and Markovitch (2007) Wikipedia 0.75
Agirre et al. (2009) Combination 0.78

This work Wacky 0.55

Table 1: Comparison with vector space experi-
ments for WS-353 Test

For the raw co-occurrence, one observes that
until the length of 1000, the best performing dis-
tance was Jaro distance, followed by CosRank,
Rank, all three of them outperforming Cosine. Be-
tween a length of 1000 and 2000, the order re-
verses and Cosine is the best performing distance.
An explanation for this is on the one hand that
Jaro and Rank distances need no preprocessing
like computing tf – idf weight and, on the other,
that words ranked on places over a certain thresh-
old (in this case 1000) are, in fact, irrelevant (or
even represent noise) for the semantic representa-
tion of the target word. For the tf – idf weight, the
traditional Cosine distance performs best, while
CosRank is on the second place.

Overall, it turns out that the differences are
minor and that measuring the distances between

82



(a) Results for experiments with raw frequencies (b) Results for experiments with tf – idf weights
Figure 1: Results for experiments on WS-353 Test with co-occurrence frequencies from the Wacky corpus

rankings instead of vectors is a valid option. The
results may thus be further used as baseline for
experimenting with this method, like, for instance
taking syntactic structure into account.

As we can see in Table 1, the best correlation
value of 0.55 (obtained by CosRank computed on
the tf – idf weights) is identical to the baseline cor-
relation values for the vector space experiments.

When inspecting the worst mismatches between
human/machine relatedness judgments between
pairs of words, we observed that most of them
were following a pattern, namely lower values as-
signed by humans almost always corresponded to
much higher values computed by machine, such in
the following examples given in Table 2:

Word Pair Human Distance Machine Distance (Jaro)
(month, hotel) 1,81 6,239567

(money,operation) 3,31 6,40989
(king, cabbage) 0,23 4,171145
(coast, forest) 3,15 6,409761

(rooster, voyage) 0,62 4,656631
(governor, interview) 3,25 6,08319

(drink, car) 3,04 5,931482
(day, summer) 3,94 6,576498

(architecture, century) 3,78 5,927852
(morality, marriage) 3,69 5,450308

Table 2: Comparison with vector space experi-
ments for WS-353 Test

One can intuitively speculate about the rea-
son of these differences; for instance, the pairs
(summer, day) and (king, cabbage) are present
in the data as collocations: ”summer day” and
”king cabbage”, which is a very large variety
of cabbage. The other pairs ((month, hotel),
(money,operation), (rooster, voyage), etc.) seem
to allow for explanations based on pragmatic in-
formation present in the data.

4 Conclusions and further work

We introduced in this paper an alternative method
to measuring word-word semantic relatedness; in-
stead of using vector space models, we proposed
to represent the target words only by rankings
(vectors) of words in their decreasing order of
co-occurrence frequency; we computed the word-

word relatedness by four different distances. We
tested this method on the standard WS-353 Test,
obtaining the co-occurrence frequency from the
Wacky corpus. The Spearman correlation with hu-
man given scores are around the baseline for vec-
tor space models, so there is hope for improve-
ment. The method is computationally less ex-
pensive. Furthermore, it provides a new frame-
work for experimenting with distributional seman-
tic compositionality, since our method can be ex-
tended from measuring word-word semantic relat-
edness to evaluating phrasal semantics. This is
in fact one of the most challenging streams of re-
search on distributional semantics: finding a prin-
cipled way to account for natural language com-
positionality.

In the future, we will extend the contribution in
this paper to evaluating phrase semantics, that dif-
fers from all the above methods in that it does not
try to learn weights or functions for the vectors,
but instead combines or aggregates two vectors
containing words ranked in their semantic contri-
bution, in order to obtain a vector for the resulting
phrase. When combining two word vectors, one
obtains an aggregation set which contains all vec-
tors for which the sum of the distances between
them and the two vectors is minimum. The vector
in the aggregation set that is closest to the syntactic
head of the new phrase is chosen to be the vector
representing it. Thus, the syntactic structure of the
phrase is taken into account. The word - phrase se-
mantic similarity can be computed as in the exper-
iment reported in this paper and the obtained val-
ues compared to some gold standard, like, for in-
stance, in SemEval 2013 task, Evaluating Phrasal
Semantics or like the dataset in (Mitchell and La-
pata, 2008).

Acknowledgments
This work was supported by the research project
PN-II-ID-PCE-2011-3-0959.

83



References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova,

M. Pasca, and A. Soroa. 2009. A Study on
Similarity and Relatedness Using Distributional and
WordNet-based Approaches. In Proceedings of
the Conference of the North American Chapter of
the Association of Computational Linguistics, HLT-
NAACL ’09, pages 19–27.

E. Agirre, M. Cuadros, G. Rigau, and A. Soroa. 2010.
Exploring Knowledge Bases for Similarity. In Lan-
guage Resources and Evaluation 2010.

M. Baroni, S. Bernardini, A. Ferraresi, and
E. Zanchetta. 2009. The WaCky Wide Web:
A Collection of Very Large Linguistically Processed
Web-Crawled Corpora. In Language Resources and
Evaluation 43 (3) 2009, pages 209–226.

L.P. Dinu and R. Ionescu. 2012. Clustering Methods
Based on Closest String via Rank Distance. In 14th
International Symposium on Symbolic and Numeric
Algorithms for Scientific Computing, SYNASC ’12,
pages 207–213.

L.P. Dinu. 2005. Rank Distance with Applications in
Similarity of Natural Languages. Fundam. Inform.,
64(1-4):135–149.

K. Erk and S. Padó. 2008. A Structured Vector Space
Model for Word Meaning in Context. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ’08, pages 897–
906.

L. Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing Search in Context: The Con-
cept Revisited. ACM Trans. Inf. Syst., 20(1):116–
131.

J. Firth. 1957. A Synopsis of Linguistic Theory 1930-
1955. Studies in Linguistic Analysis, Philological
Society, Oxford.

E. Gabrilovich and S. Markovitch. 2007. Computing
Semantic Relatedness Using Wikipedia-based Ex-
plicit Semantic Analysis. In Proceedings of the 20th
International Joint Conference on Artificial Intelli-
gence, IJCAI ’07, pages 1606–1611.

Z. Harris. 1954. Distributional Structure. Word,
10(23):146–162.

T. Hughes and D. Ramage. 2007. Lexical Seman-
tic Relatedness with Random Graph Walks. In
Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL ’07, pages 581–589.

M. A. Jaro. 1989. Advances in Record Linkage
Methodology as Applied to the 1985 Census of
Tampa Florida. Journal of the American Statistical
Society 84(406), pages 414–420.

D. Lin. 1998. Automatic Retrieval and Clustering of
Similar Words. In Proceedings of the 17th Inter-
national Conference on Computational Linguistics -
Volume 2, COLING ’98, pages 768–774.

K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavioral Research Methods, Instru-
ments and Computers, 28(2), pages 203–208.

G. A. Miller and Walter G. Charles. 1991. Contextual
Correlates of Semantic Similarity. Language and
Cognitive Processes, 6(1):1–28.

J. Mitchell and M. Lapata. 2008. Vector-based Mod-
els of Semantic Composition. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics, ACL ’08, pages 236–244.

S. Padó and M. Lapata. 2007. Dependency-Based
Construction of Semantic Space Models. Compu-
tational Linguistics, 33(2):161–199.

H. Rubenstein and J. B. Goodenough. 1965. Contex-
tual correlates of synonymy. Comunications of the
Association of Computing Machinery, 8(10):627–
633.

P. D. Turney and P. Pantel. 2010. From Frequency to
Meaning: Vector Space Models of Semantics. Jour-
nal of Artifficial Intelligence Research, 37:141–188.

84


