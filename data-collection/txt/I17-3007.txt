



















































MUSST: A Multilingual Syntactic Simplification Tool


The Companion Volume of the IJCNLP 2017 Proceedings: System Demonstrations, pages 25–28,
Taipei, Taiwan, November 27 – December 1, 2017. c©2017 AFNLP

MUSST: A Multilingual Syntactic Simplification Tool

Carolina Scarton and Lucia Specia
University of Sheffield

Sheffield, UK

Alessio Palmero Aprosio and Sara Tonelli
Fondazione Bruno Kessler

Trento, Italy

Tamara Martı́n Wanton
H.I. IBERIA

Madrid, Spain

Abstract

We describe MUSST, a multilingual syn-
tactic simplification tool. The tool sup-
ports sentence simplifications for English,
Italian and Spanish, and can be easily ex-
tended to other languages. Our implemen-
tation includes a set of general-purpose
simplification rules, as well as a sentence
selection module (to select sentences to be
simplified) and a confidence model (to se-
lect only promising simplifications). The
tool was implemented in the context of
the European project SIMPATICO on text
simplification for Public Administration
(PA) texts. Our evaluation on sentences in
the PA domain shows that we obtain cor-
rect simplifications for 76% of the simpli-
fied cases in English, 71% of the cases in
Spanish. For Italian, the results are lower
(38%) but the tool is still under develop-
ment.

1 Introduction

Text simplification is the task of reducing the lex-
ical and/or syntactic complexity of a text (Sid-
dharthan, 2004). It is common to divide this task in
two subtasks: lexical simplification (LS) and syn-
tactic simplification (SS). Whilst LS deals with the
identification and replacement of difficult words
or phrases, SS focuses on making complex syn-
tactic constructions simpler. It is known, for in-
stance, that passive voice constructions are more
complex than active voice, and that long sentences
with multiple clauses are more difficult to be un-
derstood than short sentences with a single clause.
Several tools have been developed for LS (Paet-
zold and Specia, 2016). However, we are not
aware of freely available tools for SS.

The SIMPATICO project1 addresses text sim-
plification for specific target audiences and do-
mains. The project has three use cases focus-
ing on different audiences: non-native speakers
(Sheffield, UK), elderly (Galicia, Spain) and busi-
ness and general citizens (Trento, Italy). Al-
though personalised simplifications for each user
type is our ultimate goal, we lack user-specific
data. Therefore, our first step was to design
general-purpose simplification rules which will
later be specialised for the domain under consid-
eration (PA). This solution led to the development
of MUSST, which includes SS modules for three
languages.

MUSST is based on the framework proposed
by Siddharthan (2004) and is available as an
open source Python implementation. Our rules
split conjoint clauses, relative clauses and ap-
positive phrases, and change sentences from pas-
sive into active voice. These are arguably the
most widely applicable simplification operations
across languages. We use the Stanford depen-
dency parser (Chen and Manning, 2014) for the
three languages, which enabled us to build a con-
sistent multilingual tool. MUSST is evaluated us-
ing corpora extracted from the SIMPATICO use
cases data. Such corpora (one for each language)
were checked and – where applicable – syntacti-
cally simplified by experts in the area.

Inspired by the work of Gasperin et al. (2009),
we also developed a complexity checker module in
order to select sentences that should be simplified.
In addition, we implemented a confidence model
in order to predict whether or not a simplification
produced by MUSST is good enough to be shown
to the end-user. Developing these two modules re-
quired small labelled training sets.

To the best of our knowledge, MUSST is the

1https://www.simpatico-project.eu/

25



first freely available, open-source tool for SS in
three languages, which – because of its modular
nature – can be extended to other languages using
the same framework.

2 Architecture

The architecture of MUSST has three main mod-
ules: analysis, transformation and generation (Sid-
dharthan, 2004).

2.1 Analysis

The Analysis module is responsible for processing
sentences to search for clues for the simplification
of conjoint clauses (discourse markers) and rela-
tive clauses (relative pronouns). Sentences where
cues are found trigger dedicated functions of the
Transformation module.

Discourse markers need to be classified accord-
ing to their semantics, in order to be correctly han-
dled by the simplification rules. For instance, a
conjoint clause with “and” as discourse marker
should be processed differently from a conjoint
clause with “when” as a discourse marker.

At this stage, the only mandatory text pre-
processing steps are tokenization, which is done
using the Stanford dependency parser, and lower-
casing, which is done using the built-in function
lower() in Python.

2.2 Transformation

In the Transformation module, our rules that sim-
plify conjoint clauses, relative clauses, apposi-
tive phrases and passive voice are applied. This
module is the core of MUSST. The main method
is called simplify, which receives a sentence
as input and returns one or more simplified sen-
tences. The simplification is implemented as a re-
cursive process that will keep simplifying the sen-
tence until there is no more simplification rule that
applies. The order of simplification is: appositive
phrases, conjoint clauses, relative clauses and pas-
sive voice. This order has been defined empiri-
cally.

All the simplifications are done based on the
output of the dependency parsers. For English
and Spanish, we used the parsers available in
CoreNLP2, trained with Universal Dependencies3

datasets. For Italian, we used the parser available

2http://stanfordnlp.github.io/CoreNLP/
3http://universaldependencies.org

in Tint4 (Palmero Aprosio and Moretti, 2016) (an
adapted version of CoreNLP for Italian).

Figure 1 shows the parser output for the sen-
tence “These organisations have been checked by
us and should provide you with a quality ser-
vice.”, as an example. The sentence is first sent
to the Analysis module that will search for dis-
course markers. In this case, “and” is found and
the sentence is thus sent to the conjoint clauses
rule. Such a rule searches for two tags in the root
dependencies: ADVL (adverbial clause modifier)
or CC (coordinating conjugation). In our example,
there is a CC relation between “checked” (the root)
and “and”. Since “and” is on the list of markers in
the Analysis module, the next step is to search for
a CONJ (conjunction) tag. In the example, there
is a CONJ relation between “checked” and “pro-
vide”. The conjoint clause rule is then applied and
the sentence is split into two. Each sentence is
then sent to the Generation module. The simpli-
fied sentence at this stage is “These organisations
have been checked by us. And these organisations
should provide you with a quality service.” Then,
each of these simplified sentences are sent again to
the simplifier in a recursive manner.

Figure 1: Example of parser output.

The Transformation module is also responsi-
ble for sending the Generation module all the in-
formation needed for re-generating the simplified
sentences. This information includes: discourse
marker, relative pronoun, PoS tag of main and
modal verbs and PoS tag of subject.

2.3 Generation

This module is responsible for re-constructing the
simplified sentence(s) and guaranteeing that gram-

4http://tint.fbk.eu/parsing.html

26



maticality is preserved. It needs to account for the
fact that a sentence can be split (conjoint and rel-
ative clauses and appositive phrases) or reordered
(passive voice).

Truecasing and removal of extra punctuation are
also implemented in this module. For truecas-
ing, we call a Python implementation that has a
pre-trained model for English5, and train truecas-
ing models using monolingual corpora for Spanish
and Italian. For punctuation removal we use rules
that identify punctuation repetition.

In the case of conjoint clauses, we may need
to add specific discourse markers to the simplified
sentences depending on the markers in the original
one. For example, if the complex discourse marker
is “although”, the second simplified sentence will
start with “but”.

For appositive phrases, the verb that connects
the subject to the apposition is defined according
to the number of the subject and the tense of the
main verb. For instance, the simplified version of
“Truffles, a luxury food, are delicious.” is “Truffles
are delicious. Truffles are a luxury food.”.

Changes in passive voice also require verb
changes. Such changes need to respect the tense
of the verb and the person number of the subject.
Changes in the pronoun realisation are also mod-
elled: when pronouns are the subject of the pas-
sive voice, they will become the object of the verb
in active voice.For verb conjugation we use the
NodeBox toolkit6 for English and the tool for verb
conjugation in Tint for Italian. For Spanish, we
developed a new module.

No further treatment is needed for relative
clauses.

3 Evaluation

For English, we selected 1, 100 sentences from the
Sheffield City Council website7. Such sentences
were processed by MUSST, which led to 292 sim-
plified sentences. We categorised these simpli-
fied sentences depending on whether or not they
were correct simplifications (according to gram-
mar). From the 292 sentences, 70 sentences were
considered incorrect. Errors are usually created
from parser issues. For instance, the sentence
“PE kit, school bag, packed lunch.” was incor-
rectly simplified to “PE kit packed lunch. PE kit

5https://github.com/nreimers/truecaser
6https://www.nodebox.net/code/index.

php/Linguistics
7https://www.sheffield.gov.uk/

was school bag.”. The dependency parser identi-
fied “packed” as the main verb, so the appositive
phrase rule was applied. Since such problems are
difficult to detect during simplification, we suggest
using a confidence model (Section 4.2) after sim-
plification.

For Italian, on a test set of 263 Italian sentences
from SIMPITIKI corpus (Tonelli et al., 2016), 92
were simplified by MUSST. 57 of these sentences
were judged as incorrect simplifications. The ma-
jor cause of problems is also parsing errors, espe-
cially when sentences are particularly long or have
ambiguous connectives.

For Spanish, out of 73 sentences from the Xunta
Galicia website8, 49 sentences were simplified by
MUSST. Only 14 of these sentences were consid-
ered incorrect and the main issues were also due
to parsing errors.

4 Extra modules

4.1 Complexity checker

Gasperin et al. (2009) proposes a binary classi-
fier to decide whether or not a sentence should be
split. We build a similar but more general classifier
to decide whether a sentence should be simplified
(including passive to active voice simplification).

We use the Naive Bayes implementation from
the scikit-learn toolkit9 to train a classifier with 10-
fold cross-validation10. As features, we extracted
simple counts of content words, syllables, tokens
and punctuation along with number of clauses,
discourse markers and relative pronouns.

For English, we used the 1, 100 sentences pre-
sented in Section 3. For Italian, we used a set of
405 sentences from SIMPITIKI and, for Spanish,
we used a set of 104 sentences from the Xunta
de Galicia website. All these sentences had been
manually checked and – where applicable – sim-
plified by experts, so each simplified sentence was
considered a positive example.

Table 1 shows the performance of our classifiers
in terms of precision, recall, F1 score and accu-
racy. All languages outperform the majority class
classifiers in terms of accuracy (values in brack-
ets), even though we rely on simple features and
small training sets. The best F1 was achieved by
the model for Spanish, closely followed by the
model for English. Although the model for Italian

8http://www.xunta.gal/portada/
9http://scikit-learn.org/

10Other algorithms performed worse.

27



Figure 2: Example of MUSST usage.

has the best precision, its recall is the worst. The
model for English has the lowest precision, but the
highest recall. The Spanish model has similar val-
ues for precision and recall.

F1 Precision Recall Accuracy
English 0.61 0.56 0.68 0.81 (0.78)

Italian 0.60 0.63 0.57 0.66 (0.58)
Spanish 0.62 0.61 0.62 0.76 (0.70)

Table 1: Performance of the complexity checkers.

4.2 Confidence model

In order to decide whether the simplified version
of a sentence is “good enough” for a user, we
trained a confidence model to classify a simplifi-
cation as acceptable or not. Using the 292 sen-
tences simplified by the English system and eval-
uated in Section 3, we built a confidence model
for this language. The 70 sentences classified as
incorrect (Section 3) were used as negative exam-
ples, whilst the remaining sentences received the
positive label.

As features, we used the same basic counts as
for the complexity checker (Section 4.1) along
with language model (LM) probabilities and per-
plexity and grammar checking on the simplifi-
cations. KenLM11 (Heafield, 2011) was used
to extract LM features. A Python grammar
checker was used for evaluating grammatical-
ity12. The model was trained using the Random
Forest implementation from scikit-learn with 10-
fold cross-validation and achieved 0.80 of accu-
racy (F1/Precision/recall = 0.60/0.69/0.53), out-
performing the MC classifier (accuracy = 0.61).

For Italian and Spanish, we also experimented
with the datasets presented in Section 3, but the
performance is worse because of the significantly
smaller training sets. Nevertheless, both models
outperform the majority class baseline in terms of
accuracy. For Italian, our model achieved 0.80 of
accuracy, against 0.66 for the majority class base-

11https://github.com/kpu/kenlm
12https://pypi.python.org/pypi/

grammar-check/

line. For Spanish, our model achieved 0.73 of ac-
curacy (baseline = 0.55).

5 Demo outline

MUSST is available for download at https:
//github.com/SIMPATICOProject/
SimpaticoTAEServer/tree/
ijcnlp2017-demo. During the demo
session we will present simplifications using both
the command line (e.g. as in Figure 2) and the
graphical interface of the SIMPATICO Dashboard
(http://simpatico.fbk.eu/demo2/
webdemo/index.html). We will demonstrate
the use of -comp and -conf parameters that
activate the complexity checker and confidence
model, respectively.

Finally, we will discuss how a new language can
be included into the tool.

Acknowledgments

This work has been supported by the European
Commission project SIMPATICO (H2020-EURO-
6-2015, grant number 692819).

References
D. Chen and C. D. Manning. 2014. A fast and accu-

rate dependency parser using neural networks. In
EMNLP 2014.

C. Gasperin, L. Specia, T. F. Pereira, and S. M. Aluı́sio.
2009. Learning when to simplify sentences for nat-
ural text simplification. In ENIA 2009.

K. Heafield. 2011. KenLM: Faster and Smaller Lan-
guage Model Queries. In WMT 2011.

G. H. Paetzold and L. Specia. 2016. Benchmarking
Lexical Simplification Systems. In LREC 2016.

A. Palmero Aprosio and G. Moretti. 2016. Italy goes
to Stanford: a collection of CoreNLP modules for
Italian. ArXiv e-prints.

A. Siddharthan. 2004. Syntactic simplification and text
cohesion. Ph.D. thesis, University of Cambridge.

S. Tonelli, A. Palmero Aprosio, and F. Saltori. 2016.
SIMPITIKI: a Simplification corpus for Italian ex-
tracted from Wikipedia. In CLiC-it 2016.

28


