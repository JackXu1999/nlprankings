Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 877–885,

Beijing, August 2010

877

Knowing What to Believe

(when you already know something)

Jeff Pasternack

Dan Roth

University of Illinois, Urbana-Champaign

{jpaster2, danr}@uiuc.edu

Abstract

Although much work in NLP has focused
on simply determining what a document
means, we also must know whether or not
to believe it. Fact-ﬁnding algorithms at-
tempt to identify the “truth” among com-
peting claims in a corpus, but fail
to
take advantage of the user’s prior knowl-
edge and presume that truth itself is uni-
versal and objective rather than subjec-
tive. We introduce a framework for incor-
porating prior knowledge into any fact-
ﬁnding algorithm, expressing both gen-
eral “common-sense” reasoning and spe-
ciﬁc facts already known to the user as
ﬁrst-order logic and translating this into
a tractable linear program. As our results
show, this approach scales well to even
large problems, both reducing error and
allowing the system to determine truth re-
spective to the user rather than the major-
ity. Additionally, we introduce three new
fact-ﬁnding algorithms capable of outper-
forming existing fact-ﬁnders in many of
our experiments.
Introduction

1
Although establishing the trustworthiness of the
information presented to us has always been a
challenge, the advent of the Information Age and
the Internet has made it more critical. Blogs,
wikis, message boards and other collaborative
media have eliminated the high entry barrier–
and, with it, the enforced journalistic standards–of
older, established media such as newspapers and
television, and even these sometimes loosen their
fact-checking in the face of increased competitive
pressure. Consequently, we ﬁnd that corpora de-
rived from these sources now offer far more nu-
merous views of far more questionable veracity.

If one author claims Mumbai is the largest city in
the world, and another claims it is Seoul, who do
we believe? One or both authors could be inten-
tionally lying, honestly mistaken or, alternatively,
of different viewpoints of what constitutes a “city”
(the city proper? The metropolitan area?) Truth is
not objective: there may be many valid deﬁnitions
of “city”, but we should believe the claim that ac-
cords with our user’s viewpoint. Note that the user
may be another computational system rather than
a human (e.g. building a knowledge base of city
sizes for question answering), and often neither
the user’s nor the information source’s perspective
will be explicit (e.g. an author will not fully elabo-
rate “the largest city by metropolitan area bounded
by...”) but will instead be implied (e.g. a user’s
statement that “I already know the population of
city A is X, city B is Y...” implies that his deﬁni-
tion of a city accords with these ﬁgures).

The most basic approach is to take a vote: if
multiple claims are mutually exclusive of each
other, select the one asserted by the most sources.
In our experiments, sources will be the authors
of the document containing the claim, but other
sources could be publishers/websites (when no
authorship is given), an algorithm that outputs
claims, etc. Although sometimes competitive, we
found voting to be generally lackluster. A class of
algorithms called fact-ﬁnders are often a dramatic
improvement, but are incapable of taking advan-
tage of the user’s prior knowledge. Our framework
translates prior knowledge (expressed as ﬁrst-
order logic) into a linear program that constrains
the claim beliefs produced by a fact-ﬁnder, en-
suring that our belief state is consistent with both
common sense (“cities usually grow”) and known
facts (“Los Angeles is more populous than Wi-
chita”). While in the past ﬁrst-order logic has been
translated to NP-hard integer linear programs, we
use polynomial-time-solvable linear programs, al-

878

lowing us to readily scale to large problems with
extensive prior knowledge, as demonstrated by
our experiments.

We next discuss related work, followed by
a more in-depth description of the fact-ﬁnding
algorithms used in our experiments,
includ-
ing three novel, high-performing algorithms:
Average·Log, Investment, and PooledInvestment.
We then present the framework’s mechanics and
the translation of ﬁrst-order logic into a linear pro-
gram. Finally, we present our experimental setup
and results over three domains chosen to illustrate
different aspects of the framework, demonstrating
that both our new fact-ﬁnders and our framework
offer performance improvements over the current
state of the art.

2 Related Work

The broader ﬁeld of trust can be split into three ar-
eas of interest1: theoretical, reputation-based, and
information-based.

2.1 Theoretical
Marsh (1994) observes that trust can be global
(e.g. eBay’s feedback scores), personal (each per-
son has their own trust values), or situational (per-
sonal and speciﬁc to a context). Fact-ﬁnding algo-
rithms are based on global trust, while our frame-
work establishes personal trust by exploiting the
user’s individual prior knowledge.

Probabilistic logics have been explored as an
alternate method of reasoning about trust. Man-
chala (1998) utilizes fuzzy logic (Novak et al.,
1999), an extension of propositional logic permit-
ting [0,1] belief over propositions. Yu and Singh
(2003) employs Dempster-Shafer theory (Shafer,
1976), with belief triples (mass, belief, and plausi-
bility) over sets of possibilities to permit the mod-
eling of ignorance, while Josang et al. (2006) uses
the related subjective logic (Josang, 1997). While
our belief in a claim is decidedly Bayesian (the
probability that the claim is true), “unknowns”
(discussed later) allow us to reason about igno-
rance as subjective logic and Dempster-Shafer do,
but with less complexity.

1Following the division proposed by Artz and Gil (2007);
see also (Sabater and Sierra, 2005) for a survey from a dif-
ferent perspective.

2.2 Reputation-based
Reputation-based systems determine an entity’s
trust or standing among peers via transitive rec-
ommendations, as PageRank (Brin and Page,
1998) does among web pages, Advogato (Levien,
2008) does among people, and Eigentrust (Kam-
var et al., 2003) does among peers in a net-
work. Some, such as Hubs and Authorities (Klein-
berg, 1999), are readily adapted to fact-ﬁnding, as
demonstrated later.

Information-Based

2.3
Information-based approaches utilize content
(rather than peer recommendations) to compute
trust, and are often specialized for a particular do-
main. For example, (Zeng et al., 2006) and Wik-
itrust (Adler and de Alfaro, 2007) determine trust
in a wiki’s text passages from sequences of revi-
sions but lack the claim-level granularity and gen-
eral applicability of fact-ﬁnders.

Given a large set of sources making conﬂicting
claims, fact-ﬁnders determine “the truth” by iter-
atively updating their parameters, calculating be-
lief in facts based on the trust in their sources, and
the trust in sources based on belief in their facts.
TruthFinder (Yin et al., 2008) is a straightforward
implementation of this idea. AccuVote (Dong et
al., 2009a; Dong et al., 2009b) improves on this
by using calculated source dependence (where
one source derives its information from another)
to give higher credibility to independent sources.
(Galland et al., 2010)’s 3-Estimates algorithm in-
corporates the estimated “hardness” of a fact, such
that knowing the answer to an easy question earns
less trust than to a hard one. Except for AccuVote
(whose model of repeated source-to-source copy-
ing is inapplicable to our experimental domains)
we experimented over all of these algorithms.

3 Fact-Finding

We have a set of sources S each asserting a set of

claims Cs, with C =Ss∈S Cs. Each claim c ∈ C
belongs to a mutual exclusion set Mc ⊆ C, a set
of claims (including c) that are mutually exclusive
with one another; for example, “John was born
in 1960” and “John was born in 1965” are mutu-
ally exclusive because a person cannot be born in
more than one year. If c is not mutually exclusive

879

to any other claims, then Mc = {c}. Assuming
there exists exactly one true claim c in each mu-
tual exclusion set M, our goal is to predict c for
each M, with accuracy measured by the number
of successful predictions divided by the number
of mutual exclusion sets, ignoring trivially cor-
rect claims that are the sole members of their mu-
tual exclusion set. To this end, fact-ﬁnding algo-
rithms iterate to ﬁnd the trustworthiness of each
source T i(s) at iteration i in terms of the belief
in its claims in the previous iteration Bi−1(Cs),
and belief in each claim Bi(c) in terms of T i(Sc),
where Sc = {s : s ∈ S, c ∈ Cs} is the set of
all sources asserting c. Note that “trustworthiness”
and “belief” as used within a fact-ﬁnding algo-
rithm typically do not have meaningful semantics
(i.e. they are not [0, 1] Bayesian probabilities). It-
eration continues until convergence or some pre-
deﬁned stop criteria.

3.1 Priors
Except for 3-Estimates (where the priors are dic-
tated by the algorithm itself), every fact-ﬁnder
requires priors for B0(C). For each fact-ﬁnder
we chose from B0
unif orm(c) = 1/|Mc|, and B0
B0
3.2 Algorithms
3.2.1 Sums (Hubs and Authorities)

voted(c) = |Sc|/Pd∈Mc |Sd|,

f ixed(c) = 0.5.

Hubs and Authorities (Kleinberg, 1999) gives
each page a hub score and an authority score,
where its hub score is the sum of the authority of
linked pages and its authority is the sum of the
hub scores of pages linking to it. This is adapted
to fact-ﬁnding by viewing sources as hubs (with
0 authority) and claims as authorities (with 0 hub
score):

T i(s) = Xc∈Cs

Bi−1(c) Bi(c) = Xs∈Sc

T i(s)

We normalize to prevent T i(s) and Bi(c) from
growing unbounded (dividing by maxs T i(s) and
maxc Bi(c), respectively), a technique also used
with the Investment and Average·Log algorithms
(discussed next); this avoids numerical overﬂow.
f ixed priors are used.
B0

3.2.2 Average·Log
Computing T (s) as an average of belief in
its claims overestimates the trustworthiness of
a source with relatively few claims; certainly a
source with 90% accuracy over a hundred ex-
amples is more trustworthy than a source with
90% accuracy over ten. However, summing the
belief in claims allows a source with 10% accu-
racy to obtain a high trustworthiness score by sim-
ply making many claims. Average·Log attempts
a compromise, while still using Sums’ Bi update
rule and B0

f ixed priors.

T i(s) = log |Cs| ·Pc∈Cs Bi−1(c)

|Cs|

3.2.3

Investment

In the Investment algorithm, sources “in-
vest” their trustworthiness uniformly among their
claims. The belief in each claim then grows ac-
cording to a non-linear function G, and a source’s
trustworthiness is calculated as the sum of the be-
liefs in their claims, weighted by the proportion
of trust previously contributed to each (relative to
the other investors). Since claims with higher-trust
sources get higher belief, these claims become rel-
atively more believed and their sources become
more trusted. We used G(x) = xg with g = 1.2 in
our experiments, together with B0
T i(s) = Xc∈Cs
Bi(c) = G Xs∈Sc

|Cs| ·Pr∈Sc

voted priors.
T i−1(s)

|Cs| !

Bi−1(c) ·

T i(s)

T i−1(r)

|Cr|

3.2.4 PooledInvestment

Like Investment, sources uniformly invest their
trustworthiness in claims and obtain correspond-
ing returns, so T i(s) remains the same, but now
after the belief in the claims of mutual exclusion
set M have grown according to G, they are lin-
early scaled such that the total belief of the claims
in M remains the same as it was before apply-
ing G(x) = xg, with g = 1.4 and B0
unif orm
priors used in our experiments. Given H i(c) =
Ps∈Sc

, we have:

T i(s)
|Cs|
Bi(c) = H i(c) ·

G(H i(c))
Pd∈Mc G(H i(d))

880

3.3 TruthFinder
TruthFinder (Yin et al., 2008) is pseudoprobabilis-
tic: the basic version of the algorithm below cal-
culates the “probability” of a claim by assuming
that each source’s trustworthiness is the proba-
bility of it being correct and then averages claim
beliefs to obtain trustworthiness scores. We also
used the “full”, more complex TruthFinder, omit-
ted here for brevity. B0
unif orm priors are used for
both.

Bi−1(c)
|Cs|

T i(s) = Pc∈Cs
Bi(c) = 1 − Ys∈Sc(cid:0)1 − T i(s)(cid:1)

3.3.1

3-Estimates

3-Estimates (Galland et al., 2010), also omit-
ted for brevity, differs from the other fact-ﬁnders
by adding a third set of parameters to capture the
“difﬁculty” of a claim, such that correctly assert-
ing a difﬁcult claim confers more trustworthiness
than asserting an easy one; knowing the exact pop-
ulation of a city is harder than knowing the popu-
lation of Mars (presumably 0) and we should not
trust a source merely because they provide what is
already common knowledge.

4 The Framework
To apply prior knowledge to a fact-ﬁnding algo-
rithm, we translate the user’s prior knowledge into
a linear program. We then iterate the following un-
til convergence or other stopping criteria:

1. Compute T i(s) for all s ∈ S
2. Compute Bi(c) for all c ∈ C
3. “Correct” beliefs Bi(C) with the LP

4.1 Propositional Linear Programming
To translate prior knowledge into a linear pro-
gram, we ﬁrst propositionalize our ﬁrst-order
formulae into propositional logic (Russell and
Norvig, 2003). For example, assume we know that
Tom is older than John and a person has exactly
one age (∃x,yAge(T om, x)∧Age(John, y)∧x >
y) ∧ (∀x,y,zAge(x, y) ∧ y 6= z ⇒ ¬Age(x, z)),
and our
system is considering the follow-
claims: Age(T om, 30), Age(T om, 40),
ing

⊕

Age(T om, 40))

Age(John, 25), Age(John, 35). Our proposi-
tional clauses (after removing redundancies) are
then Age(T om, 30) ⇒ Age(John, 25) ∧
∧
(Age(T om, 30)
(Age(John, 25) ⊕ Age(John, 35)).
Each claim c will be represented by a propo-
sition, and ultimately a [0, 1] variable in the
linear program corresponding,
to
P (c).2 Propositionalized constraints have previ-
ously been used with integer linear programming
(ILP) using binary {0, 1} values corresponding
to {f alse, true}, to ﬁnd an (exact) consistent
truth assignment minimizing some cost and solve
a global inference problem, e.g. (Roth and Yih,
2004; Roth and Yih, 2007). However, proposi-
tional linear programming has two signiﬁcant ad-
vantages:

informally,

1. ILP is “winner take all”, shifting all belief to
one claim in each mutual exclusion set (even
when other claims are nearly as plausible)
and ﬁnding the single most believable con-
sistent binary assignment; we instead wish to
ﬁnd a distribution of belief over the claims
that is consistent with our prior knowledge
and as close as possible to the distribution
produced by the fact-ﬁnder.

2. Linear programs can be solved in polynomial
time (e.g. by interior point methods (Kar-
markar, 1984)), but ILP is NP-hard.

To create our constraints, we ﬁrst convert our
propositional formula into conjunctive normal
form. Then, for each disjunctive clause consisting
of a set P of positive literals (claims) and a set
N of negations of literals, we add the constraint

Pc∈P cv +Pc∈N (1 − cv) ≥ 1, where cv de-

notes the [0, 1] variable corresponding to each c.
The left-hand side is the union bound of at least
one of the claims being true (or false, in the case
of negated literals); if this bound is at least 1, the
constraint is satisﬁed. This optimism can dilute
the strength of our constraints by ignoring poten-
tial dependence among claims: x ⇒ y, x ∨ y im-
plies y is true, but since we demand only yv ≥ xv
and xv + yv ≥ 1 we accept any yv ≥ 0.5 where
2This is a slight mischaracterization, since our linear con-
straints only approximate intersections and unions of events
(where each event is “claim c is true”), and we will be satis-
fying them subject to a linear cost function.

881

yv ≥ xv ≥ 1 − yv. However, when the claims
are mutually exclusive, the union bound is exact; a
common constraint is of the form q ⇒ r1∨r2∨. . .,
where the r literals are mutually exclusive, which
v + . . . ≥ qv. Fi-
translates exactly to r1
nally, observe that mutual exclusion amongst n
claims c1, c2, . . ., cn can be compactly written as
c1
v + c2

v + . . . + cn

v + r2

v = 1.

4.2 The Cost Function

Having seen how ﬁrst-order logic can be con-
verted to linear constraints, we now consider the
cost function, a distance between the new distri-
bution of belief satisfying our constraints and the
original distribution produced by the fact-ﬁnder.

First we determine the number of “votes” re-
ceived by each claim c, computed as ωc =
ω(B(c)), which should scale linearly with the cer-
tainty of the fact-ﬁnder’s belief in c. Recall that
the semantics of the belief score are particular
to the fact-ﬁnder, so different fact-ﬁnders require
different vote functions. TruthFinder has pseudo-
probabilistic [0,1] beliefs, so we use ωinv(x) =
min((1 − x)-1, minv) with minv = 1010 limiting
the maximum number of votes possible; we as-
sume 1/0 = ∞. ωinv intuitively scales with “er-
ror”: a belief of 0.99 receives ten times the votes
of 0.9 and has a tenth the error (0.01 vs. 0.1).
For the remainder of the fact-ﬁnders whose beliefs
are already “linear”, we use the identity function
ωidn(x) = x.

The most obvious choice for the cost func-
tion might be to minimize “frustrated votes”:

Pc∈C ωc(1 − cv). Unfortunately, this results in

the linear solver generally assigning 1 to the vari-
able in each mutual exclusion set with the most
votes and 0 to all others (except when constraints
prevent this), shifting all belief to the highest-vote
claim and yielding poor performance. Instead, we
wish to satisfy the constraints while keeping each
ωd,
and so shift belief among claims as little as possi-
ble. We use a weighted Manhattan distance called
VoteDistance, where the cost for increasing the
belief in a claim is proportional to the number of
votes against it, and the cost for decreasing belief

cv close to ωc/ωMc, where ωMc = Pd∈Mc

is proportional to the number of votes for it:

max(cid:18) (ωMc − ωc) · (cv − ωc/ωMc),

ωc · (ωc/ωMc − cv)

(cid:19)

Xc∈C

Thus, the belief distribution found by our LP
will be the one that satisﬁes the constraints while
simultaneously minimizing the number of votes
frustrated by the change from the original dis-
tribution. Note that for any linear expressions e
and f we can implement max(e, f ) in the objec-
tive function by replacing it with a new [−∞,∞]
helper variable x and adding the linear constraints
x ≥ e and x ≥ f.
4.3 From Values to Votes to Belief
Solving the LP gives us [0, 1] values for each vari-
able cv, but we need to calculate an updated belief
B(c). We propose two methods for this:
Vote Conservation: B(c) = ω−1(cv · ωMc)
Vote Loss: B(c) = ω−1(min(ωc, cv · ωMc))

is an inverse of

ω−1
the vote function:
idn(x) = x and ω−1
ω−1
inv(x) = 1 − (1 + y)−1. Vote
Conservation reallocates votes such that the total
number of votes in each mutual exclusion set, ωM ,
remains the same after the redistribution. How-
ever, if the constraints force c to lose votes, should
we believe the other claims in Mc more? Under
Vote Loss, a claim can only lose votes, ensuring
that if other claims in Mc become less believable,
c does not itself become more believable relative
to claims in other mutual exclusion sets. We found
Vote Loss just slightly better on average and used
it for all reported results.

“Unknown” Augmentation

4.4
Augmenting our data with “Unknown” claims en-
sures that every LP is feasible and can be used
to model our ignorance given a lack of sufﬁ-
cient information or conﬂicting constraints. An
Unknown claim UM is added to every mutual ex-
clusion set M (but invisible to the fact-ﬁnder) and
represents our belief that none of the claims in
M are sufﬁciently supported. Now we can write
the mutual exclusion constraint for M as UM +

Pc∈M cv = 1. When propositionalizing FOL, if
a disjunctive clause contains a non-negated literal
for a claim c, then we add ∨UMc to the clause.

882

For example, Age(John, 35) ⇒ Age(T om, 40)
becomes Age(John, 35) ⇒ Age(T om, 40) ∨
Age(T om, U nknown). The only exception is
when the clause contains claims from only one
mutual exclusion set (e.g. “I know Sam is 50
or 60”), and so the LP can only be infeasible
if the user directly asserts a contradiction (e.g.
“Sam is 50 and Sam is 60”). The Unknown it-
self has a ﬁxed number of votes that cannot be
lost; this effectively “smooths” our belief in the
claims and imposes a ﬂoor for believability. If
Age(Kim, 30) has 5 votes, Age(Kim, 35) has
3 votes, and Age(Kim, U nknown) is ﬁxed at 6
votes, we hold that Kim’s age is unknown due to
lack of evidence. The number of votes that should
be given to each Unknown for this purpose de-
pends, of course, on the particular fact-ﬁnder and
ω function used; in our experiments, we are not
concerned with establishing ignorance and thus
assign 0 votes.

5 Experiments

Experiments were conducted over three domains
(city population, basic biographies, and Ameri-
can vs. British spelling) with four datasets, all
using the VoteDistance cost function and Vote
Loss vote redistribution. We ﬁxed the number of
iterations of the framework (calculating T i(S),
Bi(S) and then solving the LP) at 20, which
was found sufﬁcient for all fact-ﬁnders. To eval-
uate accuracy, after the ﬁnal iteration we look
at each mutual exclusion set M and predict the
highest-belief claim c ∈ M (or, if uM had the
highest belief, the second-highest claim), break-
ing ties randomly, and check that it is the true
claim tM . We omit any M that does not contain
a true claim (all known claims are false) and any
M that is trivially correct (containing only one
claim other than uM ). All results are shown in
Table 1. Vote is the baseline, choosing either the
claim occurring in the most Wikipedia revisions
(in the Pop dataset) or claimed by the most sources
(for all other datasets). Sum is Sums (Hubs and
Authorities), 3Est is 3-Estimates, TFs is simpli-
ﬁed TruthFinder, TFc is “full” TruthFinder, A·L is
Average·Log, Inv1.2 is Investment with g = 1.2,
and Pool1.4 is PooledInvestment with g = 1.4.

IBT vs. L+I

5.1
We can enforce our prior knowledge against the
beliefs produced by the fact-ﬁnder in each itera-
tion, or we can apply these constraints just once,
after running the fact-ﬁnder for 20 iterations with-
out interference. By analogy to (Punyakanok et
al., 2005), we refer to these approaches as infer-
ence based training (IBT) and learning + inference
(L+I), respectively. Our results show that while
L+I does better when prior knowledge is not en-
tirely correct (e.g. “Growth” in the city popula-
tion domain), generally performance is compara-
ble when the effect of the constraints is mild, but
IBT can outperform when prior knowledge is vital
(as in the spelling domain) by allowing the fact-
ﬁnder to learn from the provided corrections.

5.2 Wikipedia Infoboxes
To focus on the performance of the framework,
we (like previous fact-ﬁnding work) naively as-
sume that our data are accurately extracted, but we
also require large corpora. Wikipedia Infoboxes
(Wu and Weld, 2007) are a semi-structured source
covering many domains with readily available au-
thorship, and we produced our city population and
basic biographic datasets from the most recent
full-history dump of the English Wikipedia (taken
January 2008). However, attribution is difﬁcult: if
an author edits the page but not the claim within
the infobox, is the author implicitly agreeing with
(and asserting) the claim? The best performance
was achieved by being strict for City Population
data, counting only the direct editing of a claim,
and lax for Biography data, counting any edit.
We hypothesize this is because editors may lack
speciﬁc knowledge about a city’s population (and
thus fail to correct an erroneous value) but incor-
rect birth or death dates are more noticeable.

5.3 Results
5.3.1 City Population

for

We

collected

infoboxes

settlements
(Geobox, Infobox Settlement, Infobox City, etc.)
to obtain 44,761 populations claims qualiﬁed
by year (e.g. pop(Denver, 598707, 2008)), with
4,107 authors total. We took as our “truth”
U.S. census data, which gave us 308 non-
trivial true facts to test against. Our “common
sense” knowledge is
that population grows

883

∅

∅

Pop
Pop
Pop
Pop
Pop

SynPop
SynPop
SynPop

GrowthIBT
GrowthL+I
Larger2500
IBT
Larger2500
L+I

Some results are omitted here (see text). A·L, Inv1.2, Pool1.4 are our novel algorithms

Table 1: Experimental Results (∅ indicates no prior knowledge; all values are percent accuracy)
Pool1.4
Dataset
80.19
79.87
80.84
84.09
84.09
90.00
96.42
96.01
90.01
90.20
90.24
91.32
91.17
9.65
8.86
7.89
80.68
29.95

Prior Knowledge Vote
81.49
82.79
82.79
85.39
85.39
73.45
88.31
88.31
89.80
89.20
89.20
90.58
90.58
13.54
13.69
13.69
35.10
35.10

Sum 3Est
81.49
81.82
79.87
77.92
77.92
79.55
80.52
85.06
85.06
80.52
84.87
87.76
92.16
95.46
92.43
94.77
89.53
89.80
89.20
89.61
89.20
89.61
90.88
90.58
90.58
90.91
11.96
9.37
12.72
9.02
8.86
12.08
35.10
31.88
31.72
34.62

Inv1.2
87.99
85.39
89.29
89.29
89.94
89.41
95.46
96.29
88.34
88.60
88.49
90.02
90.09
9.36
11.11
9.34
73.59
30.92

TFc
84.42
86.36
85.39
87.34
86.69
87.07
95.46
95.32
90.09
89.91
90.09
91.25
90.95
7.93
8.05
8.05
29.79
22.06

A·L
80.84
80.52
80.52
84.74
84.42
90.23
96.15
95.59
89.24
89.35
89.35
90.91
90.91
10.23
9.98
9.98
32.85
32.21

TFs
82.79
82.79
83.44
86.04
86.69
56.12
96.42
82.39
73.04
72.44
57.10
80.30
69.27
41.93
44.28
46.54
56.52
55.39

Bio
Bio
Bio
Bio
Bio
Spell
Spell
Spell
Spell
Spell

Words100
IBT
Words100
L+I
CS+Words100
IBT
CS+Words100
L+I

CS+DecadesIBT
CS+DecadesL+I

Pop±8%IBT
Pop±8%L+I

CSIBT
CSL+I

∅

∅

time (“Growth” in table 1);

over
therefore,
∀v,w,x,y,zpop(v, w, y) ∧ pop(v, x, z) ∧ y < z ⇒
x > w. Of course, this often does not hold
true: cities can shrink, but performance was
nevertheless superior
to no prior knowledge
whatsoever. The L+I approach does appreciably
better because it avoids forcing these sometimes-
incorrect constraints onto the claim beliefs while
the fact-ﬁnder iterates (which would propagate
the resulting mistakes), instead applying them
only at the end where they can correct more errors
than they create. The sparsity of the data plays
a role–only a fraction of cities have population
claims for multiple years, and those that do are
typically larger cities where the correct claim is
asserted by an overwhelming majority, greatly
limiting the potential beneﬁt of our Growth
constraints. We also considered prior knowledge
of the relative sizes of some cities, randomly
selecting 2500 pairs of them (a, b), where a
was more populous than b in year t, asserting
∀x,ypop(a, x, t) ∧ pop(b, y, t) ⇒ x > y. This
“Larger” prior knowledge proved more effective
than our oft-mistaken Growth constraint, with
modest improvement to the highest-performing
Investment
InvestmentL+I

fact-ﬁnder,

and

reaches 90.91% with 10,000 such pairs.

5.3.2 Synthetic City Population

What if attribution were certain and the data
more dense? To this end we created a synthetic
dataset. We chose 100 random (real) cities and
created 100 authors whose individual accuracy
a was drawn uniformly from [0, 1]. Between 1
and 10 claims (also determined uniformly) were
made about each city in each year from 2000
to 2008 by randomly-selected authors. For each
city with true population p and year, four incor-
rect claims were created with populations selected
uniformly from [0.5p, 1.5p], each author claiming
p with probability a and otherwise asserting one
of the four incorrect claims. Our common-sense
knowledge was that population did not change
by more than 8% per year (also tried on the
Wikipedia dataset but with virtually no effect).
Like “Growth”, “Pop±8%” does not always hold,
but a change of more than 8% is much rarer than a
shrinking city. These constraints greatly improved
results, although we note this would diminish if
inaccurate claims had less variance around the
true population.

884

5.3.3 Basic Biographies

We scanned infoboxes to ﬁnd 129,847 claimed
birth dates, 34,201 death dates, 10,418 parent-
child pairs, and 9,792 spouses. To get “true” birth
and death dates, we extracted data from sev-
eral online repositories (after satisfying ourselves
that they were independent and not derived from
Wikipedia!), eliminating any date these sources
disagreed upon, and ultimately obtained a total
of 2,685 dates to test against. Our common sense
(“CS”) knowledge was: nobody dies before they
are born, people are infertile before the age of 7,
nobody lives past 125, all spouses have overlap-
ping lifetimes, no child is born more than a year
after a parent’s (father’s) death, nobody has more
than two parents, and nobody is born or dies after
2008 (the “present day”, the year of the Wikipedia
dump). Applying this knowledge roughly halved
convergence times, but had little effect on the re-
sults due to data sparsity similar to that seen in
the population data–while we know many birth-
days and some death dates, relatively few biogra-
phies had parent-child and spouse claims. To this
we also added knowledge of the decade (but not
the exact date) in which 15,145 people were born
(“CS+Decades”). Although common sense alone
does not notably improve results, it does very well
in conjunction with speciﬁc knowledge.

5.3.4 American vs. British Spelling

Prior knowledge allows us to ﬁnd a truth that
conforms with the user’s viewpoint, even if that
viewpoint differs from the norm. After obtaining
a list of words with spellings that differed be-
tween American and British English (e.g. ”color”
vs. ”colour”), we examined the British National
Corpus as well as Washington Post and Reuters
news articles, taking the source’s (the article au-
thor’s) use of a disputed word as a claim that
his spelling was correct. Our goal was to ﬁnd the
“true” British spellings that conformed to a British
viewpoint, but American spellings predominate
by far. Consequently, without prior knowledge the
fact-ﬁnders do very poorly against our test set of
694 British words, predicting American spelling
instead in accordance with the great majority of
authors (note that accuracy from an American
perspective is 1−“British” accuracy). Next we
assumed that the user already knew the correct

spelling of 100 random words (removing these
from the test set, of course), but with little ef-
fect. Finally, we added our common sense (“CS”)
knowledge: if a spelling a is correct and of length
≥ 4, then if a is a substring of b, a ⇔ b (e.g. colour
⇔ colourful). Furthermore, while we do not know
a priori whether a spelling is American or British,
we do know if e and f are different spellings
of the same word, and, if two such spellings
have a chain of implication between them, we
can break all links in this chain (while some
American spellings will still be linked to British
spellings, this removes most such errors). Interest-
ingly, common sense alone actually hurts results
(e.g. PooledInvestment (IBT) gets 6.2%), as it es-
sentially makes the fact-ﬁnders more adept at ﬁnd-
ing the predominant American spellings! How-
ever, when some correct spellings are known, re-
sults improve greatly and demonstrate IBT’s abil-
ity to spread strong prior knowledge, easily sur-
passing L+I. Results improve further with more
known spellings (PooledInvestment gets 84.86%
with CS+Words200

IBT ).

6 Conclusion
We have introduced a new framework for in-
corporating prior knowledge into a fact-ﬁnding
system, along with several new high-performing
fact-ﬁnding algorithms (Investment, PooledIn-
vestment, and Average·Log). While the bene-
ﬁts of prior knowledge were most dramatic in
the Spelling domain, we saw gains from both
“common sense” and speciﬁc knowledge in all
experiments–even the difﬁcult Biography domain
saw faster convergence with common sense alone
and notably higher results when speciﬁc knowl-
edge was added. We ﬁnd that while prior knowl-
edge is helpful in reducing error, when the user’s
viewpoint disagrees with the norm it becomes ab-
solutely essential and, formulated as a linear pro-
gram, it need not be the computational burden that
might otherwise be expected.

Acknowledgements
This research was partly sponsored by the Army Research
Laboratory (ARL) (accomplished under Cooperative Agree-
ment Number W911NF-09-2-0053). Any opinions, ﬁndings,
and conclusion or recommendations expressed in this mate-
rial are those of the authors and do not necessarily reﬂect the
view of the ARL.

885

Roth, D and W Yih. 2007. Global Inference for Entity and
Relation Identiﬁcation via a Linear Programming Formu-
lation. In Getoor, Lise and Ben Taskar, editors, Introduc-
tion to Statistical Relational Learning. MIT Press.

Russell, Stuart and Peter Norvig. 2003. Artiﬁcial Intelli-
gence: A Modern Approach. Prentice Hall, second edi-
tion.

Sabater, Jordi and Carles Sierra. 2005. Review on Compu-
tational Trust and Reputation Models. Artiﬁcial Intelli-
gence Review, 24(1):33–60, September.

Shafer, G. 1976. A mathematical theory of evidence. Prince-

ton University Press Princeton, NJ.

Wu, Fei and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. Proceedings of the sixteenth ACM
conference on Conference on information and knowledge
management - CIKM ’07, page 41.

Yin, Xiaoxin, Philip S. Yu, and Jiawei Han. 2008. Truth Dis-
covery with Multiple Conﬂicting Information Providers
on the Web. IEEE Transactions on Knowledge and Data
Engineering, 20(6):796–808.

Yu, Bin and Munindar P. Singh. 2003. Detecting deception
in reputation management. Proceedings of the second in-
ternational joint conference on Autonomous agents and
multiagent systems - AAMAS ’03, page 73.

Zeng, H, M Alhossaini, L Ding, R Fikes, and D L McGuin-
ness. 2006. Computing trust from revision history. Intl.
Conf. on Privacy, Security and Trust.

References
Adler, B T and L de Alfaro. 2007. A content-driven reputa-

tion system for the Wikipedia. WWW ’07, 7:261–270.

Artz, D and Y Gil. 2007. A survey of trust in computer
science and the Semantic Web. Web Semantics: Science,
Services and Agents on the World Wide Web, 5(2):58–71,
June.

Brin, S and L Page. 1998. The anatomy of a large-scale
hypertextual Web search engine. Computer Networks and
ISDN Systems, 30(1-7):107–117.

Dong, X, L Berti-equille, and D Srivastava. 2009a. Integrat-
ing conﬂicting data: the role of source dependence. Tech-
nical report, AT&T Labs-Research, Florham Park, NJ.

Dong, X.L., L. Berti-Equille, and Divesh Srivastava. 2009b.
Truth discovery and copying detection in a dynamic
world. VLDB, 2(1):562–573.

Galland, Alban, Serge Abiteboul, A. Marian, and Pierre
Senellart. 2010. Corroborating information from dis-
agreeing views. In Proceedings of the third ACM interna-
tional conference on Web search and data mining, pages
131–140. ACM.

Josang, A., S. Marsh, and S. Pope. 2006. Exploring different
types of trust propagation. Lecture Notes in Computer
Science, 3986:179.

Josang, A. 1997. Artiﬁcial reasoning with subjective logic.

2nd Australian Workshop on Commonsense Reasoning.

Kamvar, S, M Schlosser, and H Garcia-molina. 2003. The
Eigentrust algorithm for reputation management in P2P
networks. WWW ’03.

Karmarkar, N. 1984. A new polynomial-time algorithm for

linear programming. Combinatorica, 4(4):373–395.

Kleinberg, J M. 1999. Authoritative sources in a hyperlinked

environment. Journal of the ACM, 46(5):604–632.

Levien, R. 2008. Attack-resistant trust metrics. Computing

with Social Trust, pages 121–132.

Manchala, D.W. 1998. Trust metrics, models and protocols
for electronic commerce transactions. Proceedings. 18th
International Conference on Distributed Computing Sys-
tems (Cat. No.98CB36183), pages 312–321.

Marsh, S. 1994. Formalising Trust as a Computational Con-

cept. PhD thesis, University of Stirling.

Novak, V, I Perﬁlieva, and J Mockof. 1999. Mathematical

principles of fuzzy logic. Kluwer Academic Publishers.

Punyakanok, V., D. Roth, W. Yih, and D. Zimak. 2005.
Learning and inference over constrained output. In Inter-
national Joint Conference on Artiﬁcial Intelligence, vol-
ume 19.

Roth, Dan and Wen-tau Yih. 2004. A linear programming
formulation for global inference in natural language tasks.
In Proc. of the Annual Conference on Computational Nat-
ural Language Learning (CoNLL), pages 1–8.

