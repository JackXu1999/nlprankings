



















































Gov2Vec: Learning Distributed Representations of Institutions and Their Legal Text


Proceedings of 2016 EMNLP Workshop on Natural Language Processing and Computational Social Science, pages 49–54,
Austin, TX, November 5, 2016. c©2016 Association for Computational Linguistics

Gov2Vec: Learning Distributed Representations of Institutions and Their
Legal Text

John J. Nay
School of Engineering
Vanderbilt University

Program on Law & Innovation
Vanderbilt Law School

Nashville, TN 37235, USA
john.j.nay@gmail.com

johnjnay.com

Abstract

We compare policy differences across institu-
tions by embedding representations of the en-
tire legal corpus of each institution and the vo-
cabulary shared across all corpora into a con-
tinuous vector space. We apply our method,
Gov2Vec, to Supreme Court opinions, Presi-
dential actions, and official summaries of Con-
gressional bills. The model discerns meaning-
ful differences between government branches.
We also learn representations for more fine-
grained word sources: individual Presidents
and (2-year) Congresses. The similarities be-
tween learned representations of Congresses
over time and sitting Presidents are negatively
correlated with the bill veto rate, and the tem-
poral ordering of Presidents and Congresses
was implicitly learned from only text. With
the resulting vectors we answer questions such
as: how does Obama and the 113th House dif-
fer in addressing climate change and how does
this vary from environmental or economic
perspectives? Our work illustrates vector-
arithmetic-based investigations of complex re-
lationships between word sources based on
their texts. We are extending this to create a
more comprehensive legal semantic map.

1 Introduction

Methods have been developed to efficiently obtain
representations of words in Rd that capture subtle
semantics across the dimensions of the vectors (Col-
lobert and Weston, 2008). For instance, after suf-
ficient training, relationships encoded in difference
vectors can be uncovered with vector arithmetic:

vec(“king”) - vec(“man”) + vec(“woman”) returns a
vector close to vec(“queen”) (Mikolov et al. 2013a).

Applying this powerful notion of distributed con-
tinuous vector space representations of words, we
embed representations of institutions and the words
from their law and policy documents into shared se-
mantic space. We can then combine positively and
negatively weighted word and government vectors
into the same query, enabling complex, targeted and
subtle similarity computations. For instance, which
government branch is more characterized by “valid-
ity and truth,” or “long-term government career”?

We apply this method, Gov2Vec, to a unique
corpus of Supreme Court opinions, Presidential ac-
tions, and official summaries of Congressional bills.
The model discerns meaningful differences between
House, Senate, President and Court vectors. We
also learn more fine-grained institutional representa-
tions: individual Presidents and Congresses (2-year
terms). The method implicitly learns important la-
tent relationships between these government actors
that was not provided during training. For instance,
their temporal ordering was learned from only their
text. The resulting vectors are used to explore differ-
ences between actors with respect to policy topics.

2 Methods

A common method for learning vector representa-
tions of words is to use a neural network to predict
a target word with the mean of its context words’
vectors, obtain the gradient with back-propagation
of the prediction errors, and update vectors in the
direction of higher probability of observing the cor-
rect target word (Bengio et al. 2003; Mikolov et

49



Gov 
Branch Word

e.g.: ‘obtain’

Word

e.g.: ‘a’ 

Word

e.g., ‘for’ 

Word

e.g.: ‘loan’

Average Context Vectors

Predict Target Word

d-dimensional vectors, 
e.g. d=200

Word

e.g., ‘this’ 

e.g., middle word in 
5-word sequence

Gov 
Branch

Gov 
Branch

e.g.: 112th House of  
Representatives

e.g.: 111th 
House

e.g.: 113th 
House

Figure 1: Gov2Vec only updates GovVecs with word predic-
tion. For Structured Gov2Vec training, which updates GovVecs

with word and Gov prediction, we set “Gov window size” to 1,

e.g. a Congress is used to predict those directly before and after.

al. 2013b). After iterating over many word contexts,
words with similar meaning are embedded in similar
locations in vector space as a by-product of the pre-
diction task (Mikolov et al. 2013b). Le and Mikolov
(2014) extend this word2vec method to learn repre-
sentations of documents. For predictions of target
words, a vector unique to the document is concate-
nated with context word vectors and subsequently
updated. Similarly, we embed institutions and their
words into a shared vector space by averaging a vec-
tor unique to an institution with context word vec-
tors when predicting that institution’s words and,
with back-propagation and stochastic gradient de-
scent, update representations for institutions and the
words (which are shared across all institutions).1

There are two hyper-parameters for the algorithm
that can strongly affect results, but suitable values
are unknown. We use a tree of Parzen estimators
search algorithm (Bergstra et al. 2013) to sample
from parameter space2 and save all models esti-
mated. Subsequent analyses are conducted across
all models, propagating our uncertainty in hyper-
parameters. Due to stochasticity in training and the
uncertainty in the hyper-parameter values, patterns

1We use a binary Huffman tree (Mikolov et al. 2013b) for
efficient hierarchical softmax prediction of words, and conduct
25 epochs while linearly decreasing the learning rate from 0.025
to 0.001.

2vector dimensionality, uniform(100, 200), and maximum
distance between the context and target words, uniform(10, 25)

robust across the ensemble are more likely to reflect
useful regularities than individual models.

Gov2Vec can be applied to more fine-grained cat-
egories than entire government branches. In this
context, there are often relationships between word
sources, e.g. Obama after Bush, that we can incor-
porate into the learning process. During training, we
alternate between updating GovVecs based on their
use in the prediction of words in their policy corpus
and their use in the prediction of other word sources
located nearby in time. We model temporal insti-
tutional relationships, but any known relationships
between entities, e.g. ranking Congresses by num-
ber of Republicans, could also be incorporated into
the Structured Gov2Vec training process (Fig. 1).

After training, we extract (M + S) × dj × J pa-
rameters, where M is the number of unique words,
S is the number of word sources, and dj the vec-
tor dimensionality, which varies across the J mod-
els (we set J = 20). We then investigate the most
cosine similar words to particular vector combi-
nations, arg maxv∗∈V1:N cos(v∗, 1W

∑W
i=1wi × si),

where cos(a, b) = ~a·~b‖~a‖‖~b‖ , wi is one ofW WordVecs
or GovVecs of interest, V1:N are theN most frequent
words in the vocabulary ofM words (N < M to ex-
clude rare words during analysis) excluding the W
query words, si is 1 or -1 for whether we’re posi-
tively or negatively weighting wi. We repeat simi-
larity queries over all J models, retain words with
> C cosine similarity, and rank the word results
based on their frequency and mean cosine similarity
across the ensemble. We also measure the similar-
ity of WordVec combinations to each GovVec and
the similarities between GovVecs to validate that the
process learns useful embeddings that capture ex-
pected relationships.

3 Data

We created a unique corpus of 59 years of all U.S.
Supreme Court opinions (1937-1975, 1991-2010),
227 years of all U.S. Presidential Memorandum,
Determinations, and Proclamations, and Executive
Orders (1789-2015), and 42 years of official sum-
maries of all bills introduced in the U.S. Congress
(1973-2014). We used official summaries rather
than full bill text because full texts are only avail-
able from 1993 and summaries are available from

50



1973. We scraped all Presidential Memorandum
(1,465), Determinations (801), Executive Orders
(5,634), and Proclamations (7,544) from the Ameri-
can Presidency Project website. The Sunlight Foun-
dation downloaded official bill summaries from the
U.S. Government Publishing Office (GPO), which
we downloaded. We downloaded Supreme Court
Decisions issued 1937–1975 (Vol. 300-422) from
the GPO, and the PDFs of Decisions issued 1991–
2010 (Vol. 502-561) from the Supreme Court. We
removed HTML artifacts, whitespace, stop words,
words occurring only once, numbers, and punctua-
tion, and converted to lower-case.

4 Results

4.1 WordVec-GovVec Similarities

We tested whether our learned vectors captured
meaningful differences between branches. Fig. 2
displays similarities between these queries and the
branches, which reflect a priori known differences.

Gov2Vec has unique capabilities that summary
statistics, e.g. word frequency, lack: it can compute
similarities between any source and word as long
as the word occurs at least in one source, whereas
word counting cannot provide meaningful similari-
ties when a word never occurs in a source’s corpus.
Most importantly, Gov2Vec can combine complex
combinations of positively and negatively weighted
vectors in a similarity query.

4.2 GovVec-GovVec Similarities

We learned representations for individual Presidents
and Congresses by using vectors for these higher
resolution word sources in the word prediction task.
To investigate if the representations capture impor-
tant latent relationships between institutions, we
compared the cosine similarities between the Con-
gresses over time (93rd–113th) and the correspond-
ing sitting Presidents (Nixon–Obama) to the bill
veto rate. We expected that a lower veto rate would
be reflected in more similar vectors, and, indeed, the
Congress-President similarity and veto rate are neg-
atively correlated (Spearman’s ρ computed on raw
veto rates and similarities: -0.74; see also Fig. 3).3

3Leveraging temporal relationships in the learning process,
Structured Gov2Vec, and just using the text, yield very similar
(impressive) results on this task. Figs. 3 and 4 and the correla-

●

●

●

●

●

●

●

●

●

●
●

●

●

●

−0.2

0.0

0.2

candidate
+, elected

+,
campaign+

long+,
term+,

government
+, career+

rule+,
precedent

+,
interpret+

validity+,
truth+

statistics
+, science

+, data
+, story−,
anecdote−

order+,
direct+,

contemplate−,
consider−

C
os

in
e 

S
im

ila
rit

y

Branch Court President House Senate

Figure 2: We compute the mean of similarities of each branch
to all words and subtract this from each branch’s similarity com-

putation to normalize branches within vector space. Positive

and negative weighting is noted with + and -. Compared to

the Court, the President is much closer to “order and direct”

than “contemplate and consider.” The opposite holds for “va-

lidity and truth.” The left panel reflects the fact that the House

is elected most often and the Court the least (never).

0.000

0.001

0.002

95 100 105 110

Congress

V
et

o 
R

at
e

−0.2

0.0

0.2

95 100 105 110

Congress

S
im

ila
rit

y 
B

et
w

ee
n 

C
ha

m
be

r 
an

d 
S

itt
in

g 
P

re
si

de
nt

Chamber

H

S

Figure 3: Loess-smoothed veto rate (left) and loess-smoothed
(across ensemble) similarity of Congresses to Presidents (right).

As a third validation, we learn vectors from only
text and project them into two dimensions with prin-
cipal components analysis. From Fig. 4 it’s ev-
ident that temporal and institutional relationships
were implicitly learned.4 One dimension (top-to-
bottom) almost perfectly rank orders Presidents and
Congresses by time, and another dimension (side-to-
side) separates the President from Congress.

tion reported are derived from the text-only Gov2Vec results.
4These are the only results reported in this paper from a sin-

gle model within the ensemble. We conducted PCA on other
models in the ensemble and the same relationships hold.

51



100_S 101_S

102_S
103_S

104_S
105_S

106_S
107_S108_S

109_S 110_S

111_S112_S 113_S

93_S

94_S

95_S

96_S

97_S
98_S

99_S
100_H

101_H

102_H
103_H

104_H
105_H

106_H
107_H

108_H

109_H
110_H111_H112_H

113_H

93_H

94_H

95_H

96_H

97_H
98_H
99_H

RichardNixon
GeraldR.Ford

JimmyCarter

RonaldReagan

GeorgeBush

WilliamJ.Clinton

GeorgeW.Bush

BarackObama

Figure 4: 2-dimensional Congresses (93-113 House and Sen-
ate) and Presidents.

4.3 GovVec-WordVec Policy Queries

Fig. 5 (top) asks: how does Obama and the 113th
House differ in addressing climate change and how
does this vary across environmental and economic
contexts? The most frequent word across the en-
semble (out of words with > 0.35 similarity to the
query) for the Obama-economic quadrant is “un-
precedented.” “Greenhouse” and “ghg” are more
frequent across models and have a higher mean sim-
ilarity for Obama-Environmental than 113th House-
Environmental.

Fig. 5 (bottom) asks: how does the House ad-
dress war from “oil” and “terror” perspectives and
how does this change after the 2001 terrorist at-
tack.5 Compared to the 106th, both the oil and
terrorism panels in the 107th (when 9-11 occurred)
have words more cosine similar to the query (further
to the right) suggesting that the 107th House was
closer to the topic of war, and the content changes
to primarily strong verbs such as instructs, directs,
requires, urges, and empowers.

5For comparisons across branches, e.g. 113th House and
Obama, Structured Gov2Vec learned qualitatively more useful
representations so we plot that here. For comparisons within
Branch, e.g. 106th and 107th House, to maximize uniqueness
of the word sources to obtain more discriminating words, we
use text-only Gov2Vec.

accelerated

amtrak

appropriations

exists

formula

formulae
gases

methane

methodologies

monitoring

nonattainment

ozone

rebates

secretary

select

updates

warming

cos(v*, climate + emissions + House113 − Obama + econ − env)

administrator

assessment

baseline

biennially

caa
epa

ghg

greenhouse

groundwater

impacts

indoor

methane

monitoring

ozone

pollutant

pollutants

radon

remediation

warming

cos(v*, climate + emissions + House113 − Obama + env − econ)

a
adaptappendix

build

change

continue

deleting

economy

forward

global

grow

headnote

inserting

iv

memorandum

order

paragraph

paragraphs

parts

prosperity

rapidly

rari

rededicate

remains

sections

subchapter

subsection
subsections

understanding

unprecedented

cos(v*, climate + emissions + Obama − House113 + econ − env)

americans

caa

chairs

change

epa

ghg
global

greenhouse

indoor

memorandum

subsection

warming

cos(v*, climate + emissions + Obama − House113 + env − econ)

113th House Economic 113th House Environmental

Obama Economic Obama Environmental

50

75

100

125

50

75

100

125

0.30 0.35 0.40 0.45 0.50 0.30 0.35 0.40 0.45 0.50

Cosine Similarity

F
re

qu
en

cy
 R

an
k

act

add

ceilings

cercla

costs

exhaust
exhaustion

fy

ina

irc

jackson

licensor

miscellaneous

recapture

relating
subpart

subtitle

cos(v*, war + 106 − 107 + oil − terror)

act

add

africa

aggravated

cba

ceilings
cessation

chapter
costs

csa

fdca

fy

ina

insanity

irc

jackson

miscellaneous

particularity

product

recapture

relating revise

subpart

subtitle

wrda

cos(v*, war + 106 − 107 + terror − oil)

amends

app

authorizes

bars

called

calls

confers

contemplated

declares

declined

directing

directs

empowered

empowers

enacted

encourages

entitled

forbidden

instructs

jones
obliged

prohibits

purport

purported
purporting

requires

undertook

urges

cos(v*, war + 107 − 106 + oil − terror)

alert

amends

calls

directs

encourages

instructs

iraq

jones

prohibits

requires

terrorist

threats

urges

cos(v*, war + 107 − 106 + terror − oil)

106th House Oil 106th House Terrorism

107th House Oil 107th House Terrorism

50

75

100

125

50

75

100

125

0.35 0.40 0.45 0.50 0.35 0.40 0.45 0.50

Cosine Similarity

F
re

qu
en

cy
 R

an
k

Figure 5: The top panel is the climate policy query comparing
the 113th U.S. House of Representatives and President Obama:

arg maxv∗ cos(v∗, wv(climate)+wv(emissions)+gv(G1)−
gv(G2)+wv(C1)−wv(C2), where G={113 House, Obama},
G1 ∈ G, G2 ∈ G, G1 6= G2, C={economic, environmental},
C1 ∈ C, C2 ∈ C, C1 6= C2. The bottom panel
is the war policy query for the U.S. House of Representa-

tives before and after the 9-11 terrorist attacks: wv(war),

G={106 House, 107 House}, C={oil, terror}. The exact query
used to create each quadrant is provided at the bottom of the

quadrant.

52



5 Additional Related Work

Political scientists model text to understand politi-
cal processes (Grimmer 2010; Roberts et al. 2014);
however, most of this work focuses on variants of
topic models (Blei et al. 2003). Djuric et al. (2015)
apply a learning procedure similar to Structured
Gov2Vec to streaming documents to learn represen-
tations of documents that are similar to those nearby
in time. Structured Gov2Vec applies this joint hi-
erarchical learning process (using entities to pre-
dict words and other entities) to non-textual entities.
Kim et al. (2014) and Kulkarni et al. (2015) train
neural language models for each year of a time or-
dered corpora to detect changes in words. Instead of
learning models for distinct times, we learn a global
model with embeddings for time-dependent entities
that can be included in queries to analyze change.
Kiros et al. (2014) learn embeddings for text at-
tributes by treating them as gating units to a word
embedding tensor. Their process is more computa-
tionally intensive than ours.

6 Conclusions and Future Work

We learned vector representations of text meta-data
on a novel data set of legal texts that includes case,
statutory, and administrative law. The representa-
tions effectively encoded important relationships be-
tween institutional actors that were not explicitly
provided during training. Finally, we demonstrated
fine-grained investigations of policy differences be-
tween actors based on vector arithmetic. More gen-
erally, the method can be applied to measuring sim-
ilarity between any entities producing text, and used
for recommendations, e.g. what’s the closest think-
tank vector to the non-profit vector representation of
the Sierra Club?

Methodologically, our next goal is to explore
where training on non-textual relations, i.e. Struc-
tural Gov2Vec, is beneficial. It seems to help stabi-
lize representations when exploiting temporal rela-
tions, but political relations may prove to be even
more useful. Substantively, our goal is to learn
a large collection of vectors representing govern-
ment actors at different resolutions and within dif-
ferent contexts6 to address a range of targeted pol-
icy queries. Once we learn these representations, re-

6For instance, learn a vector for the 111th House using its

searchers could efficiently search for differences in
law and policy across time, government branch, and
political party.

Acknowledgments

We thank the anonymous reviewers for helpful sug-
gestions.

References

Bengio, Yoshua, Rjean Ducharme, Pascal Vincent,
and Christian Janvin. 2003. A Neural Probabilistic
Language Model. J. Mach. Learn. Res. 3 (March):
1137–55.

Bergstra, James S., Daniel Yamins, and David
Cox. 2013. Making a Science of Model Search: Hy-
perparameter Optimization in Hundreds of Dimen-
sions for Vision Architectures. In Proceedings of the
30th International Conference on Machine Learn-
ing, 115–23.

Blei, David M., Andrew Y. Ng, and Michael I.
Jordan. 2003. “Latent Dirichlet Allocation.” J.
Mach. Learn. Res. 3 (March): 993–1022.

Collobert, Ronan and Jason Weston. 2008. A
Unified Architecture for Natural Language Process-
ing: Deep Neural Networks with Multitask Learn-
ing. In Proceedings of the 25th International Con-
ference on Machine Learning. 160–167. ACM.

Djuric, Nemanja, Hao Wu, Vladan Radosavlje-
vic, Mihajlo Grbovic, and Narayan Bhamidipati.
2015. Hierarchical Neural Language Models for
Joint Representation of Streaming Documents and
Their Content. In Proceedings of the 24th Inter-
national Conference on World Wide Web, 248–55.
WWW ’15. New York, NY, USA: ACM.

Grimmer, Justin. 2010. “A Bayesian Hierarchi-
cal Topic Model for Political Texts: Measuring Ex-
pressed Agendas in Senate Press Releases.” Political
Analysis 18 (1): 1–35.

Kim, Yoon, Yi-I. Chiu, Kentaro Hanaki, Darshan
Hegde, and Slav Petrov. 2014. Temporal Analy-
sis of Language Through Neural Language Models.

text and temporal relationships to other Houses, learn a vec-
tor for the 111th House using its text and political composition
relationships to other Houses (e.g. ranking by number of Re-
publicans), and then move down in temporal and institutional
resolution, e.g. to individual Members. Then use press release
text to gain a different perspective and iterate through the reso-
lutions again.

53



In Proceedings of the ACL 2014 Workshop on Lan-
guage Technologies and Computational Social Sci-
ence, 61–65. Association for Computational Lin-
guistics.

Kiros, Ryan, Richard Zemel, and Ruslan R
Salakhutdinov. 2014. A Multiplicative Model for
Learning Distributed Text-Based Attribute Repre-
sentations. In Advances in Neural Information Pro-
cessing Systems 27, edited by Z. Ghahramani, M.
Welling, C. Cortes, N. D. Lawrence, and K. Q.
Weinberger, 2348–56. Curran Associates, Inc.

Kulkarni, Vivek, Rami Al-Rfou, Bryan Perozzi,
and Steven Skiena. 2015. Statistically Significant
Detection of Linguistic Change. In Proceedings of
the 24th International Conference on World Wide
Web, 625–35. WWW ’15. New York, NY, USA:
ACM.

Le, Quoc, and Tomas Mikolov. 2014. Distributed
Representations of Sentences and Documents. In
Proceedings of the 31st International Conference on
Machine Learning, 1188–96.

Mikolov, T., W.T. Yih, and G. Zweig. 2013a. Lin-
guistic Regularities in Continuous Space Word Rep-
resentations. In HLT-NAACL, 746–51.

Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg
S Corrado, and Jeff Dean. 2013b. Distributed Rep-
resentations of Words and Phrases and Their Com-
positionality. In Advances in Neural Information
Processing Systems 26, edited by C. J. C. Burges,
L. Bottou, M. Welling, Z. Ghahramani, and K. Q.
Weinberger, 3111–9. Curran Associates, Inc.

Roberts, Margaret E., Brandon M. Stewart,
Dustin Tingley, Christopher Lucas, Jetson Leder-
Luis, Shana Kushner Gadarian, Bethany Albertson,
and David G. Rand. 2014. “Structural Topic Mod-
els for Open-Ended Survey Responses.” American
Journal of Political Science 58 (4): 1064–82.

54


