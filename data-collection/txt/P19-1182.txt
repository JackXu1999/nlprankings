



















































Expressing Visual Relationships via Language


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1873–1883
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

1873

Expressing Visual Relationships via Language

Hao Tan1, Franck Dernoncourt2, Zhe Lin2, Trung Bui2, Mohit Bansal1
1UNC Chapel Hill 2Adobe Research

{haotan, mbansal}@cs.unc.edu, {dernonco, zlin, bui}@adobe.com

Abstract

Describing images with text is a fundamen-
tal problem in vision-language research. Cur-
rent studies in this domain mostly focus on
single image captioning. However, in vari-
ous real applications (e.g., image editing, dif-
ference interpretation, and retrieval), generat-
ing relational captions for two images, can
also be very useful. This important problem
has not been explored mostly due to lack of
datasets and effective models. To push for-
ward the research in this direction, we first
introduce a new language-guided image edit-
ing dataset that contains a large number of
real image pairs with corresponding editing in-
structions. We then propose a new relational
speaker model based on an encoder-decoder
architecture with static relational attention and
sequential multi-head attention. We also ex-
tend the model with dynamic relational atten-
tion, which calculates visual alignment while
decoding. Our models are evaluated on our
newly collected and two public datasets con-
sisting of image pairs annotated with relation-
ship sentences. Experimental results, based on
both automatic and human evaluation, demon-
strate that our model outperforms all baselines
and existing methods on all the datasets.1

1 Introduction

Generating captions to describe natural images is
a fundamental research problem at the intersection
of computer vision and natural language process-
ing. Single image captioning (Mori et al., 1999;
Farhadi et al., 2010; Kulkarni et al., 2011) has
many practical applications such as text-based im-
age search, photo curation, assisting of visually-
impaired people, image understanding in social

1Our data and code are publicly available at:
https://github.com/airsplay/
VisualRelationships

Remove the people from the picture. 

Relational 
Speaker 

Figure 1: An example result of our method showing
the input image pair from our Image Editing Request
dataset, and the output instruction predicted by our re-
lational speaker model trained on the dataset.

media, etc. This task has drawn significant at-
tention in the research community with numerous
studies (Vinyals et al., 2015; Xu et al., 2015; An-
derson et al., 2018), and recent state of the art
methods have achieved promising results on large
captioning datasets, such as MS COCO (Lin et al.,
2014). Besides single image captioning, the com-
munity has also explored other visual captioning
problems such as video captioning (Venugopalan
et al., 2015; Xu et al., 2016), and referring expres-
sions (Kazemzadeh et al., 2014; Yu et al., 2017).
However, the problem of two-image captioning,
especially the task of describing the relationships
and differences between two images, is still under-
explored. In this paper, we focus on advanc-
ing research in this challenging problem by intro-
ducing a new dataset and proposing novel neural
relational-speaker models.2

To the best of our knowledge, Jhamtani and
Berg-Kirkpatrick (2018) is the only public dataset
aimed at generating natural language descriptions
for two real images. This dataset is about ‘spotting
the difference’, and hence focuses more on de-
scribing exhaustive differences by learning align-

2We will release the full data and code upon publication.

https://github.com/airsplay/VisualRelationships
https://github.com/airsplay/VisualRelationships


1874

ments between multiple text descriptions and mul-
tiple image regions; hence the differences are in-
tended to be explicitly identifiable by subtracting
two images. There are many other tasks that re-
quire more diverse, detailed and implicit relation-
ships between two images. Interpreting image
editing effects with instructions is a suitable task
for this purpose, because it has requirements of
exploiting visual transformations and it is widely
used in real life, such as explanation of complex
image editing effects for laypersons or visually-
impaired users, image edit or tutorial retrieval, and
language-guided image editing systems. We first
build a new language-guided image editing dataset
with high quality annotations by (1) crawling im-
age pairs from real image editing request web-
sites, (2) annotating editing instructions via Ama-
zon Mechanical Turk, and (3) refining the annota-
tions through experts.

Next, we propose a new neural speaker model
for generating sentences that describe the vi-
sual relationship between a pair of images. Our
model is general and not dependent on any spe-
cific dataset. Starting from an attentive encoder-
decoder baseline, we first develop a model en-
hanced with two attention-based neural compo-
nents, a static relational attention and a sequential
multi-head attention, to address these two chal-
lenges, respectively. We further extend it by de-
signing a dynamic relational attention module to
combine the advantages of these two components,
which finds the relationship between two images
while decoding. The computation of dynamic re-
lational attention is mathematically equivalent to
attention over all visual “relationships”. Thus, our
method provides a direct way to model visual re-
lationships in language.

To show the effectiveness of our models, we
evaluate them on three datasets: our new dataset,
the ”Spot-the-Diff” dataset (Jhamtani and Berg-
Kirkpatrick, 2018), and the two-image visual rea-
soning NLVR2 dataset (Suhr et al., 2019) (adapted
for our task). We train models separately on each
dataset with the same hyper-parameters and eval-
uate them on the same test set across all methods.
Experimental results demonstrate that our model
outperforms all the baselines and existing meth-
ods. The main contributions of our paper are: (1)
We create a novel human language guided image
editing dataset to boost the study in describing vi-
sual relationships; (2) We design novel relational-

speaker models, including a dynamic relational
attention module, to handle the problem of two-
image captioning by focusing on all their visual
relationships; (3) Our method is evaluated on sev-
eral datasets and achieves the state-of-the-art.

2 Datasets

We present the collection process and statistics
of our Image Editing Request dataset and briefly
introduce two public datasets (viz., Spot-the-Diff
and NLVR2). All three datasets are used to study
the task of two-image captioning and evaluating
our relational-speaker models. Examples from
these three datasets are shown in Fig. 2.

2.1 Image Editing Request Dataset
Each instance in our dataset consists of an image
pair (i.e., a source image and a target image) and a
corresponding editing instruction which correctly
and comprehensively describes the transformation
from the source image to the target image. Our
collected Image Editing Request dataset will be
publicly released along with the scripts to unify
it with the other two datasets.

2.1.1 Collection Process
To create a high-quality, diverse dataset, we follow
a three-step pipeline: image pairs collection, edit-
ing instructions annotation, and post-processing
by experts (i.e., cleaning and test set annotations
labeling).

Images Pairs Collection We first crawl the edit-
ing image pairs (i.e., a source image and a target
image) from posts on Reddit (Photoshop request
subreddit)3 and Zhopped4. Posts generally start
with an original image and an editing specifica-
tion. Other users would send their modified im-
ages by replying to the posts. We collect original
images and modified images as source images and
target images, respectively.

Editing Instruction Annotation The texts in
the original Reddit and Zhopped posts are too
noisy to be used as image editing instructions. To
address this problem, we collect the image edit-
ing instructions on MTurk using an interactive in-
terface that allows the MTurk annotators to either
write an image editing instruction corresponding
to a displayed image pair, or flag it as invalid (e.g.,
if the two images have nothing in common).

3https://www.reddit.com/r/photoshoprequest
4http://zhopped.com



1875

Convert 

Add a sword and a cloak to the squirrel. 
The blue truck is no longer there. 
 
 

A car is approaching the parking lot from the right. 
 
 

Ours (Image Editing Request) Spot-the-Diff 

NLVR2 Captioning NLVR2 Classification 

Each image shows a row of dressed dogs posing 
with a cat that is also wearing some garment. 

In at least one of the images,  
six dogs are posing for a picture,  
while on a bench. 

Each image shows a row of  
dressed dogs posing with a cat 
 that is also wearing some garment. 

True 

False 

Figure 2: Examples from three datasets: our Image Editing Request, Spot-the-Diff, and NLVR2. Each example in-
volves two natural images and an associated sentence describing their relationship. The task of generating NLVR2
captions is converted from its original classification task.

B-1 B-2 B-3 B-4 Rouge-L
Ours 52 34 21 13 45

Spot-the-Diff 41 25 15 8 31
MS COCO 38 22 15 8 34

Table 1: Human agreement on our datasets, compared
with Spot-the-Diff and MS COCO (captions=3). B-1
to B-4 are BLEU-1 to BLEU-4. Our dataset has the
highest human agreement.

Post-Processing by Experts Mturk annotators
are not always experts in image editing. To ensure
the quality of the dataset, we hire an image edit-
ing expert to label each image editing instruction
of the dataset as one of the following four options:
1. correct instruction, 2. incomplete instruction, 3.
implicit request, 4. other type of errors. Only the
data instances labeled with “correct instruction”
are selected to compose our dataset, and are used
in training or evaluating our neural speaker model.

Moreover, two additional experts are required to
write two more editing instructions (one instruc-
tion per expert) for each image pair in the valida-
tion and test sets. This process enables the dataset
to be a multi-reference one, which allows vari-
ous automatic evaluation metrics, such as BLEU,
CIDEr, and ROUGE to more accurately evaluate
the quality of generated sentences.

2.1.2 Dataset Statistics

The Image Editing Request dataset that we have
collected and annotated currently contains 3,939
image pairs (3061 in training, 383 in validation,
495 in test) with 5,695 human-annotated instruc-
tions in total. Each image pair in the training set
has one instruction, and each image pair in the val-
idation and test sets has three instructions, written
by three different annotators. Instructions have an
average length of 7.5 words (standard deviation:
4.8). After removing the words with less than
three occurrences, the dataset has a vocabulary of
786 words. The human agreement of our dataset
is shown in Table 1. The word frequencies in our
dataset are visualized in Fig. 3. Most of the images
in our dataset are realistic. Since the task is im-
age editing, target images may have some artifacts
(see Image Editing Request examples in Fig. 2 and
Fig. 5).

2.2 Existing Public Datasets

To show the generalization of our speaker model,
we also train and evaluate our model on two pub-
lic datasets, Spot-the-Diff (Jhamtani and Berg-
Kirkpatrick, 2018) and NLVR2 (Suhr et al., 2019).
Instances in these two datasets are each composed
of two natural images and a human written sen-
tence describing the relationship between the two



1876

3/4/2019 wordcloud.svg

file:///Users/hatan/Desktop/wordcloud.svg 1/1

imageremove
background

add

change

make increa
se

picture

w
hite

crop color

black
contrast

photo

entire

brightness

face

wh
ole

blue

text

replace
man

br
ig
ht
en

red
behind

put

right

zoom

head
people

co
lo
rs

filter
lighten

darken

hair girl

yellow

around

take

ro
ta
te

left

light

green
brighter

woman

effect

insert

turn

eyes

top

sky

side

area

decrease

han
d

blur

dog

da
rk
er

frame

man's
bottom

car

skin

bab
y

pink

log
o

sharpen

sign

pe
rso
n

girl's

new

hat

delete

front
corner

glare

word

one

glasses

lighter

needs

two
little

place

w
at
er

boy

give

back

onto

resize

cu
t

smaller
away

reflection

everything

purple

move slig
htly

di
sto
rt

gi
rls

shirt

edit

look

guy

orange

bigger

br
ow

n

m
ans

significantly

lines

tone

cat

ph
ot
os
ho
p

please

w
all

beach

solid

line

saturation

border

words

dogs

just

eye

adjust

select

shadow

different

en
la
rg
e

clouds

dark

grey

body
flip

finger

nose

tre
e

like

lig
hti
ng

degrees

middle

gray

circle

leash

part

colorful

tattoo

except

closer

faces

boy's
less fire

can

bit

bottle

chair

glass

size

clockwisecartoon

center

larger

child

lot

sharpness removed

blurry

tint

sun
use

exposure

flowers

vibrant

instead

outline

show

sunglasses

letters

reduce

grass

sa
nt
a

clear

moon

boys

fill

arm

hu
e

woman's

fo
re
st

shrink

camera bright

ground

trump

first

erase

star

flag

stains

apply

tiger

flame

spots
cats

guys

boat saturated

sharper

t­shirt

clearer

womans

donald

couple

window

zombie

figure

smoke

table

hands

scene

signs
heart

rest

old

building

saturate

persons

shadows

sunset

centr
e

mouth

wires

space

heads

tool

lady

half

motorcycle

photograph

lettering

portrait

number

object

smooth

mirror

wheels

turned

paint

floor

areas

lower

focus

much

want

ne
ck

cap

gunbag

fix

get

see

colorize

subjects

mountain

enhance

writing

effects

lights

collar

clone

fence

fish

rim
s

glow

bike

big

billboard

characterballoons

graffiti

gradient

mustache

border
s

cropped

clothes

inside

flow
er

com
ing

edges

treesraise

match

torch

suit

next

beer

door

day

kid

rid

imperfections

straighten

landscape

position

wrinkles

subject

visible

stripes

co
un
te
r

switch

cracks

beard

oc
ea
n

par
ts

youre

going

house

cover

arm
s

cit
y

ball

says

bar

say

yellowness

highlight
intensityscratches

cigarette

standing

straight

children

clarity

overall

guitars

colored

corners

rainbow

bubbles

correct

graphic

cooler

design

second

extend

layer

looks dress

bread leave

paste

sepia

plain

horse

three

write

paper

tiki

sand

gold

four

coin

font

dots

box

horizontal

headphones

completely

christmas

blemishes

lightning

coloring

together

pictures

isolate

ob
jec
ts

creases

monster

vehicle

looking

portion

sitting

cloudy

batman

pencil

invert

cowboy

pupils

wooden

meadow

sketch

covers

leaves

clean

shark

sides

style

shape

birds

babys

happy

boots

bring

field

claus

need

legs

vi
ew

loaf

mask

len
s

tan

paw

tv

desaturate

backround

gray
scal

e

eliminate

president

vertical

eyebrows

forehead

vibrance

drawing

hold
ing

shorten

another

clinton

natural

overlay

unknown

im
ages

baby's

hel
me
t

whiter

street

chairs

warmer

better

blonde

across

silver

wings

shade

thing

stray

sta
in

crown

cro
ss

cat's

night

marks

small
cable

bo
ke
h

blo
od

gonna

fe
et

fall

long

lion

frog

draw

tilt

game

snow

lips

pole

card

fold

cars

halo

hes

www.maxim.com

microphone

hi
gh
lig
ht
s

minecraft

butterfly

circular

american

necklace

birthday

ponytail

balance

stretch

peoples

quality

blanket

outside

red­eye

player

cr
ea
te

crease

throne

appear

square

higher

strong

poster

height

screen

symbol

family

le
tte
r

guitar

statue

facing

whiten

longer

third

blend

chest

mario

cream

hairs

anime

sab
er

boost

touch

sword

ghost

stars

stamp

every

brick

bo
ar
d

shoes

truck

mommy

flare

tears

dog's

beam

neon

time
palm

lake

auto

pool

bald

road

name

fade

date

lamp

men

ice
fit

tag

reflections

orientation

foreground

cartoonish

television

basketball

vertically

m
ou
nt
ain
s

brightess

explosion

hairstyle

rectangle

original

graphics

sandwich

godzilla

triangle

drawings
painting

dinosaur

removing

separate

entirely

ou
tlin
ed

shoulder

numbers

figures

restore

wearing

america

hillary

focused

gorilla

wording

circled
pattern

setting

zombies

animals

redness

texture

showing

sect
ion

thicker

emblem

pla
ne
t

makeup

simply

bronze

smiley

ninety

redeye

ladies

making

flames

expand

ups
ide

levels

photos

trumps

de
se
rt

within

outfit

along

jesus
extra large

swirl

thats
party

bluer

drive

plate

tubes

bunch

teeth

makes

piece

empty

grand

trash

clo
ud

rocks

smile

cig
ar

beam
s

stone
clown

theft

crowd

month

bride

bunny

chain

paws

wars

mark

dive

haze

kids

spot

iron

wood

acne

army

bi
rd

cans

wi
ll

made

jack

roof

also full

swap

duck

pony puss

Figure 3: Word cloud showing the vocabulary frequen-
cies of our Image Editing Request dataset.

images. To the best of our knowledge, these are
the only two public datasets with a reasonable
amount of data that are suitable for our task. We
next briefly introduce these two datasets.

Spot-the-Diff This dataset is designed to help
generate a set of instructions that can comprehen-
sively describe all visual differences. Thus, the
dataset contains images from video-surveillance
footage, in which differences can be easily found.
This is because all the differences could be effec-
tively captured by subtractions between two im-
ages, as shown in Fig. 2. The dataset contains
13,192 image pairs, and an average of 1.86 cap-
tions are collected for each image pair. The dataset
is split into training, validation, and test sets with
a ratio of 8:1:1.

NLVR2 The original task of Cornell Natural
Language for Visual Reasoning (NLVR2) dataset
is visual sentence classification, see Fig. 2 for an
example. Given two related images and a natu-
ral language statement as inputs, a learned model
needs to determine whether the statement cor-
rectly describes the visual contents. We convert
this classification task to a generation task by tak-
ing only the image pairs with correct descriptions.
After conversion, the amount of data is 51,020,
which is almost half of the original dataset with
a size of 107,296. We also preserve the training,
validation, and test split in the original dataset.

3 Relational Speaker Models

In this section, we aim to design a general speaker
model that describes the relationship between two
images. Due to the different kinds of visual rela-
tionships, the meanings of images vary in different

tasks: “before” and “after” in Spot-the-Diff, “left”
and “right” in NLVR2, “source” and “target” in
our Image Editing Request dataset. We use the
nomenclature of “source” and “target” for simpli-
fication, but our model is general and not designed
for any specific dataset. Formally, the model gen-
erates a sentence {w1, w2, ..., wT } describing the
relationship between the source image ISRC and
the target image ITRG. {wt}Tt=1 are the word to-
kens with a total length of T . ISRC and ITRG are
natural images in their raw RGB pixels. In the
rest of this section, we first introduce our basic at-
tentive encoder-decoder model, and show how we
gradually improve it to fit the task better.

3.1 Basic Model

Our basic model (Fig. 4(a)) is similar to the
baseline model in Jhamtani and Berg-Kirkpatrick
(2018), which is adapted from the attentive
encoder-decoder model for single image caption-
ing (Xu et al., 2015). We use ResNet-101 (He
et al., 2016) as the feature extractor to encode the
source image ISRC and the target image ITRG. The
feature maps of size N ×N × 2048 are extracted,
where N is the height or width of the feature map.
Each feature in the feature map represents a part
of the image. Feature maps are then flattened to
two N2 × 2048 feature sequences f SRC and f TRG,
which are further concatenated to a single feature
sequence f .

f SRC = ResNet (ISRC) (1)

f TRG = ResNet (ITRG) (2)

f =
[
f SRC1 , . . . , f

SRC
N2 , f

TRG
1 , , . . . , f

TRG
N2

]
(3)

At each decoding step t, the LSTM cell takes the
embedding of the previous word wt−1 as an input.
The wordwt−1 either comes from the ground truth
(in training) or takes the token with maximal prob-
ability (in evaluating). The attention module then
attends to the feature sequence f with the hidden
output ht as a query. Inside the attention module,
it first computes the alignment scores αt,i between
the query ht and each fi. Next, the feature se-
quence f is aggregated with a weighted average
(with a weight of α) to form the image context f̂ .
Lastly, the context f̂t and the hidden vector ht are
merged into an attentive hidden vector ĥt with a



1877

LSTM LSTM 

Alignment 

LSTM LSTM 

Multi-Heads Att 

LSTM 

Att Module 

  

LSTM 

(a) Basic Model (b) Multi-Head Attention (d) Dynamic Relational Attention 

 (c) Static Relational Attention 

LSTM LSTM 

Alignment 

Reduction 

Att Module 

Att Module Att Module 

ht

wtwt�1
p(wt)

Figure 4: The evolution diagram of our models to describe the visual relationships. One decoding step at t is
shown. The linear layers are omitted for clarity. The basic model (a) is an attentive encoder-decoder model, which
is enhanced by the multi-head attention (b) and static relational attention (c). Our best model (d) dynamically
computes the relational scores in decoding to avoid losing relationship information.

fully-connected layer:

w̃t−1 = embedding (wt−1) (4)

ht, ct = LSTM(w̃t−1, ht−1, ct−1) (5)

αt,i = softmaxi

(
h>t WIMGfi

)
(6)

f̂t =
∑
i

αt,ifi (7)

ĥt = tanh(W1[f̂t;ht] + b1) (8)

The probability of generating the k-th word token
at time step t is softmax over a linear transforma-
tion of the attentive hidden ĥt. The loss Lt is the
negative log likelihood of the ground truth word
token w∗t :

pt(wt,k) = softmaxk

(
WW ĥt + bW

)
(9)

Lt = − log pt(w∗t ) (10)

3.2 Sequential Multi-Head Attention
One weakness of the basic model is that the plain
attention module simply takes the concatenated
image feature f as the input, which does not differ-
entiate between the two images. We thus consider
applying a multi-head attention module (Vaswani

et al., 2017) to handle this (Fig. 4(b)). Instead of
using the simultaneous multi-head attention 5 in
Transformer (Vaswani et al., 2017), we implement
the multi-head attention in a sequential way. This
way, when the model is attending to the target im-
age, the contextual information retrieved from the
source image is available and can therefore per-
form better at differentiation or relationship learn-
ing.

In detail, the source attention head first attends
to the flattened source image feature f SRC. The
attention module is built in the same way as in
Sec. 3.1, except that it now only attends to the
source image:

αSRCt,i = softmaxi(h
>
t WSRCf

SRC
i ) (11)

f̂ SRCt =
∑
i

αSRCt,i f
SRC
i (12)

ĥSRCt = tanh(W2[f̂
SRC
t ;ht] + b2) (13)

The target attention head then takes the out-
put of the source attention ĥSRCt as a query to re-
trieve appropriate information from the target fea-

5We also tried the original multi-head attention but it is
empirically weaker than our sequential multi-head attention.



1878

ture f TRG:

αTRGt,j = softmaxj(ĥ
SRC>
t WTRGf

TRG
j ) (14)

f̂ TRGt =
∑
j

αTRGt,j f
TRG
j (15)

ĥTRGt = tanh(W3[f̂
TRG
t ; ĥ

SRC
t ] + b3) (16)

In place of ĥt, the output of the target head ĥTRGt is
used to predict the next word.6

3.3 Static Relational Attention
Although the sequential multi-head attention
model can learn to differentiate the two images,
visual relationships are not explicitly examined.
We thus allow the model to statically (i.e., not in
decoding) compute the relational score between
source and target feature sequences and reduce
the scores into two relationship-aware feature se-
quences. We apply a bi-directional relational at-
tention (Fig. 4(c)) for this purpose: one from the
source to the target, and one from the target to the
source. For each feature in the source feature se-
quence, the source-to-target attention computes its
alignment with the features in the target feature se-
quences. The source feature, the attended target
feature, and the difference between them are then
merged together with a fully-connected layer:

αS→Ti,j = softmaxj((WSf
SRC
i )

>(WTf
TRG
j )) (17)

f̂ S→Ti =
∑
j

αS→Ti,j f
TRG
j (18)

f̂ Si = tanh(W4[f
SRC
i ; f̂

S→T
i ] + b4) (19)

We decompose the attention weight into two small
matrices WS and WT so as to reduce the number
of parameters, because the dimension of the im-
age feature is usually large. The target-to-source
cross-attention is built in an opposite way: it takes
each target feature f TRGj as a query, attends to the
source feature sequence, and get the attentive fea-
ture f̂ Tj . We then use these two bidirectional at-
tentive sequences f̂ Si and f̂

T
j in the multi-head at-

tention module (shown in previous subsection) at
each decoding step.

3.4 Dynamic Relational Attention
The static relational attention module compresses
pairwise relationships (of size N4) into two

6We tried to exchange the order of two heads or have two
orders concurrently. We didn’t see any significant difference
in results between them.

relationship-aware feature sequences (of size 2×
N2). The compression saves computational re-
sources but has potential drawback in information
loss as discussed in Bahdanau et al. (2015) and Xu
et al. (2015). In order to avoid losing information,
we modify the static relational attention module
to its dynamic version, which calculates the rela-
tional scores while decoding (Fig. 4(d)).

At each decoding step t, the dynamic relational
attention calculates the alignment score at,i,j be-
tween three vectors: a source feature f SRCi , a tar-
get feature f TRGj , and the hidden state ht. Since
the dot-product used in previous attention modules
does not have a direct extension for three vectors,
we extend the dot product and use it to compute
the three-vector alignment score.

dot(x, y) =
∑
d

xd yd = x
>y (20)

dot∗(x, y, z) =
∑
d

xd ydzd = (x� y)>z (21)

at,i,j = dot
∗(WSKf

SRC
i ,WTKf

TRG
j ,WHKht) (22)

= (WSKf
SRC
i �WTKf TRGj )>WHKht (23)

where � is the element-wise multiplication.
The alignment scores (of size N4) are normal-

ized by softmax. And the attention information is
fused to the attentive hidden vector f̂Dt as previous.

αt,i,j = softmaxi,j (at,i,j) (24)

f̂ SRC-Dt =
∑
i,j

αt,i,jf
SRC
i (25)

f̂ TRG-Dt =
∑
i,j

αt,i,jf
TRG
j (26)

f̂Dt = tanh(W5[f̂
SRC-D
t ; f̂

TRG-D
t ;ht]+b5) (27)

= tanh(W5Sf̂
SRC-D
t +W5Tf̂

TRG-D
t +

W5Hht + b5) (28)

whereW5S,W5T,W5H are sub-matrices ofW5 and
W5 = [W5S,W5T,W5H].

According to Eqn. 23 and Eqn. 28, we find an
analog in conventional attention layers with fol-
lowing specifications:

• Query: ht

• Key: WSKf SRCi �WTKf TRGj
• Value: W5Sf SRCi +W5Tf TRGj

The key WSKf SRCi � WTKf TRGj and the value
W5Sf

SRC
i +W5Tf

TRG
j can be considered as repre-

sentations of the visual relationships between f SRCi



1879

Method BLEU-4 CIDEr METEOR ROUGE-L
Our Dataset (Image Editing Request)

basic model 5.04 21.58 11.58 34.66
+multi-head att 6.13 22.82 11.76 35.13
+static rel-att 5.76 20.70 12.59 35.46

-static +dynamic rel-att 6.72 26.36 12.80 37.25
Spot-the-Diff

CAPT(Jhamtani and Berg-Kirkpatrick, 2018) 7.30 26.30 10.50 25.60
DDLA(Jhamtani and Berg-Kirkpatrick, 2018) 8.50 32.80 12.00 28.60

basic model 5.68 22.20 10.98 24.21
+multi-head att 7.52 31.39 11.64 26.96
+static rel-att 8.31 33.98 12.95 28.26

-static +dynamic rel-att 8.09 35.25 12.20 31.38
NLVR2

basic model 5.04 43.39 10.82 22.19
+multi-head att 5.11 44.80 10.72 22.60
+static rel-att 4.95 45.67 10.89 22.69

-static +dynamic rel-att 5.00 46.41 10.37 22.94

Table 2: Automatic metric of test results on three datasets. Best results of the main metric are marked in bold font.
Our full model is the best on all three datasets with the main metric.

and f TRGj . It is a direct attention to the visual re-
lationship between the source and target images,
hence is suitable for the task of generating rela-
tionship descriptions.

4 Results

To evaluate the performance of our relational
speaker models (Sec. 3), we trained them on all
three datasets (Sec. 2). We evaluate our models
based on both automatic metrics as well as pair-
wise human evaluation. We also show our gener-
ated examples for each dataset.

4.1 Experimental Setup

We use the same hyperparameters when applying
our model to the three datasets. Dimensions of
hidden vectors are 512. The model is optimized
by Adam with a learning rate of 1e − 4. We
add dropout layers of rate 0.5 everywhere to avoid
over-fitting. When generating instructions for
evaluation, we use maximum-decoding: the word
wt generated at time step t is argmaxk p(wt,k).
For the Spot-the-Diff dataset, we take the “Single
sentence decoding” experiment as in Jhamtani and
Berg-Kirkpatrick (2018). We also try to mix the
three datasets but we do not see any improvement.
We also try different ways to mix the three datasets
but we do not see improvement. We first train a

unified model on the union of these datasets. The
metrics drop a lot because the tasks and language
domains (e.g., the word dictionary and lengths of
sentences) are different from each other. We next
only share the visual components to overcome the
disagreement in language. However, the image
domain are still quite different from each other (as
shown in Fig. 2). Thus, we finally separately train
three models on the three datasets with minimal
cross-dataset modifications.

4.2 Metric-Based Evaluation

As shown in Table 2, we compare the performance
of our models on all three datasets with various
automated metrics. Results on the test sets are
reported. Following the setup in Jhamtani and
Berg-Kirkpatrick (2018), we takes CIDEr (Vedan-
tam et al., 2015) as the main metric in evaluating
the Spot-the-Diff and NLVR2 datasets. However,
CIDEr is known as its problem in up-weighting
unimportant details (Kilickaya et al., 2017; Liu
et al., 2017b). In our dataset, we find that instruc-
tions generated from a small set of short phrases
could get a high CIDEr score. We thus change the
main metric of our dataset to METEOR (Banerjee
and Lavie, 2005), which is manually verified to be
aligned with human judgment on the validation set
in our dataset. To avoid over-fitting, the model is



1880

Basic Full Both Good Both Not
Ours(IEdit) 11 24 5 60

Spot-the-Diff 22 37 6 35
NLVR2 24 37 17 22

Table 3: Human evaluation on 100 examples. Image
pair and two captions generated by our basic model and
full model are shown to the user. The user chooses
one from ‘Basic’ model wins, ‘Full’ model wins, ‘Both
Good’, or ‘Both Not’. Better model marked in bold
font.

early-stopped based on the main metric on vali-
dation set. We also report the BLEU-4 (Papineni
et al., 2002) and ROUGE-L (Lin, 2004) scores.

The results on various datasets shows the grad-
ual improvement made by our novel neural com-
ponents, which are designed to better describe the
relationship between 2 images. Our full model has
a significant improvement in result over baseline.
The improvement on the NLVR2 dataset is lim-
ited because the comparison of two images was
not forced to be considered when generating in-
structions.

4.3 Human Evaluation and Qualitative
Analysis

We conduct a pairwise human evaluation on our
generated sentences, which is used in Celikyil-
maz et al. (2018) and Pasunuru and Bansal (2017).
Agarwala (2018) also shows that the pairwise
comparison is better than scoring sentences indi-
vidually. We randomly select 100 examples from
the test set in each dataset and generate captions
via our full speaker model. We ask users to choose
a better instruction between the captions generated
by our full model and the basic model, or alter-
natively indicate that the two captions are equal
in quality. The Image Editing Request dataset is
specifically annotated by the image editing expert.
The winning rate of our full model (dynamic re-
lation attention) versus the basic model is shown
in Table 3. Our full model outperforms the ba-
sic model significantly. We also show positive and
negative examples generated by our full model in
Fig. 5. In our Image Editing Request corpus, the
model was able to detect and describe the edit-
ing actions but it failed in handling the arbitrary
complex editing actions. We keep these hard ex-
amples in our dataset to match real-world require-
ments and allow follow-up future works to pursue
the remaining challenges in this task. Our model
is designed for non-localized relationships thus

we do not explicitly model the pixel-level differ-
ences; however, we still find that the model could
learn these differences in the Spot-the-Diff dataset.
Since the descriptions in Spot-the-Diff is relatively
simple, the errors mostly come from wrong enti-
ties or undetected differences as shown in Fig. 5.
Our model is also sensitive to the image contents
as shown in the NLVR2 dataset.

5 Related Work

In order to learn a robust captioning system, pub-
lic datasets have been released for diverse tasks
including single image captioning (Lin et al.,
2014; Plummer et al., 2015; Krishna et al., 2017),
video captioning (Xu et al., 2016), referring ex-
pressions (Kazemzadeh et al., 2014; Mao et al.,
2016), and visual question answering (Antol et al.,
2015; Zhu et al., 2016; Johnson et al., 2017). In
terms of model progress, recent years witnessed
strong research progress in generating natural lan-
guage sentences to describe visual contents, such
as Vinyals et al. (2015); Xu et al. (2015); Ran-
zato et al. (2016); Anderson et al. (2018) in single
image captioning, Venugopalan et al. (2015); Pan
et al. (2016); Pasunuru and Bansal (2017) in video
captioning, Mao et al. (2016); Liu et al. (2017a);
Yu et al. (2017); Luo and Shakhnarovich (2017) in
referring expressions, Jain et al. (2017); Li et al.
(2018); Misra et al. (2018) in visual question gen-
eration, and Andreas and Klein (2016); Cohn-
Gordon et al. (2018); Luo et al. (2018); Vedantam
et al. (2017) in other setups.

Single image captioning is the most relevant
problem to the two-images captioning. Vinyals
et al. (2015) created a powerful encoder-decoder
(i.e., CNN to LSTM) framework in solving the
captioning problem. Xu et al. (2015) further
equipped it with an attention module to handle
the memorylessness of fixed-size vectors. Ran-
zato et al. (2016) used reinforcement learning to
eliminate exposure bias. Recently, Anderson et al.
(2018) brought the information from object detec-
tion system to further boost the performance.

Our model is built based on the attentive
encoder-decoder model (Xu et al., 2015), which is
the same choice in Jhamtani and Berg-Kirkpatrick
(2018). We apply the RL training with self-
critical (Rennie et al., 2017) but do not see signif-
icant improvement, possibly because of the rela-
tively small data amount compared to MS COCO.
We also observe that the detection system in An-



1881

add a filter to the image 

change the background to blue 

Positive  
Examples 

Negative 
Examples 

Image Editing Request Spot-the-Diff NLVR2 

there is a bookshelf with a white  
shelf in one of the images . 

the left image shows a pair of 
shoes wearing a pair of shoes . 

the person in the white shirt is gone 

the black car in the middle row is gone 

Figure 5: Examples of positive and negative results of our model from the three datasets. Selfies are blurred.

derson et al. (2018) has a high probability to fail
in the three datasets, e.g., the detection system can
not detect the small cars and people in spot-the-
diff dataset. The DDLA (Difference Description
with Latent Alignment) method proposed in Jham-
tani and Berg-Kirkpatrick (2018) learns the align-
ment between descriptions and visual differences.
It relies on the nature of the particular dataset and
thus could not be easily transferred to other dataset
where the visual relationship is not obvious. The
two-images captioning could also be considered
as a two key-frames video captioning problem,
and our sequential multi-heads attention is a modi-
fied version of the seq-to-seq model (Venugopalan
et al., 2015). Some existing work (Chen et al.,
2018; Wang et al., 2018; Manjunatha et al., 2018)
also learns how to modify images. These datasets
and methods focus on the image colorization and
adjustment tasks, while our dataset aims to study
the general image editing request task.

6 Conclusion

In this paper, we explored the task of describ-
ing the visual relationship between two images.
We collected the Image Editing Request dataset,
which contains image pairs and human annotated
editing instructions. We designed novel relational
speaker models and evaluate them on our col-
lected and other public existing dataset. Based on
automatic and human evaluations, our relational
speaker model improves the ability to capture vi-
sual relationships. For future work, we are going
to further explore the possibility to merge the three
datasets by either learning a joint image represen-
tation or by transferring domain-specific knowl-
edge. We are also aiming to enlarge our Image
Editing Request dataset with newly-released posts
on Reddit and Zhopped.

Acknowledgments

We thank the reviewers for their helpful comments
and Nham Le for helping with the initial data
collection. This work was supported by Adobe,
ARO-YIP Award #W911NF-18-1-0336, and fac-
ulty awards from Google, Facebook, and Sales-
force. The views, opinions, and/or findings con-
tained in this article are those of the authors and
should not be interpreted as representing the offi-
cial views or policies, either expressed or implied,
of the funding agency.

References
Aseem Agarwala. 2018. Automatic photography with

google clips.

Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei
Zhang. 2018. Bottom-up and top-down attention for
image captioning and visual question answering. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 6077–6086.

Jacob Andreas and Dan Klein. 2016. Reasoning about
pragmatics with neural listeners and speakers. In
Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing, pages
1173–1182.

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C Lawrence Zitnick,
and Devi Parikh. 2015. Vqa: Visual question an-
swering. In Proceedings of the IEEE international
conference on computer vision, pages 2425–2433.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings

https://ai.googleblog.com/2018/05/automatic-photography-with-google-clips.html
https://ai.googleblog.com/2018/05/automatic-photography-with-google-clips.html


1882

of the acl workshop on intrinsic and extrinsic evalu-
ation measures for machine translation and/or sum-
marization, pages 65–72.

Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, and
Yejin Choi. 2018. Deep communicating agents for
abstractive summarization. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Pa-
pers), pages 1662–1675.

Jianbo Chen, Yelong Shen, Jianfeng Gao, Jingjing Liu,
and Xiaodong Liu. 2018. Language-based image
editing with recurrent attentive models. In Proceed-
ings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 8721–8729.

Reuben Cohn-Gordon, Noah Goodman, and Christo-
pher Potts. 2018. Pragmatically informative image
captioning with character-level inference. In Pro-
ceedings of the 2018 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Vol-
ume 2 (Short Papers), volume 2, pages 439–443.

Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: Generating sentences from images.
In European conference on computer vision, pages
15–29. Springer.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770–
778.

Unnat Jain, Ziyu Zhang, and Alexander G Schwing.
2017. Creativity: Generating diverse questions us-
ing variational autoencoders. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition, pages 6485–6494.

Harsh Jhamtani and Taylor Berg-Kirkpatrick. 2018.
Learning to describe differences between pairs of
similar images. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, pages 4024–4034.

Justin Johnson, Bharath Hariharan, Laurens van der
Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross
Girshick. 2017. Clevr: A diagnostic dataset for
compositional language and elementary visual rea-
soning. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
2901–2910.

Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,
and Tamara Berg. 2014. Referitgame: Referring to
objects in photographs of natural scenes. In Pro-
ceedings of the 2014 conference on empirical meth-
ods in natural language processing (EMNLP), pages
787–798.

Mert Kilickaya, Aykut Erdem, Nazli Ikizler-Cinbis,
and Erkut Erdem. 2017. Re-evaluating automatic
metrics for image captioning. In Proceedings of the
15th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Volume 1,
Long Papers, volume 1, pages 199–209.

Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-
son, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Yannis Kalantidis, Li-Jia Li, David A Shamma,
et al. 2017. Visual genome: Connecting language
and vision using crowdsourced dense image anno-
tations. International Journal of Computer Vision,
123(1):32–73.

Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Sim-
ing Li, Yejin Choi, Alexander C Berg, and Tamara L
Berg. 2011. Baby talk: Understanding and generat-
ing image descriptions. In Proceedings of the 24th
CVPR. Citeseer.

Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli
Ouyang, Xiaogang Wang, and Ming Zhou. 2018.
Visual question generation as dual task of visual
question answering. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 6116–6124.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. Text Summarization
Branches Out.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. 2014. Microsoft coco:
Common objects in context. In European confer-
ence on computer vision, pages 740–755. Springer.

Jingyu Liu, Liang Wang, and Ming-Hsuan Yang.
2017a. Referring expression generation and com-
prehension via attributes. In Proceedings of the
IEEE International Conference on Computer Vision,
pages 4856–4864.

Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama,
and Kevin Murphy. 2017b. Improved image cap-
tioning via policy gradient optimization of spider.
In Proceedings of the IEEE international conference
on computer vision, pages 873–881.

Ruotian Luo, Brian Price, Scott Cohen, and Gregory
Shakhnarovich. 2018. Discriminability objective for
training descriptive captions. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition, pages 6964–6974.

Ruotian Luo and Gregory Shakhnarovich. 2017.
Comprehension-guided referring expressions. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 7102–7111.

Varun Manjunatha, Mohit Iyyer, Jordan Boyd-Graber,
and Larry Davis. 2018. Learning to color from lan-
guage. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers), pages 764–769.



1883

Junhua Mao, Jonathan Huang, Alexander Toshev, Oana
Camburu, Alan L Yuille, and Kevin Murphy. 2016.
Generation and comprehension of unambiguous ob-
ject descriptions. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition,
pages 11–20.

Ishan Misra, Ross Girshick, Rob Fergus, Martial
Hebert, Abhinav Gupta, and Laurens van der
Maaten. 2018. Learning by asking questions. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 11–20.

Yasuhide Mori, Hironobu Takahashi, and Ryuichi Oka.
1999. Image-to-word transformation based on di-
viding and vector quantizing images with words. In
First International Workshop on Multimedia Intelli-
gent Storage and Retrieval Management, pages 1–9.
Citeseer.

Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, and Yuet-
ing Zhuang. 2016. Hierarchical recurrent neural en-
coder for video representation with application to
captioning. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
1029–1038.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Ramakanth Pasunuru and Mohit Bansal. 2017. Re-
inforced video captioning with entailment rewards.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
979–985.

Bryan A Plummer, Liwei Wang, Chris M Cervantes,
Juan C Caicedo, Julia Hockenmaier, and Svetlana
Lazebnik. 2015. Flickr30k entities: Collecting
region-to-phrase correspondences for richer image-
to-sentence models. In Proceedings of the IEEE
international conference on computer vision, pages
2641–2649.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2016. Sequence level train-
ing with recurrent neural networks. In International
Conference on Learning Representations.

Steven J Rennie, Etienne Marcheret, Youssef Mroueh,
Jerret Ross, and Vaibhava Goel. 2017. Self-critical
sequence training for image captioning. In Proceed-
ings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 7008–7024.

Alane Suhr, Stephanie Zhou, Iris Zhang, Huajun Bai,
and Yoav Artzi. 2019. A corpus for reasoning about
natural language grounded in photographs. In Pro-
ceedings of the 57th annual meeting on association
for computational linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Ramakrishna Vedantam, Samy Bengio, Kevin Murphy,
Devi Parikh, and Gal Chechik. 2017. Context-aware
captions from context-agnostic supervision. In Pro-
ceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition, pages 251–260.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. Cider: Consensus-based image de-
scription evaluation. In Proceedings of the IEEE
conference on computer vision and pattern recog-
nition, pages 4566–4575.

Subhashini Venugopalan, Marcus Rohrbach, Jeffrey
Donahue, Raymond Mooney, Trevor Darrell, and
Kate Saenko. 2015. Sequence to sequence-video to
text. In Proceedings of the IEEE international con-
ference on computer vision, pages 4534–4542.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In Proceedings of the IEEE
conference on computer vision and pattern recogni-
tion, pages 3156–3164.

Hai Wang, Jason D Williams, and SingBing Kang.
2018. Learning to globally edit images with textual
description. arXiv preprint arXiv:1810.05786.

Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-
vtt: A large video description dataset for bridging
video and language. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 5288–5296.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:
Neural image caption generation with visual atten-
tion. In International conference on machine learn-
ing, pages 2048–2057.

Licheng Yu, Hao Tan, Mohit Bansal, and Tamara L
Berg. 2017. A joint speaker-listener-reinforcer
model for referring expressions. In Proceedings of
the IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 7282–7290.

Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-
Fei. 2016. Visual7w: Grounded question answering
in images. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages
4995–5004.


