



















































Deep Dominance - How to Properly Compare Deep Neural Models


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2773–2785
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

2773

Deep Dominance - How to Properly Compare Deep Neural Models

Rotem Dror Segev Shlomov
Faculty of Industrial Engineering and Management, Technion, IIT

rtmdrr|segevs|roiri@technion.ac.il

Roi Reichart

Abstract

Comparing between Deep Neural Network
(DNN) models based on their performance
on unseen data is crucial for the progress of
the NLP field. However, these models have
a large number of hyper-parameters and, be-
ing non-convex, their convergence point de-
pends on the random values chosen at ini-
tialization and during training. Proper DNN
comparison hence requires a comparison be-
tween their empirical score distributions on
unseen data, rather than between single eval-
uation scores as is standard for more simple,
convex models. In this paper, we propose to
adapt to this problem a recently proposed test
for the Almost Stochastic Dominance relation
between two distributions. We define the cri-
teria for a high quality comparison method
between DNNs, and show, both theoretically
and through analysis of extensive experimen-
tal results with leading DNN models for se-
quence tagging tasks, that the proposed test
meets all criteria while previously proposed
methods fail to do so. We hope the test we
propose here will set a new working practice
in the NLP community.1

1 Introduction

A large portion of the research activity in Natural
Language Processing (NLP) is devoted to the de-
velopment of new algorithms for existing or new
tasks. To evaluate the quality of a new method, its
performance on unseen datasets is compared to the
performance of existing methods. The progress of
the field hence crucially depends on our ability to
draw conclusions from such comparisons.

In the past, most supervised NLP models have
been linear (or log-linear), convex and relatively
simple (e.g. (Toutanova et al., 2003; Finkel et al.,
2008; Ritter et al., 2011)). Hence, their training

1Our code is available at: https://github.com/
rtmdrr/deepComparison

was deterministic and the number of configura-
tions a model could have was rather small – deci-
sions about model design were usually limited to
feature selection and the selection of one of a few
loss functions. Consequently, when one model
performed better than another on unseen data it
was safe to argue that the winning model was gen-
erally better, especially when the results were sta-
tistically significant (Dror et al., 2018), and when
the effect of multiple hypothesis testing was taken
into account in cases of evaluation with multiple
datasets (Dror et al., 2017).

With the recent emergence of Deep Neural Net-
works (DNNs), data-driven performance compar-
ison has become much more complicated. While
models such as LSTM (Hochreiter and Schmid-
huber, 1997), Bi-LSTM (Schuster and Paliwal,
1997) and the transformer (Vaswani et al., 2017)
improved the state-of-the-art in many NLP tasks
(e.g. (Dozat and Manning, 2017; Hershcovich
et al., 2017; Yadav and Bethard, 2018)), it is
much more difficult to compare the performance
of algorithms that are based on these models.
This is because the loss functions of these mod-
els are non-convex (Dauphin et al., 2014), mak-
ing the solution to which they converge (a lo-
cal minimum or a saddle point) sensitive to ran-
dom weight initialization and the order of train-
ing examples. Moreover, as these complex mod-
els are not fully understood, their training is often
enhanced by heuristics such as random dropouts
(Srivastava et al., 2014) that introduces another
level of non-determinism to the training process.
Finally, the increased model complexity results in
a much larger number of configurations, governed
by a large space of hyper-parameters for model
properties such as the number of layers and the
number of neurons in each layer.

With so many degrees of freedom governed by
random and arbitrary values, when comparing two

 https://github.com/rtmdrr/deepComparison
 https://github.com/rtmdrr/deepComparison


2774

DNNs it is not possible to consider a single test-set
evaluation score for each model. If we do that, we
might compare just the best models that someone
happened to train rather than the methods them-
selves. Instead, it is necessary to compare between
the score distributions generated by different runs
of each of the models. Unfortunately, this compar-
ison task, which is fundamental to the progress of
the field, has not received a systematic treatment
thus far. Our goal is to close this gap and propose a
simple and effective comparison tool between two
DNNs based on their test set score distributions.
Particularly, we make four contributions:

Defining a DNN comparison framework: We
define three criteria that a DNN comparison tool
should meet: (a) Since we observe only a sam-
ple from the population score distribution of each
model, the decision should be significant under
well justified statistical assumptions. This assures
that future runs of the superior model are likely
to get higher scores than future runs of the infe-
rior model; (b) The decision mechanism should
be powerful, being able to make decisions in most
possible decision tasks; and, finally, (c) Since both
models depend on random decisions, it is likely
that none of them is promised to be superior over
the other in all cases (e.g. with all possible random
seeds). A powerful comparison tool should hence
augment its decision with a confidence score, re-
flecting the probability that the superior model will
indeed produce a better output.

Analysis of existing solutions (§ 3, 5): The
comparison problem we address has been high-
lighted by Reimers and Gurevych (2017b, 2018),
who established its importance in an extensive
experimentation with neural sequence models
(Reimers and Gurevych, 2017a), and proposed
two main solutions (§3). One solution, which we
refer to as the collection of statistics (COS) solu-
tion, is based on the analysis of statistics of the em-
pirical score distribution of the two algorithms –
such as their mean, median and standard deviation
(std), as well as their minimum and maximum val-
ues. Unfortunately, this solution does not respect
criterion (a) as it does not deal with significance,
and as we demonstrate in §5 its power (criterion
(b)) is also limited. Their second solution is based
on significance testing for Stochastic Order (SO)
(Lehmann, 1955), a strict criterion that is hardly
met in reality. While this solution respects crite-
rion (a), it is not designed to deal with criterion

(c), since it does not provide information beyond
its decision if one of the distributions is stochas-
tically dominant over the other or not, and as we
show in §5 its power (criterion (b)) is very limited.

A new comparison tool (§ 4): We propose a
solution that meets our three criteria. Particu-
larly, we adapt to our problem the recently pre-
sented concept of Almost Stochastic Order (ASO)
between two distributions (Álvarez-Esteban et al.,
2017),2 and the statistical significance test for this
property, which makes very modest assumptions
about the participating distributions (criterion (a)).
Further, in line with criterion (c), the test returns
a variable � ∈ [0, 1], that quantifies the degree to
which one algorithm is stochastically larger than
the other, with � = 0 reflecting stochastic order.
We further show that the test is designed to be very
powerful (criterion (b)), which is possible because
the decision on the superior algorithm is comple-
mented by the confidence score.

Extensive experimental analysis (§ 5): We re-
visit the extensive experimental setup of Reimers
and Gurevych (2017a,b), who performed 510
comparisons between strong DNN-based se-
quence tagging models. In each of their experi-
ments they compared two models – either different
models or two variants of the same model differing
in some of their hyper-parameters – and reported
the score distributions of each model across vari-
ous random seeds and hyper-parameter configura-
tions. Our analysis reveals that while our test can
declare one of the algorithms superior in 100% of
the cases, the COS approach can do that in 49.01%
of the cases, and the SO approach in a mere 0.98%.
In addition to being powerful, the decisions and
the confidence scores of our proposed test are also
well aligned with the tests proposed in previous
literature: when the previous methods are chal-
lenged, our method still makes a decision but it
also indicates the smaller gap between the algo-
rithms. We hope that this work will establish a
standard for the comparison between DNNs.

2 Performance Variance in DNNs

In this section we discuss the source of non-
determinism in DNNs, focusing on hyper-
parameter configurations and random choices.

Hyper-parameter Configurations DNNs are
complex models governed by a variety of hyper-

2We use the terms Almost Stochastic Order and Almost
Stochastic Dominance interchangeably in this paper.



2775

parameters. A formal definition of a hyper-
parameter, differentiating it from a standard pa-
rameter, is usually a parameter whose value is
set before the learning process begins. We can
roughly say that hyper-parameters determine the
structure of the model and particular algorithmic
decisions related, e.g., to its optimization. Some
popular structure-related hyper-parameters in the
DNN literature are the number of layers, layer
sizes, activation functions, loss functions, win-
dow sizes, stride values, and parameter initializa-
tion methods. Some optimization (training) re-
lated hyper-parameters are the optimization algo-
rithms, learning rates, number of epochs, momen-
tum, mini-batch sizes, whether or not to use op-
timization heuristics such as gradient clipping and
gradient normalization, and sampling and ordering
methods of the training data.

To decide on the hyper-parameter values, it is
standard to explore several configurations and ob-
serve which performs best on an unseen, held-out
dataset, commonly referred to as the development
set. For some hyper-parameters (e.g. the learning
rate and the optimization algorithm) the range of
feasible values reflects the intuitions of the model
author, and the tuned value provides some insight
about the model and the data. However, for many
other hyper-parameters (e.g. the number of neu-
rons in each layer of the model and the number of
epochs of the optimization algorithm) the range of
values and the selected values are quite arbitrary.
Hence, although hyper-parameters can be tuned
on development data, the distribution of model’s
evaluation scores across these configurations is of
interest, especially when considering the hyper-
parameters with the more arbitrary values.

Random Choices There are also hyper-
parameters that do not follow the above tuning
logic. These include some of the hyper-parameters
that govern the random ordering of the training
examples, the dropout process and the initializa-
tion of the model parameters. The values of these
hyper-parameters are often set randomly.

In other cases, randomization is introduced to
the model without an explicit hyper-parameter.
For example, a popular initialization method for
DNN weights is the Xavier method (Glorot and
Bengio, 2010). In this method, the initial weights
are sampled from a Gaussian distribution with a
mean of 0 and an std of

√
2/ni, where ni is the

number of input units of the i-th layer.

As discussed in §1, being non-convex, the con-
vergent point of DNNs is deeply affected by these
random effects. Unfortunately, exploring all pos-
sible random seeds is impossible both because
they form an uncountable set and because their
values are uninterpretable and it is hence even hard
to decide on the relevant search space for their val-
ues. This dictates the need for reporting model re-
sults with multiple random choices.

3 Comparing DNNs: Problem
Formulation and Background

Problem Definition Given two algorithms, each
associated with a set of test-set evaluation scores,
our goal is to determine which algorithm, if any,
is superior. In this research, the score distributions
are generated when running two different DNNs
with various hyper-parameter configurations and
random seeds. For both DNNs, the performance is
measured using the same evaluation measure over
the same dataset,3 but, to be as general as possible,
the number of scores may vary between the DNNs.

As noted in §1, several methods were proposed
for the comparison between the score distributions
of two DNNs. We now discuss these methods.

3.1 Collection of Statistics (COS)

This approach is based on the analysis of statistics
of the empirical score distributions. For example,
Reimers and Gurevych (2018) averaged the test-
set scores and applied the Welch’s t-test (Welch,
1947) for comparing between the means. Notice
that the Welch’s t-test is based on the assumption
that the test-set scores are drawn from normal dis-
tributions – an assumption that has not been val-
idated for DNN score distributions. Hence, this
method does not meet criterion (a) from §1, that
requires the comparison method to check for sta-
tistical significance under realistic assumptions.

Moreover, comparing only the mean of two dis-
tributions is not always sufficient for making pre-
dictions about future comparisons between the al-
gorithms. Other statistics such as the std, median
and the minimum and maximum values are of-
ten also relevant. For example, it might be that
the expected value of algorithm A is indeed larger
than that of algorithm B, but A’s std is also much
larger, making prediction very challenging. In §5
we show that if both larger mean and smaller std

3Without loss of generality we will assume that higher
values of the measure are better.



2776

is required for a decision, the COS approach is
decisive (i.e. it can declare that one algorithm
is better than the other) in only 49.01% of the
510 setups considered in Reimers and Gurevych
(2017b). This violates our criterion (b) which re-
quires the comparison test to be powerful.

3.2 Stochastic Order (SO)
Another approach, proposed by Reimers and
Gurevych (2018), tests whether a score drawn
from the distribution of algorithm A (denoted as
XA) is likely, with a probability higher than 0.5, to
be larger than a score drawn from the distribution
of algorithm B (XB). Put it formally, algorithm A
is declared superior to algorithm B if:

P (XA ≥ XB) > 0.5. (1)

To test if this requirement holds based on the
empirical score distributions of the two algo-
rithms, the authors applied the Mann-Whitney U
test for independent pairs (Mann and Whitney,
1947) – which tests whether there exists a stochas-
tic order (SO) between two random variables. This
test is non-parametric, making no assumptions
about the participating distributions except for be-
ing continuous. In the appendix we show that if
there is an SO between two distributions, the con-
dition in Equation 1 also holds.

We next describe the concept of SO in more de-
tails. But first, in order to keep our paper self-
contained, we define the cumulative distribution
function (CDF) and the empirical CDF of a prob-
ability distribution.

The CDF For a random variable X , the CDF is
defined as follows:

F (t) = P (X ≤ t).

For a sample {x1, .., xn}, the empirical CDF is de-
fined as follows:

Fn(t) =
1

n

n∑
i=1

1xi≤t =
number of xis ≤ t

n
,

where 1xi≤t is an indicator function that takes the
value of 1 if xi ≤ t, and 0 otherwise. These defini-
tions are required for the definition of SO we make
next.

Stochastic Order (SO) Lehmann (1955) de-
fines a random variable X to be stochastically
larger than a random variable Y (denoted by X �

Y ) if F (a) ≤ G(a) for all a (with a strict inequal-
ity for some values of a), where F and G are the
CDFs of X and Y , respectively. That is, if we ob-
serve a random value sampled from the first distri-
bution, it is likely to be larger than a random value
sampled from the second distribution.

If it can be concluded from the empirical score
distributions of two DNNs that SO exists be-
tween their respective population distributions,
this means that one algorithm is more likely to pro-
duce higher quality solutions than the other, and
this algorithm can be declared superior. As dis-
cussed above, Reimers and Gurevych (2018) ap-
plied the Mann-Whitney U-test to test for this re-
lationship. The U-test has high statistical power
when the tested distributions are moderate-tailed,
e.g., the normal distribution or the logistic dis-
tribution. When the distribution is heavy tailed,
e.g., the Cauchy distribution, there are several al-
ternative statistical tests that have higher statistical
power, for example likelihood based tests (Lee and
Wolfe, 1976; El Barmi and McKeague, 2013).

The main limitation of this approach is that SO
can rarely be proved to hold based on two empir-
ical distributions. Indeed, in §5 we show that an
SO holds between the two compared algorithms
only in 0.98% of the comparisons performed by
Reimers and Gurevych (2017a). Hence, while this
approach meets our criterion (a) (testing for sig-
nificance under realistic assumptions), it does not
meet criterion (b) (being a powerful test) and cri-
terion (c) (providing a confidence score).

We will next describe another approach that
does meet our three criteria.

4 Our Approach: Almost Stochastic
Dominance

Our starting point is that the requirement of SO
is unrealistic because it means that the inequality
F (a) ≤ G(a) should hold for every value of a. It
is likely that this criterion should fail to determine
dominance between two distributions even when
a ”reasonable” decision-maker would clearly pre-
fer one DNN over the other. We hence propose to
employ a relaxed version of this criterion. We next
discuss different definitions of such relaxation.

A Potential Relaxation For � > 0 and random
variables X and Y with CDFs F and G, respec-
tively, we can define the following notion of �-
stochastic dominance:



2777

X �� Y if F (a) ≤ G(a) + � for all a.

That is, we allow the distributions to violate the
stochastic order, and hence one CDF does not have
to be strictly below the other for all a.

The practical shortcomings of this definition are
apparent in cases where F (a) is greater than G(a)
for all a, with a gap bounded by, for example, �/2.
In such cases we would not want to determine that
X ∼ F is � stochastically dominant over Y ∼
G because its CDF is strictly above the CDF of
Y , and hence Y is stochastically larger than X .
However, according to this relaxation, X ∼ F is
indeed � stochastically larger than Y ∼ G.

Almost Stochastic Dominance To overcome
the limitations of the above straight forward ap-
proach, and define a relaxation of stochastic order,
we turn to a definition that is based on the propor-
tion of points in the domain of the participating
distributions for which SO holds. That is, the test
we will introduce below is based on the following
two violation sets:

VX = {a : F (a) > G(a)}.

VY = {a : F (a) < G(a)}.

Intuitively, the variable with the smaller violation
set should be declared superior and the ratio be-
tween these sets should define the gap between the
distributions.

To implement this idea, del Barrio et al. (2018)
defined the concept of almost stochastic domi-
nance. Here we describe their work, that aims to
compare two distributions, and discuss its appli-
cability to our problem of comparing two DNN
models based on the three criteria defined in §1.
We start with a definition: for a CDF F , the quan-
tile function associated with F is defined as:

F−1(t) = inf{x : t ≤ F (x)}, t ∈ (0, 1). (2)

It is possible to define stochastic order using the
quantile function in the following manner:

X � Y ⇐⇒ F−1(t) ≥ G−1(t),∀t ∈ (0, 1). (3)

The advantage of this definition is that the domain
of the quantile function is bounded between 0 and
1. This is in contrast to the CDF whose domain is
unbounded.

From this definition, it is clear that a violation
of the stochastic order between X and Y occurs
when F−1(t) < G−1(t). Hence, it is easy to re-
define VX and VY based on the quantile functions:

AX = {t ∈ (0, 1) : F−1(t) < G−1(t)}.

AY = {t ∈ (0, 1) : F−1(t) > G−1(t)}.

del Barrio et al. (2018) employed these defini-
tions in order to define the distance of each random
variable from stochastic dominance over the other:

εW2(F,G) :=

∫
AX

(F−1(t)−G−1(t))2dt(
W2(F,G)

)2 . (4)
Where W2(F,G), also known as the univariate

L2-Wasserstein distance between distributions, is
defined as:

W2(F,G) =

√∫ 1
0

(F−1(t)−G−1(t))2dt. (5)

This ratio explicitly measures the distance of X
(with CDF F ) from stochastic dominance over Y
(with CDFG) since it reflects the probability mass
for which Y dominatesX . The corresponding def-
inition for the distance of Y from being stochasti-
cally dominant over X can be received from the
above equations by replacing the roles of F and G
and integrating over AY instead of AX .

This index satisfies 0 ≤ εW2(F,G) ≤ 1 where
0 corresponds to perfect stochastic dominance of
X over Y and 1 corresponds to perfect stochas-
tic dominance of Y over X . It also holds that
εW2(F,G) = 1 − εW2(G,F ), and smaller val-
ues of the smaller index (which is by definition
bounded between 0 and 0.5) indicate a smaller dis-
tance from stochastic dominance.

Statistical Significance Testing for ASO Using
this index it is possible to formulate the follow-
ing hypothesis testing problem to test for almost
stochastic dominance:

H0 : εW2(F,G) ≥ �
H1 : εW2(F,G) < �

which tests, for a predefined � > 0, if the violation
index is smaller than �. Rejecting the null hypoth-
esis means that the first score distribution F is al-
most stochastically larger than G with � distance
from stochastic order.

del Barrio et al. (2018) proved that without fur-
ther assumptions, H0 will be rejected with a sig-
nificance level of α if:√

nm

n+m

(
εW2(Fn, Gm)− �

)
< σ̂n,mΦ

−1(α),



2778

where Fn, Gm are the empirical CDFs with n and
m samples, respectively, � is the violation level,
Φ−1 is the inverse CDF of a normal distribution
and σ̂n,m is the estimated variance of the value√

nm

n+m

(
εW2(F

∗
n , G

∗
m)− εW2(Fn, Gm)

)
,

where εW2(F
∗
n , G

∗
m) is computed using samples

X∗n, Y
∗
m from the empirical distributions Fn and

Gm.4

In addition, the minimal � for which we can
claim, with a confidence level of 1 − α, that F is
almost stochastically dominant over G is
�min(Fn, Gm, α) =

εW2(Fn, Gm)−
√

n+m
nm σ̂n,mΦ

−1(α).

If �min(Fn, Gm, α) < 0.5, we can claim
that algorithm A is better than B, and the lower
�min(Fn, Gm, α) is the greater is the gap between
the algorithms. When �min(Fn, Gm, α) = 0,
algorithm A is stochastically dominant over B.
However, if �min(Fn, Gm, α) ≥ 0.5, then F is
not almost stochastically larger than G (with confi-
dence level 1−α) and hence we should accept the
null hypothesis that algorithm A is not superior to
algorithm B.

del Barrio et al. (2018) proved that, assuming
accurate estimation of σ̂n,m, it holds that:

�min(Fn, Gm, α) = 1− �min(Gm, Fn, α).

Hence, for a given α value, one of the
algorithms will be declared superior, unless
�min(Fn, Gm, α) = �

min(Gm, Fn, α) = 0.5.
Notice that the minimal � and the rejection con-

dition of the null hypothesis depend on n and m,
the number of scores we have for each algorithm.
Hence, for the statistical test to have high statisti-
cal power we need to make sure that n and m are
big enough. While we cannot provide a method
for tuning these numbers, we note that in the ex-
tensive analysis of §5 the test had enough statis-
tical power to make decisions in all cases. The
pseudo code of our implementation is provided in
the appendix.

To summarize, the test for almost stochastic
dominance meets the three criteria defined in §1.
This is a test for statistical significance under
very minimal assumptions on the distribution from

4The more samples, the better. In our implementation we
employ the inverse transform sampling method to generate
samples.

which the performance scores are drawn (criterion
(a)). Moreover, it quantifies the gap between the
two reference distributions (criterion (c)), which
allows it to make decisions even in comparisons
where the gap between the superior algorithm and
the inferior algorithm is not large (criterion (b)).

To demonstrate the appropriateness of this
method for the comparison between two DNNs
we next revisit the extensive experimental setup of
Reimers and Gurevych (2017a).

5 Analysis

Tasks and Models In this section we demon-
strate the potential impact of testing for almost
stochastic dominance on the way empirical results
of NLP models are analyzed. We use the data
of Reimers and Gurevych (2017a)5 and Reimers
and Gurevych (2017b).6 This data contains 510
comparison setups for five common NLP sequence
tagging tasks: Part Of Speech (POS) tagging with
the WSJ corpus (Marcus et al., 1993), syntactic
chucking with the CoNLL 2000 data (Sang and
Buchholz, 2000), Named Entity Recognition with
the CoNLL 2003 data (Sang and De Meulder,
2003), Entity Recognition with the ACE2005 data
(Walker et al., 2006), and event detection with the
TempEval3 data (UzZaman et al., 2013). In each
setup two leading DNNs, either different architec-
tures or variants of the same model but with differ-
ent hyper-parameter configurations, are compared
across various choices of random seeds and hyper-
parameter configurations. The exact details of the
comparisons are beyond the scope of this paper;
they are documented in the above papers.

For each experimental setup, we report the
outcome of three alternative comparison meth-
ods: collection of statistics (COS), stochastic or-
der (SO), and almost stochastic order (ASO). For
COS, we report the mean, std, and median of the
scores for each algorithm, as well as their mini-
mum and maximum values. We consider one al-
gorithm to be superior over another only if both
its mean is greater and its std is smaller. For SO,
we employ the U-test as proposed by Reimers and
Gurevych (2018), and consider a result significant
if p ≤ 0.05 . Finally, for ASO we employ the
method of §4 and report the identity of the superior
algorithm along with its � value, using p ≤ 0.01.

5https://github.com/UKPLab/
emnlp2017-bilstm-cnn-crf

6Which was generously given to us by the authors.



2779

Analysis Structure We divide our analysis into
three cases. In Case A both the COS and the SO
approaches indicate that one of the models is supe-
rior. In Case B, the previous methods reach con-
tradicting conclusions: while COS indicates that
one of the algorithms is superior, SO comes in-
significant. Finally, in Case C both COS and SO
are indecisive. In the 510 comparisons we ana-
lyze there is no setup where SO was significant
but COS could not reach a decision. We start with
an example setup for each case and then provide a
summary of all 510 comparisons.

Results: Case A We demonstrate that if algo-
rithm A is stochastically larger than algorithm B
then all three methods agree that algorithm A is
better than B. As an example setup we analyze
the comparison between the NER models of Lam-
ple et al. (2016) and Ma and Hovy (2016) when
running both algorithms multiple times, changing
only the random seed fed into the random number
generator (41 scores from (Lample et al., 2016),
87 scores from (Ma and Hovy, 2016)). The evalu-
ation measure is F1 score. The collection of statis-
tics for the two models is presented in Table 1.

Lample et al. Ma&Hovy
Mean 0.9075 0.9056
STD 0.2237 0.3211
Median 0.9080 0.9063
Min 0.9018 0.8853
Max 0.9113 0.9100

Table 1: NER results. (Case A).

The U-test states that (Lample et al., 2016) is
stochastically larger than (Ma and Hovy, 2016)
with a p-value of 0.00025. This result is also con-
sistent with the prediction of the COS approach
as (Lample et al., 2016) is better than (Ma and
Hovy, 2016) both in terms of mean (larger) and
std (smaller). Finally, the minimum � value of the
ASO method is 0, which also reflects an SO.

Results: Case B We demonstrate that if the
measures of mean and std from the COS approach
indicate that algorithm A is better than algorithm
B but stochastic dominance does not hold, then
it also holds that A is almost stochastically larger
than B with a small � > 0. As an example case we
consider the experiment where the performance of
a BiLSTM POS tagger with one of two optimizers,
Adam (Kingma and Ba, 2014) (3898 scores) or

RMSProp (Hinton et al., 2012) (1822 scores), are
compared across various hyper-parameter config-
urations and random seeds. The evaluation mea-
sure is word level accuracy. The COS for the two
models is presented in Table 2.

Adam RMSprop
Average 0.9224 0.9190

STD 0.0604 0.0920
Median 0.9319 0.9349

Min 0.1746 0.1420
Max 0.9556 0.9573

Table 2: POS tagging results (Case B).

The result of the U-test came insignificant with
p-value of 0.4562. The COS approach predicts
that Adam is the better optimizer as both its mean
is larger and its std is smaller. When comparing
between Adam and RMSProrp, the ASO method
returns an � of 0.0159, indicating that the former
is almost stochastically larger than the latter.

We note that decisions with the COS method are
challenging as it potentially involves a large num-
ber of statistics (five in this analysis). Our decision
here is to make the COS prediction based on the
mean and std of the score distribution, even when
according to other statistics the conclusion might
have been different. We consider this ambiguity
an inherent limitation of the COS method.

Results: Case C Finally, we address the case
where stochastic dominance does not hold and no
conclusions can be drawn from the statistics col-
lection. Our observation is that even in these cases
ASO is able to determine which algorithm is bet-
ter with a reasonable level of confidence. We
consider again a BiLSTM architecture, this time
for NER, where the comparison is between two
dropout policies – no dropout (225 scores) and
variational dropout (2599 scores). The evaluation
measure is the F1 score and the collection of statis-
tics is presented in Table 3.

Variational No Dropout
Mean 0.8850 0.8772
STD 0.0392 0.0247

Median 0.8896 0.8799
Min 0.0119 0.5547
Max 0.9098 0.8995

Table 3: NER Results (Case C).



2780

(a) Case A (b) Case B (c) Case C

Figure 1: An histogram of � values of the ASO method for cases A, B and C.

The U-test came insignificant with a p-value of
0.5. COS is also inconclusive as the mean result
of the variational dropout approach is larger, but so
also its std. In this case, looking at the other statis-
tics also gives a mixed picture as the median and
max values of the variational approach are larger,
but its min value is substantially smaller.

The ASO approach indicates that the no dropout
approach is almost stochastically larger, with � =
0.0279. An in-depth consideration supports this
decision as the much larger std and the much
smaller minimum of the variational approach are
indicators of a skewed score distribution that
leaves low certainty about future performance.

Results: Summary We now turn to a summary
of our analysis across the 510 comparisons of
Reimers and Gurevych (2017a). Table 4 presents
the percentage of comparisons that fall into each
category, along with the average and std of the �
value of ASO for each case (all ASO results are
significant with p ≤ 0.01). Figure 1 presents the
histogram of these � values in each case.

% of comparisons Avg. � � std
Case A 0.98% 0.0 0.0
Case B 48.04% 0.072 0.108
Case C 50.98% 0.202 0.143

Table 4: Results summary over the 510 comparisons
of Reimers and Gurevych (2017a).

The number of comparisons that fall into case A
is only 0.98%, indicating that it is rare that a deci-
sion about stochastic dominance of one algorithm
can be reached when comparing DNNs. We con-
sider this a strong indication that the Mann Whit-
ney U test is not suitable for DNN comparison as
it has very little statistical power (criterion (b)).

COS makes a decision in 49.01% of the com-

parisons (case A and B). This method is also
somewhat powerful (criterion (b)), but much less
so than ASO that is decisive in all 510 compar-
isons. The � values of ASO are higher for case B
than for case A (middle line of the table, middle
graph of the figure). For case C the � distribution
is qualitatively different – � receives a range of val-
ues (rightmost graph of the figure) and its average
is 0.202 (bottom line of the table). We consider
this to be a desired behavior as the more complex
the picture drawn by COS and SO is, the less con-
fident we expect ASO to be. Being able to make a
decision in all 510 comparisons while quantifying
the gap between the distributions, we believe that
ASO is an appropriate tool for DNN comparison.

6 Error Rate Analysis

While our extensive analysis indicates the quality
of the ASO test, it does not allow us to estimate
its false positive and false negative rates. This is
because in our 510 comparisons there is no oracle
(or gold standard) that says if one of the algorithms
is superior. Below we provide such analyses.

False Positive Rate The ASO test is defined
such that the ε value required for rejecting the con-
clusion that algorithm A is better than B is defined
by the practitioner. While ε = 0.5 indicates a clear
rejection, most researchers would probably set a
lower ε threshold. Our goal in the next analysis
is to present a case where the false positive rate
of ASO is very low, even when one refrains from
declaring one algorithm as better than the other
only when ε is very close to 0.5.

To do that, we consider a scenario where each
of the 255 score distributions of the experiments
in § 5 is compared to a variant of the same dis-
tribution after a Gaussian noise with a 0 expecta-
tion and a standard deviation of 0.001 is added to



2781

(a) False Positive Rate Experiment (b) False Negative Rate Experiment

Figure 2: Histograms of the � values of the ASO test in the ablation experiments.

each of the scores. Since in all the tasks we con-
sider the scores are in the [0, 1] range, the value
of 0.001 is equivalent to 0.1%. Since the average
of the standard deviations of these 255 score dis-
tributions is 0.06, our noise is small but not negli-
gible. We choose this relatively small symmetric
noise so that with a high probability the original
score distribution and the modified one should not
be considered different. We run 100 comparisons
for each of the 255 algorithms.

We compute the ε such that a value of 0 means
that the non-noisy version is better than the noisy
one with the strongest confidence, while the value
of 1 means the exact opposite (both values are not
observed in practice). A value of 0.5 indicates that
no algorithm is superior – the correct prediction.

Figure 2 (a) presents a histogram of the ε val-
ues. The averaged ε is 0.502 with a standard de-
viation of 0.0472, and 95% of the ε values are in
[0.396, 0.631]. This means that if we set a thresh-
old of 0.4 on ε (i.e. lower than 0.4 or higher than
0.6), the false positive rate would be lower than
5%. In comparison, the COS approach declares
the noisy version superior in 26.2% of the 255
comparisons, and the non-noisy version in 23.8%:
a false positive rate of 50%.7 The SO test makes
no mistakes, as a false positive of this test is equiv-
alent to an ε value of 0 or 1 for ASO.

Finally, we also considered a setup where for
each of the 255 algorithms the performance score
set was randomly split into two equal sized sets.
We repeated this process 100 times for each algo-
rithm, using ASO to compare between the sets. In
all cases we observed an averaged ε of 0.5, indi-
cating that the method avoids false positive predic-
tions when an algorithm is compared to itself.

7Recall that we consider one algorithm superior over the
other according to COS when both the mean of its scores is
larger than the mean of the other, and its std is smaller.

False Negative Rate This analysis complements
the previous one by demonstrating the low false
negative rate of ASO in a case where it is clear that
one distribution is better than the other. For each
of the 255 score distributions we generate a noisy
distribution by randomly splitting the scores into
a set A of 14 of the scores and the complementary
set Â of the rest of the scores. For each score s we
sample a noise parameter φ from a Gaussian with
a 0 expectation and an std of 0.01, adding to s the
value of (−1) · φ2 if s ∈ A, and φ2 if s ∈ Â. The
noisy distribution is superior to the original one,
with a high probability. As before we perform 100
comparisons for each of the 255 algorithms.

We compute ε such that a value of 0 would
mean that the noisy version is superior. The ε val-
ues are plotted in Figure 2 (b): their average is
0.134, standard deviation is 0.07 and more than
99% of the values are lower than 0.4 (the same
threshold as in the first experiment). The COS test
deems the noisy distribution superior in 87.4% of
the cases, while in the rest it considers none of the
distributions superior. SO has a false negative rate
of 100% as ε > 0 in all experiments.

7 Conclusions

We considered the comparison of two DNNs
based on their test-set score distribution. We de-
fined three criteria for a high quality comparison
method, demonstrated that previous methods do
not meet these criteria and proposed to use the re-
cently proposed test for almost stochastic domi-
nance that does meet these criteria. We analyzed
the extensive experimental setup of Reimers and
Gurevych (2017a) and demonstrated the effective-
ness of our proposed test. Having released our
code, we hope this will become a new evaluation
standard in the NLP community.



2782

References
PC Álvarez-Esteban, Eustasio del Barrio, Juan Antonio

Cuesta-Albertos, C Matrán, et al. 2017. Models for
the assessment of treatment improvement: the ideal
and the feasible. Statistical Science, 32(3):469–485.

Eustasio del Barrio, Juan A Cuesta-Albertos, and Car-
los Matrán. 2018. An optimal transportation ap-
proach for assessing almost stochastic order. In
The Mathematics of the Uncertain, pages 33–44.
Springer.

Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre,
Kyunghyun Cho, Surya Ganguli, and Yoshua Ben-
gio. 2014. Identifying and attacking the saddle point
problem in high-dimensional non-convex optimiza-
tion. In Advances in neural information processing
systems, pages 2933–2941.

Timothy Dozat and Christopher D Manning. 2017.
Deep biaffine attention for neural dependency pars-
ing. In Proc. of ICLR.

Rotem Dror, Gili Baumer, Marina Bogomolov, and Roi
Reichart. 2017. Replicability analysis for natural
language processing: Testing significance with mul-
tiple datasets. Transactions of the Association for
Computational Linguistics, 5:471–486.

Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Re-
ichart. 2018. The hitchhikers guide to testing statis-
tical significance in natural language processing. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 1383–1392.

Hammou El Barmi and Ian W McKeague. 2013. Em-
pirical likelihood-based tests for stochastic order-
ing. Bernoulli: official journal of the Bernoulli
Society for Mathematical Statistics and Probability,
19(1):295.

Jenny Rose Finkel, Alex Kleeman, and Christopher D
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. Proceedings of ACL-
08: HLT, pages 959–967.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neu-
ral networks. In Proceedings of the thirteenth in-
ternational conference on artificial intelligence and
statistics, pages 249–256.

Daniel Hershcovich, Omri Abend, and Ari Rappoport.
2017. A transition-based directed acyclic graph
parser for UCCA. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
1127–1138.

Geoffrey Hinton, Nitish Srivastava, and Kevin Swer-
sky. 2012. Neural networks for machine learning
lecture 6a overview of mini-batch gradient descent.
Cited on, page 14.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
arXiv preprint arXiv:1603.01360.

Young Jack Lee and Douglas A Wolfe. 1976.
A distribution-free test for stochastic ordering.
Journal of the American Statistical Association,
71(355):722–727.

Erich Leo Lehmann. 1955. Ordered families of dis-
tributions. The Annals of Mathematical Statistics,
pages 399–419.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end
sequence labeling via bi-directional lstm-cnns-crf.
arXiv preprint arXiv:1603.01354.

Henry B Mann and Donald R Whitney. 1947. On a test
of whether one of two random variables is stochasti-
cally larger than the other. The annals of mathemat-
ical statistics, pages 50–60.

Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank.

Nils Reimers and Iryna Gurevych. 2017a. Optimal hy-
perparameters for deep lstm-networks for sequence
labeling tasks. arXiv preprint arXiv:1707.06799.

Nils Reimers and Iryna Gurevych. 2017b. Report-
ing score distributions makes a difference: Perfor-
mance study of lstm-networks for sequence tagging.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
338–348.

Nils Reimers and Iryna Gurevych. 2018. Why com-
paring single performance scores does not allow
to draw conclusions about machine learning ap-
proaches. arXiv preprint arXiv:1803.09578.

Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the conference on empiri-
cal methods in natural language processing, pages
1524–1534. Association for Computational Linguis-
tics.

Erik F Sang and Sabine Buchholz. 2000. Introduc-
tion to the conll-2000 shared task: Chunking. arXiv
preprint cs/0009008.

Erik F Sang and Fien De Meulder. 2003. Intro-
duction to the conll-2003 shared task: Language-
independent named entity recognition. arXiv
preprint cs/0306050.



2783

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673–2681.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 conference of the North
American chapter of the association for computa-
tional linguistics on human language technology-
volume 1, pages 173–180. Association for Compu-
tational Linguistics.

Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, James Allen, Marc Verhagen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-3:
Evaluating time expressions, events, and temporal
relations. In Second Joint Conference on Lexical
and Computational Semantics (* SEM), Volume 2:
Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), volume 2,
pages 1–9.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. Ace 2005 multilin-
gual training corpus. Linguistic Data Consortium,
Philadelphia, 57.

Bernard L Welch. 1947. The generalization ofstu-
dent’s’ problem when several different population
variances are involved. Biometrika, 34(1/2):28–35.

Vikas Yadav and Steven Bethard. 2018. A survey on
recent advances in named entity recognition from
deep learning models. In Proceedings of the 27th In-
ternational Conference on Computational Linguis-
tics, pages 2145–2158.



2784

A Proof - Equivalent Definitions of
Stochastic Order

As discussed in §3, our goal here is to prove that if
a random variable X is stochastically larger than
a random variable Y (denoted by X � Y ), then
it also holds that P (X ≥ Y ) > 0.5. This lemma
explains why Reimers and Gurevych (2018) em-
ployed the Mann-Whitney U test that tests for
stochastic order, while their requirement for stat-
ing that one algorithm is better than the other was
that P (X ≥ Y ) > 0.5 (where X is the score
distribution of the superior algorithm and Y is the
score distribution of the inferior algorithm).

Lemma 1. If X � Y then P (X ≥ Y ) > 0.5.

Proof. For every two continuous random variables
X,Y it holds that:

P (X ≥ Y ) + P (Y > X) = 1.

Let us first assume that X and Y are i.i.d and
continuous. If this is the case then:

P (X ≥ Y ) + P (Y > X) = 1
P (X ≥ Y ) + P (X > Y ) = 1

2P (X ≥ Y ) = 1
P (X ≥ Y ) = 0.5.

The first pass is true because X and Y are iden-
tically distributed and the second pass is true be-
cause X and Y are continuous random variables.

Assuming that the density functions of the ran-
dom variables X and Y exist (which is true be-
cause they are continues variables), we can write
P (X ≥ Y ) in the following manner:

P (X ≥ Y ) =
∫ ∞
y=−∞

∫ ∞
x=y

fX(x) · fY (y)dxdy

=

∫ ∞
y=−∞

fY (y) · P (X ≥ y)dy

=

∫ ∞
y=−∞

fY (y) · P (Y ≥ y)dy = 0.5.

Where the equality to 0.5 was proved above.
In our case, X � Y . This means that X and Y

are independent but are not identically distributed.
By definition of stochastic order this also means
that P (X ≥ a) > P (Y ≥ a), for all a with strict

inequality for at least one value of a. We get that:

P (X ≥ Y ) =
∫ ∞
y=−∞

∫ ∞
x=y

fX(x) · fY (y)dxdy

=

∫ ∞
y=−∞

fY (y) · P (X ≥ y)dy

>

∫ ∞
y=−∞

fY (y) · P (Y ≥ y)dy = 0.5.

Where the last pass holds because X is stochasti-
cally larger than Y . We get that P (X ≥ Y ) >
0.5.

Note that the opposite direction does not always
hold, i.e., it is easy to come up with an example
where P (X ≥ Y ) > 0.5 but there is no stochastic
order between the two random variables. How-
ever, the opposite direction is true with an addi-
tional assumption that the CDFs do not cross one
another (which we do not prove here).

B Hypothesis Testing for Almost
Stochastic Dominance

In this section we discuss the implementation of
the algorithm for hypothesis testing for the almost
stochastic dominance relation between two ran-
dom variables (empirical score distributions). The
code of the algorithm is publicly available.

We are given two sets of scores from two algo-
rithms, n scores from algorithm A and m scores
from algorithm B: A = {x1, x2, ..., xn}, B =
{y1, y2, ..., ym}. The pseudocode of the algorithm
is as follows:

1. Sort the data points from the smallest to
the largest in both sets, creating two lists:
A = [x(1), ..., x(n)] and B = [y(1), ..., y(m)],
where x(i) is the i-th smallest value.

2. Build the empirical score distributions
Fn, Gm using the following formula:

Fn(t) =
1

n

n∑
i=1

1x(i) ≤ t =
number of xis ≤ t

n

3. Build the empirical inverse score distribu-
tions F−1(t), G−1(t) using the following for-
mula:8

F−1(t) = inf{x : t ≤ F (x)}, t ∈ (0, 1)
8It is possible to compute the inverse CDF without explic-

itly computing the CDF.



2785

4. Compute the index of stochastic dominance
violation εW2(F,G) (equation 4 of the main
paper). In practice we compute the integral
operation using the definition of the Riemann
integral. That is, when computing

∫ 1
0 f(t)dt

we partition the interval between 0 and 1 into
small parts of size ∆ and compute the sum of
the function value in this part times ∆).

5. Estimate σ: take many samples X∗n,Y
∗
m from

the empirical distributions Fn and Gm; for
each of those samples compute the expres-
sion:√

nm

n+m

(
εW2(F

∗
n , G

∗
m)− εW2(Fn, Gm)

)
and use the variance of those values as the
estimate for σ2, take the square root of that
estimator for σ̂n,m. The more samples, the
better. In our implementation we employ the
inverse transform sampling method to gener-
ate samples.

6. The minimal � for which we can claim that al-
gorithm A is almost stochastically larger than
algorithm B with confidence level of 1−α is:
�min(Fn, Gm, α) =

εW2(Fn, Gm)−
√

n+m
nm σ̂n,mΦ

−1(α).


