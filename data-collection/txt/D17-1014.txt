



















































Towards Decoding as Continuous Optimisation in Neural Machine Translation


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 146–156
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Towards Decoding as Continuous Optimisation in
Neural Machine Translation

Cong Duy Vu Hoang† and Gholamreza Haffari‡ and Trevor Cohn†

† University of Melbourne
Melbourne, VIC, Australia

‡Monash University
Clayton, VIC, Australia

vhoang2@student.unimelb.edu.au, gholamreza.haffari@monash.edu,
t.cohn@unimelb.edu.au

Abstract

We propose a novel decoding approach for
neural machine translation (NMT) based
on continuous optimisation. We refor-
mulate decoding, a discrete optimization
problem, into a continuous problem, such
that optimization can make use of efficient
gradient-based techniques. Our powerful
decoding framework allows for more ac-
curate decoding for standard neural ma-
chine translation models, as well as en-
abling decoding in intractable models such
as intersection of several different NMT
models. Our empirical results show that
our decoding framework is effective, and
can leads to substantial improvements in
translations, especially in situations where
greedy search and beam search are not fea-
sible. Finally, we show how the technique
is highly competitive with, and comple-
mentary to, reranking.

1 Introduction

Sequence to sequence learning with neural net-
works (Graves, 2013; Sutskever et al., 2014; Lip-
ton et al., 2015) is typically associated with two
phases: training and decoding (a.k.a. inference).
Model parameters are learned by optimising the
training objective, in order that the model can
produce good translations when decoding unseen
sentences. The majority of research has focused
on the training paradigm or network architec-
ture, however effective means of decoding have
been under-investigated. Conventional heuristic-
based approaches for approximate inference in-
clude greedy, beam, and stochastic search. Greedy
and beam search have been empirically proved to
be adequate for many sequence to sequence tasks,
and are the standard methods for NMT decoding.

However, these inference approaches have sev-
eral drawbacks. Firstly, although NMT models use
a left-to-right generation which would appear to
facilitate efficient search, the models themselves
use a recurrent architecture, and accordingly are
non-Markov. This prevents exact dynamic pro-
gramming solutions, and moreover, limits the po-
tential to incorporate additional global features or
constraints. Global factors can be highly useful
in producing better and more diverse translations.
Secondly, the sequential decoding of symbols in
the target sequence, the inter-dependencies among
the target symbols are not fully exploited. For
example, when decoding the words of the target
sentence in a left-to-right manner, the right con-
text is not exploited leading potentially to inferior
performance (see Watanabe and Sumita (2002a)
who apply this idea in traditional statistical MT).
A natural way to capture this is to intersect left-
to-right and right-to-left models, however the re-
sulting model has no natural generation order, and
thus standard decoding methods are unsuitable.

We introduce a novel decoding framework (§ 3)
that relaxes this discrete optimisation problem into
a continuous optimisation problem. This is akin
to linear programming relaxation approach for ap-
proximate inference in graphical models with dis-
crete random variables, where the exact inference
is NP-hard (Sontag, 2010; Belanger and McCal-
lum, 2016). The resulting continuous optimisation
problem is challenging due to the non-linearity
and non-convexity of the relaxed decoding ob-
jective. We make use of stochastic gradient de-
scent (SGD) and exponentiated gradient (EG) al-
gorithms for decoding based on our relaxation ap-
proach.1 Our decoding framework is powerful and
flexible, as it enables us to decode with global con-
straints involving intersection of multiple NMT

1Both methods are mainly used for training in prior work.

146



models (§4). We present experimental results on
Chinese-English and German-English translation
tasks, confirming the effectiveness of our relaxed
optimisation method for decoding (§5).

2 Neural Machine Translation

We briefly review the attentional neural transla-
tion model proposed by Bahdanau et al. (2015) as
a sequence-to-sequence neural model onto which
we apply our decoding framework.

In neural machine translation (NMT), the prob-
ability of the target sentence y given a source sen-
tence x is written as:

PΘ (y|x) =
|y|∑
i=1

log PΘ (yi|y<i,x) (1)

yi|y<i,x ∼ softmax (fff(Θ,y<i,x))

where f is a non-linear function of the previously
generated sequence of words y<i, the source sen-
tence x, and the model parameters Θ. In this pa-
per, we realise fff as follows:

fff(Θ,y<i,x) = Wo ·MLP
(
ci,EEE

yi−1
T , gi

)
+ bo

gi = RNN
φ
dec

(
ci,EEE

yi−1
T , gi−1

)
where MLP is a single hidden layer neural net-
work with tanh activation function, and EEEyi−1T is
the embedding of the target word yi−1 in the em-
bedding matrix EEET ∈ Rne×|VT | of the target lan-
guage vocabulary VT and ne is the embedding di-
mension. The state gi of the decoder RNN is a
function of yi−1, its previous state gi−1, and the
context ci =

∑|x|
j=1 αijhj summarises parts of the

source sentence which are attended to, where

αi = softmax(ei) ; eij = MLP (gi−1,hj)

hj = biRNNθenc
(
EEE
xj
S ,
−→
h j−1,

←−
h j+1

)
In above,

−→
h i and

←−
h i are the states of the left-to-

right and right-to-left RNNs encoding the source
sentence, and EEExjS is the embedding of the source
word xj in the embedding matrix EEES ∈ Rn′e×|VS |
of the source language vocabulary VS and n′e is the
embedding dimension.

Given a bilingual corpus D, the model param-
eters are learned by maximizing the conditional
log-likelihood,

Θ∗ := argmaxΘ
∑

(x,y)∈D
log PΘ (y | x) . (2)

The model parameters Θ include the weight ma-
trix Wo ∈ R|VT |×nh and the bias bo ∈ R|VT |
– with nH denoting the hidden dimension size
– as well as the RNN encoder biRNNθenc / de-
coder RNNφdec parameters, word embedding ma-
trices, and the parameters of the attention mecha-
nism. The model is trained end-to-end by optimis-
ing the training objective using stochastic gradient
descent (SGD) or its variants. In this paper, we fo-
cus on the decoding problem, which we turn to in
the next section.

3 Decoding as Continuous Optimisation

In decoding, we are interested in finding the high-
est probability translation for a given source sen-
tence:

minimisey − PΘ (y | x) s.t. y ∈ Yx (3)
where Yx is the space of possible translations
for the source sentence x. In general, search-
ing Yx to find the highest probability translation
is intractable due to the recurrent nature of eqn
(1) which prevents dynamic programming for ef-
ficient search. This is problematic, as the space of
translations is exponentially large with respect to
the output length |y|.

We now formulate this discrete optimisation
problem as a continuous one, and then use stan-
dard algorithms for continuous optimisation for
decoding. Let us assume that the maximum length
of a possible translation for a source sentence is
known and denote it as `. The best translation for
a given source sentence solves the following opti-
misation problem:

y∗ = arg min
y1,...,y`

∑̀
i=1

− log PΘ (yi | y<i,x) (4)

s.t. ∀i ∈ {1 . . . `} : yi ∈ VT .
where we allow the translation to be padded with
sentinel symbols to the right, which are ignored
in computing the model probability. Equivalently,
we can rewrite the above discrete optimisation
problem as follows:

arg min
ỹ1,...,ỹ`

−
∑̀
i=1

ỹi · log softmax (fff (Θ, ỹ<i,x))

s.t. ∀i ∈ {1 . . . `} : ỹi ∈ I|VT | (5)
where ỹi are vectors using the one-hot representa-
tion of the target words I|VT |.

147



Algorithm 1 The EG Algorithm for Decoding by Optimisation
1: For all i initialise ŷ0i ∈ ∆|VT |
2: for t = 1, . . . ,MaxIter do . Q(.) is defined as eqn (6)
3: For all i, w : calculate∇t−1i,w =

∂Q(ŷt−11 ,...,ŷ
t−1
` )

∂ŷi(w)
. using back-propagation

4: For all i, w : update ŷti(w) ∝ ŷt−1i (w) · exp
(
−η∇t−1i,w

)
. η is the step size

5: return arg mintQ(ŷt1, . . . , ŷt`)

We now convert the optimisation problem (5) to
a continuous one by dropping the integrality con-
straints ỹi ∈ I|V | and require the variables to take
values from the probability simplex:

arg min
ŷ1,...,ŷ`

−
∑̀
i=1

ŷi · log softmax (fff (Θ, ŷ<i,x))

s.t. ∀i ∈ {1 . . . `} : ŷi ∈ ∆|VT |

where ∆|VT | is the |VT |-dimensional probability
simplex, i.e., {ŷi ∈ [0, 1]|VT | : ‖ŷi‖1 = 1}. Intu-
itively, this amounts to replacing EEEyiT with the
expected embedding of target language words
Eŷi(w)[EEE

w
T ] under the distribution ŷi.

After solving the above constrained continuous
optimisation problem, there is no guarantee that
the resulting solution {ŷ∗i }`i=1 will comprise one-
hot vectors, i.,e., target language words. Instead it
can find fractional solutions, that require ‘round-
ing’ in order to to resolve them to lexical items.
To solve this problem, we take the arg max,2 i.e.,
take the highest scoring word for each position ŷ∗i .
We leave exploration of more elaborate projection
techniques to the future work.

In the context of graphical models, the above
relaxation technique gives rise to linear program-
ming for approximate inference (Sontag, 2010;
Belanger and McCallum, 2016). However, our de-
coding problem is much harder due to the non-
linearity and non-convexity of the objective func-
tion operating on high dimensional space for deep
models. We now turn our attention to optimisation
algorithms to effectively solve the decoding opti-
misation problem.

3.1 Exponentiated Gradient (EG)

Exponentiated gradient (Kivinen and Warmuth,
1997) is an elegant algorithm for solving optimisa-
tion problems involving simplex constraints. Re-

2Ties are broken arbitrarily.

call our constrained optimisation problem:

arg min
ŷ1,...,ŷ`

Q(ŷ1, . . . , ŷ`)

s.t. ∀i ∈ {1 . . . `} : ŷi ∈ ∆|VT |
where Q(ŷ1, . . . , ŷ`) is defined as

−
∑̀
i=1

ŷi · log softmax (fff (Θ, ŷ<i,x)) . (6)

EG is an iterative algorithm, which updates each
distribution ŷti in the current time-step t based on
the distributions of the previous time-step as fol-
lows:

∀w ∈ VT : ŷti(w) =
1
Zti
ŷt−1i (w) exp

(
−η∇t−1i,w

)
where η is the step size, ∇t−1i,w =

∂Q(ŷt−11 ,...,ŷ
t−1
` )

∂ŷi(w)

and Zti is the normalisation constant

Zti =
∑
w∈VT

ŷt−1i (w) exp
(
−η∇t−1i,w

)
.

The partial derivatives ∇i,w are calculated using
the back propagation algorithm treating {ŷi}`i=1
as parameters and the original parameters of the
model Θ as constants. Adapting EG to our decod-
ing problem leads to Algorithm 1.

It can be shown that the EG algorithm is a gra-
dient descent algorithm for minimising the follow-
ing objective function subject to the simplex con-
straints:

Q(ŷ1, . . . , ŷ`)− γ
∑̀
i=1

∑
w∈VT

ŷi(w) log
1

ŷi(w)

= Q(ŷ1, . . . , ŷ`)− γ
∑̀
i=1

Entropy(ŷi) (7)

In other words, the algorithm looks for the max-
imum entropy solution which also maximizes the

148



log likelihood under the model. There are intrigu-
ing parallels with the maximum entropy formula-
tion of log-linear models (Berger et al., 1996). In
our setting, the entropy term acts as a prior which
discourages overly-confident estimates in the ab-
sence of sufficient evidence.

3.2 Stochastic Gradient Descent (SGD)
To be able to apply SGD to our optimisation prob-
lem, we need to make sure that the simplex con-
straints are enforced. One way to achieve this is
by reparameterising using the softmax transfor-
mation, i.e. ŷi = softmax (r̂i). The resulting
unconstrained optimisation problem, now over r̂i,
becomes

arg min
r̂1,...,r̂`

−
∑̀
i=1

softmax (r̂i) · log softmax (f (Θ, ŷ<i,x))

where EEEyiT is replaced with the expected embed-
ding of the target words under the distribution re-
sulted from the Esoftmax(r̂i) [EEE

w
T ] in the model.

To apply SGD updates, we need the gradient
of the objective function with respect to the new
variables r̂i which can be derived with the back-
propagation algorithm based on the chain rule:

∂Q

∂r̂i(w)
=
∑
w′∈VT

∂Q(.)
∂ŷi(w′)

∂ŷi(w′)
∂r̂i(w)

The resulting SGD algorithm is summarized in Al-
gorithm 2.

4 Decoding in Extended NMT

Our decoding framework allows us to effectively
and flexibly add additional global factors over
the output symbols during inference. This en-
ables decoding for richer global models, for which
there is no effective means of greedy decoding or
beam search. We outline several such models, and
their corresponding relaxed objective functions for
optimisation-based decoding.

Bidirectional Ensemble. Standard NMT gener-
ates the translation in a left-to-right manner, condi-
tioning each target word on its left context. How-
ever, the joint probability of the translation can be
decomposed in a myriad of different orders; one
compelling alternative would be to condition each
target word on its right context, i.e., generating
the target sentence from right-to-left. We would
not expect a right-to-left model to outperform a
left-to-right, however, as the left-to-right ordering

reflects the natural temporal order of spoken lan-
guage. However, the right-to-left model is likely
to provide a complementary signal in translation,
as it will be bringing different biases and making
largely independent prediction errors to those of
the left-to-right model. For this reason, we pro-
pose to use both models, and seek to find trans-
lations that have high probability according both
models (this mirrors work on bidirectional decod-
ing in classical statistical machine translation by
Watanabe and Sumita (2002b).) Decoding un-
der the ensemble of these models leads to an in-
tractable search problem, not well suited to tradi-
tional greedy or beam search algorithms, which re-
quire a fixed generation order of the target words.
This ensemble decoding problem can be formu-
lated simply in our linear relaxation approach, us-
ing the following objective function:

C+bidir :=− α log PΘ← (y | x)
− (1− α) log PΘ→ (y | x) ; (8)

where α is an interpolation hyper-parameter,
which we set to 0.5; Θ→ and Θ← are the pre-
trained left-to-right and right-to-left models, re-
spectively. This bidirectional agreement may also
lead to improvement in translation diversity, as
shown in Li and Jurafsky (2016) in a re-ranking
evaluation.

Bilingual Ensemble. Another source of com-
plementary information is in terms of the transla-
tion direction, that is forward translation from the
source to the target language, and reverse trans-
lation in the target to source direction. Decod-
ing must find a translation which scores well un-
der both the forward and reverse translation mod-
els. This is inspired by the direct and reverse
feature functions commonly used in classical dis-
criminative SMT (Och and Ney, 2002) which have
been shown to offer some complementary bene-
fits (although see Lopez and Resnik (2006)). More
specifically, we decode for the best translation in
the intersection of the source-to-target and target-
to-source models by minimizing the following ob-
jective function:

C+biling :=− α log PΘs→t (y | x)
− (1− α) log PΘs←t (x | y) ; (9)

where α is an interpolation hyper-parameter to be
fine-tuned; and Θs→t and Θs←t are the pre-trained

149



Algorithm 2 The SGD Algorithm for Decoding by Optimisation
1: For all i initialise r̂0i
2: for t = 1, . . . ,MaxIter do . Q(.) is defined in eqn (6) and ŷi = softmax(r̂i)
3: For all i, w : calculate∇t−1i,w =

∑
w′∈VT

∂Q(ŷt−11 ,...,ŷ
t−1
` )

∂ŷi(w′)
∂ŷi(w

′)
∂r̂i(w)

. using backpropagation

4: For all i, w : update r̂ti(w) = r̂
t−1
i (w)− η∇t−1i,w . η is the step size

5: return arg mintQ(softmax(r̂t1), . . . , softmax(r̂t`))

# tokens # types # sents
BTEC zh→en

train 422k / 454k 3k / 3k 44,016
dev 10k / 10k 1k / 1k 1,006
test 5k / 5k 1k / 1k 506

TED Talks de→en
train 4m / 4m 26k / 19k 194,181
dev-test2010 33k / 35k 4k / 3k 1,565
test2014 26k / 27k 4k / 3k 1,305

WMT 2016 de→en
train 107m / 108m 90k / 78k 4m
dev-test2013&14 154k / 152k 20k / 13k 6003
test2015 54k / 54k 10k / 8k 2169

Table 1: Statistics of the training and evalua-
tion sets; token and types are presented for both
source/target languages.

source-to-target and target-to-source models, re-
spectively. Decoding for the best translation under
the above objective function leads to an intractable
search problem, as the reverse model is global over
the target language, meaning there is no obvious
means of search with a greedy algorithm or simi-
lar.

Discussion. There are two important considera-
tions on how best to initialise the relaxed optimisa-
tion in the above settings, and how best to choose
the step size. As the relaxed optimisation prob-
lem is, in general, non-convex, finding a plausible
initialisation is likely to be important for avoiding
local optima. Furthermore, a proper step size is a
key in the success of the EG-based and SGD-based
optimisation algorithms, and there is no obvious
method how to best choose its value. We may also
adaptively change the step size using (scheduled)
annealing or via the line search. We return to this
considerations in the experimental evaluation.

5 Experiments

5.1 Setup

Datasets. We conducted our experiments on
datasets with different scales, translating between
Chinese→English using the BTEC corpus, and
German→English using the IWSLT 2015 TED

Talks(Cettolo et al., 2014) and WMT 20163 cor-
pora. The statistics of the datasets can be found in
Table 1.

NMT Models. We implemented our
continuous-optimisation based decoding method
on top of the Mantidae toolkit4 (Cohn et al.,
2016), and using the dynet deep learning library5

(Neubig et al., 2017). All neural network models
were configured with 512 input embedding and
hidden layer dimensions, and 256 alignment
dimension, with 1 and 2 hidden layers in the
source and target, respectively. We used a LSTM
recurrent structure (Hochreiter and Schmidhuber,
1997) for both source and target RNN sequences.
For the vocabulary, we use word frequency cut-off
of 5, and words rarer than this were mapped to
a sentinel. For the large-scale WMT dataset,
we applied byte-pair encoding (BPE) method
(Sennrich et al., 2016) to better handle unknown
words.6 For training our neural models, we use
early stopping based on development perplexity,
which usually occurs after 5-8 epochs.

Evaluation Metrics. We evaluated in terms of
search error, measured using the model score of
the inferred solution (either continuous or dis-
crete), as well as measuring the end transla-
tion quality with case-insensitive BLEU (Pap-
ineni et al., 2002). The continuous cost measures
− 1|ŷ| logPΘ (ŷ | x) under the model Θ; the dis-
crete model score has the same formulation, al-
beit using the discrete rounded solution y (see §3).
Note the cost can be used as a tool for selecting the
best inference solution, as well as assessing con-
vergence, as we illustrate below.

3http://www.statmt.org/wmt16/
translation-task.html

4https://github.com/duyvuleo/Mantidae
5https://github.com/clab/dynet
6With BPE, the out of vocabulary rates on heldout data

are < 1%.

150



●

●

●●●●●●●●●●●●●● ● ● ● ● ● ● ● ●

●

●
●●●●●●●●●●●●●● ● ● ● ● ● ● ● ●

1

10

1 5 20 50 100 200 400
iterations

C
C

os
t

●
● ●●●●●●●●●●●●●● ● ● ● ● ● ● ● ●

●
● ●●●●●●●●●●●●●● ● ● ● ● ● ● ● ●

1

10

1 5 20 50 100 200 400
iterations

D
C

os
t

● ● ●●●●●●●●●●●●●● ● ● ● ● ● ● ● ●● ● ●●●●
●●●●●●●●●● ● ● ● ● ● ● ● ●

0

10

20

30

40

1 5 20 50 100 200 400
iterations

B
LE

U

Initialisation ● beam greedy uniform Method ● ● ●EG EG−400 EG−MOM

Figure 1: Analysis on effects of initialisation states (uniform vs. greedy vs. beam), step size annealing,
momentum mechanism from BTEC zh→en translation. EG-400: EG algorithm with step size η = 400
(otherwise η = 50); EG-MOM: EG algorithm with momentum.

5.2 Results and Analysis

Initialisation and Step Size. As our relaxed op-
timisation problems are non-convex, local optima
are likely to be a problem. We test this empiri-
cally, focusing on the effect that initialisation and
step size, η, have on the inference quality.

For plausible initialisation states, we evaluate
different strategies: uniform in which the relaxed
variables ŷ are initialised to 1|VT | ; and greedy or
beam whereby ŷ are initialised based on an al-
ready good solution produced by a baseline de-
coder with greedy (gdec) or beam (bdec). Instead
of using the Viterbi outputs as a one-hot represen-
tation, we initialise to the probability prediction
vectors,7 which serves to limit attraction of the ini-
tialisation condition, which is likely to be a local
(but not global) optima.

Figure 1 illustrates the effect of initialisation on
the EG algorithm, in terms of search error (left
and middle) and translation quality (right), as we
vary the number of iterations of inference. There is
clear evidence of non-convexity: all initialisation
methods can be seen to converge using all three
measures, however they arrive at highly different
solutions. Uniform initialisation is clearly not a
viable approach, while greedy and beam initial-
isation both yield much better results. The best
initialisation, beam, outperforms both greedy and
beam decoding in terms of BLEU.

Note that the EG algorithm has fairly slow con-
vergence, requiring at least 100 iterations, irre-
spective of the initialisation. To overcome this,

7Here, EG uses softmax normalization whereas SGD
uses the pre-softmax vector.

we use momentum (Qian, 1999) to accelerate the
convergence by modifying the term ∇ti,w in Algo-
rithm 1 with a weighted moving average of past
gradients:

∇t−1i,w = γ∇t−2i,w + η
∂Q(ŷt−11 , . . . , ŷ

t−1
` )

∂ŷi(w)

where we set the momentum term γ = 0.9.
The EG with momentum (EG-MOM) converges
after fewer iterations (about 35), and results in
marginally better BLEU scores. The momentum
technique is usually used for SGD involving addi-
tive updates; it is interesting to see it also works in
EG with multiplicative updates.

The step size, η, is another important hyper-
parameter for gradient based search. We tune the
step size using line search over [10, 400] over the
development set. Figure 1 illustrates the effect of
changing step size from 50 to 400 (compare EG
and EG-400 with uniform), which results in a
marked difference of about 10 BLEU points, un-
derlining the importance of tuning this value. We
found that EG with momentum had less of a re-
liance on step size, with optimal values in [10, 50];
we use this setting hereafter.

Continuous vs Discrete Costs. Another impor-
tant question is whether the assumption behind
continuous relaxation is valid, i.e., if we optimise
a continuous cost to solve a discrete problem, do
we improve the discrete output? Although the
continuous cost diminishes with inference itera-
tions (Figure 1 left), and appears to converge, it is
not clear whether this corresponds to a better dis-
crete output (note that the discrete cost and BLEU

151



●

●

●●

●

●

●

●

●●

●

●●●
●

●
●

●

●

●

●
●

●● ●

● ●

●
●

●●

●

●

●

●

●
●

●

●

●

●
●

●
●

●

●●
●

●

●

●

●

●

●●

●
● ●

●●

●

●

●

●

●

●

●

●

●

●
●

●

●

●
●

●

●

●

●

●
●

● ● ●

● ●

●

●

●●
● ●

●
●

●

●

●
●

●

●

●
●

●

● ●●

●
●

●

●

●●
●

●

●

●

●

●

●
●

●● ●

●

●●

●

●
●

●

●

●
●●

●

●
●●

●

●

●

●

●
●

●
●

●
●●

●

●

●

●●

●

● ●

●
●

●

●

●●

●

●
●

●
●

●●

●
●

●●

●
●●

●

●

●

●

●

●

●

● ●

●

●

●

● ●

●

●●

●

●●

●

●
●

●

●

●
●

●

●●
●

●
●

●●

●

●

●

●

●

●

●
●●

●●

●

● ●●●

●

●

●
●

●

●
●

● ●●

●

●
●

●
●

●

● ●

●

●

●
●

●●●

●

●

● ●

●●
●

●●

● ●●

●

●
●

●
●

●

●

●

●

● ●●

●●

● ●

●
●●

●●

● ●
●

●

●●
●

●

●

●●

● ●
●●

●
●

●
●

●

●

●

●
●

●

●
●

●
●

●●●

●

●

●

●

●

●

●

●

● ●

●

●
●

●

●● ●

●

●
● ●

●
●

●
●●

●

●

●
●

●

●●
●

●

●

●●

●

●●●
●

● ●
●

●●
●

●
●

●

●●

●

●

●●

●

●

●●

● ●

● ●
●

●

●

●

●
●

●

●

●

●

●
●

● ●

●

●

●

●●
●

●

●

●

●

●
●

●
●

●
● ●

●●

●

●

●

●

●●

●

●●

●

●
●

●●●

●

●

●

●

●

●
●

●

●

●

●
●●

●●

●

●

●●

●● ●
●

●
●

●●

●

●●

●

●

●● ●●

●

●
●●

●
●

●

●
●

●

●

●

●

●

●

●
●●●
●

●

●●
●

●
●

●
● ●

●
●

●

●

●
●

●

● ●0.0

0.5

1.0

1.5

2.0

2.5

0.0 0.3 0.6 0.9
CCost

D
C
os
t

Figure 2: Comparing discrete vs continuous costs
from BTEC zh→en translation, using the EG al-
gorithm with momentum, η = 50. Each point cor-
responds to a sentence.

BLEU AvgLen

bdecleft-to-right 26.69 20.73
filtered rerank 26.84 20.66
EGdec w/ beam init 27.34 20.73
full rerank 27.34 21.76
EGdec w/ rerank init 27.78 21.70

Table 2: The BLEU evaluation results with EG al-
gorithm against 100-best reranking on WMT eval-
uation dataset.

scores do show improvements Figure 1 centre and
right.) Figure 2 illustrates the relation between the
two cost measures, showing that in most cases the
discrete and continuous costs are identical. Linear
relaxation fails only for a handful of cases, where
the nearest discrete solution is significantly worse
than it would appear using the continuous cost.

EG vs SGD. Both the EG and SGD algorithms
are iterative methods for solving the relaxed op-
timisation problem with simplex constraints. We
measure empirically their difference in terms of
quality of inference and speed of convergence, as
illustrated in Figure 3. Observe that SGD requires
150 iterations for convergence, whereas EG re-
quires many fewer (50). This concurs with pre-
vious work on learning structured prediction mod-
els with EG (Globerson et al., 2007). Further, the
EG algorithm consistently produces better results
in terms of both model cost and BLEU.

EG vs Reranking. Reranking is an alternative
method for integrating global factors into the ex-
isting NMT systems. We compare our EG decod-
ing algorithm against the reranking approach with
bidirectional factor where the N-best outputs of a
left-to-right decoder is re-scored with the forced
decoder operating in a right-to-left fashion. The

●

●
● ●

● ●

●●●●● ● ● ● ● ● ● ● ● ● ● ● ● ●

●

●

● ● ● ●●●●●● ● ● ● ● ● ● ● ● ● ● ● ● ●

37.6

37.8

38.0

38.2

38.4

0.3

0.4

0.5
0.6

0.8

1.0

B
LE

U
C

C
ost

1 5 10 20 50 100 200 400
iterations

Method ● EG SGD

Figure 3: Analysis on convergence and perfor-
mance comparing SOFTMAX and EG algorithms
from BTEC zh→en translation. Both algorithms
use momentum and step size 50.

results are shown in Table 2. Our EG algorithm
initialised with the reranked output achieves the
best BLEU score. We also compare reranking
with EG algorithm initialised with the beam de-
coder, where for direct comparison we filter out
sentences with length greater than that of the beam
output in the k-best lists. These results show that
the EG algorithm is capable of effectively exploit-
ing the search space.

Beyond achieving similar or better translations
to re-ranking, note that EG is simpler in imple-
mentation, as it does not require kbest lists, weight
tuning and so forth. Instead this is replaced with
iterative gradient descent. The run-time of the
two methods are comparable, when reranking uses
modest k, however EG can be considerably faster
when k is large, as is typically done to extract the
full benefit from re-ranking. This performance dif-
ference is a consequence of GPU acceleration of
the dense vector operations in EG inference.

Computational Efficiency. We also quantify
the computational efficiency of the proposed de-
coding approach. Benchmarking on a GPU Ti-
tan X for decoding BTEC zh→en, the average
time per sentence is 0.02 secs for greedy, 0.07s for
beam=5, 0.11s for beam=10, and 3.1s for relaxed
EG decoding, which uses an average of 35 EG it-
erations. The majority of time in the EG algorithm
is in the forward and backward passes, taking 30%
and 67% of the time, respectively. Our imple-

152



BTEC TEDTalks WMT
zh→ en de→ en de→ en

gdecleft-to-right 35.98 23.16 24.41
gdecright-to-left 35.86 21.95 23.59
EGdecgreedy init 36.34 23.28 24.63

+bidir. 36.67 23.91 25.37†
+bilingual 36.88† 24.01† 25.21

bdecleft-to-right 38.02 23.95 26.69
bdecright-to-left 37.38 23.13 26.11
EGdecbeam init 38.38 24.02 26.66

+bidir. 39.13† 24.72† 27.34†
+bilingual 38.25 24.60 26.82

Table 3: The BLEU evaluation results across eval-
uation datasets for EG algorithm variants against
the baselines; bold: statistically significantly bet-
ter than the best greedy or beam baseline, †: best
performance on dataset.

mentation was not optimised thoroughly, and it
is likely that it could be made significantly faster,
which we defer to future research.

Main Results. Table 3 shows our experimental
results across all datasets, evaluating the EG al-
gorithm and its variants.8 For the EG algorithm
with greedy initialisation (top), we see small but
consistent improvements in terms of BLEU. Beam
initialisation led to overall higher BLEU scores,
and again demonstrating a similar pattern of im-
provements, albeit of a lower magnitude, over the
initialisation values.

Next we evaluate the capability of our infer-
ence method with extended NMT models, where
approximate algorithms such as greedy or beam
search are infeasible. With the bidirectional en-
semble, we obtained the statistically significant
BLEU score improvements compared to the uni-
directional models, for either greedy or beam ini-
tialisation. This is interesting in the sense that
the unidirectional right-to-left model always per-
forms worse than the left-to-right model. How-
ever, our method with bidirectional ensemble is
capable of combining their strengths in a unified
setting. For the bilingual ensemble, we see similar
effects, with better BLEU score improvements in
most cases, albeit of a lower magnitude, over the
bidirectional one. This is likely to be due to a dis-
parity with the training condition for the models,

8Due to the space constraints, we report results for the EG
algorithm only. See also translation examples in Figure 4.

which were learned independently of one another.
Overall, decoding in extended NMT models

leads to performance improvements compared to
baseline methods. This is one of the main findings
in this work, and augurs well for its extension to
other global model variants.

6 Related Work

Decoding (inference) for neural models is an im-
portant task; however, there is limited research
in this space perhaps due to the challenging na-
ture of this task, with only a few works exploring
some extensions to improve upon them. The most
widely-used inference methods include sampling
(Cho, 2016), greedy and beam search (Sutskever
et al., 2014; Bahdanau et al., 2015, inter alia), and
reranking (Birch, 2016; Li and Jurafsky, 2016).

Cho (2016) proposed to perturb the neural
model by injecting noise(s) in the hidden transi-
tion function of the conditional recurrent neural
language model during greedy or beam search,
and execute multiple parallel decoding runs. This
strategy can improves over greedy and beam
search; however, it is not clear how, when and
where noise should be injected to be beneficial.
Recently, Wiseman and Rush (2016) proposed
beam search optimisation while training neural
models, where the model parameters are updated
in case the gold standard falls outside of the beam.
This exposes the model to its past incorrect pre-
dicted labels, hence making the training more ro-
bust. This is orthogonal to our approach where we
focus on the decoding problem with a pre-trained
model.

Reranking has also been proposed as a means
of global model combination: Birch (2016) and
Li and Jurafsky (2016) re-rank the left-to-right de-
coded translations based on the scores of a right-
to-left model, learning to more diverse transla-
tions. Related, Li et al. (2016) learn to adjust the
beam diversity with reinforcement learning.

Perhaps most relevant is Snelleman (2016), per-
formed concurrently to this work, who also pro-
posed an inference method for NMT using linear
relaxation. Snelleman’s method was similar to our
SGD approach, however he did not manage to out-
perform beam search baselines with an encoder-
decoder. In contrast we go much further, propos-
ing the EG algorithm, which we show works much
more effectively than SGD, and demonstrate how
this can be applied to inference in an attentional

153



BTEC zh→en
Source 我确定我昨天给旅馆打过电话并且做了预定。
Reference i am sure that i called the hotel yesterday and made a reservation .

beam dec (l2r) i ’m sure i called the hotel reservation and i made a reservation .

beam dec (r2l) i ’m sure i made this hotel reservation and made a reservation .

rerank +bidir. i ’m sure i called the hotel reservation and i made a reservation .

rerank +biling. i ’m sure i called the hotel reservation and i made a reservation .

EGdec i ’m sure i called the hotel yesterday and i made a reservation .

+bidir. i ’m sure i called the hotel yesterday and i made a reservation .

+biling. i ’m sure i called the hotel yesterday and i made a reservation .

TED Talks de→en
Source wir sind doch alle gute bürger der sozialen medien , bei denen die währung neid ist . stimmt ’ s ?

Reference i mean , we ’re all good citizens of social media , are n’t we , where the currency is envy ?

beam dec (l2r) we ’re all great UNK of social media , where the currency is envy . right ?

beam dec (r2l) we ’re all good citizens in social media , which is where that is envy . right ?

rerank +bidir. we ’re all good citizens of social media , where the currency is envy . right ?

rerank +biling. we ’re all good citizens of social media , where the currency is envy . right ?

EGdec we ’re all great UNK of social media , where the currency is envy . right ?

+bidir. we ’re all good UNK of social media , where the currency is envy . right ?

+biling. we ’re all good citizens of social media , where the currency is envy . right ?

WMT de→en
Source neben dem wm-titel 2007 und dem gewinn der champions league 2014 holte er 2008 ( hsg nordhorn ) und 2010 ( tbv lemgo

) den ehf-pokal .

Reference besides the 2007 world championship he also won the champions league in may and the ehf-cup in 2008 ( hsg nordhorn )

and 2010 ( tbv lemgo ) .

beam dec (l2r)∗ in addition to the title 2007 in 2007 and the win of the champions league 2014 in 2008 ( hsg nordhorn ) and 2010 ( tbv
lemgo ) , he won the ehf cup .

beam dec (r2l) in addition to the world championship title 2007 and winning the champions league in 2014 , he won the ehf-cup in 2008

( hsg nordhorn ) and 2010 ( tbv lemgo ) .

EGdec in addition to the title 2007 in 2007 and the win of the champions league 2014 in 2008 ( hsg nordhorn ) and 2010 ( tbv

lemgo ) , he won the ehf cup .

+bidir. in addition to the title championship in 2007 and the win of the champions league 2014 in 2008 ( hsg nordhorn ) and 2010

( tbv lemgo ) , he won the ehf cup .

+biling. in addition to the world title title 2007 and the win of the champions league 2014 in 2008 ( hsg nordhorn ) and 2010 ( tbv

lemgo ) , he won the ehf cup .

Figure 4: Translation examples generated by the models. ∗: reranking with bidirectional (+bidir.) and
bilingual (+biling.) produced the same translation string.

encoder-decoder. Moreover, we demonstrate the
utility of related optimisation for inference over
global ensembles of models, resulting in consis-
tent improvements in search error and end transla-
tion quality.

Recently, relaxation techniques have been ap-
plied to deep models for training and inference
in text classification (Belanger and McCallum,
2016; Belanger et al., 2017), and fully differ-
entiable training of sequence-to-sequence models
with scheduled-sampling (Goyal et al., 2017). Our
work has applied the relaxation technique specifi-
cally for decoding in NMT models.

7 Conclusions

This work presents the first attempt in formulat-
ing decoding in NMT as a continuous optimisation
problem. The core idea is to drop the integrality
(i.e. one-hot vector) constraint from the predic-
tion variables and allow them to have soft assign-
ments within the probability simplex while min-
imising the loss function produced by the neural

model. We have provided two optimisation algo-
rithms – exponentiated gradient (EG) and stochas-
tic gradient descent (SGD) – for optimising the
resulting contained optimisation problem, where
our findings show the effectiveness of EG com-
pared to SGD. Thanks to our framework, we have
been able to decode when intersecting left-to-right
and right-to-left as well as source-to-target and
target-to-source NMT models. Our results show
that our decoding framework is effective and leads
to substantial improvements in translations gener-
ated from the intersected models, where the typi-
cal greedy or beam search algorithms are not ap-
plicable.

This work raises several compelling possibili-
ties which we intend to address in future work,
such as improving decoding speed, integrating ad-
ditional constraints such as word coverage and fer-
tility into decoding,9 and applying our method to
other intractable structured prediction problems.

9These constraints have only been used for training in the
previous works (Cohn et al., 2016; Mi et al., 2016).

154



Acknowledgments

We thank the reviewers for valuable feedbacks and
discussions. Cong Duy Vu Hoang is supported
by Australian Government Research Training Pro-
gram Scholarships at the University of Melbourne,
Australia. This work was supported by the Aus-
tralian Research Council through their Discovery
and Future Fellowship programmes.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural Machine Translation by Jointly
Learning to Align and Translate. In Proc. of 3rd
International Conference on Learning Representa-
tions (ICLR2015).

D. Belanger, B. Yang, and A. McCallum. 2017. End-
to-End Learning for Structured Prediction Energy
Networks. In Proceedings of the 34th Interna-
tional Conference on International Conference on
Machine Learning - Volume 48, ICML’17.

David Belanger and Andrew McCallum. 2016. Struc-
tured prediction energy networks. In Proceedings
of the 33rd International Conference on Interna-
tional Conference on Machine Learning - Volume
48, ICML’16, pages 983–992. JMLR.org.

Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Comput.
Linguist., 22(1):39–71.

Rico Sennrich; Barry Haddow; Alexandra Birch. 2016.
Edinburgh Neural Machine Translation Systems for
WMT 16. In Proceedings of the First Conference
on Machine Translation, Volume 2: Shared Task Pa-
pers. Berlin, Germany.

M. Cettolo, J. Niehues, S. Stuker, L. Bentivogli, and
M. Federico. 2014. Report on the 11th IWSLT
Evaluation Campaign. In Proc. of The Interna-
tional Workshop on Spoken Language Translation
(IWSLT).

K. Cho. 2016. Noisy Parallel Approximate Decoding
for Conditional Recurrent Language Model. ArXiv
e-prints.

Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vy-
molova, Kaisheng Yao, Chris Dyer, and Gholam-
reza Haffari. 2016. Incorporating Structural Align-
ment Biases into an Attentional Neural Translation
Model. In Proceedings of the 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 876–885, San Diego, California. As-
sociation for Computational Linguistics.

Amir Globerson, Terry Y. Koo, Xavier Carreras, and
Michael Collins. 2007. Exponentiated Gradient Al-
gorithms for Log-linear Structured Prediction. In

Proceedings of the 24th International Conference on
Machine Learning, ICML ’07, pages 305–312, New
York, NY, USA. ACM.

Kartik Goyal, Chris Dyer, and Taylor Berg-
Kirkpatrick. 2017. Differentiable scheduled
sampling for credit assignment. In Proceedings
of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short
Papers), ACL’17, Vancouver, Canada.

A. Graves. 2013. Generating Sequences With Recur-
rent Neural Networks. ArXiv e-prints.

Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long
Short-Term Memory. Neural Comput., 9(8):1735–
1780.

Jyrki Kivinen and Manfred K. Warmuth. 1997. Expo-
nentiated Gradient Versus Gradient Descent for Lin-
ear Predictors. Inf. Comput., 132(1):1–63.

J. Li and D. Jurafsky. 2016. Mutual Information and
Diverse Decoding Improve Neural Machine Trans-
lation. ArXiv e-prints.

J. Li, W. Monroe, and D. Jurafsky. 2016. A Simple,
Fast Diverse Decoding Algorithm for Neural Gener-
ation. ArXiv e-prints.

Z. C. Lipton, J. Berkowitz, and C. Elkan. 2015. A Crit-
ical Review of Recurrent Neural Networks for Se-
quence Learning. ArXiv e-prints.

Adam Lopez and Philip Resnik. 2006. Word-based
alignment, phrase-based translation: Whats the link.
In Proceedings of 7th Biennial Conference of the As-
sociation for Machine Translation in the Americas
(AMTA).

Haitao Mi, Baskaran Sankaran, Zhiguo Wang, and Abe
Ittycheriah. 2016. Coverage Embedding Models for
Neural Machine Translation. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 955–960, Austin,
Texas. Association for Computational Linguistics.

G. Neubig, C. Dyer, Y. Goldberg, A. Matthews,
W. Ammar, A. Anastasopoulos, M. Ballesteros,
D. Chiang, D. Clothiaux, T. Cohn, K. Duh,
M. Faruqui, C. Gan, D. Garrette, Y. Ji, L. Kong,
A. Kuncoro, G. Kumar, C. Malaviya, P. Michel,
Y. Oda, M. Richardson, N. Saphra, S. Swayamdipta,
and P. Yin. 2017. DyNet: The Dynamic Neural Net-
work Toolkit. ArXiv e-prints.

Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 295–302.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings

155



of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Ning Qian. 1999. On the momentum term in gradi-
ent descent learning algorithms . Neural Networks,
12(1):145 – 151.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural Machine Translation of Rare Words
with Subword Units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715–
1725, Berlin, Germany. Association for Computa-
tional Linguistics.

Emanuel Snelleman. 2016. Decoding neural machine
translation using gradient descent. Master’s the-
sis, Chalmers University of Technology, Gothen-
burg, Sweden.

David Sontag. 2010. Approximate Inference in Graph-
ical Models using LP Relaxations. Ph.D. thesis,
Massachusetts Institute of Technology, Department
of Electrical Engineering and Computer Science.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of the 27th International
Conference on Neural Information Processing Sys-
tems, NIPS’14, pages 3104–3112, Cambridge, MA,
USA. MIT Press.

Taro Watanabe and Eiichiro Sumita. 2002a. Bidirec-
tional Decoding for Statistical Machine Translation.
In Proceedings of the 19th International Conference
on Computational Linguistics - Volume 1, COLING
’02, pages 1–7, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Taro Watanabe and Eiichiro Sumita. 2002b. Bidirec-
tional decoding for statistical machine translation.
In Proceedings of the 19th international conference
on Computational linguistics-Volume 1, pages 1–7.

Sam Wiseman and Alexander M. Rush. 2016.
Sequence-to-Sequence Learning as Beam-Search
Optimization. In Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 1296–1306, Austin, Texas. Asso-
ciation for Computational Linguistics.

156


