



















































Canonical Context-Free Grammars and Strong Learning: Two Approaches


Proceedings of the 14th Meeting on the Mathematics of Language (MoL 14), pages 99–111,
Chicago, USA, July 25–26, 2015. c©2015 Association for Computational Linguistics

Canonical Context-Free Grammars and Strong Learning: Two Approaches

Alexander Clark
Department of Philosophy,

King’s College London
alexander.clark@kcl.ac.uk

Abstract

Strong learning of context-free grammars is
the problem of learning a grammar which is
not just weakly equivalent to a target gram-
mar but isomorphic or structurally equivalent
to it. This is closely related to the problem
of defining a canonical grammar for the lan-
guage. The current proposal for strong learn-
ing of a small class of CFGs uses grammars
whose nonterminals correspond to congruence
classes of the language, in particular to a sub-
set of those that satisfy a primality condition.
Here we extend this approach to larger classes
of CFGs where the nonterminals correspond
instead to closed sets of strings; to elements
of the syntactic concept lattice. We present
two different classes of canonical context-free
grammars. One is based on all of the primes in
the lattice: the other, more suitable for strong
learning algorithms is based on a subset of
primes that are irreducible in a certain sense.

1 Introduction

This paper is concerned with the problem of strong
learning of context-free grammars in the distribu-
tional framework. One approach, initiated in (Clark,
2014) is to develop strong learning algorithms by
defining canonical grammars based on properties of
algebraic structures associated with the language:
specifically the syntactic monoid of the language. In
that paper a strong learning result was presented for
a subclass of the class of substitutable languages,
languages which have a simple language theoretic
closure property.

In this paper we will extend the canonical gram-
mar ideas to a larger class of grammars, while not
presenting a full strong learning result, for reasons
of space and some technical details not yet resolved.
Rather than using the syntactic monoid, we use
the syntactic concept lattice (SCL), (Clark, 2013),
a richer structure that is suitable for modeling all
context-free grammars. In the case of substitutable
languages the syntactic monoid is almost identical
to the syntactic concept lattice.

We want these canonical grammars to be as un-
ambiguous as possible, and to use as few nontermi-
nals as possible. These two obvious principles pull
in the same direction: a grammar with extra nonter-
minals will typically have extra derivations and thus
a higher degree of ambiguity. Finding some global
minimum leads in general to intractable computa-
tional problems – the set covering problem, a clas-
sic NP-hard problem – and the answer may be in-
determinate (in that there may be two structurally
distinct minima). So rather we stipulate some tech-
nical notion which is more determinate, and can be
efficiently identified (though we do not talk about
the algorithmic issues here). In particular, we want
the grammars defined to be compatible with effi-
cient learning algorithms for context-free grammars
(Yoshinaka, 2012a; Leiß, 2014).

In the case of the monoid, we only have one op-
eration, concatenation, and given a derivation tree
with unlabeled interior nodes, each node in the tree
can only be legally labeled with the unique congru-
ence class of the yield of the subtree. Thus given the
unlabeled trees, the labeling is determined.

In the case of the SCL, we have a lattice struc-

99



ture, and so there are many different possible ways
of modeling the structure, and many different ways
of labeling a given tree from the very specific to the
very general. We previously argued in (Clark, 2011)
that the most general labelings would be optimal;
that view now seems simplistic.

We argue that we should only model those unpre-
dictable parts of the structure, that is to say those
places where the structure differs from the free struc-
ture P(Σ∗). The grammar does not need to state that
{u}◦{v} = {uv} or that {u}∪{v} = {u, v}: these
are true in the free structure. It is only when these are
not equal that we need to represent the difference.

We will give definitions of the basic mathematical
concepts we use in Section 2, including a brief intro-
duction to the syntactic concept lattice in Section 2.4
to make the paper self-contained.

In Section 3 we explain the relation between
strong learning and canonical grammars.

Then in Section 4 we extend the definition of
primes from congruence classes to closed sets of
strings. Section 5 presents our first family of canon-
ical grammars that are based directly on all of the
primes in the language.

In the case of concepts the lattice structure means
that there may be many different concepts that con-
tain a given string, and so in Section 6 we discuss
how to exploit the lattice structure to select a smaller
set of categories that are irreducible in some sense;
and then in Section 7 we present a second family of
canonical grammars based on this restricted subset
of the primes. We finish with a worked example to
illustrate the abstract mathematical discussion and
some discussion.

2 Preliminaries

2.1 Strings, Languages and Contexts

We assume a fixed alphabet Σ and write Σ∗ for the
set of strings. A language is a subset of Σ∗, we
write concatenation of languages L,M as L · M ,
or sometimes just LM . The empty string is λ. We
take a symbol � 6∈ Σ, and using this we define a
context as an element of Σ∗�Σ∗, written l�r. We
define � as l�r � w = lwr, and extend these to
sets of strings and contexts in the usual way. The
empty context λ�λ = � is particularly important:
of course �� w = w.

2.2 Grammars
We define CFGs standardly as a tuple 〈Σ, V, S, P 〉
where S ∈ V is a single start symbol, V is the set
of nonterminals and P is a finite subset of V × (Σ∪
V )∗, written as N → α. The derivation process is
denoted by⇒ and⇒∗. We define

L(G,N) = {w ∈ Σ∗ | N ⇒∗G w}
and define L(G) = L(G,S). We also define the set
of derivation contexts:

C(G,N) = {l�r ∈ Σ∗�Σ∗ | S ⇒∗G lNr}
The following property corresponds to the context-
free property of the derivation process:

C(G,N)� L(G,N) ⊆ L(G).
Two grammars G1 = 〈Σ, V1, S1, P1〉, G2 =
〈Σ, V2, S2, P2〉 are weakly equivalent if L(G1) =
L(G2). They are isomorphic if there is a bijection
φ : V1 → V2, such that φ(S1) = S2 and φ(P1) =
P2, extending φ to productions and sets of produc-
tions in the natural way. Isomorphic grammars are
identical except for a relabeling of the nonterminal
symbols. Clearly isomorphism implies weak equiv-
alence.

2.3 Lattices
We assume some familiarity with lattices: see
(Davey and Priestley, 2002) for basic definitions. We
write >,⊥,∨,∧ as standard. An element x is join-
irreducible iff x = a ∨ b implies a = x or b = x;
dually it is meet irreducible iff x = a ∧ b implies
a = x or b = x. For some lattices, the set of
join irreducible elements and the set of meet irre-
ducible elements can form a “basis” for the lattice, in
that every element can be represented as a finite join
of join-irreducible elements and/or a finite meet of
meet-irreducible elements. In the lattice of all sub-
sets of Σ∗, P(Σ∗) the join irreducible sets are the
singleton sets {w} for any string, and the meet irre-
ducible sets are Σ∗ \ {w}.

A descending chain is a strictly descending se-
quence of elements of a lattice X0 ⊃ X2 ⊃ . . . Xn.
A lattice satisfies the descending chain condition
(DCC) if there are no infinite descending chains. If a
lattice satisfies the DCC, then every nonempty sub-
set has at least one minimal element. We define the
ascending chain condition (ACC) dually.

100



2.4 The Syntactic Concept Lattice
We now describe the syntactic concept lattice
briefly; for fuller descriptions see e.g. (Clark, 2013;
Leiß, 2014; Wurm, 2012). Given a fixed languageL,
we have a Galois connection between sets of strings
and sets of contexts defined, where S is a set of
strings and C is a set of contexts:

S. = {l�r | l�r � S ⊆ L}

and
C/ = {w ∈ Σ∗ | C � w ⊆ L}

.
A closed set of strings is a set of strings S such

that S = S./, a closed set of contexts is one such
that C = C/.. A concept is an ordered pair 〈S,C〉
such that S = C/ and C = S.. In this case both
S and C are closed. We will therefore often re-
fer to a concept through the corresponding closed
set of strings. Note that for any such concept, the
following property holds, which corresponds to the
context-free property of the CFG derivations:

C � S ⊆ L

Clearly w ∈ L iff � ∈ {w}., and so L is closed.
The Syntactic Concept Lattice of L, written B(L)

is the collection of concepts, with the following con-
stants, relations and operations, which we define in
terms of the closed sets of strings alone: S ∨ T =
(S ∪ T )./, S ∧ T = S ∩ T , S ◦ T = (S · T )./,
S ≤ T iff S ⊆ T , > = Σ∗, ⊥ = ∅./. With these
operations B(L) is a complete idempotent semiring,
and furthermore a complete residuated lattice.

This lattice forms a hierarchy of all distribution-
ally definable sets of strings in the language. There
will be a finite number of elements iff the language is
regular. Minimal grammars will have nonterminals
that correspond to elements of the syntactic concept
lattice as shown by (Clark, 2013). Given a context-
free grammar G such that L(G) = L, we define
a universal morphism hL : V → B(L) given by
hL(N) = L(G,N)./. We extend this to a CFG-
morphism in the obvious way. (Clark, 2013) proved
that for all CFGs L(hL(G)) = L. Therefore any
CFG for L can be mapped to a possibly smaller
grammar whose nonterminals are elements of B(L).
We can therefore assume that the nonterminals of the
grammar are elements of B(L).

3 Weak and Strong Learning

We will not present any learning algorithms here,
but the work is motivated by learning considerations
and so we need to make the background assumptions
clear. In standard models of learning, there is a tar-
get grammar G∗ and the learner, using information
only about L(G∗), must eventually return a gram-
mar Ĝ such that L(Ĝ) = L(G∗). In strong learning
(Clark, 2014) in contrast, given the same informa-
tion source, the learner must pick a Ĝ such that Ĝ is
isomorphic to G∗.

(Clark, 2014) observes that the existence of a
canonical grammar is a necessary condition for a
strong learning algorithm. Any strong learning algo-
rithm will implicitly define a canonical grammar for
any language in the class of languages that it learns.
Much of that paper is in fact concerned with pre-
cisely that definition. Accordingly in this paper we
focus on defining a canonical grammar rather than
directly presenting a learning algorithm.

The universal property of the syntactic concept
lattice is an important tool. This means that rather
than dealing directly with CFGs which are arbitrary
and intractable we can deal with the lattices B(L)
which have nice mathematical properties. We can
assume without loss of generality that the nonter-
minals of the grammar will correspond to concepts
or closed sets of strings: to elements of the lattice.
Given this, there is a natural notion of a produc-
tion being correct: X → w is correct if w ∈ X ,
X → Y1 . . . Yn is correct if X ⊇ Y1 · · · · · Yn.

Given that a language that is not regular will have
an infinite number of concepts, we need a principled
way of selecting a finite number of these in an ap-
propriate way so that we have a finite grammar. The
general approach we take is to identify some ele-
ments that are irreducible in some sense with respect
to the algebraic structure of the residuated lattice.

In the case of substitutable grammars, the closed
sets of strings are almost exactly the congruence
classes—except for >,⊥ and {λ}./, every closed
set of strings is either equal to a congruence class
or a congruence class together with λ. There seems
to be only one plausible way to define a grammar,
given that the mathematical structure of the congru-
ence classes is just a monoid. Since this structure is
so simple, there is only one reasonable irreducibility

101



property that we can use to select from the congru-
ence classes: primality, which we define later. In the
case of general CFLs things are unsurprisingly much
more complicated. There seem to be two different
factors to be considered. One factor concerns, as in
the case of the monoid, the concatenation structure
of the strings—the monoid structure of B(L)—and
the other concerns the partial order: the lattice struc-
ture of B(L).

We start by discussing the concatenation structure
in Section 4 and discuss the lattice structure later in
Section 6.

4 Primes

Since a monoid is a very simple algebraic structure,
with a single associative binary operation, there is
only one reasonable technique to define a subset of
elements of the syntactic monoid in such a way that
the grammar based on those elements is well be-
haved. (Clark, 2014) argues that we should repre-
sent only those elements where concatenation differs
from the free operation of concatenation: in other
words where [uv] ⊃ [u][v]. Language theoretically
these represent places where the monoid has some
nontrivial structure and grammatically they provide
evidence for a nontrivial nonterminal: a nontermi-
nal which occurs on the left hand side of more than
one production. Congruence classes which have this
desirable property are called primes.

For a congruence class the definition of a prime
is straightforward. If a nonzero nonunit1 congru-
ence class X has a nontrivial decomposition into
two congruence classes Y, Z such that X = Y · Z
then it is composite. The trivial decompositions are
X = X · [λ] = [λ] · X , where [λ] = {w ∈ Σ∗ |
{w}. = {λ}.}.

In the case of a CFL which is not substitutable,
we need a different criterion, since we may use con-
cepts that are not congruence classes but unions of
congruence classes. This is complicated by the fact
that the empty string may occur in many different
closed sets of strings.

Definition 1. X ∈ B(L∗) is composite if there are
Y,Z ∈ B(L) such that X 6= Y and X 6= Z and
X = Y ·Z. An element of B(L∗) is prime if it is not

1The zero congruence class, if it exists, is ∅/ and the unit is
[λ].

composite. We write P(L∗) for the set of primes of
a language L∗. The unit prime is {λ}./.

We define P(L∗)λ = P(L∗) \ {{λ}./}, the set of
nonunit primes.

Note that here we do not exclude {λ}./. Clearly
for any closed set of strings X = X · {λ}./ =
{λ}./ · X . For the condition to be nontrivial, we
clearly need to exclude such cases, but we also want
to exclude cases such as a∗ = a∗ · a∗ and a∗b∗ =
(a∗b∗) · b∗.
Lemma 1. For every a ∈ Σ, {a}./ is prime.

Proof. Supppose {a}./ = B · C. Since a ∈ {a}./
either a ∈ B and λ ∈ C or vice versa. Assume the
former. Since a ∈ B, this means that B ⊇ {a}./
and since λ ∈ C, this means that B ⊆ {a}./.
Therefore B = {a}./, or, by a similar argument
C = {a}./. Therefore it is prime.

Lemma 2. {λ}./ is prime.

Proof. Suppose {λ}./ = X ·Y . Clearly λ ∈ X,λ ∈
Y . Therefore X ⊆ {λ}./ and Y ⊆ {λ}./. But
if X,Y are in B(L∗), they must both be equal to
{λ}./ since that is the smallest concept that contains
λ.

Definition 2. If X ∈ B(L) and α ∈ P(L)+,
a nonempty string of primes, we write ᾱ for the
concatenation of the primes in α. So if α =
〈A1, . . . , An〉 then ᾱ = A1 · · · · · An. We say that
α is a prime decomposition of X iff ᾱ = X , and
none of the elements of α are unit. In the special
case where X = {λ}./ we consider 〈{λ}./〉 to be a
prime decomposition.

Example 1. If L = {anbn | n > 0}, then the
primes are {a}, {b} and L′ = L ∪ {λ}, together
with >,⊥ and {λ}./. L has the prime decomposi-
tion 〈{a}, L′, {b}〉.

We need to consider two cases: one where a prime
contains λ and one where it does not. If a closed
set of strings contains the empty string, that means
that it represents an optional category; it can be re-
placed by the empty string. If X is a closed set of
strings that contains λ and X = Y · Z, then clearly
λ ∈ Y ∩Z and Y ⊆ X and Z ⊆ X . Therefore a de-
composition of a concept that contains λ will be into

102



proper subsets of that concept. Decompositions us-
ing concepts with λ may not terminate, if the lattice
has infinite descending chains.

Example 2. Let L1 = (ba)∗, and Ln = Ln−1 ·
(ban)∗. Consider the language L =

⋃
n Ln. This

is a closed set of strings with an infinite descending
chain L ⊃ L \ L1 ⊃ L \ (L1 ∪ L2) · · · . For each
n, (ban)∗ is closed and prime, and L has no finite
prime decomposition.

Lemma 3. If B(L∗) satisfies DCC then every ele-
ment has a prime decomposition.

Proof. IfX is prime, then it has a length one decom-
position, 〈X〉. Define the width of a non empty set
of strings to be the minimum length of a string in the
set. If it contains the empty string, then the width is
zero.

Let M be the set of all non-zero non-unit con-
cepts without prime decompositions. Suppose it is
nonempty; then it has at least one minimal element,
by the DCC. Take a minimal element of minimal
width, X . Suppose X has width n. It is not prime
by assumption and so X = Y · Z. Case 1: width
of X is zero and so both Y and Z contain the empty
string: therefore Y, Z are both proper subsets of X .
Therefore they are not in M (since X is minimal).
Moreover they are not zero or unit, therefore they
have prime decompositions such that Y = ᾱ and
Z = β̄ and therefore αβ is a decomposition of X .
Case 2: the width of X is greater than zero; and the
width of Y and Z are both less than width of X .
Then Y and Z are both not in M and therefore both
have prime decompositions and therefore so doesX .
Case 3: the width of X is greater than zero, and one
of Y or Z had width zero. Assume that λ ∈ Y (the
other case is identical), then Z is a proper subset of
X and therefore has a prime decomposition, and Y
has width less than X and is therefore also has a
prime decomposition.

These decompositions aren’t necessarily unique,
but this lemma shows that the set of primes is suffi-
ciently large to express any concept we want through
finite concatenations.

It is not the case that every closed set of strings
that is composite has a unique prime decomposition;
even if we restrict ourselves to maximal decomposi-
tions: decompositions where no element can be re-

placed with a larger one. Clearly we can decompose,
for example a∗ into {λ, ak} · (a∗ \ {ak}) for any k,
and for a suitable language these can be prime.

Example 3. Consider the language L = {a, aa}
this has closed sets of strings A = {a} and B =
{λ, a}. L = A · B = B · A. So L is composite
and it has two distinct prime factorisations, which
are clearly both maximal.

It simplifies the analysis here if we assume that
all the concepts in a language have unique prime de-
compositions; accordingly we will restrict ourselves
to that case for the moment, though we will remove
this requirement in Section 7.

Definition 3. A language has the unique factorisa-
tion property (UFP) if every closed set of strings
with a nonempty distribution has a unique maxi-
mal prime factorisation; as before we stipulate that
{λ}./ has such a unique factorisation.

If a language has the UFP, and P is a closed set
of strings, then we can write Φ(P ) for the unique
prime factorisation, which is a string of primes, in
the case of P , is the length 1 string. If α = Φ(P )
then ᾱ = P .

Now if we have languages which are UFP, DCC
and a finite number of primes, then we can define a
unique grammar.

Definition 4. We define the class of languages LP
to be the set of languages which satisfy all of the
following three conditions:

1. the unique prime decomposition property,

2. have no infinite ascending or descending
chains,

3. and have a finite number of primes.

All substitutable context free languages with a fi-
nite number of primes satisfy these conditions and
so this is an extension of the approach in (Clark,
2014). All regular languages have a finite number
of primes and therefore satisfy the chain conditions,
but may not be uniquely decomposable.

Suppose we have a language L ∈ LP and a prime
N , we can construct a set of productions with N on
the left hand side as follows.

Definition 5. Let Γ ⊆ B(L)+. We define a preorder
on these strings by α - β iff ᾱ ⊆ β̄.

103



The maximal elements of Γ wrt this pre-order are
the elements

max Γ = {α ∈ Γ | ∀β ∈ Γ, β̄ ⊇ ᾱ⇒ ᾱ = β̄}.

Definition 6. Define Γ(N) = {α ∈ P(L)+λ | N ⊃
ᾱ}. We can take the maximal elements of this set:

max(Γ(N)) = {α ∈ Γ(N) | ∀α′ ∈ Γ(N), ᾱ′ ⊇
ᾱ⇒ α′ = α}

The elements of max(Γ(N)) will be the right
hand sides of productions with N on the left hand
side.

Lemma 4. If α ∈ Γ(N) then there is an α′ ∈
max(Γ(N)) such that ᾱ ⊆ ᾱ′.

Proof. Consider the set {γ ∈ Γ(N) | γ̄ ⊇ ᾱ}. If
B(L) has no infinite ascending chains, then every
non empty set has a maximal element; so let α′ be
a maximal element of this set which clearly satisfies
the condition required.

Note that 〈N〉 is not in Γ(N), since we require a
strict superset relation; this saves the definition from
vacuity. If we have infinite ascending chains then we
may not have a maximal element. This motivates the
use of an ascending chain condition.

5 Canonical Grammar based on primes

We are now in a position to define a set of canonical
grammars for LP. Given the nature of the problem it
is inevitable that we will have to have a large num-
ber of restrictions on the sorts of grammars that we
can learn. There are two ways of framing these re-
strictions either as restrictions on the grammars that
generate the language or as language theoretic re-
strictions themselves. Here we stick to the latter ap-
proach; we take a language, and define criteria that
define a class of languages. As the reader will see
though, we can express the constraints we need in
purely language theoretic terms, though those con-
straints will correspond naturally to finiteness con-
straints on the various sets of nonterminals and pro-
ductions.

Definition 7. For each language L ∈ LP we define
the grammar GP(L) as the tuple 〈Σ, V, S, P 〉 where

• S is a distinguished symbol,

• V = {S} ∪P(L)λ. If >. = ∅ then we remove
> from V : if ⊥ = ∅ then we remove ⊥,

• P is the union of the following sets of produc-
tions, for all N ∈ P(L)λ
N → a if a ∈ N , and a ∈ Σ
N → λ if λ ∈ N
N → α for all α ∈ max(Γ(N))
S → Φ(L).

Lemma 5. For every prime N , L(G,N) ⊇ N .

Proof. Induction on length of w. Base case; |w| ≤
1. in which case if w ∈ N then there is a rule N →
w. Otherwise if w ∈ N and w = a1 . . . an where
ai ∈ Σ, then since N ⊇ a./1 . . . a./n , and each a./i
is prime, we know that we have a correct production
〈a./1 . . . a./n 〉 ∈ β(N). Therefore there is some α ∈
γ(N) such that w ∈ ᾱ, and a production N → α.
Since w ∈ ᾱ we must have strings w1 . . . wk = w
such that k = |α|, and for each 1 ≤ i ≤ k wi ∈ Ni
and α = N1 . . . Nk.

By induction otherwise each wi has length less
than n and thus Ni

∗⇒ wi, and therefore N ∗⇒ w.
We should also consider the case where k = 1, in
which case we have a unary rule, and the case where
all but one of the wi have length 0. In both cases
we have one prime Q strictly less than N , and since
we have a finite number of primes, and no unary cy-
cles a simple induction on the derivation height will
suffice.

Lemma 6. The canonical grammar for G is finite
and generates L.

Proof. Note that all rules are correct and thus by in-
duction we can show that L(G,N) ⊆ N , which
combined with the previous lemma tells us that
L(G,P ) = P for all primes.

It is finite since we cannot have two productions
whose right hand sides start with the same prime.
Note that B(L) is a residuated lattice and that for
any two closed sets of strings X,Y , X\Y = {w |
X · {w} ⊆ Y } is closed. If P → Nα and P → Nβ
are both productions with |α| > 0, |β| > 0 then
since α ≤ N\P we have that α = β since they
are both equal to the unique prime factorisation of
N\P . Therefore if there are only n primes, we can

104



have at most 2n+ |Σ|+1 productions with P on the
left hand side.

Finally we verify that S generates the right
strings, which is immediate. Either L is prime, in
which case it is trivial since we have a production
S → L, or L has prime decomposition N1 . . . Nk,
in which case we can see that the production S →
N1 . . . Nk combined with the fact that L(Ni) = Ni,
gives the fact that L(S) = L.

This gives us a canonical grammar class for LP but
the grammars are still very redundant and ambigu-
ous. In the next section we consider how to select a
smaller set of nonterminals.

Lemma 7. Every language with a finite number of
primes and no infinite chains is a context free lan-
guage.

Proof. Clearly LP is a proper subset of the context
free languages, but even if we have no unique prime
decomposition, we can still get a CFG by picking
some shortest prime decomposition nondeterminis-
tically.

We can weaken these conditions as we shall see as
they are not necessary conditions; here is an example
of a CFL with infinite chains that still receives an
adequate canonical grammar.

Example 4. Let L = {w ∈ {a, b}+ : |w|a > |w|b}
The congruence classes are obviously indexed by
|w|a − |w|b. Call these En = {w | |w|a − |w|b =
n}. The closed sets of strings are Cn = {w |
|w|a − |w|b ≥ n}. L = C1 is prime, C−1 is
prime, C0 is prime. Cn for n > 1 are composite.
Cn ◦ Cm = Cn+m.
C2 = C1 ◦ C1 = b./ ◦ b./ and it is composite.

Since C1 = E1 ∪ C0, C1 · C1 = (E1 ∪ C0) · (E1 ∪
C0) = E1 ·E1 ∪ (E1 ·C0) ∪ (C0 ·C0) ∪ (C0 ·E1).
Since λ ∈ C0 this means that C1 · C1 ⊇ C0 ∪ E1 ∪
E2 = C2. Ergo this is composite. Therefore there
are exactly three primes. However this still has a
simple canonical grammar.

6 Lattice structure

For general context-free languages there may be
very many prime concepts, and as a result gram-
mars based on all primes may be excessively am-
biguous, and unsuitable for the description of natural

languages.2 Indeed there are finite languages where
the number of primes is exponential in the number
of strings in the language.

Example 5. For some large Σ = {a1, . . . , an} de-
fine L = {aiaj | i 6= j}. Clearly |L| = n(n − 1).
Every nonempty proper subset X of Σ is closed; de-
fined by the set of contexts {�ai | ai 6∈ X}. B(L)
therefore has 2n + 1 concepts, none of which are
composite.

In as case such as this the grammar defined by
GP(L) will be exponentially larger than |L|; which
is clearly undesirable.

Moreover, the previous approach relies on the
number of primes in the language being finite. Many
simple languages have an infinite number of primes
though, and so it is natural to try to extend this ap-
proach by considering some additional properties
that might serve to pick out a finite subset of these
primes. We need some additional constraints to get
smaller and less ambiguous grammars. While it is
natural to look to the meet and join irreducible ele-
ments of the syntactic concept lattice, it seems better
to use a slightly larger set; the images of the irre-
ducible elements of the free lattice. We call these
semi-irreducible; we would like to use the terms
join-prime and meet-prime, but these already have
different meanings in lattice theory.

Definition 8. Suppose X ∈ B(L); we say that X is
join-semi-irreducible (JSI) if X = {w}./ for some
w ∈ Σ∗. we say that X is meet-semi-irreducible
(MSI) if X = {l�r}/ for some l, r ∈ Σ∗.

Observe that if X is join-semi-irreducible then it
contains some strings that are not in any lower con-
cepts. Similarly if X is meet-semi-irreducible then
it contains some contexts that do not occur in any
higher concepts. Note that L is always MSI.

We will illustrate this with a simple example, us-
ing a finite language.

Example 6. Consider the language generated by the
CFG, S → AX,S → BY , A → a,A → c, A →
m,B → b, B → c, X → x,X → z,X → m,Y →
y, Y → z. This is a finite language which consists
of 11 strings, all of length 2. The string cz receives
two parses under this grammar:

2We do not discuss here the difficult question of whether nat-
ural language grammars exhibit spurious ambiguity—syntactic
ambiguity that does not relate to semantic ambiguity.

105



S

X

z

A

c

S

Y

z

B

c

Figure 1 shows the lattice for this language. Note
that the ambiguous letters/words {c,m, z} are at the
bottom of the diagram. For example {c}. = C(A)∪
C(B); this is clearly JSI, since it is defined by c, but
not MSI. At the top of the diagram are concepts that
are MSI, but not JSI. In the middle, marked with
boxes we have the concepts that are MSI-JSI.

We can represent every element either as a meet
of MSI-concepts or a join of JSI-concepts.

Lemma 8. For any X ∈ B(L), X = ⋃{{w}./ |
w ∈ X} and X. = ⋃{{l�r}/. | l�r ∈ X.}.

We can now discuss the role of these concepts.
Suppose we have some derivation in a grammar
S
∗⇒ lNr ∗⇒ lwr, where N is the nonterminal cor-

responds to some concept.
This places two constraints on N . On the one

hand l�r ∈ N., in other words N ≤ {l�r}/,
which is an MSI-concept. On the other hand w ∈ N
in other words N ≥ w./, a JSI-concept. Clearly
{l�r}/ ≥ w./ since lwr ∈ L. In the special case
where {l�r}/ = w./, we know then that this must
be the value of N . Therefore the elements that are
MSI, JSI and primes are very special.

Definition 9. A closed set of strings is an MSI-JSI-
prime, if it is MSI, JSI and prime.

7 Grammar based on MSI-JSI-primes

We now consider how to define a class of grammars
where the nonterminals consist only of the set of
MSI-JSI-primes. Rather than requiring that the lat-
tice contains no infinite chains, and using the UFP,
we define a weaker property, which more directly
determines the finiteness of the relevant sets of pro-
ductions. We define the non-standard term finitely
Noetherian.

Definition 10. We say that a set Γ ⊆ B(L)+, with
the pre-order -, is finitely Noetherian if |max Γ| is
finite and every element of Γ is less than some max-
imal element.

A set can fail to be finitely Noetherian either be-
cause it has infinite ascending chains with no max-
imal elements, or because it has an infinite number
of maximal elements.

Definition 11. LMJ is the set of all languages L such
that

1. L is nonempty and does not contain λ.

2. There are a finite number of MSI-JSI-primes;
we let V denote the set of MSI-JSI-primes, ex-
cept for {λ}./, > if >. = ∅, and ⊥ if ⊥ = ∅.

3. For each a ∈ Σ and each l�r ∈ a. there is
some X ∈ V such that a ∈ X and l�r ∈ X..

4. For every X ∈ V , {α ∈ V + | ᾱ ⊂ X} is
finitely Noetherian.

5. {α ∈ V + | ᾱ ⊆ L} is finitely Noetherian.

LMJ is incomparable with LP, as the following two
examples show.

Example 7. L = {ax, bx, ay, cy, bz, cz}. This lan-
guage in is LP but not LMJ as there are no MSI-JSI-
primes that contain for example a. {a, b} is MSI as it
is defined by �x, but not JSI. {a} on the other hand
is JSI but not MSI.

Example 8. L = {anxbn | x ≥ 0} ∪ {anxbn |
x ≥ 0}. This is in LMJ but not LP. Note that for all
n, {bn, cn} is closed and MSI as it is defined by the
single context anx�, and is clearly prime, but not
JSI. Therefore there are infinitely many primes. The
MSI-JSI-primes are only {anxbn | x ≥ 0}, {anxcn |
x ≥ 0}, {a}, {b}, {c} and {x}.
Definition 12. If L ∈ LMJ and V is the set of MSI-
JSI-primes, then for a set of strings X we write
Γ(X) = {α ∈ V + | ᾱ ⊂ X}, and ∆(X) = {α ∈
V + | ᾱ ⊆ X}.
Definition 13. For every language L in LMJ, we de-
fine a grammar

GMJ(L) = 〈V ∪ {S}, S, PL ∪ PB ∪ PS〉

where

• V is the set of MSI-JSI-primes of L.

• S is a distinguished symbol.

106



>

{b, c} {a, c,m} {z, x,m} {z, y}

{z}{c}

{a, b, c,m} {x, y, z,m}

{m}

L {λ}

⊥

Figure 1: Hasse diagram of the syntactic concept lattice of language in Example 6. All of these elements are prime.
The boxed elements are those which are both MSI and JSI, which, apart from the empty string concept, correspond to
the nonterminals of the original grammar. {a, b, c,m} is MSI, since it is equal to {�z}/, but it is not JSI.

• PL is the set of all productions X → a such
that X ∈ V , a ∈ Σ ∪ λ and a ∈ X , and there
is no Y ∈ V such that a ∈ Y and Y < X .

• PB is the set of productions X → α for every
X ∈ V and for every α ∈ max Γ(X)

• PS is the set of productions S → α for every
α ∈ max ∆(L).

Note that by Definition 11, this is finite and a CFG.
We now show that this grammar will generate all

of the strings in the language. We do this by a joint
induction on the length of the strings and the height
of the nonterminals in the lattice; it seems easier to
write these proofs as reductios.

Lemma 9. For any X ∈ V , If λ ∈ X then

X
∗⇒G∗ λ.

Proof. Suppose this is false. Take a minimalX ∈ V
such that λ ∈ X but it is not the case thatX ∗⇒G∗ λ.
If X is a minimal element of the set of nontermi-
nals that contain λ then there would be a produc-
tion X → λ in G∗; therefore X is not minimal.
Let Y be some nonterminal less than X such that
λ ∈ Y . Since X is minimal we have Y ∗⇒ λ. Now
〈Y 〉 ∈ Γ(X) so there is some production X → α
such that ᾱ ⊇ Y . Now if α = Z1 . . . Zk then each
of the Zi must be a proper subset of X that contains

λ, (since λ ∈ Y ⊆ ᾱ). Since they are proper sub-
sets we have Zi

∗⇒ λ and thus X ∗⇒ λ which is a
contradiction.

Lemma 10. For any a ∈ Σ and X ∈ V , if a ∈ X
then

X
∗⇒G∗ a.

Proof. We use just the same argument as the pre-
vious proof, except that when we consider α =
Z1 . . . Zk, there must be at least one i such that a ∈
Zi and λ ∈ Zj for all j 6= i. By Lemma 9, Zj ∗⇒ λ,
by minimality of the counterexample Zi

∗⇒ a, and
therefore X ∗⇒ a.
Lemma 11. For any w = a1 . . . an ∈ {l�r}/ for
some l�r, there are A1, . . . , An ∈ V such that ai ∈
Ai, and

〈A1 . . . An〉 ⊆ {l�r}/.
Proof. By induction on n. Base case, n = 1 is trivial
by Part 3 of Definition 11.

Clearly a1 ∈ {l�a2 . . . anr}/. Pick some A1 ∈
V such that a1 ∈ A1 and l�a2 . . . anr ∈ A.1. Since
A1 ∈ V there is some v1 such that A1 = v./1 . Since
a2 . . . an ∈ {lv1�r}/ then by the inductive hypoth-
esis there are A2, . . . , An such that 〈A2 . . . An〉 ⊆
{lv1�r}/, and therefore

〈A1 . . . An〉 ⊆ {l�r}/.
The result then follows by induction.

107



Lemma 12. For anyX ∈ V , w ∈ X if |w| > 1 then

X
∗⇒ w.

Proof. Suppose this is false for some w and X; pick
a shortest w and a minimal X .

By Lemma 10, |w| > 1. Let w = a1 . . . an. Let
l�r be some context such that X = {l�r}/. By
Lemma 11, we know that we have some A1 . . . An
such that 〈A1 . . . An〉 ⊆ X , ai ∈ Ai, for 1 ≤ i ≤ n.
Since X is a prime, we know that 〈A1 . . . An〉 ⊂
X . Therefore there is some X → β such that
〈A1 . . . An〉 ⊆ β̄. Let β = B1 . . . Bk for some
k ≥ 1. Now w ∈ β̄. Therefore there are strings
v1 . . . vk = w such that vj ∈ Bj , 1 ≤ j ≤ k. Now
for each vj , Bj either |vj | < |w| or Bj ⊂ X , and
so by the assumption at the beginning of the proof,
Bj

∗⇒ vj . Therefore X ⇒ B1 . . . Bk ⇒ v1 . . . vk =
w which is a contradiction.

Theorem 1. For every L∗ ∈ LMJ, L(GMJ(L∗)) =
L∗.

Proof. Write G∗ for GMJ(L∗). It is easy to see that
each production X → α is correct. A simple in-
duction establishes that L(G∗) ⊆ L∗. The nontriv-
ial part of the proof is to show that every string in
L∗ is generated from S. Given the previous lemmas,
this is straightforward; the proofs are a bit repetitive,
following the earlier lemmas with minor variations.
Suppose w ∈ L∗.

• If w = a for some a ∈ Σ then pick some
X ∈ V such that � ∈ X. and a ∈ X . By
Lemma 10, X ∗⇒ a and X ⊆ L; If X = L
then there is a unary rule S → X . Otherwise
〈X〉 ∈ ∆(L) and so there is some production
S → B1 . . . Bk such that a ∈ 〈B1 . . . Bk〉. So
there must be some Bi such that a ∈ Bi and
λ ∈ Bj for j 6= i, and the result follows by
Lemmas 10 and 9.

• If |w| > 1, then there are two cases. First
it might be that L ∈ V , in which case there
is a single rule S → L, and it is immedi-
ate by Lemma 12. If not, then we can use
the same argument as in Lemma 12, which
we take rapidly here. If w = a1 . . . an ∈
L, then we have some A1, . . . , An such that
〈A1 . . . An〉 ∈ ∆(L) and Ai ∗⇒ ai for 1 ≤

Label Strings Context
OAUX can, λ eagles � eat.
AUX can � eagles eat?

N eagles � that eat fly.
NP eagles that eat � can eat.
RC that can eat, λ, . . . eagles � can fly.
VI eat, fly eagles that eat can �.
VP can eat, died eagles that � can fly.
C that eagles � can fly fly.

STOP . eagles fly �
Q ? can eagles fly �

Table 1: Prime concepts of the example; note that they
are all MSI-JSI-primes except for OAUX.

i ≤ n. Therefore we have some rule S →
B1 . . . Bk where w ∈ 〈B1, . . . Bk〉 and there-
fore strings v1, . . . vk such that w = v1 . . . vk
and vi ∈ Bi; therefore S ⇒ B1 . . . Bk ∗⇒
v1 . . . vk = w.

Therefore L(G∗) ⊇ L∗ which establishes the theo-
rem.

8 Example

We will illustrate some properties of our proposed
solution with a simple toy example, based approx-
imately on an example in (Berwick et al., 2011).
We use a finite language as this shows most sharply
the distinction between weak learning, which is triv-
ial, and strong learning, which even in the case of
acyclic CFGs is either impossible or intractable in
the general case, depending on how it is formalised.
The language is generated by the following gram-
mar; optional elements are in brackets.

S→ NP VP ., S→ CAN NP VI ?
NP→ EAGLES (RC), NP→ THEY
RC→ THAT VP
VP→ (CAN) VI
VP→ DIED
VI→ EAT, VI→ FLY

It contains examples like “can eagles that fly eat?”
and “eagles that can fly can eat.” “can eagles that
died eat?”. The language contains some optional
elements and as a result is not substitutable. This
is not the minimal example as for technical reasons
the example needs to be sufficiently complex. Very
simple examples may not contain enough informa-

108



S

STOP

.

VP

V

fly

OAUX

can

NP

RC

λ

N

eagles

S

STOP

.

VP

V

fly

OAUX

can

NP

RC

VP

V

eat

OAUX

λ

that

N

eagles

Figure 2: Example parse trees using GP(·). Note that where optional elements do not appear, we have an empty
(phonologically null) constituent. This is a notational variant of having a unary rule.

tion to indicate the structure unequivocally. So for
example, we added a word “died”. This alters the
structure of the lattice and means that the class VP
is then a prime. Without this addition, the con-
cept corresponding to VP would be decomposable as
{CAN, λ}·{EAT, FLY}. Similarly we added the word
THEY to the class of NP. Without these additions, the
language would not contain enough information to
distinguish whether, for example, the relative clause
attaches to the preceding noun or the following verb.
Consider the language L = {ab, acb}. There is no
motive, based on the strings, for the claim that c at-
taches to the left or the right. But if we enlarge it
slightly to {ab, acb, ad} it is more natural to attach
the c optionally to the b.

The lattice contains 43 elements of which 10
are prime, which are listed in Table 1; we la-
bel each prime with a mnemonic label, reusing
the nonterminal symbols from the target gram-
mar for ease of reading. The composite ele-
ments contain for example the set of four strings
{EAT EAT, EAT FLY, FLY EAT, FLY FLY } which is
clearly not prime.

Figure 2 and 3 show some trees of some of the
sentences that illustrate the structure of the derived
grammars.

9 Discussion and Conclusion

The two methods we present here do not exhaust
the possibilities: rather they present two extremes.
One method uses all primes, the other uses what

RC OAUX

AUX{λ}

NP

N

VP

VI

Figure 4: Lattice fragments showing some of the primes.
The word CAN, forms a concept on its own, AUX, since
when it occurs at the beginning of a sentence, it is obliga-
tory. The concept RC which contains the sequences THAT
EAT, THAT CAN FLY and so on, is always optional, and so
there is no corresponding concept that does not contain λ.

seems to be the smallest possible set of primes that
we can define using these techniques. Our reliance
on individual contexts and substrings in the defi-
nition of MSI-JSI-primes is both natural but inad-
equate. In terms of earlier traditions of analytical
linguistics, we are following Sestier-Kunze domi-
nation rather than Dobrushin-domination (Marcus,
1994). (Kunze, 1967 1968) argues that for the ade-
quate description of German lexical categories sim-
ple Dobrushin domination is inadequate. Note also
the similar notions in (Adriaans, 1999) of context-
separability and expression-separability. The set of
MSI-JSI-primes may well be too restricted; it seems
necessary to have nonterminals that correspond to
cases where {l�r}/ ) {w}./. We leave this prob-
lem for future work.

It also seems quite natural to define a dual of pri-
mality. For a closed set of strings X , we can de-

109



S

Q

?

V

fly

NP

RC

VP

eat

that

N

eagles

AUX

can

S

Q

?

V

fly

NP

RC

λ

N

eagles

AUX

can

S

STOP

.

VP

V

fly

AUX

can

NP

RC

VP

V

eat

that

N

eagles

Figure 3: Example parse trees using the second approach, GMJ(·). Here we do not have OAUX, and so we have unary
rules VP→ V.

fine conditions X. = Y�Z, for some closed sets
of strings Y, Z. There are also weaker variants of
this. These correspond to a condition that the non-
terminal occur on the left hand side of more than one
production.

From a linguistic point of view, it is interesting
that these approaches give local trees of potentially
unbounded ranks as well as empty constituents and
unary rules. This therefore means that the claim that
natural languages only use a binary syntactic opera-
tion (MERGE) becomes a contentful empirical claim.

There is a close relation between the models used
here and the primal and dual weak learning algo-
rithms presented in (Yoshinaka, 2012b); in partic-
ular, the categories that are MSI, have the 1-finite-
context-property (FCP) and the JSI-concepts have
the 1-finite-kernel-property (FKP). The languages in
LMJ therefore will be weakly learnable under certain
paradigms. In order to turn the results here into a
full strong learning result requires then the efficient
computation of the canonical grammar given a suffi-
ciently large weakly correct grammar. There do ap-
pear to be some technical problems to overcome: for
example showing that the number of errors made in
selecting the MSI-JSI-primes will only ever be finite.

Given the extension of distributional learning to
multiple context-free grammars (MCFGs) (Seki et
al., 1991) by (Yoshinaka, 2011), and the extension
of the syntactic concept lattice in (Clark and Yoshi-
naka, 2014), it seems possible to straightforwardly
extend these methods to at least some MCFGs. In
particular the notion of a closed set of strings being

composite is naturally generalised by replacing the
single concatenation operation ·, with the family of
all non-deleting non-permuting linear regular func-
tions of appropriate arities.

The existence of these canonical grammars seems
to be related in an interesting way to algebraic prop-
erties of the syntactic concept lattice. Indeed the
finite cardinality of the lattice is exactly equivalent
to the regularity of the language. It seems that
other finiteness properties of the lattice—for exam-
ple, compactness, the chain conditions etc.—may be
crucial. More generally, the results presented here
show that it may be possible to have strong learn-
ing algorithms for some quite large classes of lan-
guages. This suggests that the orthodox view that
semantic information is required to learn syntactic
structure may be mistaken; the set of strings of the
language may define an intrinsic structure that can
be learned purely distributionally. If the structures
so defined can support compositional interpretation
of the semantics, then this would provide strong em-
pirical support for this approach.

Acknowledgments

I am grateful to Ryo Yoshinaka, Makoto Kanazawa
and Greg Kobele, for technical comments; and to
Bob Berwick, Paul Pietroski and George Walkden
for discussions that have helped to motivate this ap-
proach.

110



References

Pieter Adriaans. 1999. Learning shallow context-free
languages under simple distributions. Technical Re-
port ILLC Report PP-1999-13, Institute for Logic,
Language and Computation, Amsterdam.

R.C. Berwick, P. Pietroski, B. Yankama, and N. Chom-
sky. 2011. Poverty of the stimulus revisited. Cogni-
tive Science, 35:1207–1242.

Alexander Clark and Ryo Yoshinaka. 2014. An algebraic
approach to multiple context-free grammars. In Sergei
Soloviev and Nicholas Asher, editors, Logical Aspects
of Computational Linguistics. Springer.

Alexander Clark. 2011. A language theoretic approach
to syntactic structure. In Makoto Kanazawa, András
Kornai, Marcus Kracht, and Hiroyuki Seki, editors,
The Mathematics of Language, pages 39–56. Springer
Berlin Heidelberg.

Alexander Clark. 2013. The syntactic concept lat-
tice: Another algebraic theory of the context-free lan-
guages? Journal of Logic and Computation.

Alexander Clark. 2014. Learning trees from strings:
A strong learning algorithm for some context-free
grammars. Journal of Machine Learning Research,
14:3537–3559.

B. A. Davey and H. A. Priestley. 2002. Introduction to
Lattices and Order. Cambridge University Press.

J. Kunze. 1967–1968. Versuch eines objektivierten
Grammatikmodells I, II. Z. Zeitschriff Phonetik
Sprachwiss. Kommunikat, 20-21.

Hans Leiß. 2014. Learning context free grammars with
the finite context property: A correction of A.Clark’s
algorithm. In Glyn Morrill, Reinhard Muskens, Rainer
Osswald, and Frank Richter, editors, Formal Gram-
mar, volume 8612 of Lecture Notes in Computer Sci-
ence, pages 121–137. Springer Berlin Heidelberg.

S Marcus. 1994. The status of research in the field of
analytical algebraic models of language. In Carlos
Martı́n-Vide, editor, Current Issues in Mathematical
Linguistics, pages 3–23. Elsevier.

H. Seki, T. Matsumura, M. Fujii, and T. Kasami. 1991.
On multiple context-free grammars. Theoretical Com-
puter Science, 88(2):229.

Christian Wurm. 2012. Completeness of full Lambek
calculus for syntactic concept lattices. In Proceed-
ings of the 17th conference on Formal Grammar 2012
(FG).

R. Yoshinaka. 2011. Efficient learning of multiple
context-free languages with multidimensional substi-
tutability from positive data. Theoretical Computer
Science, 412(19):1821 – 1831.

R. Yoshinaka. 2012a. Integration of the dual approaches
in the distributional learning of context-free grammars.

In Adrian-Horia Dediu and Carlos Martı́n-Vide, ed-
itors, Language and Automata Theory and Applica-
tions, volume 7183 of Lecture Notes in Computer Sci-
ence, pages 538–550. Springer Berlin Heidelberg.

Ryo Yoshinaka. 2012b. Integration of the dual ap-
proaches in the distributional learning of context-free
grammars. In Adrian-Horia Dediu and Carlos Martı́n-
Vide, editors, Language and Automata Theory and Ap-
plications, volume 7183 of Lecture Notes in Computer
Science, pages 538–550. Springer Berlin Heidelberg.

111


