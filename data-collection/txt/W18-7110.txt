




































Feedback Strategies for Form and Meaning
in a Real-life Language Tutoring System

Ramon Ziai Björn Rudzewitz

Kordula De Kuthy Florian Nuxoll Detmar Meurers

Collaborative Research Center 833
Department of Linguistics, ICALL Research Group∗

LEAD Graduate School & Research Network
University of Tübingen

Abstract

We describe ongoing work on an English lan-

guage tutoring system currently being used

as part of regular instruction in twelve Ger-

man high school classes. In contrast to the

traditional ICALL system approach analyzing

learner language, we build on the approach

of Rudzewitz et al. (2018) to generate vari-

ants of target answers based on task and tar-

get language models and combine this offline

step with an online process flexibly matching

learner answers with these variants. We ex-

tend the approach by advancing the search en-

gine used in the online step to return more rel-

evant results. Then we extend the approach

to meaning-focused feedback, showing how it

can be realized in the system in addition to the

form-focused feedback. We conclude with an

outlook on an intervention study we have de-

signed to evaluate the system.

1 Introduction

Second Language Acquisition (SLA) has long

recognized the need for immediate feedback on

learner production (Mackey, 2006). However, in

real-life classrooms, there is limited opportunity

for such immediate feedback if every student is to

be considered according to her needs.

Intelligent Language Tutoring Systems make it

possible to address this shortcoming since they of-

fer the possibility of automated, immediate feed-

back while the learner is working on the task, and

many students can use the system at the same time

whenever they want to, whereas opportunities for

interaction with a teacher or other tutor are much

more limited.

However, in order to provide accurate, help-

ful feedback, the erroneous forms produced by

learners need to be characterized. If one analyzes

∗ http://icall-research.de

learner language directly, one runs into the prob-

lem that state-of-the-art NLP is not equipped to

deal with non-standard language in a way that sup-

ports fine-grained feedback. This is not surpris-

ing given that the linguistic categories system was

developed for well-formed, native language, thus

NLP tools generally treat the analysis of learner

language as a robustness problem, covering up the

type of deviation or error that the learner produced

instead of characterizing it (Dı́az Negrillo et al.,

2010; Meurers and Dickinson, 2017). As an ex-

ample, consider that a standard POS tagger would

typically assign the tag VBD to the overregularized

form teached based on the suffix analysis fallback

strategy commonly used for unknown words.

If we know what task the learner language was

produced for, this challenge can be addressed to

some degree: instead of analyzing the learner pro-

ductions directly, one can start out from the ex-

pected target forms and systematically transform

them into well-formed and ill-formed variations of

the target (Rudzewitz et al., 2018).

In this paper, we expand on that idea and present

feedback strategies supporting both form- and

meaning-oriented tasks. After reviewing the pro-

cess responsible for generating the well-formed

and ill-formed variation, we zoom in on the search

process executed at feedback time, outlining how

standard search engine technology was adapted to

serve the needs of a language tutoring system. We

show how the same basis of generated variation

can support meaning-oriented feedback, using an

alignment-based approach inspired by research on

Short Answer Assessment (Meurers et al., 2011).

We demonstrate this with feedback for reading

and listening comprehension where the learner is

pointed to the relevant source of information in

This work is licensed under a Creative Commons
Attribution 4.0 International Licence. Licence details:
http://creativecommons.org/licenses/by/4.0/.

Ramon Ziai, Bjoern Rudzewitz, Kordula De Kuthy, Florian Nuxoll and Detmar Meurers 2018. Feedback

strategies for form and meaning in a real-life language tutoring system. Proceedings of the 7th Workshop

on NLP for Computer Assisted Language Learning at SLTC 2018 (NLP4CALL 2018). Linköping Electronic

Conference Proceedings 152: 91–98.

91



the task material. We conclude with an outlook

on the design of an intervention study we are cur-

rently running in twelve 7th grade classrooms in

Germany.

2 System Setup

The feedback strategies discussed in this article

are implemented as part of a web-based online

workbook, the FeedBook (Rudzewitz et al., 2018;

Meurers et al., 2018). The foreign language tutor-

ing system is an adaptation of a paper workbook

for a 7th grade English textbook approved for use

in German high schools.

Figure 1 provides an authentic example of a stu-

dent solution to an exercise in the printed work-

book on the use of type II conditionals. For such

paper-based exercises, feedback is typically given

in a delayed fashion by the teacher, when dis-

cussing the exercise summarily in class or some-

times by returning marked-up exercise sheets, not

while the student is actually thinking about and

working on the task.

In contrast, the system we describe provides an

interface for students to select and interactively

work on exercises. For exercises that aim at teach-

ing grammar topics, students receive automatic,

immediate feedback by the system informing them

whether their answer is correct (via a green check

mark) or why their answer is incorrect (via red

color, highlighting of the error span, and a meta-

linguistic feedback message). In fact, rather than

pointing out the error as such, we instead for-

mulate scaffolding feedback messages designed

to guide the learner towards the solution, without

giving it away.

The process of entering an answer and receiv-

ing feedback can be repeated, incrementally lead-

ing the student to the correct answer. If there are

multiple errors in a learner response, the system

presents the feedback one at a time. Figure 2

shows the same learner production we saw in Fig-

ure 1 together with the interactive feedback imme-

diately provided by the system after this is typed

in.

Students can save and resume work, interact

with the system to receive automatic feedback and

revise their answers, and eventually submit their

final solutions to the teacher. In case the an-

swers in a given exercise are all correct, the sys-

tem grades the submission automatically, without

requiring teacher interaction.

For those answers that are not correct with re-

spect to a given target answer, the teacher can

manually annotate the learner answer with feed-

back parallel to the traditional mark-up process

known from printed workbooks. Any such man-

ual feedback is saved in a feedback memory and

suggested automatically to the teacher in case the

form occurs in another learner response to this ex-

ercise.

The system also provides students with au-

tomatic, immediate feedback for many exercise

types, where they traditionally would either not re-

ceive it or only after long delay resulting from col-

lecting and manually marking up homework as-

signments. From the teacher’s perspective, the

system relieves them from very repetitive and

time-consuming work. The exercises are embed-

ded in a full web application with a messaging

system for communication, a profile management

including e-mail settings, tutorials for using the

system, classroom management, and various func-

tions orthogonal to the NLP-related issues.

3 Hypothesis Generation Revisited

The generation of well-formed and ill-formed an-

swers expected for a given exercise builds on

the generation framework proposed by Rudzewitz

et al. (2018) which generate variants of target an-

swers for each task that one wants to provide feed-

back for.

The crucial components of the framework are i)

a set of rules organized in layers that transform one

variant to another variant, introducing one change

at a time, ii) a common representation format for

adding, removing and querying units of linguistic

analysis (the CAS, see Götz and Suhre 2004), and

iii) a breadth-first search algorithm that traverses

the rule layers, applying rules and passing the out-

put variants of rules to rules in the next layer along

with their linguistic analysis.

The setup consists of four layers: in the first

layer normalizations like contractions are per-

formed. In the second layer transformations are

conducted that yield linguistically well-formed,

but task-inappropriate forms like tense changes.

As the next step, the third layer introduces changes

that result in morphologically ill-formed answers,

for example regular endings for irregular verbs

in the simple past. Finally, the fourth layer re-

joins and normalizes different generated variants.

Not every layer introduces new diagnoses: for

Proceedings of the 7th Workshop on NLP for Computer Assisted Language Learning at SLTC 2018 (NLP4CALL 2018)

92



Figure 1: Traditional paper-based exercise

Figure 2: Interactive exercise with form-oriented feedback

Proceedings of the 7th Workshop on NLP for Computer Assisted Language Learning at SLTC 2018 (NLP4CALL 2018)

93



the normalization rules, the previous diagnoses are

passed on. At each point, the current variant and

corresponding analysis is saved so they can be

used later for feedback. The system (at the time

of publication) generated 95.386 distinct hypothe-

ses for 3.211 target answers.

Table 1 provides some example derivations that

result from rule interactions.

4 The Search Mechanism

Given the generation approach outlined in the pre-

vious section, it should come as no surprise that

especially for short-answer tasks such as the one

in Figure 2, the number of generated variants can

get very large. This is especially true for items

with multiple target answers given that a separate

calculation is done for every target answer.

When a learner uses the system and triggers the

feedback mechanism for a given item, it is neces-

sary to compare the learner answer to the relevant

pre-stored generated variants, determine whether

the student made one of the errors present (and

thus known to the system) in the variant, and if

so, provide feedback. Since it is infeasible to tra-

verse and compare all variants, Rudzewitz et al.

(2018) use the search engine framework Lucene1

to efficiently index and query the stored variants.

Every variant is treated like a document indexed

by Lucene.

In examining the feedback behavior of such a

system, we noticed that Lucene did not always re-

turn the most relevant variant for a given learner

answer and task. Looking further into this is-

sue, we discovered that this behavior was due to

the term weighting scheme used by Lucene and

other search engines, known as TF-IDF (Salton

and McGill, 1983). TF-IDF works by balancing

the frequency of a word in a given document (TF)

against the inverse frequency of the word occur-

ring over the whole set of documents (IDF), result-

ing in low values for very frequent words, and high

values for topic-specific words only occurring in

few documents.

While this is the desired behavior when look-

ing for specific content, it is not suitable for the

present problem of finding relevant variants for

learner answers. We therefore modified the ap-

proach of Rudzewitz et al. (2018) by i) eliminat-

ing the IDF part of the weighting scheme, and ii)

1https://lucene.apache.org/core/

introducing task-specific term weighting into the

search.

In order to realize the latter, we draw on in-

formation gathered during the generation process.

We always store the transformation result r of a

rule application, i.e., the part of a variant which

was changed by the rule. So the set of all trans-

formation results R is known before the learner

interacts with the system. We can thus look for

instances of each r ∈ R of a given task and item
(such as the incorrect tense forms shown in Ta-

ble 1) in the learner answer and assign a higher

weight for parts of the learner answer that match

r. The weighting is implemented using a Lucene

feature called “query boosting”, which allows for

assignment of different weights to sub-strings of

the query. We use the same weight for all matches

(currently 5.0), whereas the non-matched answer
parts receive the standard weight of 1.0.

As a result of this modification, the system is

able to give more task-relevant feedback for the

learner answer in Figure 2 despite a low token

overlap of the learner answer with the target an-

swer. In order to also obtain a quantitative re-

sult, we ran the new search mechanism against

the same data used for coverage testing in Rudze-

witz et al. (2018): we observed a 6% increase in

types of answers covered by construction-specific

feedback (16.9% / 1085 instances vs. 10.9% / 696

instances) as a consequence of the search mecha-

nism introduced above.

5 Meaning-oriented Feedback

Depending on the nature of the exercise, it is

essential to draw the learners’ attention not just

to forms but also to content-related misconcep-

tions. Indeed, for meaning-based exercises such

as listening- or reading comprehension, feedback

on meaning should take priority over feedback on

form. The strategies needed to detect such errors

are very different from the ones used for the form-

oriented feedback described so far. In contrast

to analyzing or generating variation in form, one

needs to abstract over it and recognize meaning

equivalence of different forms. A learner answer

can then be accepted as correct whenever meaning

equivalence has been established between it and

the target answer.

There is a vast body of work on automated

short-answer grading (see Burrows et al. 2015 for

an overview), but the overwhelming majority of

Proceedings of the 7th Workshop on NLP for Computer Assisted Language Learning at SLTC 2018 (NLP4CALL 2018)

94



target layer 1 layer 2 layer 3
are you doing are you doing are you doing are you doing

were you doing were you do was you do
have you been doing have you been do have you been dos
had you been doing had you been do had you been dos
will you do are you do will you dos
did you do . . . did you dos
. . . are you dos

was you doing
is you dos
is you doing
. . .

friendlier friendlier friendlier friendlier
more friendly more friendlier most friendlier
friendlyer more friendlyer most friendlyer
. . . friendliest

. . . friendlyest
. . .

Table 1: Examples for generated answer variants

work only lends itself to the task of holistically

scoring learner answers, not detecting the type

of divergence from target answers. We chose to

adapt the alignment-based CoMiC system (Meur-

ers et al., 2011) to our needs. Instead of classi-

fying learner productions, we use the alignment

information from CoMiC as evidence for equiva-

lence or divergence (e.g., missing information) of

the learner answer from the target answer.

Given the means of detecting meaning errors,

the question arises how to point the student in the

right direction. Since it is pedagogically not ac-

ceptable to reveal (parts of) the correct answer,

an alternate means of scaffolding for meaning-

oriented exercises such as reading and listening

comprehension is needed. How can this be done?

Our general approach is to draw the learner’s at-

tention to relevant parts of the task context. This

can be a part of the reading text or listening clip,

the question being asked, or the instruction text.

Figure 3 shows feedback on a learner answer with

missing information. The system reacts to the

problem by visually highlighting the relevant part

of the reading text, pointing the learner into the

direction of the correct answer.

For listening comprehension exercises, the

overall strategy is the same, but instead of high-

lighting or displaying text, we provide an ex-

cerpt of the corresponding audio clip that contains

the information necessary for answering the given

question. Figure 4 illustrates an example for such

feedback. Since the current number of suitable

tasks in FeedBook is limited, a teacher from the

project team manually specified the relevant part

of the task context for each task. In the future, we

plan to automatically identify these information

sources in reading texts or transcripts of listening

texts. Furthermore, we are in the process of com-

piling a test suite for meaning-oriented feedback

in order to quantitatively evaluate our approach.

6 Summary

We presented extensions to the language tutor-

ing system FeedBook currently in use in English

7th grade classrooms. The extensions are i) a

task-based optimization of the search strategy nec-

essary when comparing learner answers to pre-

stored variants and ii) the addition of meaning-

oriented scaffolding feedback for reading and lis-

tening activities. We demonstrated both exten-

sions by example. The first extension shows that

if the task is known and target answers exist, it is

possible to give accurate feedback on learner lan-

guage without having to directly process it. The

second extension makes it possible to give help-

ful, pedagogically sound scaffolding feedback on

meaning-oriented tasks.

7 Outlook: Towards a Large-Scale

Intervention Study

Moving forward, it will be necessary to evaluate

the effectiveness of the system in terms of learn-

ing outcomes. Very few ICALL systems have been

evaluated in real-life formal learning contexts (for

some notable exceptions, cf. Nagata, 1996; Heift,

2004, 2010; Choi, 2016), let alone in terms of stan-

dards for intervention studies established in psy-

chology and empirical educational science. How-

ever, in order to raise awareness for and show the

impact of ICALL systems, it arguably is crucial

Proceedings of the 7th Workshop on NLP for Computer Assisted Language Learning at SLTC 2018 (NLP4CALL 2018)

95



Figure 3: Meaning-oriented feedback for reading comprehension exercise

Figure 4: Meaning-oriented feedback for listening comprehension exercise

Proceedings of the 7th Workshop on NLP for Computer Assisted Language Learning at SLTC 2018 (NLP4CALL 2018)

96



to provide large-scale evaluation in terms of exter-

nally established measures of learning outcomes.

In our case, we want to measure the impact of in-

teractive feedback on the individual learning out-

comes of 7th grade school children.

We have set up a randomized controlled field

study that compares two groups of students re-

ceiving immediate feedback on different grammat-

ical constructions throughout the current school

year. The variables that are relevant to control

in such a context include: a) the learners’ lan-

guage proficiency, b) individual differences in ap-

titude/cognition, c) motivational factors, and, last

but far from least, d) the teacher, known to have

the strongest influence on learning outcome in

classrooms.

For a), we plan to administer both a C-Test

measuring general language proficiency as well

as a construction-specific grammar test geared to-

wards testing grammar topics that are part of the

7th grade English curriculum. When piloting the

grammar test, we observed that conducting a sys-

tematic pre-test of all constructions at the begin-

ning of the school year, before the students have

covered these constructions in class, is very time

consuming and leads to significant student frus-

tration. Students are not used to being tested on

material they have not systematically covered in

school yet. So for the main study, we are distribut-

ing the pre-tests of the grammatical constructions

throughout the school year to just before the spe-

cific construction is being covered in class.

In order to control for b), we will employ estab-

lished individual difference tests such as MLAT-

5 (Carroll and Sapon, 1959) to determine fixed

traits of learners, such as working memory capac-

ity. For motivation and other background traits (c),

we will use a questionnaire where students answer

a range of questions on the subject they learn, the

languages they speak, and other relevant informa-

tion. Originally, we had planned to administer all

these tests using our web-based platform. To en-

sure that these tests are conducted systematically,

this is supposed to happen in class.

It turns out, however, that in the current state

of the German secondary school system, the over-

head of scheduling classes in computer rooms pro-

viding a sufficient number of computers that are

functional and connected to the Internet is a sig-

nificant burden for teachers. Conducting tests on

paper, on the other hand, means having to manu-

ally enter the data later, which for studies of this

size is very work intensive and error prone. For

some of the individual difference tests, it is possi-

ble, though, to let students complete them at home

using the digital device they also use to access the

tutoring system. In pilot testing some tests in such

a way outside of class, we found that in such a set-

ting it is very difficult to ensure that all students ac-

tually complete the tests. To enforce completion,

in the main study we are only making the interac-

tive online exercises for the next chapter available

in the tutoring system once the tests scheduled at

that point have been completed by a student.

To account for the teacher factor (d), the in-

tervention study uses within-class randomization.

We divided the grammar topics in the curricu-

lum into two groups and assign students randomly

to one of these groups. Students get immediate

system feedback on the constructions assigned to

their group, while not receiving automated feed-

back on the other grammar topics. Both groups

thus receive feedback from the system, but sys-

tematically for different constructions. If the inter-

active feedback is effective, the two student groups

should differ in their performance on the dif-

ferent grammatical construction and general lan-

guage proficiency posttest. Except for the presum-

ably stable traits, such as working memory and

the background and motivation questionnaires, all

tests are administered following a pre-/posttest de-

sign.

In addition to the twelve test classes with

within-class randomization, we also recruited

a separate class as a business-as-usual control,

where the traditional paper workbook is used and

only the tests are administered. We intentionally

did not make the comparison with business-as-

usual the main focus of our study since we want

to determine the effect of interactive scaffolding

feedback on learning, not the well-known newness

effect of using a web-based computer system in

comparison to a paper-based workbook.

Acknowledgements

We are grateful to the high school students, par-

ents and teachers using the FeedBook system and

providing much useful feedback. We would also

like to thank the reviewers for their detailed and

helpful comments.

Proceedings of the 7th Workshop on NLP for Computer Assisted Language Learning at SLTC 2018 (NLP4CALL 2018)

97



References

Steven Burrows, Iryna Gurevych, and Benno Stein.
2015. The eras and trends of automatic short an-
swer grading. International Journal of Artificial In-
telligence in Education, 25(1):60–117.

John B. Carroll and Stanley M. Sapon. 1959. Modern
language aptitude test. Psychological Corporation,
San Antonio, TX, US.

Inn-Chull Choi. 2016. Efficacy of an ICALL tutoring
system and process-oriented corrective feedback.
Computer Assisted Language Learning, 29(2):334–
364.

Ana Dı́az Negrillo, Detmar Meurers, Salvador Valera,
and Holger Wunsch. 2010. Towards interlanguage
POS annotation for effective learner corpora in SLA
and FLT. Language Forum, 36(1–2):139–154.

Thilo Götz and Oliver Suhre. 2004. Design and im-
plementation of the uima common analysis system.
IBM Systems Journal, 43(3):476–489.

Trude Heift. 2004. Corrective feedback and learner up-
take in call. ReCALL, 16(2):416–431.

Trude Heift. 2010. Prompting in CALL: A longitudinal
study of learner uptake. Modern Language Journal,
94(2):198–216.

Alison Mackey. 2006. Feedback, noticing and in-
structed second language learning. Applied Linguis-
tics, 27(3):405–430.

Detmar Meurers and Markus Dickinson. 2017. Ev-
idence and interpretation in language learning re-
search: Opportunities for collaboration with compu-
tational linguistics. Language Learning, 67(2).

Detmar Meurers, Kordula De Kuthy, Verena Möller,
Florian Nuxoll, Björn Rudzewitz, and Ramon Ziai.
2018. Digitale Differenzierung benötigt Informatio-
nen zu Sprache, Aufgabe und Lerner. Zur Gener-
ierung von individuellem Feedback in einem inter-
aktiven Arbeitsheft [Digitial differentiation requires
information on language, task, and learner. On the
generation of individual feedback in an interactive
workbook]. FLuL – Fremdsprachen Lehren und
Lernen, 47(2):64–82.

Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey
Bailey. 2011. Integrating parallel analysis modules
to evaluate the meaning of answers to reading com-
prehension questions. IJCEELL. Special Issue on
Automatic Free-text Evaluation, 21(4):355–369.

Noriko Nagata. 1996. Computer vs. workbook instruc-
tion in second language acquistion. CALICO Jour-
nal, 14(1):53–75.

Björn Rudzewitz, Ramon Ziai, Kordula De Kuthy, Ver-
ena Möller, Florian Nuxoll, and Detmar Meurers.
2018. Generating feedback for English foreign lan-
guage exercises. In Proceedings of the 13th Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications (BEA), pages 127–136. ACL.

Gerard Salton and Michael J. McGill. 1983. Introduc-
tion to modern information retrieval. McGraw-Hill,
New York.

Proceedings of the 7th Workshop on NLP for Computer Assisted Language Learning at SLTC 2018 (NLP4CALL 2018)

98


