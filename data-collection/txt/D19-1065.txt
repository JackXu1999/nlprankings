



















































Neural Naturalist: Generating Fine-Grained Image Comparisons


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 708‚Äì717,
Hong Kong, China, November 3‚Äì7, 2019. c¬©2019 Association for Computational Linguistics

708

Neural Naturalist: Generating Fine-Grained Image Comparisons

Maxwell Forbes
ü¶â

Christine Kaeser-Chen
ü¶ú

Piyush Sharma
ü¶ú

Serge Belongie
ü¶úüïä

ü¶â

University of Washington
ü¶ú

Google Research
üïä

Cornell University and Cornell Tech
mbforbes@cs.washington.edu

{christinech,piyushsharma}@google.com
sjb344@cornell.edu

https://mbforbes.github.io/neural-naturalist/

Abstract

We introduce the new Birds-to-Words dataset
of 41k sentences describing fine-grained dif-
ferences between photographs of birds. The
language collected is highly detailed, while
remaining understandable to the everyday
observer (e.g., ‚Äúheart-shaped face,‚Äù ‚Äúsquat
body‚Äù). Paragraph-length descriptions natu-
rally adapt to varying levels of taxonomic and
visual distance‚Äîdrawn from a novel strati-
fied sampling approach‚Äîwith the appropriate
level of detail. We propose a new model called
Neural Naturalist that uses a joint image en-
coding and comparative module to generate
comparative language, and evaluate the results
with humans who must use the descriptions to
distinguish real images.

Our results indicate promising potential for
neural models to explain differences in visual
embedding space using natural language, as
well as a concrete path for machine learning to
aid citizen scientists in their effort to preserve
biodiversity.

1 Introduction

Humans are adept at making fine-grained compar-
isons, but sometimes require aid in distinguishing
visually similar classes. Take, for example, a cit-
izen science effort like iNaturalist,1 where every-
day people photograph wildlife, and the commu-
nity reaches a consensus on the taxonomic label
for each instance. Many species are visually sim-
ilar (e.g., Figure 1, top), making them difficult for
a casual observer to label correctly. This puts an
undue strain on lieutenants of the citizen science
community to curate and justify labels for a large
number of instances. While everyone may be ca-
pable of making such distinctions visually, non-
experts require training to know what to look for.

ü¶â

Work done during an internship at Google.
1https://www.inaturalist.org

‚ÄúAnimal 2 looks smaller and has a stouter, darker bill than Animal 
1. Animal 2 has black spots on its wings. Animal 2 has a black 
hood that extends down onto its breast, and the rest of its breast is 
white with orange only on its sides. In comparison, Animal 1‚Äôs 
breast is entirely orange.‚Äù

‚ÄúAnimal 2 is brightly red-colored all over, except for a black oval 
around its beak. Animal 1 has more muted red and grey colors.‚Äù

vs

perceptual difficulty: medium

hi
gh

ly
 d

et
ai

le
d

fe
w

er
 d

et
ai

ls

body partdescriptive phrase

perceptual difficulty: high

Animal 1 Animal 1

Animal 1 Animal 1

Animal 2 Animal 2

Animal 2 Animal 2

vs

Figure 1: The Birds-to-Words dataset: comparative de-
scriptions adapt naturally to the appropriate level of de-
tail (orange underlines). A difficult distinction (TOP) is
given a longer and more fined-grained comparison than
an easier one (BOTTOM). Annotators organically use
everyday language to refer to parts (green highlights).

Field guides exist for the purpose helping peo-
ple learn how to distinguish between species. Un-
fortunately, field guides are costly to create be-
cause writing such a guide requires expert knowl-
edge of class-level distinctions.

In this paper, we study the problem of explain-
ing the differences between two images using nat-
ural language. We introduce a new dataset called
Birds-to-Words of paragraph-length descriptions
of the differences between pairs of bird pho-
tographs. We find several benefits from eliciting
comparisons: (a) without a guide, annotators nat-
urally break down the subject of the image (e.g.,

https://mbforbes.github.io/neural-naturalist/


709

a bird) into pieces understood by the everyday ob-
server (e.g., head, wings, legs); (b) by sampling
comparisons from varying visual and taxonomic
distances, the language exhibits naturally adap-
tive granularity of detail based on the distinctions
required (e.g., ‚Äúred body‚Äù vs ‚Äútiny stripe above
its eye‚Äù); (c) in contrast to requiring comparisons
between categories (e.g., comparing one species
vs. another), non-experts can provide high-quality
annotations without needing domain expertise.

We also propose the Neural Naturalist model
architecture for generating comparisons given two
images as input. After embedding images into a
latent space with a CNN, the model combines the
two image representations with a joint encoding
and comparative module before passing them to a
Transformer decoder. We find that introducing a
comparative module‚Äîan additional Transformer
encoder‚Äîover the combined latent image repre-
sentations yields better generations.

Our results suggest that these classes of neural
models can assist in fine-grained visual domains
when humans require aid to distinguish closely
related instances. Non-experts‚Äîsuch as amateur
naturalists trying to tell apart two species‚Äîstand
to benefit from comparative explanations. Our
work approaches this sweet-spot of visual exper-
tise, where any two in-domain images can be com-
pared, and the language is detailed, adaptive to
the types of differences observed, and still under-
standable by laypeople.

Recent work has made impressive progress on
context sensitive image captioning. One direction
of work uses class labels as context, with the ob-
jective of generating captions that distinguish why
the image belongs to one class over others (Hen-
dricks et al., 2016; Vedantam et al., 2017). An-
other choice is to use a second image as context,
and generate a caption that distinguishes one im-
age from another. Previous work has studied ways
to generalize single-image captions into compar-
ative language (Vedantam et al., 2017), as well
as comparing two images with high pixel overlap
(e.g., surveillance footage) (Jhamtani and Berg-
Kirkpatrick, 2018). Our work complements these
efforts by studying directly comparative, everyday
language on image pairs with no pixel overlap.

Our approach outlines a new way for models
to aid humans in making visual distinctions. The
Neural Naturalist model requires two instances as
input; these could be, for example, a query image

‚Ä¶

‚Ä¶

‚Ä¶

increasing visual and taxonom
ic distance

sampling cut off
at taxonomic CLASS

pivot species
sampled subtree

too distant

pivot pivot
visual visual

species species

genus genus

order order

Figure 2: Illustration of pivot-branch stratified sam-
pling algorithm used to construct the Birds-to-
Words dataset. The algorithm harnesses visual and
taxonomic distances (increasing vertically) to create a
challenging task with board coverage.

and an image from a candidate class. By differ-
entiating between these two inputs, a model may
help point out subtle distinctions (e.g., one animal
has spots on its side), or features that indicate a
good match (e.g., only a slight difference in size).
These explanations can aid in understanding both
differences between species, as well as variance
within instances of a single species.

2 Birds-to-Words Dataset

Our goal is to collect a dataset of tuples (i1, i2, t),
where i1 and i2 are images, and t is a natural lan-
guage comparison between the two. Given a do-
main D, this collection depends critically on the
criteria we use to select image pairs.

If we sample image pairs uniformly at random,
we will end up with comparisons encompassing
a broad range of phenomena. For example, two
images that are quite different will yield categor-
ical comparisons (‚ÄúOne is a bird, one is a mush-
room.‚Äù). Alternatively, if the two images are very
similar, such as two angles of the same creature,



710

Images
Dataset Domain Lang Ctx Cap Example

CUB Captions
(R, 2016)

Birds M 1 1 ‚ÄúAn all black bird with a very long rectrices and relatively dull
bill.‚Äù

CUB-Justify
(V, 2017)

Birds S 7 1 ‚ÄúThe bird has white orbital feathers, a black crown, and yellow
tertials.‚Äù

Spot-the-Diff
(J&B, 2018)

Surveilance E 2 1‚Äì2 ‚ÄùSilver car is gone. Person in a white t shirt appears. 3rd person
in the group is gone.‚Äù

Birds-to-Words
(this work)

Birds E 2 2 ‚ÄúAnimal1 is gray, while animal2 is white. Animal2 has a long,
yellow beak, while animal1‚Äôs beak is shorter and gray. Animal2
appears to be larger than animal1.‚Äù

Table 1: Comparison with recent fine-grained language-and-vision datasets. Lang values: S = scientific, E =
everyday, M = mixed. Images Ctx = number of images shown, Images Cap = number of images described in
caption. Dataset citations: R = Reed et al., V = Vedantam et al., J&B = Jhamtani and Berg-Kirkpatrick.

comparisons between them will focus on highly
detailed nuances, such as variations in pose. These
phenomena support rich lines of research, such as
object classification (Deng et al., 2009) and pose
estimation (Murphy-Chutorian and Trivedi, 2009).

We aim to land somewhere in the middle. We
wish to consider sets of distinguishable but inti-
mately related pairs. This sweet spot of visual
similarity is akin to the genre of differences stud-
ied in fine-grained visual classification (Wah et al.,
2011; Krause et al., 2013). We approach this col-
lection with a two-phase data sampling procedure.
We first select pivot images by sampling from our
full domain uniformly at random. We then branch
from these images into a set of secondary im-
ages that emphases fine-grained comparisons, but
yields broad coverage over the set of sensible re-
lations. Figure 2 provides an illustration of our
sampling procedure.

2.1 Domain
We sample images from iNaturalist, a citizen sci-
ence effort to collect research-grade2 observations
of plants and animals in the wild. We restrict
our domain D to instances labeled under the taxo-
nomic CLASS3 Aves (i.e., birds). While a broader
domain would yield some comparable instances
(e.g., bird and dragonfly share some common body
parts), choosing only Aves ensures that all in-
stances will be similar enough structurally to be
comparable, and avoids the gut reaction compar-

2Research-grade observations have met or exceeded iNat-
uralist‚Äôs guidelines for community consensus of the taxo-
nomic label for a photograph.

3To disambiguate class, we use CLASS to denote the tax-
onomic rank in scientific classification, and simply ‚Äúclass‚Äù to
refer to the machine learning usage of the term as a label in
classification.

Birds-to-Words

Birds-to-Words Dataset

Image pairs 3,347
Paragraphs / pair 4.8
Paragraphs 16,067
Tokens / paragraph 32.1 MEAN

Sentences 40,969
Sentences / paragraph 2.6 MEAN

Clarity rating ‚â• 4/5
Train / dev / test 80% / 10% / 10%

Figure 3: Annotation lengths for compared datasets
(TOP), and statistics for the proposed Birds-to-
Words dataset (BOTTOM). The Birds-to-Words dataset
has a large mass of long descriptions in comparison to
related datasets.

ison pointing out the differences in animal type.
This choice yields 1.7M research-grade images
and corresponding taxonomic labels from iNatu-
ralist. We then perform pivot-branch sampling on
this set to choose pairs for annotation.

2.2 Pivot Images

The Aves domain in iNaturalist contains instances
of 9k distinct species, with heavy observation bias
to more common species (such as the mallard
duck). We uniformly sample species from the set
of 9k to help overcome this bias. In total, we select
405 species and corresponding photographs to use
as i1 images.



711

Elementwise Mult.

‚äô1E 2E M=

‚Ä¶

‚Ä¶

D1

Di

Joint
Encoding

Image Embedding Decoding

Passthrough

Mutation

Comparative
Module

‚ÄúAnimal 2 has a blue back
and wings, while Animal 1 ‚Ä¶‚Äù

i1

i2

CNN
Dense features

1E 1E
Multi-Head
Attention

Embed

Per-token cross-entropy loss

Reshaped

Cropping, Adjustments

Transformer Decoder
Layer i

Transformer Decoder
Layer 1

Transformer Decoder
 Layer n

SoftmaxLinear

2E

2E M

Maxpool

max( =

Addition

1E 2E M=+

Subtraction

1E 2E M=- ),1E 2E M

J

Passthrough

‚Ä¶

J

‚Ä¶

Transformer Encoder
variants

variants

detail

Transformer
Encoder

Layer 1 Layer N

C

C

J

1 CN

X

DN

C

Multi-Head Attention

Figure 4: The proposed Neural Naturalist model architecture. The multiplicative joint encoding and Transformer-
based comparative module yield the best comparisons between images.

2.3 Branching Images

We use both a visual similarity measure and tax-
onomy to sample a set of comparison images i2
branching off from each pivot image i1. We use a
branching factor of k = 12 from each pivot image.

To capture visually similar images to i1, we
employ a similarity function V(i1, i2). We use
an Inception-v4 (Szegedy et al., 2017) network
pretrained on ImageNet (Deng et al., 2009) and
then fine-tuned to perform species classification
on all research-grade observations in iNaturalist.
We take the embedding for each image from the
last layer of the network before the final softmax.
We perform a k-nearest neighbor search by quan-
tizing each embedding and using L2 distance (Wu
et al., 2017; Guo et al., 2016), selecting the kv = 2
closest images in embedding space.

We also use the iNaturalist scientific taxonomy
T (D) to sample images at varying levels of taxo-
nomic distance from i1. We select kt = 10 tax-
onomically branched images by sampling two im-
ages each from the same SPECIES (` = 1), GENUS,
FAMILY, ORDER, and CLASS (` = 5) as c. This
yields 4,860 raw image pairs (i1, i2).

2.4 Language Collection

For each image pair (i1, i2), we elicit five natu-
ral language paragraphs describing the differences
between them.

An annotator is instructed to write a paragraph
(usually 2‚Äì5 sentences) comparing and contrasting

the animal appearing in each image. We instruct
annotators not to explicitly mention the species
(e.g., ‚ÄúAnimal 1 is a penguin‚Äù), and to instead fo-
cus on visual details (e.g., ‚ÄúAnimal 1 has a black
body and a white belly‚Äù). They are additionally
instructed to avoid mentioning aspects of the back-
ground, scenery, or pose captured in the photo-
graph (e.g., ‚ÄúAnimal 2 is perched on a coconut‚Äù).

We discard all annotations for an image pair
where either image did not have at least 45 pos-
itive ratings of image clarity. This yields a to-
tal of 3,347 image pairs, annotated with 16,067
paragraphs. Detailed statistics of the Birds-to-
Words dataset are shown in Figure 3, and exam-
ples are provided in Figure 5. Further details
of our both our algorithmic approach and dataset
construction are given in Appendices A and B.

3 Neural Naturalist Model

Task Given two images (i1, i2) as input, our
task is to generate a natural language paragraph
t = x1 . . . xn that compares the two images.

Architecture Recent image captioning ap-
proaches (Xu et al., 2015; Sharma et al., 2018)
extract image features using a convolutional neu-
ral network (CNN) which serve as input to a lan-
guage decoder, typically a recurrent neural net-
work (RNN) (Mikolov et al., 2010) or Transformer
(Vaswani et al., 2017). We extend this paradigm
with a joint encoding step and comparative mod-
ule to study how best to encode and transform



712

animal1 is brown and white with a short 
beak . animal2 is brown and gray with a 
long gray beak .
animal1 is white with dark brown and 
white wings and a golden head . animal2 
is brown-gold with dark solid-colored 
brown wings and a dark head .

MM

GG

animal1 is a dull yellow with grey tail 
feathers while animal2 is a yellow-green

animal1 has dark orange claws , while animal2 
has grey claws . animal1 has yellow coloring 
with black on the top of the head and in tiny 
wing patches . animal2 is mostly green with 
red on the neck and brown on the wings .

MM

GG

animal1 has a black beak , while animal2 has a 
pale grey beak . animal1 is mostly black , while 
animal2 is mostly dark brown and white . animal1 
has a black eye , while animal2 has a gold eye .
animal1 is covered in black feathers , while 
animal2 has light grey abdomen and chest with 
variety of dark brown and light brown feathers 
over wings , back and head .

MM

GG

Animal 1 Animal 1 Animal 1 Animal 1Animal 2 Animal 2Animal 2 Animal 2

Animal 2 Animal 2

Animal 1 Animal 1

Animal 1 Animal 1
animal1 is two - toned brown with a 
white patch on its head . animal2 is multi 
- colored with longer tail feathers .

animal1 is brown and white with a 
squatty body with a light brown head . 
animal2 is multi - colored with a light 
blue and black head .

MM

GG

these animals appear exactly the same .

animal1 and animal2 look the same .

MM

GG

animal1 is white with brown wings 
while animal2 is yellow with black 
head
animal1 has a long dark tail and 
flecked dark wings with a white 
curved beak . animal2 has a shorter 
beak and a yellow breast and head 
with a shorter brown tail .

MM

GG

animal1 is brown with black spots on the 
body while animal2 is tan with a white 
neck and black head

animal1 is brown and white with a long 
yellow and brown beak . animal2 is 
gray with a short light pink beak .

MM

GG

Animal 2 Animal 2Animal 1 Animal 1

Animal 1 Animal 1Animal 2 Animal 2

Animal 1 Animal 1Animal 2 Animal 2

animal2 ' s colors are brighter than animal1 . 
animal2 has more earthy colors than animal1 . 
animal1 is a bit bigger than animal2 .

both animals appear to be the same .

MM

GG

Animal 1 Animal 1Animal 2 Animal 2
animal2 has a heart shaped face , whereas animal1 
has an oval face . animal2 has entirely dark eyes . 
animal2 has a white beak , whereas animal1 has a 
dark beak . animal2 has more white in its feathers .

animal1 has yellow eyes . animal2 has black eyes . 
animal2 is lighter in color than animal1 . animal2 
has a heart shaped face . animal1 doesn ' t .

MM

GG

Animal 1 Animal 1Animal 2 Animal 2

Animal 2 Animal 2

Figure 5: Samples from the dev split of the proposed Birds-to-Words dataset, along with Neural Naturalist model
output (M) and one of five ground truth paragraphs (G). The second row highlights failure cases in red. The model
produces coherent descriptions of variable granularity, though emphasis and assignment can be improved.

multiple latent image embeddings. A schematic of
the model is outlined in Figure 4, and its key com-
ponents are described in the upcoming sections.

3.1 Image Embedding

Both input images are first processed using CNNs
with shared weights. In this work, we consider
ResNet (He et al., 2016) and Inception (Szegedy
et al., 2017) architectures. In both cases, we ex-
tract the representation from the deepest layer im-
mediately before the classification layer. This
yields a dense 2D grid of local image feature vec-
tors, shaped (d, d, f). We then flatten each feature
grid into a (d2, f) shaped matrix:

E1 = „Äàe11,1, . . . , e1d,d„Äâ = CNN(i1)
E2 = „Äàe21,1, . . . , e2d,d„Äâ = CNN(i2)

3.2 Joint Encoding

We define a joint encoding J of the images which
contains both embedded images (E1,E2), a mu-
tated combination (M), or both. We consider
as possible mutations M ‚àà {E1 + E2,E1 ‚àí
E2,max(E1,E2),E1ÔøΩE2}. We try these encod-
ing variants to explore whether simple mutations
can effectively combine the image representations.



713

3.3 Comparative Module

Given the joint encoding of the images (J), we
would like to represent the differences in feature
space (C) in order to generate comparative de-
scriptions. We explore two variants at this stage.
The first is a direct passthrough of the joint en-
coding (C = J). This is analogous to ‚Äústandard‚Äù
CNN+LSTM architectures, which embed images
and pass them directly to an LSTM for decod-
ing. Because we try different joint encodings, a
passthrough here also allows us to study their ef-
fects in isolation.

Our second variant is an N -layer Transformer
encoder. This provides an additional self-attentive
mutations over the latent representations J. Each
layer contains a multi-headed attention mecha-
nism (ATTNMH). The intent is that self-attention
in Transformer encoder layers will guide compar-
isons across the joint image encoding.

Denoting LN as Layer Norm and FF as Feed For-
ward, with Ci as the output of the ith layer of the
Transformer encoder, C0 = J, and C = CN :

CHi = LN(Ci‚àí1 + ATTNMH(Ci‚àí1))

Ci = LN(C
H
i + FF(C

H
i ))

3.4 Decoder

We use an N -layer Transformer decoder architec-
ture to produce distributions over output tokens.
The Transformer decoder is similar to an encoder,
but it contains an intermediary multi-headed atten-
tion which has access to the encoder‚Äôs output C at
every time step.

DH1i = LN(X+ ATTNMASK,MH(X))

DH2i = LN(D
H1
i + ATTNMH(D

H1
i ,C))

Di = LN(D
H2
i + FF(D

H2
i ))

Here we denote the text observed during training
as X, which is modulated with a position-based
encoding and masked in the first multi-headed at-
tention.

4 Experiments

We train the Neural Naturalist model to produce
descriptions of the differences between images
in the Birds-to-Words dataset. We partition the
dataset into train (80%), val (10%), and test (10%)
sections by splitting based on the pivot images i1.

This ensures i1 species are unique across the dif-
ferent splits.

We provide model hyperparameters and opti-
mization details in Appendix C.

4.1 Baselines and Variants

The most frequent paragraph baseline produces
only the most observed description in the train-
ing data, which is that the two animals appear to
be exactly the same. Text-Only samples captions
from the training data according to their empiri-
cal distribution. Nearest Neighbor embeds both
images and computes the lowest total L2 distance
to a training set pair, sampling a caption from
it. We include two standard neural baselines,
CNN (+ Attention) + LSTM, which concatenate
the images embeddings, optionally perform atten-
tion, and decode with an LSTM. The main model
variants we consider are a simple joint encoding
(J = „ÄàE1,E2„Äâ), no comparative module (C = J),
a small (1-layer) decoder, and our full Neural Nat-
uralist model. We also try several other ablations
and model variants, which we describe later.

4.2 Quantitative Results

Automatic Metrics We evaluate our model
using three machine-graded text metrics: BLEU-
4 (Papineni et al., 2002), ROUGE-L (Lin, 2004),
and CIDEr-D (Vedantam et al., 2015). Each gen-
erated paragraph is compared to all five reference
paragraphs.

For human performance, we use a one-vs-rest
scheme to hold one reference paragraph out and
compute its metric using the other four. We av-
erage this score across twenty-five runs over the
entire split in question.

Results using these metrics are given in Table 2
for the main baselines and model variants. We ob-
serve improvement across BLEU-4 and ROUGE-
L scores compared to baselines. Curiously, we
observe that the CIDEr-D metric is susceptible to
common patterns in the data; our model, when
stopped at its highest CIDEr-D score, outputs
a variant of, ‚Äúthese animals appear exactly the
same‚Äù for 95% of paragraphs, nearly mimick-
ing the behavior of the most frequent paragraph
(Freq.) baseline. The corpus-level behavior of
CIDEr-D gives these outputs a higher score. We
observed anecdotally higher quality outputs corre-
lated with ROUGE-L score, which we verify using
a human evaluation (paragraph after next).



714

Dev Test

BLEU-4 ROUGE-L CIDEr-D BLEU-4 ROUGE-L CIDEr-D

Most Frequent 0.20 0.31 0.42 0.20 0.30 0.43
Text-Only 0.14 0.36 0.05 0.14 0.36 0.07
Nearest Neighbor 0.18 0.40 0.15 0.14 0.36 0.06

CNN + LSTM (Vinyals et al., 2015) 0.22 0.40 0.13 0.20 0.37 0.07
CNN + Attn. + LSTM (Xu et al., 2015) 0.21 0.40 0.14 0.19 0.38 0.11

Neural Naturalist ‚Äì Simple Joint Encoding 0.23 0.44 0.23 - - -
Neural Naturalist ‚Äì No Comparative Module 0.09 0.27 0.09 - - -
Neural Naturalist ‚Äì Small Decoder 0.22 0.42 0.25 - - -
Neural Naturalist ‚Äì Full 0.24 0.46 0.28 0.22 0.43 0.25

Human 0.26 +/- 0.02 0.47 +/- 0.01 0.39 +/- 0.04 0.27 +/- 0.01 0.47 +/- 0.01 0.42 +/- 0.03

Table 2: Experimental results for comparative paragraph generation on the proposed dataset. For human captions,
mean and standard deviation are given for a one-vs-rest scheme across twenty-five runs. We observed that CIDEr-
D scores had little correlation with description quality. The Neural Naturalist model benefits from a strong joint
encoding and Transformer-based comparative module, achieving the highest BLEU-4 and ROUGE-L scores.

Ablations and Model Variants We ablate and
vary each of the main model components, running
the automatic metrics to study coarse changes in
the model‚Äôs behavior. Results for these experi-
ments are given in Table 3. For the joint encod-
ing, we try combinations of four element-wise op-
erations with and without both encoded images.
To study the comparative module in greater detail,
we examine its effect on the top three joint encod-
ings: (i1, i2,‚àí), ‚àí, and ÔøΩ. After fixing the best
joint encoding and comparative module, we also
try variations of the decoder (Transformer depth),
as well as decoding algorithms (greedy decoding,
multinomial sampling, and beamsearch).

Overall, we we see that the choice of joint en-
coding requires a balance with the choice of com-
parative module. More disruptive joint encod-
ings (like element-wise multiplication ÔøΩ) appear
too destructive when passed directly to a decoder,
but yield the best performance when paired with
a deep comparative module. Others (like subtrac-
tion) function moderately well on their own, and
are further improved when a comparative module
is introduced.

Human Evaluation To verify our obser-
vations about model quality and automatic met-
rics, we also perform a human evaluation of the
generated paragraphs. We sample 120 instances
from the test set, taking twenty each from the six
categories for choosing comparative images (vi-
sual similarity in embedding space, plus five taxo-
nomic distances). We provide annotators with the
two images in a random order, along with the out-
put from the model at hand. Annotators must de-
cide which image contains Animal 1, and which

contains Animal 2, or they may say that there is no
way to tell (e.g., for a description like ‚Äúboth look
exactly the same‚Äù).

We collect three annotations per datum, and
score a decision only if‚â• 2/3 annotators made that
choice. A model receives +1 point if annotators
decide correctly, 0 if they cannot decide or agree
there is no way to tell, and -1 point if they decide
incorrectly (label the images backwards). This
scheme penalizes a model for confidently writing
incorrect descriptions. The total score is then nor-
malized to the range [‚àí1, 1]. Note that Human
uses one of the five gold paragraphs sampled at
random.

Results for this experiment are shown in Ta-
ble 4. In this measure, we see the frequency and
text-only baselines now fall flat, as expected. The
frequency baseline never receives any points, and
the text-only baseline is often penalized for incor-
rectly guessing. Our model is successful at mak-
ing distinctions between visually distinct species
(GENUS column and ones further right), which
is near the challenge level of current fine-grained
visual classification tasks. However, it struggles
on the two data subsets with highest visual sim-
ilarity (VISUAL, SPECIES). The significant gap
between all methods and human performance in
these columns indicates ultra fine-grained distinc-
tions are still possible for humans to describe, but
pose a challenge for current models to capture.

4.3 Qualitative Analysis

In Figure 5, we present several examples of the
model output for pairs of images in the dev set,
along with one of the five reference paragraphs. In



715

Joint Encoding Decoding
Algorithm

Dev

i1 i2 ‚àí + max ÔøΩ Comparative Module Decoder BLEU-4 ROUGE-L CIDEr-D

X X

6-Layer
Transformer

6-Layer
Transformer

Beamsearch

0.23 0.44 0.23
X 0.23 0.45 0.27

X 0.24 0.43 0.28
X 0.23 0.43 0.24

X 0.24 0.46 0.28
X X X 0.22 0.44 0.22
X X X 0.22 0.42 0.25
X X X 0.21 0.42 0.22
X X X 0.22 0.43 0.23
X X X X X X 0.21 0.43 0.20

X Passthrough
6-Layer

Transformer
Beamsearch

0.00 0.02 0.00
X 1-L Transformer 0.24 0.44 0.27
X 3-L Transformer 0.24 0.44 0.27
X 6-L Transformer 0.24 0.46 0.28

X Passthrough
6-Layer

Transformer
Beamsearch

0.22 0.40 0.22
X 1-L Transformer 0.21 0.41 0.26
X 3-L Transformer 0.22 0.41 0.22
X 6-L Transformer 0.23 0.45 0.27

X X X Passthrough
6-Layer

Transformer
Beamsearch

0.09 0.27 0.09
X X X 1-L Transformer 0.24 0.43 0.24
X X X 3-L Transformer 0.22 0.42 0.26
X X X 6-L Transformer 0.22 0.44 0.22

X
6-Layer

Transformer

1-L Transformer
Beamsearch

0.22 0.42 0.25
X 3-L Transformer 0.23 0.42 0.25
X 6-L Transformer 0.24 0.46 0.28

X
6-Layer

Transformer
6-Layer

Transformer

Greedy 0.21 0.44 0.18
X Multinomial 0.20 0.42 0.16
X Beamsearch 0.24 0.46 0.28

Table 3: Variants and ablations for the Neural Naturalist model. We find the best performing combination is
an elementwise multiplication (ÔøΩ) for the joint encoding, a 6-layer Transformer comparative module, a 6-layer
Transformer decoder, and using beamsearch to perform inference.

the following section, we split an analysis of the
model into two parts: largely positive findings, as
well as common error cases.

Positive Findings

We find that the model exhibits dynamic granu-
larity, by which we mean that it adjusts the mag-
nitude of the descriptions based on the scale of
differences between the two animals. If two ani-
mals are quite similar, it generates fine-grained de-
scriptions such as, ‚ÄúAnimal 2 has a slightly more
curved beak than Animal 1,‚Äù or ‚ÄúAnimal 1 is more
iridescent than Animal 2.‚Äù If instead the two an-
imals are very different, it will generate text de-
scribing larger-scale differences, like, ‚ÄúAnimal 1
has a much longer neck than Animal 2,‚Äù or ‚ÄúAni-
mal 1 is mostly white with a black head. Animal 2
is almost completely yellow.‚Äù

We also observe that the model is able to pro-

duce coherent paragraphs of varying linguistic
structure. These include a range of compar-
isons set up across both single and multiple sen-
tences. For example, one it generates straight-
forward comparisons of the form, Animal 1 has
X, while Animal 2 has Y. But it also generates
contrastive expressions with longer dependencies,
such as Animal 1 is X, Y, and Z. Animal 2 is very
similar, except W. Furthermore, the model will mix
and match different comparative structures within
a single paragraph.

Finally, in addition to varying linguistic struc-
ture, we find the model is able to produce coher-
ent semantics through a series of statements. For
example, consider the following full output: ‚ÄúAn-
imal 1 has a very long neck compared to Animal
2. Animal 1 has shorter legs than Animal 2. An-
imal 1 has a black beak, Animal 2 has a brown
beak. Animal 1 has a yellow belly. Animal 2 has



716

VISUAL SPECIES GENUS FAMILY ORDER CLASS

Freq. 0.00 0.00 0.00 0.00 0.00 0.00
Text-Only 0.00 -0.10 -0.05 0.00 0.15 -0.15
CNN + LSTM -0.15 0.20 0.15 0.50 0.40 0.15
CNN + Attn. + LSTM 0.15 0.15 0.15 -0.05 0.05 0.20
Neural Naturalist 0.10 -0.10 0.35 0.40 0.45 0.55

Human 0.55 0.55 0.85 1.00 1.00 1.00

Table 4: Human evaluation results on 120 test set sam-
ples, twenty per column. Scale: -1 (perfectly wrong)
to 1 (perfectly correct). Columns are ordered left-to-
right by increasing distance. Our model outperforms
baselines for several distances, though highly similar
comparisons still prove difficult.

darker wings than Animal 1.‚Äù The range of con-
cepts in the output covers neck, legs, beak, belly,
wings without repeating any topic or getting side-
tracked.

Error Analysis

We also observe several patterns in the model‚Äôs
shortcomings. The most prominent error case is
that the model will sometimes hallucinate differ-
ences (Figure 5, bottom row). These range from
pointing out significant changes that are miss-
ing (e.g., ‚Äúa black head‚Äù where there is none
(Fig. 5, bottom left)), to clawing at subtle distinc-
tions where there are none (e.g., ‚Äú[its] colors are
brighter . . . and [it] is a bit bigger‚Äù (Fig. 5, bot-
tom right)). We suspect that the model has learned
some associations between common features in
animals, and will sometimes favor these associa-
tions over visual evidence.

The second common error case is missing obvi-
ous distinctions. This is observed in Fig. 5 (bot-
tom middle), where the prominent beak of Ani-
mal 1 is ignored by the model in favor of mundane
details. While outlying features make for lively
descriptions, we hypothesize that the model may
sometimes avoid taking them into account given
its per-token cross entropy learning objective.

Finally, we also observe the model sometimes
swaps which features are attributed to which
animal. This is partially observed in Fig. 5 (bot-
tom left), where the ‚Äúblack head‚Äù actually belongs
to Animal 1, not Animal 2. We suspect that mix-
ing up references may be a trade-off for the repre-
sentational power of attending over both images;
there is no explicit bookkeeping mechanism to en-
force which phrases refer to which feature com-
parisons in each image.

5 Related Work

Employing visual comparisons to elicit focused
natural language observations was proposed by
(Maji, 2012), and later investigated in the context
of crowdsourcing by (Zou et al., 2015). We take
inspiration from these works.

Previous work has collected natural language
captions of bird photographs: CUB Captions
(Reed et al., 2016) and CUB-Justify (Vedantam
et al., 2017) are both language annotations on
top of the CUB-2011 dataset of bird photographs
(Wah et al., 2011). In addition to describing two
photos instead of one, the language in our dataset
is more complex by comparison, containing a di-
versity of comparative structures and implied se-
mantics. We also collect our data without an
anatomical guide for annotators, yielding every-
day language in place of scientific terminology.

Conceptually, our paper offers a complemen-
tary approach to works that generate single-image
class-discriminative or image-discriminative cap-
tions (Hendricks et al., 2016; Vedantam et al.,
2017). Rather than discriminative captioning, we
focus on comparative language as a means for
bridging the gap between varying granularities of
visual diversity.

Methodologically, our work is most closely re-
lated to the Spot-the-diff dataset (Jhamtani and
Berg-Kirkpatrick, 2018). While dataset captions
two images with only a small section of pixels
that change (surveillance footage), we consider
image pairs with no pixel overlap, which motivates
our stratified sampling approach for drawing good
comparisons.

Finally, the recently released NLVR2 dataset
(Suhr et al., 2018) introduces a challenging nat-
ural language reasoning task using two images as
context. Our work instead focuses on generating
comparative language rather than reasoning.

6 Conclusion

We present the new Birds-to-Words dataset and
Neural Naturalist model for generating compar-
ative explanations of fine-grained visual differ-
ences. This dataset features paragraph-length,
adaptively detailed descriptions written in every-
day language. We hope that continued study of
this area will produce models that can aid humans
in critical domains like citizen science.



717

References
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai

Li, and Li Fei-Fei. 2009. Imagenet: A large-scale
hierarchical image database.

Ruiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, and
David Simcha. 2016. Quantization based fast inner
product search. In Artificial Intelligence and Statis-
tics, pages 482‚Äì490.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770‚Äì
778.

Lisa Anne Hendricks, Zeynep Akata, Marcus
Rohrbach, Jeff Donahue, Bernt Schiele, and Trevor
Darrell. 2016. Generating visual explanations. In
European Conference on Computer Vision, pages
3‚Äì19. Springer.

Harsh Jhamtani and Taylor Berg-Kirkpatrick. 2018.
Learning to describe differences between pairs of
similar images. arXiv preprint arXiv:1808.10584.

Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-
Fei. 2013. 3d object representations for fine-grained
categorization. In 4th International IEEE Workshop
on 3D Representation and Recognition (3dRR-13),
Sydney, Australia.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. Text Summarization
Branches Out.

Subhransu Maji. 2012. Discovering a lexicon of parts
and attributes. In European Conference on Com-
puter Vision, pages 21‚Äì30. Springer.

TomaÃÅsÃå Mikolov, Martin KarafiaÃÅt, LukaÃÅsÃå Burget, Jan
CÃåernockyÃÄ, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Eleventh
annual conference of the international speech com-
munication association.

Erik Murphy-Chutorian and Mohan Manubhai Trivedi.
2009. Head pose estimation in computer vision: A
survey. IEEE transactions on pattern analysis and
machine intelligence, 31(4):607‚Äì626.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311‚Äì318. Association for
Computational Linguistics.

Scott Reed, Zeynep Akata, Honglak Lee, and Bernt
Schiele. 2016. Learning deep representations of
fine-grained visual descriptions. In Proceedings of
the IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 49‚Äì58.

Piyush Sharma, Nan Ding, Sebastian Goodman, and
Radu Soricut. 2018. Conceptual captions: A
cleaned, hypernymed, image alt-text dataset for au-
tomatic image captioning. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 2556‚Äì2565.

Alane Suhr, Stephanie Zhou, Iris Zhang, Huajun Bai,
and Yoav Artzi. 2018. A corpus for reasoning about
natural language grounded in photographs. CoRR,
abs/1811.00491.

Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke,
and Alexander A Alemi. 2017. Inception-v4,
inception-resnet and the impact of residual connec-
tions on learning. In Thirty-First AAAI Conference
on Artificial Intelligence.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998‚Äì6008.

Ramakrishna Vedantam, Samy Bengio, Kevin Murphy,
Devi Parikh, and Gal Chechik. 2017. Context-aware
captions from context-agnostic supervision. In Pro-
ceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition, pages 251‚Äì260.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. Cider: Consensus-based image de-
scription evaluation. In Proceedings of the IEEE
conference on computer vision and pattern recog-
nition, pages 4566‚Äì4575.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In Proceedings of the IEEE
conference on computer vision and pattern recogni-
tion, pages 3156‚Äì3164.

Catherine Wah, Steve Branson, Peter Welinder, Pietro
Perona, and Serge Belongie. 2011. The caltech-ucsd
birds-200-2011 dataset.

Xiang Wu, Ruiqi Guo, Ananda Theertha Suresh, San-
jiv Kumar, Daniel N Holtmann-Rice, David Simcha,
and Felix Yu. 2017. Multiscale quantization for fast
similarity search. In Advances in Neural Informa-
tion Processing Systems, pages 5745‚Äì5755.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:
Neural image caption generation with visual atten-
tion. In International conference on machine learn-
ing, pages 2048‚Äì2057.

James Y Zou, Kamalika Chaudhuri, and Adam Tau-
man Kalai. 2015. Crowdsourcing feature discovery
via adaptively chosen comparisons. arXiv preprint
arXiv:1504.00064.

http://arxiv.org/abs/1811.00491
http://arxiv.org/abs/1811.00491

