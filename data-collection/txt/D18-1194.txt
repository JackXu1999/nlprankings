



















































Cross-lingual Decompositional Semantic Parsing


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1664–1675
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1664

Cross-lingual Decompositional Semantic Parsing

Sheng Zhang
Johns Hopkins University

Xutai Ma
Johns Hopkins Univeristy

Rachel Rudinger
Johns Hopkins University

Kevin Duh
Johns Hopkins University

Benjamin Van Durme
Johns Hopkins University

Abstract

We introduce the task of cross-lingual decom-
positional semantic parsing: mapping content
provided in a source language into a decom-
positional semantic analysis based on a tar-
get language. We present: (1) a form of de-
compositional semantic analysis designed to
allow systems to target varying levels of struc-
tural complexity (shallow to deep analysis),
(2) an evaluation metric to measure the simi-
larity between system output and reference se-
mantic analysis, (3) an end-to-end model with
a novel annotating mechanism that supports
intra-sentential coreference, and (4) an evalu-
ation dataset on which our model outperforms
strong baselines by at least 1.75 F1 score.

1 Introduction

We are concerned here with representing the se-
mantics of multiple natural languages in a sin-
gle semantic analysis. Renewed interest in se-
mantic analysis has led to a surge of proposed
new frameworks, e.g., GMB (Basile et al., 2012),
AMR (Banarescu et al., 2013), UCCA (Abend
and Rappoport, 2013), and UDS (White et al.,
2016), as well as further calls to attend to existing
efforts, e.g., Episodic Logic (EL) (Schubert and
Hwang, 2000; Schubert, 2000; Hwang and Schu-
bert, 1994; Schubert, 2014), or Discourse Repre-
sentation Theory (Kamp, 1981; Heim, 1988).

Many of these efforts are limited to the analysis
of English, but with a number of exceptions, e.g.,
recent efforts by Bos et al. (2017), ongoing efforts
in Minimal Recursion Semantics (MRS) (Copes-
take et al., 1995), multilingual FrameNet anno-
tation and parsing (Fung and Chen, 2004; Padó
and Lapata, 2005), among others. For many lan-
guages, semantic analysis cannot be performed di-
rectly, owing to a lack of training data. While
there is active work in the community focused on
rapid construction of resources for low resource

languages (Strassel and Tracey, 2016), it remains
an expensive and perhaps infeasible solution to as-
sume in-language annotated resources for devel-
oping semantic parsing technologies. In contrast,
bitext is easier to get: it occurs often without re-
searcher involvement,1 and even when not avail-
able, it may be easier to find bilingual speakers
that can translate a text, than it is to find experts
that will create in-language semantic annotations.
In addition, we are simply further along in being
able to automatically understand English than we
are other languages, resulting from the bias in in-
vestment in English-rooted resources.

Therefore, we propose the task of cross-lingual
decompositional semantic parsing, which aims at
transducing a sentence in the source language
(e.g., Chinese sentence in Fig. 1b) into a de-
compositional semantic analysis derived based on
English, via bitext. The efforts of decomposi-
tional semantics (White et al., 2016) focus on ap-
proaches to annotating meaning based on fine-
grained scalar judgments which reflect the ambi-
guity of language, and the underspecification of
meaning in context. Our contributions include:
(1) A form of decompositional semantic analysis
allowing systems to target varying levels of struc-
tural complexity.
(2) An evaluation metric to measure the similarity
between system and reference semantic analysis.
(3) An encoder-decoder model for cross-lingual
decompositional semantic parsing. With a coref-
erence annotating mechanism, the model solves
intra-sentential coreference explicitly.
(4) The first evaluation dataset for cross-lingual
decompositional semantic parsing.2

Experiments demonstrate our model achieves
38.78% F1, outperforming strong baselines.

1For example, owing to a government decree.
2http://decomp.io

http://decomp.io


1665

  FACTUAL 1.0
 …                …

  FACTUAL 1.0
 …                …

x

y

p1

p2

p3z

w

hwere reportedhi
<latexit sha1_base64="4e6mlumABMYyGjxuTO67GfGu0dw=">AAACE3icbVA9SwNBFNyLXzF+RS1tFoMgCOEigtoFbSwjGBPIhbC3eUmW7O0du+/UcORH2PhXbCxUbG3s/DduLldo4sDCMDOPt2/8SAqDrvvt5BYWl5ZX8quFtfWNza3i9s6tCWPNoc5DGeqmzwxIoaCOAiU0Iw0s8CU0/OHlxG/cgTYiVDc4iqAdsL4SPcEZWqlTPPIkU30JHsIDJveggWqIQo3QHXem4mDs6TTTKZbcspuCzpNKRkokQ61T/PK6IY8DUMglM6ZVcSNsJ0yj4BLGBS82EDE+ZH1oWapYAKadpEeN6YFVurQXavsU0lT9PZGwwJhR4NtkwHBgZr2J+J/XirF31k6EimIExaeLerGkGNJJQ7QrNHCUI0sY18L+lfIB04yj7bFgS6jMnjxP6sfl87J7fVKqXmRt5Mke2SeHpEJOSZVckRqpE04eyTN5JW/Ok/PivDsf02jOyWZ2yR84nz+Yg6AB</latexit><latexit sha1_base64="4e6mlumABMYyGjxuTO67GfGu0dw=">AAACE3icbVA9SwNBFNyLXzF+RS1tFoMgCOEigtoFbSwjGBPIhbC3eUmW7O0du+/UcORH2PhXbCxUbG3s/DduLldo4sDCMDOPt2/8SAqDrvvt5BYWl5ZX8quFtfWNza3i9s6tCWPNoc5DGeqmzwxIoaCOAiU0Iw0s8CU0/OHlxG/cgTYiVDc4iqAdsL4SPcEZWqlTPPIkU30JHsIDJveggWqIQo3QHXem4mDs6TTTKZbcspuCzpNKRkokQ61T/PK6IY8DUMglM6ZVcSNsJ0yj4BLGBS82EDE+ZH1oWapYAKadpEeN6YFVurQXavsU0lT9PZGwwJhR4NtkwHBgZr2J+J/XirF31k6EimIExaeLerGkGNJJQ7QrNHCUI0sY18L+lfIB04yj7bFgS6jMnjxP6sfl87J7fVKqXmRt5Mke2SeHpEJOSZVckRqpE04eyTN5JW/Ok/PivDsf02jOyWZ2yR84nz+Yg6AB</latexit><latexit sha1_base64="4e6mlumABMYyGjxuTO67GfGu0dw=">AAACE3icbVA9SwNBFNyLXzF+RS1tFoMgCOEigtoFbSwjGBPIhbC3eUmW7O0du+/UcORH2PhXbCxUbG3s/DduLldo4sDCMDOPt2/8SAqDrvvt5BYWl5ZX8quFtfWNza3i9s6tCWPNoc5DGeqmzwxIoaCOAiU0Iw0s8CU0/OHlxG/cgTYiVDc4iqAdsL4SPcEZWqlTPPIkU30JHsIDJveggWqIQo3QHXem4mDs6TTTKZbcspuCzpNKRkokQ61T/PK6IY8DUMglM6ZVcSNsJ0yj4BLGBS82EDE+ZH1oWapYAKadpEeN6YFVurQXavsU0lT9PZGwwJhR4NtkwHBgZr2J+J/XirF31k6EimIExaeLerGkGNJJQ7QrNHCUI0sY18L+lfIB04yj7bFgS6jMnjxP6sfl87J7fVKqXmRt5Mke2SeHpEJOSZVckRqpE04eyTN5JW/Ok/PivDsf02jOyWZ2yR84nz+Yg6AB</latexit>

hin Biloxihi
<latexit sha1_base64="ltFJrqfk4dwGrNRkzXAoEQJtt4Q=">AAACD3icbVA9TwJBFNzzE/ELtbTZSIxW5DAmakewscTEExKOkL3lARv29i677wyE8BNs/Cs2Fmpsbe38Ny7HFQpOsslk5r28nQliKQy67reztLyyurae28hvbm3v7Bb29u9NlGgOHo9kpBsBMyCFAg8FSmjEGlgYSKgHg+upX38AbUSk7nAUQytkPSW6gjO0Urtw4kumehJ8hCGOhaJVIaOhmLRnQn/i69RvF4puyU1BF0k5I0WSodYufPmdiCchKOSSGdMsuzG2xkyj4BImeT8xEDM+YD1oWqpYCKY1TgNN6LFVOrQbafsU0lT9vTFmoTGjMLCTIcO+mfem4n9eM8HuZcvGjBMExWeHuomkGNFpO7QjNHCUI0sY18L+lfI+04yj7TBvSyjPR14k3lnpquTenhcr1ayNHDkkR+SUlMkFqZAbUiMe4eSRPJNX8uY8OS/Ou/MxG11ysp0D8gfO5w/u6p3/</latexit><latexit sha1_base64="ltFJrqfk4dwGrNRkzXAoEQJtt4Q=">AAACD3icbVA9TwJBFNzzE/ELtbTZSIxW5DAmakewscTEExKOkL3lARv29i677wyE8BNs/Cs2Fmpsbe38Ny7HFQpOsslk5r28nQliKQy67reztLyyurae28hvbm3v7Bb29u9NlGgOHo9kpBsBMyCFAg8FSmjEGlgYSKgHg+upX38AbUSk7nAUQytkPSW6gjO0Urtw4kumehJ8hCGOhaJVIaOhmLRnQn/i69RvF4puyU1BF0k5I0WSodYufPmdiCchKOSSGdMsuzG2xkyj4BImeT8xEDM+YD1oWqpYCKY1TgNN6LFVOrQbafsU0lT9vTFmoTGjMLCTIcO+mfem4n9eM8HuZcvGjBMExWeHuomkGNFpO7QjNHCUI0sY18L+lfI+04yj7TBvSyjPR14k3lnpquTenhcr1ayNHDkkR+SUlMkFqZAbUiMe4eSRPJNX8uY8OS/Ou/MxG11ysp0D8gfO5w/u6p3/</latexit><latexit sha1_base64="ltFJrqfk4dwGrNRkzXAoEQJtt4Q=">AAACD3icbVA9TwJBFNzzE/ELtbTZSIxW5DAmakewscTEExKOkL3lARv29i677wyE8BNs/Cs2Fmpsbe38Ny7HFQpOsslk5r28nQliKQy67reztLyyurae28hvbm3v7Bb29u9NlGgOHo9kpBsBMyCFAg8FSmjEGlgYSKgHg+upX38AbUSk7nAUQytkPSW6gjO0Urtw4kumehJ8hCGOhaJVIaOhmLRnQn/i69RvF4puyU1BF0k5I0WSodYufPmdiCchKOSSGdMsuzG2xkyj4BImeT8xEDM+YD1oWqpYCKY1TgNN6LFVOrQbafsU0lT9vTFmoTGjMLCTIcO+mfem4n9eM8HuZcvGjBMExWeHuomkGNFpO7QjNHCUI0sY18L+lfI+04yj7TBvSyjPR14k3lnpquTenhcr1ayNHDkkR+SUlMkFqZAbUiMe4eSRPJNX8uY8OS/Ou/MxG11ysp0D8gfO5w/u6p3/</latexit>

h30 peoplehi
<latexit sha1_base64="WVuMu67TSBqNvmnaqaY1QFKVHzQ=">AAACD3icbVA9TwJBEN3DL8Qv1NJmIzFakUNN1I5oY4mJCAl3IXvLABv29i67c0Zy4SfY+FdsLNTY2tr5b1zgCgVfMsnLezO7My+IpTDout9ObmFxaXklv1pYW9/Y3Cpu79yZKNEc6jySkW4GzIAUCuooUEIz1sDCQEIjGFyN/cY9aCMidYvDGPyQ9ZToCs7QSu3ioSeZ6knwEB4wPXFpDFEsYdSeCv2Rpyd+u1hyy+4EdJ5UMlIiGWrt4pfXiXgSgkIumTGtihujnzKNgtv3C15iIGZ8wHrQslSxEIyfTg4a0QOrdGg30rYU0on6eyJloTHDMLCdIcO+mfXG4n9eK8HuuZ8KFScIik8/6iaSYkTH6dCO0MBRDi1hXAu7K+V9phlHm2HBhlCZPXme1I/LF2X35rRUvczSyJM9sk+OSIWckSq5JjVSJ5w8kmfySt6cJ+fFeXc+pq05J5vZJX/gfP4AZKWdqQ==</latexit><latexit sha1_base64="WVuMu67TSBqNvmnaqaY1QFKVHzQ=">AAACD3icbVA9TwJBEN3DL8Qv1NJmIzFakUNN1I5oY4mJCAl3IXvLABv29i67c0Zy4SfY+FdsLNTY2tr5b1zgCgVfMsnLezO7My+IpTDout9ObmFxaXklv1pYW9/Y3Cpu79yZKNEc6jySkW4GzIAUCuooUEIz1sDCQEIjGFyN/cY9aCMidYvDGPyQ9ZToCs7QSu3ioSeZ6knwEB4wPXFpDFEsYdSeCv2Rpyd+u1hyy+4EdJ5UMlIiGWrt4pfXiXgSgkIumTGtihujnzKNgtv3C15iIGZ8wHrQslSxEIyfTg4a0QOrdGg30rYU0on6eyJloTHDMLCdIcO+mfXG4n9eK8HuuZ8KFScIik8/6iaSYkTH6dCO0MBRDi1hXAu7K+V9phlHm2HBhlCZPXme1I/LF2X35rRUvczSyJM9sk+OSIWckSq5JjVSJ5w8kmfySt6cJ+fFeXc+pq05J5vZJX/gfP4AZKWdqQ==</latexit><latexit sha1_base64="WVuMu67TSBqNvmnaqaY1QFKVHzQ=">AAACD3icbVA9TwJBEN3DL8Qv1NJmIzFakUNN1I5oY4mJCAl3IXvLABv29i67c0Zy4SfY+FdsLNTY2tr5b1zgCgVfMsnLezO7My+IpTDout9ObmFxaXklv1pYW9/Y3Cpu79yZKNEc6jySkW4GzIAUCuooUEIz1sDCQEIjGFyN/cY9aCMidYvDGPyQ9ZToCs7QSu3ioSeZ6knwEB4wPXFpDFEsYdSeCv2Rpyd+u1hyy+4EdJ5UMlIiGWrt4pfXiXgSgkIumTGtihujnzKNgtv3C15iIGZ8wHrQslSxEIyfTg4a0QOrdGg30rYU0on6eyJloTHDMLCdIcO+mfXG4n9eK8HuuZ8KFScIik8/6iaSYkTH6dCO0MBRDi1hXAu7K+V9phlHm2HBhlCZPXme1I/LF2X35rRUvczSyJM9sk+OSIWckSq5JjVSJ5w8kmfySt6cJ+fFeXc+pq05J5vZJX/gfP4AZKWdqQ==</latexit>

hwas hithi
<latexit sha1_base64="j/CoKzN5XWgoYyrXxTP/rzpaFBE=">AAACDXicbVA9TwJBEN3DL8Qv1NJmIyGxIocxUTuijSUmIiQcIXvLABv29i67cyq53C+w8a/YWKixtbfz37jAFQq+ZJK3781kZ54fSWHQdb+d3NLyyupafr2wsbm1vVPc3bs1Yaw5NHgoQ93ymQEpFDRQoIRWpIEFvoSmP7qc+M070EaE6gbHEXQCNlCiLzhDK3WLZU8yNZDgITxgcs8MHQpMu7PnMPX01O0WS27FnYIukmpGSiRDvVv88nohjwNQyCUzpl11I+wkTKPgEtKCFxuIGB+xAbQtVSwA00mm56S0bJUe7YfalkI6VX9PJCwwZhz4tjNgODTz3kT8z2vH2D/rJEJFMYLis4/6saQY0kk2tCc0cJRjSxjXwu5K+ZBpxtEmWLAhVOdPXiSN48p5xb0+KdUusjTy5IAckiNSJaekRq5InTQIJ4/kmbySN+fJeXHenY9Za87JZvbJHzifP4JZnT0=</latexit><latexit sha1_base64="j/CoKzN5XWgoYyrXxTP/rzpaFBE=">AAACDXicbVA9TwJBEN3DL8Qv1NJmIyGxIocxUTuijSUmIiQcIXvLABv29i67cyq53C+w8a/YWKixtbfz37jAFQq+ZJK3781kZ54fSWHQdb+d3NLyyupafr2wsbm1vVPc3bs1Yaw5NHgoQ93ymQEpFDRQoIRWpIEFvoSmP7qc+M070EaE6gbHEXQCNlCiLzhDK3WLZU8yNZDgITxgcs8MHQpMu7PnMPX01O0WS27FnYIukmpGSiRDvVv88nohjwNQyCUzpl11I+wkTKPgEtKCFxuIGB+xAbQtVSwA00mm56S0bJUe7YfalkI6VX9PJCwwZhz4tjNgODTz3kT8z2vH2D/rJEJFMYLis4/6saQY0kk2tCc0cJRjSxjXwu5K+ZBpxtEmWLAhVOdPXiSN48p5xb0+KdUusjTy5IAckiNSJaekRq5InTQIJ4/kmbySN+fJeXHenY9Za87JZvbJHzifP4JZnT0=</latexit><latexit sha1_base64="j/CoKzN5XWgoYyrXxTP/rzpaFBE=">AAACDXicbVA9TwJBEN3DL8Qv1NJmIyGxIocxUTuijSUmIiQcIXvLABv29i67cyq53C+w8a/YWKixtbfz37jAFQq+ZJK3781kZ54fSWHQdb+d3NLyyupafr2wsbm1vVPc3bs1Yaw5NHgoQ93ymQEpFDRQoIRWpIEFvoSmP7qc+M070EaE6gbHEXQCNlCiLzhDK3WLZU8yNZDgITxgcs8MHQpMu7PnMPX01O0WS27FnYIukmpGSiRDvVv88nohjwNQyCUzpl11I+wkTKPgEtKCFxuIGB+xAbQtVSwA00mm56S0bJUe7YfalkI6VX9PJCwwZhz4tjNgODTz3kT8z2vH2D/rJEJFMYLis4/6saQY0kk2tCc0cJRjSxjXwu5K+ZBpxtEmWLAhVOdPXiSN48p5xb0+KdUusjTy5IAckiNSJaekRq5InTQIJ4/kmbySN+fJeXHenY9Za87JZvbJHzifP4JZnT0=</latexit>

hdeadhi
<latexit sha1_base64="cJ9C5+6STPgpCbJnhQMfvBs0LC8=">AAACCnicbZBNS8NAEIY3ftb6VfXoJbQInkoqgnorevFYwdhCU8pmM2mXbjZhdyKWkLsX/4oXDype/QXe/Ddu0x60dWDh4X1nmJ3XTwTX6Djf1tLyyuraemmjvLm1vbNb2du/03GqGLgsFrHq+FSD4BJc5CigkyigkS+g7Y+uJn77HpTmsbzFcQK9iA4kDzmjaKR+peoJKgcCPIQHzAKgQd6f8jD3VGH1KzWn7hRlL0JjBjUyq1a/8uUFMUsjkMgE1brbcBLsZVQhZwLyspdqSCgb0QF0DUoage5lxS25fWSUwA5jZZ5Eu1B/T2Q00noc+aYzojjU895E/M/rphie9zIukxRBsumiMBU2xvYkGDvgChiKsQHKFDd/tdmQKsrQxFc2ITTmT14E96R+UXduTmvNy1kaJXJIquSYNMgZaZJr0iIuYeSRPJNX8mY9WS/Wu/UxbV2yZjMH5E9Znz8yw5v9</latexit><latexit sha1_base64="cJ9C5+6STPgpCbJnhQMfvBs0LC8=">AAACCnicbZBNS8NAEIY3ftb6VfXoJbQInkoqgnorevFYwdhCU8pmM2mXbjZhdyKWkLsX/4oXDype/QXe/Ddu0x60dWDh4X1nmJ3XTwTX6Djf1tLyyuraemmjvLm1vbNb2du/03GqGLgsFrHq+FSD4BJc5CigkyigkS+g7Y+uJn77HpTmsbzFcQK9iA4kDzmjaKR+peoJKgcCPIQHzAKgQd6f8jD3VGH1KzWn7hRlL0JjBjUyq1a/8uUFMUsjkMgE1brbcBLsZVQhZwLyspdqSCgb0QF0DUoage5lxS25fWSUwA5jZZ5Eu1B/T2Q00noc+aYzojjU895E/M/rphie9zIukxRBsumiMBU2xvYkGDvgChiKsQHKFDd/tdmQKsrQxFc2ITTmT14E96R+UXduTmvNy1kaJXJIquSYNMgZaZJr0iIuYeSRPJNX8mY9WS/Wu/UxbV2yZjMH5E9Znz8yw5v9</latexit><latexit sha1_base64="cJ9C5+6STPgpCbJnhQMfvBs0LC8=">AAACCnicbZBNS8NAEIY3ftb6VfXoJbQInkoqgnorevFYwdhCU8pmM2mXbjZhdyKWkLsX/4oXDype/QXe/Ddu0x60dWDh4X1nmJ3XTwTX6Djf1tLyyuraemmjvLm1vbNb2du/03GqGLgsFrHq+FSD4BJc5CigkyigkS+g7Y+uJn77HpTmsbzFcQK9iA4kDzmjaKR+peoJKgcCPIQHzAKgQd6f8jD3VGH1KzWn7hRlL0JjBjUyq1a/8uUFMUsjkMgE1brbcBLsZVQhZwLyspdqSCgb0QF0DUoage5lxS25fWSUwA5jZZ5Eu1B/T2Q00noc+aYzojjU895E/M/rphie9zIukxRBsumiMBU2xvYkGDvgChiKsQHKFDd/tdmQKsrQxFc2ITTmT14E96R+UXduTmvNy1kaJXJIquSYNMgZaZJr0iIuYeSRPJNX8mY9WS/Wu/UxbV2yZjMH5E9Znz8yw5v9</latexit>

hby a storm surgehi
<latexit sha1_base64="WdnttyK+XUb52jvcPidwADLQ8Y8=">AAACFnicbVA9SwNBFNyLXzF+RS1tFoNgFS4iqF3QxjKCZwK5EPY2L8mSvb1j9514HPkXNv4VGwsVW7Hz37hJrtDEgYVhZh5v3wSxFAZd99spLC2vrK4V10sbm1vbO+XdvTsTJZqDxyMZ6VbADEihwEOBElqxBhYGEprB6GriN+9BGxGpW0xj6IRsoERfcIZW6parvmRqIMFHeMAsSCmjBiMdUpPoAYy7M3049vU01i1X3Ko7BV0ktZxUSI5Gt/zl9yKehKCQS2ZMu+bG2MmYRsEljEt+YiBmfMQG0LZUsRBMJ5veNaZHVunRfqTtU0in6u+JjIXGpGFgkyHDoZn3JuJ/XjvB/nknEypOEBSfLeonkmJEJyXRntDAUaaWMK6F/SvlQ6YZR1tlyZZQmz95kXgn1Yuqe3NaqV/mbRTJATkkx6RGzkidXJMG8Qgnj+SZvJI358l5cd6dj1m04OQz++QPnM8fUhOg3g==</latexit><latexit sha1_base64="WdnttyK+XUb52jvcPidwADLQ8Y8=">AAACFnicbVA9SwNBFNyLXzF+RS1tFoNgFS4iqF3QxjKCZwK5EPY2L8mSvb1j9514HPkXNv4VGwsVW7Hz37hJrtDEgYVhZh5v3wSxFAZd99spLC2vrK4V10sbm1vbO+XdvTsTJZqDxyMZ6VbADEihwEOBElqxBhYGEprB6GriN+9BGxGpW0xj6IRsoERfcIZW6parvmRqIMFHeMAsSCmjBiMdUpPoAYy7M3049vU01i1X3Ko7BV0ktZxUSI5Gt/zl9yKehKCQS2ZMu+bG2MmYRsEljEt+YiBmfMQG0LZUsRBMJ5veNaZHVunRfqTtU0in6u+JjIXGpGFgkyHDoZn3JuJ/XjvB/nknEypOEBSfLeonkmJEJyXRntDAUaaWMK6F/SvlQ6YZR1tlyZZQmz95kXgn1Yuqe3NaqV/mbRTJATkkx6RGzkidXJMG8Qgnj+SZvJI358l5cd6dj1m04OQz++QPnM8fUhOg3g==</latexit><latexit sha1_base64="WdnttyK+XUb52jvcPidwADLQ8Y8=">AAACFnicbVA9SwNBFNyLXzF+RS1tFoNgFS4iqF3QxjKCZwK5EPY2L8mSvb1j9514HPkXNv4VGwsVW7Hz37hJrtDEgYVhZh5v3wSxFAZd99spLC2vrK4V10sbm1vbO+XdvTsTJZqDxyMZ6VbADEihwEOBElqxBhYGEprB6GriN+9BGxGpW0xj6IRsoERfcIZW6parvmRqIMFHeMAsSCmjBiMdUpPoAYy7M3049vU01i1X3Ko7BV0ktZxUSI5Gt/zl9yKehKCQS2ZMu+bG2MmYRsEljEt+YiBmfMQG0LZUsRBMJ5veNaZHVunRfqTtU0in6u+JjIXGpGFgkyHDoZn3JuJ/XjvB/nknEypOEBSfLeonkmJEJyXRntDAUaaWMK6F/SvlQ6YZR1tlyZZQmz95kXgn1Yuqe3NaqV/mbRTJATkkx6RGzkidXJMG8Qgnj+SZvJI358l5cd6dj1m04OQz++QPnM8fUhOg3g==</latexit>

hin one blockhof flatsi
<latexit sha1_base64="zWG6MOcbaHgs7hiCLcn4IdSX2qg=">AAACIXicbVDLSgMxFM34rPVVdekmWARXZSqCdld047KCtYVOKZn0ThuaSYbkjljKfIsbf8WNC5XuxJ8xfSy09ULI4Zx7SM4JEyks+v6Xt7K6tr6xmdvKb+/s7u0XDg4frE4NhzrXUptmyCxIoaCOAiU0EwMsDiU0wsHNRG88grFCq3scJtCOWU+JSHCGjuoUKoFkqichQHjCkVBUK6Ch1HyQdWZcP5vdOqKRZGizwEwdnULRL/nTocugPAdFMp9apzAOupqnMSjkklnbKvsJtkfMoOASsnyQWkgYH7AetBxULAbbHk0jZvTUMV0aaeOOQjplfztGLLZ2GIduM2bYt4vahPxPa6UYXbVd8CRFUHz2UJRKippO+qJdYYCjHDrAuBHur5T3mWEcXat5V0J5MfIyqJ+XKiX/7qJYvZ63kSPH5ISckTK5JFVyS2qkTjh5Jq/knXx4L96b9+mNZ6sr3txzRP6M9/0DwhGl4Q==</latexit><latexit sha1_base64="zWG6MOcbaHgs7hiCLcn4IdSX2qg=">AAACIXicbVDLSgMxFM34rPVVdekmWARXZSqCdld047KCtYVOKZn0ThuaSYbkjljKfIsbf8WNC5XuxJ8xfSy09ULI4Zx7SM4JEyks+v6Xt7K6tr6xmdvKb+/s7u0XDg4frE4NhzrXUptmyCxIoaCOAiU0EwMsDiU0wsHNRG88grFCq3scJtCOWU+JSHCGjuoUKoFkqichQHjCkVBUK6Ch1HyQdWZcP5vdOqKRZGizwEwdnULRL/nTocugPAdFMp9apzAOupqnMSjkklnbKvsJtkfMoOASsnyQWkgYH7AetBxULAbbHk0jZvTUMV0aaeOOQjplfztGLLZ2GIduM2bYt4vahPxPa6UYXbVd8CRFUHz2UJRKippO+qJdYYCjHDrAuBHur5T3mWEcXat5V0J5MfIyqJ+XKiX/7qJYvZ63kSPH5ISckTK5JFVyS2qkTjh5Jq/knXx4L96b9+mNZ6sr3txzRP6M9/0DwhGl4Q==</latexit><latexit sha1_base64="zWG6MOcbaHgs7hiCLcn4IdSX2qg=">AAACIXicbVDLSgMxFM34rPVVdekmWARXZSqCdld047KCtYVOKZn0ThuaSYbkjljKfIsbf8WNC5XuxJ8xfSy09ULI4Zx7SM4JEyks+v6Xt7K6tr6xmdvKb+/s7u0XDg4frE4NhzrXUptmyCxIoaCOAiU0EwMsDiU0wsHNRG88grFCq3scJtCOWU+JSHCGjuoUKoFkqichQHjCkVBUK6Ch1HyQdWZcP5vdOqKRZGizwEwdnULRL/nTocugPAdFMp9apzAOupqnMSjkklnbKvsJtkfMoOASsnyQWkgYH7AetBxULAbbHk0jZvTUMV0aaeOOQjplfztGLLZ2GIduM2bYt4vahPxPa6UYXbVd8CRFUHz2UJRKippO+qJdYYCjHDrAuBHur5T3mWEcXat5V0J5MfIyqJ+XKiX/7qJYvZ63kSPH5ISckTK5JFVyS2qkTjh5Jq/knXx4L96b9+mNZ6sr3txzRP6M9/0DwhGl4Q==</latexit>

AWARENESSSENTIENTINSTIGATION      …

0
0

0.9
…

AWAREN
ESS

SENTIEN
T

INSTIGA
TION      

…

0.0
0.0
0.1
…

ARG

ARG

ARG

AR
G

ARG

ARG
ARG

INST

INST

INST
INST INST INST

INST

ARGArgument Edge

INSTInstance Edge

ATTRAttribute Edge
SPR Prop

erty Score
s

SPR Property Scores

Attribute Scores

ATTR

ATTR

Attribute Scores

(a) UDS graph representation.

“30 people were reported dead in one block of flats which was hit by a storm surge.”

were reported , in Biloxi flats, storm surge hit which30 people dead in one block PTCP

(b) Chinese sentence with Leipzig gloss.

Figure 1: Input and output of cross-lingual decompositional semantic parsing.

2 Semantic Analysis

The goal of cross-lingual decompositional seman-
tic parsing is to provide a semantic analysis which
can be used for various types of deep and shal-
low processing on the target language side. Many
forms of semantic analysis are potentially suitable
for this goal, e.g., AMR (Banarescu et al., 2013),
UCCA (Abend and Rappoport, 2013), and Uni-
versal Decompositional Semantics (White et al.,
2016). Here we choose Universal Decomposi-
tional Semantics (UDS), but note that our ap-
proach is applicable to other potential graph se-
mantic formalisms.

The reasons for choosing UDS are three-fold:
(1) Compatibility: UDS relates to Robust Min-
imal Recursion Semantics (RMRS) (Copestake,
2007), aiming for a maximal degree of semantic
compatibility. With UDS, shallow analysis, such
as predicate-argument extraction (Zhang et al.,
2017a), can be regarded as producing a seman-
tics which is underspecified and reusable with re-
spect to deeper analysis, such as lexical seman-
tics and inference (White et al., 2016). (2) Ro-
bustness and Speed: There exists a robust frame-
work, PredPatt (White et al., 2016), for auto-
matically creating UDS from raw sentences and
their Universal Dependencies. PredPatt has been

shown to be fast and accurate enough to process
large volumes of text (Zhang et al., 2017c). (3)
Cross-lingual validity: PredPatt is based purely
on non-lexical and linguistically well-founded pat-
terns from Universal Dependencies, which is de-
signed to be cross-linguistically consistent.

There are three forms to represent UDS:
flat, graph, or linearized representations. They
are created for different purposes, and are
inter-convertible. Flat representation relates to
RMRS (Copestake, 2007), and we defer its de-
scription to Appendix A.

2.1 Graph Representation

The graph representation as shown in Fig. 1a is
developed to improve ease of readability, parser
evaluation, and integration with lexical semantics.
The structure of the graph representation is a tuple
G = 〈V,E〉: a set of variables V (e.g., p1 and x),
and a set of edges E. There are 3 types of edges:
(1) Argument edges describe argument relations
between variable pairs. Deeper analysis such as
Semantic Proto-Role (SPR) properties (Reisinger
et al., 2015) can be attached to argument edges.
SPR analysis can be considered as a scalar re-
gression problem (White et al., 2016), where each
predicate-argument pair is annotated with scalar
values for different SPR properties. (2) Instance



1666

 in Biloxih) (30 peopleh) were reportedh (      deadh (in one blockh of flats) ) ] [      was hith (by a storm surgeh) ]
COREF COREF

Figure 2: UDS linearized representation. Deeper analysis such as SPR and factuality is not shown.

edges describe instances of variables in the target
language. The subscript “h” indicates the syntac-
tic head of an instance. (3) Attribute edges are
unary, which describe various attributes of vari-
ables, such as event factuality (Saurı́ and Puste-
jovsky, 2009) and word senses (Miller, 1995). The
graph representation can be viewed as an under-
specified version of Dependency Minimal Recur-
sion Semantics (DMRS) (Copestake, 2009) due to
the underspecification of scope. Different from
DMRS, the graph representation is linked cleanly
to Universal Dependency syntax via PredPatt.

2.2 Linearized Representation
The linearized representation aims to facilitate
learning of semantic parsers. Recently parsers
based on RNN that make use of linearized rep-
resentation have achieved state-of-the-art perfor-
mance in constituency parsing (Choe and Char-
niak, 2016), logical form prediction (Dong and La-
pata, 2016; Jia and Liang, 2016), and AMR pars-
ing (Barzdins and Gosko, 2016; Peng et al., 2017).
There was also work on predicting linearized
semantic representations before RNN based ap-
proaches (Wong and Mooney, 2006).

Fig. 2 shows an example of UDS linearized
representation. Intra-sentential coreference occurs
when an instance refers to an antecedent, where
we replace the instance with a special symbol “•”
and add a COREF link between “•” and its an-
tecedent. The linearized representation can be
viewed as a sequence of tokens with a list of
COREF links. Brackets, parentheses, and the spe-
cial symbol “•” are all considered as tokens in this
representation. The COREF links are drawn as a vi-
sual convenience, and the actual linearized repre-
sentation achieves this via co-indexing, and is thus
fully linear. We describe the procedure of convert-
ing graph representation to linearized representa-
tion in Appendix B.

3 Related Work

Our work synthesizes two strands of research, se-
mantic analysis and cross-lingual learning.

The semantic analysis targeted in this work is
akin to that of Hobbs (2003), but our eventual goal

is to transduce texts from arbitrary human lan-
guages into a “...broad, language-like, inference-
enabling [semantic representation] in the spirit
of Montague...” (Schubert, 2015). Unlike efforts
such as by Schubert and colleagues that directly
target such an analysis, we are pursuing a strat-
egy that incrementally increases the complexity of
the target analysis in accordance with our ability to
fashion models capable of producing it.3 Embrac-
ing underspecification in the name of tractability
is exemplified by MRS (Copestake et al., 2005;
Copestake, 2009), the so-called slacker semantics,
and we draw inspiration from that work. Anal-
yses such as AMR (Banarescu et al., 2013) also
make use of underspecification, but usually this is
only implicit: certain aspects of meaning are sim-
ply not annotated. Unlike AMR, but akin to de-
cisions made in PropBank (Palmer et al., 2005)
(which forms the majority of the AMR ontologi-
cal backbone), we target an analysis with a close
correspondence to natural language syntax. Un-
like interlingua (Mitamura et al., 1991; Dorr and
Habash, 2002) that maps the source language into
an intermediate analysis, and then maps it into the
target language, we are not concerned with gen-
erating text from the semantic analysis. Substan-
tial prior work on semantic analyses exists, includ-
ing HPSG-based analyses (Copestake et al., 2005),
CCG-based analyses (Steedman, 2000; Baldridge
and Kruijff, 2002; Bos et al., 2004), and Universal
Dependencies based analyses (White et al., 2016;
Reddy et al., 2017). See (Schubert, 2015; Abend
and Rappoport, 2017) for further discussion.

Cross-lingual learning has previously been ap-
plied to various NLP tasks. Yarowsky et al.
(2001); Padó and Lapata (2009); Evang and
Bos (2016); Faruqui and Kumar (2015) focused
on projecting existing annotations on source-
language text to the target language. Zeman and
Resnik (2008); Ganchev et al. (2009); McDon-
ald et al. (2011); Naseem et al. (2012); Wang and
Manning (2014) enabled model transfer by shar-

3E.g., in Fig. 1a we recognize “by a storm surge” as an
initial structural unit, with multiple potential analysis, which
may be further refined based on the capabilities of a given
cross-lingual semantic parser.



1667

ing features or model parameters for different lan-
guages. Sudo et al. (2004); Zhang et al. (2017a,b);
Mei et al. (2018) worked on cross-lingual informa-
tion extraction and demonstrated the advantages of
end-to-end learning. In this work, we explore end-
to-end cross-lingual learning.

4 Evaluation Metric S

UDS can be represented in three forms. Evalu-
ating such forms is crucial to the development of
parsing algorithms. However, there is no method
directly available for evaluation. Related meth-
ods come from semantic parsing, whose results
are mainly evaluated in three ways: (1) task cor-
rectness (Tang and Mooney, 2001), which evalu-
ates on a specific NLP task that uses the parsing
results; (2) whole-parse correctness (Zettlemoyer
and Collins, 2005), which counts the number of
parsing results that are completely correct; and (3)
Smatch (Cai and Knight, 2013), which computes
the number of exactly matched edges between two
semantic structures.

Nevertheless, our task needs an evaluation met-
ric that can be used regardless of specific tasks
or domains, and is able to differentiate two UDS
graph representations with similar instances, SPR
analysis, or attributes. We design an evaluation
metric S that computes the similarity between two
graph representations.

As described in Section 2.1, the graph repre-
sentation is a tuple G = (V,E). For two graphs
G1 = (V1, E1) and G2 = (V2, E2), we define the
score S as the maximum soft edge matching score
between G1 and G2:

S(G1,G2) = max
m∈M

[ ∑
(e

(i)
1 ,e

(j)
2 )∈P

fT (e
(i)
1 , e

(j)
2 )
]

wherem is a mapping from variables in V1 to vari-
ables in V2. Given a mappingm, P is a set of edge
pairs: for each pair (e(i)1 , e

(j)
2 ), variables(s) in e

(i)
1

are mapped to variables(s) in e(j)2 via m. fT com-
putes the matching score for a pair of edges be-
longing to type T ∈ {ARG, INST, ATTR}. The
matching score is normalized to [0, 1].

The precision and recall are computed by
S(G1,G2)/|E1|, and S(G1,G2)/|E2| respectively.

In this work, fARG = fATTR = e−MAE, where
MAE computes the mean absolute error between
two set of scores s1 and s2:

∑n
i |s

(i)
1 − s

(i)
2 |/n.

fINST = BLEU (Papineni et al., 2002) which com-
pute the BLEU score of an instance pair.4

Finding an optimal variable mapping m that
yields the highest S is NP-complete. We instead
adopt a strategy used in Smatch (Cai and Knight,
2013) that does a hill-climbing search with smart
initialization plus 4 random restarts, and has been
shown to give the best trade-off between accuracy
and speed. Smatch for evaluating semantic struc-
tures can be considered as a special case of S,
where fT = δ, the Kronecker delta.

5 Model

We formulate the task of cross-lingual decompo-
sitional semantic parsing as a joint problem of
sequence-to-sequence learning, coreference res-
olution and decompositional semantic analysis.
The input is a sentence X in the source language,
e.g., the Chinese sentence in Fig. 1b. The out-
put is a UDS linearized representation (Y,C,D)
based on the target language: Y is a sequence of
tokens; C is a set of COREF links; andD is a set of
scores for decompositional analysis, such as SPR
and factuality.

The goal is to learn a conditional probability
distribution P (Y,C,D|X) whose most likely con-
figuration, given the input sentence, outputs the
true UDS linearized representation with decom-
positional analysis. While the standard encoder-
decoder framework shows the state-of-the-art per-
formance in sequence-to-sequence learning (Choe
and Charniak, 2016; Jia and Liang, 2016; Barzdins
and Gosko, 2016), it cannot directly solve intra-
sentential conference and decompositional seman-
tic analyses in our task. To achieve this goal,
we propose an encoder-decoder architecture in-
corporated with a coreference annotating mech-
anism5 and decompositional analysis. As illus-
trated in Fig. 3, Encoder transforms the input se-
quence into hidden states; Decoder reads the hid-
den states, and then at each time step generates
a token and creates its COREF link; Decomposi-
tional Analysis, based on the decoder output, per-
forms SPR analysis for predicate-argument pairs,
and factuality analysis for predicates.

4Future work could consider, e.g., a modified BLEU that
considers Levenshtein distance between tokens for a more ro-
bust partial-scoring in the face of transliteration errors.

5Similar coreference mechanism has been proposed by Ji
et al. (2017).



1668

Encoder

peopleh ) were reportedh ( deadh

Token Generation & Coreference Link 

peopleh ) were reportedh (30

SPR Module

AWARENESS
SENTIENT
INSTIGATION      
…

0.8
0.9
0.1
…

peopleh

Factuality 
Module

Decoder

FACTUAL     1.0

COREF

Decompostional
Analysis

Figure 3: Illustration of the model architecture.

5.1 Encoder
The encoder employs a bidirectional recurrent
neural network (Schuster and Paliwal, 1997) with
LSTM units (Hochreiter and Schmidhuber, 1997).
It encodes the input X = x1, . . . , xN 6 into a se-
quence of hidden states h = h1, . . . , hN . Each
hidden state hi is a concatenation of a left-to-right
hidden state

−→
hi and a right-to-left hidden state

←−
hi ,

5.2 Decoder
Given the encoder hidden states, the decoder pre-
dicts the linearized representation (as shown in
Fig. 2) according to the conditional probability
P (Y,C | X) which is decomposed as a product
of the decoding probabilities at each time step t:

P (Y,C | X) =
M∏
t=1

P (yt, ct | y<t, c<t, X) (1)

where yt is the decoded token at time step t, and
ct is the source of the COREF link for yt, i.e., the
antecedent of yt. The set of possible antecedents
of yt is A(t) = {�, y1, . . . , yt−1}: a dummy an-
tecedent � and all preceding tokens. � represents a
scenario, where the token is not a special symbol
“•”, and it refers to none of the preceding tokens.

6For simplicity, we use X (and Y ) to represent both to-
kens as well as their word embeddings.

y<t and c<t are the preceding tokens and their an-
tecedents. We omit y<t and c<t from the notation
when the context is unambiguous.

The decoding probability at each time step t is
decomposed as

P (yt, ct) = P (yt)P (ct|yt) (2)

where P (yt) is the token generation probability,
and P (ct|yt) is the antecedent probability.
Token Generation: The probability distribution
of the generated token yt is defined as

P (yt) = softmax(FFNNg(st, at)) (3)

where FFNNg is a two-layer feed-forward neural
network over the decoder hidden state st and the
attention-weighted vector at. st is computed by

st = RNN(yt−1, st−1), (4)

where RNN is a recurrent neural network using
LSTM. at is computed by the attention mechanism
(Bahdanau et al., 2014; Luong et al., 2015),

at =

N∑
i

αt,ihi, (5)

αt,i =
exp (s>t (Wahi + ba)))∑N
j=1 exp (s

>
t (Wahj + ba))

, (6)

where Wa is a transform matrix and ba is a bias.
Coref Link: The probability of yt referring to the
preceding token yk, i.e., ct = yk, is defined as

P (ct = yk|yt) =
exp (SCORE(yt, yk))∑

y′k∈A(t)
exp (SCORE(yt, yk′))

,

(7)

SCORE(yt, yk) is a pairwise score for a COREF link
from yk to yt, defined as:

SCORE(yt, yk) = sc(yt) + sp(yk) + sa(yt, yk)
(8)

There are three factors in this pairwise score,
which is akin to Lee et al. (2017): (1) sc(yt),
whether yt should refer to a preceding instance;
(2) sp(yk), whether yk shoud be a candidate
source of such a coreference; and (3) sa(yt, yk),
whether yk is an antecedent of yt.

Fig. 4 shows the details of the scoring architec-
ture. At the core of the three factors are vector
representations γ(yt) for each token yt, which is
described in detail in the following section. Given



1669

yk yt

Token 
representations

Preceding token score 
sp(yk) 

Antecedent
score

COREF link score 
SCORE(yt, yk) 

Current token score 
sc(yt) 

sa(yt, yk) 

γ 

Figure 4: Scoring architecture in the copy mech-
anism between a preceding token yk and the cur-
rently considered token yt.

the currently considered token yt and a preceding
token yk, the scoring functions above are com-
puted via standard feed-foward neural networks:

sc(yt) =wc · FFNNc(γ(yt)) (9)
sp(yk) =wp · FFNNp(γ(yk)) (10)

sa(yt, yk) =wa · FFNNa
(
[γ(yt), γ(yk

)
,

γ(yt) ◦ γ(yk)]) (11)

where · denotes dot product, ◦ denotes element-
wise multiplication, and FFNN denotes a two-layer
feed-foward neural network over the input. The in-
put of FFNNa is a concatenation of vector represen-
tations γ(yt) and γ(yk), and their explicit element-
wise similarity γ(yt) ◦ γ(yk).
Token representations: To accurately predict
COREF link scores as well as decompositional
analysis (which is described in the following sec-
tion), we consider three types of information in
each token representation γ(yt): (1) the token it-
self yt, (2) on the decoder side, the preceding con-
text y<t, and (3) on the encoder side, the input se-
quence X = x1, . . . , xN .

The lexical information of the token itself yt
is represented by its word embedding et. The
preceding context y<t is encoded by the decoder
RNN in Equation (4). We use the decoder hidden
state st to represent the preceding context informa-
tion. The encoder-side context is represented by
an attention-weighted weight at defined in Equa-
tion (6). All the above information is concatenated
to produce the final token representation γ(yt):

γ(yt) = [et, st, at] (12)

5.3 Decompositional Analyses
The decompositional analyses D contains scores
for Semantic Proto-Role (SPR) properties DSPR,
and scores for event factuality DFACT.

SPR: Given a predicate-argument pair (yi, yj), we
denote the score for SPR property p as D(yi,yj)SPRp .
As shown in Fig. 3, we concatenate the token rep-
resentations of predicate and argument head to-
kens γ(yi) and γ(yj) as the input to a SPR mod-
ule. We employ the state-of-the-art SPR module
in Rudinger et al. (2018a), defined as:

D̂
(yi,yj)
SPRp =WSPRpReLU(Wshared[γ(yi), γ(yj)])

(13)
where Wshared is the weight matrix shared across
all properties. WSPRp is the weight matrix for SPR
property p. Then, the log-likelihood of the score of
SPR property p is defined as the negative L2 loss,
i.e., −|D̂(yi,yj)SPRp −D

(yi,yj)
SPRp |2.

Factuality: We consider predicting event factual-
ity as a scalar regression problem (White et al.,
2016), and denote the factuality score of predicate
yk as D

(yk)
FACT. As shown in Fig. 3, we take the to-

ken representation of predicate head token γ(yk)
as the input to the state-of-the-art factuality mod-
ule (Rudinger et al., 2018c):

D̂
(yk)
FACT = V2ReLU (V1γ(yk) + b1) + b2, (14)

where V1 and V2 are weight matrices, and b1 and
b2 are biases. The log-likelihood of factuality
score is defined as negative of the Huber loss (Hu-
ber, 1964) with δ = 1.

We assume conditional independence among
decompositional analysis:

P (D|X,Y,C) =
∏

(yi,yj)

∏
p

(D
(yi,yj)
SPRp |X,Y,C)∏

yk

P (D
(yk)
FACT|X,Y,C) (15)

5.4 Learning
Given the input sentence X , the output sequence
of tokens Y , and the COREF links C, and the de-
compositional analysis D, the objective is to min-
imize the below negative log-likelihood:

L =− logP (Y,C,D|X)

=−
M∑
t=1

[µ1 logP (yt) + µ2 logP (ct|yt)]−

µ3 logP (D|X,Y,C)
To increase the convergence rate, we pretrain

the model by setting the weights µ1 = 1 and
µ2 = µ3 = 0 to only optimize the token gener-
ation accuracy. After the model converges, we set
µ2 = µ3 = 1 and lower µ1 = 0.1.



1670

S metric BLEUINST MAESPR MAEFACT
Precision Recall F1

Pipeline 35.08 30.10 32.39 15.03 N/A N/A

Variant (a) 39.31 32.93 35.84 16.74 0.75 1.11

Variant (b) 42.76 33.20 37.38 17.71 0.74 1.14

Variant (c) 41.74 33.28 37.03 18.01 0.80 1.14

Our model 45.33 33.88 38.78 19.61 0.71 1.06

Table 1: Evaluation of results on the test set.
(In-domain test results are shown in Appendix D.)

6 Experiments

We now describe the evaluation data, baselines,
and experimental results. Hyperparameter settings
are reported in Appendix C.

6.1 Data

We choose Chinese as the source language and En-
glish as the target language. For test, we selected
270 sentences from the Universal Dependencies
(UD) English Treebank (Silveira et al., 2014) test
set, which have human-annotated SPR (White
et al., 2016) and factuality (Rudinger et al., 2018c)
analyses. We then created linearized represen-
tations for these sentences using PredPatt based
on their gold UD syntax. Meanwhile, the Chi-
nese translations of these sentences were created
by crowdworkers on Amazon Mechanical Turk.
The test dataset will be released upon publica-
tion. For training, we first collected about 1.8M
Chinese-English sentence bitexts from the GALE
project (Cohen, 2007), then tokenized Chinese
sentences with Stanford Word Segmenter (Chang
et al., 2008). We created linearized representa-
tions for English sentences using PredPatt based
on automatic UD syntax generated by SyntaxNet
Parser (Andor et al., 2016), and added SPR and
factuality annotations using the state-of-the-art
models (Rudinger et al., 2018b,c) trained on SPR
v2.x and It-happened v2.0 respectively.7 We hold
out 20K training sentences for validation and in-
domain test. Table 2 reports the dataset statistics.

6.2 Variants

We evaluate our model described in Section 5 and
three variants: (a) We replace the coreference an-
notating mechanism by randomly choosing an an-

7Both datasets are available at http://decomp.net

No. sents Source

Train 1,879,172 GALE
Validation 10,000 GALE
In-domain Test 10,000 GALE
Test 270 UD Treebank

Table 2: Statistics of the evaluation data.

tecedent from all preceding instances. (b) We pre-
process the data by replacing the special symbol
“•” with the syntactic head of its antecedent. Dur-
ing training and testing, we replace the corefer-
ence annotating mechanism with a heuristic that
solves coreference by randomly choosing an an-
tecedent among preceding instances which have
the same syntactic head. (c) We remove the
decoder-side information in the token representa-
tion γ(yt) defined in Equation (12) and only keep
the encoder-side information at. We also include
a Pipeline approach where Chinese sentences are
first translated into English by a neural machine
translation system (Klein et al., 2017) and are then
annotated by a UD parser (Andor et al., 2016). The
UDS linearized representation of Pipeline are cre-
ated by PredPatt based the automatic UD parses.

6.3 Results

Table 1 reports the experimental results on the test
set. Results on the in-domain test set are simi-
lar and shown in Appendix D. In Table 1, S met-
ric (defined in Section 4) measures the similarity
between predicted and reference graph represen-
tations. Based on the optimal variable mapping
provided by the S metric, we are able to eval-
uate our model and the variants in different as-
pects: BLEUINST measures the BLEU score of
all matched instance edges; MAESPR measures the

http://decomp.net


1671

mean absolute error of SPR property scores of all
matched argument edges; and MAEFACT measures
the mean absolute error of factuality scores of all
matched attribute edges.

Overall, our proposed model outperforms the
variants in every aspect. Variants (a) and (b) use
simple heuristics to solve coreference, and achieve
reasonable results: they both employ sequence-to-
sequence models to predict graph representations,
which can be considered a replica of state-of-
the-art approaches for structured prediction (Choe
and Charniak, 2016; Barzdins and Gosko, 2016;
Peng et al., 2017). Compared to our model which
employs the coreference annotating mechanism,
these two variants suffer notable loss in the pre-
cision of S metric. As a result, their performance
drops on the other metrics. Variant (c) only uses
the encoder-side information for token representa-
tion, resulting in significant loss in MAESPR and
MAEFACT. In the pipeline approach, each compo-
nent is trained independently. During test, resid-
ual errors from each component are propagated
through the pipeline. As expected, it shows a sig-
nificant performance drop.

Precision Recall F1

Variant (a) 10.38 31.23 15.58
Variant (b) 88.42 50.59 64.36
Variant (c) 84.12 35.99 50.41
Our model 96.63 97.62 97.12

Table 3: Coreference evaluation (MUC) based on
forced decoding.

Coreference occurs 589 times in the test set. To
evaluate the coreference accuracy of our model,
we force the decoder to generate the reference tar-
get sequence, and only predict coreference via the
copy mechanism, or its variants. In Table 3, we
report the precision, recall, and F1 for the stan-
dard MUC using the official coreference scorer
of the CoNLL-2011/2012 shared tasks (Pradhan
et al., 2014). Since coreference in our setup oc-
curs at the sentence level, our model achieves high
performance. Variant (a) randomly choosing an-
tecedents performs poorly, whereas variant (b),
which solves coreference only based on syntactic
heads, achieves a relatively high score. Variant (c)
demonstrates that only using encoder-side infor-
mation in the coreference annotating mechanism
leads a significant performance drop.

Our
Model

Monolingual
SOTA

awareness 0.852 0.879
change location 0.491 0.492
change possession 0.448 0.488
changed 0.307 0.352
change state 0.362 0.373
existed after 0.426 0.478
existed before 0.602 0.618
existed during 0.336 0.358
instigation 0.597 0.59
partitive 0.317 0.359
sentient 0.849 0.88
volition 0.818 0.837
was for benefit 0.566 0.578
was used 0.268 0.203

Table 4: Pearson coefficient of each SPR property.

Since our model and the state-of-the-art mono-
lingual SPR model (Rudinger et al., 2018c) use
the same test set, we are able to compare the per-
formance of our model against the monolingual
model by forcing the decoder and the coreference
mechanism to create the reference graph repre-
sentation and only predicting the SPR property
scores. Table 4 shows the Pearson coefficient of
each SPR property. While our model only has the
access to the sentence in the source language dur-
ing the encoding stage,8 the performance is com-
parable to the state-of-the-art monolingual model.

7 Conclusions

We introduce the task of cross-lingual decomposi-
tional semantic parsing, which maps content pro-
vided in a source language into decompositional
analysis based on a target language. We present:
UDS graph/linearized representations as the tar-
get semantic interface, the S metric for evalua-
tion, and the Chinese-English decompositional se-
mantic parsing dataset. We propose an end-to-
end learning approach with a coreference anno-
tating mechanism which outperforms three strong
baselines. We separately evaluate the coreference
mechanism and SPR prediction, showing promis-
ing results. The representations for cross-lingual
decompositional semantics, the evaluation metric,
and the evaluation dataset provided in this work

8The state-of-the-art monolingual SPR model directly en-
codes the sentence in the target language.



1672

will be beneficial to the increasing interests in se-
mantic analysis and cross-lingual applications.

Acknowledgments

Thank you to the anonymous reviewers for their
feedback. This work was supported in part by the
JHU Human Language Technology Center of Ex-
cellence (HLTCOE), and DARPA LORELEI and
AIDA. The U.S. Government is authorized to re-
produce and distribute reprints for Governmental
purposes. The views and conclusions contained in
this publication are those of the authors and should
not be interpreted as representing official policies
or endorsements of DARPA or the U.S. Govern-
ment.

References
Omri Abend and Ari Rappoport. 2013. Universal con-

ceptual cognitive annotation (ucca). In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 228–238, Sofia, Bulgaria. Association
for Computational Linguistics.

Omri Abend and Ari Rappoport. 2017. The state of the
art in semantic representation. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
77–89, Vancouver, Canada. Association for Compu-
tational Linguistics.

Daniel Andor, Chris Alberti, David Weiss, Aliaksei
Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
malized transition-based neural networks. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 2442–2452, Berlin, Germany. Asso-
ciation for Computational Linguistics.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Jason Baldridge and Geert-Jan Kruijff. 2002. Cou-
pling ccg and hybrid logic dependency semantics.
In Proceedings of 40th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 319–
326, Philadelphia, Pennsylvania, USA. Association
for Computational Linguistics.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse, pages 178–186.

Guntis Barzdins and Didzis Gosko. 2016. Riga at
semeval-2016 task 8: Impact of smatch extensions
and character-level neural translation on amr parsing
accuracy. In Proceedings of the 10th International
Workshop on Semantic Evaluation (SemEval-2016),
pages 1143–1147, San Diego, California. Associa-
tion for Computational Linguistics.

Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In LREC 2012, Eighth Interna-
tional Conference on Language Resources and Eval-
uation.

Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a ccg parser.
In Proceedings of the 20th International Confer-
ence on Computational Linguistics, COLING ’04,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Johan Bos, Kilian Evang, Johannes Bjerva, Lasha
Abzianidze, Hessel Haagsma, Rik van Noord, Pierre
Ludmann, and Duc-Duy Nguyen. 2017. The par-
allel meaning bank: Towards a multilingual corpus
of translations annotated with compositional mean-
ing representations. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL 2017, Valen-
cia, Spain, April 3-7, 2017, Volume 2: Short Papers,
pages 242–247.

Shu Cai and Kevin Knight. 2013. Smatch: an evalua-
tion metric for semantic feature structures. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 748–752, Sofia, Bulgaria. Associa-
tion for Computational Linguistics.

Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 224–232, Columbus, Ohio.
Association for Computational Linguistics.

Do Kook Choe and Eugene Charniak. 2016. Parsing
as language modeling. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2331–2336, Austin, Texas.
Association for Computational Linguistics.

Jordan Cohen. 2007. The gale project: A description
and an update. In Automatic Speech Recognition
& Understanding, 2007. ASRU. IEEE Workshop on,
pages 237–237. IEEE.

Ann Copestake. 2007. Semantic composition with
(robust) minimal recursion semantics. In Proceed-
ings of the Workshop on Deep Linguistic Processing,
pages 73–80. Association for Computational Lin-
guistics.



1673

Ann Copestake. 2009. Invited Talk: slacker semantics:
Why superficiality, dependency and avoidance of
commitment can be the right way to go. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the ACL (EACL 2009), pages 1–9, Athens,
Greece. Association for Computational Linguistics.

Ann Copestake, Dan Flickinger, Rob Malouf, Susanne
Riehemann, and Ivan Sag. 1995. Translation us-
ing minimal recursion semantics. In In Proceedings
of the Sixth International Conference on Theoretical
and Methodological Issues in Machine Translation.

Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A Sag. 2005. Minimal recursion semantics: An
introduction. Research on Language and Computa-
tion, 3(2-3):281–332.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
33–43, Berlin, Germany. Association for Computa-
tional Linguistics.

Bonnie Dorr and Nizar Habash. 2002. Interlingua ap-
proximation: A generation-heavy approach. In In
Proceedings of AMTA-2002. University of Chicago
Press.

Kilian Evang and Johan Bos. 2016. Cross-lingual
learning of an open-domain semantic parser. In Pro-
ceedings of COLING 2016, the 26th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 579–588. The COLING 2016 Or-
ganizing Committee.

Manaal Faruqui and Shankar Kumar. 2015. Multi-
lingual open relation extraction using cross-lingual
projection. In Proceedings of the 2015 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 1351–1356, Denver, Colorado.
Association for Computational Linguistics.

Pascale Fung and Benfeng Chen. 2004. Biframenet:
Bilingual frame semantics resources construction
by cross-lingual induction. In In Proceedings of
the 20th International Conference on Computational
Linguistics, pages 931–935.

Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
369–377, Suntec, Singapore. Association for Com-
putational Linguistics.

Irene Heim. 1988. The Semantics of Definite and Indef-
inite Noun Phrases. Ph. d. dissertation, New York
and London.

Jerry R. Hobbs. 2003. Discourse and Inference.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Peter J Huber. 1964. Robust estimation of a location
parameter. The annals of mathematical statistics,
pages 73–101.

Chung Hee Hwang and Lenhart K. Schubert. 1994. In-
terpreting tense, aspect and time adverbials: A com-
positional, unified approach. In Temporal Logic,
pages 238–264, Berlin, Heidelberg. Springer Berlin
Heidelberg.

Yangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin
Choi, and Noah A. Smith. 2017. Dynamic entity
representations in neural language models. In Pro-
ceedings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1830–
1839, Copenhagen, Denmark. Association for Com-
putational Linguistics.

Robin Jia and Percy Liang. 2016. Data recombination
for neural semantic parsing. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
12–22, Berlin, Germany. Association for Computa-
tional Linguistics.

H Kamp. 1981. A theory of truth and semantic rep-
resentation, 277-322, jag groenendijk, tmv janssen
and mbj stokhof, eds.

G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M.
Rush. 2017. OpenNMT: Open-Source Toolkit for
Neural Machine Translation. ArXiv e-prints.

Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle-
moyer. 2017. End-to-end neural coreference reso-
lution. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 188–197, Copenhagen, Denmark. Asso-
ciation for Computational Linguistics.

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Proceedings of
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1412–1421, Lis-
bon, Portugal. Association for Computational Lin-
guistics.

Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 62–72, Edinburgh, Scotland, UK. Asso-
ciation for Computational Linguistics.

Hongyuan Mei, Sheng Zhang, Kevin Duh, and Ben-
jamin Van Durme. 2018. Halo: Learning Semantics-
Aware Representations for Cross-Lingual Informa-
tion Extraction. In Proceedings of the Seventh Joint
Conference on Lexical and Computational Seman-
tics, pages 142–147. Association for Computational
Linguistics.



1674

George A. Miller. 1995. Wordnet: A lexical database
for english. Commun. ACM, 38(11):39–41.

Teruko Mitamura, Eric H. Nyberg, and Jaime G. Car-
bonell. 1991. An efficient interlingua translation
system for multi-lingual document production. In
Proceedings of Machine Translation Summit III,
pages 2–4.

Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 629–637, Jeju
Island, Korea. Association for Computational Lin-
guistics.

Sebastian Padó and Mirella Lapata. 2005. Cross-
linguistic projection of role-semantic information.
In Proceedings of the Conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, HLT ’05, pages 859–866,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Sebastian Padó and Mirella Lapata. 2009. Cross-
lingual annotation projection of semantic roles. J.
Artif. Int. Res., 36(1):307–340.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational linguistics,
31(1):71–106.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Xiaochang Peng, Chuan Wang, Daniel Gildea, and Ni-
anwen Xue. 2017. Addressing the data sparsity is-
sue in neural amr parsing. In Proceedings of the
15th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Volume 1,
Long Papers, pages 366–375, Valencia, Spain. As-
sociation for Computational Linguistics.

Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-
uard Hovy, Vincent Ng, and Michael Strube. 2014.
Scoring coreference partitions of predicted men-
tions: A reference implementation. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 30–35, Baltimore, Maryland. Associa-
tion for Computational Linguistics.

Siva Reddy, Oscar Täckström, Slav Petrov, Mark
Steedman, and Mirella Lapata. 2017. Universal se-
mantic parsing. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 89–101, Copenhagen, Denmark.
Association for Computational Linguistics.

Drew Reisinger, Rachel Rudinger, Francis Ferraro,
Craig Harman, Kyle Rawlins, and Benjamin Van
Durme. 2015. Semantic proto-roles. Transactions
of the Association for Computational Linguistics,
3:475–488.

Rachel Rudinger, Adam Teichert, Ryan Culkin, Sheng
Zhang, and Benjamin Van Durme. 2018a. Neu-
ral davidsonian semantic proto-role labeling. arXiv
preprint arXiv:1804.07976.

Rachel Rudinger, Adam Teichert, Ryan Culkin, Sheng
Zhang, and Benjamin Van Durme. 2018b. Neu-
ral davidsonian semantic proto-role labeling. arXiv
preprint arXiv:1804.07976.

Rachel Rudinger, Aaron Steven White, and Benjamin
Van Durme. 2018c. Neural models of factuality.
In Proceedings of the Annual Meeting of the North
American Association of Computational Linguistics
(NAACL).

Roser Saurı́ and James Pustejovsky. 2009. Factbank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227.

Lenhart Schubert. 2014. From treebank parses to
episodic logic and commonsense inference. In Pro-
ceedings of the ACL 2014 Workshop on Semantic
Parsing, pages 55–60, Baltimore, MD. Association
for Computational Linguistics.

Lenhart Schubert. 2015. Semantic representation. In
Proceedings of AAAI Conference on Artificial Intel-
ligence.

Lenhart K Schubert. 2000. The situations we talk
about. In Logic-based artificial intelligence, pages
407–439. Springer.

Lenhart K. Schubert and Chung Hee Hwang. 2000.
Natural language processing and knowledge repre-
sentation. chapter Episodic Logic Meets Little Red
Riding Hood: A Comprehensive Natural Represen-
tation for Language Understanding, pages 111–174.
MIT Press, Cambridge, MA, USA.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673–2681.

Natalia Silveira, Timothy Dozat, Marie-Catherine
de Marneffe, Samuel Bowman, Miriam Connor,
John Bauer, and Christopher D. Manning. 2014. A
gold standard dependency corpus for English. In
Proceedings of the Ninth International Conference
on Language Resources and Evaluation (LREC-
2014).

Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA, USA.

Stephanie Strassel and Jennifer Tracey. 2016. Lorelei
language packs: Data, tools, and resources for
technology development in low resource languages.



1675

In Proceedings of the Tenth International Confer-
ence on Language Resources and Evaluation (LREC
2016), Paris, France. European Language Resources
Association (ELRA).

Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2004. Cross-lingual information extraction sys-
tem evaluation. In Proceedings of the 20th inter-
national Conference on Computational Linguistics,
page 882. Association for Computational Linguis-
tics.

Lappoon R Tang and Raymond J Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In European
Conference on Machine Learning, pages 466–477.
Springer.

Mengqiu Wang and Christopher D. Manning. 2014.
Cross-lingual projected expectation regularization
for weakly supervised learning. Transactions of the
Association of Computational Linguistics, 2:55–66.

Aaron Steven White, Drew Reisinger, Keisuke Sak-
aguchi, Tim Vieira, Sheng Zhang, Rachel Rudinger,
Kyle Rawlins, and Benjamin Van Durme. 2016.
Universal decompositional semantics on universal
dependencies. In Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 1713–1723, Austin, Texas. Asso-
ciation for Computational Linguistics.

Yuk Wah Wong and Raymond Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of the Human Language
Technology Conference of the NAACL, Main Con-
ference, pages 439–446, New York City, USA. As-
sociation for Computational Linguistics.

David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the First International Conference
on Human Language Technology Research, HLT
’01, pages 1–8, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proceedings of the IJCNLP-08 Workshop
on NLP for Less Privileged Languages.

Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the Twenty-First Con-
ference on Uncertainty in Artificial Intelligence,
UAI’05, pages 658–666, Arlington, Virginia, United
States. AUAI Press.

Sheng Zhang, Kevin Duh, and Benjamin Van Durme.
2017a. MT/IE: Cross-lingual open information ex-
traction with neural sequence-to-sequence models.
In Proceedings of the 15th Conference of the Euro-
pean Chapter of the Association for Computational

Linguistics: Volume 2, Short Papers, pages 64–70,
Valencia, Spain. Association for Computational Lin-
guistics.

Sheng Zhang, Kevin Duh, and Benjamin Van Durme.
2017b. Selective Decoding for Cross-lingual Open
Information Extraction. In Proceedings of the
Eighth International Joint Conference on Natural
Language Processing (Volume 1: Long Papers),
pages 832–842, Taipei, Taiwan. Asian Federation of
Natural Language Processing.

Sheng Zhang, Rachel Rudinger, and Benjamin Van
Durme. 2017c. An Evaluation of PredPatt and Open
IE via Stage 1 Semantic Role Labeling. In IWCS
2017 — 12th International Conference on Compu-
tational Semantics — Short papers.


