Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 120–128,

Chiang Mai, Thailand, November 8 – 13, 2011. c(cid:13)2011 AFNLP

120

Japanese Pronunciation Prediction as
Phrasal Statistical Machine Translation

Jun Hatori1

Hisami Suzuki2

1Department of Computer Science, University of Tokyo

7-3-1 Hongo, Bunkyo, Tokyo 113-0033, Japan

2Microsoft Research / One Microsoft Way, Redmond, WA 98052, USA

hatori@is.s.u-tokyo.ac.jp

hisamis@microsoft.com

Abstract

This paper addresses the problem of pre-
dicting the pronunciation of Japanese text.
The difﬁculty of this task lies in the high
degree of ambiguity in the pronunciation
of Japanese characters and words. Previ-
ous approaches have either considered the
task as a word-level classiﬁcation problem
based on a dictionary, which does not fare
well in handling out-of-vocabulary (OOV)
words; or solely focused on the pronun-
ciation prediction of OOV words without
considering the contextual disambiguation
of word pronunciations in text.
In this
paper, we propose a uniﬁed approach
within the framework of phrasal statisti-
cal machine translation (SMT) that com-
bines the strengths of the dictionary-based
and substring-based approaches. Our ap-
proach is novel in that we combine word-
and character-based pronunciations from a
dictionary within an SMT framework: the
former captures the idiosyncratic proper-
ties of word pronunciation, while the latter
provides the ﬂexibility to predict the pro-
nunciation of OOV words. We show that
based on an extensive evaluation on vari-
ous test sets, our model signiﬁcantly out-
performs the previous state-of-the-art sys-
tems, achieving around 90% accuracy in
most domains.

1 Introduction
This paper1 explores the problem of assigning pro-
nunciation to Japanese text, which consists of a
mixture of ideographic and phonetic characters.
The task is naturally important for the text-to-
speech application (Schroeter et al., 2002), and
has been researched in that context as letter-to-
phoneme conversion, which converts an ortho-

graphic character sequence into phonemes.
In
addition to speech applications, the task is also
crucial for those languages such as Chinese and
Japanese, where users generally type in the pro-
nunciations of words, which are then converted
into the desired character string via the software
application called input methods (e.g. Gao et al.
(2002a); Gao et al. (2002b)).

Predicting the pronunciation of Japanese text
is particularly challenging because the word and
character pronunciations are highly ambiguous.
Japanese orthography employs four sets of char-
acters: hiragana and katakana (called generally
as kana), which are syllabary systems and thus
phonemic; kanji, which is ideographic and con-
sists of several thousand characters; and Roman
alphabet. Out of these, kanji characters typi-
cally have multiple possible pronunciations2; es-
pecially those in frequent use tend to have many
— between 5 and 10, sometimes as many as 20.
This yields an exponential number of pronunci-
ation possibilities when multiple kanji characters
are combined in a word. Also, the pronunciation
of a word is frequently idiosyncratic.

This idiosyncratic property of the word pronun-
ciation naturally motivates us to take a dictionary-
based approach. Traditionally, most approaches to
Japanese pronunciation prediction have regarded
the problem as a word pronunciation disambigua-
tion task. Since there are no white spaces be-
tween words in Japanese text, these approaches
ﬁrst segment an input sentence/phrase into words,
and then select a word-level pronunciation among
those deﬁned in a dictionary (Nagano et al., 2006;
Neubig and Mori, 2010). For example, given a
word “人気”, these methods try to select the most
appropriate pronunciation out of the three dictio-
nary entries: ninki (popularity), hitoke (sign of
life) and jinki (people’s atmosphere), depending
on the context. However, in these approaches, seg-

1This work was conducted during the ﬁrst author’s intern-

ship at Microsoft Research.

2In UniDic (Den et al., 2007), the average number of pro-

nunciations per kanji character is 2.3.

121

mentation errors tend to result in the failure of the
following step of pronunciation prediction. More-
over, since the dictionary-based approach is inap-
plicable to those words that are not in the dictio-
nary, there needs to be a separate mechanism for
handling out-of-vocabulary (OOV) words.

Nonetheless, the problem of OOV words has
received little attention to date. Traditional sys-
tems either bypass this problem completely and
assign no pronunciation to OOV words, as Mecab
(Kudo et al., 2004), a Japanese morphological
analyzer, does; or use a simple model to cover
them (e.g. Neubig and Mori (2010) uses a noisy-
channel model with a character bigram language
model). Our previous work (Hatori and Suzuki,
2011) explicitly addresses the problem of predict-
ing the pronunciation of OOV words, but focuses
solely on predicting the pronunciation of nouns
that are found in Wikipedia in isolation, and does
not address the contextual disambiguation of pro-
nunciation at the sentence level.

In this paper, we propose a uniﬁed approach
based on the framework of phrasal statistical ma-
chine translation (SMT), addressing the whole
sentence pronunciation assignment while integrat-
ing the OOV pronunciation prediction as part of
the whole task. The novelty of our approach
lies in using word and single-character pronunci-
ations from a dictionary within the SMT frame-
work: the former captures the idiosyncratic prop-
erties of word pronunciation, while the latter pro-
vides the ﬂexibility to predict the pronunciation of
OOV words based on the sequence of pronuncia-
tions at the substring level.

(partial) words).

In addressing the pronunciation disambigua-
tion problem within the framework of phrasal
SMT, we extend the use of composed operations,
which were applied in a limited manner in Ha-
tori and Suzuki (2011). Within our dictionary-
based model, the composed operations are able to
incorporate the composition of dictionary words
(i.e. phrases) as well as substrings of the char-
acter sequence (i.e.
In this
sense, our approach is more like a standard mono-
tone phrasal SMT, rather than the substring-based
string transduction. We also propose to use the
joint n-gram model as a feature function, which
has been proven to be effective in the letter-to-
phoneme conversion task (Bisani and Ney, 2008;
Jiampojamarn et al., 2010).
In the context of
our current task, this feature not only incorporates
smoothed contextual information for the purpose
of pronunciation disambiguation, but also captures
the dependency between single-kanji pronuncia-

tions, which is effective for predicting the pronun-
ciation of OOV words.

We collected an extensive evaluation set for the
task, including newswire articles, search query
logs, person names, and Wikipedia-derived in-
stances. Using these test sets, we show that
our model signiﬁcantly outperforms the previous
state-of-the-art systems, achieving around 90%
accuracy in most test domains, which is the best
known result on the task of Japanese pronuncia-
tion prediction to date. We also give a detailed
analysis of the comparison of the proposed model
with an SVM-based model, KyTea (Neubig and
Mori, 2010), through which we hope to shed light
on the remaining issues in solving this task.

2 Background
2.1 Pronunciation Prediction: Task Setting
We deﬁne the task of pronunciation prediction as
converting a string of orthographic characters rep-
resenting a sentence (or a word or phrase) into a
sequence of hiragana, which corresponds to how
the string is pronounced. For example, given a
Japanese sentence “東京都美術館の狩野探幽展に行っ
た。” (“I went to the Exhibition of Tanyu Kano at
the Tokyo Metropolitan Art Museum.”), the sys-
tem is expected to output a sequence of hiragana,
“とうきょうとびじゅつかんのかのうたんゆうてんにい
った。”, pronounced as tookyoo to bijutsukan no
kanoo tanyuu ten ni itta. The task involves two
sub-problems: (a) contextual disambiguation of a
word pronunciation, e.g., 行った can be pronounced
either as いった itta “went” or おこなった okonatta
“did” depending on the context; (b) pronunciation
prediction of OOV words, e.g., in the above exam-
ple, 狩野探幽展 (“the Exhibition of Tanyu Kano”)
is not likely to be in the dictionary, so the pronun-
ciation must be reasonably guessed based on the
possible pronunciations of individual characters.

2.2 Related Work
Our research on pronunciation prediction is in-
spired by previous research on string transduction.
The most directly relevant is the work on letter-to-
phoneme conversion. Previous approaches to this
task include joint n-gram models (e.g., Bisani and
Ney (2002); Chen (2003); Bisani and Ney (2008))
and discriminatively trained substring-based mod-
els (e.g., Jiampojamarn et al. (2007); Jiampoja-
marn et al. (2008)). This task is typically evaluated
at the word level, and therefore does not include
contextual disambiguation.

Similar techniques to the letter-to-phoneme task

122

have also been widely applied to the translitera-
tion task (Knight and Graehl (1998)). The most
relevant to the current task include an approach
based on substring operations in the SMT frame-
work (e.g., Sherif and Kondrak (2007), Cherry and
Suzuki (2009)), and those that use joint n-gram
estimation method for the task of transliteration
(e.g., Li et al. (2004); Jiampojamarn et al. (2010)).
However, similarly to the letter-to-phoneme task,
the contextual disambiguation of the words has not
received much attention.

The task of Japanese pronunciation prediction
itself has been a topic of investigation. Sumita and
Sugaya (2006) proposed a method to use the web
for assigning word pronunciation, but their focus
is limited to the pronunciation disambiguation of
known proper nouns. Kurata et al. (2007) and
Sasada et al. (2009) discuss the methods of dis-
ambiguating new word pronunciation candidates
using speech data. Nagano et al. (2006) and Mori
et al. (2010b) investigated the use of the joint n-
gram estimation to this task.

More recently, Neubig and Mori (2010) pro-
posed a classiﬁer-based system called KyTea,
which is one of the current state-of-the-art systems
for the task of Japanese pronunciation prediction.
As we use this system as one of our baseline sys-
tems, we describe this work in some detail here.
KyTea exploits an SVM-based two-step approach,
which performs a word segmentation step, fol-
lowed by a pronunciation disambiguation step for
each word segment. In the pronunciation predic-
tion step, if the word in question exists in the dic-
tionary, KyTea uses character and character-type
n-grams within a window as features for the SVM
classiﬁer. For OOV words, a simple OOV model
based on a noisy channel model with a character
bigram language model is used. While KyTea uses
the discriminative indicator features, our model in-
stead uses character/joint n-gram language mod-
els and composed operations (to be explained in
Section 3.3.2) to capture the context for the pur-
pose of pronunciation disambiguation. The use of
the indicator features essentially requires proba-
bilistic optimization of a large number of weights,
making the training less scalable than our model,
which only requires frequencies of operations and
phrases in the training data.

In our previous work (Hatori and Suzuki, 2011),
we addressed the pronunciation prediction of
Japanese words in a semi-supervised, substring-
based framework, using word-pronunciation pairs
automatically extracted from Wikipedia. Though
we obtained more than 70% accuracy on

Figure 1: Overview of the model.

Wikipedia data, the model is quite speciﬁc to han-
dling the noun phrases in Wikipedia, and it is not
clear if the approach can handle the pronuncia-
tion assignment of a general text, which includes
the pronunciation prediction and disambiguation
of the words of all types at the sentence level.
Since our current work is an extension of this ap-
proach, we also adopt our previous work as one of
our baseline models in Section 4.4.

3 Pronunciation Prediction Model
This section describes our phrasal SMT-based
approach to pronunciation prediction, which is
an extension of our previous work (Hatori and
Suzuki, 2011). We assume that the task of trans-
lating a Japanese orthography string to a hiragana
string is basically monotone and without insertion
or deletion. The overview of our model is given
in Figure 1. The components of the model will be
explained below.

3.1 Training and Decoding
As is widely used in SMT research (Och, 2003),
we adopt a discriminative learning framework that
uses component generative models as real-valued
features (Cherry and Suzuki, 2009). Given the
source sequence s and the target character se-
quence t, we deﬁne real-valued features over s and
t, fi(s, t) for i ∈ {1, . . . , n}. The score of a se-
quence pair hs, ti is given by the inner product of
the weight vector λ = (λ1, . . . , λn) and the fea-
ture vector f (s, t).

For the training of model parameters, we use the
averaged perceptron (Collins and Roark, 2004):
given a training corpus of transduction derivations,
each of which describes a word/substring opera-
tion sequence converting s into t, the perceptron
iteratively updates the weight vector every time it
encounters an instance for which the model out-
puts a wrong sequence. For decoding, we use a

source
sentence
dict.-based
operations

東 京 都 美 術 館 に 行 っ た 。

Tokyo Metro-
東京 都 美術館

politan Art Museum to
に

went

行った

と
to

とうきょう

tookyoo
東京都

びじゅつかん

bijutsukan
美術館

いった

に
ni
に行った

itta

とうきょうと びじゅつかん

にいった

composed 
operations

joint n-gram

.
。

。
.
。

。

東京／とうきょう

char. n-gram

都／と

美術館／びじゅつかん

に／に 行った／いった

。／。

と う き ょ う と び じ ゅ つ か ん に い っ た

。

123

Japanese text. Since the original corpora do not
have any word segmentation or word/substring
alignments, we ﬁrst need to obtain them to con-
struct the translation table for the decoder. In pre-
vious work, KyTea used a corpus that is manu-
ally aligned using words as a unit of alignment,
while Hatori and Suzuki (2011) used an unsuper-
vised substring-based alignment. The former is
not scalable easily, while the latter cannot take
advantage of existing dictionaries. In this work,
we use a novel application of dictionary-based
phrasal decoder in order to create an aligned cor-
pus, which allows us to use dictionary informa-
tion while learning substring-based alignments for
handling OOV pronunciation prediction.

3.3.1 Dictionary-based model
In the dictionary-based model we propose, align-
ments are obtained using a phrasal decoder which
is based on a dictionary. This essentially treats the
dictionary entries as the minimal unit of substring
operations, instead of using single-kanji pronun-
ciations estimated from training corpora as in the
case of the substring-based model (Hatori and
Suzuki, 2011). We ﬁrst build a simple dictionary-
based decoder with only two features: the forward
translation probability and the phrase count; and
then use it to decode a paired corpus to obtain the
alignments between the source and target strings.
In this process, instances including any operation
that is not deﬁned in the dictionary are discarded;
this is a major difference with the substring-based
model of Hatori and Suzuki (2011), which uses all
instances of training data.

Since Japanese dictionaries typically include
single-kanji entries as well as word entries3,
dictionary-based substring operations actually
consist of both single-kanji (that is not a word
per se) and word pronunciations. This is why our
dictionary-based model is still able to handle OOV
words. We show in Section 5 that the beneﬁt of
removing noisy training samples by this process
outweighs the risk of discarding infrequent or non-
standard pronunciations that do not exist in the
dictionary.

3.3.2 Composed operations
Our previous work (Hatori and Suzuki, 2011) ex-
ploits composed operations in order to include lo-
cal contextual information in the substring-based
model. Given a paired corpus, they use an aligner
to obtain single-character alignments, which maps
3This is because each kanji character is a morpheme rep-

resenting a meaning, and is worth an entry in dictionaries.

Figure 2: Overview of the training.

stack decoder (Zens and Ney, 2004).

3.2 Features
For our baseline model features, we ﬁrst use those
from Hatori and Suzuki (2011): the bidirectional
translation probabilities, P (t|s) and P (s|t), the
target character n-gram probability, P (t), the tar-
get character count, and the phrase count.
In
addition, we incorporate the joint n-gram prob-
ability, P (s, t), as a feature (described in Sec-
tion 3.2.1). The estimation of the translation and
joint/character n-gram probabilities requires a set
of training corpus with source and target align-
ment at the word/substring level. Once these prob-
abilities have been estimated by using the fre-
quency of (the sequences of) operations in the
training set, we only need a small tuning set to
adjust the feature weights of the model. This
makes online training and domain adaptation easy,
and makes our model more scalable compared to
fully discriminative systems with indicator fea-
tures, such as KyTea.

3.2.1 Joint n-gram Language Model Feature
Motivated by the success in the transliteration task
(Jiampojamarn et al., 2010), we incorporate the
joint n-gram language model into our SMT-based
framework. The joint n-gram sequence is the se-
quence of operations used in the transduction: for
example, when a paired sentence “床屋に行く／とこ
やにいく” is decomposed into three operations “床
屋→とこや, に→に, 行く→いく”, the corresponding
joint n-gram sequence is “h 床屋, とこや i h に, に i
h 行く, いく i”. The effectiveness of this feature is
conﬁrmed in our experiments in Section 5.2.

3.3 Translation Table
The corpora we use are a collection of pairs of a
Japanese sentence and its hiragana sequence, as
described as “paired corpus” in Figure 2. These
are just like bilingual corpora if we regard the hi-
ragana sequence as monotonically translated from

東京都美術館に行った。／とうきょうとびじゅつかんにいった。
ふたご座流星群／ふたござりゅうせいぐん

paired corpus

aligned corpus

phrasal aligner

or

phrasal decoder

dictionary

東京／とうきょう  都／と  美術館／びじゅつかん  に／に  行った／いった  。／。
ふたご／ふたご  座／ざ  流星群／りゅうせいぐん

translation table
trans. probability

char. n-gram prob.
joint n-gram prob.

input (kanji+kana)
phrasal decoder

output (kana)

124

one kanji to one or more kana characters, which
are then composed into larger operations. This
procedure makes it possible to obtain longer align-
ments with limited memory, rather than using the
source phrase length larger than one. In the current
work, we extend the use of composed operations
so that they work properly with the joint n-gram
estimation.

The composed operations are beneﬁcial for cap-
turing contextual information. For example, the
phrase “行った” can be pronounced in two ways:
itta “went” and okonatta “did”, which cannot be
distinguished without any context. However, if
this phrase is preceded by a hiragana particle に
ni “to”, we can assume that the correct pronun-
ciation is most likely itta, because the pronunci-
ation ni okonatta is unusual (行った okonatta is
seldom preceded by に ni). The composed op-
erations are also useful in capturing the pronun-
ciation of compound nouns: for example, due to
the phonological process called rendaku (sequen-
tial voicing) (Vance, 1987), 食器-棚 “plate rack”
is pronounced as shokki-dana, while the compo-
nents of this word are individually pronounced as
shokki (“plate”) and tana (“rack”). By considering
the compositions of operations, we can capture the
pronunciation in the context of a compound word.
Our phrasal decoder considers all (i.e. composed
and non-composed) operations during the decod-
ing, but longer (composed) operations are gener-
ally preferred when available because the phrase
count feature usually receives a negative weight.

However,

the simultaneous use of these op-
erations of different size may cause a problem
when the joint n-gram estimation is applied: be-
cause composed operations include multiple non-
composed operations,
they break the indepen-
dence assumption of n-gram occurrences in the
language model. For example, given a paral-
lel phrase “展覧会に行った／てんらんかいにいった”
(went to an exhibition), which is decomposed into
“展覧会／てんらんかい, に／に, 行った／いった” by
dictionary-based alignments, the joint n-gram lan-
guage model expects that the occurrence of “に／に
” (non-composed operation) is independent of that
of “に-行った／に-いった” (composed operation), but
this is not the case. To avoid this, we let the model
retain the original operations even after they are
composed. As shown in Figure 1, even after the
two operations “に→に” and “行った→いった” are
merged into a composed operation “に-行った→に-
いった”, the joint n-gram probability is still esti-
mated based on the original (non-composed) op-
erations. For efﬁciency purposes, we only retain

the decomposition of the ﬁrst appearance of each
composed operation even if multiple different de-
compositions are possible.

4 Experiments
4.1 Dictionary
In the dictionary-based framework, we need a dic-
tionary based on which we obtain the alignments.
We use a combination of three dictionaries: Uni-
Dic (Den et al., 2007), Iwanami Dictionary, and
an in-house dictionary that was available to us of
unknown origin. UniDic is a dictionary resource
available for research purposes, which is updated
on a regular basis and includes 625k word forms as
of the version 1.3.12 release (July 2009). Iwanami
Dictionary consists of 107k words, which expands
into 325k surface forms after considering okuri-
gana (verb inﬂectional ending) variants. The in-
house dictionary consists of a total of 226k words
and single-kanji pronunciations. After removing
duplicates, the combined dictionary consists of
770k entries. Note that these dictionaries are also
used as part of training data.

3,

in

we

parallel

corpora

described

4.2 Training and Test Data
need
As
Section
word/substring-aligned
to
train the models. We used three different sources
of training data in our experiments. First, follow-
ing Hatori and Suzuki (2011), we used Wikipedia:
following the heuristics described in the paper, we
extracted about 460k noisy word-pronunciation
pairs from Japanese Wikipedia articles as of
January 24, 2010. Of these pairs, we set aside 3k
instances for use in development and evaluation,
and used the rest for training (referred to as “Wiki-
Train”). Secondly, since word-pronunciation pairs
extracted from Wikipedia are noisy4 and mostly
consist of noun phrases, we also used a newspaper
corpus, which is comprised of 1.4m sentence
pairs, referred to as “News-Train”. Finally, for
the comparison with KyTea, we use a publicly
available corpus, the Balanced Corpus of Con-
temporary Written Japanese (Maekawa (2008)).
Speciﬁcally, we use the 2009 Core Data of this
corpus, which consists of 37k sentences annotated
with pronunciations (referred to as “BCCWJ”).

Our test data consist of six datasets from various
domains. Table 1 shows the statistics of these cor-
pora, with the OOV rate estimated using KyTea5
4We have found that roughly 10% of these instances are

invalid word-pronunciation pairs.

5We ran KyTea 0.13 with the built-in default model. For

125

Test set
News-1 (N1)
News-2 (N2)
Query-1 (Q1)
Query-2 (Q2)
Name (PN)
Wiki (WP)

#Instance Avg. len. OOV rate
0.3%
0.3%
3.5%
12.7%
23.4%
13.7%

867
739
1,049
3,078
9,170
2,000

51.8
44.9
3.8
5.7
3.0
4.1

Table 1: Statistics of test sets, where ”Avg. len.”
is the average length of an instance in the number
of characters.

• News-1(N1) and News-2(N2): collections of
newswire articles available as Microsoft Re-
search IME Corpus (Suzuki and Gao, 2005).
These articles are from different newspapers
from the news corpus we used in training. In
preparing these test sets,
instances including
Arabic and kanji numerals (0,1,…,9, 〇, 一,…,
九), or Roman alphabets are excluded6.
• Query-1(Q1) and Query-2(Q2): query logs
from a search engine (source undisclosed for
blind reviewing). These sets consist of various
instances ranging from general noun phrases to
relatively new proper nouns.
• Name(PN):
difﬁcult-to-
collection
pronounce words, mostly consisting of person
names.
• Wiki(WP):
word-
pronunciation pairs from Wikipedia, which
consists mostly of proper nouns including
names of people and locations as well as terms
that are difﬁcult to pronounce.
For the tuning of the weights of the model, we
used 200 held-out instances for each test domain,
except that the development set of Query-1 is also
used for the tuning for Query-2, and the set of Wiki
is used for the tuning for Name.

manually-cleaned

of

a

4.3 Experimental settings
We use our original implementation of the phrasal
aligner and decoder, which is also used as our im-
plementation of the substring-based model of Ha-
tori and Suzuki (2011). An ITG-based aligner
with EM algorithm (Zhang et al., 2008) is used
with monotonic setting; we set the source (kanji)
and target (kana) phrase length limits to 1 and
4, and prohibit alignments to a null symbol in

News-1/2, the OOV rate in the table is the OOV word rate
based on the KyTea’s output. For the other test sets, the ﬁg-
ures show the rate of the instances (words or phrases) that
contain any OOV word, again based on the KyTea’s output

6This is because there exist different standards in how
to pronounce them. For example,
the literal pronuncia-
tion is preferred for text-to-speech applications, whereas just
outputting numerals as such suits better for the training of
Japanese input methods.

either source or target side. The decoder runs
with the beam size of 20. The maximum num-
ber of composed operations is 4 for the substring-
based model of Hatori and Suzuki (2011), and
3 for the proposed dictionary-based model.
In
the substring-based model, character 5-gram and
joint 4-gram language models with Kneser-Ney
smoothing and the BoS (beginning-of-string) and
EoS (end-of-string) symbols are used;
in the
dictionary-based model, character 5-gram and
joint 3-gram models with the same settings are
used. We did not use the infrequent operation cut-
off. All of these parameters and settings are set
based on the preliminary experiments. As the eval-
uation measure, we use instance-level accuracy,
which is calculated based on the percentage of the
outputs that exactly match the gold standard: in-
stances correspond to sentences in News-1/2, and
to words or phrases in all other test domains. The
statistical signiﬁcance of the results is given using
McNemar’s test.

4.4 Baseline Models
We describe three baseline models that we use as
reference in our experiment.
• Mecab: Mecab version 0.987, which is
the state-of-the-art morphological analyzer for
Japanese that also outputs pronunciations of
words (Kudo et al., 2004), with the off-the-shelf
IPA Dictionary containing 392k word entries
provided at the author’s page.
• KyTea: KyTea version 0.138, which is described
in Section 2.2. In our comparison experiment,
we run KyTea version 0.13 both as is (using
their pre-trained model), and as trained by us
to allow the comparison of the framework using
the same publicly available training data.
• HS11: HS11 is our reimplementation of the
substring-based model by Hatori and Suzuki
(2011), which was shown to outperform the
substring-based joint
trigram model on a
Wikipedia test set.

5 Results and Discussion
5.1 Main Results
Table 2 shows the performance of the proposed
model along with various baseline models. The
ﬁrst two lines are the result of the off-the-shelf,
pre-trained systems. Mecab achieves around or
above 80% accuracy on ﬁve out of six test sets,
although the result on Wiki is below 60% because

7http://mecab.sourceforge.net/
8http://www.phontron.com/kytea/

126

Q2

Q1

N2

N1
PN WP
Model
78.8 79.7 88.0 79.8 79.8 55.9
Mecab
83.6 85.9 92.9 85.6 52.9 62.9
KyTea
23.3 31.8 87.7 73.3 83.9 64.5
HS11
37.6 31.8 93.3 82.7 90.5 72.9
HS11+
Proposed 89.7 88.6 95.5 87.8 92.9 70.2

Table 2: Instance-level accuracy (in %) of pronun-
ciation prediction models. The upper two models
use the off-the-shelf models; the lower three mod-
els are trained using the same resources: Wiki-
Train, News-Train, and the combined dictionary.

the system does not have a mechanism to handle
OOV words. The second row shows the result of
KyTea using the off-the-shelf “full SVM model”9,
which is trained on several resources including
BCCWJ and UniDic. It generally does better than
Mecab, but the accuracies on the high OOV rate
domains (i.e. Name and Wiki) are still quite low.
The bottom three models are all trained with
the same resources: Wiki-Train and News-Train
with all the three dictionaries.
“HS11” is the
substring-based model proposed by Hatori and
Suzuki (2011), while “HS11+” is the model en-
hanced with two additional features: the joint n-
gram feature (as described in Section 3.2), and the
dictionary feature, whose value is the total length
(in souce characters) of words matching any dic-
tionary entry.10 By comparing these two models,
the effectiveness of these features over the model
“HS11” is quite clear. However, the accuracy is
below 40% on newswire test sets, where each in-
stance is a full sentence. We assume that this is
because the substring-based model cannot capture
the contextual information that is broad enough,
and also is easily affected by noise in the train-
ing data. Our proposed model, corresponding to
the last line in the table, overcomes this problem
and achives the best accuracy in all but one test
domain (Wiki), showing the effectiveness and ro-
bustness of the dictionary-based approach. We
lags behind “HS11+” on Wiki, probably because
the dictionary-based model discards many opera-
tions that are uncommon, but are still useful for
the pronunciation of OOV words in Wikipedia.

Table 3 shows the direct comparison between
KyTea and the proposed model trained11 with
exactly the same datasets: BCCWJ, Wiki-Train,
9We could not train KyTea with the same dataset as the

proposed model uses due to memory limitation.

10The dictionary is also used as the training data.
11Our training of KyTea is performed as follows: we ﬁrst
train a segmentation model for KyTea using BCCWJ and
UniDic, and use this model to segment the substring-aligned
Wiki-Train instances to obtain a corpus with consistent seg-
mentation, which is then used to train the ﬁnal model.

Q2
PN WP
79.5 67.9 65.8
83.4 61.7 64.1
73.8 75.4 92.8† 84.9† 62.8 64.3

N1 N2 Q1
Model
68.5 65.3 88.0
KyTea (w/noise)
KyTea (wo/noise) 75.3 75.5 91.5
Proposed
Table 3:
Instance-level accuracy (in %) of
the models trained on Wiki-Train and BC-
CWJ with UniDic.
“†” denotes a statistically-
signiﬁcant (p < 0.01) difference between “KyTea
(wo/noise)” and “Proposed”.

and UniDic, all of which are from publicly avail-
able resources. Whereas “KyTea (w/noise)” uses
all the instances for training, “KyTea (wo/noise)”
uses only the instances that are ﬁltered using
dictionary-based operations12. Note that
this
cleaning process is also a novel contribution of
our work. As is observed from Table 3,
this
cleaning process resulted in a large improvement
in accuracy, with the exception of the Name and
Wiki sets. After inspecting the errors manually,
we have found that this is because the UniDic-
based operations do not include many single-kanji
pronunciations that are commonly used in per-
son’s names, such as “美 mi” and “人 to”. How-
ever, this problems seems negligible when a larger
dictionary including common pronunciations for
person’s names is available.
In the comparison
in Table 2, where the models use a combination
of three dictionaries, the dictionary-based model
“Proposed” performs better than the substring-
based model “HS11+” even on the Name set.

Overall,

the proposed model outperforms
“KyTea (wo/noise)” in four out of six test sets, and
the differences in the remaining two sets (News-
1/2) are not statistically signiﬁcant. Considering
also that the training data is relatively small in
this comparison experiment13, we can conclude
that our model has at least a comparable perfor-
mance to KyTea for the task of pronunciation dis-
ambiguation, while achieving a superior perfor-
mance on the task of pronunciation prediction for
OOV words. A manual analysis of the results also
showed that our model indeed has an advantage in
outputting phonetically natural pronunciation se-
quences, partially resolving problems related to
on/kun14 and rendaku, as in 契約-切れ keiyaku-
1227.6% of the instances in Wiki-Train is ﬁltered out. This
percentage is larger than the noise rate of 10% in this corpus,
which Hatori and Suzuki (2011) reported, because the sole
use of UniDic does not cover many single-kanji pronuncia-
tions, as mentioned later in this paragraph.

13Since the translation probabilities in our model are based
on unregularized frequency, our model is less powerful with
small training data, while it is more scalable.

14Pronunciations of kanji are classiﬁed into on and kun
pronunciations (corresponding to their origin, Chinese and

127

N1 N2 Q1 Q2
PN WP
Model
89.7 88.6 95.5 87.8 92.9 70.2
Proposed (D)
- wo/joint n-gram -5.5 -3.3 -1.5 -3.8 -4.4 -4.2
- wo/composed op.
-3.9 -4.0 -2.6 -1.2 -1.8 -2.9

Table 4: Feature ablation results for the dictionary-
based model trained with Wiki-Train, News-Train
and the combined dictionary. All the losses in ac-
curacy were statistically signiﬁcant (p < 0.01).

gire (individually pronounced as keiyaku and kire;
“contract expiration”). Although KyTea wrongly
output keiyaku-kire to this instance, the proposed
model was able to output the correct pronunci-
ation by learning that the pronunciation of 切れ
tends to be gire after the pronunciation ku, from
other instances such as 句-切れ ku-gire (segments in
haiku). On the other hand, KyTea is better at cap-
turing generalized context by using a character-
type feature, resolving instances such as “ブラン
ド-米” (katakana + mai; “brand rice”), while the
proposed model wrongly output the most frequent
pronunciation bei for 米.

5.2 Feature Ablation Experiments
Table 4 shows the results of the feature abla-
tion experiment of the proposed model. As we
mentioned in Section 3.2.1, the advantage of the
joint n-gram language model is twofold:
incor-
porating smoothed context into word pronuncia-
tion disambiguation (which is the dominant prob-
lem in News-1/2), as well as incorporating single-
kanji pronunciation dependencies into pronunci-
ation prediction for OOV words (considered to
be common in Name and Wiki). The improve-
ment observed in these domains suggests that
the joint n-gram probability successfully captured
these two aspects. The use of composed opera-
tions showed large improvement particularly on
News-1/2, proving its utility for the pronunciation
disambiguation aspect of this task.

5.3 Data Ablation Experiments
Figure 3 shows the performance of the proposed
model with respect to the number of News-Train
sentences used for training.
In this experiment,
the model is ﬁrst trained only with Wiki-Train;
then, sentences from News-Train are incremen-
tally added. This can be seen as a process for
adapting a word-based model to a fully sentential,
disambiguation-capable model. As expected, the
accuracy is consistently improved in the news do-
main as more sentences are added, while the accu-
racy remains almost unchanged in the rest of the
Japanese), each of which tends to be used consecutively.

Figure 3: Performance (accuracy in %) of the pro-
posed model with respect to the log of the number
of additional training sentences from News-Train.

domains, without showing any negative effect by
the additional out-of-domain training data. These
results suggest that our model is robust and can
adapt to new domains with a simple addition of
training data.

6 Conclusion

We have presented a uniﬁed approach to the task
of Japanese pronunciation prediction. Based on
the framework of phrasal SMT, our model seam-
lessly and robustly integrates the task of word pro-
nunciation disambiguation and pronunciation pre-
diction for OOV words. Its basic components are
trained in an unsupervised manner, and work in
the presence of noise in training data. The model
also has potential to adapt to a new domain when
additional training data is available. We have
performed an extensive evaluation on various test
sets, and showed that our model achieves the new
state-of-the-art accuracy on the task of Japanese
pronunciation prediction.

Looking into the future, we would like to see if
the proposed model is effective in a general task of
transliteration within a sentential context, which
is conceivable as an application of phonetic in-
put (e.g., inputting Arabic using Roman text and
converting it automatically into Arabic scripts).
On the task of Japanese pronunciation prediction,
we are also interested in incorporating class-based
features, such as character type information and
on/kun dependencies, by using both existing re-
sources and clustering methods.

Acknowledgement

We are grateful to Graham Neubig for providing
us with detailed information on KyTea, and to
anonymous reviewers for useful comments.

 100

 90

 80

 70

 60

 50

 40

 30

      1

N1
N2
Q1
Q2
PN
WP

     10

    100

   1000

  10000  100000 1000000

128

Tohru Nagano, Shinsuke Mori,

tically tagged corpus (in Japanese). Technical Re-
port, SIG, Information Processing Society of Japan.
and Masafumi
Nishimura. 2006. An n-gram-based approach to
phoneme and accent estimation for tts (in Japanese).
Transactions of Information Processing Society of
Japan, 47:1793–1801.

Graham Neubig and Shinsuke Mori. 2010. Word-
based partial annotation for efﬁcient corpus con-
struction.
In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2010).

Franz Josef Och. 2003. Minimum error rate training

for statistical machine translation. In ACL.

Tetsuro Sasada, Shinsuke Mori, and Tatsuya Kawa-
hara. 2009. Domain adaptation of statistical kana–
kanji conversion system by automatic acquisition
of contextual information with unknown words (in
Japanese). In Proceedings of the 15th Annual Meet-
ing of the Association for Natural Language Pro-
cessing.

Juergen Schroeter, Alistair Conkie, Ann Syrdal, Mark
Beutnagel, Matthias Jilka, Volker Strom, Yeon-Jun
Kim, Hong-Goo Kang, and David Kapilow. 2002.
A perspective on the next challenges for TTS re-
search. In Proceedings of the IEEE 2002 Workshop
on Speech Synthesis.

Tarek Sherif and Grzegorz Kondrak. 2007. Substring-

based transliteration. In ACL.

Eiichiro Sumita and Fumiaki Sugaya. 2006. Word
In

pronunciation disambiguation using the web.
NAACL.

Hisami Suzuki and Jianfeng Gao. 2005. Microsoft
Research IME Corpus. MSR Technical Report No.
2005-168.

Timothy J. Vance. 1987. An Introduction to Japanese

Phonology. State University of New York Press.

Richard Zens and Hermann Ney. 2004. Improvements
In

in phrase-based statistical machine translation.
HLT-NAACL.

Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL.

References
Maximilian Bisani and Hermann Ney. 2002. Investi-
gations on joint-multigram models for grapheme-to-
phoneme conversion. In Proceedings of the Interna-
tional Conference on Spoken Language Processing.
Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50:434–451.

Maximilian Bisani and Hermann Ney. 2008.

Stanley F. Chen. 2003. Conditional and joint models
for grapheme-to-phoneme conversion. In Proceed-
ings of the European Conference on Speech Commu-
nication and Technology.

Colin Cherry and Hisami Suzuki. 2009. Discrim-
In

inative substring decoding for transliteration.
EMNLP.

Michael Collins and Brian Roark. 2004. Incremental

parsing with the perceptron algorithm. In ACL.

Yasuharu Den, Toshinobu Ogiso, Hideki Ogura, At-
sushi Yamada, Nobuaki Minematsu, Kiyotaka Uchi-
moto, and Hanae Koiso. 2007. The development of
an electronic dictionary for morphological analysis
and its application to Japanese corpus linguistics (in
Japanese). Japanese linguistics, 22:101–122.

Jianfeng Gao, Mingjing Li, Joshua T. Goodman, and
Kai-Fu Lee. 2002a. Toward a uniﬁed approach
to statistical language modeling for chinese. ACM
Transactions on Asian Language Information Pro-
cessing, 1:3–33.

Jianfeng Gao, Hisami Suzuki, and Yang Wen. 2002b.
Exploiting headword dependency and predictive
clustering for language modeling. In EMNLP.

Jun Hatori and Hisami Suzuki. 2011. Predicting word
pronunciation in Japanese. In CICLing 2011, Lec-
ture Notes in Computer Science (6609), pages 477–
492. Springer.

Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme
conversion. In HLT-NAACL.

Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In ACL.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Integrating joint n-gram fea-
In

Kondrak.
tures into a discriminative training framework.
NAACL.

2010.

Kevin Knight and Jonathan Graehl. 1998. Machine

transliteration. Computational Linguistics, 24.

Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
Appliying conditional random ﬁelds to

2004.
Japanese morphological analysis. In EMNLP.

Gakuto Kurata, Shinsuke Mori, Nobuyasu Itoh, and
Masafumi Nishimura. 2007. Unsupervised lexicon
acquisition from speech and text. In Proceedings of
ICASSP-2007.

Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In ACL.

Kikuo Maekawa.

the
KOTONOHA-BCCWJ corpus (in Japanese). Ni-
hongo no kenkyu (Studies in Japanese), 4:82–95.

Compilation of

2008.

Shinsuke Mori, Tetsuro Sasada, and Graham Neubig.
2010b. Language model estimation from a stochas-

