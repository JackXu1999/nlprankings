



















































Discriminative Approach to Fill-in-the-Blank Quiz Generation for Language Learners


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 238–242,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Discriminative Approach to Fill-in-the-Blank Quiz Generation for
Language Learners

Keisuke Sakaguchi1∗ Yuki Arase2 Mamoru Komachi1†
1Nara Institute of Science and Technology

8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
2Microsoft Research Asia

Bldg.2, No. 5 Danling St., Haidian Dist., Beijing, P. R. China
{keisuke-sa, komachi}@is.naist.jp, yukiar@microsoft.com

Abstract

We propose discriminative methods to
generate semantic distractors of fill-in-the-
blank quiz for language learners using a
large-scale language learners’ corpus. Un-
like previous studies, the proposed meth-
ods aim at satisfying both reliability and
validity of generated distractors; distrac-
tors should be exclusive against answers
to avoid multiple answers in one quiz,
and distractors should discriminate learn-
ers’ proficiency. Detailed user evaluation
with 3 native and 23 non-native speakers
of English shows that our methods achieve
better reliability and validity than previous
methods.

1 Introduction

Fill-in-the-blank is a popular style used for eval-
uating proficiency of language learners, from
homework to official tests, such as TOEIC1 and
TOEFL2. As shown in Figure 1, a quiz is com-
posed of 4 parts; (1) sentence, (2) blank to fill in,
(3) correct answer, and (4) distractors (incorrect
options). However, it is not easy to come up with
appropriate distractors without rich experience in
language education. There are two major require-
ments that distractors should satisfy: reliability
and validity (Alderson et al., 1995). First, distrac-
tors should be reliable; they are exclusive against
the answer and none of distractors can replace the
answer to avoid allowing multiple correct answers
in one quiz. Second, distractors should be valid;
they discriminate learners’ proficiency adequately.

∗ This work has been done when the author was visiting
Microsoft Research Asia.

† Now at Tokyo Metropolitan University (Email: ko-
machi@tmu.ac.jp).

1http://www.ets.org/toeic
2http://www.ets.org/toefl

Each side, government and opposition, is _____   
the other for the political crisis, and for the violence. ���
���
(a) blaming   (b) accusing   (c) BOTH 	


Figure 1: Example of a fill-in-the-blank quiz,
where (a) blaming is the answer and (b) accusing
is a distractor.

There are previous studies on distractor gener-
ation for automatic fill-in-the-blank quiz genera-
tion (Mitkov et al., 2006). Hoshino and Nakagawa
(2005) randomly selected distractors from words
in the same document. Sumita et al. (2005) used
an English thesaurus to generate distractors. Liu et
al. (2005) collected distractor candidates that are
close to the answer in terms of word-frequency,
and ranked them by an association/collocation
measure between the candidate and surrounding
words in a given context. Dahlmeier and Ng
(2011) generated candidates for collocation er-
ror correction for English as a Second Language
(ESL) writing using paraphrasing with native lan-
guage (L1) pivoting technique. This method takes
an sentence containing a collocation error as in-
put and translates it into L1, and then translate it
back to English to generate correction candidates.
Although the purpose is different, the technique is
also applicable for distractor generation. To our
best knowledge, there have not been studies that
fully employed actual errors made by ESL learn-
ers for distractor generation.

In this paper, we propose automated distrac-
tor generation methods using a large-scale ESL
corpus with a discriminative model. We focus
on semantically confusing distractors that measure
learners’ competence to distinguish word-sense
and select an appropriate word. We especially tar-
get verbs, because verbs are difficult for language
learners to use correctly (Leacock et al., 2010).
Our proposed methods use discriminative models

238



Orig.	 I	 stop	 company	 on	 today	 .	
Corr.	 I	 quit	 a	 company	 today	 .	
Type	 NA	 #REP#	 #DEL#	 NA	 #INS#	 NA	 NA	

Figure 2: Example of a sentence correction pair
and error tags (Replacement, Deletion and Inser-
tion).

trained on error patterns extracted from an ESL
corpus, and can generate exclusive distractors with
taking context of a given sentence into considera-
tion.

We conduct human evaluation using 3 native
and 23 non-native speakers of English. The result
shows that 98.3% of distractors generated by our
methods are reliable. Furthermore, the non-native
speakers’ performance on quiz generated by our
method has about 0.76 of correlation coefficient
with their TOEIC scores, which shows that dis-
tractors generated by our methods satisfy validity.

Contributions of this paper are twofold; (1) we
present methods for generating reliable and valid
distractors, (2) we also demonstrate the effective-
ness of ESL corpus and discriminative models on
distractor generation.

2 Proposed Method

To generate distractors, we first need to decide
which word to be blanked. We then generate can-
didates of distractors and rank them based on a
certain criterion to select distractors to output.

In this section, we propose our methods for ex-
tracting target words from ESL corpus and select-
ing distractors by a discriminative model that con-
siders long-distance context of a given sentence.

2.1 Error-Correction Pair Extraction

We use the Lang-8 Corpus of Learner English3 as
a large-scale ESL corpus, which consists of 1.2M
sentence correction pairs. For generating semantic
distractors, we regard a correction as a target and
the misused word as one of the distractor candi-
dates.

In the Lang-8 corpus, there is no clue to align
the original and corrected words. In addition,
words may be deleted and inserted in the corrected
sentence, which makes the alignment difficult.
Therefore, we detect word deletion, insertion, and
replacement by dynamic programming4. We com-

3http://cl.naist.jp/nldata/lang-8/
4The implementation is available at https:

//github.com/tkyf/epair

Feature Example
Word[i-2] ,
Word[i-1] is
Word[i+1] the
Word[i+2] other
Dep[i] child nsubj side, aux is, dobj other, prep for
Class accuse

Table 1: Example of features and class label ex-
tracted from a sentence: Each side, government
and opposition, is *accusing/blaming the other for
the political crisis, and for the violence.

pare a corrected sentence against its original sen-
tence, and when word insertion and deletion er-
rors are identified, we put a spaceholder (Figure
2). We then extract error-correction (i.e. replace-
ment) pairs by comparing trigrams around the re-
placement in the original and corrected sentences,
for considering surrounding context of the target.
These error-correction pairs are a mixture of gram-
matical mistakes, spelling errors, and semantic
confusions. Therefore, we identify pairs due to se-
mantic confusion; we exclude grammatical error
corrections by eliminating pairs whose error and
correction have different part-of-speech (POS)5,
and exclude spelling error corrections based on
edit-distance. As a result, we extract 689 unique
verbs (lemma) and 3,885 correction pairs in total.

Using the error-correction pairs, we calculate
conditional probabilities P (we|wc), which repre-
sent how probable that ESL learners misuse the
word wc as we. Based on the probabilities, we
compute a confusion matrix. The confusion ma-
trix can generate distractors reflecting error pat-
terns of ESL learners. Given a sentence, we iden-
tify verbs appearing in the confusion matrix and
make them blank, then outputs distractor candi-
dates that have high confusion probability. We
rank the candidates by a generative model to
consider the surrounding context (e.g. N-gram).
We refer to this generative method as Confusion-
matrix Method (CFM).

2.2 Discriminative Model for Distractor
Generation and Selection

To generate distractors that considers long-
distance context and reflects detailed syntactic in-
formation of the sentence, we train multiple clas-
sifiers for each target word using error-correction
pairs extracted from ESL corpus. A classifier for

5Because the Lang-8 corpus does not have POS tags, we
assign POS by the NLTK (http://nltk.org/) toolkit.

239



a target word takes a sentence (in which the tar-
get word appears) as an input and outputs a verb
as the best distractor given the context using fol-
lowing features: 5-gram (±1 and ±2 words of the
target) lemmas and dependency type with the tar-
get child (lemma). The dependent is normalized
when it is a pronoun, date, time, or number (e.g. he
→ #PRP#) to avoid making feature space sparse.
Table 1 shows an example of features and a class
label for the classifier of a target verb (blame).

These classifiers are based on a discriminative
model: Support Vector Machine (SVM)6 (Vapnik,
1995). We propose two methods for training the
classifiers.

First, we directly use the corrected sentences in
the Lang-8 corpus. As shown in Table 1, we use
the 5-gram and dependency features7, and use the
original word (misused word by ESL learners) as
a class. We refer to this method as DiscESL.

Second, we train classifiers with an ESL-
simulated native corpus, because (1) the number
of sentences containing a certain error-correction
pair is still limited in the ESL corpus and (2)
corrected sentences are still difficult to parse cor-
rectly due to inherent noise in the Lang-8 corpus.
Specifically, we use articles collected from Voice
of America (VOA) Learning English8, which con-
sist of 270k sentences. For each target in a given
sentence, we artificially change the target into an
incorrect word according to the error probabilities
obtained from the learners confusion matrix ex-
plained in Section 2.2. In order to collect a suf-
ficient amount of training data, we generate 100
samples for each training sentence in which the
target word is replaced into an erroneous word.
We refer to this method as DiscSimESL9.

3 Evaluation with Native-Speakers

In this experiment, we evaluate the reliability of
generated distractors. The authors asked the help
of 3 native speakers of English (1 male and 2 fe-
males, majoring computer science) from an au-
thor’s graduate school. We provide each partici-
pant a gift card of $30 as a compensation when
completing the task.

6We use Linear SVM with default settings in the scikit-
learn toolkit 0.13.1. http://scikit-learn.org

7We use the Stanford CoreNLP 1.3.4 http://nlp.
stanford.edu/software/corenlp.shtml

8http://learningenglish.voanews.com/
9The implementation is available at https:

//github.com/keisks/disc-sim-esl

Method Corpus Model
Proposed
CFM ESL Generative
DiscESL ESL Discriminative
DiscSimESL Pseudo-ESL Discriminative
Baseline
THM Native Generative
RTM Native Generative

Table 2: Summary of proposed methods (CFM:
Confusion Matrix Method, DiscESL: Discrimina-
tive model with ESL corpus, DiscSimESL: Dis-
criminative model with simulated ESL corpus)
and baseline (THM: Thesaurus Method, RTM:
Roundtrip Method).

In order to compare distractors generated by dif-
ferent methods, we ask participants to solve the
generated fill-in-the-blank quiz presented in Fig-
ure 1. Each quiz has 3 options: (a) only word A
is correct, (b) only word B is correct, (c) both are
correct. The source sentences to generate a quiz
are collected from VOA, which are not included in
the training dataset of the DiscSimESL. We gen-
erate 50 quizzes using different sentences per each
method to avoid showing the same sentence mul-
tiple times to participants. We randomly ordered
the quizzes generated by different methods for fair
comparison.

We compare the proposed methods to two base-
lines implementing previous studies: Thesaurus-
based Method (THM) and Roundtrip Translation
Method (RTM). Table 2 shows a summary of each
method. The THM is based on (Sumita et al.,
2005) and extract distractor candidates from syn-
onyms of the target extracted from WordNet10.
The RTM is based on (Dahlmeier and Ng, 2011)
and extracts distractor candidates from roundtrip
(pivoting) translation lexicon constructed from the
WIT3 corpus (Cettolo et al., 2012)11, which cov-
ers a wide variety of topics. We build English-
Japanese and Japanese-English word-based trans-
lation tables using GIZA++ (IBM Model4). In
this dictionary, the target word is translated into
Japanese words and they are translated back to En-
glish as distractor candidates. To consider (local)
context, the candidates generated by the THM,
RTM, and CFM are re-ranked by 5-gram language

10WordNet 3.0 http://wordnet.princeton.
edu/wordnet/

11Available at http://wit3.fbk.eu
240



Method RAD (%) κ
Proposed
CFM 94.5 (93.1 - 96.0) 0.55
DiscESL 95.0 (93.6 - 96.3) 0.73
DiscSimESL 98.3 (97.5 - 99.1) 0.69
Baseline
THM 89.3 (87.4 - 91.3) 0.57
RTM 93.6 (92.1 - 95.1) 0.53

Table 3: Ratio of appropriate distractors (RAD)
with a 95% confidence interval and inter-rater
agreement statistics κ.

model score trained on Google 1T Web Corpus
(Brants and Franz, 2006) with IRSTLM toolkit12.

As an evaluation metric, we compute the ratio
of appropriate distractors (RAD) by the following
equation: RAD = NAD/NALL, where NALL is
the total number of quizzes and NAD is the num-
ber of quizzes on which more than or equal to 2
participants agree by selecting the correct answer.
When at least 2 participants select the option (c)
(both options are correct), we determine the dis-
tractor as inappropriate. We also compute the av-
erage of inter-rater agreement κ among all partici-
pants for each method.

Table 3 shows the results of the first experiment;
RAD with a 95% confidence interval and inter-
rater agreement κ. All of our proposed methods
outperform baselines regarding RAD with high
inter-rater agreement. In particular, DiscSimESL
achieves 9.0% and 4.7% higher RAD than THM
and RTM, respectively. These results show that
the effectiveness of using ESL corpus to gener-
ate reliable distractors. With respect to κ, our
discriminative models achieve from 0.12 to 0.2
higher agreement than baselines, indicating that
the discriminative models can generate sound dis-
tractors more effectively than generative models.
The lower κ on generative models may be because
the distractors are semantically too close to the tar-
get (correct answer) as following examples:

The coalition has *published/issued a
report saying that ... .

As a result, the quiz from generative models is not
reliable since both published and issued are cor-
rect.

4 Evaluation with ESL Learners

In this experiment, we evaluate the validity of gen-
erated distractors regarding ESL learners’ profi-

12The irstlm toolkit 5.80 http://sourceforge.
net/projects/irstlm/files/irstlm/

Method r Corr Dist Both Std
Proposed
CFM 0.71 56.7 29.6 13.5 11.5
DiscESL 0.48 62.4 27.9 10.4 12.8
DiscSimESL 0.76 64.0 20.7 15.1 13.4
Baseline
THM 0.68 57.2 28.1 14.6 10.7
RTM 0.67 63.4 26.9 9.5 13.2

Table 4: (1) Correlation coefficient r against par-
ticipants’ TOEIC scores, (2) the average percent-
age of correct answer (Corr), incorrect answer of
distractor (Dist), and incorrect answer that both
are correct (Both) chosen by participants, and (3)
standard deviation (Std) of Corr.

300 400 500 600 700 800 900 1000

TOEIC Score
20

30

40

50

60

70

80

90

100

A
cc

ur
ac

y
(%

)
DiscSimESL
Thesaurus (THM)

Figure 3: Correlation between the participants’
TOEIC scores and accuracy on THM and Disc-
SimESL.

ciency. Twenty-three Japanese native speakers (15
males and 8 females) are participated. All the par-
ticipants, who have taken at least 8 years of En-
glish education, self-report proficiency levels as
the TOEIC scores from 380 to 99013. All the par-
ticipants are graduate students majoring in science
related courses. We call for participants by e-
mailing to a graduate school. We provide each
participant a gift card of $10 as a compensation
when completing the task. We ask participants
to solve 20 quizzes per each method in the same
manner as Section 3. To evaluate validity of dis-
tractors, we use only reliable quizzes accepted in
Section 3. Namely, we exclude quizzes whose op-
tions are both correct. We evaluate correlation be-
tween learners’ accuracy for the generated quizzes
and the TOEIC score.

Table 4 represents the results; the highest corre-

13The official score range of the TOEIC is from 10 to 990.
241



lation coefficient r and standard deviation on Disc-
SimESL shows that its distractors achieve best va-
lidity. Figure 3 depicts the correlations between
the participants’ TOEIC scores and accuracy (i.e.
Corr.) on THM and DiscSimESL. It illustrates that
DiscSimESL achieves higher level of positive cor-
relation than THM. Table 4 also shows high per-
centage of choosing “(c) both are correct” on Disc-
SimESL, which indicates that distractors gener-
ated from DiscSimESL are difficult to distinguish
for ESL learners but not for native speakers as a
following example:

..., she found herself on stage ...
*playing/performing a number one hit.

A relatively lower correlation coefficient on
DiscESL may be caused by inherent noise on pars-
ing the Lang-8 corpus and domain difference from
quiz sentences (VOA).

5 Conclusion

We have presented methods that automatically
generate semantic distractors of a fill-in-the-blank
quiz for ESL learners. The proposed methods em-
ploy discriminative models trained using error pat-
terns extracted from ESL corpus and can gener-
ate reliable distractors by taking context of a given
sentence into consideration. The human evalua-
tion shows that 98.3% of distractors are reliable
when generated by our method (DiscSimESL).
The results also demonstrate 0.76 of correlation
coefficient to their TOEIC scores, indicating that
the distractors have better validity than previous
methods. As future work, we plan to extend
our methods for other POS, such as adjective and
noun. Moreover, we will take ESL learners’ pro-
ficiency into account for generating distractors of
appropriate levels for different learners.

Acknowledgments

This work was supported by the Microsoft Re-
search Collaborative Research (CORE) Projects.
We are grateful to Yangyang Xi for granting per-
mission to use text from Lang-8 and Takuya Fu-
jino for his error pair extraction algorithm. We
would also thank anonymous reviewers for valu-
able comments and suggestions.

References
Charles Alderson, Caroline Clapham, and Dianne Wall.

1995. Language Test Construction and Evaluation.
Cambridge University Press.

Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram Corpus version 1.1. Technical report, Google
Research.

Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. WIT3 : Web Inventory of Transcribed
and Translated Talks. In Proceedings of the 16th
Conference of the European Associattion for Ma-
chine Translation (EAMT), pages 261–268, Trent,
Italy, May.

Daniel Dahlmeier and Hwee Tou Ng. 2011. Cor-
recting semantic collocation errors with l1-induced
paraphrases. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 107–117, Edinburgh, Scotland, UK.,
July.

Ayako Hoshino and Hiroshi Nakagawa. 2005. A Real-
Time Multiple-Choice Question Generation for Lan-
guage Testing ― A Preliminary Study ―. In Pro-
ceedings of the 2nd Workshop on Building Educa-
tional Applications Using NLP, pages 17–20, Ann
Arbor, June.

Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel R. Tetreault. 2010. Automated Grammat-
ical Error Detection for Language Learners. Syn-
thesis Lectures on Human Language Technologies.
Morgan & Claypool Publishers.

Chao-Lin Liu, Chun-Hung Wang, Zhao-Ming Gao,
and Shang-Ming Huang. 2005. Applications of
Lexical Information for Algorithmically Composing
Multiple-Choice Cloze Items. In Proceedings of the
2nd Workshop on Building Educational Applications
Using NLP, pages 1–8, Ann Arbor, June.

Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis.
2006. A Computer-Aided Environment for Generat-
ing Multiple-Choice Test Items. Natural Language
Engineering, 12:177–194, 5.

Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Ya-
mamoto. 2005. Measuring Non-native Speak-
ers’ Proficiency of English by Using a Test with
Automatically-Generated Fill-in-the-Blank Ques-
tions. In Proceedings of the 2nd Workshop on Build-
ing Educational Applications Using NLP, pages 61–
68, Ann Arbor, June.

Vladimir Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer.

242


