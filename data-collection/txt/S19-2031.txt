



















































ELiRF-UPV at SemEval-2019 Task 3: Snapshot Ensemble of Hierarchical Convolutional Neural Networks for Contextual Emotion Detection


Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 195–199
Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics

195

ELiRF-UPV at SemEval-2019 Task 3: Snapshot Ensemble of Hierarchical
Convolutional Neural Networks for Contextual Emotion Detection

José-Ángel González, Lluı́s-F. Hurtado, Ferran Pla
Departament de Sistemas Informàtics i Computació

Universitat Politècnica de València.
Camı́ de Vera, sn
46022, València

{jogonba2, lhurtado, fpla}@dsic.upv.es

Abstract

This paper describes the approach developed
by the ELiRF-UPV team at SemEval 2019
Task 3: Contextual Emotion Detection in Text.
We have developed a Snapshot Ensemble of
1D Hierarchical Convolutional Neural Net-
works to extract features from 3-turn conver-
sations in order to perform contextual emotion
detection in text. This Snapshot Ensemble is
obtained by averaging the models selected by
a Genetic Algorithm that optimizes the evalua-
tion measure. The proposed ensemble obtains
better results than a single model and it obtains
competitive and promising results on Contex-
tual Emotion Detection in Text.

1 Introduction

Emotion Detection problem arises in the context
of conversational interactions, among two or more
agents, when one agent is interested in knowing
the emotional state of other agent involved in the
conversation. The detection of emotions is a dif-
ficult task when the content is expressed by using
only text, due to the lack of facial and hand ges-
ture expressions, voice modulations, etc. More-
over, the task becomes more complex if the detec-
tion of emotions is applied only on a short piece of
text without including context. This is because the
context can act as an emotion modifier of a given
turn in the conversation.

Although, researchers mainly focus on emotion
detection on text in absence of context (Moham-
mad et al., 2018) (Klinger et al., 2018), tipically
extracted from social media, recently, there are
few works that approach the emotion detection in
conversations by using context information (Haz-
arika et al., 2018b) (Majumder et al., 2018) (Haz-
arika et al., 2018a). These contextual systems
work on long conversations where different users
are involved and they use multimodal data, specif-
ically, text, audio and video in order to address the

emotion detection problem on large multi-party
conversations.

In this work, we present an approach to the Se-
meval 2019 Task 3: Contextual Emotion Detec-
tion in Text (Chatterjee et al., 2019). This task
is a simplification of the text emotion detection
problem on conversations where each conversa-
tion have only three utterances. Only two different
users are involved in each conversation, where the
first and third turn corresponds to the first user and
the second turn corresponds to the second user.
The goal of this tasks is to predict the emotion
of the third turn. We propose a Snapshot Ensem-
ble (SE) of 1D Hierarchical Convolutional Neu-
ral Networks (HCNN) trained to extract useful in-
formation from 3-turn conversations. Our system
was designed following some ideas of (Morris and
Keltner, 2000) and (Majumder et al., 2018). Con-
cretely, we consider the inter-turn and self-turn de-
pendencies (Morris and Keltner, 2000) along with
the context given by the preceding utterances (Ma-
jumder et al., 2018) to determine the emotion of a
given turn.

2 System Description

2.1 Preprocessing

For the tokenization process, our system used
TweetTokenizer from NLTK (Loper and Bird,
2002). In addition, we performed some other ac-
tions. All the text was transformed to lowercase.
Multiple spaces were converted to a single space.
Urls were replaced by the tag ”url”. We trans-
formed multiple instances of punctuation marks in
a single one (e.g., ”???” → ”?”). In order to ex-
tract semantic representations of the unicode emo-
jis, they are replaced by their description using the
Common Locale Data Repository (CLDR) Short
Name (e.g., → ”grinning face with star eyes”).
Moreover, non relevant and common words are



196

removed from these descriptions (”grinning face
with star eyes”→ ”grinning star eyes”).

2.2 Word Embeddings
It is well known that word embeddings (WE)
learned from the same domain of a downstream
task usually lead to obtain better results than those
obtained using general domain WE. Due to the
fact that we did not have sentences of the task
to learn word embeddings from them, we used
embeddings learned from Twitter posts because
we considered that the characteristics of tweets
are similar to the task language. Both of them
have a noisy nature and they share common fea-
tures of the internet language (slang, letter homo-
phones, onomatopoeic spelling, emojis, lexical er-
rors, etc.). Therefore, we used 400-dimensional
WE obtained from a skip-gram model trained
with 400 million tweets gathered from 1/3/2013
to 28/2/2014 (Godin et al., 2015).

2.3 Hierarchical Convolutional Neural
Networks

We considered several characteristics of the task
in order to design our system. First, the utterances
are short and there are many short-term dependen-
cies among these words. Therefore, we propose
to use 1D Convolutional Neural Networks (Kim,
2014) (CNN) to extract a rich semantic representa-
tion of each utterance. Second, the conversations
are composed only by 3 utterances, for that rea-
son, it is not required to uses models with high
capacity to learn long contexts. Thus, we pro-
pose to use another CNN on top of the first CNN
that extracts sentence representations, in order to
obtain representation of conversations. We called
this approach Hierarchical Convolutional Neural
Networks (HCNN) following the work of (Yang
et al., 2016).

As input to the model, each utterance j (com-
posed by a maximum of N words) in a conversa-
tion i is arranged in a matrix Mj ∈ RN×d, where
each row corresponds with a word in the utter-
ance j, represented by using d-dimensional WE.
As each conversation is a sequence of three ut-
terances, these conversations are arranged in a 3-
dimensional matrix where each channel j is the
representation of the utterance j in the conversa-
tion, i.e. for the conversation i, Mi ∈ R3×N×d.
On all the matrices of Mi, 1D Dropout (Srivastava
et al., 2014) was used to augment the dataset, by
deleting words of each utterance with p = 0.3.

Given the representation of the conversation i,
Mi, for each utterance independently, a CNN with
kernels of different sizes is applied in order to ob-
tain a composition of word embeddings that can
extract semantic/emotional properties from each
utterance. At this first level, we use f1 = 256 ker-
nels of sizes {2, 4, 6} and their weights are shared
among the three channels. From that, for each ut-
terance, three new matrices are obtained. These
matrices capture relevant features for each kernel
size and utterance. These features are pooled into
a vector by using 1D Global Max Pooling (GMP).

The resulting three vectors from the previous
level were concatenated as rows to obtain a matrix
representation of the conversation i composed by
the CNN map of its sentences, Wi ∈ R3×f1 . We
considered that conversation features could be rel-
evant for the task. At this level, in order to extract
these relevant features and following the ideas
in (Morris and Keltner, 2000) (Majumder et al.,
2018), the system is intended to take into account
the context and potentially the emotions given by
preceding utterances to determine the emotion ex-
pressed by the last utterance. To do this, a CNN
with f2 = 256 kernels of sizes {1, 2, 3}were used.
The size of the filters is crucial to understand what
features the system is capable to learn.

Concretely, 3-size kernels: semantic/emotional
features over all the contexts (full conversation);
2-size kernels: inter-turn features and seman-
tic/emotional features of preceding and later ut-
terances given a context of two utterances; 1-size
kernels: self-turn features and semantic/emotional
features of each utterance independently.

On the output maps of this second CNN1, GMP
is used in order to extract the most relevant fea-
tures from each dimension and the resulting vec-
tors are concatenated. Later, a fully connected
layer L1 with 512 neurons is used to fuse the con-
catenated vectors. Finally, to obtain a probabil-
ity distribution over C classes ({happy, sad, angry,
others}) we use a softmax fully connected layer
L2. Figure 1 shows the proposed model architec-
ture.

2.4 Snapshot Ensemble

Generally, ensemble models outperform single
models in similar tasks (Duppada et al., 2018)
(Rozental et al., 2018). Therefore, we decided

1After all the CNN layers (at two levels), BatchNormal-
ization, LeakyReLU and Dropout are applied



197

Xi,1

Xi,2

Xi,3

WL1 WL2

1D Convolution

Kernel Height 2

1D Convolution

Kernel Height 4

1D Convolution

Kernel Height 6

GMP

GMP

GMP

Concatenate

1D Convolution

Kernel Height 1

1D Convolution

Kernel Height 2

1D Convolution

Kernel Height 3

GMP

GMP

GMP

Concatenate

FC
Layer

FC
Layer

L1

L2

...

...

...

...

...

...

Figure 1: Hierarchical Convolutional Neural Networks.

to use ensemble methods instead of trying differ-
ent architectures. We used the ideas of Snapshot
Ensemble (SE) (Huang et al., 2017) to combine
HCNN trained until reaching good and diverse lo-
cal minima by using SGD and a cosine learning
rate with T = 24 training iterations, M = 6 learn-
ing cycles, and initial learning rate alpha = 0.4.

From this training method, we took 24 snap-
shots (one for each training iteration). From the
set of snapshots S = {si / 1 ≤ i ≤ 24 ∧ si :
R3×N×d → RC}, we generate 4 different systems:

1. Best snapshot of all iterations

f1 = argmax
si

µF1(si(x), y) (1)

2. Average of all snapshots

f2 =
1

|S|
∑
si∈S

si(x) (2)

3. Average of best snapshot at each learning cy-
cle

f3 =
M

T

T
M
−1∑

i=0

argmax
si∈S[ TM ·i,

T
M
·(i+1)]

µF1(si(x), y)

(3)

4. Average of genetic selected snapshots

f4 =
1

|g(S)|
∑

si∈g(S)

g(S)i si(x) (4)

where x and y are the input and the target, re-
spectively, and g(S)i is the decision of a genetic
algorithm to include the snapshot si in the ensem-
ble. We used this method in order to discretely
select (g(S)i ∈ {0, 1}) what snapshots are well-
suited for the final averaging ensemble which tries
to optimize µF1. The genetic algorithm (Mitchell,

1998) starts with a population of 400 individuals,
they are crossed by using two point crossover, mu-
tated with flip bit and selected by using tourna-
ment selection during 100 generations. Moreover,
this algorithm addresses a multi-objective prob-
lem, it must to reach combinations of snapshots
whose averaged predictions yield to high values
of µF1 while minimizing the number of models in
the ensemble (the final genetic ensemble is com-
posed by 6 system, i.e. as many systems as learn-
ing cycles) These decisions were taken in order
to reduce the overfiting risk during the learning of
the ensemble i.e. we prioritize simpler ensembles
which are composed by discretely selected snap-
shots.

3 Analysis of Results

In order to evaluate different configurations of our
system we used the development set given by the
task organizers. On this development set, ablation
analysis on single HCNN was carried out in order
to observe if the input Dropout and the incorpora-
tion of L1 layer yield to better results (the capac-
ity of HCNN must be greater when including both
techniques). The results of this ablation analysis
are shown in Table 1.

System µP µR µF1
Vanilla 70.65 75.06 72.79
Dropout 71.78 76.26 73.95
L1 72.36 75.23 73.84
Dropout + L1 75.42 75.78 75.60

Table 1: Ablation analysis of input Dropout and L1
layer on HCNN (development set)

Vanilla system is a single HCNN without in-
put Dropout neither the L1 layer. It can be ob-
served that, the systems with Dropout and L1 out-
performed the Vanilla version of HCNN in terms
of µP , µR and µF1. In terms of µP , the sys-
tems which incorporate L1 achieved better results.
However, although Dropout + L1 obtained the
best improvement in terms of µP , the highest µR
was obtained using only Dropout. This could in-
dicate that data augmentation could be useful to
increase the µR but it is required more network
capacity to handle this augmentation in order to
increase also the µP .

These results were obtained by using a single
HCNN with adam as update rule (Kingma and Ba,
2014) with default learning rate. However, the SE



198

training mode with Vanilla SGD and cosine learn-
ing rate, along with the proposed ensemble gener-
ation, allows the Dropout + L1 system to reach
better results (Table 2).

Ensemble µP µR µF1
Best snapshot (single) 74.82 78.41 76.58
Average All 74.38 77.93 76.18
Best per Cycle 75.29 77.45 76.35
Genetic Average 75.73 80.09 77.85

Table 2: Results on development set with several SE of
HCNN.

In this case, the best single model (Best snap-
shot) obtained in the SE training mode, provided
higher µR than Dropout + L1 at the expense of
a reduction in µP . This improvement of 3 points
ofµR yields also an increase of theµF1 measure.

Among the ensembles, only Genetic Average
improves the Best snapshot and Dropout + L1
systems in all the metrics. This is due to a big
increase in µR. This suggests that it is possible
to improve the µF1 results by balancing µP and
µR.

The other ensembles obtain lower results in
terms of µR and µF1 than Best snapshot, which
is a single model. Moreover, all SE (including
Best snapshot) except Genetic Average are less
accurate (lower µP ) than Dropout + L1. How-
ever, all of them improved considerably the µR.

Due to the SE HCNN models generally outper-
formed the best single model Dropout + L1 in
terms of µF1 on the development set, we submit-
ted all these systems to be evaluated on the test
set. The results are shown in Table 3. It can be
seen that the best system is Genetic Average, the
same behavior observed on the development set.
Although Best snapshot is more accurate than the
ensembles (higher µP ), two of the three ensem-
bles yields better results µF1. Moreover, a big
degradation in the results are observed, all systems
goes from 77 µF1 on the development set, to 74
µF1 on the test set.

System µP µR µF1
Best snapshot (single) 75.69 72.60 74.11
Average All 73.15 75.00 74.07
Best per Cycle 73.27 75.12 74.18
Genetic Average 73.43 75.72 74.56

Table 3: Results on test set with several SE of HCNN.

Table 4 shows the results of our best system
(Genetic Average) at class level. The worse clas-
sified classes in terms of F1 were Angry and
Happy.

Class P R F1
Angry 68.73 78.19 73.16
Happy 75.19 69.37 72.16
Sad 77.82 80.00 78.90

Table 4: Results at class level of Genetic Average on
test set.

4 Conclusion and Future Work

In this paper, we have presented Snapshot Ensem-
bles of Hierarchical Convolutional Neural Net-
works to address the Semeval 2019 Task 3: Con-
textual Emotion Detection in Text. Our system is
based on the use of a Genetic Algorithm in or-
der to ensemble different snapshots of the same
model. This ensemble outperformed single mod-
els and also classical snapshot ensembles, obtain-
ing competitive results in the addressed task.

Due to the fact that in the proposed system, the
semantic and emotional information is only pro-
vided by the representation of the words and the
utterances, as future work we plan to study differ-
ent word and sentence embeddings. It would be
also interesting to incorporate other emotional or
sentiment features such as: Sentiment Unit (Rad-
ford et al., 2017), DeepMoji (Felbo et al., 2017),
Sentiment Specific WE (Tang et al., 2014); or po-
larity lexicons. Moreover, we are also interested in
work with more powerful word embeddings such
as BERT (Devlin et al., 2018) in order to incorpo-
rate a richer semantic word representation.

Acknowledgments

This work has been partially supported by the
Spanish MINECO and FEDER founds under
project AMIC (TIN2017-85854-C4-2-R) and the
GiSPRO project (PROMETEU/2018/176). Work
of José-Ángel González is also financed by Uni-
versitat Politècnica de València under grant PAID-
01-17.

References
Ankush Chatterjee, Kedhar Nath Narahari, Meghana

Joshi, and Puneet Agrawal. 2019. Semeval-2019
task 3: Emocontext: Contextual emotion detection



199

in text. In Proceedings of The 13th International
Workshop on Semantic Evaluation (SemEval-2019),
Minneapolis, Minnesota.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Venkatesh Duppada, Royal Jain, and Sushant Hiray.
2018. Seernet at semeval-2018 task 1: Domain
adaptation for affect in tweets. In Proceedings of
The 12th International Workshop on Semantic Eval-
uation, pages 18–23. Association for Computational
Linguistics.

Bjarke Felbo, Alan Mislove, Anders Søgaard, Iyad
Rahwan, and Sune Lehmann. 2017. Using millions
of emoji occurrences to learn any-domain represen-
tations for detecting sentiment, emotion and sar-
casm. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1615–1625. Association for Computa-
tional Linguistics.

Fréderic Godin, Baptist Vandersmissen, Wesley
De Neve, and Rik Van de Walle. 2015. Multime-
dia lab $@$ acl wnut ner shared task: Named entity
recognition for twitter microposts using distributed
word representations. In Proceedings of the Work-
shop on Noisy User-generated Text, pages 146–153.
Association for Computational Linguistics.

Devamanyu Hazarika, Soujanya Poria, Rada Mihal-
cea, Erik Cambria, and Roger Zimmermann. 2018a.
Icon: Interactive conversational memory network
for multimodal emotion detection. In Proceedings
of the 2018 Conference on Empirical Methods in
Natural Language Processing, pages 2594–2604.
Association for Computational Linguistics.

Devamanyu Hazarika, Soujanya Poria, Amir Zadeh,
Erik Cambria, Louis-Philippe Morency, and Roger
Zimmermann. 2018b. Conversational memory net-
work for emotion recognition in dyadic dialogue
videos. In NAACL-HLT.

Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu,
John E. Hopcroft, and Kilian Q. Weinberger. 2017.
Snapshot ensembles: Train 1, get m for free. CoRR,
abs/1704.00109.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1746–1751. As-
sociation for Computational Linguistics.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Roman Klinger, Orphee De Clercq, Saif Mohammad,
and Alexandra Balahur. 2018. Iest: Wassa-2018
implicit emotions shared task. In Proceedings of
the 9th Workshop on Computational Approaches to

Subjectivity, Sentiment and Social Media Analysis,
pages 31–42. Association for Computational Lin-
guistics.

Edward Loper and Steven Bird. 2002. Nltk: The natu-
ral language toolkit. In Proceedings of the ACL-02
Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing and Com-
putational Linguistics - Volume 1, ETMTNLP ’02,
pages 63–70, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Navonil Majumder, Soujanya Poria, Devamanyu Haz-
arika, Rada Mihalcea, Alexander F. Gelbukh, and
Erik Cambria. 2018. Dialoguernn: An attentive
rnn for emotion detection in conversations. CoRR,
abs/1811.00405.

Melanie Mitchell. 1998. An Introduction to Genetic
Algorithms. MIT Press, Cambridge, MA, USA.

Saif Mohammad, Felipe Bravo-Marquez, Mohammad
Salameh, and Svetlana Kiritchenko. 2018. Semeval-
2018 task 1: Affect in tweets. In Proceedings of
The 12th International Workshop on Semantic Eval-
uation, pages 1–17. Association for Computational
Linguistics.

Michael W. Morris and Dacher Keltner. 2000. How
emotions work: The social functions of emotional
expression in negotiations. Research in Organiza-
tional Behavior, 22:1 – 50.

Alec Radford, Rafal Józefowicz, and Ilya Sutskever.
2017. Learning to generate reviews and discovering
sentiment. CoRR, abs/1704.01444.

Alon Rozental, Daniel Fleischer, and Zohar Kelrich.
2018. Amobee at iest 2018: Transfer learning from
language models. In Proceedings of the 9th Work-
shop on Computational Approaches to Subjectivity,
Sentiment and Social Media Analysis, pages 43–49.
Association for Computational Linguistics.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. J. Mach. Learn. Res., 15(1):1929–
1958.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1555–1565. Asso-
ciation for Computational Linguistics.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alexander J. Smola, and Eduard H. Hovy. 2016. Hi-
erarchical attention networks for document classifi-
cation. In HLT-NAACL.

https://doi.org/10.18653/v1/S18-1002
https://doi.org/10.18653/v1/S18-1002
https://doi.org/10.18653/v1/D17-1169
https://doi.org/10.18653/v1/D17-1169
https://doi.org/10.18653/v1/D17-1169
https://doi.org/10.18653/v1/D17-1169
https://doi.org/10.18653/v1/W15-4322
https://doi.org/10.18653/v1/W15-4322
https://doi.org/10.18653/v1/W15-4322
https://doi.org/10.18653/v1/W15-4322
http://aclweb.org/anthology/D18-1280
http://aclweb.org/anthology/D18-1280
https://doi.org/10.3115/v1/D14-1181
https://doi.org/10.3115/v1/D14-1181
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1412.6980
http://aclweb.org/anthology/W18-6206
http://aclweb.org/anthology/W18-6206
https://doi.org/10.3115/1118108.1118117
https://doi.org/10.3115/1118108.1118117
https://doi.org/10.18653/v1/S18-1001
https://doi.org/10.18653/v1/S18-1001
https://doi.org/https://doi.org/10.1016/S0191-3085(00)22002-9
https://doi.org/https://doi.org/10.1016/S0191-3085(00)22002-9
https://doi.org/https://doi.org/10.1016/S0191-3085(00)22002-9
http://aclweb.org/anthology/W18-6207
http://aclweb.org/anthology/W18-6207
http://dl.acm.org/citation.cfm?id=2627435.2670313
http://dl.acm.org/citation.cfm?id=2627435.2670313
https://doi.org/10.3115/v1/P14-1146
https://doi.org/10.3115/v1/P14-1146
https://doi.org/10.3115/v1/P14-1146

