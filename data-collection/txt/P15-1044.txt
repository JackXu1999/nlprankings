



















































Training a Natural Language Generator From Unaligned Data


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 451–461,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Training a Natural Language Generator From Unaligned Data

Ondřej Dušek and Filip Jurčíček
Charles University in Prague, Faculty of Mathematics and Physics

Institute of Formal and Applied Linguistics
Malostranské náměstí 25, CZ-11800 Prague, Czech Republic

{odusek,jurcicek}@ufal.mff.cuni.cz

Abstract

We present a novel syntax-based natural
language generation system that is train-
able from unaligned pairs of input mean-
ing representations and output sentences.
It is divided into sentence planning, which
incrementally builds deep-syntactic de-
pendency trees, and surface realization.
Sentence planner is based on A* search
with a perceptron ranker that uses novel
differing subtree updates and a simple fu-
ture promise estimation; surface realiza-
tion uses a rule-based pipeline from the
Treex NLP toolkit.

Our first results show that training from
unaligned data is feasible, the outputs of
our generator are mostly fluent and rele-
vant.

1 Introduction

We present a novel approach to natural lan-
guage generation (NLG) that does not require fine-
grained alignment in training data and uses deep
dependency syntax for sentence plans. We include
our first results on the BAGEL restaurant recom-
mendation data set of Mairesse et al. (2010).

In our setting, the task of a natural language
generator is that of converting an abstract meaning
representation (MR) into a natural language utter-
ance. This corresponds to the sentence planning
and surface realization NLG stages as described
by Reiter and Dale (2000). It also reflects the in-
tended usage in a spoken dialogue system (SDS),
where the NLG component is supposed to trans-
late a system output action into a sentence. While
the content planning NLG stage has been used in
SDS (e.g., Rieser and Lemon (2010)), we believe
that deciding upon the contents of the system’s ut-
terance is generally a task for the dialogue man-
ager. We focus mainly on the sentence planning

part in this work, and reuse an existing rule-based
surface realizer to test the capabilities of the gen-
erator in an end-to-end setting.

Current NLG systems usually require a sepa-
rate training data alignment step (Mairesse et al.,
2010; Konstas and Lapata, 2013). Many of them
use a CFG or operate in a phrase-based fashion
(Angeli et al., 2010; Mairesse et al., 2010), which
limits their ability to capture long-range syntactic
dependencies. Our generator includes alignment
learning into sentence planner training and uses
deep-syntactic trees with a rule-based surface re-
alization step, which ensures grammatical correct-
ness of the outputs. Unlike previous approaches
to trainable sentence planning (e.g., Walker et al.
(2001); Stent et al. (2004)), our generator does not
require a handcrafted base sentence planner.

This paper is structured as follows: in Section 2,
we describe the architecture of our generator. Sec-
tions 3 and 4 then provide further details on its
main components. In Section 5, we describe our
experiments on the BAGEL data set, followed by
an analysis of the results in Section 6. Section 7
compares our generator to previous related works
and Section 8 concludes the paper.

2 Generator Architecture

Our generator (see Figure 1) operates in two stages
that roughly correspond to the traditional NLG
stages of sentence planning and surface realiza-
tion. In the first stage, a statistical sentence
planner generates deep-syntactic dependency trees
from the input meaning representation. These are
converted into plain text sentences in the second
stage by the (mostly rule-based) surface realizer.

We use deep-syntax dependency trees to repre-
sent the sentence plan, i.e. the intermediate data
structure between the two aforementioned stages.
These are ordered dependency trees that only con-
tain nodes for content words (nouns, full verbs, ad-
jectives, adverbs) and coordinating conjunctions.

451



meaning representation (dialogue acts)

Sentence
planner

A* searchcandidategenerator

scorer
expand candidate
sentence plan tree

into new candidates

score candidates
to select next one
to be expanded

sentence plan 
(deep syntax tree)

plain text sentence

Surface
realizer

mostly 
rule-based 

pipeline
(from Treex
NLP toolkit)

Word ordering

Agreement

Compound
verb forms

Grammatical
words

Punctuation

Word Inflection

Phonetic changes

inform(name=X,  type=placetoeat, 
           eattype=restaurant,  area=riverside, 
           food=Italian)

t-tree

X-name
n:subj

be
v:fin

italian
adj:attr

restaurant
n:obj

river
n:by+X

X is an italian restaurant by the river.

Figure 1: Overall structure of our generator

Each node has a lemma and a formeme – a concise
description of its surface morphosyntactic form,
which may include prepositions and/or subordi-
nate conjunctions (Dušek et al., 2012). This struc-
ture is based on the deep-syntax trees of the Func-
tional Generative Description (Sgall et al., 1986),
but it has been simplified to fit our purposes (see
Figure 1 in the middle).

There are several reasons for taking the tra-
ditional two-step approach to generation (as op-
posed to joint approaches, see Section 7) and us-
ing deep syntax trees as the sentence plan format:
First, generating into deep syntax simplifies the
task for the statistical sentence planner – the plan-

ner does not need to handle surface morphology
and auxiliary words. Second, a rule-based syntac-
tic realizer allows us to ensure grammatical cor-
rectness of the output sentences, which would be
more difficult in a sequence-based and/or statisti-
cal approach.1 And third, a rule-based surface re-
alizer from our sentence plan format is relatively
easy to implement and can be reused for any do-
main within the same language. As in our case, it
is also possible to reuse and/or adapt an existing
surface realizer (see Section 4).

Deep-syntax annotation of sentences in the
training set is needed to train the sentence plan-
ner, but we assume automatic annotation and reuse
an existing deep-syntactic analyzer from the Treex
NLP framework (Popel and Žabokrtský, 2010).2

We use dialogue acts (DA) as defined in the
BAGEL restaurant data set of Mairesse et al.
(2010) as a MR in our experiments throughout this
paper. Here, a DA consists of a dialogue act type,
which is always “inform” in the set, and a list of
slot-value pairs (SVPs) that contain information
about a restaurant, such as food type or location
(see the top of Figure 1). Our generator can be
easily adapted to a different MR, though.

3 Sentence Planner

The sentence planner is based on a variant of the
A* algorithm (Hart et al., 1968; Och et al., 2001;
Koehn et al., 2003). It starts from an empty sen-
tence plan tree and tries to find a path to the opti-
mal sentence plan by iteratively adding nodes. It
keeps two sets of hypotheses, i.e., candidate sen-
tence plan trees, sorted by their score – hypotheses
to expand (open set) and already expanded (closed
set). It uses the following two subcomponents to
guide the search:

• a candidate generator that is able to incre-
mentally generate candidate sentence plan
trees (see Section 3.1),

• a scorer/ranker that scores the appropriate-
ness of these trees for the input MR (see Sec-
tion 3.2).

1This issue would become more pressing in languages
with richer morphology than English.

2See http://ufal.mff.cuni.cz/treex. Domain-
independent deep syntax analysis for several languages is
included in this framework; the English pipeline used here
involves a statistical part-of-speech tagger (Spoustová et al.,
2007) and a dependency parser (McDonald et al., 2005), fol-
lowed by a rule-based conversion to deep syntax trees.

452



t-tree
be

v:fin

t-tree

recommend
v:fin

t-tree

serve
v:fin

t-tree

be
v:fin

t-tree

restaurant
n:obj

be
v:fin

t-tree

be
v:fin

t-tree

X-name
n:subj

be
v:fin

t-tree

restaurant
n:subj

be
v:fin

t-tree

X-name
n:subj

be
v:fin

t-tree

X-name
n:subj

restaurant
n:obj

be
v:fin

t-tree

X-name
n:subj

bar
n:obj

Original sentence
plan tree:

Its successors (selection):

Figure 2: Candidate generator example inputs and
outputs

The basic workflow of the sentence planner al-
gorithm then looks as follows:

Init: Start from an open set with a single empty
sentence plan tree and an empty closed set.

Loop: 1. Select the best-scoring candidate C
from the open set. Add C to closed
set.

2. The candidate generator generates C,
a set of possible successors to C.
These are trees that have more nodes
than C and are deemed viable. Note
that C may be empty.

3. The scorer scores all successors in
C and if they are not already in the
closed set, it adds them to the open
set.

4. Check if the best successor in the
open set scores better than the best
candidate in the closed set.

Stop: The algorithm finishes if the top score in
the open set is lower than the top score in
the closed set for d consecutive iterations,
or if there are no more candidates in the
open set. It returns the best-scoring candi-
date from both sets.

3.1 Generating Sentence Plan Candidates
Given a sentence plan tree, which is typically in-
complete and may be even empty, the candidate
generator generates its successors by adding one
new node in all possible positions and with all pos-
sible lemmas and formemes (see Figure 2). While
a naive implementation – trying out any combina-
tion of lemmas and formemes found in the training
data – works in principle, it leads to an unman-
ageable number of candidate trees even for a very
small domain. Therefore, we include several rules
that limit the number of trees generated:

1. Lemma-formeme compatibility – only nodes
with a combination of lemma and formeme
seen in the training data are generated.

2. Syntactic viability – the new node must
be compatible with its parent node (i.e.,
this combination, including the dependency
left/right direction, must be seen in the train-
ing data).

3. Number of children – no node can have more
children than the maximum for this lemma-
formeme combination seen in the training
data.

4. Tree size – the generated tree cannot have
more nodes than trees seen in the training
data. The same limitation applies to the in-
dividual depth levels – the training data limit
the number of nodes on the n-th depth level
as well as the maximum depth of any tree.

This is further conditioned on the input SVPs
– the maximums are only taken from training
examples that contain the same SVPs that ap-
pear on the current input.

5. Weak semantic compatibility – we only in-
clude nodes that appear in the training data
alongside the elements of the input DA, i.e.,
nodes that appear in training examples con-
taining SVPs from the current input,

6. Strong semantic compatibility – for each
node (lemma and formeme), we make a
“compatibility list” of SVPs and slots that are
present in all training data examples contain-
ing this node. We then only allow generating
this node if all of them are present in the cur-
rent input DA. To allow for more generaliza-
tion, this rule can be applied just to lemmas

453



(disregarding formemes), and a certain num-
ber of SVPs/slots from the compatibility list
may be required at maximum.

Only Rules 4 (partly), 5, and 6 depend on the
format of the input meaning representation. Using
a different MR would require changing these rules
to work with atomic substructures of the new MR
instead of SVPs.

While especially Rules 5 and 6 exclude a vast
number of potential candidate trees, this limitation
is still much weaker than using hard alignment
links between the elements of the MR and the out-
put words or phrases. It leaves enough room to
generate many combinations unseen in the train-
ing data (cf. Section 6) while keeping the search
space manageable. To limit the space of potential
tree candidates even further, one could also use au-
tomatic alignment scores between the elements of
the input MR and the tree nodes (obtained using a
tool such as GIZA++ (Och and Ney, 2003)).

3.2 Scoring Sentence Plan Trees

The scorer for the individual sentence plan tree
candidates is a function that maps global features
from the whole sentence plan tree t and the input
MR m to a real-valued score that describes the fit-
ness of t in the context of m.

We first describe the basic version of the scorer
and then our two improvements – differing subtree
updates and future promise estimation.

Basic perceptron scorer
The basic scorer is based on the linear percep-
tron ranker of Collins and Duffy (2002), where the
score is computed as a simple dot product of the
features and the corresponding weight vector:

score(t,m) = w> · feat(t,m)

In the training phase, the weights w are ini-
tialized to one. For each input MR, the system
tries to generate the best sentence plan tree given
current weights, ttop. The score of this tree is
then compared to the score of the correct gold-
standard tree tgold.3 If ttop 6= tgold and the
gold-standard tree ranks worse than the generated
one (score(ttop,m) > score(tgold,m)), the weight
vector is updated by the feature value difference of

3Note that the “gold-standard” sentence plan trees are ac-
tually produced by automatic annotation. For the purposes of
scoring, they are, however, treated as gold standard.

the generated and the gold-standard tree:

w = w + α · (feat(tgold,m)− feat(ttop,m))

where α is a predefined learning rate.

Differing subtree updates
In the basic version described above, the scorer is
trained to score full sentence plan trees. However,
it is also used to score incomplete sentence plans
during the decoding. This leads to a bias towards
bigger trees regardless of their fitness for the input
MR. Therefore, we introduced a novel modifica-
tion of the perceptron updates to improve scoring
of incomplete sentence plans: In addition to up-
dating the weights using the top-scoring candidate
ttop and the gold-standard tree tgold (see above),
we also use their differing subtrees titop, t

i
gold for

additional updates.
Starting from the common subtree tc of ttop and

tgold, pairs of differing subtrees titop, t
i
gold are cre-

ated by gradually adding nodes from ttop into titop
and from tgold into tigold (see Figure 3). To main-
tain the symmetry of the updates in case that the
sizes of ttop and tgold differ, more nodes may be
added in one step.4 The additional updates then
look as follows:

t0top = t
0
gold = tc

for i in 1, . . .min{|ttop| − |tc|, |tgold| − |tc|} − 1 :
titop = t

i−1
top + node(s) from ttop

tigold = t
i−1
gold + node(s) from tgold

w = w + α · (feat(tigold,m)− feat(titop,m))
Future promise estimation
To further improve scoring of incomplete sentence
plan trees, we incorporate a simple future promise
estimation for the A* search intended to boost
scores of sentence plans that are expected to fur-
ther grow.5 It is based on the expected number
of children Ec(n) of different node types (lemma-
formeme pairs).6 Given all nodes n1 . . . n|t| in a

4For example, if tgold has 6 more nodes than tc and ttop
has 4 more, there will be 3 pairs of differing subtrees, with
tigold having 2, 4, and 5 more nodes than tc and t

i
top having

1, 2, and 3 more nodes than tc.
We have also evaluated a variant where both sets of sub-

trees tigold, t
i
top were not equal in size, but this resulted in

degraded performance.
5Note that this is not the same as future path cost in

the original A* path search, but it plays an analogous role:
weighing hypotheses of different size.

6Ec(n) is measured as the average number of children
over all occurrences of the given node type in the training
data. It is expected to be domain-specific.

454



t-tree

X
n:subj

be
v:fin

restaurant
n:obj

moderate
adj:attr

price
n:attr

range
n:in+X

Gold standard tgold:

cheap
adj:attr

italian
adj:attr

t-tree

X
n:subj

be
v:fin

restaurant
n:obj

Top generated ttop:

t-tree

X
n:subj

be
v:fin

restaurant
n:obj

Common subtree tc: Differing subtrees for update:

t-tree

X
n:subj

be
v:fin

restaurant
n:obj

price
n:attr

range

+

cheap
adj:attr

t-tree

X
n:subj

be
v:fin

restaurant
n:obj

-t1gold t1top

Figure 3: An example of differing subtrees
The gold standard tree tgold has three more nodes than the common subtree tc, while the top generated tree ttop has two more.
Only one pair of differing subtrees t1gold, t

1
top is built, where two nodes are added into t1gold and one node into t

1
top.

sentence plan tree t, the future promise is com-
puted in the following way:

fp = λ ·
∑

w ·
|t|∑

i=1

max{0, Ec(ni)− c(ni)}

where c(ni) is the current number of children of
node ni, λ is a preset weight parameter, and

∑
w

is the sum of the current perceptron weights. Mul-
tiplying by the weights sum makes future promise
values comparable to trees scores.

Future promise is added to tree scores through-
out the tree generation process, but it is disre-
garded for the termination criterion in the Stop
step of the generation algorithm and in perceptron
weight updates.

Averaging weights and parallel training
To speed up training using parallel processing, we
use the iterative parameter mixing approach of
McDonald et al. (2010), where training data are
split into several parts and weight updates are av-
eraged after each pass through the training data.
Following Collins (2002), we record the weights
after each training pass, take an average at the end,
and use this as the final weights for prediction.

4 Surface Realizer

We use the English surface realizer from the Treex
NLP toolkit (cf. Section 2 and (Ptáček, 2008)). It
is a simple pipeline of mostly rule-based blocks
that gradually change the deep-syntactic trees into
surface dependency trees, which are then lin-
earized to sentences. It includes the following
steps:

• Agreement – morphological attributes of
some nodes are deduced based on agreement

with other nodes (such as in subject-predicate
agreement).

• Word ordering – the input trees are already
ordered, so only a few rules for grammatical
words are applied.

• Compound verb forms – additional verbal
nodes are added for verbal particles (infini-
tive or phrasal verbs) and for compound ex-
pressions of tense, mood, and modality.

• Grammatical words – prepositions, subordi-
nating conjunctions, negation particles, arti-
cles, and other grammatical words are added
into the sentence.

• Punctuation – nodes for commas, final punc-
tuation, quotes, and brackets are introduced.

• Word Inflection – words are inflected accord-
ing to the information from formemes and
agreement.

• Phonetic changes – English “a” becomes
“an” based on the following word.

The realizer is designed as domain-independent
and handles most English grammatical phenom-
ena. A simple “round-trip” test – using au-
tomatic analysis with subsequent generation –
reached a BLEU score (Papineni et al., 2002)
of 89.79% against the original sentences on the
whole BAGEL data set, showing only minor dif-
ferences between the input sentence and genera-
tion output (mostly in punctuation).

455



restaurant
n:obj

X-area
n:in+X

and
x

X-area
n:in+X

restaurant
n:obj

X-area
n:in+X

and
x

X-area
n:in+X

Figure 4: Coordination structures conversion:
original (left) and our format (right).

5 Experimental Setup

Here we describe the data set used in our experi-
ments, the needed preprocessing steps, and the set-
tings of our generator specific to the data set.

5.1 Data set

We performed our experiments on the BAGEL
data set of Mairesse et al. (2010), which fits
our usage scenario in a spoken dialogue sys-
tem and is freely available.7 It contains a to-
tal of 404 sentences from a restaurant informa-
tion domain (describing the restaurant location,
food type, etc.), which correspond to 202 dia-
logue acts, i.e., each dialogue act has two para-
phrases. Restaurant names, phone numbers, and
other “non-enumerable” properties are abstracted
– replaced by an “X” symbol – throughout the gen-
eration process. Note that while the data set con-
tains alignment of source SVPs to target phrases,
we do not use it in our experiments.

For sentence planner training, we automatically
annotate all the sentences using the Treex deep
syntactic analyzer (see Section 2). The annotation
obtained from the Treex analyzer is further simpli-
fied for the sentence planner in two ways:

• Only lemmas and formemes are used in the
sentence planner. Other node attributes are
added in the surface realization step (see Sec-
tion 5.2).

• We convert the representation of coordination
structures into a format inspired by Universal
Dependencies.8 In the original Treex anno-
tation style, the conjunction heads both con-
juncts, whereas in our modification, the first

7Available for download at: http://farm2.user.
srcf.net/research/bagel/.

8http://universaldependencies.github.io

conjunct is at the top, heading the coordina-
tion and the second conjunct (see Figure 4).

The coordinations can be easily converted back for
the surface realizer, and the change makes the task
easier for the sentence planner: it may first gener-
ate one node and then decide whether it will add a
conjunction and a second conjunct.

5.2 Generator settings
In our candidate generator, we use all the limita-
tion heuristics described in Section 3.1. For strong
semantic compatibility (Rule 6), we use just lem-
mas and require at most 5 SVPs/slots from the
lemma’s compatibility list in the input DA.

We use the following feature types for our sen-
tence planner scorer:

• current tree properties – tree depth, total
number of nodes, number of repeated nodes

• tree and input DA – number of nodes per SVP
and number of repeated nodes per repeated
SVP,

• node features – lemma, formeme, and num-
ber of children of all nodes in the current tree,
and combinations thereof,

• input features – whole SVPs (slot + value),
just slots, and pairs of slots in the DA,

• combinations of node and input features,
• repeat features – occurrence of repeated lem-

mas and/or formemes in the current tree com-
bined with repeated slots in the input DA,

• dependency features – parent-child pairs for
lemmas and/or formemes, including and ex-
cluding their left-right order,

• sibling features – sibling pairs for lemmas
and/or formemes, also combined with SVPs,

• bigram features – pairs of lemmas and/or
formemes adjacent in the tree’s left-right or-
der, also combined with SVPs.

All feature values are normalized to have a mean
of 0 and a standard deviation of 1, with normaliza-
tion coefficients estimated from training data.

The feature set can be adapted for a different
MR format – it only must capture all important
parts of the MR, e.g., for a tree-like MR, the nodes
and edges, and possibly combinations thereof.

456



Setup
BLEU for training portion NIST for training portion

10% 20% 30% 50% 100% 10% 20% 30% 50% 100%
Basic perc. 46.90 52.81 55.43 54.53 54.24 4.295 4.652 4.669 4.758 4.643
+ Diff-tree upd. 44.16 50.86 53.61 55.71 58.70 3.846 4.406 4.532 4.674 4.876
+ Future promise 37.25 53.57 53.80 58.15 59.89 3.331 4.549 4.607 5.071 5.231

Table 1: Evaluation on the BAGEL data set (averaged over all ten cross-validation folds)
“Training portion” denotes the percentage of the training data used in the experiment. “Basic perc.” = basic perceptron updates,
“+ Diff-tree upd.” = with differing subtree perceptron updates, “+ Future promise” = with future promise estimation. BLEU
scores are shown as percentages.

Based on our preliminary experiments, we use
100 passes over the training data and limit the
number of iterations d that do not improve score
to 3 for training and 4 for testing. We use a hard
maximum of 200 sentence planner iterations per
input DA. The learning rate α is set to 0.1. We use
training data parts of 36 or 37 training examples
(1/10th of the full training set) in parallel training.
If future promise is used, its weight λ is set to 0.3.

The Treex English realizer expects not only
lemmas and formemes, but also additional gram-
matical attributes for all nodes. In our experi-
ments, we simply use the most common values
found in the training data for the particular nodes
as this is sufficient for our domain. In larger do-
mains, some of these attributes may have to be also
included in sentence plans.

6 Results

Same as Mairesse et al. (2010), we use 10-fold
cross-validation where DAs seen at training time
are never used for testing, i.e., both paraphrases or
none of them are present in the full training set.
We evaluate using BLEU and NIST scores (Pap-
ineni et al., 2002; Doddington, 2002) against both
reference paraphrases for a given test DA.

The results of our generator are shown in Ta-
ble 1, both for standard perceptron updates and our
improvements – differing subtree updates and fu-
ture promise estimation (see Section 3.2).

Our generator did not achieve the same perfor-
mance as that of Mairesse et al. (2010) (ca. 67%).9

However, our task is substantially harder since
the generator also needs to learn the alignment
of phrases to SVPs and determine whether all re-
quired information is present on the output (see
also Section 7). Our differing tree updates clearly
bring a substantial improvement over standard per-

9Mairesse et al. (2010) do not give a precise BLEU score
number in their paper, they only show the values in a graph.

ceptron updates, and scores keep increasing with
bigger amounts of training data used, whereas
with plain perceptron updates, the scores stay flat.
The increase with 100% is smaller since all train-
ing DAs are in fact used twice, each time with a
different paraphrase.10 A larger training set with
different DAs should bring a bigger improvement.
Using future promise estimation boosts the scores
even further, by a smaller amount for BLEU but
noticeably for NIST. Both improvements on the
full training set are considered statistically signif-
icant at 95% confidence level by the paired boot-
strap resampling test (Koehn, 2004). A manual in-
spection of a small sample of the results confirmed
that the automatic scores reflect the quality of the
generated sentences well.

If we look closer at the generated sentences (see
Table 2), it becomes clear that the generator learns
to produce meaningful utterances which mostly
correspond well to the input DA. It is able to pro-
duce original paraphrases and generalizes to pre-
viously unseen DAs.

On the other hand, not all required information
is always present, and some facts are sometimes
repeated or irrelevant information appears. This
mostly happens with input slot-value pairs that oc-
cur only rarely in the training data; we believe that
a larger training set will solve this problem. Alter-
natively, one could introduce additional scorer fea-
tures to discourage conflicting information. An-
other problem is posed by repeated slots in the in-
put DA, which are sometimes not reflected prop-
erly in the generated sentence. This suggests that
a further refinement of the scorer feature set may
be needed.

10We used the two paraphrases that come with each DA
as independent training instances. While having two differ-
ent gold-standard outputs for a single input is admittedly not
ideal for a discriminative learner, it still brings an improve-
ment in our case.

457



Input DA inform(name=X-name, type=placetoeat, eattype=restaurant, near=X-near, food=Continental, food=French)
Reference X is a French and continental restaurant near X.
Generated X is a French and continental restaurant near X.
Input DA inform(name=X-name, type=placetoeat, area=riverside, near=X-near, eattype=restaurant)
Reference X restaurant is near X on the riverside.
Generated X is a restaurant in the riverside area near X.
Input DA inform(name=X-name, type=placetoeat, area=X-area, pricerange=moderate, eattype=restaurant)
Reference X is a moderately priced restaurant in X.
Generated X is a restaurant in the X area.
Input DA inform(name=X-name, type=placetoeat, eattype=restaurant, area=riverside, food=French)
Reference X is a French restaurant on the riverside.
Generated X is a French restaurant in the riverside area which serves French food.
Input DA inform(name=X-name, type=placetoeat, eattype=restaurant, pricerange=moderate,

area=X-area, food=Contemporary, food=English)
Reference X is a moderately priced English contemporary restaurant in X.
Generated X is an English restaurant in the X area which serves expensive food in the moderate price range

located in X.
Input DA inform(name=X-name, type=placetoeat, eattype=restaurant, area=citycentre, near=X-near,

food="Chinese takeaway", food=Japanese)
Reference X is a Chinese takeaway and Japanese restaurant in the city centre near X.
Generated X is a Japanese restaurant in the centre of town near X and X.
Input DA inform(name=X-name, type=placetoeat, pricerange=moderate, eattype=restaurant)
Reference X is a restaurant that offers moderate price range.
Generated X is a restaurant in the moderate price range.

Table 2: Example generated sentences
Sentences generated when training on the full set and using differing subtree updates and future promise estimation.

7 Related Work

Previous trainable methods in sentence planning
use in principle two techniques: First, in the over-
generation and ranking approach (Walker et al.,
2001; Stent et al., 2004), many sentence plans are
generated using a rule-based planner and then the
best one is selected by a statistical ranker. Second,
parameter optimization trains adjustable parame-
ters of a handcrafted generator to produce outputs
with desired properties (Paiva and Evans, 2005;
Mairesse and Walker, 2008). As opposed to our
approach, both methods require an existing hand-
crafted sentence planner.

Other previous works combine sentence plan-
ning and surface realization into a single step and
do not require a handcrafted base module. Wong
and Mooney (2007) experiment with a phrase-
based machine translation system, comparing and
combining it with an inverted semantic parser
based on synchronous context-free grammars. Lu
et al. (2009) use tree conditional random fields
over hybrid trees that combine natural language
phrases with formal semantic expressions. Angeli
et al. (2010) generate text from database records
through a sequence of classifiers, gradually se-
lecting database records, fields, and correspond-
ing textual realizations to describe them. Konstas
and Lapata (2013) recast the whole NLG problem
as parsing over a probabilistic context-free gram-

mar estimated from database records and their de-
scriptions. Mairesse et al. (2010) convert input
DAs into “semantic stacks”, which correspond to
natural language phrases and contain slots and
their values on top of each other. Their genera-
tion model uses two dynamic Bayesian networks:
the first one performs an ordering of the input se-
mantic stacks, inserting intermediary stacks which
correspond to grammatical phrases, the second
one then produces a concrete surface realization.
Dethlefs et al. (2013) approach generation as a
sequence labeling task and use a conditional ran-
dom field classifier, assigning a word or a phrase
to each input MR element.

Unlike our work, the joint approaches typi-
cally include the alignment of input MR elements
to output words in a separate preprocessing step
(Wong and Mooney, 2007; Angeli et al., 2010), or
require pre-aligned training data (Mairesse et al.,
2010; Dethlefs et al., 2013). In addition, their ba-
sic algorithm often requires a specific input MR
format, e.g., a tree (Wong and Mooney, 2007; Lu
et al., 2009) or a flat database (Angeli et al., 2010;
Konstas and Lapata, 2013; Mairesse et al., 2010).

While dependency-based deep syntax has been
used previously in statistical NLG, the approaches
known to us (Bohnet et al., 2010; Belz et al., 2012;
Ballesteros et al., 2014) focus only on the surface
realization step and do not include a sentence plan-

458



ner, whereas our work is mainly focused on statis-
tical sentence planning and uses a rule-based real-
izer.

Our approach to sentence planning is most sim-
ilar to Zettlemoyer and Collins (2007), which use
a candidate generator and a perceptron ranker for
CCG parsing. Apart from proceeding in the in-
verse direction and using dependency trees, we use
only very generic rules in our candidate generator
instead of language-specific ones, and we incorpo-
rate differing subtree updates and future promise
estimation into our ranker.

8 Conclusions and Further Work

We have presented a novel natural language gen-
erator, capable of learning from unaligned pairs
of input meaning representation and output utter-
ances. It consists of a novel, A*-search-based sen-
tence planner and a largely rule-based surface re-
alizer from the Treex NLP toolkit. The sentence
planner is, to our knowledge, first to use depen-
dency syntax and learn alignment of semantic el-
ements to words or phrases jointly with sentence
planning.

We tested our generator on the BAGEL restau-
rant information data set of Mairesse et al. (2010).
We have achieved very promising results, the ut-
terances produced by our generator are mostly flu-
ent and relevant. They did not surpass the BLEU
score of the original authors; however, our task is
substantially harder as our generator does not re-
quire fine-grained alignments on the input. Our
novel feature of the sentence planner ranker – us-
ing differing subtrees for perceptron weight up-
dates – has brought a significant performance im-
provement.

The generator source code, along with config-
uration files for experiments on the BAGEL data
set, is available for download on Github.11

In future work, we plan to evaluate our genera-
tor on further domains, such as geographic infor-
mation (Kate et al., 2005), weather reports (Liang
et al., 2009), or flight information (Dahl et al.,
1994). In order to improve the performance of our
generator and remove the dependency on domain-
specific features, we plan to replace the percep-
tron ranker with a neural network. We also want
to experiment with removing the dependency on
the Treex surface realizer by generating directly
into dependency trees or structures into which de-

11https://github.com/UFAL-DSG/tgen

pendency trees can be converted in a language-
independent way.

Acknowledgments

This work was funded by the Ministry of Edu-
cation, Youth and Sports of the Czech Republic
under the grant agreement LK11221 and core re-
search funding, SVV project 260 104, and GAUK
grant 2058214 of Charles University in Prague. It
used language resources stored and distributed by
the LINDAT/CLARIN project of the Ministry of
Education, Youth and Sports of the Czech Repub-
lic (project LM2010013).

The authors would like to thank Lukáš Žilka,
Ondřej Plátek, and the anonymous reviewers for
helpful comments on the draft.

References
G. Angeli, P. Liang, and D. Klein. 2010. A simple

domain-independent probabilistic approach to gen-
eration. In Proc. of the 2010 Conference on Empir-
ical Methods in Natural Language Processing, page
502–512.

M. Ballesteros, S. Mille, and L. Wanner. 2014.
Classifiers for data-driven deep sentence genera-
tion. In Proceedings of the 8th International Natural
Language Generation Conference, pages 108–112,
Philadelphia.

A. Belz, B. Bohnet, S. Mille, L. Wanner, and M. White.
2012. The Surface Realisation Task: Recent De-
velopments and Future Plans. In INLG 2012, pages
136–140.

B. Bohnet, L. Wanner, S. Mille, and A. Burga. 2010.
Broad coverage multilingual deep sentence genera-
tion with a stochastic multi-level realizer. In Proc.
of the 23rd International Conference on Computa-
tional Linguistics, page 98–106.

M. Collins and N. Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete
structures, and the voted perceptron. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, page 263–270, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

M. Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, page 1–8. Associa-
tion for Computational Linguistics.

D. A. Dahl, M. Bates, M. Brown, W. Fisher,
K. Hunicke-Smith, D. Pallett, E. Rudnicky, and
E. Shriberg. 1994. Expanding the scope of the ATIS

459



task: the ATIS-3 corpus. In in Proc. ARPA Hu-
man Language Technology Workshop ’92, Plains-
boro, NJ, pages 43–48. Morgan Kaufmann.

N. Dethlefs, H. Hastie, H. Cuayáhuitl, and O. Lemon.
2013. Conditional Random Fields for Responsive
Surface Realisation using Global Features. In Pro-
ceedings of ACL, Sofia.

G. Doddington. 2002. Automatic evaluation
of machine translation quality using N-gram co-
occurrence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology Research, pages 138–145, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.

O. Dušek, Z. Žabokrtský, M. Popel, M. Majliš,
M. Novák, and D. Mareček. 2012. Formemes
in English-Czech deep syntactic MT. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, page 267–274, Montreal.

P. E. Hart, N. J. Nilsson, and B. Raphael. 1968. A
formal basis for the heuristic determination of mini-
mum cost paths. IEEE Transactions on Systems Sci-
ence and Cybernetics, 4(2):100–107.

R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005.
Learning to transform natural to formal languages.
In Proceedings of the National Conference on Ar-
tificial Intelligence, volume 20. Menlo Park, CA;
Cambridge, MA; London; AAAI Press; MIT Press;
1999.

P. Koehn, F. J. Och, and D. Marcu. 2003. Statis-
tical phrase-based translation. In Proceedings of
NAACL-HLT - Volume 1, page 48–54, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

P. Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, page 388–395.

I. Konstas and M. Lapata. 2013. A global model for
concept-to-text generation. Journal of Artificial In-
telligence Research, 48:305–346.

P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1-Volume 1, page 91–99.

W. Lu, H. T. Ng, and W. S. Lee. 2009. Natural
language generation with tree conditional random
fields. In Proc. of the 2009 Conference on Empir-
ical Methods in Natural Language Processing: Vol-
ume 1-Volume 1, page 400–409.

F. Mairesse and M. Walker. 2008. Trainable gen-
eration of big-five personality styles through data-
driven parameter estimation. In Proc. of the 46th
Annual Meeting of the ACL (ACL), page 165–173.

F. Mairesse, M. Gašić, F. Jurčíček, S. Keizer, B. Thom-
son, K. Yu, and S. Young. 2010. Phrase-based sta-
tistical language generation using graphical models
and active learning. In Proc. of the 48th Annual
Meeting of the ACL, page 1552–1561.

R. McDonald, F. Pereira, K. Ribarov, and J. Hajič.
2005. Non-projective dependency parsing using
spanning tree algorithms. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
page 523–530.

R. McDonald, K. Hall, and G. Mann. 2010. Dis-
tributed training strategies for the structured percep-
tron. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 456–464. Association for Computational Lin-
guistics.

F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19–51.

F. J. Och, N. Ueffing, and H. Ney. 2001. An efficient
A* search algorithm for statistical machine trans-
lation. In Proceedings of the Workshop on Data-
driven Methods in Machine Translation - Volume 14,
page 1–8, Stroudsburg, PA, USA. Association for
Computational Linguistics.

D. S. Paiva and R. Evans. 2005. Empirically-based
control of natural language generation. In Proc.
of the 43rd Annual Meeting of ACL, page 58–65,
Stroudsburg, PA, USA. ACL.

K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th annual
meeting of the Association for Computational Lin-
guistics, page 311–318.

M. Popel and Z. Žabokrtský. 2010. TectoMT: modu-
lar NLP framework. In Proceedings of IceTAL, 7th
International Conference on Natural Language Pro-
cessing, page 293–304, Reykjavík.

J. Ptáček. 2008. Two tectogrammatical realizers side
by side: Case of English and Czech. In Fourth In-
ternational Workshop on Human-Computer Conver-
sation, Bellagio, Italy.

E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge Univ. Press.

V. Rieser and O. Lemon. 2010. Natural language gen-
eration as planning under uncertainty for spoken dia-
logue systems. In Empirical methods in natural lan-
guage generation, page 105–120.

P. Sgall, E. Hajičová, and J. Panevová. 1986. The
meaning of the sentence in its semantic and prag-
matic aspects. D. Reidel, Dordrecht.

460



D. J. Spoustová, J. Hajič, J. Votrubec, P. Krbec, and
P. Květoň. 2007. The Best of Two Worlds: Co-
operation of Statistical and Rule-based Taggers for
Czech. In Proceedings of the Workshop on Balto-
Slavonic Natural Language Processing: Informa-
tion Extraction and Enabling Technologies, pages
67–74. Association for Computational Linguistics.

A. Stent, R. Prasad, and M. Walker. 2004. Trainable
sentence planning for complex information presen-
tation in spoken dialog systems. In Proceedings of
the 42nd Annual Meeting on Association for Com-
putational Linguistics, pages 79–86.

M. A. Walker, O. Rambow, and M. Rogati. 2001.
SPoT: a trainable sentence planner. In Proc. of
2nd meeting of NAACL, page 1–8, Stroudsburg, PA,
USA. ACL.

Y. W. Wong and R. J. Mooney. 2007. Generation
by inverting a semantic parser that uses statistical
machine translation. In Proc. of Human Language
Technologies: The Conference of the North Amer-
ican Chapter of the ACL (NAACL-HLT-07), page
172–179.

L. S. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logi-
cal form. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 678–687, Prague.

461


