@proceedings{ws-2018-nlp,
    title = "Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})",
    author = "Park, Eunjeong L.  and
      Hagiwara, Masato  and
      Milajevs, Dmitrijs  and
      Tan, Liling",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-2500",
}
@inproceedings{gardner-etal-2018-allennlp,
    title = "{A}llen{NLP}: A Deep Semantic Natural Language Processing Platform",
    author = "Gardner, Matt  and
      Grus, Joel  and
      Neumann, Mark  and
      Tafjord, Oyvind  and
      Dasigi, Pradeep  and
      Liu, Nelson F.  and
      Peters, Matthew  and
      Schmitz, Michael  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-2501",
    doi = "10.18653/v1/W18-2501",
    pages = "1--6",
    abstract = "Modern natural language processing (NLP) research requires writing code. Ideally this code would provide a precise definition of the approach, easy repeatability of results, and a basis for extending the research. However, many research codebases bury high-level parameters under implementation details, are challenging to run and debug, and are difficult enough to extend that they are more likely to be rewritten. This paper describes AllenNLP, a library for applying deep learning methods to NLP research that addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions. AllenNLP has already increased the rate of research experimentation and the sharing of NLP components at the Allen Institute for Artificial Intelligence, and we are working to have the same impact across the field.",
}
@inproceedings{nothman-etal-2018-stop,
    title = "Stop Word Lists in Free Open-source Software Packages",
    author = "Nothman, Joel  and
      Qin, Hanmin  and
      Yurchak, Roman",
    booktitle = "Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-2502",
    doi = "10.18653/v1/W18-2502",
    pages = "7--12",
    abstract = "Open-source software packages for language processing often include stop word lists. Users may apply them without awareness of their surprising omissions (e.g. {``}hasn{'}t{''} but not {``}hadn{'}t{''}) and inclusions ({``}computer{''}), or their incompatibility with a particular tokenizer. Motivated by issues raised about the Scikit-learn stop list, we investigate variation among and consistency within 52 popular English-language stop lists, and propose strategies for mitigating these issues.",
}
@inproceedings{hu-etal-2018-texar,
    title = "{T}exar: A Modularized, Versatile, and Extensible Toolbox for Text Generation",
    author = "Hu, Zhiting  and
      Yang, Zichao  and
      Zhao, Tiancheng  and
      Shi, Haoran  and
      He, Junxian  and
      Wang, Di  and
      Ma, Xuezhe  and
      Liu, Zhengzhong  and
      Liang, Xiaodan  and
      Qin, Lianhui  and
      Chaplot, Devendra Singh  and
      Tan, Bowen  and
      Yu, Xingjiang  and
      Xing, Eric",
    booktitle = "Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-2503",
    doi = "10.18653/v1/W18-2503",
    pages = "13--22",
    abstract = "We introduce Texar, an open-source toolkit aiming to support the broad set of text generation tasks. Different from many existing toolkits that are specialized for specific applications (e.g., neural machine translation), Texar is designed to be highly flexible and versatile. This is achieved by abstracting the common patterns underlying the diverse tasks and methodologies, creating a library of highly reusable modules and functionalities, and enabling arbitrary model architectures and various algorithmic paradigms. The features make Texar particularly suitable for technique sharing and generalization across different text generation applications. The toolkit emphasizes heavily on extensibility and modularized system design, so that components can be freely plugged in or swapped out. We conduct extensive experiments and case studies to demonstrate the use and advantage of the toolkit.",
}
@inproceedings{gildea-etal-2018-acl,
    title = "The {ACL} Anthology: Current State and Future Directions",
    author = "Gildea, Daniel  and
      Kan, Min-Yen  and
      Madnani, Nitin  and
      Teichmann, Christoph  and
      Villalba, Mart{\'\i}n",
    booktitle = "Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-2504",
    doi = "10.18653/v1/W18-2504",
    pages = "23--28",
    abstract = "The Association of Computational Linguistic{'}s Anthology is the open source archive, and the main source for computational linguistics and natural language processing{'}s scientific literature. The ACL Anthology is currently maintained exclusively by community volunteers and has to be available and up-to-date at all times. We first discuss the current, open source approach used to achieve this, and then discuss how the planned use of Docker images will improve the Anthology{'}s long-term stability. This change will make it easier for researchers to utilize Anthology data for experimentation. We believe the ACL community can directly benefit from the extension-friendly architecture of the Anthology. We end by issuing an open challenge of reviewer matching we encourage the community to rally towards.",
}
@inproceedings{agirre-etal-2018-risk,
    title = "The risk of sub-optimal use of Open Source {NLP} Software: {UKB} is inadvertently state-of-the-art in knowledge-based {WSD}",
    author = "Agirre, Eneko  and
      L{\'o}pez de Lacalle, Oier  and
      Soroa, Aitor",
    booktitle = "Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-2505",
    doi = "10.18653/v1/W18-2505",
    pages = "29--33",
    abstract = "UKB is an open source collection of programs for performing, among other tasks, Knowledge-Based Word Sense Disambiguation (WSD). Since it was released in 2009 it has been often used out-of-the-box in sub-optimal settings. We show that nine years later it is the state-of-the-art on knowledge-based WSD. This case shows the pitfalls of releasing open source NLP software without optimal default settings and precise instructions for reproducibility.",
}
@inproceedings{pressel-etal-2018-baseline,
    title = "{B}aseline: A Library for Rapid Modeling, Experimentation and Development of Deep Learning Algorithms targeting {NLP}",
    author = "Pressel, Daniel  and
      Ray Choudhury, Sagnik  and
      Lester, Brian  and
      Zhao, Yanjie  and
      Barta, Matt",
    booktitle = "Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-2506",
    doi = "10.18653/v1/W18-2506",
    pages = "34--40",
    abstract = "We introduce Baseline: a library for reproducible deep learning research and fast model development for NLP. The library provides easily extensible abstractions and implementations for data loading, model development, training and export of deep learning architectures. It also provides implementations for simple, high-performance, deep learning models for various NLP tasks, against which newly developed models can be compared. Deep learning experiments are hard to reproduce, Baseline provides functionalities to track them. The goal is to allow a researcher to focus on model development, delegating the repetitive tasks to the library.",
}
@inproceedings{kuchaiev-etal-2018-openseq2seq,
    title = "{O}pen{S}eq2{S}eq: Extensible Toolkit for Distributed and Mixed Precision Training of Sequence-to-Sequence Models",
    author = "Kuchaiev, Oleksii  and
      Ginsburg, Boris  and
      Gitman, Igor  and
      Lavrukhin, Vitaly  and
      Case, Carl  and
      Micikevicius, Paulius",
    booktitle = "Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-2507",
    doi = "10.18653/v1/W18-2507",
    pages = "41--46",
    abstract = "We present OpenSeq2Seq {--} an open-source toolkit for training sequence-to-sequence models. The main goal of our toolkit is to allow researchers to most effectively explore different sequence-to-sequence architectures. The efficiency is achieved by fully supporting distributed and mixed-precision training. OpenSeq2Seq provides building blocks for training encoder-decoder models for neural machine translation and automatic speech recognition. We plan to extend it with other modalities in the future.",
}
@inproceedings{germann-etal-2018-integrating,
    title = "Integrating Multiple {NLP} Technologies into an Open-source Platform for Multilingual Media Monitoring",
    author = "Germann, Ulrich  and
      Liepins, Ren{\=a}rs  and
      Gosko, Didzis  and
      Barzdins, Guntis",
    booktitle = "Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-2508",
    doi = "10.18653/v1/W18-2508",
    pages = "47--51",
    abstract = "The open-source SUMMA Platform is a highly scalable distributed architecture for monitoring a large number of media broadcasts in parallel, with a lag behind actual broadcast time of at most a few minutes. It assembles numerous state-of-the-art NLP technologies into a fully automated media ingestion pipeline that can record live broadcasts, detect and transcribe spoken content, translate from several languages (original text or transcribed speech) into English, recognize Named Entities, detect topics, cluster and summarize documents across language barriers, and extract and store factual claims in these news items. This paper describes the intended use cases and discusses the system design decisions that allowed us to integrate state-of-the-art NLP modules into an effective workflow with comparatively little effort.",
}
@inproceedings{rush-2018-annotated,
    title = "The Annotated Transformer",
    author = "Rush, Alexander",
    booktitle = "Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-2509",
    doi = "10.18653/v1/W18-2509",
    pages = "52--60",
    abstract = "(Note this is not being submitted blind. The chair of the workshop requested this submission unblinded from me on twitter, so assuming that is okay.) A major goal of open-source NLP is to quickly and accurately reproduce the results of new work, in a manner that the community can easily use and modify. While most papers publish enough detail for replication, it still may be difficult to achieve good results in practice. This paper presents a worked exercise of paper reproduction with the goal of implementing the results of the recent Transformer model. The replication exercise aims at simple code structure that follows closely with the original work, while achieving an efficient usable system.",
}
