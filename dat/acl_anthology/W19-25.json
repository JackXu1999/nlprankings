[{"id":"W19-2501","title":"Modeling Word Emotion in Historical Language: Quantity Beats Supposed Stability in Seed Word Selection","authors":["Hellrich, Johannes","Buechel, Sven","Hahn, Udo"],"emails":["johannes.hellrich@uni-jena.de","sven.buechel@uni-jena.de","udo.hahn@uni-jena.de"],"author_id":["johannes-hellrich","sven-buechel","udo-hahn"],"abstract":"To understand historical texts, we must be aware that language{---}including the emotional connotation attached to words{---}changes over time. In this paper, we aim at estimating the emotion which is associated with a given word in former language stages of English and German. Emotion is represented following the popular Valence-Arousal-Dominance (VAD) annotation scheme. While being more expressive than polarity alone, existing word emotion induction methods are typically not suited for addressing it. To overcome this limitation, we present adaptations of two popular algorithms to VAD. To measure their effectiveness in diachronic settings, we present the first gold standard for historical word emotions, which was created by scholars with proficiency in the respective language stages and covers both English and German. In contrast to claims in previous work, our findings indicate that hand-selecting small sets of seed words with supposedly stable emotional meaning is actually harm- rather than helpful.","pages":"1--11","doi":"10.18653\/v1\/W19-2501","url":"https:\/\/www.aclweb.org\/anthology\/W19-2501","publisher":"Association for Computational Linguistics","address":"Minneapolis, USA","year":"2019","month":"June","booktitle":"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature"},{"id":"W19-2502","title":"Clustering-Based Article Identification in Historical Newspapers","authors":["Riedl, Martin","Betz, Daniela","Pad{\\'o}, Sebastian"],"emails":["riedlmn@ims.uni-stuttgart.de","mail@danielabetz.de","pado@ims.uni-stuttgart.de"],"author_id":["martin-riedl","daniela-betz","sebastian-pado"],"abstract":"This article focuses on the problem of identifying articles and recovering their text from within and across newspaper pages when OCR just delivers one text file per page. We frame the task as a segmentation plus clustering step. Our results on a sample of 1912 New York Tribune magazine shows that performing the clustering based on similarities computed with word embeddings outperforms a similarity measure based on character n-grams and words. Furthermore, the automatic segmentation based on the text results in low scores, due to the low quality of some OCRed documents.","pages":"12--17","doi":"10.18653\/v1\/W19-2502","url":"https:\/\/www.aclweb.org\/anthology\/W19-2502","publisher":"Association for Computational Linguistics","address":"Minneapolis, USA","year":"2019","month":"June","booktitle":"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature"},{"id":"W19-2503","title":"The Scientization of Literary Study","authors":["Degaetano-Ortlieb, Stefania","Piper, Andrew"],"emails":["s.degaetano@mx.uni-saarland.de","andrew.piper@mcgill.ca"],"author_id":["stefania-degaetano-ortlieb","andrew-piper"],"abstract":"Scholarly practices within the humanities have historically been perceived as distinct from the natural sciences. We look at literary studies, a discipline strongly anchored in the humanities, and hypothesize that over the past half-century literary studies has instead undergone a process of {``}scientization{''}, adopting linguistic behavior similar to the sciences. We test this using methods based on information theory, comparing a corpus of literary studies articles (around 63,400) with a corpus of standard English and scientific English respectively. We show evidence for {``}scientization{''} effects in literary studies, though at a more muted level than scientific English, suggesting that literary studies occupies a middle ground with respect to standard English in the larger space of academic disciplines. More generally, our methodology can be applied to investigate the social positioning and development of language use across different domains (e.g. scientific disciplines, language varieties, registers).","pages":"18--28","doi":"10.18653\/v1\/W19-2503","url":"https:\/\/www.aclweb.org\/anthology\/W19-2503","publisher":"Association for Computational Linguistics","address":"Minneapolis, USA","year":"2019","month":"June","booktitle":"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature"},{"id":"W19-2504","title":"Are Fictional Voices Distinguishable? Classifying Character Voices in Modern Drama","authors":["Vishnubhotla, Krishnapriya","Hammond, Adam","Hirst, Graeme"],"emails":["vkpriya@cs.toronto.edu","adam.hammond@utoronto.ca","gh@cs.toronto.edu"],"author_id":["krishnapriya-vishnubhotla","adam-hammond","graeme-hirst"],"abstract":"According to the literary theory of Mikhail Bakhtin, a dialogic novel is one in which characters speak in their own distinct voices, rather than serving as mouthpieces for their authors. We use text classification to determine which authors best achieve dialogism, looking at a corpus of plays from the late nineteenth and early twentieth centuries. We find that the SAGE model of text generation, which highlights deviations from a background lexical distribution, is an effective method of weighting the words of characters{'} utterances. Our results show that it is indeed possible to distinguish characters by their speech in the plays of canonical writers such as George Bernard Shaw, whereas characters are clustered more closely in the works of lesser-known playwrights.","pages":"29--34","doi":"10.18653\/v1\/W19-2504","url":"https:\/\/www.aclweb.org\/anthology\/W19-2504","publisher":"Association for Computational Linguistics","address":"Minneapolis, USA","year":"2019","month":"June","booktitle":"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature"},{"id":"W19-2505","title":"Automatic Alignment and Annotation Projection for Literary Texts","authors":["Steinbach, Uli","Rehbein, Ines"],"emails":["steinbach@cl.uni-heidelberg.de","rehbein@cl.uni-heidelberg.de"],"author_id":["uli-steinbach","ines-rehbein"],"abstract":"This paper presents a modular NLP pipeline for the creation of a parallel literature corpus, followed by annotation transfer from the source to the target language. The test case we use to evaluate our pipeline is the automatic transfer of quote and speaker mention annotations from English to German. We evaluate the different components of the pipeline and discuss challenges specific to literary texts. Our experiments show that after applying a reasonable amount of semi-automatic postprocessing we can obtain high-quality aligned and annotated resources for a new language.","pages":"35--45","doi":"10.18653\/v1\/W19-2505","url":"https:\/\/www.aclweb.org\/anthology\/W19-2505","publisher":"Association for Computational Linguistics","address":"Minneapolis, USA","year":"2019","month":"June","booktitle":"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature"},{"id":"W19-2506","title":"Inferring missing metadata from environmental policy texts","authors":["Bethard, Steven","Laparra, Egoitz","Wang, Sophia","Zhao, Yiyun","Al-Ghezi, Ragheb","Lien, Aaron","L{\\'o}pez-Hoffman, Laura"],"emails":["bethard@email.arizona.edu","laparra@email.arizona.edu","rxnsp689@email.arizona.edu","yiyunzhao@email.arizona.edu","raghebalghezi@email.arizona.edu","alien@email.arizona.edu","lauralh@email.arizona.edu"],"author_id":["steven-bethard","egoitz-laparra","sophia-wang","yiyun-zhao","ragheb-al-ghezi","aaron-lien","laura-lopez-hoffman"],"abstract":"The National Environmental Policy Act (NEPA) provides a trove of data on how environmental policy decisions have been made in the United States over the last 50 years. Unfortunately, there is no central database for this information and it is too voluminous to assess manually. We describe our efforts to enable systematic research over US environmental policy by extracting and organizing metadata from the text of NEPA documents. Our contributions include collecting more than 40,000 NEPA-related documents, and evaluating rule-based baselines that establish the difficulty of three important tasks: identifying lead agencies, aligning document versions, and detecting reused text.","pages":"46--51","doi":"10.18653\/v1\/W19-2506","url":"https:\/\/www.aclweb.org\/anthology\/W19-2506","publisher":"Association for Computational Linguistics","address":"Minneapolis, USA","year":"2019","month":"June","booktitle":"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature"},{"id":"W19-2507","title":"Stylometric Classification of Ancient {G}reek Literary Texts by Genre","authors":["Gianitsos, Efthimios","Bolt, Thomas","Chaudhuri, Pramit","Dexter, Joseph"],"emails":["","","",""],"author_id":["efthimios-gianitsos","thomas-bolt","pramit-chaudhuri","joseph-dexter"],"abstract":"Classification of texts by genre is an important application of natural language processing to literary corpora but remains understudied for premodern and non-English traditions. We develop a stylometric feature set for ancient Greek that enables identification of texts as prose or verse. The set contains over 20 primarily syntactic features, which are calculated according to custom, language-specific heuristics. Using these features, we classify almost all surviving classical Greek literature as prose or verse with {\\textgreater}97{\\%} accuracy and F1 score, and further classify a selection of the verse texts into the traditional genres of epic and drama.","pages":"52--60","doi":"10.18653\/v1\/W19-2507","url":"https:\/\/www.aclweb.org\/anthology\/W19-2507","publisher":"Association for Computational Linguistics","address":"Minneapolis, USA","year":"2019","month":"June","booktitle":"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature"},{"id":"W19-2508","title":"A framework for streamlined statistical prediction using topic models","authors":["Glenny, Vanessa","Tuke, Jonathan","Bean, Nigel","Mitchell, Lewis"],"emails":["","","","lewis.mitchell@adelaide.edu.au"],"author_id":["vanessa-glenny","jonathan-tuke","nigel-bean","lewis-mitchell"],"abstract":"In the Humanities and Social Sciences, there is increasing interest in approaches to information extraction, prediction, intelligent linkage, and dimension reduction applicable to large text corpora. With approaches in these fields being grounded in traditional statistical techniques, the need arises for frameworks whereby advanced NLP techniques such as topic modelling may be incorporated within classical methodologies. This paper provides a classical, supervised, statistical learning framework for prediction from text, using topic models as a data reduction method and the topics themselves as predictors, alongside typical statistical tools for predictive modelling. We apply this framework in a Social Sciences context (applied animal behaviour) as well as a Humanities context (narrative analysis) as examples of this framework. The results show that topic regression models perform comparably to their much less efficient equivalents that use individual words as predictors.","pages":"61--70","doi":"10.18653\/v1\/W19-2508","url":"https:\/\/www.aclweb.org\/anthology\/W19-2508","publisher":"Association for Computational Linguistics","address":"Minneapolis, USA","year":"2019","month":"June","booktitle":"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature"},{"id":"W19-2509","title":"Revisiting {NMT} for Normalization of Early {E}nglish Letters","authors":["H{\\\"a}m{\\\"a}l{\\\"a}inen, Mika","S{\\\"a}ily, Tanja","Rueter, Jack","Tiedemann, J{\\\"o}rg","M{\\\"a}kel{\\\"a}, Eetu"],"emails":["mika.h{\\\"a}m{\\\"a}l{\\\"a}inen@helsinki.fi","tanja.s{\\\"a}ily@helsinki.fi","jack.rueter@helsinki.fi","j{\\\"o}rg.tiedemann@helsinki.fi","eetu.m{\\\"a}kel{\\\"a}@helsinki.fi"],"author_id":["mika-hamalainen","tanja-saily","jack-rueter","jorg-tiedemann","eetu-makela"],"abstract":"This paper studies the use of NMT (neural machine translation) as a normalization method for an early English letter corpus. The corpus has previously been normalized so that only less frequent deviant forms are left out without normalization. This paper discusses different methods for improving the normalization of these deviant forms by using different approaches. Adding features to the training data is found to be unhelpful, but using a lexicographical resource to filter the top candidates produced by the NMT model together with lemmatization improves results.","pages":"71--75","doi":"10.18653\/v1\/W19-2509","url":"https:\/\/www.aclweb.org\/anthology\/W19-2509","publisher":"Association for Computational Linguistics","address":"Minneapolis, USA","year":"2019","month":"June","booktitle":"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature"},{"id":"W19-2510","title":"Graph convolutional networks for exploring authorship hypotheses","authors":["Lippincott, Tom"],"emails":["tom@cs.jhu.edu"],"author_id":["tom-lippincott"],"abstract":"This work considers a task from traditional literary criticism: annotating a structured, composite document with information about its sources. We take the Documentary Hypothesis, a prominent theory regarding the composition of the first five books of the Hebrew bible, extract stylistic features designed to avoid bias or overfitting, and train several classification models. Our main result is that the recently-introduced graph convolutional network architecture outperforms structurally-uninformed models. We also find that including information about the granularity of text spans is a crucial ingredient when employing hidden layers, in contrast to simple logistic regression. We perform error analysis at several levels, noting how some characteristic limitations of the models and simple features lead to misclassifications, and conclude with an overview of future work.","pages":"76--81","doi":"10.18653\/v1\/W19-2510","url":"https:\/\/www.aclweb.org\/anthology\/W19-2510","publisher":"Association for Computational Linguistics","address":"Minneapolis, USA","year":"2019","month":"June","booktitle":"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature"},{"id":"W19-2511","title":"Semantics and Homothetic Clustering of Hafez Poetry","authors":["Rahgozar, Arya","Inkpen, Diana"],"emails":["arahgoza@uottawa.ca","diana@site.uottawa.ca"],"author_id":["arya-rahgozar","diana-inkpen"],"abstract":"We have created two sets of labels for Hafez (1315-1390) poems, using unsupervised learn- ing. Our labels are the only semantic cluster- ing alternative to the previously existing, hand- labeled, gold-standard classification of Hafez poems, to be used for literary research. We have cross-referenced, measured and analyzed the agreements of our clustering labels with Houman{'}s chronological classes. Our features are based on topic modeling and word embed- dings. We also introduced a similarity of similarities{'} features, we called homothetic clustering approach that proved effective, in case of Hafez{'}s small corpus of ghazals2. Although all our experiments showed different clusters when compared with Houman{'}s classes, we think they were valid in their own right to have provided further insights, and have proved useful as a contrasting alternative to Houman{'}s classes. Our homothetic clusterer and its feature design and engineering framework can be used for further semantic analysis of Hafez{'}s poetry and other similar literary research.","pages":"82--90","doi":"10.18653\/v1\/W19-2511","url":"https:\/\/www.aclweb.org\/anthology\/W19-2511","publisher":"Association for Computational Linguistics","address":"Minneapolis, USA","year":"2019","month":"June","booktitle":"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature"},{"id":"W19-2512","title":"Computational Linguistics Applications for Multimedia Services","authors":["Rim, Kyeongmin","Lynch, Kelley","Pustejovsky, James"],"emails":["krim@brandeis.edu","kmlynch@brandeis.edu","jamesp@brandeis.edu"],"author_id":["kyeongmin-rim","kelley-lynch","james-pustejovsky"],"abstract":"We present Computational Linguistics Applications for Multimedia Services (CLAMS), a platform that provides access to computational content analysis tools for archival multimedia material that appear in different media, such as text, audio, image, and video. The primary goal of CLAMS is: (1) to develop an interchange format between multimodal metadata generation tools to ensure interoperability between tools; (2) to provide users with a portable, user-friendly workflow engine to chain selected tools to extract meaningful analyses; and (3) to create a public software development kit (SDK) for developers that eases deployment of analysis tools within the CLAMS platform. CLAMS is designed to help archives and libraries enrich the metadata associated with their mass-digitized multimedia collections, that would otherwise be largely unsearchable.","pages":"91--97","doi":"10.18653\/v1\/W19-2512","url":"https:\/\/www.aclweb.org\/anthology\/W19-2512","publisher":"Association for Computational Linguistics","address":"Minneapolis, USA","year":"2019","month":"June","booktitle":"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature"},{"id":"W19-2513","title":"Correcting Whitespace Errors in Digitized Historical Texts","authors":["Soni, Sandeep","Klein, Lauren","Eisenstein, Jacob"],"emails":["sandeepsoni@gatech.edu","lauren.klein@lmc.gatech.edu","jacobe@gmail.com"],"author_id":["sandeep-soni","lauren-klein","jacob-eisenstein"],"abstract":"Whitespace errors are common to digitized archives. This paper describes a lightweight unsupervised technique for recovering the original whitespace. Our approach is based on count statistics from Google n-grams, which are converted into a likelihood ratio test computed from interpolated trigram and bigram probabilities. To evaluate this approach, we annotate a small corpus of whitespace errors in a digitized corpus of newspapers from the 19th century United States. Our technique identifies and corrects most whitespace errors while introducing a minimal amount of oversegmentation: it achieves 77{\\%} recall at a false positive rate of less than 1{\\%}, and 91{\\%} recall at a false positive rate of less than 3{\\%}.","pages":"98--103","doi":"10.18653\/v1\/W19-2513","url":"https:\/\/www.aclweb.org\/anthology\/W19-2513","publisher":"Association for Computational Linguistics","address":"Minneapolis, USA","year":"2019","month":"June","booktitle":"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature"},{"id":"W19-2514","title":"On the Feasibility of Automated Detection of Allusive Text Reuse","authors":["Manjavacas, Enrique","Long, Brian","Kestemont, Mike"],"emails":["enrique.manjavacas@uantwerpen.be","blong2@alumni.nd.edu","mike.kestemont@uantwerpen.be"],"author_id":["enrique-manjavacas","brian-long","mike-kestemont"],"abstract":"The detection of allusive text reuse is particularly challenging due to the sparse evidence on which allusive references rely {---} commonly based on none or very few shared words. Arguably, lexical semantics can be resorted to since uncovering semantic relations between words has the potential to increase the support underlying the allusion and alleviate the lexical sparsity. A further obstacle is the lack of evaluation benchmark corpora, largely due to the highly interpretative character of the annotation process. In the present paper, we aim to elucidate the feasibility of automated allusion detection. We approach the matter from an Information Retrieval perspective in which referencing texts act as queries and referenced texts as relevant documents to be retrieved, and estimate the difficulty of benchmark corpus compilation by a novel inter-annotator agreement study on query segmentation. Furthermore, we investigate to what extent the integration of lexical semantic information derived from distributional models and ontologies can aid retrieving cases of allusive reuse. The results show that (i) despite low agreement scores, using manual queries considerably improves retrieval performance with respect to a windowing approach, and that (ii) retrieval performance can be moderately boosted with distributional semantics.","pages":"104--114","doi":"10.18653\/v1\/W19-2514","url":"https:\/\/www.aclweb.org\/anthology\/W19-2514","publisher":"Association for Computational Linguistics","address":"Minneapolis, USA","year":"2019","month":"June","booktitle":"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature"},{"id":"W19-2515","title":"The limits of {S}panglish?","authors":["Bullock, Barbara","Guzm{\\'a}n, Wally","Toribio, Almeida Jacqueline"],"emails":["bbullock@austin.utexas.edu","gualbertoguzman@utexas.edu","toribio@austin.utexas.edu"],"author_id":["barbara-bullock","wally-guzman","almeida-jacqueline-toribio"],"abstract":"Linguistic code-switching (C-S) is common in oral bilingual vernacular speech. When used in literature, C-S becomes an artistic choice that can mirror the patterns of bilingual interactions. But it can also potentially exceed them. What are the limits of C-S? We model features of C-S in corpora of contemporary U.S. Spanish-English literary and conversational data to analyze why some critics view the {`}Spanglish{'} texts of Ilan Stavans as deviating from a C-S norm.","pages":"115--121","doi":"10.18653\/v1\/W19-2515","url":"https:\/\/www.aclweb.org\/anthology\/W19-2515","publisher":"Association for Computational Linguistics","address":"Minneapolis, USA","year":"2019","month":"June","booktitle":"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature"},{"id":"W19-2516","title":"Sign Clustering and Topic Extraction in Proto-Elamite","authors":["Born, Logan","Kelley, Kate","Kambhatla, Nishant","Chen, Carolyn","Sarkar, Anoop"],"emails":["loborn@sfu.ca","kathryn.kelley@ubc.ca","nkambhat@sfu.ca","nll-contact@sfu.ca","anoop@sfu.ca"],"author_id":["logan-born","kate-kelley","nishant-kambhatla","carolyn-chen","anoop-sarkar"],"abstract":"We describe a first attempt at using techniques from computational linguistics to analyze the undeciphered proto-Elamite script. Using hierarchical clustering, n-gram frequencies, and LDA topic models, we both replicate results obtained by manual decipherment and reveal previously-unobserved relationships between signs. This demonstrates the utility of these techniques as an aid to manual decipherment.","pages":"122--132","doi":"10.18653\/v1\/W19-2516","url":"https:\/\/www.aclweb.org\/anthology\/W19-2516","publisher":"Association for Computational Linguistics","address":"Minneapolis, USA","year":"2019","month":"June","booktitle":"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature"}]