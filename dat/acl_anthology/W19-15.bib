@proceedings{ws-2019-structured,
    title = "Proceedings of the Third Workshop on Structured Prediction for {NLP}",
    author = "Martins, Andre  and
      Vlachos, Andreas  and
      Kozareva, Zornitsa  and
      Ravi, Sujith  and
      Lampouras, Gerasimos  and
      Niculae, Vlad  and
      Kreutzer, Julia",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-1500",
}
@inproceedings{ding-koehn-2019-parallelizable,
    title = "Parallelizable Stack Long Short-Term Memory",
    author = "Ding, Shuoyang  and
      Koehn, Philipp",
    booktitle = "Proceedings of the Third Workshop on Structured Prediction for {NLP}",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-1501",
    doi = "10.18653/v1/W19-1501",
    pages = "1--6",
    abstract = "Stack Long Short-Term Memory (StackLSTM) is useful for various applications such as parsing and string-to-tree neural machine translation, but it is also known to be notoriously difficult to parallelize for GPU training due to the fact that the computations are dependent on discrete operations. In this paper, we tackle this problem by utilizing state access patterns of StackLSTM to homogenize computations with regard to different discrete operations. Our parsing experiments show that the method scales up almost linearly with increasing batch size, and our parallelized PyTorch implementation trains significantly faster compared to the Dynet C++ implementation.",
}
@inproceedings{gupta-durrett-2019-tracking,
    title = "Tracking Discrete and Continuous Entity State for Process Understanding",
    author = "Gupta, Aditya  and
      Durrett, Greg",
    booktitle = "Proceedings of the Third Workshop on Structured Prediction for {NLP}",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-1502",
    doi = "10.18653/v1/W19-1502",
    pages = "7--12",
    abstract = "Procedural text, which describes entities and their interactions as they undergo some process, depicts entities in a uniquely nuanced way. First, each entity may have some observable discrete attributes, such as its state or location; modeling these involves imposing global structure and enforcing consistency. Second, an entity may have properties which are not made explicit but can be effectively induced and tracked by neural networks. In this paper, we propose a structured neural architecture that reflects this dual nature of entity evolution. The model tracks each entity recurrently, updating its hidden continuous representation at each step to contain relevant state information. The global discrete state structure is explicitly modelled with a neural CRF over the changing hidden representation of the entity. This CRF can explicitly capture constraints on entity states over time, enforcing that, for example, an entity cannot move to a location after it is destroyed. We evaluate the performance of our proposed model on QA tasks over process paragraphs in the ProPara dataset and find that our model achieves state-of-the-art results.",
}
@inproceedings{bommasani-etal-2019-sparse,
    title = "{SPARSE}: Structured Prediction using Argument-Relative Structured Encoding",
    author = "Bommasani, Rishi  and
      Katiyar, Arzoo  and
      Cardie, Claire",
    booktitle = "Proceedings of the Third Workshop on Structured Prediction for {NLP}",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-1503",
    doi = "10.18653/v1/W19-1503",
    pages = "13--17",
    abstract = "We propose structured encoding as a novel approach to learning representations for relations and events in neural structured prediction. Our approach explicitly leverages the structure of available relation and event metadata to generate these representations, which are parameterized by both the attribute structure of the metadata as well as the learned representation of the arguments of the relations and events. We consider affine, biaffine, and recurrent operators for building hierarchical representations and modelling underlying features. We apply our approach to the second-order structured prediction task studied in the 2016/2017 Belief and Sentiment analysis evaluations (BeSt): given a document and its entities, relations, and events (including metadata and mentions), determine the sentiment of each entity towards every relation and event in the document. Without task-specific knowledge sources or domain engineering, we significantly improve over systems and baselines that neglect the available metadata or its hierarchical structure. We observe across-the-board improvements on the BeSt 2016/2017 sentiment analysis task of at least 2.3 (absolute) and 10.6{\%} (relative) F-measure over the previous state-of-the-art.",
}
@inproceedings{zupon-etal-2019-lightly,
    title = "Lightly-supervised Representation Learning with Global Interpretability",
    author = "Zupon, Andrew  and
      Alexeeva, Maria  and
      Valenzuela-Esc{\'a}rcega, Marco  and
      Nagesh, Ajay  and
      Surdeanu, Mihai",
    booktitle = "Proceedings of the Third Workshop on Structured Prediction for {NLP}",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-1504",
    doi = "10.18653/v1/W19-1504",
    pages = "18--28",
    abstract = "We propose a lightly-supervised approach for information extraction, in particular named entity classification, which combines the benefits of traditional bootstrapping, i.e., use of limited annotations and interpretability of extraction patterns, with the robust learning approaches proposed in representation learning. Our algorithm iteratively learns custom embeddings for both the multi-word entities to be extracted and the patterns that match them from a few example entities per category. We demonstrate that this representation-based approach outperforms three other state-of-the-art bootstrapping approaches on two datasets: CoNLL-2003 and OntoNotes. Additionally, using these embeddings, our approach outputs a globally-interpretable model consisting of a decision list, by ranking patterns based on their proximity to the average entity embedding in a given class. We show that this interpretable model performs close to our complete bootstrapping model, proving that representation learning can be used to produce interpretable models with small loss in performance. This decision list can be edited by human experts to mitigate some of that loss and in some cases outperform the original model.",
}
@inproceedings{luo-etal-2019-semi,
    title = "Semi-Supervised Teacher-Student Architecture for Relation Extraction",
    author = "Luo, Fan  and
      Nagesh, Ajay  and
      Sharp, Rebecca  and
      Surdeanu, Mihai",
    booktitle = "Proceedings of the Third Workshop on Structured Prediction for {NLP}",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-1505",
    doi = "10.18653/v1/W19-1505",
    pages = "29--37",
    abstract = "Generating a large amount of training data for information extraction (IE) is either costly (if annotations are created manually), or runs the risk of introducing noisy instances (if distant supervision is used). On the other hand, semi- supervised learning (SSL) is a cost-efficient solution to combat lack of training data. In this paper, we adapt Mean Teacher (Tarvainen and Valpola, 2017), a denoising SSL framework to extract semantic relations between pairs of entities. We explore the sweet spot of amount of supervision required for good performance on this binary relation extraction task. Addition- ally, different syntax representations are incorporated into our models to enhance the learned representation of sentences. We evaluate our approach on the Google-IISc Distant Supervision (GDS) dataset, which removes test data noise present in all previous distance supervision datasets, which makes it a reliable evaluation benchmark (Jat et al., 2017). Our results show that the SSL Mean Teacher approach nears the performance of fully-supervised approaches even with only 10{\%} of the labeled corpus. Further, the syntax-aware model out- performs other syntax-free approaches across all levels of supervision.",
}
