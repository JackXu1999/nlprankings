@proceedings{ws-2019-arabic,
    title = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    author = "El-Hajj, Wassim  and
      Belguith, Lamia Hadrich  and
      Bougares, Fethi  and
      Magdy, Walid  and
      Zitouni, Imed  and
      Tomeh, Nadi  and
      El-Haj, Mahmoud",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4600",
}
@inproceedings{kalimuthu-etal-2019-incremental,
    title = "Incremental Domain Adaptation for Neural Machine Translation in Low-Resource Settings",
    author = "Kalimuthu, Marimuthu  and
      Barz, Michael  and
      Sonntag, Daniel",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4601",
    doi = "10.18653/v1/W19-4601",
    pages = "1--10",
    abstract = "We study the problem of incremental domain adaptation of a generic neural machine translation model with limited resources (e.g., budget and time) for human translations or model training. In this paper, we propose a novel query strategy for selecting {``}unlabeled{''} samples from a new domain based on sentence embeddings for Arabic. We accelerate the fine-tuning process of the generic model to the target domain. Specifically, our approach estimates the informativeness of instances from the target domain by comparing the distance of their sentence embeddings to embeddings from the generic domain. We perform machine translation experiments (Ar-to-En direction) for comparing a random sampling baseline with our new approach, similar to active learning, using two small update sets for simulating the work of human translators. For the prescribed setting we can save more than 50{\%} of the annotation costs without loss in quality, demonstrating the effectiveness of our approach.",
}
@inproceedings{tawfik-etal-2019-morphology,
    title = "Morphology-aware Word-Segmentation in Dialectal {A}rabic Adaptation of Neural Machine Translation",
    author = "Tawfik, Ahmed  and
      Emam, Mahitab  and
      Essam, Khaled  and
      Nabil, Robert  and
      Hassan, Hany",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4602",
    doi = "10.18653/v1/W19-4602",
    pages = "11--17",
    abstract = "Parallel corpora available for building machine translation (MT) models for dialectal Arabic (DA) are rather limited. The scarcity of resources has prompted the use of Modern Standard Arabic (MSA) abundant resources to complement the limited dialectal resource. However, dialectal clitics often differ between MSA and DA. This paper compares morphology-aware DA word segmentation to other word segmentation approaches like Byte Pair Encoding (BPE) and Sub-word Regularization (SR). A set of experiments conducted on Egyptian Arabic (EA), Levantine Arabic (LA), and Gulf Arabic (GA) show that a sufficiently accurate morphology-aware segmentation used in conjunction with BPE outperforms the other word segmentation approaches.",
}
@inproceedings{attia-etal-2019-pos,
    title = "{POS} Tagging for Improving Code-Switching Identification in {A}rabic",
    author = "Attia, Mohammed  and
      Samih, Younes  and
      Elkahky, Ali  and
      Mubarak, Hamdy  and
      Abdelali, Ahmed  and
      Darwish, Kareem",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4603",
    doi = "10.18653/v1/W19-4603",
    pages = "18--29",
    abstract = "When speakers code-switch between their native language and a second language or language variant, they follow a syntactic pattern where words and phrases from the embedded language are inserted into the matrix language. This paper explores the possibility of utilizing this pattern in improving code-switching identification between Modern Standard Arabic (MSA) and Egyptian Arabic (EA). We try to answer the question of how strong is the POS signal in word-level code-switching identification. We build a deep learning model enriched with linguistic features (including POS tags) that outperforms the state-of-the-art results by 1.9{\%} on the development set and 1.0{\%} on the test set. We also show that in intra-sentential code-switching, the selection of lexical items is constrained by POS categories, where function words tend to come more often from the dialectal language while the majority of content words come from the standard language.",
}
@inproceedings{mulki-etal-2019-syntax,
    title = "Syntax-Ignorant N-gram Embeddings for Sentiment Analysis of {A}rabic Dialects",
    author = "Mulki, Hala  and
      Haddad, Hatem  and
      Gridach, Mourad  and
      Babao{\u{g}}lu, Ismail",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4604",
    doi = "10.18653/v1/W19-4604",
    pages = "30--39",
    abstract = "Arabic sentiment analysis models have employed compositional embedding features to represent the Arabic dialectal content. These embeddings are usually composed via ordered, syntax-aware composition functions and learned within deep neural frameworks. With the free word order and the varying syntax nature across the different Arabic dialects, a sentiment analysis system developed for one dialect might not be efficient for the others. Here we present syntax-ignorant n-gram embeddings to be used in sentiment analysis of several Arabic dialects. The proposed embeddings were composed and learned using an unordered composition function and a shallow neural model. Five datasets of different dialects were used to evaluate the produced embeddings in the sentiment analysis task. The obtained results revealed that, our syntax-ignorant embeddings could outperform word2vec model and doc2vec both variant models in addition to hand-crafted system baselines, while a competent performance was noticed towards baseline systems that adopted more complicated neural architectures.",
}
@inproceedings{lachraf-etal-2019-arbengvec,
    title = "{A}rb{E}ng{V}ec : {A}rabic-{E}nglish Cross-Lingual Word Embedding Model",
    author = "Lachraf, Raki  and
      Nagoudi, El Moatez Billah  and
      Ayachi, Youcef  and
      Abdelali, Ahmed  and
      Schwab, Didier",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4605",
    doi = "10.18653/v1/W19-4605",
    pages = "40--48",
    abstract = "Word Embeddings (WE) are getting increasingly popular and widely applied in many Natural Language Processing (NLP) applications due to their effectiveness in capturing semantic properties of words; Machine Translation (MT), Information Retrieval (IR) and Information Extraction (IE) are among such areas. In this paper, we propose an open source ArbEngVec which provides several Arabic-English cross-lingual word embedding models. To train our bilingual models, we use a large dataset with more than 93 million pairs of Arabic-English parallel sentences. In addition, we perform both extrinsic and intrinsic evaluations for the different word embedding model variants. The extrinsic evaluation assesses the performance of models on the cross-language Semantic Textual Similarity (STS), while the intrinsic evaluation is based on the Word Translation (WT) task.",
}
@inproceedings{alqahtani-etal-2019-homograph,
    title = "Homograph Disambiguation through Selective Diacritic Restoration",
    author = "Alqahtani, Sawsan  and
      Aldarmaki, Hanan  and
      Diab, Mona",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4606",
    doi = "10.18653/v1/W19-4606",
    pages = "49--59",
    abstract = "Lexical ambiguity, a challenging phenomenon in all natural languages, is particularly prevalent for languages with diacritics that tend to be omitted in writing, such as Arabic. Omitting diacritics leads to an increase in the number of homographs: different words with the same spelling. Diacritic restoration could theoretically help disambiguate these words, but in practice, the increase in overall sparsity leads to performance degradation in NLP applications. In this paper, we propose approaches for automatically marking a subset of words for diacritic restoration, which leads to selective homograph disambiguation. Compared to full or no diacritic restoration, these approaches yield selectively-diacritized datasets that balance sparsity and lexical disambiguation. We evaluate the various selection strategies extrinsically on several downstream applications: neural machine translation, part-of-speech tagging, and semantic textual similarity. Our experiments on Arabic show promising results, where our devised strategies on selective diacritization lead to a more balanced and consistent performance in downstream applications.",
}
@inproceedings{liu-etal-2019-arabic,
    title = "{A}rabic Named Entity Recognition: What Works and What{'}s Next",
    author = "Liu, Liyuan  and
      Shang, Jingbo  and
      Han, Jiawei",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4607",
    doi = "10.18653/v1/W19-4607",
    pages = "60--67",
    abstract = "This paper presents the winning solution to the Arabic Named Entity Recognition challenge run by Topcoder.com. The proposed model integrates various tailored techniques together, including representation learning, feature engineering, sequence labeling, and ensemble learning. The final model achieves a test F{\_}1 score of 75.82{\%} on the AQMAR dataset and outperforms baselines by a large margin. Detailed analyses are conducted to reveal both its strengths and limitations. Specifically, we observe that (1) representation learning modules can significantly boost the performance but requires a proper pre-processing and (2) the resulting embedding can be further enhanced with feature engineering due to the limited size of the training data. All implementations and pre-trained models are made public.",
}
@inproceedings{eljundi-etal-2019-hulmona,
    title = "h{ULM}on{A}: The Universal Language Model in {A}rabic",
    author = "ElJundi, Obeida  and
      Antoun, Wissam  and
      El Droubi, Nour  and
      Hajj, Hazem  and
      El-Hajj, Wassim  and
      Shaban, Khaled",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4608",
    doi = "10.18653/v1/W19-4608",
    pages = "68--77",
    abstract = "Arabic is a complex language with limited resources which makes it challenging to produce accurate text classification tasks such as sentiment analysis. The utilization of transfer learning (TL) has recently shown promising results for advancing accuracy of text classification in English. TL models are pre-trained on large corpora, and then fine-tuned on task-specific datasets. In particular, universal language models (ULMs), such as recently developed BERT, have achieved state-of-the-art results in various NLP tasks in English. In this paper, we hypothesize that similar success can be achieved for Arabic. The work aims at supporting the hypothesis by developing the first Universal Language Model in Arabic (hULMonA - حلمنا meaning our dream), demonstrating its use for Arabic classifications tasks, and demonstrating how a pre-trained multi-lingual BERT can also be used for Arabic. We then conduct a benchmark study to evaluate both ULM successes with Arabic sentiment analysis. Experiment results show that the developed hULMonA and multi-lingual ULM are able to generalize well to multiple Arabic data sets and achieve new state of the art results in Arabic Sentiment Analysis for some of the tested sets.",
}
@inproceedings{adouane-etal-2019-neural,
    title = "Neural Models for Detecting Binary Semantic Textual Similarity for {A}lgerian and {MSA}",
    author = "Adouane, Wafia  and
      Bernardy, Jean-Philippe  and
      Dobnik, Simon",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4609",
    doi = "10.18653/v1/W19-4609",
    pages = "78--87",
    abstract = "We explore the extent to which neural networks can learn to identify semantically equivalent sentences from a small variable dataset using an end-to-end training. We collect a new noisy non-standardised user-generated Algerian (ALG) dataset and also translate it to Modern Standard Arabic (MSA) which serves as its regularised counterpart. We compare the performance of various models on both datasets and report the best performing configurations. The results show that relatively simple models composed of 2 LSTM layers outperform by far other more sophisticated attention-based architectures, for both ALG and MSA datasets.",
}
@inproceedings{el-kishky-etal-2019-constrained,
    title = "Constrained Sequence-to-sequence {S}emitic Root Extraction for Enriching Word Embeddings",
    author = "El-Kishky, Ahmed  and
      Fu, Xingyu  and
      Addawood, Aseel  and
      Sobh, Nahil  and
      Voss, Clare  and
      Han, Jiawei",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4610",
    doi = "10.18653/v1/W19-4610",
    pages = "88--96",
    abstract = "In this paper, we tackle the problem of {``}root extraction{''} from words in the Semitic language family. A challenge in applying natural language processing techniques to these languages is the data sparsity problem that arises from their rich internal morphology, where the substructure is inherently non-concatenative and morphemes are interdigitated in word formation. While previous automated methods have relied on human-curated rules or multiclass classification, they have not fully leveraged the various combinations of regular, sequential concatenative morphology within the words and the internal interleaving within templatic stems of roots and patterns. To address this, we propose a constrained sequence-to-sequence root extraction method. Experimental results show our constrained model outperforms a variety of methods at root extraction. Furthermore, by enriching word embeddings with resulting decompositions, we show improved results on word analogy, word similarity, and language modeling tasks.",
}
@inproceedings{alqaisi-okeefe-2019-en,
    title = "En-Ar Bilingual Word Embeddings without Word Alignment: Factors Effects",
    author = "Alqaisi, Taghreed  and
      O{'}Keefe, Simon",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4611",
    doi = "10.18653/v1/W19-4611",
    pages = "97--107",
    abstract = "This paper introduces the first attempt to investigate morphological segmentation on En-Ar bilingual word embeddings using bilingual word embeddings model without word alignment (BilBOWA). We investigate the effect of sentence length and embedding size on the learning process. Our experiment shows that using the D3 segmentation scheme improves the accuracy of learning bilingual word embeddings up to 10 percentage points compared to the ATB and D0 schemes in all different training settings.",
}
@inproceedings{mozannar-etal-2019-neural,
    title = "Neural {A}rabic Question Answering",
    author = "Mozannar, Hussein  and
      Maamary, Elie  and
      El Hajal, Karl  and
      Hajj, Hazem",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4612",
    doi = "10.18653/v1/W19-4612",
    pages = "108--118",
    abstract = "This paper tackles the problem of open domain factual Arabic question answering (QA) using Wikipedia as our knowledge source. This constrains the answer of any question to be a span of text in Wikipedia. Open domain QA for Arabic entails three challenges: annotated QA datasets in Arabic, large scale efficient information retrieval and machine reading comprehension. To deal with the lack of Arabic QA datasets we present the Arabic Reading Comprehension Dataset (ARCD) composed of 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-SQuAD). Our system for open domain question answering in Arabic (SOQAL) is based on two components: (1) a document retriever using a hierarchical TF-IDF approach and (2) a neural reading comprehension model using the pre-trained bi-directional transformer BERT. Our experiments on ARCD indicate the effectiveness of our approach with our BERT-based reader achieving a 61.3 F1 score, and our open domain system SOQAL achieving a 27.6 F1 score.",
}
@inproceedings{attia-elkahky-2019-segmentation,
    title = "Segmentation for Domain Adaptation in {A}rabic",
    author = "Attia, Mohammed  and
      Elkahky, Ali",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4613",
    doi = "10.18653/v1/W19-4613",
    pages = "119--129",
    abstract = "Segmentation serves as an integral part in many NLP applications including Machine Translation, Parsing, and Information Retrieval. When a model trained on the standard language is applied to dialects, the accuracy drops dramatically. However, there are more lexical items shared by the standard language and dialects than can be found by mere surface word matching. This shared lexicon is obscured by a lot of cliticization, gemination, and character repetition. In this paper, we prove that segmentation and base normalization of dialects can help in domain adaptation by reducing data sparseness. Segmentation will improve a system performance by reducing the number of OOVs, help isolate the differences and allow better utilization of the commonalities. We show that adding a small amount of dialectal segmentation training data reduced OOVs by 5{\%} and remarkably improves POS tagging for dialects by 7.37{\%} f-score, even though no dialect-specific POS training data is included.",
}
@inproceedings{helwe-etal-2019-assessing,
    title = "Assessing {A}rabic Weblog Credibility via Deep Co-learning",
    author = "Helwe, Chadi  and
      Elbassuoni, Shady  and
      Al Zaatari, Ayman  and
      El-Hajj, Wassim",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4614",
    doi = "10.18653/v1/W19-4614",
    pages = "130--136",
    abstract = "Assessing the credibility of online content has garnered a lot of attention lately. We focus on one such type of online content, namely weblogs or blogs for short. Some recent work attempted the task of automatically assessing the credibility of blogs, typically via machine learning. However, in the case of Arabic blogs, there are hardly any datasets available that can be used to train robust machine learning models for this difficult task. To overcome the lack of sufficient training data, we propose deep co-learning, a semi-supervised end-to-end deep learning approach to assess the credibility of Arabic blogs. In deep co-learning, multiple weak deep neural network classifiers are trained using a small labeled dataset, and each using a different view of the data. Each one of these classifiers is then used to classify unlabeled data, and its prediction is used to train the other classifiers in a semi-supervised fashion. We evaluate our deep co-learning approach on an Arabic blogs dataset, and we report significant improvements in performance compared to many baselines including fully-supervised deep learning models as well as ensemble models.",
}
@inproceedings{alshargi-etal-2019-morphologically,
    title = "Morphologically Annotated Corpora for Seven {A}rabic Dialects: Taizi, Sanaani, Najdi, Jordanian, Syrian, Iraqi and {M}oroccan",
    author = "Alshargi, Faisal  and
      Dibas, Shahd  and
      Alkhereyf, Sakhar  and
      Faraj, Reem  and
      Abdulkareem, Basmah  and
      Yagi, Sane  and
      Kacha, Ouafaa  and
      Habash, Nizar  and
      Rambow, Owen",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4615",
    doi = "10.18653/v1/W19-4615",
    pages = "137--147",
    abstract = "We present a collection of morphologically annotated corpora for seven Arabic dialects: Taizi Yemeni, Sanaani Yemeni, Najdi, Jordanian, Syrian, Iraqi and Moroccan Arabic. The corpora collectively cover over 200,000 words, and are all manually annotated in a common set of standards for orthography, diacritized lemmas, tokenization, morphological units and English glosses. These corpora will be publicly available to serve as benchmarks for training and evaluating systems for Arabic dialect morphological analysis and disambiguation.",
}
@inproceedings{sawalha-etal-2019-construction,
    title = "Construction and Annotation of the Jordan Comprehensive Contemporary {A}rabic Corpus ({JCCA})",
    author = "Sawalha, Majdi  and
      Alshargi, Faisal  and
      AlShdaifat, Abdallah  and
      Yagi, Sane  and
      Qudah, Mohammad A.",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4616",
    doi = "10.18653/v1/W19-4616",
    pages = "148--157",
    abstract = "To compile a modern dictionary that catalogues the words in currency, and to study linguistic patterns in the contemporary language, it is necessary to have a corpus of authentic texts that reflect current usage of the language. Although there are numerous Arabic corpora, none claims to be representative of the language in terms of the combination of geographical region, genre, subject matter, mode, and medium. This paper describes a 100-million-word corpus that takes the British National Corpus (BNC) as a model. The aim of the corpus is to be balanced, annotated, comprehensive, and representative of contemporary Arabic as written and spoken in Arab countries today. It will be different from most others in not being heavily-dominated by the news or in mixing the classical with the modern. In this paper is an outline of the methodology adopted for the design, construction, and annotation of this corpus. DIWAN (Alshargi and Rambow, 2015) was used to annotate a one-million-word snapshot of the corpus. DIWAN is a dialectal word annotation tool, but we upgraded it by adding a new tag-set that is based on traditional Arabic grammar and by adding the roots and morphological patterns of nouns and verbs. Moreover, the corpus we constructed covers the major spoken varieties of Arabic.",
}
@inproceedings{durgar-el-kahlout-etal-2019-translating,
    title = "Translating Between Morphologically Rich Languages: An {A}rabic-to-{T}urkish Machine Translation System",
    author = "Durgar El-Kahlout, {\.I}lknur  and
      Bekta{\c{s}}, Emre  and
      Erdem, Naime {\c{S}}eyma  and
      Kaya, Hamza",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4617",
    doi = "10.18653/v1/W19-4617",
    pages = "158--166",
    abstract = "This paper introduces the work on building a machine translation system for Arabic-to-Turkish in the news domain. Our work includes collecting parallel datasets in several ways for a new and low-resourced language pair, building baseline systems with state-of-the-art architectures and developing language specific algorithms for better translation. Parallel datasets are mainly collected three different ways; i) translating Arabic texts into Turkish by professional translators, ii) exploiting the web for open-source Arabic-Turkish parallel texts, iii) using back-translation. We per-formed preliminary experiments for Arabic-to-Turkish machine translation with neural(Marian) machine translation tools with a novel morphologically motivated vocabulary reduction method.",
}
@inproceedings{khaddaj-etal-2019-improved,
    title = "Improved Generalization of {A}rabic Text Classifiers",
    author = "Khaddaj, Alaa  and
      Hajj, Hazem  and
      El-Hajj, Wassim",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4618",
    doi = "10.18653/v1/W19-4618",
    pages = "167--174",
    abstract = "While transfer learning for text has been very active in the English language, progress in Arabic has been slow, including the use of Domain Adaptation (DA). Domain Adaptation is used to generalize the performance of any classifier by trying to balance the classifier{'}s accuracy for a particular task among different text domains. In this paper, we propose and evaluate two variants of a domain adaptation technique: the first is a base model called Domain Adversarial Neural Network (DANN), while the second is a variation that incorporates representational learning. Similar to previous approaches, we propose the use of proxy A-distance as a metric to assess the success of generalization. We make use of ArSentDLEV, a multi-topic dataset collected from the Levantine countries, to test the performance of the models. We show the superiority of the proposed method in accuracy and robustness when dealing with the Arabic language.",
}
@inproceedings{zeroual-etal-2019-osian,
    title = "{OSIAN}: Open Source International {A}rabic News Corpus - Preparation and Integration into the {CLARIN}-infrastructure",
    author = "Zeroual, Imad  and
      Goldhahn, Dirk  and
      Eckart, Thomas  and
      Lakhouaja, Abdelhak",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4619",
    doi = "10.18653/v1/W19-4619",
    pages = "175--182",
    abstract = "The World Wide Web has become a fundamental resource for building large text corpora. Broadcasting platforms such as news websites are rich sources of data regarding diverse topics and form a valuable foundation for research. The Arabic language is extensively utilized on the Web. Still, Arabic is relatively an under-resourced language in terms of availability of freely annotated corpora. This paper presents the first version of the Open Source International Arabic News (OSIAN) corpus. The corpus data was collected from international Arabic news websites, all being freely available on the Web. The corpus consists of about 3.5 million articles comprising more than 37 million sentences and roughly 1 billion tokens. It is encoded in XML; each article is annotated with metadata information. Moreover, each word is annotated with lemma and part-of-speech. the described corpus is processed, archived and published into the CLARIN infrastructure. This publication includes descriptive metadata via OAI-PMH, direct access to the plain text material (available under Creative Commons Attribution-Non-Commercial 4.0 International License - CC BY-NC 4.0), and integration into the WebLicht annotation platform and CLARIN{'}s Federated Content Search FCS.",
}
@inproceedings{algotiml-etal-2019-arabic,
    title = "{A}rabic Tweet-Act: Speech Act Recognition for {A}rabic Asynchronous Conversations",
    author = "Algotiml, Bushra  and
      Elmadany, AbdelRahim  and
      Magdy, Walid",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4620",
    doi = "10.18653/v1/W19-4620",
    pages = "183--191",
    abstract = "Speech acts are the actions that a speaker intends when performing an utterance within conversations. In this paper, we proposed speech act classification for asynchronous conversations on Twitter using multiple machine learning methods including SVM and deep neural networks. We applied the proposed methods on the ArSAS tweets dataset. The obtained results show that superiority of deep learning methods compared to SVMs, where Bi-LSTM managed to achieve an accuracy of 87.5{\%} and a macro-averaged F1 score 61.5{\%}. We believe that our results are the first to be reported on the task of speech-act recognition for asynchronous conversations on Arabic Twitter.",
}
@inproceedings{abu-farha-magdy-2019-mazajak,
    title = "{M}azajak: An Online {A}rabic Sentiment Analyser",
    author = "Abu Farha, Ibrahim  and
      Magdy, Walid",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4621",
    doi = "10.18653/v1/W19-4621",
    pages = "192--198",
    abstract = "Sentiment analysis (SA) is one of the most useful natural language processing applications. Literature is flooding with many papers and systems addressing this task, but most of the work is focused on English. In this paper, we present {``}Mazajak{''}, an online system for Arabic SA. The system is based on a deep learning model, which achieves state-of-the-art results on many Arabic dialect datasets including SemEval 2017 and ASTD. The availability of such system should assist various applications and research that rely on sentiment analysis as a tool.",
}
@inproceedings{bouamor-etal-2019-madar,
    title = "The {MADAR} Shared Task on {A}rabic Fine-Grained Dialect Identification",
    author = "Bouamor, Houda  and
      Hassan, Sabit  and
      Habash, Nizar",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4622",
    doi = "10.18653/v1/W19-4622",
    pages = "199--207",
    abstract = "In this paper, we present the results and findings of the MADAR Shared Task on Arabic Fine-Grained Dialect Identification. This shared task was organized as part of The Fourth Arabic Natural Language Processing Workshop, collocated with ACL 2019. The shared task includes two subtasks: the MADAR Travel Domain Dialect Identification subtask (Subtask 1) and the MADAR Twitter User Dialect Identification subtask (Subtask 2). This shared task is the first to target a large set of dialect labels at the city and country levels. The data for the shared task was created or collected under the Multi-Arabic Dialect Applications and Resources (MADAR) project. A total of 21 teams from 15 countries participated in the shared task.",
}
@inproceedings{priban-taylor-2019-zcu,
    title = "ZCU-NLP at MADAR 2019: Recognizing {A}rabic Dialects",
    author = "P{\v{r}}ib{\'a}{\v{n}}, Pavel  and
      Taylor, Stephen",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4623",
    doi = "10.18653/v1/W19-4623",
    pages = "208--213",
    abstract = "In this paper, we present our systems for the MADAR Shared Task: Arabic Fine-Grained Dialect Identification. The shared task consists of two subtasks. The goal of Subtask{--} 1 (S-1) is to detect an Arabic city dialect in a given text and the goal of Subtask{--}2 (S-2) is to predict the country of origin of a Twitter user by using tweets posted by the user. In S-1, our proposed systems are based on language modelling. We use language models to extract features that are later used as an input for other machine learning algorithms. We also experiment with recurrent neural networks (RNN), but these experiments showed that simpler machine learning algorithms are more successful. Our system achieves 0.658 macro F1-score and our rank is 6th out of 19 teams in S-1 and 7th in S-2 with 0.475 macro F1-score.",
}
@inproceedings{eltanbouly-etal-2019-simple,
    title = {Simple But Not Na{\"\i}ve: Fine-Grained {A}rabic Dialect Identification Using Only N-Grams},
    author = "Eltanbouly, Sohaila  and
      Bashendy, May  and
      Elsayed, Tamer",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4624",
    doi = "10.18653/v1/W19-4624",
    pages = "214--218",
    abstract = "This paper presents the participation of Qatar University team in MADAR shared task, which addresses the problem of sentence-level fine-grained Arabic Dialect Identification over 25 different Arabic dialects in addition to the Modern Standard Arabic. Arabic Dialect Identification is not a trivial task since different dialects share some features, e.g., utilizing the same character set and some vocabularies. We opted to adopt a very simple approach in terms of extracted features and classification models; we only utilize word and character n-grams as features, and Na ̈{\i}ve Bayes models as classifiers. Surprisingly, the simple approach achieved non-na ̈{\i}ve performance. The official results, reported on a held-out testing set, show that the dialect of a given sentence can be identified at an accuracy of 64.58{\%} by our best submitted run.",
}
@inproceedings{kchaou-etal-2019-lium,
    title = "{LIUM}-{MIRACL} Participation in the {MADAR} {A}rabic Dialect Identification Shared Task",
    author = "Kchaou, Sam{\'e}h  and
      Bougares, Fethi  and
      Hadrich-Belguith, Lamia",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4625",
    doi = "10.18653/v1/W19-4625",
    pages = "219--223",
    abstract = "This paper describes the joint participation of the LIUM and MIRACL Laboratories at the Arabic dialect identification challenge of the MADAR Shared Task (Bouamor et al., 2019) conducted during the Fourth Arabic Natu- ral Language Processing Workshop (WANLP 2019). We participated to the Travel Domain Dialect Identification subtask. We built several systems and explored different techniques in- cluding conventional machine learning meth- ods and deep learning algorithms. Deep learn- ing approaches did not perform well on this task. We experimented several classification systems and we were able to identify the di- alect of an input sentence with an F1-score of 65.41{\%} on the official test set using only the training data supplied by the shared task orga- nizers.",
}
@inproceedings{fares-etal-2019-arabic,
    title = "{A}rabic Dialect Identification with Deep Learning and Hybrid Frequency Based Features",
    author = "Fares, Youssef  and
      El-Zanaty, Zeyad  and
      Abdel-Salam, Kareem  and
      Ezzeldin, Muhammed  and
      Mohamed, Aliaa  and
      El-Awaad, Karim  and
      Torki, Marwan",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4626",
    doi = "10.18653/v1/W19-4626",
    pages = "224--228",
    abstract = "Studies on Dialectical Arabic are growing more important by the day as it becomes the primary written and spoken form of Arabic on- line in informal settings. Among the impor- tant problems that should be explored is that of dialect identification. This paper reports different techniques that can be applied to- wards such goal and reports their performance on the Multi Arabic Dialect Applications and Resources (MADAR) Arabic Dialect Corpora. Our results show that improving on traditional systems using frequency based features and non deep learning classifiers is a challenging task. We propose different models based on different word and document representations. Our top model is able to achieve an F1 macro averaged score of 65.66 on MADAR{'}s small- scale parallel corpus of 25 dialects and Modern Standard Arabic (MSA).",
}
@inproceedings{ghoul-lejeune-2019-michael,
    title = "{MICHAEL}: Mining Character-level Patterns for {A}rabic Dialect Identification ({MADAR} Challenge)",
    author = {Ghoul, Dhaou  and
      Lejeune, Ga{\"e}l},
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4627",
    doi = "10.18653/v1/W19-4627",
    pages = "229--233",
    abstract = "We present MICHAEL, a simple lightweight method for automatic Arabic Dialect Identification on the MADAR travel domain Dialect Identification (DID). MICHAEL uses simple character-level features in order to perform a pre-processing free classification. More precisely, Character N-grams extracted from the original sentences are used to train a Multinomial Naive Bayes classifier. This system achieved an official score (accuracy) of 53.25{\%} with 1{\textless}=N{\textless}=3 but showed a much better result with character 4-grams (62.17{\%} accuracy).",
}
@inproceedings{mishra-mujadia-2019-arabic,
    title = "{A}rabic Dialect Identification for Travel and Twitter Text",
    author = "Mishra, Pruthwik  and
      Mujadia, Vandan",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4628",
    doi = "10.18653/v1/W19-4628",
    pages = "234--238",
    abstract = "This paper presents the results of the experiments done as a part of MADAR Shared Task in WANLP 2019 on Arabic Fine-Grained Dialect Identification. Dialect Identification is one of the prominent tasks in the field of Natural language processing where the subsequent language modules can be improved based on it. We explored the use of different features like char, word n-gram, language model probabilities, etc on different classifiers. Results show that these features help to improve dialect classification accuracy. Results also show that traditional machine learning classifier tends to perform better when compared to neural network models on this task in a low resource setting.",
}
@inproceedings{talafha-etal-2019-mawdoo3,
    title = "Mawdoo3 {AI} at {MADAR} Shared Task: {A}rabic Tweet Dialect Identification",
    author = "Talafha, Bashar  and
      Farhan, Wael  and
      Altakrouri, Ahmed  and
      Al-Natsheh, Hussein",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4629",
    doi = "10.18653/v1/W19-4629",
    pages = "239--243",
    abstract = "Arabic dialect identification is an inherently complex problem, as Arabic dialect taxonomy is convoluted and aims to dissect a continuous space rather than a discrete one. In this work, we present machine and deep learning approaches to predict 21 fine-grained dialects form a set of given tweets per user. We adopted numerous feature extraction methods most of which showed improvement in the final model, such as word embedding, Tf-idf, and other tweet features. Our results show that a simple LinearSVC can outperform any complex deep learning model given a set of curated features. With a relatively complex user voting mechanism, we were able to achieve a Macro-Averaged F1-score of 71.84{\%} on MADAR shared subtask-2. Our best submitted model ranked second out of all participating teams.",
}
@inproceedings{ragab-etal-2019-mawdoo3,
    title = "Mawdoo3 {AI} at {MADAR} Shared Task: {A}rabic Fine-Grained Dialect Identification with Ensemble Learning",
    author = "Ragab, Ahmad  and
      Seelawi, Haitham  and
      Samir, Mostafa  and
      Mattar, Abdelrahman  and
      Al-Bataineh, Hesham  and
      Zaghloul, Mohammad  and
      Mustafa, Ahmad  and
      Talafha, Bashar  and
      Freihat, Abed Alhakim  and
      Al-Natsheh, Hussein",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4630",
    doi = "10.18653/v1/W19-4630",
    pages = "244--248",
    abstract = "In this paper we discuss several models we used to classify 25 city-level Arabic dialects in addition to Modern Standard Arabic (MSA) as part of MADAR shared task (sub-task 1). We propose an ensemble model of a group of experimentally designed best performing classifiers on a various set of features. Our system achieves an accuracy of 69.3{\%} macro F1-score with an improvement of 1.4{\%} accuracy from the baseline model on the DEV dataset. Our best run submitted model ranked as third out of 19 participating teams on the TEST dataset with only 0.12{\%} macro F1-score behind the top ranked system.",
}
@inproceedings{de-francony-etal-2019-hierarchical,
    title = "Hierarchical Deep Learning for {A}rabic Dialect Identification",
    author = "de Francony, Gael  and
      Guichard, Victor  and
      Joshi, Praveen  and
      Afli, Haithem  and
      Bouchekif, Abdessalam",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4631",
    doi = "10.18653/v1/W19-4631",
    pages = "249--253",
    abstract = "In this paper, we present two approaches for Arabic Fine-Grained Dialect Identification. The first approach is based on Recurrent Neural Networks (BLSTM, BGRU) using hierarchical classification. The main idea is to separate the classification process for a sentence from a given text in two stages. We start with a higher level of classification (8 classes) and then the finer-grained classification (26 classes). The second approach is given by a voting system based on Naive Bayes and Random Forest. Our system achieves an F1 score of 63.02 {\%} on the subtask evaluation dataset.",
}
@inproceedings{abu-kwaik-saad-2019-arbdialectid,
    title = "{A}rb{D}ialect{ID} at {MADAR} Shared Task 1: Language Modelling and Ensemble Learning for Fine Grained {A}rabic Dialect Identification",
    author = "Abu Kwaik, Kathrein  and
      Saad, Motaz",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4632",
    doi = "10.18653/v1/W19-4632",
    pages = "254--258",
    abstract = "In this paper, we present a Dialect Identification system (ArbDialectID) that competed at Task 1 of the MADAR shared task, MADARTravel Domain Dialect Identification. We build a course and a fine-grained identification model to predict the label (corresponding to a dialect of Arabic) of a given text. We build two language models by extracting features at two levels (words and characters). We firstly build a coarse identification model to classify each sentence into one out of six dialects, then use this label as a feature for the fine-grained model that classifies the sentence among 26 dialects from different Arab cities, after that we apply ensemble voting classifier on both sub-systems. Our system ranked 1st that achieving an f-score of 67.32{\%}. Both the models and our feature engineering tools are made available to the research community.",
}
@inproceedings{meftouh-etal-2019-smart,
    title = "The {SM}ar{T} Classifier for {A}rabic Fine-Grained Dialect Identification",
    author = "Meftouh, Karima  and
      Abidi, Karima  and
      Harrat, Salima  and
      Smaili, Kamel",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4633",
    doi = "10.18653/v1/W19-4633",
    pages = "259--263",
    abstract = "This paper describes the approach adopted by the SMarT research group to build a dialect identification system in the framework of the Madar shared task on Arabic fine-grained dialect identification. We experimented several approaches, but we finally decided to use a Multinomial Naive Bayes classifier based on word and character ngrams in addition to the language model probabilities. We achieved a score of 67.73{\%} in terms of Macro accuracy and a macro-averaged F1-score of 67.31{\%}",
}
@inproceedings{lippincott-etal-2019-jhu,
    title = "{JHU} System Description for the {MADAR} {A}rabic Dialect Identification Shared Task",
    author = "Lippincott, Tom  and
      Shapiro, Pamela  and
      Duh, Kevin  and
      McNamee, Paul",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4634",
    doi = "10.18653/v1/W19-4634",
    pages = "264--268",
    abstract = "Our submission to the MADAR shared task on Arabic dialect identification employed a language modeling technique called Prediction by Partial Matching, an ensemble of neural architectures, and sources of additional data for training word embeddings and auxiliary language models. We found several of these techniques provided small boosts in performance, though a simple character-level language model was a strong baseline, and a lower-order LM achieved best performance on Subtask 2. Interestingly, word embeddings provided no consistent benefit, and ensembling struggled to outperform the best component submodel. This suggests the variety of architectures are learning redundant information, and future work may focus on encouraging decorrelated learning.",
}
@inproceedings{abbas-etal-2019-st,
    title = "{ST} {MADAR} 2019 Shared Task: {A}rabic Fine-Grained Dialect Identification",
    author = "Abbas, Mourad  and
      Lichouri, Mohamed  and
      Freihat, Abed Alhakim",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4635",
    doi = "10.18653/v1/W19-4635",
    pages = "269--273",
    abstract = "This paper describes the solution that we propose on MADAR 2019 Arabic Fine-Grained Dialect Identification task. The proposed solution utilized a set of classifiers that we trained on character and word features. These classifiers are: Support Vector Machines (SVM), Bernoulli Naive Bayes (BNB), Multinomial Naive Bayes (MNB), Logistic Regression (LR), Stochastic Gradient Descent (SGD), Passive Aggressive(PA) and Perceptron (PC). The system achieved competitive results, with a performance of 62.87 {\%} and 62.12 {\%} for both development and test sets.",
}
@inproceedings{elaraby-zahran-2019-character,
    title = "A Character Level Convolutional {B}i{LSTM} for {A}rabic Dialect Identification",
    author = "Elaraby, Mohamed  and
      Zahran, Ahmed",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4636",
    doi = "10.18653/v1/W19-4636",
    pages = "274--278",
    abstract = "In this paper, we describe CU-RAISA teamcontribution to the 2019Madar shared task2, which focused on Twitter User fine-grained dialect identification.Among par-ticipating teams, our system ranked the4th(with 61.54{\%}) F1-Macro measure.Our sys-tem is trained using a character level convo-lutional bidirectional long-short-term memorynetwork trained on 2k users{'} data. We showthat training on concatenated user tweets asinput is further superior to training on usertweets separately and assign user{'}s label on themode of user{'}s tweets{'} predictions.",
}
@inproceedings{zhang-abdul-mageed-2019-army,
    title = "No Army, No Navy: {BERT} Semi-Supervised Learning of {A}rabic Dialects",
    author = "Zhang, Chiyu  and
      Abdul-Mageed, Muhammad",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4637",
    doi = "10.18653/v1/W19-4637",
    pages = "279--284",
    abstract = "We present our deep leaning system submitted to MADAR shared task 2 focused on twitter user dialect identification. We develop tweet-level identification models based on GRUs and BERT in supervised and semi-supervised set-tings. We then introduce a simple, yet effective, method of porting tweet-level labels at the level of users. Our system ranks top 1 in the competition, with 71.70{\%} macro F1 score and 77.40{\%} accuracy.",
}
@inproceedings{talafha-etal-2019-team,
    title = "Team {JUST} at the {MADAR} Shared Task on {A}rabic Fine-Grained Dialect Identification",
    author = "Talafha, Bashar  and
      Fadel, Ali  and
      Al-Ayyoub, Mahmoud  and
      Jararweh, Yaser  and
      AL-Smadi, Mohammad  and
      Juola, Patrick",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4638",
    doi = "10.18653/v1/W19-4638",
    pages = "285--289",
    abstract = "In this paper, we describe our team{'}s effort on the MADAR Shared Task on Arabic Fine-Grained Dialect Identification. The task requires building a system capable of differentiating between 25 different Arabic dialects in addition to MSA. Our approach is simple. After preprocessing the data, we use Data Augmentation (DA) to enlarge the training data six times. We then build a language model and extract n-gram word-level and character-level TF-IDF features and feed them into an MNB classifier. Despite its simplicity, the resulting model performs really well producing the 4th highest F-measure and region-level accuracy and the 5th highest precision, recall, city-level accuracy and country-level accuracy among the participating teams.",
}
@inproceedings{samih-etal-2019-qc,
    title = "{QC}-{GO} Submission for {MADAR} Shared Task: {A}rabic Fine-Grained Dialect Identification",
    author = "Samih, Younes  and
      Mubarak, Hamdy  and
      Abdelali, Ahmed  and
      Attia, Mohammed  and
      Eldesouki, Mohamed  and
      Darwish, Kareem",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4639",
    doi = "10.18653/v1/W19-4639",
    pages = "290--294",
    abstract = "This paper describes the QC-GO team submission to the MADAR Shared Task Subtask 1 (travel domain dialect identification) and Subtask 2 (Twitter user location identification). In our participation in both subtasks, we explored a number of approaches and system combinations to obtain the best performance for both tasks. These include deep neural nets and heuristics. Since individual approaches suffer from various shortcomings, the combination of different approaches was able to fill some of these gaps. Our system achieves F1-Scores of 66.1{\%} and 67.0{\%} on the development sets for Subtasks 1 and 2 respectively.",
}
