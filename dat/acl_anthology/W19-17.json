[{"id":"W19-1701","title":"A user study to compare two conversational assistants designed for people with hearing impairments","authors":["Virkkunen, Anja","Lukkarila, Juri","Palom{\\\"a}ki, Kalle","Kurimo, Mikko"],"emails":["anja.virkkunen@aalto.fi","juri.lukkarila@aalto.fi","kalle.palomaki@aalto.fi","mikko.kurimo@aalto.fi"],"author_id":["anja-virkkunen","juri-lukkarila","kalle-palomaki","mikko-kurimo"],"abstract":"Participating in conversations can be difficult for people with hearing loss, especially in acoustically challenging environments. We studied the preferences the hearing impaired have for a personal conversation assistant based on automatic speech recognition (ASR) technology. We created two prototypes which were evaluated by hearing impaired test users. This paper qualitatively compares the two based on the feedback obtained from the tests. The first prototype was a proof-of-concept system running real-time ASR on a laptop. The second prototype was developed for a mobile device with the recognizer running on a separate server. In the mobile device, augmented reality (AR) was used to help the hearing impaired observe gestures and lip movements of the speaker simultaneously with the transcriptions. Several testers found the systems useful enough to use in their daily lives, with majority preferring the mobile AR version. The biggest concern of the testers was the accuracy of the transcriptions and the lack of speaker identification.","pages":"1--8","doi":"10.18653\/v1\/W19-1701","url":"https:\/\/www.aclweb.org\/anthology\/W19-1701","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies"},{"id":"W19-1702","title":"Modeling Acoustic-Prosodic Cues for Word Importance Prediction in Spoken Dialogues","authors":["Kafle, Sushant","Alm, Cissi Ovesdotter","Huenerfauth, Matt"],"emails":["sxk5664@rit.edu","coagla@rit.edu","matt.huenerfauth@rit.edu"],"author_id":["sushant-kafle","cissi-ovesdotter-alm","matt-huenerfauth"],"abstract":"Prosodic cues in conversational speech aid listeners in discerning a message. We investigate whether acoustic cues in spoken dialogue can be used to identify the importance of individual words to the meaning of a conversation turn. Individuals who are Deaf and Hard of Hearing often rely on real-time captions in live meetings. Word error rate, a traditional metric for evaluating automatic speech recognition (ASR), fails to capture that some words are more important for a system to transcribe correctly than others. We present and evaluate neural architectures that use acoustic features for 3-class word importance prediction. Our model performs competitively against state-of-the-art text-based word-importance prediction models, and it demonstrates particular benefits when operating on imperfect ASR output.","pages":"9--16","doi":"10.18653\/v1\/W19-1702","url":"https:\/\/www.aclweb.org\/anthology\/W19-1702","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies"},{"id":"W19-1703","title":"Permanent Magnetic Articulograph ({PMA}) vs Electromagnetic Articulograph ({EMA}) in Articulation-to-Speech Synthesis for Silent Speech Interface","authors":["Cao, Beiming","Sebkhi, Nordine","Mau, Ted","Inan, Omer T.","Wang, Jun"],"emails":["","","","",""],"author_id":["beiming-cao","nordine-sebkhi","ted-mau","omer-t-inan","jun-wang"],"abstract":"Silent speech interfaces (SSIs) are devices that enable speech communication when audible speech is unavailable. Articulation-to-speech (ATS) synthesis is a software design in SSI that directly converts articulatory movement information into audible speech signals. Permanent magnetic articulograph (PMA) is a wireless articulator motion tracking technology that is similar to commercial, wired Electromagnetic Articulograph (EMA). PMA has shown great potential for practical SSI applications, because it is wireless. The ATS performance of PMA, however, is unknown when compared with current EMA. In this study, we compared the performance of ATS using a PMA we recently developed and a commercially available EMA (NDI Wave system). Datasets with same stimuli and size that were collected from tongue tip were used in the comparison. The experimental results indicated the performance of PMA was close to, although not as equally good as that of EMA. Furthermore, in PMA, converting the raw magnetic signals to positional signals did not significantly affect the performance of ATS, which support the future direction in PMA-based ATS can be focused on the use of positional signals to maximize the benefit of spatial analysis.","pages":"17--23","doi":"10.18653\/v1\/W19-1703","url":"https:\/\/www.aclweb.org\/anthology\/W19-1703","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies"},{"id":"W19-1704","title":"Speech-based Estimation of Bulbar Regression in Amyotrophic Lateral Sclerosis","authors":["Wisler, Alan","Teplansky, Kristin","Green, Jordan","Yunusova, Yana","Campbell, Thomas","Heitzman, Daragh","Wang, Jun"],"emails":["","","","","","",""],"author_id":["alan-wisler","kristin-teplansky","jordan-r-green","yana-yunusova","thomas-campbell","daragh-heitzman","jun-wang"],"abstract":"Amyotrophic Lateral Sclerosis (ALS) is a progressive neurological disease that leads to degeneration of motor neurons and, as a result, inhibits the ability of the brain to control muscle movements. Monitoring the progression of ALS is of fundamental importance due to the wide variability in disease outlook that exists across patients. This progression is typically tracked using the ALS functional rating scale - revised (ALSFRS-R), which is the current clinical assessment of a patient{'}s level of functional impairment including speech and other motor tasks. In this paper, we investigated automatic estimation of the ALSFRS-R bulbar subscore from acoustic and articulatory movement samples. Experimental results demonstrated the AFSFRS-R bulbar subscore can be predicted from speech samples, which has clinical implication for automatic monitoring of the disease progression of ALS using speech information.","pages":"24--31","doi":"10.18653\/v1\/W19-1704","url":"https:\/\/www.aclweb.org\/anthology\/W19-1704","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies"},{"id":"W19-1705","title":"A Blissymbolics Translation System","authors":["Sohail, Usman","Traum, David"],"emails":["msohail@usc.edu","traum@ict.usc.edu"],"author_id":["usman-sohail","david-traum"],"abstract":"Blissymbolics (Bliss) is a pictographic writing system that is used by people with communication disorders. Bliss attempts to create a writing system that makes words easier to distinguish by using pictographic symbols that encapsulate meaning rather than sound, as the English alphabet does for example. Users of Bliss rely on human interpreters to use Bliss. We created a translation system from Bliss to natural English with the hopes of decreasing the reliance on human interpreters by the Bliss community. We first discuss the basic rules of Blissymbolics. Then we point out some of the challenges associated with developing computer assisted tools for Blissymbolics. Next we talk about our ongoing work in developing a translation system, including current limitations, and future work. We conclude with a set of examples showing the current capabilities of our translation system.","pages":"32--36","doi":"10.18653\/v1\/W19-1705","url":"https:\/\/www.aclweb.org\/anthology\/W19-1705","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies"},{"id":"W19-1706","title":"Investigating Speech Recognition for Improving Predictive {AAC}","authors":["Adhikary, Jiban","Watling, Robbie","Fletcher, Crystal","Stanage, Alex","Vertanen, Keith"],"emails":["jiban@mtu.edu","rwatling@mtu.edu","tafletch@mtu.edu","amstanag@mtu.edu","vertanen@mtu.edu"],"author_id":["jiban-adhikary","robbie-watling","crystal-fletcher","alex-stanage","keith-vertanen"],"abstract":"Making good letter or word predictions can help accelerate the communication of users of high-tech AAC devices. This is particularly important for real-time person-to-person conversations. We investigate whether per forming speech recognition on the speaking-side of a conversation can improve language model based predictions. We compare the accuracy of three plausible microphone deployment options and the accuracy of two commercial speech recognition engines (Google and IBM Watson). We found that despite recognition word error rates of 7-16{\\%}, our ensemble of N-gram and recurrent neural network language models made predictions nearly as good as when they used the reference transcripts.","pages":"37--43","doi":"10.18653\/v1\/W19-1706","url":"https:\/\/www.aclweb.org\/anthology\/W19-1706","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies"},{"id":"W19-1707","title":"Noisy Neural Language Modeling for Typing Prediction in {BCI} Communication","authors":["Dong, Rui","Smith, David","Dudy, Shiran","Bedrick, Steven"],"emails":["dongrui@ccs.neu.edu","dasmith@ccs.neu.edu","dudy@ohsu.edu","bedricks@ohsu.edu"],"author_id":["rui-dong","david-a-smith","shiran-dudy","steven-bedrick"],"abstract":"Language models have broad adoption in predictive typing tasks. When the typing history contains numerous errors, as in open-vocabulary predictive typing with brain-computer interface (BCI) systems, we observe significant performance degradation in both n-gram and recurrent neural network language models trained on clean text. In evaluations of ranking character predictions, training recurrent LMs on noisy text makes them much more robust to noisy histories, even when the error model is misspecified. We also propose an effective strategy for combining evidence from multiple ambiguous histories of BCI electroencephalogram measurements.","pages":"44--51","doi":"10.18653\/v1\/W19-1707","url":"https:\/\/www.aclweb.org\/anthology\/W19-1707","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies"}]