[{"id":"W17-5001","title":"Question Difficulty {--} How to Estimate Without Norming, How to Use for Automated Grading","authors":["Pad{\\'o}, Ulrike"],"emails":["ulrike.pado@hft-stuttgart.de"],"author_id":["ulrike-pado"],"abstract":"Question difficulty estimates guide test creation, but are too costly for small-scale testing. We empirically verify that Bloom{'}s Taxonomy, a standard tool for difficulty estimation during question creation, reliably predicts question difficulty observed after testing in a short-answer corpus. We also find that difficulty is mirrored in the amount of variation in student answers, which can be computed before grading. We show that question difficulty and its approximations are useful for \\textit{automated grading}, allowing us to identify the optimal feature set for grading each question even in an unseen-question setting.","pages":"1--10","doi":"10.18653\/v1\/W17-5001","url":"https:\/\/www.aclweb.org\/anthology\/W17-5001","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5002","title":"Combining {CNN}s and Pattern Matching for Question Interpretation in a Virtual Patient Dialogue System","authors":["Jin, Lifeng","White, Michael","Jaffe, Evan","Zimmerman, Laura","Danforth, Douglas"],"emails":["jin@ling.osu.edu","white@ling.osu.edu","jaffe@ling.osu.edu","zimmerman.411@osu.edu","doug.danforth@osumc.edu"],"author_id":["lifeng-jin","michael-white","evan-jaffe","laura-zimmerman","douglas-danforth"],"abstract":"For medical students, virtual patient dialogue systems can provide useful training opportunities without the cost of employing actors to portray standardized patients. This work utilizes word- and character-based convolutional neural networks (CNNs) for question identification in a virtual patient dialogue system, outperforming a strong word- and character-based logistic regression baseline. While the CNNs perform well given sufficient training data, the best system performance is ultimately achieved by combining CNNs with a hand-crafted pattern matching system that is robust to label sparsity, providing a 10{\\%} boost in system accuracy and an error reduction of 47{\\%} as compared to the pattern-matching system alone.","pages":"11--21","doi":"10.18653\/v1\/W17-5002","url":"https:\/\/www.aclweb.org\/anthology\/W17-5002","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5003","title":"Continuous fluency tracking and the challenges of varying text complexity","authors":["Beigman Klebanov, Beata","Loukina, Anastassia","Sabatini, John","O{'}Reilly, Tenaha"],"emails":["bbeigmanklebanov@ets.org","aloukina@ets.org","jsabatini@ets.org","toreilly@ets.org"],"author_id":["beata-beigman-klebanov","anastassia-loukina","john-sabatini","tenaha-oreilly"],"abstract":"This paper is a preliminary report on using text complexity measurement in the service of a new educational application. We describe a reading intervention where a child takes turns reading a book aloud with a virtual reading partner. Our ultimate goal is to provide meaningful feedback to the parent or the teacher by continuously tracking the child{'}s improvement in reading fluency. We show that this would not be a simple endeavor, due to an intricate relationship between text complexity from the point of view of comprehension and reading rate.","pages":"22--32","doi":"10.18653\/v1\/W17-5003","url":"https:\/\/www.aclweb.org\/anthology\/W17-5003","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5004","title":"Auxiliary Objectives for Neural Error Detection Models","authors":["Rei, Marek","Yannakoudakis, Helen"],"emails":["marek.rei@cl.cam.ac.uk","helen.yannakoudakis@cl.cam.ac.uk"],"author_id":["marek-rei","helen-yannakoudakis"],"abstract":"We investigate the utility of different auxiliary objectives and training strategies within a neural sequence labeling approach to error detection in learner writing. Auxiliary costs provide the model with additional linguistic information, allowing it to learn general-purpose compositional features that can then be exploited for other objectives. Our experiments show that a joint learning approach trained with parallel labels on in-domain data improves performance over the previous best error detection system. While the resulting model has the same number of parameters, the additional objectives allow it to be optimised more efficiently and achieve better performance.","pages":"33--43","doi":"10.18653\/v1\/W17-5004","url":"https:\/\/www.aclweb.org\/anthology\/W17-5004","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5005","title":"Linked Data for Language-Learning Applications","authors":["Loughnane, Robyn","McCurdy, Kate","Kolb, Peter","Selent, Stefan"],"emails":["rloughnane@babbel.com","kmccurdy@babbel.com","pkolb@babbel.com","sselent@babbel.com"],"author_id":["robyn-loughnane","kate-mccurdy","peter-kolb","stefan-selent"],"abstract":"The use of linked data within language-learning applications is an open research question. A research prototype is presented that applies linked-data principles to store linguistic annotation generated from language-learning content using a variety of NLP tools. The result is a database that links learning content, linguistic annotation and open-source resources, on top of which a diverse range of tools for language-learning applications can be built.","pages":"44--51","doi":"10.18653\/v1\/W17-5005","url":"https:\/\/www.aclweb.org\/anthology\/W17-5005","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5006","title":"Predicting Specificity in Classroom Discussion","authors":["Lugini, Luca","Litman, Diane"],"emails":["lucalugini@cs.pitt.edu","litman@cs.pitt.edu"],"author_id":["luca-lugini","diane-litman"],"abstract":"High quality classroom discussion is important to student development, enhancing abilities to express claims, reason about other students{'} claims, and retain information for longer periods of time. Previous small-scale studies have shown that one indicator of classroom discussion quality is specificity. In this paper we tackle the problem of predicting specificity for classroom discussions. We propose several methods and feature sets capable of outperforming the state of the art in specificity prediction. Additionally, we provide a set of meaningful, interpretable features that can be used to analyze classroom discussions at a pedagogical level.","pages":"52--61","doi":"10.18653\/v1\/W17-5006","url":"https:\/\/www.aclweb.org\/anthology\/W17-5006","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5007","title":"A Report on the 2017 Native Language Identification Shared Task","authors":["Malmasi, Shervin","Evanini, Keelan","Cahill, Aoife","Tetreault, Joel","Pugh, Robert","Hamill, Christopher","Napolitano, Diane","Qian, Yao"],"emails":["shervin.malmasi@mq.edu.au","kevanini@ets.org","acahill@ets.org","joel.tetreault@grammarly.com","","","",""],"author_id":["shervin-malmasi","keelan-evanini","aoife-cahill","joel-tetreault","robert-pugh","christopher-hamill","diane-napolitano","yao-qian"],"abstract":"Native Language Identification (NLI) is the task of automatically identifying the native language (L1) of an individual based on their language production in a learned language. It is typically framed as a classification task where the set of L1s is known a priori. Two previous shared tasks on NLI have been organized where the aim was to identify the L1 of learners of English based on essays (2013) and spoken responses (2016) they provided during a standardized assessment of academic English proficiency. The 2017 shared task combines the inputs from the two prior tasks for the first time. There are three tracks: NLI on the essay only, NLI on the spoken response only (based on a transcription of the response and i-vector acoustic features), and NLI using both responses. We believe this makes for a more interesting shared task while building on the methods and results from the previous two shared tasks. In this paper, we report the results of the shared task. A total of 19 teams competed across the three different sub-tasks. The fusion track showed that combining the written and spoken responses provides a large boost in prediction accuracy. Multiple classifier systems (e.g. ensembles and meta-classifiers) were the most effective in all tasks, with most based on traditional classifiers (e.g. SVMs) with lexical\/syntactic features.","pages":"62--75","doi":"10.18653\/v1\/W17-5007","url":"https:\/\/www.aclweb.org\/anthology\/W17-5007","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5008","title":"Evaluation of Automatically Generated Pronoun Reference Questions","authors":["Satria, Arief Yudha","Tokunaga, Takenobu"],"emails":["satria.a.aa@m.titech.ac.jp","take@c.titech.ac.jp"],"author_id":["arief-yudha-satria","takenobu-tokunaga"],"abstract":"This study provides a detailed analysis of evaluation of English pronoun reference questions which are created automatically by machine. Pronoun reference questions are multiple choice questions that ask test takers to choose an antecedent of a target pronoun in a reading passage from four options. The evaluation was performed from two perspectives: the perspective of English teachers and that of English learners. Item analysis suggests that machine-generated questions achieve comparable quality with human-made questions. Correlation analysis revealed a strong correlation between the scores of machine-generated questions and that of human-made questions.","pages":"76--85","doi":"10.18653\/v1\/W17-5008","url":"https:\/\/www.aclweb.org\/anthology\/W17-5008","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5009","title":"Predicting Audience{'}s Laughter During Presentations Using Convolutional Neural Network","authors":["Chen, Lei","Lee, Chong Min"],"emails":["hen@ets.org","ee001@ets.org"],"author_id":["lei-chen","chungmin-lee"],"abstract":"Public speakings play important roles in schools and work places and properly using humor contributes to effective presentations. For the purpose of automatically evaluating speakers{'} humor usage, we build a presentation corpus containing humorous utterances based on TED talks. Compared to previous data resources supporting humor recognition research, ours has several advantages, including (a) both positive and negative instances coming from a homogeneous data set, (b) containing a large number of speakers, and (c) being open. Focusing on using lexical cues for humor recognition, we systematically compare a newly emerging text classification method based on Convolutional Neural Networks (CNNs) with a well-established conventional method using linguistic knowledge. The advantages of the CNN method are both getting higher detection accuracies and being able to learn essential features automatically.","pages":"86--90","doi":"10.18653\/v1\/W17-5009","url":"https:\/\/www.aclweb.org\/anthology\/W17-5009","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5010","title":"Collecting fluency corrections for spoken learner {E}nglish","authors":["Caines, Andrew","Flint, Emma","Buttery, Paula"],"emails":["apc38@cam.ac.uk","emf40@cam.ac.uk","pjb48@cam.ac.uk"],"author_id":["andrew-caines","emma-flint","paula-buttery"],"abstract":"We present crowdsourced collection of error annotations for transcriptions of spoken learner English. Our emphasis in data collection is on fluency corrections, a more complete correction than has traditionally been aimed for in grammatical error correction research (GEC). Fluency corrections require improvements to the text, taking discourse and utterance level semantics into account: the result is a more naturalistic, holistic version of the original. We propose that this shifted emphasis be reflected in a new name for the task: {`}holistic error correction{'} (HEC). We analyse crowdworker behaviour in HEC and conclude that the method is useful with certain amendments for future work.","pages":"91--100","doi":"10.18653\/v1\/W17-5010","url":"https:\/\/www.aclweb.org\/anthology\/W17-5010","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5011","title":"Exploring Relationships Between Writing {\\&} Broader Outcomes With Automated Writing Evaluation","authors":["Burstein, Jill","McCaffrey, Dan","Beigman Klebanov, Beata","Ling, Guangming"],"emails":["jburstein@ets.org","dmccaffrey@ets.org","bbeigmanklebanov@ets.org","gling@ets.org"],"author_id":["jill-burstein","dan-mccaffrey","beata-beigman-klebanov","guangming-ling"],"abstract":"Writing is a challenge, especially for at-risk students who may lack the prerequisite writing skills required to persist in U.S. 4-year postsecondary (college) institutions. Educators teaching postsecondary courses requiring writing could benefit from a better understanding of writing achievement and its role in postsecondary success. In this paper, novel exploratory work examined how automated writing evaluation (AWE) can inform our understanding of the relationship between postsecondary writing skill and broader success outcomes. An exploratory study was conducted using test-taker essays from a standardized writing assessment of postsecondary student learning outcomes. Findings showed that for the essays, AWE features were found to be predictors of broader outcomes measures: college success and learning outcomes measures. Study findings illustrate AWE{'}s potential to support educational analytics {--} i.e., relationships between writing skill and broader outcomes {--} taking a step toward moving AWE beyond writing assessment and instructional use cases.","pages":"101--108","doi":"10.18653\/v1\/W17-5011","url":"https:\/\/www.aclweb.org\/anthology\/W17-5011","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5012","title":"An Investigation into the Pedagogical Features of Documents","authors":["Sheng, Emily","Natarajan, Prem","Gordon, Jonathan","Burns, Gully"],"emails":["ewsheng@isi.edu","pnataraj@isi.edu","jgordon@isi.edu","burns@isi.edu"],"author_id":["emily-sheng","prem-natarajan","jonathan-gordon","gully-burns"],"abstract":"Characterizing the content of a technical document in terms of its learning utility can be useful for applications related to education, such as generating reading lists from large collections of documents. We refer to this learning utility as the {``}pedagogical value{''} of the document to the learner. While pedagogical value is an important concept that has been studied extensively within the education domain, there has been little work exploring it from a computational, i.e., natural language processing (NLP), perspective. To allow a computational exploration of this concept, we introduce the notion of {``}pedagogical roles{''} of documents (e.g., Tutorial and Survey) as an intermediary component for the study of pedagogical value. Given the lack of available corpora for our exploration, we create the first annotated corpus of pedagogical roles and use it to test baseline techniques for automatic prediction of such roles.","pages":"109--120","doi":"10.18653\/v1\/W17-5012","url":"https:\/\/www.aclweb.org\/anthology\/W17-5012","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5013","title":"Combining Multiple Corpora for Readability Assessment for People with Cognitive Disabilities","authors":["Yaneva, Victoria","Or{\\u{a}}san, Constantin","Evans, Richard","Rohanian, Omid"],"emails":["v.yaneva@wlv.ac.uk","omid.rohanian@wlv.ac.uk","r.j.evans@wlv.ac.uk","c.orasan@wlv.ac.uk"],"author_id":["victoria-yaneva","constantin-orasan","richard-evans","omid-rohanian"],"abstract":"Given the lack of large user-evaluated corpora in disability-related NLP research (e.g. text simplification or readability assessment for people with cognitive disabilities), the question of choosing suitable training data for NLP models is not straightforward. The use of large generic corpora may be problematic because such data may not reflect the needs of the target population. The use of the available user-evaluated corpora may be problematic because these datasets are not large enough to be used as training data. In this paper we explore a third approach, in which a large generic corpus is combined with a smaller population-specific corpus to train a classifier which is evaluated using two sets of unseen user-evaluated data. One of these sets, the ASD Comprehension corpus, is developed for the purposes of this study and made freely available. We explore the effects of the size and type of the training data used on the performance of the classifiers, and the effects of the type of the unseen test datasets on the classification performance.","pages":"121--132","doi":"10.18653\/v1\/W17-5013","url":"https:\/\/www.aclweb.org\/anthology\/W17-5013","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5014","title":"Automatic Extraction of High-Quality Example Sentences for Word Learning Using a Determinantal Point Process","authors":["Tolmachev, Arseny","Kurohashi, Sadao"],"emails":["arseny@nlp.ist.i.kyoto-u.ac.jp","kuro@i.kyoto-u.ac.jp"],"author_id":["arseny-tolmachev","sadao-kurohashi"],"abstract":"Flashcard systems are effective tools for learning words but have their limitations in teaching word usage. To overcome this problem, we propose a novel flashcard system that shows a new example sentence on each repetition. This extension requires high-quality example sentences, automatically extracted from a huge corpus. To do this, we use a Determinantal Point Process which scales well to large data and allows to naturally represent sentence similarity and quality as features. Our human evaluation experiment on Japanese language indicates that the proposed method successfully extracted high-quality example sentences.","pages":"133--142","doi":"10.18653\/v1\/W17-5014","url":"https:\/\/www.aclweb.org\/anthology\/W17-5014","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5015","title":"Distractor Generation for {C}hinese Fill-in-the-blank Items","authors":["Jiang, Shu","Lee, John"],"emails":["jshmjs45@gmail.com","jsylee@cityu.edu.hk"],"author_id":["shu-jiang","john-lee"],"abstract":"This paper reports the first study on automatic generation of distractors for fill-in-the-blank items for learning Chinese vocabulary. We investigate the quality of distractors generated by a number of criteria, including part-of-speech, difficulty level, spelling, word co-occurrence and semantic similarity. Evaluations show that a semantic similarity measure, based on the word2vec model, yields distractors that are significantly more plausible than those generated by baseline methods.","pages":"143--148","doi":"10.18653\/v1\/W17-5015","url":"https:\/\/www.aclweb.org\/anthology\/W17-5015","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5016","title":"An Error-Oriented Approach to Word Embedding Pre-Training","authors":["Farag, Youmna","Rei, Marek","Briscoe, Ted"],"emails":["youmna.farag@cl.cam.ac.uk","marek.rei@cl.cam.ac.uk","ted.briscoe@cl.cam.ac.uk"],"author_id":["youmna-farag","marek-rei","ted-briscoe"],"abstract":"We propose a novel word embedding pre-training approach that exploits writing errors in learners{'} scripts. We compare our method to previous models that tune the embeddings based on script scores and the discrimination between correct and corrupt word contexts in addition to the generic commonly-used embeddings pre-trained on large corpora. The comparison is achieved by using the aforementioned models to bootstrap a neural network that learns to predict a holistic score for scripts. Furthermore, we investigate augmenting our model with error corrections and monitor the impact on performance. Our results show that our error-oriented approach outperforms other comparable ones which is further demonstrated when training on more data. Additionally, extending the model with corrections provides further performance gains when data sparsity is an issue.","pages":"149--158","doi":"10.18653\/v1\/W17-5016","url":"https:\/\/www.aclweb.org\/anthology\/W17-5016","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5017","title":"Investigating neural architectures for short answer scoring","authors":["Riordan, Brian","Horbach, Andrea","Cahill, Aoife","Zesch, Torsten","Lee, Chong Min"],"emails":["","","","",""],"author_id":["brian-riordan","andrea-horbach","aoife-cahill","torsten-zesch","chungmin-lee"],"abstract":"Neural approaches to automated essay scoring have recently shown state-of-the-art performance. The automated essay scoring task typically involves a broad notion of writing quality that encompasses content, grammar, organization, and conventions. This differs from the short answer content scoring task, which focuses on content accuracy. The inputs to neural essay scoring models {--} ngrams and embeddings {--} are arguably well-suited to evaluate content in short answer scoring tasks. We investigate how several basic neural approaches similar to those used for automated essay scoring perform on short answer scoring. We show that neural architectures can outperform a strong non-neural baseline, but performance and optimal parameter settings vary across the more diverse types of prompts typical of short answer scoring.","pages":"159--168","doi":"10.18653\/v1\/W17-5017","url":"https:\/\/www.aclweb.org\/anthology\/W17-5017","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5018","title":"Human and Automated {CEFR}-based Grading of Short Answers","authors":["Tack, Ana{\\\"\\i}s","Fran{\\c{c}}ois, Thomas","Roekhaut, Sophie","Fairon, C{\\'e}drick"],"emails":["anais.tack@uclouvain.be","thomas.francois@uclouvain.be","sroekhaut@altissia.com","cedrick.fairon@uclouvain.be"],"author_id":["anais-tack","thomas-francois","sophie-roekhaut","cedrick-fairon"],"abstract":"This paper is concerned with the task of automatically assessing the written proficiency level of non-native (L2) learners of English. Drawing on previous research on automated L2 writing assessment following the Common European Framework of Reference for Languages (CEFR), we investigate the possibilities and difficulties of deriving the CEFR level from short answers to open-ended questions, which has not yet been subjected to numerous studies up to date. The object of our study is twofold: to examine the intricacy involved with both human and automated CEFR-based grading of short answers. On the one hand, we describe the compilation of a learner corpus of short answers graded with CEFR levels by three certified Cambridge examiners. We mainly observe that, although the shortness of the answers is reported as undermining a clear-cut evaluation, the length of the answer does not necessarily correlate with inter-examiner disagreement. On the other hand, we explore the development of a soft-voting system for the automated CEFR-based grading of short answers and draw tentative conclusions about its use in a computer-assisted testing (CAT) setting.","pages":"169--179","doi":"10.18653\/v1\/W17-5018","url":"https:\/\/www.aclweb.org\/anthology\/W17-5018","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5019","title":"{GEC} into the future: Where are we going and how do we get there?","authors":["Sakaguchi, Keisuke","Napoles, Courtney","Tetreault, Joel"],"emails":["keisuke@cs.jhu.edu","napoles@cs.jhu.edu","joel.tetreault@grammarly.com"],"author_id":["keisuke-sakaguchi","courtney-napoles","joel-tetreault"],"abstract":"The field of grammatical error correction (GEC) has made tremendous bounds in the last ten years, but new questions and obstacles are revealing themselves. In this position paper, we discuss the issues that need to be addressed and provide recommendations for the field to continue to make progress, and propose a new shared task. We invite suggestions and critiques from the audience to make the new shared task a community-driven venture.","pages":"180--187","doi":"10.18653\/v1\/W17-5019","url":"https:\/\/www.aclweb.org\/anthology\/W17-5019","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5020","title":"Detecting Off-topic Responses to Visual Prompts","authors":["Rei, Marek"],"emails":["marek.rei@cl.cam.ac.uk"],"author_id":["marek-rei"],"abstract":"Automated methods for essay scoring have made great progress in recent years, achieving accuracies very close to human annotators. However, a known weakness of such automated scorers is not taking into account the semantic relevance of the submitted text. While there is existing work on detecting answer relevance given a textual prompt, very little previous research has been done to incorporate visual writing prompts. We propose a neural architecture and several extensions for detecting off-topic responses to visual prompts and evaluate it on a dataset of texts written by language learners.","pages":"188--197","doi":"10.18653\/v1\/W17-5020","url":"https:\/\/www.aclweb.org\/anthology\/W17-5020","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5021","title":"Combining Textual and Speech Features in the {NLI} Task Using State-of-the-Art Machine Learning Techniques","authors":["Ircing, Pavel","{\\v{S}}vec, Jan","Zaj{\\'\\i}c, Zbyn{\\v{e}}k","Hladk{\\'a}, Barbora","Holub, Martin"],"emails":["ircing@kky.zcu.cz","honzas@kky.zcu.cz","zzajic@kky.zcu.cz","hladka@ufal.mff.cuni.cz","holub@ufal.mff.cuni.cz"],"author_id":["pavel-ircing","jan-svec","zbynek-zajic","barbora-hladka","martin-holub"],"abstract":"We summarize the involvement of our CEMI team in the {''}NLI Shared Task 2017{''}, which deals with both textual and speech input data. We submitted the results achieved by using three different system architectures; each of them combines multiple supervised learning models trained on various feature sets. As expected, better results are achieved with the systems that use both the textual data and the spoken responses. Combining the input data of two different modalities led to a rather dramatic improvement in classification performance. Our best performing method is based on a set of feed-forward neural networks whose hidden-layer outputs are combined together using a softmax layer. We achieved a macro-averaged F1 score of 0.9257 on the evaluation (unseen) test set and our team placed first in the main task together with other three teams.","pages":"198--209","doi":"10.18653\/v1\/W17-5021","url":"https:\/\/www.aclweb.org\/anthology\/W17-5021","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5022","title":"Native Language Identification Using a Mixture of Character and Word N-grams","authors":["Mohammadi, Elham","Veisi, Hadi","Amini, Hessam"],"emails":["elham.mohammadi@ut.ac.ir","h.veisi@ut.ac.ir","hessam.amini@ut.ac.ir"],"author_id":["elham-mohammadi","hadi-veisi","hessam-amini"],"abstract":"Native language identification (NLI) is the task of determining an author{'}s native language, based on a piece of his\/her writing in a second language. In recent years, NLI has received much attention due to its challenging nature and its applications in language pedagogy and forensic linguistics. We participated in the NLI2017 shared task under the name UT-DSP. In our effort to implement a method for native language identification, we made use of a fusion of character and word N-grams, and achieved an optimal F1-Score of 77.64{\\%}, using both essay and speech transcription datasets.","pages":"210--216","doi":"10.18653\/v1\/W17-5022","url":"https:\/\/www.aclweb.org\/anthology\/W17-5022","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5023","title":"Ensemble Methods for Native Language Identification","authors":["Chan, Sophia","Honari Jahromi, Maryam","Benetti, Benjamin","Lakhani, Aazim","Fyshe, Alona"],"emails":["schan1@uvic.ca","mhonari@uvic.ca","bbenetti@uvic.ca","aazimlakhani@uvic.ca","afyshe@uvic.ca"],"author_id":["sophia-chan","maryam-honari-jahromi","benjamin-benetti","aazim-lakhani","alona-fyshe"],"abstract":"Our team{---}Uvic-NLP{---}explored and evaluated a variety of lexical features for Native Language Identification (NLI) within the framework of ensemble methods. Using a subset of the highest performing features, we train Support Vector Machines (SVM) and Fully Connected Neural Networks (FCNN) as base classifiers, and test different methods for combining their outputs. Restricting our scope to the closed essay track in the NLI Shared Task 2017, we find that our best SVM ensemble achieves an F1 score of 0.8730 on the test set.","pages":"217--223","doi":"10.18653\/v1\/W17-5023","url":"https:\/\/www.aclweb.org\/anthology\/W17-5023","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5024","title":"Can string kernels pass the test of time in Native Language Identification?","authors":["Ionescu, Radu Tudor","Popescu, Marius"],"emails":["raducu.ionescu@gmail.com","popescunmarius@gmail.com"],"author_id":["radu-tudor-ionescu","marius-popescu"],"abstract":"We describe a machine learning approach for the 2017 shared task on Native Language Identification (NLI). The proposed approach combines several kernels using multiple kernel learning. While most of our kernels are based on character p-grams (also known as n-grams) extracted from essays or speech transcripts, we also use a kernel based on i-vectors, a low-dimensional representation of audio recordings, provided by the shared task organizers. For the learning stage, we choose Kernel Discriminant Analysis (KDA) over Kernel Ridge Regression (KRR), because the former classifier obtains better results than the latter one on the development set. In our previous work, we have used a similar machine learning approach to achieve state-of-the-art NLI results. The goal of this paper is to demonstrate that our shallow and simple approach based on string kernels (with minor improvements) can pass the test of time and reach state-of-the-art performance in the 2017 NLI shared task, despite the recent advances in natural language processing. We participated in all three tracks, in which the competitors were allowed to use only the essays (essay track), only the speech transcripts (speech track), or both (fusion track). Using only the data provided by the organizers for training our models, we have reached a macro F1 score of 86.95{\\%} in the closed essay track, a macro F1 score of 87.55{\\%} in the closed speech track, and a macro F1 score of 93.19{\\%} in the closed fusion track. With these scores, our team (UnibucKernel) ranked in the first group of teams in all three tracks, while attaining the best scores in the speech and the fusion tracks.","pages":"224--234","doi":"10.18653\/v1\/W17-5024","url":"https:\/\/www.aclweb.org\/anthology\/W17-5024","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5025","title":"Neural Networks and Spelling Features for Native Language Identification","authors":["Bjerva, Johannes","Grigonyt{\\.e}, Gintar{\\.e}","{\\\"O}stling, Robert","Plank, Barbara"],"emails":["j.bjerva@rug.nl","gintare@ling.su.se","robert@ling.su.se","b.plank@rug.nl"],"author_id":["johannes-bjerva","gintare-grigonyte","robert-ostling","barbara-plank"],"abstract":"We present the RUG-SU team{'}s submission at the Native Language Identification Shared Task 2017. We combine several approaches into an ensemble, based on spelling error features, a simple neural network using word representations, a deep residual network using word and character features, and a system based on a recurrent neural network. Our best system is an ensemble of neural networks, reaching an F1 score of 0.8323. Although our system is not the highest ranking one, we do outperform the baseline by far.","pages":"235--239","doi":"10.18653\/v1\/W17-5025","url":"https:\/\/www.aclweb.org\/anthology\/W17-5025","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5026","title":"A study of N-gram and Embedding Representations for Native Language Identification","authors":["Vajjala, Sowmya","Banerjee, Sagnik"],"emails":["sowmya@iastate.edu","sagnik@iastate.edu"],"author_id":["sowmya-vajjala","sagnik-banerjee"],"abstract":"We report on our experiments with N-gram and embedding based feature representations for Native Language Identification (NLI) as a part of the NLI Shared Task 2017 (team name: NLI-ISU). Our best performing system on the test set for written essays had a macro F1 of 0.8264 and was based on word uni, bi and trigram features. We explored n-grams covering word, character, POS and word-POS mixed representations for this task. For embedding based feature representations, we employed both word and document embeddings. We had a relatively poor performance with all embedding representations compared to n-grams, which could be because of the fact that embeddings capture semantic similarities whereas L1 differences are more stylistic in nature.","pages":"240--248","doi":"10.18653\/v1\/W17-5026","url":"https:\/\/www.aclweb.org\/anthology\/W17-5026","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5027","title":"A Shallow Neural Network for Native Language Identification with Character N-grams","authors":["Sari, Yunita","Rifqi Fatchurrahman, Muhammad","Dwiastuti, Meisyarah"],"emails":["y.sari@sheffield.ac.uk","muh.rifqi.fatchurrahman@gmail.com","meisyarah.dwiastuti@gmail.com"],"author_id":["yunita-sari","muhammad-rifqi-fatchurrahman","meisyarah-dwiastuti"],"abstract":"This paper describes the systems submitted by GadjahMada team to the Native Language Identification (NLI) Shared Task 2017. Our models used a continuous representation of character n-grams which are learned jointly with feed-forward neural network classifier. Character n-grams have been proved to be effective for style-based identification tasks including NLI. Results on the test set demonstrate that the proposed model performs very well on essay and fusion tracks by obtaining more than 0.8 on both F-macro score and accuracy.","pages":"249--254","doi":"10.18653\/v1\/W17-5027","url":"https:\/\/www.aclweb.org\/anthology\/W17-5027","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5028","title":"Fewer features perform well at Native Language Identification task","authors":["Rama, Taraka","{\\c{C}}{\\\"o}ltekin, {\\c{C}}a{\\u{g}}r{\\i}"],"emails":["taraka-rama.kasicheyanula@uni-tuebingen.de","ccoltekin@sfs.uni-tuebingen.de"],"author_id":["taraka-rama","cagri-coltekin"],"abstract":"This paper describes our results at the NLI shared task 2017. We participated in essays, speech, and fusion task that uses text, speech, and i-vectors for the task of identifying the native language of the given input. In the essay track, a linear SVM system using word bigrams and character 7-grams performed the best. In the speech track, an LDA classifier based only on i-vectors performed better than a combination system using text features from speech transcriptions and i-vectors. In the fusion task, we experimented with systems that used combination of i-vectors with higher order n-grams features, combination of i-vectors with word unigrams, a mean probability ensemble, and a stacked ensemble system. Our finding is that word unigrams in combination with i-vectors achieve higher score than systems trained with larger number of $n$-gram features. Our best-performing systems achieved F1-scores of 87.16{\\%}, 83.33{\\%} and 91.75{\\%} on the essay track, the speech track and the fusion track respectively.","pages":"255--260","doi":"10.18653\/v1\/W17-5028","url":"https:\/\/www.aclweb.org\/anthology\/W17-5028","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5029","title":"Structured Generation of Technical Reading Lists","authors":["Gordon, Jonathan","Aguilar, Stephen","Sheng, Emily","Burns, Gully"],"emails":["jgordon@isi.edu","aguilars@usc.edu","ewsheng@isi.edu","burns@isi.edu"],"author_id":["jonathan-gordon","stephen-aguilar","emily-sheng","gully-burns"],"abstract":"Learners need to find suitable documents to read and prioritize them in an appropriate order. We present a method of automatically generating reading lists, selecting documents based on their pedagogical value to the learner and ordering them using the structure of concepts in the domain. Resulting reading lists related to computational linguistics were evaluated by advanced learners and judged to be near the quality of those generated by domain experts. We provide an open-source implementation of our method to enable future work on reading list generation.","pages":"261--270","doi":"10.18653\/v1\/W17-5029","url":"https:\/\/www.aclweb.org\/anthology\/W17-5029","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5030","title":"Effects of Lexical Properties on Viewing Time per Word in Autistic and Neurotypical Readers","authors":["{\\v{S}}tajner, Sanja","Yaneva, Victoria","Mitkov, Ruslan","Ponzetto, Simone Paolo"],"emails":["sanja@informatik.uni-mannheim.de","v.yaneva@wlv.ac.uk","r.mitkov@wlv.ac.uk","simone@informatik.uni-mannheim.de"],"author_id":["sanja-stajner","victoria-yaneva","ruslan-mitkov","simone-paolo-ponzetto"],"abstract":"Eye tracking studies from the past few decades have shaped the way we think of word complexity and cognitive load: words that are long, rare and ambiguous are more difficult to read. However, online processing techniques have been scarcely applied to investigating the reading difficulties of people with autism and what vocabulary is challenging for them. We present parallel gaze data obtained from adult readers with autism and a control group of neurotypical readers and show that the former required higher cognitive effort to comprehend the texts as evidenced by three gaze-based measures. We divide all words into four classes based on their viewing times for both groups and investigate the relationship between longer viewing times and word length, word frequency, and four cognitively-based measures (word concreteness, familiarity, age of acquisition and imagability).","pages":"271--281","doi":"10.18653\/v1\/W17-5030","url":"https:\/\/www.aclweb.org\/anthology\/W17-5030","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5031","title":"Transparent text quality assessment with convolutional neural networks","authors":["{\\\"O}stling, Robert","Grigonyte, Gintare"],"emails":["robert@ling.su.se","gintare@ling.su.se"],"author_id":["robert-ostling","gintare-grigonyte"],"abstract":"We present a very simple model for text quality assessment based on a deep convolutional neural network, where the only supervision required is one corpus of user-generated text of varying quality, and one contrasting text corpus of consistently high quality. Our model is able to provide local quality assessments in different parts of a text, which allows visual feedback about where potentially problematic parts of the text are located, as well as a way to evaluate which textual features are captured by our model. We evaluate our method on two corpora: a large corpus of manually graded student essays and a longitudinal corpus of language learner written production, and find that the text quality metric learned by our model is a fairly strong predictor of both essay grade and learner proficiency level.","pages":"282--286","doi":"10.18653\/v1\/W17-5031","url":"https:\/\/www.aclweb.org\/anthology\/W17-5031","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5032","title":"Artificial Error Generation with Machine Translation and Syntactic Patterns","authors":["Rei, Marek","Felice, Mariano","Yuan, Zheng","Briscoe, Ted"],"emails":["marek.rei@cl.cam.ac.uk","mariano.felice@cl.cam.ac.uk","zheng.yuan@cl.cam.ac.uk","ted.briscoe@cl.cam.ac.uk"],"author_id":["marek-rei","mariano-felice","zheng-yuan","ted-briscoe"],"abstract":"Shortage of available training data is holding back progress in the area of automated error detection. This paper investigates two alternative methods for artificially generating writing errors, in order to create additional resources. We propose treating error generation as a machine translation task, where grammatically correct text is translated to contain errors. In addition, we explore a system for extracting textual patterns from an annotated corpus, which can then be used to insert errors into grammatically correct sentences. Our experiments show that the inclusion of artificially generated errors significantly improves error detection accuracy on both FCE and CoNLL 2014 datasets.","pages":"287--292","doi":"10.18653\/v1\/W17-5032","url":"https:\/\/www.aclweb.org\/anthology\/W17-5032","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5033","title":"Modelling semantic acquisition in second language learning","authors":["Kochmar, Ekaterina","Shutova, Ekaterina"],"emails":["ek358@cam.ac.uk","es407@cam.ac.uk"],"author_id":["ekaterina-kochmar","ekaterina-shutova"],"abstract":"Using methods of statistical analysis, we investigate how semantic knowledge is acquired in English as a second language and evaluate the pace of development across a number of predicate types and content word combinations, as well as across the levels of language proficiency and native languages. Our exploratory study helps identify the most problematic areas for language learners with different backgrounds and at different stages of learning.","pages":"293--302","doi":"10.18653\/v1\/W17-5033","url":"https:\/\/www.aclweb.org\/anthology\/W17-5033","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5034","title":"Multiple Choice Question Generation Utilizing An Ontology","authors":["Stasaski, Katherine","Hearst, Marti A."],"emails":["stasaski@berkeley.edu","hearst@berkeley.edu"],"author_id":["katherine-stasaski","marti-a-hearst"],"abstract":"Ontologies provide a structured representation of concepts and the relationships which connect them. This work investigates how a pre-existing educational Biology ontology can be used to generate useful practice questions for students by using the connectivity structure in a novel way. It also introduces a novel way to generate multiple-choice distractors from the ontology, and compares this to a baseline of using embedding representations of nodes. An assessment by an experienced science teacher shows a significant advantage over a baseline when using the ontology for distractor generation. A subsequent study with three science teachers on the results of a modified question generation algorithm finds significant improvements. An in-depth analysis of the teachers{'} comments yields useful insights for any researcher working on automated question generation for educational applications.","pages":"303--312","doi":"10.18653\/v1\/W17-5034","url":"https:\/\/www.aclweb.org\/anthology\/W17-5034","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5035","title":"Simplifying metaphorical language for young readers: A corpus study on news text","authors":["Wolska, Magdalena","Clausen, Yulia"],"emails":["magdalena.wolska@uni-tuebingen.de","yulia.clausen@gmail.com"],"author_id":["magdalena-wolska","yulia-clausen"],"abstract":"The paper presents first results of an ongoing project on text simplification focusing on linguistic metaphors. Based on an analysis of a parallel corpus of news text professionally simplified for different grade levels, we identify six types of simplification choices falling into two broad categories: preserving metaphors or dropping them. An annotation study on almost 300 source sentences with metaphors (grade level 12) and their simplified counterparts (grade 4) is conducted. The results show that most metaphors are preserved and when they are dropped, the semantic content tends to be preserved rather than dropped, however, it is reworded without metaphorical language. In general, some of the expected tendencies in complexity reduction, measured with psycholinguistic variables linked to metaphor comprehension, are observed, suggesting good prospect for machine learning-based metaphor simplification.","pages":"313--318","doi":"10.18653\/v1\/W17-5035","url":"https:\/\/www.aclweb.org\/anthology\/W17-5035","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5036","title":"Language Based Mapping of Science Assessment Items to Skills","authors":["Nadeem, Farah","Ostendorf, Mari"],"emails":["farahn@uw.edu","ostendor@uw.edu"],"author_id":["farah-nadeem","mari-ostendorf"],"abstract":"Knowledge of the association between assessment questions and the skills required to solve them is necessary for analysis of student learning. This association, often represented as a Q-matrix, is either hand-labeled by domain experts or learned as latent variables given a large student response data set. As a means of automating the match to formal standards, this paper uses neural text classification methods, leveraging the language in the standards documents to identify online text for a proxy training task. Experiments involve identifying the topic and crosscutting concepts of middle school science questions leveraging multi-task training. Results show that it is possible to automatically build a Q-matrix without student response data and using a modest number of hand-labeled questions.","pages":"319--326","doi":"10.18653\/v1\/W17-5036","url":"https:\/\/www.aclweb.org\/anthology\/W17-5036","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5037","title":"Connecting the Dots: Towards Human-Level Grammatical Error Correction","authors":["Chollampatt, Shamil","Ng, Hwee Tou"],"emails":["shamil@u.nus.edu","nght@comp.nus.edu.sg"],"author_id":["shamil-chollampatt","hwee-tou-ng"],"abstract":"We build a grammatical error correction (GEC) system primarily based on the state-of-the-art statistical machine translation (SMT) approach, using task-specific features and tuning, and further enhance it with the modeling power of neural network joint models. The SMT-based system is weak in generalizing beyond patterns seen during training and lacks granularity below the word level. To address this issue, we incorporate a character-level SMT component targeting the misspelled words that the original SMT-based system fails to correct. Our final system achieves 53.14{\\%} F 0.5 score on the benchmark CoNLL-2014 test set, an improvement of 3.62{\\%} F 0.5 over the best previous published score.","pages":"327--333","doi":"10.18653\/v1\/W17-5037","url":"https:\/\/www.aclweb.org\/anthology\/W17-5037","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5038","title":"Question Generation for Language Learning: From ensuring texts are read to supporting learning","authors":["Chinkina, Maria","Meurers, Detmar"],"emails":["maria.chinkina@uni-tuebingen.de","detmar.meurers@uni-tuebingen.de"],"author_id":["maria-chinkina","detmar-meurers"],"abstract":"In Foreign Language Teaching and Learning (FLTL), questions are systematically used to assess the learner{'}s understanding of a text. Computational linguistic approaches have been developed to generate such questions automatically given a text (e.g., Heilman, 2011). In this paper, we want to broaden the perspective on the different functions questions can play in FLTL and discuss how automatic question generation can support the different uses. Complementing the focus on meaning and comprehension, we want to highlight the fact that questions can also be used to make learners notice form aspects of the linguistic system and their interpretation. Automatically generating questions that target linguistic forms and grammatical categories in a text in essence supports incidental focus-on-form (Loewen, 2005) in a meaning-focused reading task. We discuss two types of questions serving this purpose, how they can be generated automatically; and we report on a crowd-sourcing evaluation comparing automatically generated to manually written questions targeting particle verbs, a challenging linguistic form for learners of English.","pages":"334--344","doi":"10.18653\/v1\/W17-5038","url":"https:\/\/www.aclweb.org\/anthology\/W17-5038","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5039","title":"Systematically Adapting Machine Translation for Grammatical Error Correction","authors":["Napoles, Courtney","Callison-Burch, Chris"],"emails":["napoles@cs.jhu.edu","ccb@cis.upenn.edu"],"author_id":["courtney-napoles","chris-callison-burch"],"abstract":"n this work we adapt machine translation (MT) to grammatical error correction, identifying how components of the statistical MT pipeline can be modified for this task and analyzing how each modification impacts system performance. We evaluate the contribution of each of these components with standard evaluation metrics and automatically characterize the morphological and lexical transformations made in system output. Our model rivals the current state of the art using a fraction of the training data.","pages":"345--356","doi":"10.18653\/v1\/W17-5039","url":"https:\/\/www.aclweb.org\/anthology\/W17-5039","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5040","title":"Fine-grained essay scoring of a complex writing task for native speakers","authors":["Horbach, Andrea","Scholten-Akoun, Dirk","Ding, Yuning","Zesch, Torsten"],"emails":["andrea.horbach@uni-due.de","dirk.scholten@uni-due.de","yuning.ding@stud.uni-due.de","torsten.zesch@uni-due.de"],"author_id":["andrea-horbach","dirk-scholten-akoun","yuning-ding","torsten-zesch"],"abstract":"Automatic essay scoring is nowadays successfully used even in high-stakes tests, but this is mainly limited to holistic scoring of learner essays. We present a new dataset of essays written by highly proficient German native speakers that is scored using a fine-grained rubric with the goal to provide detailed feedback. Our experiments with two state-of-the-art scoring systems (a neural and a SVM-based one) show a large drop in performance compared to existing datasets. This demonstrates the need for such datasets that allow to guide research on more elaborate essay scoring methods.","pages":"357--366","doi":"10.18653\/v1\/W17-5040","url":"https:\/\/www.aclweb.org\/anthology\/W17-5040","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5041","title":"Exploring Optimal Voting in Native Language Identification","authors":["Goutte, Cyril","L{\\'e}ger, Serge"],"emails":["outte@gmail.com","eger@nrc.ca"],"author_id":["cyril-goutte","serge-leger"],"abstract":"We describe the submissions entered by the National Research Council Canada in the NLI-2017 evaluation. We mainly explored the use of voting, and various ways to optimize the choice and number of voting systems. We also explored the use of features that rely on no linguistic preprocessing. Long ngrams of characters obtained from raw text turned out to yield the best performance on all textual input (written essays and speech transcripts). Voting ensembles turned out to produce small performance gains, with little difference between the various optimization strategies we tried. Our top systems achieved accuracies of 87{\\%} on the essay track, 84{\\%} on the speech track, and close to 92{\\%} by combining essays, speech and i-vectors in the fusion track.","pages":"367--373","doi":"10.18653\/v1\/W17-5041","url":"https:\/\/www.aclweb.org\/anthology\/W17-5041","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5042","title":"{CIC}-{FBK} Approach to Native Language Identification","authors":["Markov, Ilia","Chen, Lingzhen","Strapparava, Carlo","Sidorov, Grigori"],"emails":["imarkov@nlp.cic.ipn.mx","lzchen.cs@gmail.com","strappa@fbk.eu","sidorov@cic.ipn.mx"],"author_id":["ilia-markov","lingzhen-chen","carlo-strapparava","grigori-sidorov"],"abstract":"We present the CIC-FBK system, which took part in the Native Language Identification (NLI) Shared Task 2017. Our approach combines features commonly used in previous NLI research, i.e., word n-grams, lemma n-grams, part-of-speech n-grams, and function words, with recently introduced character n-grams from misspelled words, and features that are novel in this task, such as typed character n-grams, and syntactic n-grams of words and of syntactic relation tags. We use log-entropy weighting scheme and perform classification using the Support Vector Machines (SVM) algorithm. Our system achieved 0.8808 macro-averaged F1-score and shared the 1st rank in the NLI Shared Task 2017 scoring.","pages":"374--381","doi":"10.18653\/v1\/W17-5042","url":"https:\/\/www.aclweb.org\/anthology\/W17-5042","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5043","title":"The Power of Character N-grams in Native Language Identification","authors":["Kulmizev, Artur","Blankers, Bo","Bjerva, Johannes","Nissim, Malvina","van Noord, Gertjan","Plank, Barbara","Wieling, Martijn"],"emails":["a.kulmizev@student.rug.nl","b.blankers@student.rug.nl","j.bjerva@rug.nl","m.nissim@rug.nl","g.j.m.van.noord@rug.nl","b.plank@rug.nl","m.b.wieling@rug.nl"],"author_id":["artur-kulmizev","bo-blankers","johannes-bjerva","malvina-nissim","gertjan-van-noord","barbara-plank","martijn-wieling"],"abstract":"In this paper, we explore the performance of a linear SVM trained on language independent character features for the NLI Shared Task 2017. Our basic system (GRONINGEN) achieves the best performance (87.56 F1-score) on the evaluation set using only 1-9 character n-grams as features. We compare this against several ensemble and meta-classifiers in order to examine how the linear system fares when combined with other, especially non-linear classifiers. Special emphasis is placed on the topic bias that exists by virtue of the assessment essay prompt distribution.","pages":"382--389","doi":"10.18653\/v1\/W17-5043","url":"https:\/\/www.aclweb.org\/anthology\/W17-5043","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5044","title":"Classifier Stacking for Native Language Identification","authors":["Li, Wen","Zou, Liang"],"emails":["wl9@indiana.edu","liazou@indiana.edu"],"author_id":["wen-li","liang-zou"],"abstract":"This paper reports our contribution (team WLZ) to the NLI Shared Task 2017 (essay track). We first extract lexical and syntactic features from the essays, perform feature weighting and selection, and train linear support vector machine (SVM) classifiers each on an individual feature type. The output of base classifiers, as probabilities for each class, are then fed into a multilayer perceptron to predict the native language of the author. We also report the performance of each feature type, as well as the best features of a type. Our system achieves an accuracy of 86.55{\\%}, which is among the best performing systems of this shared task.","pages":"390--397","doi":"10.18653\/v1\/W17-5044","url":"https:\/\/www.aclweb.org\/anthology\/W17-5044","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5045","title":"Native Language Identification on Text and Speech","authors":["Zampieri, Marcos","Ciobanu, Alina Maria","Dinu, Liviu P."],"emails":["marcos.zampieri@uni-koeln.de","",""],"author_id":["marcos-zampieri","alina-maria-ciobanu","liviu-p-dinu"],"abstract":"This paper presents an ensemble system combining the output of multiple SVM classifiers to native language identification (NLI). The system was submitted to the NLI Shared Task 2017 fusion track which featured students essays and spoken responses in form of audio transcriptions and iVectors by non-native English speakers of eleven native languages. Our system competed in the challenge under the team name ZCD and was based on an ensemble of SVM classifiers trained on character n-grams achieving 83.58{\\%} accuracy and ranking 3rd in the shared task.","pages":"398--404","doi":"10.18653\/v1\/W17-5045","url":"https:\/\/www.aclweb.org\/anthology\/W17-5045","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5046","title":"Native Language Identification using Phonetic Algorithms","authors":["Smiley, Charese","K{\\\"u}bler, Sandra"],"emails":["chsmiley@indiana.edu","skuebler@indiana.edu"],"author_id":["charese-smiley","sandra-kubler"],"abstract":"In this paper, we discuss the results of the IUCL system in the NLI Shared Task 2017. For our system, we explore a variety of phonetic algorithms to generate features for Native Language Identification. These features are contrasted with one of the most successful type of features in NLI, character n-grams. We find that although phonetic features do not perform as well as character n-grams alone, they do increase overall F1 score when used together with character n-grams.","pages":"405--412","doi":"10.18653\/v1\/W17-5046","url":"https:\/\/www.aclweb.org\/anthology\/W17-5046","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5047","title":"A deep-learning based native-language classification by using a latent semantic analysis for the {NLI} Shared Task 2017","authors":["Oh, Yoo Rhee","Jeon, Hyung-Bae","Song, Hwa Jeon","Lee, Yun-Kyung","Park, Jeon-Gue","Lee, Yun-Keun"],"emails":["yroh@etri.re.kr","hbjeon@etri.re.kr","songhj@etri.re.kr","yunklee@etri.re.kr","jgp@etri.re.kr","yklee@etri.re.kr"],"author_id":["yoo-rhee-oh","hyung-bae-jeon","hwa-jeon-song","yun-kyung-lee","jeon-gue-park","yun-keun-lee"],"abstract":"This paper proposes a deep-learning based native-language identification (NLI) using a latent semantic analysis (LSA) as a participant (ETRI-SLP) of the NLI Shared Task 2017 where the NLI Shared Task 2017 aims to detect the native language of an essay or speech response of a standardized assessment of English proficiency for academic purposes. To this end, we use the six unit forms of a text data such as character 4\/5\/6-grams and word 1\/2\/3-grams. For each unit form of text data, we convert it into a count-based vector, extract a 2000-rank LSA feature, and perform a linear discriminant analysis (LDA) based dimension reduction. From the count-based vector or the LSA-LDA feature, we also obtain the output prediction values of a support vector machine (SVM) based classifier, the output prediction values of a deep neural network (DNN) based classifier, and the bottleneck values of a DNN based classifier. In order to incorporate the various kinds of text-based features and a speech-based i-vector feature, we design two DNN based ensemble classifiers for late fusion and early fusion, respectively. From the NLI experiments, the F1 (macro) scores are obtained as 0.8601, 0.8664, and 0.9220 for the essay track, the speech track, and the fusion track, respectively. The proposed method has comparable performance to the top-ranked teams for the speech and fusion tracks, although it has slightly lower performance for the essay track.","pages":"413--422","doi":"10.18653\/v1\/W17-5047","url":"https:\/\/www.aclweb.org\/anthology\/W17-5047","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5048","title":"Fusion of Simple Models for Native Language Identification","authors":["Kepler, Fabio","Astudillo, Ramon","Abad, Alberto"],"emails":["fabio@kepler.pro.br","ramon@astudillo.com","alberto.abad@inesc-id.pt"],"author_id":["fabio-kepler","ramon-astudillo","alberto-abad"],"abstract":"In this paper we describe the approaches we explored for the 2017 Native Language Identification shared task. We focused on simple word and sub-word units avoiding heavy use of hand-crafted features. Following recent trends, we explored linear and neural networks models to attempt to compensate for the lack of rich feature use. Initial efforts yielded f1-scores of 82.39{\\%} and 83.77{\\%} in the development and test sets of the fusion track, and were officially submitted to the task as team L2F. After the task was closed, we carried on further experiments and relied on a late fusion strategy for combining our simple proposed approaches with modifications of the baselines provided by the task. As expected, the i-vectors based sub-system dominates the performance of the system combinations, and results in the major contributor to our achieved scores. Our best combined system achieves 90.1{\\%} and 90.2{\\%} f1-score in the development and test sets of the fusion track, respectively.","pages":"423--429","doi":"10.18653\/v1\/W17-5048","url":"https:\/\/www.aclweb.org\/anthology\/W17-5048","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5049","title":"Stacked Sentence-Document Classifier Approach for Improving Native Language Identification","authors":["Cimino, Andrea","Dell{'}Orletta, Felice"],"emails":["andrea.cimino@ilc.cnr.it","felice.dell{'}orletta@ilc.cnr.it"],"author_id":["andrea-cimino","felice-dellorletta"],"abstract":"In this paper, we describe the approach of the ItaliaNLP Lab team to native language identification and discuss the results we submitted as participants to the essay track of NLI Shared Task 2017. We introduce for the first time a 2-stacked sentence-document architecture for native language identification that is able to exploit both local sentence information and a wide set of general-purpose features qualifying the lexical and grammatical structure of the whole document. When evaluated on the official test set, our sentence-document stacked architecture obtained the best result among all the participants of the essay track with an F1 score of 0.8818.","pages":"430--437","doi":"10.18653\/v1\/W17-5049","url":"https:\/\/www.aclweb.org\/anthology\/W17-5049","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5050","title":"Using Gaze to Predict Text Readability","authors":["Gonz{\\'a}lez-Gardu{\\~n}o, Ana Valeria","S{\\o}gaard, Anders"],"emails":["fcm220@alumni.ku.dk","soegaard@di.ku.dk"],"author_id":["ana-valeria-gonzalez-garduno","anders-sogaard"],"abstract":"We show that text readability prediction improves significantly from hard parameter sharing with models predicting first pass duration, total fixation duration and regression duration. Specifically, we induce multi-task Multilayer Perceptrons and Logistic Regression models over sentence representations that capture various aggregate statistics, from two different text readability corpora for English, as well as the Dundee eye-tracking corpus. Our approach leads to significant improvements over Single task learning and over previous systems. In addition, our improvements are consistent across train sample sizes, making our approach especially applicable to small datasets.","pages":"438--443","doi":"10.18653\/v1\/W17-5050","url":"https:\/\/www.aclweb.org\/anthology\/W17-5050","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5051","title":"Annotating Orthographic Target Hypotheses in a {G}erman {L}1 Learner Corpus","authors":["Laarmann-Quante, Ronja","Ortmann, Katrin","Ehlert, Anna","Vogel, Maurice","Dipper, Stefanie"],"emails":["laarmann-quante@linguistics.rub.de","katrin.ortmann@rub.de","anna.ehlert@rub.de","maurice.vogel@rub.de","dipper@linguistics.rub.de"],"author_id":["ronja-laarmann-quante","katrin-ortmann","anna-ehlert","maurice-vogel","stefanie-dipper"],"abstract":"NLP applications for learners often rely on annotated learner corpora. Thereby, it is important that the annotations are both meaningful for the task, and consistent and reliable. We present a new longitudinal L1 learner corpus for German (handwritten texts collected in grade 2{--}4), which is transcribed and annotated with a target hypothesis that strictly only corrects orthographic errors, and is thereby tailored to research and tool development for orthographic issues in primary school. While for most corpora, transcription and target hypothesis are not evaluated, we conducted a detailed inter-annotator agreement study for both tasks. Although we achieved high agreement, our discussion of cases of disagreement shows that even with detailed guidelines, annotators differ here and there for different reasons, which should also be considered when working with transcriptions and target hypotheses of other corpora, especially if no explicit guidelines for their construction are known.","pages":"444--456","doi":"10.18653\/v1\/W17-5051","url":"https:\/\/www.aclweb.org\/anthology\/W17-5051","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"},{"id":"W17-5052","title":"A Large Scale Quantitative Exploration of Modeling Strategies for Content Scoring","authors":["Madnani, Nitin","Loukina, Anastassia","Cahill, Aoife"],"emails":["nmadnani@ets.org","aloukina@ets.org","acahill@ets.org"],"author_id":["nitin-madnani","anastassia-loukina","aoife-cahill"],"abstract":"We explore various supervised learning strategies for automated scoring of content knowledge for a large corpus of 130 different content-based questions spanning four subject areas (Science, Math, English Language Arts, and Social Studies) and containing over 230,000 responses scored by human raters. Based on our analyses, we provide specific recommendations for content scoring. These are based on patterns observed across multiple questions and assessments and are, therefore, likely to generalize to other scenarios and prove useful to the community as automated content scoring becomes more popular in schools and classrooms.","pages":"457--467","doi":"10.18653\/v1\/W17-5052","url":"https:\/\/www.aclweb.org\/anthology\/W17-5052","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications"}]