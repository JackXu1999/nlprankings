[{"id":"W19-5301","title":"Findings of the 2019 Conference on Machine Translation ({WMT}19)","authors":["Barrault, Lo{\\\"\\i}c","Bojar, Ond{\\v{r}}ej","Costa-juss{\\`a}, Marta R.","Federmann, Christian","Fishel, Mark","Graham, Yvette","Haddow, Barry","Huck, Matthias","Koehn, Philipp","Malmasi, Shervin","Monz, Christof","M{\\\"u}ller, Mathias","Pal, Santanu","Post, Matt","Zampieri, Marcos"],"emails":["","","","","","","","","","","","","","",""],"author_id":["loic-barrault","ondrej-bojar","marta-r-costa-jussa","christian-federmann","mark-fishel","yvette-graham","barry-haddow","matthias-huck","philipp-koehn","shervin-malmasi","christof-monz","mathias-muller","santanu-pal","matt-post","marcos-zampieri"],"abstract":"This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2019. Participants were asked to build machine translation systems for any of 18 language pairs, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. The task was also opened up to additional test suites to probe specific aspects of translation.","pages":"1--61","doi":"10.18653\/v1\/W19-5301","url":"https:\/\/www.aclweb.org\/anthology\/W19-5301","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5302","title":"Results of the {WMT}19 Metrics Shared Task: Segment-Level and Strong {MT} Systems Pose Big Challenges","authors":["Ma, Qingsong","Wei, Johnny","Bojar, Ond{\\v{r}}ej","Graham, Yvette"],"emails":["qingsong.mqs@gmail.com","jwei@umass.edu","bojar@ufal.mff.cuni.cz","graham.yvette@gmail.com"],"author_id":["qingsong-ma","johnny-wei","ondrej-bojar","yvette-graham"],"abstract":"This paper presents the results of the WMT19 Metrics Shared Task. Participants were asked to score the outputs of the translations systems competing in the WMT19 News Translation Task with automatic metrics. 13 research groups submitted 24 metrics, 10 of which are reference-less {``}metrics{''} and constitute submissions to the joint task with WMT19 Quality Estimation Task, {``}QE as a Metric{''}. In addition, we computed 11 baseline metrics, with 8 commonly applied baselines (BLEU, SentBLEU, NIST, WER, PER, TER, CDER, and chrF) and 3 reimplementations (chrF+, sacreBLEU-BLEU, and sacreBLEU-chrF). Metrics were evaluated on the system level, how well a given metric correlates with the WMT19 official manual ranking, and segment level, how well the metric correlates with human judgements of segment quality. This year, we use direct assessment (DA) as our only form of manual evaluation.","pages":"62--90","doi":"10.18653\/v1\/W19-5302","url":"https:\/\/www.aclweb.org\/anthology\/W19-5302","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5303","title":"Findings of the First Shared Task on Machine Translation Robustness","authors":["Li, Xian","Michel, Paul","Anastasopoulos, Antonios","Belinkov, Yonatan","Durrani, Nadir","Firat, Orhan","Koehn, Philipp","Neubig, Graham","Pino, Juan","Sajjad, Hassan"],"emails":["","","","","","","","","",""],"author_id":["xian-li","paul-michel","antonios-anastasopoulos","yonatan-belinkov","nadir-durrani","orhan-firat","philipp-koehn","graham-neubig","juan-pino","hassan-sajjad"],"abstract":"We share the findings of the first shared task on improving robustness of Machine Translation (MT). The task provides a testbed representing challenges facing MT models deployed in the real world, and facilitates new approaches to improve models{'} robustness to noisy input and domain mismatch. We focus on two language pairs (English-French and English-Japanese), and the submitted systems are evaluated on a blind test set consisting of noisy comments on Reddit and professionally sourced translations. As a new task, we received 23 submissions by 11 participating teams from universities, companies, national labs, etc. All submitted systems achieved large improvements over baselines, with the best improvement having +22.33 BLEU. We evaluated submissions by both human judgment and automatic evaluation (BLEU), which shows high correlations (Pearson{'}s r = 0.94 and 0.95). Furthermore, we conducted a qualitative analysis of the submitted systems using compare-mt, which revealed their salient differences in handling challenges in this task. Such analysis provides additional insights when there is occasional disagreement between human judgment and BLEU, e.g. systems better at producing colloquial expressions received higher score from human judgment.","pages":"91--102","doi":"10.18653\/v1\/W19-5303","url":"https:\/\/www.aclweb.org\/anthology\/W19-5303","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5304","title":"The University of {E}dinburgh{'}s Submissions to the {WMT}19 News Translation Task","authors":["Bawden, Rachel","Bogoychev, Nikolay","Germann, Ulrich","Grundkiewicz, Roman","Kirefu, Faheem","Miceli Barone, Antonio Valerio","Birch, Alexandra"],"emails":["rachel.bawden@ed.ac.uk","","","","","",""],"author_id":["rachel-bawden","nikolay-bogoychev","ulrich-germann","roman-grundkiewicz","faheem-kirefu","antonio-valerio-miceli-barone","alexandra-birch"],"abstract":"The University of Edinburgh participated in the WMT19 Shared Task on News Translation in six language directions: English\u2194Gujarati, English\u2194Chinese, German\u2192English, and English\u2192Czech. For all translation directions, we created or used back-translations of monolingual data in the target language as additional synthetic training data. For English\u2194Gujarati, we also explored semi- supervised MT with cross-lingual language model pre-training, and translation pivoting through Hindi. For translation to and from Chinese, we investigated character-based tokenisation vs. sub-word segmentation of Chinese text. For German\u2192English, we studied the impact of vast amounts of back-translated training data on translation quality, gaining a few additional insights over Edunov et al. (2018). For English\u2192Czech, we compared different preprocessing and tokenisation regimes.","pages":"103--115","doi":"10.18653\/v1\/W19-5304","url":"https:\/\/www.aclweb.org\/anthology\/W19-5304","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5305","title":"{GTCOM} Neural Machine Translation Systems for {WMT}19","authors":["Bei, Chao","Zong, Hao","Yuan, Conghu","Liu, Qingming","Fan, Baoyong"],"emails":["beichao@gtcom.com.cn","zonghao@gtcom.com.cn","yuanconghu@gtcom.com.cn","liuqingming@gtcom.com.cn","fanbaoyong@gtcom.com.cn"],"author_id":["chao-bei","hao-zong","conghu-yuan","qingming-liu","baoyong-fan"],"abstract":"This paper describes the Global Tone Communication Co., Ltd.{'}s submission of the WMT19 shared news translation task. We participate in six directions: English to (Gujarati, Lithuanian and Finnish) and (Gujarati, Lithuanian and Finnish) to English. Further, we get the best BLEU scores in the directions of English to Gujarati and Lithuanian to English (28.2 and 36.3 respectively) among all the participants. The submitted systems mainly focus on back-translation, knowledge distillation and reranking to build a competitive model for this task. Also, we apply language model to filter monolingual data, back-translated data and parallel data. The techniques we apply for data filtering include filtering by rules, language models. Besides, We conduct several experiments to validate different knowledge distillation techniques and right-to-left (R2L) reranking.","pages":"116--121","doi":"10.18653\/v1\/W19-5305","url":"https:\/\/www.aclweb.org\/anthology\/W19-5305","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5306","title":"Machine Translation with parfda, {M}oses, kenlm, nplm, and {PRO}","authors":["Bi{\\c{c}}ici, Ergun"],"emails":["ergun.bicici@boun.edu.tr"],"author_id":["ergun-bicici"],"abstract":"We build parfda Moses statistical machine translation (SMT) models for most language pairs in the news translation task. We experiment with a hybrid approach using neural language models integrated into Moses. We obtain the constrained data statistics on the machine translation task, the coverage of the test sets, and the upper bounds on the translation results. We also contribute a new testsuite for the German-English language pair and a new automated key phrase extraction technique for the evaluation of the testsuite translations.","pages":"122--128","doi":"10.18653\/v1\/W19-5306","url":"https:\/\/www.aclweb.org\/anthology\/W19-5306","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5307","title":"{LIUM}{'}s Contributions to the {WMT}2019 News Translation Task: Data and Systems for {G}erman-{F}rench Language Pairs","authors":["Bougares, Fethi","Wottawa, Jane","Baillot, Anne","Barrault, Lo{\\\"\\i}c","Bardet, Adrien"],"emails":["fethi.bougares@univ-lemans.fr","jane.wottawa@univ-lemans.fr","anne.baillot@univ-lemans.fr","loic.barrault@univ-lemans.fr",""],"author_id":["fethi-bougares","jane-wottawa","anne-baillot","loic-barrault","adrien-bardet"],"abstract":"This paper describes the neural machine translation (NMT) systems of the LIUM Laboratory developed for the French\u2194German news translation task of the Fourth Conference onMachine Translation (WMT 2019). The chosen language pair is included for the first time in the WMT news translation task. We de-scribe how the training and the evaluation data was created. We also present our participation in the French\u2194German translation directions using self-attentional Transformer networks with small and big architectures.","pages":"129--133","doi":"10.18653\/v1\/W19-5307","url":"https:\/\/www.aclweb.org\/anthology\/W19-5307","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5308","title":"The University of {M}aryland{'}s {K}azakh-{E}nglish Neural Machine Translation System at {WMT}19","authors":["Briakou, Eleftheria","Carpuat, Marine"],"emails":["ebriakou@cs.umd.edu","marine@cs.umd.edu"],"author_id":["eleftheria-briakou","marine-carpuat"],"abstract":"This paper describes the University of Maryland{'}s submission to the WMT 2019 Kazakh-English news translation task. We study the impact of transfer learning from another low-resource but related language. We experiment with different ways of encoding lexical units to maximize lexical overlap between the two language pairs, as well as back-translation and ensembling. The submitted system improves over a Kazakh-only baseline by +5.45 BLEU on newstest2019.","pages":"134--140","doi":"10.18653\/v1\/W19-5308","url":"https:\/\/www.aclweb.org\/anthology\/W19-5308","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5309","title":"{DBMS}-{KU} Interpolation for {WMT}19 News Translation Task","authors":["Budiwati, Sari Dewi","Siagian, Al Hafiz Akbar Maulana","Fatyanosa, Tirana Noor","Aritsugi, Masayoshi"],"emails":["saridewi@st.cs.kumamoto-u.ac.jp","alha002@st.cs.kumamoto-u.ac.jp","fatyanosa@st.cs.kumamoto-u.ac.jp","aritsugi@cs.kumamoto-u.ac.jp"],"author_id":["sari-dewi-budiwati","al-hafiz-akbar-maulana-siagian","tirana-noor-fatyanosa","masayoshi-aritsugi"],"abstract":"This paper presents the participation of DBMS-KU Interpolation system in WMT19 shared task, namely, Kazakh-English language pair. We examine the use of interpolation method using a different language model order. Our Interpolation system combines a direct translation with Russian as a pivot language. We use 3-gram and 5-gram language model orders to perform the language translation in this work. To reduce noise in the pivot translation process, we prune the phrase table of source-pivot and pivot-target. Our experimental results show that our Interpolation system outperforms the Baseline in terms of BLEU-cased score by +0.5 and +0.1 points in Kazakh-English and English-Kazakh, respectively. In particular, using the 5-gram language model order in our system could obtain better BLEU-cased score than utilizing the 3-gram one. Interestingly, we found that by employing the Interpolation system could reduce the perplexity score of English-Kazakh when using 3-gram language model order.","pages":"141--146","doi":"10.18653\/v1\/W19-5309","url":"https:\/\/www.aclweb.org\/anthology\/W19-5309","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5310","title":"Lingua Custodia at {WMT}{'}19: Attempts to Control Terminology","authors":["Burlot, Franck"],"emails":["franck.burlot@linguacustodia.com"],"author_id":["franck-burlot"],"abstract":"This paper describes Lingua Custodia{'}s submission to the WMT{'}19 news shared task for German-to-French on the topic of the EU elections. We report experiments on the adaptation of the terminology of a machine translation system to a specific topic, aimed at providing more accurate translations of specific entities like political parties and person names, given that the shared task provided no in-domain training parallel data dealing with the restricted topic. Our primary submission to the shared task uses backtranslation generated with a type of decoding allowing the insertion of constraints in the output in order to guarantee the correct translation of specific terms that are not necessarily observed in the data.","pages":"147--154","doi":"10.18653\/v1\/W19-5310","url":"https:\/\/www.aclweb.org\/anthology\/W19-5310","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5311","title":"The {TALP}-{UPC} Machine Translation Systems for {WMT}19 News Translation Task: Pivoting Techniques for Low Resource {MT}","authors":["Casas, Noe","Fonollosa, Jos{\\'e} A. R.","Escolano, Carlos","Basta, Christine","Costa-juss{\\`a}, Marta R."],"emails":["noe.casas@upc.edu","jose.fonollosa@upc.edu","carlos.escolano@upc.edu","christine.raouf.saad.basta@upc.edu","marta.ruiz@upc.edu"],"author_id":["noe-casas","jose-a-r-fonollosa","carlos-escolano","christine-basta","marta-r-costa-jussa"],"abstract":"In this article, we describe the TALP-UPC research group participation in the WMT19 news translation shared task for Kazakh-English. Given the low amount of parallel training data, we resort to using Russian as pivot language, training subword-based statistical translation systems for Russian-Kazakh and Russian-English that were then used to create two synthetic pseudo-parallel corpora for Kazakh-English and English-Kazakh respectively. Finally, a self-attention model based on the decoder part of the Transformer architecture was trained on the two pseudo-parallel corpora.","pages":"155--162","doi":"10.18653\/v1\/W19-5311","url":"https:\/\/www.aclweb.org\/anthology\/W19-5311","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5312","title":"{K}yoto University Participation to the {WMT} 2019 News Shared Task","authors":["Cromieres, Fabien","Kurohashi, Sadao"],"emails":["fabien@nlp.ist.i.kyoto-u.ac.jp","kuro@i.kyoto-u.ac.jp"],"author_id":["fabien-cromieres","sadao-kurohashi"],"abstract":"We describe here the experiments we did for the the news translation shared task of WMT 2019. We focused on the new German-to-French language direction, and mostly used current standard approaches to develop a Neural Machine Translation system. We make use of the Tensor2Tensor implementation of the Transformer model. After carefully cleaning the data and noting the importance of the good use of recent monolingual data for the task, we obtain our final result by combining the output of a diverse set of trained models through the use of their {``}checkpoint agreement{''}.","pages":"163--167","doi":"10.18653\/v1\/W19-5312","url":"https:\/\/www.aclweb.org\/anthology\/W19-5312","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5313","title":"{NICT}{'}s Supervised Neural Machine Translation Systems for the {WMT}19 News Translation Task","authors":["Dabre, Raj","Chen, Kehai","Marie, Benjamin","Wang, Rui","Fujita, Atsushi","Utiyama, Masao","Sumita, Eiichiro"],"emails":["raj.dabre@nict.go.jp","khchen@nict.go.jp","bmarie@nict.go.jp","wangrui@nict.go.jp","atsushi.fujita@nict.go.jp","mutiyama@nict.go.jp","eiichiro.sumita@nict.go.jp"],"author_id":["raj-dabre","kehai-chen","benjamin-marie","rui-wang","atsushi-fujita","masao-utiyama","eiichiro-sumita"],"abstract":"In this paper, we describe our supervised neural machine translation (NMT) systems that we developed for the news translation task for Kazakh\u2194English, Gujarati\u2194English, Chinese\u2194English, and English\u2192Finnish translation directions. We focused on leveraging multilingual transfer learning and back-translation for the extremely low-resource language pairs: Kazakh\u2194English and Gujarati\u2194English translation. For the Chinese\u2194English translation, we used the provided parallel data augmented with a large quantity of back-translated monolingual data to train state-of-the-art NMT systems. We then employed techniques that have been proven to be most effective, such as back-translation, fine-tuning, and model ensembling, to generate the primary submissions of Chinese\u2194English. For English\u2192Finnish, our submission from WMT18 remains a strong baseline despite the increase in parallel corpora for this year{'}s task.","pages":"168--174","doi":"10.18653\/v1\/W19-5313","url":"https:\/\/www.aclweb.org\/anthology\/W19-5313","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5314","title":"The University of {S}ydney{'}s Machine Translation System for {WMT}19","authors":["Ding, Liang","Tao, Dacheng"],"emails":["ldin3097@uni.sydney.edu.au","dacheng.tao@sydney.edu.au"],"author_id":["liang-ding","dacheng-tao"],"abstract":"This paper describes the University of Sydney{'}s submission of the WMT 2019 shared news translation task. We participated in the Finnish-{\\textgreater}English direction and got the best BLEU(33.0) score among all the participants. Our system is based on the self-attentional Transformer networks, into which we integrated the most recent effective strategies from academic research (e.g., BPE, back translation, multi-features data selection, data augmentation, greedy model ensemble, reranking, ConMBR system combination, and postprocessing). Furthermore, we propose a novel augmentation method Cycle Translation and a data mixture strategy Big\/Small parallel construction to entirely exploit the synthetic corpus. Extensive experiments show that adding the above techniques can make continuous improvements of the BLEU scores, and the best result outperforms the baseline (Transformer ensemble model trained with the original parallel corpus) by approximately 5.3 BLEU score, achieving the state-of-the-art performance.","pages":"175--182","doi":"10.18653\/v1\/W19-5314","url":"https:\/\/www.aclweb.org\/anthology\/W19-5314","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5315","title":"{U}d{S}-{DFKI} Participation at {WMT} 2019: Low-Resource (en-gu) and Coreference-Aware (en-de) Systems","authors":["Espa{\\~n}a-Bonet, Cristina","Ruiter, Dana"],"emails":["enabith@dfki.de","druiter@lsv.uni-saarland.de"],"author_id":["cristina-espana-bonet","dana-ruiter"],"abstract":"This paper describes the UdS-DFKI submission to the WMT2019 news translation task for Gujarati{--}English (low-resourced pair) and German{--}English (document-level evaluation). Our systems rely on the on-line extraction of parallel sentences from comparable corpora for the first scenario and on the inclusion of coreference-related information in the training data in the second one.","pages":"183--190","doi":"10.18653\/v1\/W19-5315","url":"https:\/\/www.aclweb.org\/anthology\/W19-5315","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5316","title":"The {IIIT}-H {G}ujarati-{E}nglish Machine Translation System for {WMT}19","authors":["Goyal, Vikrant","Sharma, Dipti Misra"],"emails":["vikrant.goyal@research.iiit.ac.in","dipti@iiit.ac.in"],"author_id":["vikrant-goyal","dipti-misra-sharma"],"abstract":"This paper describes the Neural Machine Translation system of IIIT-Hyderabad for the Gujarati\u2192English news translation shared task of WMT19. Our system is basedon encoder-decoder framework with attention mechanism. We experimented with Multilingual Neural MT models. Our experiments show that Multilingual Neural Machine Translation leveraging parallel data from related language pairs helps in significant BLEU improvements upto 11.5, for low resource language pairs like Gujarati-English","pages":"191--195","doi":"10.18653\/v1\/W19-5316","url":"https:\/\/www.aclweb.org\/anthology\/W19-5316","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5317","title":"Kingsoft{'}s Neural Machine Translation System for {WMT}19","authors":["Guo, Xinze","Liu, Chang","Li, Xiaolong","Wang, Yiran","Li, Guoliang","Wang, Feng","Xu, Zhitao","Yang, Liuyi","Ma, Li","Li, Changliang"],"emails":["guoxinze@kingsoft.com","liuchang10@kingsoft.com","lixiaolong2@kingsoft.com","wangyiran3@kingsoft.com","liguoliang@kingsoft.com","wangfeng5@kingsoft.com","xuzhitao@kingsoft.com","yangliuyi@kingsoft.com","mali5@kingsoft.com","lichangliang@kingsoft.com"],"author_id":["xinze-guo","chang-liu","xiaolong-li","yiran-wang","guoliang-li","feng-wang","zhitao-xu","liuyi-yang","li-ma","changliang-li"],"abstract":"This paper describes the Kingsoft AI Lab{'}s submission to the WMT2019 news translation shared task. We participated in two language directions: English-Chinese and Chinese-English. For both language directions, we trained several variants of Transformer models using the provided parallel data enlarged with a large quantity of back-translated monolingual data. The best translation result was obtained with ensemble and reranking techniques. According to automatic metrics (BLEU) our Chinese-English system reached the second highest score, and our English-Chinese system reached the second highest score for this subtask.","pages":"196--202","doi":"10.18653\/v1\/W19-5317","url":"https:\/\/www.aclweb.org\/anthology\/W19-5317","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5318","title":"The {AFRL} {WMT}19 Systems: Old Favorites and New Tricks","authors":["Gwinnup, Jeremy","Erdmann, Grant","Anderson, Tim"],"emails":["jeremy.gwinnup.1@us.af.mil","grant.erdmann@us.af.mil","timothy.anderson.20@us.af.mil"],"author_id":["jeremy-gwinnup","grant-erdmann","tim-anderson"],"abstract":"This paper describes the Air Force Research Laboratory (AFRL) machine translation systems and the improvements that were developed during the WMT19 evaluation campaign. This year, we refine our approach to training popular neural machine translation toolkits, experiment with a new domain adaptation technique and again measure improvements in performance on the Russian{--}English language pair.","pages":"203--208","doi":"10.18653\/v1\/W19-5318","url":"https:\/\/www.aclweb.org\/anthology\/W19-5318","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5319","title":"Evaluating the Supervised and Zero-shot Performance of Multi-lingual Translation Models","authors":["Hokamp, Chris","Glover, John","Gholipour Ghalandari, Demian"],"emails":["","",""],"author_id":["chris-hokamp","john-glover","demian-gholipour-ghalandari"],"abstract":"We study several methods for full or partial sharing of the decoder parameters of multi-lingual NMT models. Using only the WMT 2019 shared task parallel datasets for training, we evaluate both fully supervised and zero-shot translation performance in 110 unique translation directions. We use additional test sets and re-purpose evaluation methods recently used for unsupervised MT in order to evaluate zero-shot translation performance for language pairs where no gold-standard parallel data is available. To our knowledge, this is the largest evaluation of multi-lingual translation yet conducted in terms of the total size of the training data we use, and in terms of the number of zero-shot translation pairs we evaluate. We conduct an in-depth evaluation of the translation performance of different models, highlighting the trade-offs between methods of sharing decoder parameters. We find that models which have task-specific decoder parameters outperform models where decoder parameters are fully shared across all tasks.","pages":"209--217","doi":"10.18653\/v1\/W19-5319","url":"https:\/\/www.aclweb.org\/anthology\/W19-5319","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5320","title":"The {MLLP}-{UPV} Supervised Machine Translation Systems for {WMT}19 News Translation Task","authors":["Iranzo-S{\\'a}nchez, Javier","Garc{\\'e}s D{\\'\\i}az-Mun{\\'\\i}o, Gon{\\c{c}}al","Civera, Jorge","Juan, Alfons"],"emails":["jairsan@vrain.upv.es","ggarces@vrain.upv.es","jcivera@vrain.upv.es","ajuan@vrain.upv.es"],"author_id":["javier-iranzo-sanchez","goncal-garces-diaz-munio","jorge-civera","alfons-juan"],"abstract":"This paper describes the participation of the MLLP research group of the Universitat Polit{\\`e}cnica de Val{\\`e}ncia in the WMT 2019 News Translation Shared Task. In this edition, we have submitted systems for the German \u2194 English and German \u2194 French language pairs, participating in both directions of each pair. Our submitted systems, based on the Transformer architecture, make ample use of data filtering, synthetic data and domain adaptation through fine-tuning.","pages":"218--224","doi":"10.18653\/v1\/W19-5320","url":"https:\/\/www.aclweb.org\/anthology\/W19-5320","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5321","title":"{M}icrosoft Translator at {WMT} 2019: Towards Large-Scale Document-Level Neural Machine Translation","authors":["Junczys-Dowmunt, Marcin"],"emails":["marcinjd@microsoft.com"],"author_id":["marcin-junczys-dowmunt"],"abstract":"This paper describes the Microsoft Translator submissions to the WMT19 news translation shared task for English-German. Our main focus is document-level neural machine translation with deep transformer models. We start with strong sentence-level baselines, trained on large-scale data created via data-filtering and noisy back-translation and find that back-translation seems to mainly help with translationese input. We explore fine-tuning techniques, deeper models and different ensembling strategies to counter these effects. Using document boundaries present in the authentic and synthetic parallel data, we create sequences of up to 1000 subword segments and train transformer translation models. We experiment with data augmentation techniques for the smaller authentic data with document-boundaries and for larger authentic data without boundaries. We further explore multi-task training for the incorporation of document-level source language monolingual data via the BERT-objective on the encoder and two-pass decoding for combinations of sentence-level and document-level systems. Based on preliminary human evaluation results, evaluators strongly prefer the document-level systems over our comparable sentence-level system. The document-level systems also seem to score higher than the human references in source-based direct assessment.","pages":"225--233","doi":"10.18653\/v1\/W19-5321","url":"https:\/\/www.aclweb.org\/anthology\/W19-5321","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5322","title":"{CUNI} Submission for Low-Resource Languages in {WMT} News 2019","authors":["Kocmi, Tom","Bojar, Ond{\\v{r}}ej"],"emails":["",""],"author_id":["tom-kocmi","ondrej-bojar"],"abstract":"This paper describes the CUNI submission to the WMT 2019 News Translation Shared Task for the low-resource languages: Gujarati-English and Kazakh-English. We participated in both language pairs in both translation directions. Our system combines transfer learning from a different high-resource language pair followed by training on backtranslated monolingual data. Thanks to the simultaneous training in both directions, we can iterate the backtranslation process. We are using the Transformer model in a constrained submission.","pages":"234--240","doi":"10.18653\/v1\/W19-5322","url":"https:\/\/www.aclweb.org\/anthology\/W19-5322","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5323","title":"{CUNI} Systems for the Unsupervised News Translation Task in {WMT} 2019","authors":["Kvapil{\\'\\i}kov{\\'a}, Ivana","Mach{\\'a}{\\v{c}}ek, Dominik","Bojar, Ond{\\v{r}}ej"],"emails":["","",""],"author_id":["ivana-kvapilikova","dominik-machacek","ondrej-bojar"],"abstract":"In this paper we describe the CUNI translation system used for the unsupervised news shared task of the ACL 2019 Fourth Conference on Machine Translation (WMT19). We follow the strategy of Artetxe ae at. (2018b), creating a seed phrase-based system where the phrase table is initialized from cross-lingual embedding mappings trained on monolingual data, followed by a neural machine translation system trained on synthetic parallel data. The synthetic corpus was produced from a monolingual corpus by a tuned PBMT model refined through iterative back-translation. We further focus on the handling of named entities, i.e. the part of vocabulary where the cross-lingual embedding mapping suffers most. Our system reaches a BLEU score of 15.3 on the German-Czech WMT19 shared task.","pages":"241--248","doi":"10.18653\/v1\/W19-5323","url":"https:\/\/www.aclweb.org\/anthology\/W19-5323","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5324","title":"A Comparison on Fine-grained Pre-trained Embeddings for the {WMT}19{C}hinese-{E}nglish News Translation Task","authors":["Li, Zhenhao","Specia, Lucia"],"emails":["zhenhao.li18@imperial.ac.uk","l.specia@imperial.ac.uk"],"author_id":["zhenhao-li","lucia-specia"],"abstract":"This paper describes our submission to theWMT 2019 Chinese-English (zh-en) newstranslation shared task.Our systems arebased on RNN architectures with pre-trainedembeddings which utilize character and sub-character information. We compare modelswith these different granularity levels usingdifferent evaluating metics. We find that a finergranularity embeddings can help the model ac-cording to character level evaluation and thatthe pre-trained embeddings can also be bene-ficial for model performance marginally whenthe training data is limited.","pages":"249--256","doi":"10.18653\/v1\/W19-5324","url":"https:\/\/www.aclweb.org\/anthology\/W19-5324","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5325","title":"The {N}iu{T}rans Machine Translation Systems for {WMT}19","authors":["Li, Bei","Li, Yinqiao","Xu, Chen","Lin, Ye","Liu, Jiqiang","Liu, Hui","Wang, Ziyang","Zhang, Yuhao","Xu, Nuo","Wang, Zeyang","Feng, Kai","Chen, Hexuan","Liu, Tengbo","Li, Yanyang","Wang, Qiang","Xiao, Tong","Zhu, Jingbo"],"emails":["","","","","","","","","neu@outlook.com","","","","","","","xiaotong@mail.neu.edu.cn","zhujingbo@mail.neu.edu.cn"],"author_id":["bei-li","yinqiao-li","chen-xu","ye-lin","jiqiang-liu","hui-liu","ziyang-wang","yuhao-zhang","nuo-xu","zeyang-wang","kai-feng","hexuan-chen","tengbo-liu","yanyang-li","qiang-wang","tong-xiao","jingbo-zhu"],"abstract":"This paper described NiuTrans neural machine translation systems for the WMT 2019 news translation tasks. We participated in 13 translation directions, including 11 supervised tasks, namely EN$\\leftrightarrow${ZH, DE, RU, KK, LT}, GU$\\rightarrow$EN and the unsupervised DE$\\leftrightarrow$CS sub-track. Our systems were built on Deep Transformer and several back-translation methods. Iterative knowledge distillation and ensemble+reranking were also employed to obtain stronger models. Our unsupervised submissions were based on NMT enhanced by SMT. As a result, we achieved the highest BLEU scores in KK{\\textless}-{\\textgreater}EN, GU-{\\textgreater}EN directions, ranking 2nd in RU-{\\textgreater}EN, DE{\\textless}-{\\textgreater}CS and 3rd in ZH-{\\textgreater}EN, LT-{\\textgreater}EN, EN-{\\textgreater}RU, EN{\\textless}-{\\textgreater}DE among all constrained submissions.","pages":"257--266","doi":"10.18653\/v1\/W19-5325","url":"https:\/\/www.aclweb.org\/anthology\/W19-5325","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5326","title":"Multi-Source Transformer for {K}azakh-{R}ussian-{E}nglish Neural Machine Translation","authors":["Littell, Patrick","Lo, Chi-kiu","Larkin, Samuel","Stewart, Darlene"],"emails":["","","","tewart@nrc-cnrc.gc.ca"],"author_id":["patrick-littell","chi-kiu-lo","samuel-larkin","darlene-stewart"],"abstract":"We describe the neural machine translation (NMT) system developed at the National Re-search Council of Canada (NRC) for the Kazakh-English news translation task of the Fourth Conference on Machine Translation (WMT19). Our submission is a multi-source NMT taking both the original Kazakh sentence and its Russian translation as input for translating into English.","pages":"267--274","doi":"10.18653\/v1\/W19-5326","url":"https:\/\/www.aclweb.org\/anthology\/W19-5326","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5327","title":"Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring","authors":["Liu, Zihan","Xu, Yan","Winata, Genta Indra","Fung, Pascale"],"emails":["zliurc@connect.ust.hk","yxucb@connect.ust.hk","giwinata@connect.ust.hk","pascale@ece.ust.hk"],"author_id":["zihan-liu","yan-xu","genta-indra-winata","pascale-fung"],"abstract":"This paper describes CAiRE{'}s submission to the unsupervised machine translation track of the WMT{'}19 news shared task from German to Czech. We leverage a phrase-based statistical machine translation (PBSMT) model and a pre-trained language model to combine word-level neural machine translation (NMT) and subword-level NMT models without using any parallel data. We propose to solve the morphological richness problem of languages by training byte-pair encoding (BPE) embeddings for German and Czech separately, and they are aligned using MUSE (Conneau et al., 2018). To ensure the fluency and consistency of translations, a rescoring mechanism is proposed that reuses the pre-trained language model to select the translation candidates generated through beam search. Moreover, a series of pre-processing and post-processing approaches are applied to improve the quality of final translations.","pages":"275--282","doi":"10.18653\/v1\/W19-5327","url":"https:\/\/www.aclweb.org\/anthology\/W19-5327","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5328","title":"{JUMT} at {WMT}2019 News Translation Task: A Hybrid Approach to Machine Translation for {L}ithuanian to {E}nglish","authors":["Mahata, Sainik Kumar","Garain, Avishek","Rayala, Adityar","Das, Dipankar","Bandyopadhyay, Sivaji"],"emails":["sainik.mahata@gmail.com","avishekgarain@gmail.com","mailsofadityar@gmail.com","dipankar.dipnil2005@gmail.com","ju@yahoo.com"],"author_id":["sainik-mahata","avishek-garain","adityar-rayala","dipankar-das","sivaji-bandyopadhyay"],"abstract":"In the current work, we present a description of the system submitted to WMT 2019 News Translation Shared task. The system was created to translate news text from Lithuanian to English. To accomplish the given task, our system used a Word Embedding based Neural Machine Translation model to post edit the outputs generated by a Statistical Machine Translation model. The current paper documents the architecture of our model, descriptions of the various modules and the results produced using the same. Our system garnered a BLEU score of 17.6.","pages":"283--286","doi":"10.18653\/v1\/W19-5328","url":"https:\/\/www.aclweb.org\/anthology\/W19-5328","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5329","title":"{J}ohns {H}opkins University Submission for {WMT} News Translation Task","authors":["Marchisio, Kelly","Lal, Yash Kumar","Koehn, Philipp"],"emails":["kmarchi1@jhu.edu","yash@jhu.edu","phi@jhu.edu"],"author_id":["kelly-marchisio","yash-kumar-lal","philipp-koehn"],"abstract":"We describe the work of Johns Hopkins University for the shared task of news translation organized by the Fourth Conference on Machine Translation (2019). We submitted systems for both directions of the English-German language pair. The systems combine multiple techniques {--} sampling, filtering, iterative backtranslation, and continued training {--} previously used to improve performance of neural machine translation models. At submission time, we achieve a BLEU score of 38.1 for De-En and 42.5 for En-De translation directions on newstest2019. Post-submission, the score is 38.4 for De-En and 42.8 for En-De. Various experiments conducted in the process are also described.","pages":"287--293","doi":"10.18653\/v1\/W19-5329","url":"https:\/\/www.aclweb.org\/anthology\/W19-5329","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5330","title":"{NICT}{'}s Unsupervised Neural and Statistical Machine Translation Systems for the {WMT}19 News Translation Task","authors":["Marie, Benjamin","Sun, Haipeng","Wang, Rui","Chen, Kehai","Fujita, Atsushi","Utiyama, Masao","Sumita, Eiichiro"],"emails":["bmarie@nict.go.jp","sun.haipeng@nict.go.jp","wangrui@nict.go.jp","khchen@nict.go.jp","atsushi.fujita@nict.go.jp","mutiyama@nict.go.jp","eiichiro.sumita@nict.go.jp"],"author_id":["benjamin-marie","haipeng-sun","rui-wang","kehai-chen","atsushi-fujita","masao-utiyama","eiichiro-sumita"],"abstract":"This paper presents the NICT{'}s participation in the WMT19 unsupervised news translation task. We participated in the unsupervised translation direction: German-Czech. Our primary submission to the task is the result of a simple combination of our unsupervised neural and statistical machine translation systems. Our system is ranked first for the German-to-Czech translation task, using only the data provided by the organizers ({``}constraint{'}{''}), according to both BLEU-cased and human evaluation. We also performed contrastive experiments with other language pairs, namely, English-Gujarati and English-Kazakh, to better assess the effectiveness of unsupervised machine translation in for distant language pairs and in truly low-resource conditions.","pages":"294--301","doi":"10.18653\/v1\/W19-5330","url":"https:\/\/www.aclweb.org\/anthology\/W19-5330","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5331","title":"{PROMT} Systems for {WMT} 2019 Shared Translation Task","authors":["Molchanov, Alexander"],"emails":[""],"author_id":["alexander-molchanov"],"abstract":"This paper describes the PROMT submissions for the WMT 2019 Shared News Translation Task. This year we participated in two language pairs and in three directions: English-Russian, English-German and German-English. All our submissions are Marian-based neural systems. We use significantly more data compared to the last year. We also present our improved data filtering pipeline.","pages":"302--307","doi":"10.18653\/v1\/W19-5331","url":"https:\/\/www.aclweb.org\/anthology\/W19-5331","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5332","title":"{JU}-{S}aarland Submission to the {WMT}2019 {E}nglish{--}{G}ujarati Translation Shared Task","authors":["Mondal, Riktim","Nayek, Shankha Raj","Chowdhury, Aditya","Pal, Santanu","Naskar, Sudip Kumar","van Genabith, Josef"],"emails":["riktimrules@gmail.com","shankharaj29@gmail.com","adityachowdhury21@gmail.com","santanu.pal@uni-saarland.de","sudip.naskar@cse.jdvu.ac.in","josef.vangenabith@uni-saarland.de"],"author_id":["riktim-mondal","shankha-raj-nayek","aditya-chowdhury","santanu-pal","sudip-kumar-naskar","josef-van-genabith"],"abstract":"In this paper we describe our joint submission (JU-Saarland) from Jadavpur University and Saarland University in the WMT 2019 news translation shared task for English{--}Gujarati language pair within the translation task sub-track. Our baseline and primary submissions are built using Recurrent neural network (RNN) based neural machine translation (NMT) system which follows attention mechanism. Given the fact that the two languages belong to different language families and there is not enough parallel data for this language pair, building a high quality NMT system for this language pair is a difficult task. We produced synthetic data through back-translation from available monolingual data. We report the translation quality of our English{--}Gujarati and Gujarati{--}English NMT systems trained at word, byte-pair and character encoding levels where RNN at word level is considered as the baseline and used for comparison purpose. Our English{--}Gujarati system ranked in the second position in the shared task.","pages":"308--313","doi":"10.18653\/v1\/W19-5332","url":"https:\/\/www.aclweb.org\/anthology\/W19-5332","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5333","title":"{F}acebook {FAIR}{'}s {WMT}19 News Translation Task Submission","authors":["Ng, Nathan","Yee, Kyra","Baevski, Alexei","Ott, Myle","Auli, Michael","Edunov, Sergey"],"emails":["","","","","",""],"author_id":["nathan-ng","kyra-yee","alexei-baevski","myle-ott","michael-auli","sergey-edunov"],"abstract":"This paper describes Facebook FAIR{'}s submission to the WMT19 shared news translation task. We participate in four language directions, English {\\textless}-{\\textgreater} German and English {\\textless}-{\\textgreater} Russian in both directions. Following our submission from last year, our baseline systems are large BPE-based transformer models trained with the FAIRSEQ sequence modeling toolkit. This year we experiment with different bitext data filtering schemes, as well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific data, then decode using noisy channel model reranking. Our system improves on our previous system{'}s performance by 4.5 BLEU points and achieves the best case-sensitive BLEU score for the translation direction English\u2192Russian.","pages":"314--319","doi":"10.18653\/v1\/W19-5333","url":"https:\/\/www.aclweb.org\/anthology\/W19-5333","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5334","title":"e{T}ranslation{'}s Submissions to the {WMT} 2019 News Translation Task","authors":["Oravecz, Csaba","Bontcheva, Katina","Lardilleux, Adrien","Tihanyi, L{\\'a}szl{\\'o}","Eisele, Andreas"],"emails":["oravecz.csaba@gmail.com","katina.bontcheva@sogeti.lu","adrien.lardilleux@c-dev.eu","tihanyi1123@gmail.com","andreas.eisele@ec.europa.eu"],"author_id":["csaba-oravecz","katina-bontcheva","adrien-lardilleux","laszlo-tihanyi","andreas-eisele"],"abstract":"This paper describes the submissions of the eTranslation team to the WMT 2019 news translation shared task. The systems have been developed with the aim of identifying and following rather than establishing best practices, under the constraints imposed by a low resource training and decoding environment normally used for our production systems. Thus most of the findings and results are transferable to systems used in the eTranslation service. Evaluations suggest that this approach is able to produce decent models with good performance and speed without the overhead of using prohibitively deep and complex architectures.","pages":"320--326","doi":"10.18653\/v1\/W19-5334","url":"https:\/\/www.aclweb.org\/anthology\/W19-5334","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5335","title":"Tilde{'}s Machine Translation Systems for {WMT} 2019","authors":["Pinnis, Marcis","Kri{\\v{s}}lauks, Rihards","Rikters, Mat{\\=\\i}ss"],"emails":["marcis.pinnis@tilde.lv","rihards.kri{\\v{s}}lauks@tilde.lv","mat{\\=\\i}ss.rikters@tilde.lv"],"author_id":["marcis-pinnis","rihards-krislauks","matiss-rikters"],"abstract":"The paper describes the development process of Tilde{'}s NMT systems for the WMT 2019 shared task on news translation. We trained systems for the English-Lithuanian and Lithuanian-English translation directions in constrained and unconstrained tracks. We build upon the best methods of the previous year{'}s competition and combine them with recent advancements in the field. We also present a new method to ensure source domain adherence in back-translated data. Our systems achieved a shared first place in human evaluation.","pages":"327--334","doi":"10.18653\/v1\/W19-5335","url":"https:\/\/www.aclweb.org\/anthology\/W19-5335","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5336","title":"Apertium-fin-eng{--}Rule-based Shallow Machine Translation for {WMT} 2019 Shared Task","authors":["Pirinen, Tommi"],"emails":["tommi.antero.pirinen@uni-hamburg.de"],"author_id":["tommi-a-pirinen"],"abstract":"In this paper we describe a rule-based, bi-directional machine translation system for the Finnish{---}English language pair. The baseline system was based on the existing data of FinnWordNet, omorfi and apertium-eng. We have built the disambiguation, lexical selection and translation rules by hand. The dictionaries and rules have been developed based on the shared task data. We describe in this article the use of the shared task data as a kind of a test-driven development workflow in RBMT development and show that it suits perfectly to a modern software engineering continuous integration workflow of RBMT and yields big increases to BLEU scores with minimal effort.","pages":"335--341","doi":"10.18653\/v1\/W19-5336","url":"https:\/\/www.aclweb.org\/anthology\/W19-5336","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5337","title":"{E}nglish-{C}zech Systems in {WMT}19: Document-Level Transformer","authors":["Popel, Martin","Mach{\\'a}{\\v{c}}ek, Dominik","Auersperger, Michal","Bojar, Ond{\\v{r}}ej","Pecina, Pavel"],"emails":["surname@ufal.mff.cuni.cz","","","",""],"author_id":["martin-popel","dominik-machacek","michal-auersperger","ondrej-bojar","pavel-pecina"],"abstract":"We describe our NMT systems submitted to the WMT19 shared task in English\u2192Czech news translation. Our systems are based on the Transformer model implemented in either Tensor2Tensor (T2T) or Marian framework. We aimed at improving the adequacy and coherence of translated documents by enlarging the context of the source and target. Instead of translating each sentence independently, we split the document into possibly overlapping multi-sentence segments. In case of the T2T implementation, this {``}document-level{''}-trained system achieves a +0.6 BLEU improvement (p {\\textless} 0.05) relative to the same system applied on isolated sentences. To assess the potential effect document-level models might have on lexical coherence, we performed a semi-automatic analysis, which revealed only a few sentences improved in this aspect. Thus, we cannot draw any conclusions from this week evidence.","pages":"342--348","doi":"10.18653\/v1\/W19-5337","url":"https:\/\/www.aclweb.org\/anthology\/W19-5337","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5338","title":"The {RWTH} Aachen University Machine Translation Systems for {WMT} 2019","authors":["Rosendahl, Jan","Herold, Christian","Kim, Yunsu","Gra{\\c{c}}a, Miguel","Wang, Weiyue","Bahar, Parnia","Gao, Yingbo","Ney, Hermann"],"emails":["","","","","","","",""],"author_id":["jan-rosendahl","christian-herold","yunsu-kim","miguel-graca","weiyue-wang","parnia-bahar","yingbo-gao","hermann-ney"],"abstract":"This paper describes the neural machine translation systems developed at the RWTH Aachen University for the German-English, Chinese-English and Kazakh-English news translation tasks of the Fourth Conference on Machine Translation (WMT19). For all tasks, the final submitted system is based on the Transformer architecture. We focus on improving data filtering and fine-tuning as well as systematically evaluating interesting approaches like unigram language model segmentation and transfer learning. For the De-En task, none of the tested methods gave a significant improvement over last years winning system and we end up with the same performance, resulting in 39.6{\\%} BLEU on newstest2019. In the Zh-En task, we show 1.3{\\%} BLEU improvement over our last year{'}s submission, which we mostly attribute to the splitting of long sentences during translation. We further report results on the Kazakh-English task where we gain improvements of 11.1{\\%} BLEU over our baseline system. On the same task we present a recent transfer learning approach, which uses half of the free parameters of our submission system and performs on par with it.","pages":"349--355","doi":"10.18653\/v1\/W19-5338","url":"https:\/\/www.aclweb.org\/anthology\/W19-5338","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5339","title":"The {U}niversitat d{'}Alacant Submissions to the {E}nglish-to-{K}azakh News Translation Task at {WMT} 2019","authors":["S{\\'a}nchez-Cartagena, V{\\'\\i}ctor M.","P{\\'e}rez-Ortiz, Juan Antonio","S{\\'a}nchez-Mart{\\'\\i}nez, Felipe"],"emails":["fsanchez@dlsi.ua.es","japerez@dlsi.ua.es","vmsanchez@dlsi.ua.es"],"author_id":["victor-m-sanchez-cartagena","juan-antonio-perez-ortiz","felipe-sanchez-martinez"],"abstract":"This paper describes the two submissions of Universitat d{'}Alacant to the English-to-Kazakh news translation task at WMT 2019. Our submissions take advantage of monolingual data and parallel data from other language pairs by means of iterative backtranslation, pivot backtranslation and transfer learning. They also use linguistic information in two ways: morphological segmentation of Kazakh text, and integration of the output of a rule-based machine translation system. Our systems were ranked second in terms of chrF++ despite being built from an ensemble of only 2 independent training runs.","pages":"356--363","doi":"10.18653\/v1\/W19-5339","url":"https:\/\/www.aclweb.org\/anthology\/W19-5339","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5340","title":"CUED@WMT19:EWC{\\&}LMs","authors":["Stahlberg, Felix","Saunders, Danielle","de Gispert, Adri{\\`a}","Byrne, Bill"],"emails":["fs439@cam.ac.uk","ds636@cam.ac.uk","agispert@sdl.com","bbyrne@sdl.com"],"author_id":["felix-stahlberg","danielle-saunders","adria-de-gispert","bill-byrne"],"abstract":"Two techniques provide the fabric of the Cambridge University Engineering Department{'}s (CUED) entry to the WMT19 evaluation campaign: elastic weight consolidation (EWC) and different forms of language modelling (LMs). We report substantial gains by fine-tuning very strong baselines on former WMT test sets using a combination of checkpoint averaging and EWC. A sentence-level Transformer LM and a document-level LM based on a modified Transformer architecture yield further gains. As in previous years, we also extract n-gram probabilities from SMT lattices which can be seen as a source-conditioned n-gram LM.","pages":"364--373","doi":"10.18653\/v1\/W19-5340","url":"https:\/\/www.aclweb.org\/anthology\/W19-5340","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5341","title":"{B}aidu Neural Machine Translation Systems for {WMT}19","authors":["Sun, Meng","Jiang, Bojian","Xiong, Hao","He, Zhongjun","Wu, Hua","Wang, Haifeng"],"emails":["","","","","hua@baidu.com","wanghaifeng@baidu.com"],"author_id":["meng-sun","bojian-jiang","hao-xiong","zhongjun-he","hua-wu","haifeng-wang"],"abstract":"In this paper we introduce the systems Baidu submitted for the WMT19 shared task on Chinese{\\textless}-{\\textgreater}English news translation. Our systems are based on the Transformer architecture with some effective improvements. Data selection, back translation, data augmentation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. Our Chinese-{\\textgreater}English system achieved the highest case-sensitive BLEU score among all constrained submissions, and our English-{\\textgreater}Chinese system ranked the second in all submissions.","pages":"374--381","doi":"10.18653\/v1\/W19-5341","url":"https:\/\/www.aclweb.org\/anthology\/W19-5341","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5342","title":"University of Tartu{'}s Multilingual Multi-domain {WMT}19 News Translation Shared Task Submission","authors":["T{\\\"a}ttar, Andre","Korotkova, Elizaveta","Fishel, Mark"],"emails":["andre.tattar@ut.ee","elizaveta.korotkova@ut.ee","fishel@ut.ee"],"author_id":["andre-tattar","elizaveta-korotkova","mark-fishel"],"abstract":"This paper describes the University of Tartu{'}s submission to the news translation shared task of WMT19, where the core idea was to train a single multilingual system to cover several language pairs of the shared task and submit its results. We only used the constrained data from the shared task. We describe our approach and its results and discuss the technical issues we faced.","pages":"382--385","doi":"10.18653\/v1\/W19-5342","url":"https:\/\/www.aclweb.org\/anthology\/W19-5342","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5343","title":"Neural Machine Translation for {E}nglish{--}{K}azakh with Morphological Segmentation and Synthetic Data","authors":["Toral, Antonio","Edman, Lukas","Yeshmagambetova, Galiya","Spenader, Jennifer"],"emails":["a.toral.ruiz@rug.nl","j.l.edman@student.rug.nl","g.yeshmagambetova@student.rug.nl","j.spenader@ai.rug.nl"],"author_id":["antonio-toral","lukas-edman","galiya-yeshmagambetova","jennifer-spenader"],"abstract":"This paper presents the systems submitted by the University of Groningen to the English{--} Kazakh language pair (both translation direc- tions) for the WMT 2019 news translation task. We explore the potential benefits of (i) morpho- logical segmentation (both unsupervised and rule-based), given the agglutinative nature of Kazakh, (ii) data from two addi- tional languages (Turkish and Russian), given the scarcity of English{--}Kazakh data and (iii) synthetic data, both for the source and for the target language. Our best sub- missions ranked second for Kazakh\u2192English and third for English\u2192Kazakh in terms of the BLEU automatic evaluation metric.","pages":"386--392","doi":"10.18653\/v1\/W19-5343","url":"https:\/\/www.aclweb.org\/anthology\/W19-5343","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5344","title":"The {LMU} Munich Unsupervised Machine Translation System for {WMT}19","authors":["Stojanovski, Dario","Hangya, Viktor","Huck, Matthias","Fraser, Alexander"],"emails":["stojanovski@cis.lmu.de","hangyav@cis.lmu.de","mhuck@cis.lmu.de","fraser@cis.lmu.de"],"author_id":["dario-stojanovski","viktor-hangya","matthias-huck","alexander-fraser"],"abstract":"We describe LMU Munich{'}s machine translation system for German\u2192Czech translation which was used to participate in the WMT19 shared task on unsupervised news translation. We train our model using monolingual data only from both languages. The final model is an unsupervised neural model using established techniques for unsupervised translation such as denoising autoencoding and online back-translation. We bootstrap the model with masked language model pretraining and enhance it with back-translations from an unsupervised phrase-based system which is itself bootstrapped using unsupervised bilingual word embeddings.","pages":"393--399","doi":"10.18653\/v1\/W19-5344","url":"https:\/\/www.aclweb.org\/anthology\/W19-5344","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5345","title":"Combining Local and Document-Level Context: The {LMU} Munich Neural Machine Translation System at {WMT}19","authors":["Stojanovski, Dario","Fraser, Alexander"],"emails":["stojanovski@cis.lmu.de","fraser@cis.lmu.de"],"author_id":["dario-stojanovski","alexander-fraser"],"abstract":"We describe LMU Munich{'}s machine translation system for English\u2192German translation which was used to participate in the WMT19 shared task on supervised news translation. We specifically participated in the document-level MT track. The system used as a primary submission is a context-aware Transformer capable of both rich modeling of limited contextual information and integration of large-scale document-level context with a less rich representation. We train this model by fine-tuning a big Transformer baseline. Our experimental results show that document-level context provides for large improvements in translation quality, and adding a rich representation of the previous sentence provides a small additional gain.","pages":"400--406","doi":"10.18653\/v1\/W19-5345","url":"https:\/\/www.aclweb.org\/anthology\/W19-5345","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5346","title":"{IITP}-{MT} System for {G}ujarati-{E}nglish News Translation Task at {WMT} 2019","authors":["Sen, Sukanta","Gupta, Kamal Kumar","Ekbal, Asif","Bhattacharyya, Pushpak"],"emails":["sukanta.pcs15@iitp.ac.in","kamal.pcs17@iitp.ac.in","asif@iitp.ac.in","pb@iitp.ac.in"],"author_id":["sukanta-sen","kamal-kumar-gupta","asif-ekbal","pushpak-bhattacharyya"],"abstract":"We describe our submission to WMT 2019 News translation shared task for Gujarati-English language pair. We submit constrained systems, i.e, we rely on the data provided for this language pair and do not use any external data. We train Transformer based subword-level neural machine translation (NMT) system using original parallel corpus along with synthetic parallel corpus obtained through back-translation of monolingual data. Our primary systems achieve BLEU scores of 10.4 and 8.1 for Gujarati\u2192English and English\u2192Gujarati, respectively. We observe that incorporating monolingual data through back-translation improves the BLEU score significantly over baseline NMT and SMT systems for this language pair.","pages":"407--411","doi":"10.18653\/v1\/W19-5346","url":"https:\/\/www.aclweb.org\/anthology\/W19-5346","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5347","title":"The University of {H}elsinki Submissions to the {WMT}19 News Translation Task","authors":["Talman, Aarne","Sulubacak, Umut","V{\\'a}zquez, Ra{\\'u}l","Scherrer, Yves","Virpioja, Sami","Raganato, Alessandro","Hurskainen, Arvi","Tiedemann, J{\\\"o}rg"],"emails":["aarne.talman@helsinki.fi","umut.sulubacak@helsinki.fi","ra{\\'u}l.v{\\'a}zquez@helsinki.fi","yves.scherrer@helsinki.fi","sami.virpioja@helsinki.fi","alessandro.raganato@helsinki.fi","arvi.hurskainen@helsinki.fi","j{\\\"o}rg.tiedemann@helsinki.fi"],"author_id":["aarne-talman","umut-sulubacak","raul-vazquez","yves-scherrer","sami-virpioja","alessandro-raganato","arvi-hurskainen","jorg-tiedemann"],"abstract":"In this paper we present the University of Helsinki submissions to the WMT 2019 shared news translation task in three language pairs: English-German, English-Finnish and Finnish-English. This year we focused first on cleaning and filtering the training data using multiple data-filtering approaches, resulting in much smaller and cleaner training sets. For English-German we trained both sentence-level transformer models as well as compared different document-level translation approaches. For Finnish-English and English-Finnish we focused on different segmentation approaches and we also included a rule-based system for English-Finnish.","pages":"412--423","doi":"10.18653\/v1\/W19-5347","url":"https:\/\/www.aclweb.org\/anthology\/W19-5347","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5348","title":"{M}icrosoft Research Asia{'}s Systems for {WMT}19","authors":["Xia, Yingce","Tan, Xu","Tian, Fei","Gao, Fei","He, Di","Chen, Weicong","Fan, Yang","Gong, Linyuan","Leng, Yichong","Luo, Renqian","Wang, Yiren","Wu, Lijun","Zhu, Jinhua","Qin, Tao","Liu, Tie-Yan"],"emails":["","","","","","","","","","","","","","",""],"author_id":["yingce-xia","xu-tan","fei-tian","fei-gao","di-he","weicong-chen","yang-fan","linyuan-gong","yichong-leng","renqian-luo","yiren-wang","lijun-wu","jinhua-zhu","tao-qin","tie-yan-liu"],"abstract":"We Microsoft Research Asia made submissions to 11 language directions in the WMT19 news translation tasks. We won the first place for 8 of the 11 directions and the second place for the other three. Our basic systems are built on Transformer, back translation and knowledge distillation. We integrate several of our rececent techniques to enhance the baseline systems: multi-agent dual learning (MADL), masked sequence-to-sequence pre-training (MASS), neural architecture optimization (NAO), and soft contextual data augmentation (SCA).","pages":"424--433","doi":"10.18653\/v1\/W19-5348","url":"https:\/\/www.aclweb.org\/anthology\/W19-5348","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5349","title":"The En-{R}u Two-way Integrated Machine Translation System Based on Transformer","authors":["Yu, Doron"],"emails":[""],"author_id":["doron-yu"],"abstract":"Machine translation is one of the most popular areas in natural language processing. WMT is a conference to assess the level of machine translation capabilities of organizations around the world, which is the evaluation activity we participated in. In this review we participated in a two-way translation track from Russian to English and English to Russian. We used official training data, 38 million parallel corpora, and 10 million monolingual corpora. The overall framework we use is the Transformer neural machine translation model, supplemented by data filtering, post-processing, reordering and other related processing methods. The BLEU value of our final translation result from Russian to English is 38.7, ranking 5th, while from English to Russian is 27.8, ranking 10th.","pages":"434--439","doi":"10.18653\/v1\/W19-5349","url":"https:\/\/www.aclweb.org\/anthology\/W19-5349","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5350","title":"{DFKI}-{NMT} Submission to the {WMT}19 News Translation Task","authors":["Zhang, Jingyi","van Genabith, Josef"],"emails":["hang@dfki.de","enabith@dfki.de"],"author_id":["jingyi-zhang","josef-van-genabith"],"abstract":"This paper describes the DFKI-NMT submission to the WMT19 News translation task. We participated in both English-to-German and German-to-English directions. We trained Transformer models and adopted various techniques for effectively training our models, including data selection, back-translation and in-domain fine-tuning. We give a detailed analysis of the performance of our system.","pages":"440--444","doi":"10.18653\/v1\/W19-5350","url":"https:\/\/www.aclweb.org\/anthology\/W19-5350","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5351","title":"Linguistic Evaluation of {G}erman-{E}nglish Machine Translation Using a Test Suite","authors":["Avramidis, Eleftherios","Macketanz, Vivien","Strohriegel, Ursula","Uszkoreit, Hans"],"emails":["eleftherios.avramidis@dfki.de","vivien.macketanz@dfki.de","ursula.strohriegel@dfki.de","hans.uszkoreit@dfki.de"],"author_id":["eleftherios-avramidis","vivien-macketanz","ursula-strohriegel","hans-uszkoreit"],"abstract":"We present the results of the application of a grammatical test suite for German-to-English MT on the systems submitted at WMT19, with a detailed analysis for 107 phenomena organized in 14 categories. The systems still translate wrong one out of four test items in average. Low performance is indicated for idioms, modals, pseudo-clefts, multi-word expressions and verb valency. When compared to last year, there has been a improvement of function words, non verbal agreement and punctuation. More detailed conclusions about particular systems and phenomena are also presented.","pages":"445--454","doi":"10.18653\/v1\/W19-5351","url":"https:\/\/www.aclweb.org\/anthology\/W19-5351","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5352","title":"A Test Suite and Manual Evaluation of Document-Level {NMT} at {WMT}19","authors":["Rysov{\\'a}, Kate{\\v{r}}ina","Rysov{\\'a}, Magdal{\\'e}na","Musil, Tom{\\'a}{\\v{s}}","Pol{\\'a}kov{\\'a}, Lucie","Bojar, Ond{\\v{r}}ej"],"emails":["rysova@ufal.mff.cuni.cz","magdalena.rysova@ufal.mff.cuni.cz","musil@ufal.mff.cuni.cz","polakova@ufal.mff.cuni.cz","bojar@ufal.mff.cuni.cz"],"author_id":["katerina-rysova","magdalena-rysova","tomas-musil","lucie-polakova","ondrej-bojar"],"abstract":"As the quality of machine translation rises and neural machine translation (NMT) is moving from sentence to document level translations, it is becoming increasingly difficult to evaluate the output of translation systems. We provide a test suite for WMT19 aimed at assessing discourse phenomena of MT systems participating in the News Translation Task. We have manually checked the outputs and identified types of translation errors that are relevant to document-level translation.","pages":"455--463","doi":"10.18653\/v1\/W19-5352","url":"https:\/\/www.aclweb.org\/anthology\/W19-5352","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5353","title":"Evaluating Conjunction Disambiguation on {E}nglish-to-{G}erman and {F}rench-to-{G}erman {WMT} 2019 Translation Hypotheses","authors":["Popovi{\\'c}, Maja"],"emails":["maja.popovic@adaptcentre.ie"],"author_id":["maja-popovic"],"abstract":"We present a test set for evaluating an MT system{'}s capability to translate ambiguous conjunctions depending on the sentence structure. We concentrate on the English conjunction {``}but{''} and its French equivalent {``}mais{''} which can be translated into two different German conjunctions. We evaluate all English-to-German and French-to-German submissions to the WMT 2019 shared translation task. The evaluation is done mainly automatically, with additional fast manual inspection of unclear cases. All systems almost perfectly recognise the target conjunction {``}aber{''}, whereas accuracies for the other target conjunction {``}sondern{''} range from 78{\\%} to 97{\\%}, and the errors are mostly caused by replacing it with the alternative conjunction {``}aber{''}. The best performing system for both language pairs is a multilingual Transformer {``}TartuNLP{''} system trained on all WMT 2019 language pairs which use the Latin script, indicating that the multilingual approach is beneficial for conjunction disambiguation. As for other system features, such as using synthetic back-translated data, context-aware, hybrid, etc., no particular (dis)advantages can be observed. Qualitative manual inspection of translation hypotheses shown that highly ranked systems generally produce translations with high adequacy and fluency, meaning that these systems are not only capable of capturing the right conjunction whereas the rest of the translation hypothesis is poor. On the other hand, the low ranked systems generally exhibit lower fluency and poor adequacy.","pages":"464--469","doi":"10.18653\/v1\/W19-5353","url":"https:\/\/www.aclweb.org\/anthology\/W19-5353","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5354","title":"The {M}u{C}o{W} Test Suite at {WMT} 2019: Automatically Harvested Multilingual Contrastive Word Sense Disambiguation Test Sets for Machine Translation","authors":["Raganato, Alessandro","Scherrer, Yves","Tiedemann, J{\\\"o}rg"],"emails":["alessandro.raganato@helsinki.fi","yves.scherrer@helsinki.fi","j{\\\"o}rg.tiedemann@helsinki.fi"],"author_id":["alessandro-raganato","yves-scherrer","jorg-tiedemann"],"abstract":"Supervised Neural Machine Translation (NMT) systems currently achieve impressive translation quality for many language pairs. One of the key features of a correct translation is the ability to perform word sense disambiguation (WSD), i.e., to translate an ambiguous word with its correct sense. Existing evaluation benchmarks on WSD capabilities of translation systems rely heavily on manual work and cover only few language pairs and few word types. We present MuCoW, a multilingual contrastive test suite that covers 16 language pairs with more than 200 thousand contrastive sentence pairs, automatically built from word-aligned parallel corpora and the wide-coverage multilingual sense inventory of BabelNet. We evaluate the quality of the ambiguity lexicons and of the resulting test suite on all submissions from 9 language pairs presented in the WMT19 news shared translation task, plus on other 5 language pairs using NMT pretrained models. The MuCoW test suite is available at http:\/\/github.com\/Helsinki-NLP\/MuCoW.","pages":"470--480","doi":"10.18653\/v1\/W19-5354","url":"https:\/\/www.aclweb.org\/anthology\/W19-5354","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5355","title":"{SAO} {WMT}19 Test Suite: Machine Translation of Audit Reports","authors":["Vojt{\\v{e}}chov{\\'a}, Tereza","Nov{\\'a}k, Michal","Klou{\\v{c}}ek, Milo{\\v{s}}","Bojar, Ond{\\v{r}}ej"],"emails":["vojtechova@ufal.mff.cuni.cz","mnovak@ufal.mff.cuni.cz","kloucek@ufal.mff.cuni.cz","bojar@ufal.mff.cuni.cz"],"author_id":["tereza-vojtechova","michal-novak","milos-kloucek","ondrej-bojar"],"abstract":"This paper describes a machine translation test set of documents from the auditing domain and its use as one of the {``}test suites{''} in the WMT19 News Translation Task for translation directions involving Czech, English and German. Our evaluation suggests that current MT systems optimized for the general news domain can perform quite well even in the particular domain of audit reports. The detailed manual evaluation however indicates that deep factual knowledge of the domain is necessary. For the naked eye of a non-expert, translations by many systems seem almost perfect and automatic MT evaluation with one reference is practically useless for considering these details. Furthermore, we show on a sample document from the domain of agreements that even the best systems completely fail in preserving the semantics of the agreement, namely the identity of the parties.","pages":"481--493","doi":"10.18653\/v1\/W19-5355","url":"https:\/\/www.aclweb.org\/anthology\/W19-5355","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5356","title":"{WMDO}: Fluency-based Word Mover{'}s Distance for Machine Translation Evaluation","authors":["Chow, Julian","Specia, Lucia","Madhyastha, Pranava"],"emails":["julian.chow16@imperial.ac.uk","l.specia@imperial.ac.uk","pranava@imperial.ac.uk"],"author_id":["julian-chow","lucia-specia","pranava-swaroop-madhyastha"],"abstract":"We propose WMDO, a metric based on distance between distributions in the semantic vector space. Matching in the semantic space has been investigated for translation evaluation, but the constraints of a translation{'}s word order have not been fully explored. Building on the Word Mover{'}s Distance metric and various word embeddings, we introduce a fragmentation penalty to account for fluency of a translation. This word order extension is shown to perform better than standard WMD, with promising results against other types of metrics.","pages":"494--500","doi":"10.18653\/v1\/W19-5356","url":"https:\/\/www.aclweb.org\/anthology\/W19-5356","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5357","title":"Meteor++ 2.0: Adopt Syntactic Level Paraphrase Knowledge into Machine Translation Evaluation","authors":["Guo, Yinuo","Hu, Junfeng"],"emails":["gyn0806@pku.edu.cn","hujf@pku.edu.cn"],"author_id":["yinuo-guo","junfeng-hu"],"abstract":"This paper describes Meteor++ 2.0, our submission to the WMT19 Metric Shared Task. The well known Meteor metric improves machine translation evaluation by introducing paraphrase knowledge. However, it only focuses on the lexical level and utilizes consecutive n-grams paraphrases. In this work, we take into consideration syntactic level paraphrase knowledge, which sometimes may be skip-grams. We describe how such knowledge can be extracted from Paraphrase Database (PPDB) and integrated into Meteor-based metrics. Experiments on WMT15 and WMT17 evaluation datasets show that the newly proposed metric outperforms all previous versions of Meteor.","pages":"501--506","doi":"10.18653\/v1\/W19-5357","url":"https:\/\/www.aclweb.org\/anthology\/W19-5357","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5358","title":"{Y}i{S}i - a Unified Semantic {MT} Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources","authors":["Lo, Chi-kiu"],"emails":["chikiu.lo@nrc-cnrc.gc.ca"],"author_id":["chi-kiu-lo"],"abstract":"We present YiSi, a unified automatic semantic machine translation quality evaluation and estimation metric for languages with different levels of available resources. Underneath the interface with different language resources settings, YiSi uses the same representation for the two sentences in assessment. Besides, we show significant improvement in the correlation of YiSi-1{'}s scores with human judgment is made by using contextual embeddings in multilingual BERT{--}Bidirectional Encoder Representations from Transformers to evaluate lexical semantic similarity. YiSi is open source and publicly available.","pages":"507--513","doi":"10.18653\/v1\/W19-5358","url":"https:\/\/www.aclweb.org\/anthology\/W19-5358","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5359","title":"{EED}: Extended Edit Distance Measure for Machine Translation","authors":["Stanchev, Peter","Wang, Weiyue","Ney, Hermann"],"emails":["","",""],"author_id":["peter-stanchev","weiyue-wang","hermann-ney"],"abstract":"Over the years a number of machine translation metrics have been developed in order to evaluate the accuracy and quality of machine-generated translations. Metrics such as BLEU and TER have been used for decades. However, with the rapid progress of machine translation systems, the need for better metrics is growing. This paper proposes an extension of the edit distance, which achieves better human correlation, whilst remaining fast, flexible and easy to understand.","pages":"514--520","doi":"10.18653\/v1\/W19-5359","url":"https:\/\/www.aclweb.org\/anthology\/W19-5359","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5360","title":"Filtering Pseudo-References by Paraphrasing for Automatic Evaluation of Machine Translation","authors":["Yoshimura, Ryoma","Shimanaka, Hiroki","Matsumura, Yukio","Yamagishi, Hayahide","Komachi, Mamoru"],"emails":["","","","yamagishi-hayahide@ed.tmu.ac.jp","komachi@tmu.ac.jp"],"author_id":["ryoma-yoshimura","hiroki-shimanaka","yukio-matsumura","hayahide-yamagishi","mamoru-komachi"],"abstract":"In this paper, we introduce our participation in the WMT 2019 Metric Shared Task. We propose an improved version of sentence BLEU using filtered pseudo-references. We propose a method to filter pseudo-references by paraphrasing for automatic evaluation of machine translation (MT). We use the outputs of off-the-shelf MT systems as pseudo-references filtered by paraphrasing in addition to a single human reference (gold reference). We use BERT fine-tuned with paraphrase corpus to filter pseudo-references by checking the paraphrasability with the gold reference. Our experimental results of the WMT 2016 and 2017 datasets show that our method achieved higher correlation with human evaluation than the sentence BLEU (SentBLEU) baselines with a single reference and with unfiltered pseudo-references.","pages":"521--525","doi":"10.18653\/v1\/W19-5360","url":"https:\/\/www.aclweb.org\/anthology\/W19-5360","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5361","title":"Naver Labs {E}urope{'}s Systems for the {WMT}19 Machine Translation Robustness Task","authors":["Berard, Alexandre","Calapodescu, Ioan","Roux, Claude"],"emails":["alexandre.berard@naverlabs.com","ioan.calapodescu@naverlabs.com","claude.roux@naverlabs.com"],"author_id":["alexandre-berard1","ioan-calapodescu","claude-roux"],"abstract":"This paper describes the systems that we submitted to the WMT19 Machine Translation robustness task. This task aims to improve MT{'}s robustness to noise found on social media, like informal language, spelling mistakes and other orthographic variations. The organizers provide parallel data extracted from a social media website in two language pairs: French-English and Japanese-English (one for each language direction). The goal is to obtain the best scores on unseen test sets from the same source, according to automatic metrics (BLEU) and human evaluation. We propose one single and one ensemble system for each translation direction. Our ensemble models ranked first in all language pairs, according to BLEU evaluation. We discuss the pre-processing choices that we made, and present our solutions for robustness to noise and domain adaptation.","pages":"526--532","doi":"10.18653\/v1\/W19-5361","url":"https:\/\/www.aclweb.org\/anthology\/W19-5361","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5362","title":"{NICT}{'}s Supervised Neural Machine Translation Systems for the {WMT}19 Translation Robustness Task","authors":["Dabre, Raj","Sumita, Eiichiro"],"emails":["raj.dabre@nict.go.jp","eiichiro.sumita@nict.go.jp"],"author_id":["raj-dabre","eiichiro-sumita"],"abstract":"In this paper we describe our neural machine translation (NMT) systems for Japanese\u2194English translation which we submitted to the translation robustness task. We focused on leveraging transfer learning via fine tuning to improve translation quality. We used a fairly well established domain adaptation technique called Mixed Fine Tuning (MFT) (Chu et. al., 2017) to improve translation quality for Japanese\u2194English. We also trained bi-directional NMT models instead of uni-directional ones as the former are known to be quite robust, especially in low-resource scenarios. However, given the noisy nature of the in-domain training data, the improvements we obtained are rather modest.","pages":"533--536","doi":"10.18653\/v1\/W19-5362","url":"https:\/\/www.aclweb.org\/anthology\/W19-5362","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5363","title":"System Description: The Submission of {FOKUS} to the {WMT} 19 Robustness Task","authors":["Grozea, Cristian"],"emails":["cristian.grozea@fokus.fraunhofer.de"],"author_id":["cristian-grozea"],"abstract":"This paper describes the systems of Fraunhofer FOKUS for the WMT 2019 machine translation robustness task. We have made submissions to the EN-FR, FR-EN, and JA-EN language pairs. The first two were made with a baseline translator, trained on clean data for the WMT 2019 biomedical translation task. These baselines improved over the baselines from the MTNT paper by 2 to 4 BLEU points, but where not trained on the same data. The last one used the same model class and training procedure, with induced typos in the training data to increase the model robustness.","pages":"537--538","doi":"10.18653\/v1\/W19-5363","url":"https:\/\/www.aclweb.org\/anthology\/W19-5363","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5364","title":"{CUNI} System for the {WMT}19 Robustness Task","authors":["Helcl, Jind{\\v{r}}ich","Libovick{\\'y}, Jind{\\v{r}}ich","Popel, Martin"],"emails":["helcl@ufal.mff.cuni.cz","libovicky@ufal.mff.cuni.cz","popel@ufal.mff.cuni.cz"],"author_id":["jindrich-helcl","jindrich-libovicky","martin-popel"],"abstract":"We present our submission to the WMT19 Robustness Task. Our baseline system is the Charles University (CUNI) Transformer system trained for the WMT18 shared task on News Translation. Quantitative results show that the CUNI Transformer system is already far more robust to noisy input than the LSTM-based baseline provided by the task organizers. We further improved the performance of our model by fine-tuning on the in-domain noisy data without influencing the translation quality on the news domain.","pages":"539--543","doi":"10.18653\/v1\/W19-5364","url":"https:\/\/www.aclweb.org\/anthology\/W19-5364","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5365","title":"{NTT}{'}s Machine Translation Systems for {WMT}19 Robustness Task","authors":["Murakami, Soichiro","Morishita, Makoto","Hirao, Tsutomu","Nagata, Masaaki"],"emails":["souichirou.murakami.cr@nttdocomo.com","makoto.morishita.gr@hco.ntt.co.jp","tsutomu.hirao.kp@hco.ntt.co.jp","masaaki.nagata.et@hco.ntt.co.jp"],"author_id":["soichiro-murakami","makoto-morishita","tsutomu-hirao","masaaki-nagata"],"abstract":"This paper describes NTT{'}s submission to the WMT19 robustness task. This task mainly focuses on translating noisy text (e.g., posts on Twitter), which presents different difficulties from typical translation tasks such as news. Our submission combined techniques including utilization of a synthetic corpus, domain adaptation, and a placeholder mechanism, which significantly improved over the previous baseline. Experimental results revealed the placeholder mechanism, which temporarily replaces the non-standard tokens including emojis and emoticons with special placeholder tokens during translation, improves translation accuracy even with noisy texts.","pages":"544--551","doi":"10.18653\/v1\/W19-5365","url":"https:\/\/www.aclweb.org\/anthology\/W19-5365","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5366","title":"{JHU} 2019 Robustness Task System Description","authors":["Post, Matt","Duh, Kevin"],"emails":["",""],"author_id":["matt-post","kevin-duh"],"abstract":"We describe the JHU submissions to the French{--}English, Japanese{--}English, and English{--}Japanese Robustness Task at WMT 2019. Our goal was to evaluate the performance of baseline systems on both the official noisy test set as well as news data, in order to ensure that performance gains in the latter did not come at the expense of general-domain performance. To this end, we built straightforward 6-layer Transformer models and experimented with a handful of variables including subword processing (FR\u2192EN) and a handful of hyperparameters settings (JA\u2194EN). As expected, our systems performed reasonably.","pages":"552--558","doi":"10.18653\/v1\/W19-5366","url":"https:\/\/www.aclweb.org\/anthology\/W19-5366","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5367","title":"Robust Machine Translation with Domain Sensitive Pseudo-Sources: {B}aidu-{OSU} {WMT}19 {MT} Robustness Shared Task System Report","authors":["Zheng, Renjie","Liu, Hairong","Ma, Mingbo","Zheng, Baigong","Huang, Liang"],"emails":["zrenj11@gmail.com","liang.huang.sh@gmail.com","cosmmb@gmail.com","zbgzbg2007@gmail.com","lhrbss@gmail.com"],"author_id":["renjie-zheng","hairong-liu","mingbo-ma","baigong-zheng","liang-huang"],"abstract":"This paper describes the machine translation system developed jointly by Baidu Research and Oregon State University for WMT 2019 Machine Translation Robustness Shared Task. Translation of social media is a very challenging problem, since its style is very different from normal parallel corpora (e.g. News) and also include various types of noises. To make it worse, the amount of social media parallel corpora is extremely limited. In this paper, we use a domain sensitive training method which leverages a large amount of parallel data from popular domains together with a little amount of parallel data from social media. Furthermore, we generate a parallel dataset with pseudo noisy source sentences which are back-translated from monolingual data using a model trained by a similar domain sensitive way. In this way, we achieve more than 10 BLEU improvement in both En-Fr and Fr-En translation compared with the baseline methods.","pages":"559--564","doi":"10.18653\/v1\/W19-5367","url":"https:\/\/www.aclweb.org\/anthology\/W19-5367","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"},{"id":"W19-5368","title":"Improving Robustness of Neural Machine Translation with Multi-task Learning","authors":["Zhou, Shuyan","Zeng, Xiangkai","Zhou, Yingqi","Anastasopoulos, Antonios","Neubig, Graham"],"emails":["shuyanzh@cs.cmu.edu","xiangkaz@cs.cmu.edu","yingqiz@cs.cmu.edu","aanastas@cs.cmu.edu","gneubig@cs.cmu.edu"],"author_id":["shuyan-zhou","xiangkai-zeng","yingqi-zhou","antonios-anastasopoulos","graham-neubig"],"abstract":"While neural machine translation (NMT) achieves remarkable performance on clean, in-domain text, performance is known to degrade drastically when facing text which is full of typos, grammatical errors and other varieties of noise. In this work, we propose a multi-task learning algorithm for transformer-based MT systems that is more resilient to this noise. We describe our submission to the WMT 2019 Robustness shared task based on this method. Our model achieves a BLEU score of 32.8 on the shared task French to English dataset, which is 7.1 BLEU points higher than the baseline vanilla transformer trained with clean text.","pages":"565--571","doi":"10.18653\/v1\/W19-5368","url":"https:\/\/www.aclweb.org\/anthology\/W19-5368","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"}]