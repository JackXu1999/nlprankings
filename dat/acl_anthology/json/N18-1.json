[{"id":"N18-1001","title":"Label-Aware Double Transfer Learning for Cross-Specialty Medical Named Entity Recognition","authors":["Wang, Zhenghui","Qu, Yanru","Chen, Liheng","Shen, Jian","Zhang, Weinan","Zhang, Shaodian","Gao, Yimei","Gu, Gen","Chen, Ken","Yu, Yong"],"emails":["","","chen.ken@synyi.com","shaodian@apex.sjtu.edu.cn","felixwzh@apex.sjtu.edu.cn","wnzhang@apex.sjtu.edu.cn","","","",""],"author_id":["zhenghui-wang","yanru-qu","liheng-chen","jian-shen","weinan-zhang","shaodian-zhang","yimei-gao","gen-gu","ken-chen","yong-yu"],"abstract":"We study the problem of named entity recognition (NER) from electronic medical records, which is one of the most fundamental and critical problems for medical text mining. Medical records which are written by clinicians from different specialties usually contain quite different terminologies and writing styles. The difference of specialties and the cost of human annotation makes it particularly difficult to train a universal medical NER system. In this paper, we propose a label-aware double transfer learning framework (La-DTL) for cross-specialty NER, so that a medical NER system designed for one specialty could be conveniently applied to another one with minimal annotation efforts. The transferability is guaranteed by two components: (i) we propose label-aware MMD for feature representation transfer, and (ii) we perform parameter transfer with a theoretical upper bound which is also label aware. We conduct extensive experiments on 12 cross-specialty NER tasks. The experimental results demonstrate that La-DTL provides consistent accuracy improvement over strong baselines. Besides, the promising experimental results on non-medical NER scenarios indicate that La-DTL is potential to be seamlessly adapted to a wide range of NER tasks.","pages":"1--15","doi":"10.18653\/v1\/N18-1001","url":"https:\/\/www.aclweb.org\/anthology\/N18-1001","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1002","title":"Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss","authors":["Xu, Peng","Barbosa, Denilson"],"emails":["pxu4@ualberta.ca","denilson@ualberta.ca"],"author_id":["peng-xu","denilson-barbosa"],"abstract":"The task of Fine-grained Entity Type Classification (FETC) consists of assigning types from a hierarchy to entity mentions in text. Existing methods rely on distant supervision and are thus susceptible to noisy labels that can be out-of-context or overly-specific for the training sentence. Previous methods that attempt to address these issues do so with heuristics or with the help of hand-crafted features. Instead, we propose an end-to-end solution with a neural network model that uses a variant of cross-entropy loss function to handle out-of-context labels, and hierarchical loss normalization to cope with overly-specific ones. Also, previous work solve FETC a multi-label classification followed by ad-hoc post-processing. In contrast, our solution is more elegant: we use public word embeddings to train a single-label that jointly learns representations for entity mentions and their context. We show experimentally that our approach is robust against noise and consistently outperforms the state-of-the-art on established benchmarks for the task.","pages":"16--25","doi":"10.18653\/v1\/N18-1002","url":"https:\/\/www.aclweb.org\/anthology\/N18-1002","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1003","title":"Joint Bootstrapping Machines for High Confidence Relation Extraction","authors":["Gupta, Pankaj","Roth, Benjamin","Sch{\\\"u}tze, Hinrich"],"emails":["pankaj.gupta@siemens.com","beroth@cis.lmu.de","inquiries@cis.lmu.de"],"author_id":["pankaj-gupta","benjamin-roth","hinrich-schutze"],"abstract":"Semi-supervised bootstrapping techniques for relationship extraction from text iteratively expand a set of initial seed instances. Due to the lack of labeled data, a key challenge in bootstrapping is semantic drift: if a false positive instance is added during an iteration, then all following iterations are contaminated. We introduce BREX, a new bootstrapping method that protects against such contamination by highly effective confidence assessment. This is achieved by using entity and template seeds jointly (as opposed to just one as in previous work), by expanding entities and templates in parallel and in a mutually constraining fashion in each iteration and by introducing higherquality similarity measures for templates. Experimental results show that BREX achieves an F1 that is 0.13 (0.87 vs. 0.74) better than the state of the art for four relationships.","pages":"26--36","doi":"10.18653\/v1\/N18-1003","url":"https:\/\/www.aclweb.org\/anthology\/N18-1003","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1004","title":"A Deep Generative Model of Vowel Formant Typology","authors":["Cotterell, Ryan","Eisner, Jason"],"emails":["ryan.cotterell@jhu.edu","eisner@jhu.edu"],"author_id":["ryan-cotterell","jason-eisner"],"abstract":"What makes some types of languages more probable than others? For instance, we know that almost all spoken languages contain the vowel phoneme \/i\/; why should that be? The field of linguistic typology seeks to answer these questions and, thereby, divine the mechanisms that underlie human language. In our work, we tackle the problem of vowel system typology, i.e., we propose a generative probability model of which vowels a language contains. In contrast to previous work, we work directly with the acoustic information{---}the first two formant values{---}rather than modeling discrete sets of symbols from the international phonetic alphabet. We develop a novel generative probability model and report results on over 200 languages.","pages":"37--46","doi":"10.18653\/v1\/N18-1004","url":"https:\/\/www.aclweb.org\/anthology\/N18-1004","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1005","title":"Fortification of Neural Morphological Segmentation Models for Polysynthetic Minimal-Resource Languages","authors":["Kann, Katharina","Mager Hois, Jesus Manuel","Meza-Ruiz, Ivan Vladimir","Sch{\\\"u}tze, Hinrich"],"emails":["kann@cis.lmu.de","mmager@turing.iimas.unam.mx","ivanvladimir@turing.iimas.unam.mx","inquiries@cislmu.org"],"author_id":["katharina-kann","jesus-manuel-mager-hois","ivan-meza-ruiz","hinrich-schutze"],"abstract":"Morphological segmentation for polysynthetic languages is challenging, because a word may consist of many individual morphemes and training data can be extremely scarce. Since neural sequence-to-sequence (seq2seq) models define the state of the art for morphological segmentation in high-resource settings and for (mostly) European languages, we first show that they also obtain competitive performance for Mexican polysynthetic languages in minimal-resource settings. We then propose two novel multi-task training approaches{---}one with, one without need for external unlabeled resources{---}, and two corresponding data augmentation methods, improving over the neural baseline for all languages. Finally, we explore cross-lingual transfer as a third way to fortify our neural model and show that we can train one single multi-lingual model for related languages while maintaining comparable or even improved performance, thus reducing the amount of parameters by close to 75{\\%}. We provide our morphological segmentation datasets for Mexicanero, Nahuatl, Wixarika and Yorem Nokki for future research.","pages":"47--57","doi":"10.18653\/v1\/N18-1005","url":"https:\/\/www.aclweb.org\/anthology\/N18-1005","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1006","title":"Improving Character-Based Decoding Using Target-Side Morphological Information for Neural Machine Translation","authors":["Passban, Peyman","Liu, Qun","Way, Andy"],"emails":["peyman.passban@adaptcentre.ie","qun.liu@adaptcentre.ie","andy.way@adaptcentre.ie"],"author_id":["peyman-passban","qun-liu","andy-way"],"abstract":"Recently, neural machine translation (NMT) has emerged as a powerful alternative to conventional statistical approaches. However, its performance drops considerably in the presence of morphologically rich languages (MRLs). Neural engines usually fail to tackle the large vocabulary and high out-of-vocabulary (OOV) word rate of MRLs. Therefore, it is not suitable to exploit existing word-based models to translate this set of languages. In this paper, we propose an extension to the state-of-the-art model of Chung et al. (2016), which works at the character level and boosts the decoder with target-side morphological information. In our architecture, an additional morphology table is plugged into the model. Each time the decoder samples from a target vocabulary, the table sends auxiliary signals from the most relevant affixes in order to enrich the decoder{'}s current state and constrain it to provide better predictions. We evaluated our model to translate English into German, Russian, and Turkish as three MRLs and observed significant improvements.","pages":"58--68","doi":"10.18653\/v1\/N18-1006","url":"https:\/\/www.aclweb.org\/anthology\/N18-1006","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1007","title":"Parsing Speech: a Neural Approach to Integrating Lexical and Acoustic-Prosodic Information","authors":["Tran, Trang","Toshniwal, Shubham","Bansal, Mohit","Gimpel, Kevin","Livescu, Karen","Ostendorf, Mari"],"emails":["ttmt001@uw.edu","shtoshni@ttic.edu","mbansal@cs.unc.edu","kgimpel@ttic.edu","klivescu@ttic.edu","ostendor@uw.edu"],"author_id":["trang-tran","shubham-toshniwal","mohit-bansal","kevin-gimpel","karen-livescu","mari-ostendorf"],"abstract":"In conversational speech, the acoustic signal provides cues that help listeners disambiguate difficult parses. For automatically parsing spoken utterances, we introduce a model that integrates transcribed text and acoustic-prosodic features using a convolutional neural network over energy and pitch trajectories coupled with an attention-based recurrent neural network that accepts text and prosodic features. We find that different types of acoustic-prosodic features are individually helpful, and together give statistically significant improvements in parse and disfluency detection F1 scores over a strong text-only baseline. For this study with known sentence boundaries, error analyses show that the main benefit of acoustic-prosodic features is in sentences with disfluencies, attachment decisions are most improved, and transcription errors obscure gains from prosody.","pages":"69--81","doi":"10.18653\/v1\/N18-1007","url":"https:\/\/www.aclweb.org\/anthology\/N18-1007","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1008","title":"Tied Multitask Learning for Neural Speech Translation","authors":["Anastasopoulos, Antonios","Chiang, David"],"emails":["aanastas@nd.edu","dchiang@nd.edu"],"author_id":["antonios-anastasopoulos","david-chiang"],"abstract":"We explore multitask models for neural translation of speech, augmenting them in order to reflect two intuitive notions. First, we introduce a model where the second task decoder receives information from the decoder of the first task, since higher-level intermediate representations should provide useful information. Second, we apply regularization that encourages transitivity and invertibility. We show that the application of these notions on jointly trained models improves performance on the tasks of low-resource speech transcription and translation. It also leads to better performance when using attention information for word discovery over unsegmented input.","pages":"82--91","doi":"10.18653\/v1\/N18-1008","url":"https:\/\/www.aclweb.org\/anthology\/N18-1008","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1009","title":"Please Clap: Modeling Applause in Campaign Speeches","authors":["Gillick, Jon","Bamman, David"],"emails":["jongillick@berkeley.edu","dbamman@berkeley.edu"],"author_id":["jon-gillick","david-bamman"],"abstract":"This work examines the rhetorical techniques that speakers employ during political campaigns. We introduce a new corpus of speeches from campaign events in the months leading up to the 2016 U.S. presidential election and develop new models for predicting moments of audience applause. In contrast to existing datasets, we tackle the challenge of working with transcripts that derive from uncorrected closed captioning, using associated audio recordings to automatically extract and align labels for instances of audience applause. In prediction experiments, we find that lexical features carry the most information, but that a variety of features are predictive, including prosody, long-term contextual dependencies, and theoretically motivated features designed to capture rhetorical techniques.","pages":"92--102","doi":"10.18653\/v1\/N18-1009","url":"https:\/\/www.aclweb.org\/anthology\/N18-1009","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1010","title":"Attentive Interaction Model: Modeling Changes in View in Argumentation","authors":["Jo, Yohan","Poddar, Shivani","Jeon, Byungsoo","Shen, Qinlan","Ros{\\'e}, Carolyn","Neubig, Graham"],"emails":["yohanj@cs.cmu.edu","spoddar2@cs.cmu.edu","byungsoj@cs.cmu.edu","qinlans@cs.cmu.edu","cprose@cs.cmu.edu","gneubig@cs.cmu.edu"],"author_id":["yohan-jo","shivani-poddar","byungsoo-jeon","qinlan-shen","carolyn-rose","graham-neubig"],"abstract":"We present a neural architecture for modeling argumentative dialogue that explicitly models the interplay between an Opinion Holder{'}s (OH{'}s) reasoning and a challenger{'}s argument, with the goal of predicting if the argument successfully changes the OH{'}s view. The model has two components: (1) vulnerable region detection, an attention model that identifies parts of the OH{'}s reasoning that are amenable to change, and (2) interaction encoding, which identifies the relationship between the content of the OH{'}s reasoning and that of the challenger{'}s argument. Based on evaluation on discussions from the Change My View forum on Reddit, the two components work together to predict an OH{'}s change in view, outperforming several baselines. A posthoc analysis suggests that sentences picked out by the attention model are addressed more frequently by successful arguments than by unsuccessful ones.","pages":"103--116","doi":"10.18653\/v1\/N18-1010","url":"https:\/\/www.aclweb.org\/anthology\/N18-1010","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1011","title":"Automatic Focus Annotation: Bringing Formal Pragmatics Alive in Analyzing the Information Structure of Authentic Data","authors":["Ziai, Ramon","Meurers, Detmar"],"emails":["rziai@sfs.uni-tuebingen.de","dm@sfs.uni-tuebingen.de"],"author_id":["ramon-ziai","detmar-meurers"],"abstract":"Analyzing language in context, both from a theoretical and from a computational perspective, is receiving increased interest. Complementing the research in linguistics on discourse and information structure, in computational linguistics identifying discourse concepts was also shown to improve the performance of certain applications, for example, Short Answer Assessment systems (Ziai and Meurers, 2014). Building on the research that established detailed annotation guidelines for manual annotation of information structural concepts for written (Dipper et al., 2007; Ziai and Meurers, 2014) and spoken language data (Calhoun et al., 2010), this paper presents the first approach automating the analysis of focus in authentic written data. Our classification approach combines a range of lexical, syntactic, and semantic features to achieve an accuracy of 78.1{\\%} for identifying focus.","pages":"117--128","doi":"10.18653\/v1\/N18-1011","url":"https:\/\/www.aclweb.org\/anthology\/N18-1011","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1012","title":"Dear Sir or Madam, May {I} Introduce the {GYAFC} Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer","authors":["Rao, Sudha","Tetreault, Joel"],"emails":["raosudha@cs.umd.edu","joel.tetreault@grammarly.com"],"author_id":["sudha-rao","joel-tetreault"],"abstract":"Style transfer is the task of automatically transforming a piece of text in one particular style into another. A major barrier to progress in this field has been a lack of training and evaluation datasets, as well as benchmarks and automatic metrics. In this work, we create the largest corpus for a particular stylistic transfer (formality) and show that techniques from the machine translation community can serve as strong baselines for future work. We also discuss challenges of using automatic metrics.","pages":"129--140","doi":"10.18653\/v1\/N18-1012","url":"https:\/\/www.aclweb.org\/anthology\/N18-1012","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1013","title":"Improving Implicit Discourse Relation Classification by Modeling Inter-dependencies of Discourse Units in a Paragraph","authors":["Dai, Zeyu","Huang, Ruihong"],"emails":["jzdaizeyu@tamu.edu","huangrh@tamu.edu"],"author_id":["zeyu-dai","ruihong-huang"],"abstract":"We argue that semantic meanings of a sentence or clause can not be interpreted independently from the rest of a paragraph, or independently from all discourse relations and the overall paragraph-level discourse structure. With the goal of improving implicit discourse relation classification, we introduce a paragraph-level neural networks that model inter-dependencies between discourse units as well as discourse relation continuity and patterns, and predict a sequence of discourse relations in a paragraph. Experimental results show that our model outperforms the previous state-of-the-art systems on the benchmark corpus of PDTB.","pages":"141--151","doi":"10.18653\/v1\/N18-1013","url":"https:\/\/www.aclweb.org\/anthology\/N18-1013","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1014","title":"A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation","authors":["Juraska, Juraj","Karagiannis, Panagiotis","Bowden, Kevin","Walker, Marilyn"],"emails":["jjuraska@ucsc.edu","pkaragia@ucsc.edu","kkbowden@ucsc.edu","mawalker@ucsc.edu"],"author_id":["juraj-juraska","panagiotis-karagiannis","kevin-bowden","marilyn-walker"],"abstract":"Natural language generation lies at the core of generative dialogue systems and conversational agents. We describe an ensemble neural language generator, and present several novel methods for data representation and augmentation that yield improved results in our model. We test the model on three datasets in the restaurant, TV and laptop domains, and report both objective and subjective evaluations of our best model. Using a range of automatic metrics, as well as human evaluators, we show that our approach achieves better results than state-of-the-art models on the same datasets.","pages":"152--162","doi":"10.18653\/v1\/N18-1014","url":"https:\/\/www.aclweb.org\/anthology\/N18-1014","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1015","title":"A Melody-Conditioned Lyrics Language Model","authors":["Watanabe, Kento","Matsubayashi, Yuichiroh","Fukayama, Satoru","Goto, Masataka","Inui, Kentaro","Nakano, Tomoyasu"],"emails":["kento.w@ecei.tohoku.ac.jp","y-matsu@ecei.tohoku.ac.jp","s.fukayama@aist.go.jp","m.goto@aist.go.jp","inui@ecei.tohoku.ac.jp","t.nakano@aist.go.jp"],"author_id":["kento-watanabe","yuichiroh-matsubayashi","satoru-fukayama","masataka-goto","kentaro-inui","tomoyasu-nakano"],"abstract":"This paper presents a novel, data-driven language model that produces entire lyrics for a given input melody. Previously proposed models for lyrics generation suffer from the inability of capturing the relationship between lyrics and melody partly due to the unavailability of lyrics-melody aligned data. In this study, we first propose a new practical method for creating a large collection of lyrics-melody aligned data and then create a collection of 1,000 lyrics-melody pairs augmented with precise syllable-note alignments and word\/sentence\/paragraph boundaries. We then provide a quantitative analysis of the correlation between word\/sentence\/paragraph boundaries in lyrics and melodies. We then propose an RNN-based lyrics language model conditioned on a featurized melody. Experimental results show that the proposed model generates fluent lyrics while maintaining the compatibility between boundaries of lyrics and melody structures.","pages":"163--172","doi":"10.18653\/v1\/N18-1015","url":"https:\/\/www.aclweb.org\/anthology\/N18-1015","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1016","title":"Discourse-Aware Neural Rewards for Coherent Text Generation","authors":["Bosselut, Antoine","Celikyilmaz, Asli","He, Xiaodong","Gao, Jianfeng","Huang, Po-Sen","Choi, Yejin"],"emails":["antoineb@cs.washington.edu","aslicel@microsoft.com","xiaodong.he@jd.com","jfgao@microsoft.com","pshuang@microsoft.com","yejin@cs.washington.edu"],"author_id":["antoine-bosselut","asli-celikyilmaz","xiaodong-he","jianfeng-gao","po-sen-huang","yejin-choi"],"abstract":"In this paper, we investigate the use of discourse-aware rewards with reinforcement learning to guide a model to generate long, coherent text. In particular, we propose to learn neural rewards to model cross-sentence ordering as a means to approximate desired discourse structure. Empirical results demonstrate that a generator trained with the learned reward produces more coherent and less repetitive text than models trained with cross-entropy or with reinforcement learning with commonly used scores as rewards.","pages":"173--184","doi":"10.18653\/v1\/N18-1016","url":"https:\/\/www.aclweb.org\/anthology\/N18-1016","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1017","title":"Natural Answer Generation with Heterogeneous Memory","authors":["Fu, Yao","Feng, Yansong"],"emails":["francis_yao@pku.edu.cn","fengyansong@pku.edu.cn"],"author_id":["yao-fu","yansong-feng"],"abstract":"Memory augmented encoder-decoder framework has achieved promising progress for natural language generation tasks. Such frameworks enable a decoder to retrieve from a memory during generation. However, less research has been done to take care of the memory contents from different sources, which are often of heterogeneous formats. In this work, we propose a novel attention mechanism to encourage the decoder to actively interact with the memory by taking its heterogeneity into account. Our solution attends across the generated history and memory to explicitly avoid repetition, and introduce related knowledge to enrich our generated sentences. Experiments on the answer sentence generation task show that our method can effectively explore heterogeneous memory to produce readable and meaningful answer sentences while maintaining high coverage for given answer information.","pages":"185--195","doi":"10.18653\/v1\/N18-1017","url":"https:\/\/www.aclweb.org\/anthology\/N18-1017","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1018","title":"Query and Output: Generating Words by Querying Distributed Word Representations for Paraphrase Generation","authors":["Ma, Shuming","Sun, Xu","Li, Wei","Li, Sujian","Li, Wenjie","Ren, Xuancheng"],"emails":["shumingma@pku.edu.cn","xusun@pku.edu.cn","liweitj47@pku.edu.cn","lisujian@pku.edu.cn","cswjli@comp.polyu.edu.hk","renxc@pku.edu.cn"],"author_id":["shuming-ma","xu-sun","wei-li","sujian-li","wenjie-li","xuancheng-ren"],"abstract":"Most recent approaches use the sequence-to-sequence model for paraphrase generation. The existing sequence-to-sequence model tends to memorize the words and the patterns in the training dataset instead of learning the meaning of the words. Therefore, the generated sentences are often grammatically correct but semantically improper. In this work, we introduce a novel model based on the encoder-decoder framework, called Word Embedding Attention Network (WEAN). Our proposed model generates the words by querying distributed word representations (i.e. neural word embeddings), hoping to capturing the meaning of the according words. Following previous work, we evaluate our model on two paraphrase-oriented tasks, namely text simplification and short text abstractive summarization. Experimental results show that our model outperforms the sequence-to-sequence baseline by the BLEU score of 6.3 and 5.5 on two English text simplification datasets, and the ROUGE-2 F1 score of 5.7 on a Chinese summarization dataset. Moreover, our model achieves state-of-the-art performances on these three benchmark datasets.","pages":"196--206","doi":"10.18653\/v1\/N18-1018","url":"https:\/\/www.aclweb.org\/anthology\/N18-1018","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1019","title":"Simplification Using Paraphrases and Context-Based Lexical Substitution","authors":["Kriz, Reno","Miltsakaki, Eleni","Apidianaki, Marianna","Callison-Burch, Chris"],"emails":["rekriz@seas.upenn.edu","eleni@choosito.com","marapi@seas.upenn.edu","ccb@seas.upenn.edu"],"author_id":["reno-kriz","eleni-miltsakaki","marianna-apidianaki","chris-callison-burch"],"abstract":"Lexical simplification involves identifying complex words or phrases that need to be simplified, and recommending simpler meaning-preserving substitutes that can be more easily understood. We propose a complex word identification (CWI) model that exploits both lexical and contextual features, and a simplification mechanism which relies on a word-embedding lexical substitution model to replace the detected complex words with simpler paraphrases. We compare our CWI and lexical simplification models to several baselines, and evaluate the performance of our simplification system against human judgments. The results show that our models are able to detect complex words with higher accuracy than other commonly used methods, and propose good simplification substitutes in context. They also highlight the limited contribution of context features for CWI, which nonetheless improve simplification compared to context-unaware models.","pages":"207--217","doi":"10.18653\/v1\/N18-1019","url":"https:\/\/www.aclweb.org\/anthology\/N18-1019","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1020","title":"Zero-Shot Question Generation from Knowledge Graphs for Unseen Predicates and Entity Types","authors":["Elsahar, Hady","Gravier, Christophe","Laforest, Frederique"],"emails":["hady.elsahar@univ-st-etienne.fr","christophe.gravier@univ-st-etienne.fr","frederique.laforest@univ-st-etienne.fr"],"author_id":["hady-elsahar","christophe-gravier","frederique-laforest"],"abstract":"We present a neural model for question generation from knowledge graphs triples in a {``}Zero-shot{''} setup, that is generating questions for predicate, subject types or object types that were not seen at training time. Our model leverages triples occurrences in the natural language corpus in a encoder-decoder architecture, paired with an original part-of-speech copy action mechanism to generate questions. Benchmark and human evaluation show that our model outperforms state-of-the-art on this task.","pages":"218--228","doi":"10.18653\/v1\/N18-1020","url":"https:\/\/www.aclweb.org\/anthology\/N18-1020","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1021","title":"Automated Essay Scoring in the Presence of Biased Ratings","authors":["Amorim, Evelin","Can{\\c{c}}ado, Marcia","Veloso, Adriano"],"emails":["evelin@dcc.ufmg.br","mcancado@ufmg.br","adrianov@dcc.ufmg.br"],"author_id":["evelin-amorim","marcia-cancado","adriano-veloso"],"abstract":"Studies in Social Sciences have revealed that when people evaluate someone else, their evaluations often reflect their biases. As a result, rater bias may introduce highly subjective factors that make their evaluations inaccurate. This may affect automated essay scoring models in many ways, as these models are typically designed to model (potentially biased) essay raters. While there is sizeable literature on rater effects in general settings, it remains unknown how rater bias affects automated essay scoring. To this end, we present a new annotated corpus containing essays and their respective scores. Different from existing corpora, our corpus also contains comments provided by the raters in order to ground their scores. We present features to quantify rater bias based on their comments, and we found that rater bias plays an important role in automated essay scoring. We investigated the extent to which rater bias affects models based on hand-crafted features. Finally, we propose to rectify the training set by removing essays associated with potentially biased scores while learning the scoring model.","pages":"229--237","doi":"10.18653\/v1\/N18-1021","url":"https:\/\/www.aclweb.org\/anthology\/N18-1021","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1022","title":"Content-Based Citation Recommendation","authors":["Bhagavatula, Chandra","Feldman, Sergey","Power, Russell","Ammar, Waleed"],"emails":["chandrab@allenai.org","sergey@data-cowboys.com","russell.power@gmail.com","waleeda@allenai.org"],"author_id":["chandra-bhagavatula","sergey-feldman","russell-power","waleed-ammar"],"abstract":"We present a content-based method for recommending citations in an academic paper draft. We embed a given query document into a vector space, then use its nearest neighbors as candidates, and rerank the candidates using a discriminative model trained to distinguish between observed and unobserved citations. Unlike previous work, our method does not require metadata such as author names which can be missing, e.g., during the peer review process. Without using metadata, our method outperforms the best reported results on PubMed and DBLP datasets with relative improvements of over 18{\\%} in F1@20 and over 22{\\%} in MRR. We show empirically that, although adding metadata improves the performance on standard metrics, it favors self-citations which are less useful in a citation recommendation setup. We release an online portal for citation recommendation based on our method, (URL: \\url{http:\/\/bit.ly\/citeDemo}) and a new dataset OpenCorpus of 7 million research articles to facilitate future research on this task.","pages":"238--251","doi":"10.18653\/v1\/N18-1022","url":"https:\/\/www.aclweb.org\/anthology\/N18-1022","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1023","title":"Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences","authors":["Khashabi, Daniel","Chaturvedi, Snigdha","Roth, Michael","Upadhyay, Shyam","Roth, Dan"],"emails":["danielkh@cis.upenn.edu","snigdha@ucsc.edu","danroth@cis.upenn.edu","shyamupa@cis.upenn.edu","mroth@coli.uni-sb.de"],"author_id":["daniel-khashabi","snigdha-chaturvedi","michael-roth","shyam-upadhyay","dan-roth"],"abstract":"We present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. We solicit and verify questions and answers for this challenge through a 4-step crowdsourcing experiment. Our challenge dataset contains 6,500+ questions for 1000+ paragraphs across 7 different domains (elementary school science, news, travel guides, fiction stories, etc) bringing in linguistic diversity to the texts and to the questions wordings. On a subset of our dataset, we found human solvers to achieve an F1-score of 88.1{\\%}. We analyze a range of baselines, including a recent state-of-art reading comprehension system, and demonstrate the difficulty of this challenge, despite a high human performance. The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills.","pages":"252--262","doi":"10.18653\/v1\/N18-1023","url":"https:\/\/www.aclweb.org\/anthology\/N18-1023","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1024","title":"Neural Automated Essay Scoring and Coherence Modeling for Adversarially Crafted Input","authors":["Farag, Youmna","Yannakoudakis, Helen","Briscoe, Ted"],"emails":["youmna.farag@cl.cam.ac.uk","helen.yannakoudakis@cl.cam.ac.uk","ted.briscoe@cl.cam.ac.uk"],"author_id":["youmna-farag","helen-yannakoudakis","ted-briscoe"],"abstract":"We demonstrate that current state-of-the-art approaches to Automated Essay Scoring (AES) are not well-suited to capturing adversarially crafted input of grammatical but incoherent sequences of sentences. We develop a neural model of local coherence that can effectively learn connectedness features between sentences, and propose a framework for integrating and jointly training the local coherence model with a state-of-the-art AES model. We evaluate our approach against a number of baselines and experimentally demonstrate its effectiveness on both the AES task and the task of flagging adversarial input, further contributing to the development of an approach that strengthens the validity of neural essay scoring models.","pages":"263--271","doi":"10.18653\/v1\/N18-1024","url":"https:\/\/www.aclweb.org\/anthology\/N18-1024","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1025","title":"{Q}uick{E}dit: Editing Text {\\&} Translations by Crossing Words Out","authors":["Grangier, David","Auli, Michael"],"emails":["",""],"author_id":["david-grangier","michael-auli"],"abstract":"We propose a framework for computer-assisted text editing. It applies to translation post-editing and to paraphrasing. Our proposal relies on very simple interactions: a human editor modifies a sentence by marking tokens they would like the system to change. Our model then generates a new sentence which reformulates the initial sentence by avoiding marked words. The approach builds upon neural sequence-to-sequence modeling and introduces a neural network which takes as input a sentence along with change markers. Our model is trained on translation bitext by simulating post-edits. We demonstrate the advantage of our approach for translation post-editing through simulated post-edits. We also evaluate our model for paraphrasing through a user study.","pages":"272--282","doi":"10.18653\/v1\/N18-1025","url":"https:\/\/www.aclweb.org\/anthology\/N18-1025","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1026","title":"Tempo-Lexical Context Driven Word Embedding for Cross-Session Search Task Extraction","authors":["Sen, Procheta","Ganguly, Debasis","Jones, Gareth"],"emails":["procheta.sen2@mail.dcu.ie","debasis.ganguly1@ie.ibm.com","ones@dcu.ie"],"author_id":["procheta-sen","debasis-ganguly","gareth-jones"],"abstract":"Task extraction is the process of identifying search intents over a set of queries potentially spanning multiple search sessions. Most existing research on task extraction has focused on identifying tasks within a single session, where the notion of a session is defined by a fixed length time window. By contrast, in this work we seek to identify tasks that span across multiple sessions. To identify tasks, we conduct a global analysis of a query log in its entirety without restricting analysis to individual temporal windows. To capture inherent task semantics, we represent queries as vectors in an abstract space. We learn the embedding of query words in this space by leveraging the temporal and lexical contexts of queries. Embedded query vectors are then clustered into tasks. Experiments demonstrate that task extraction effectiveness is improved significantly with our proposed method of query vector embedding in comparison to existing approaches that make use of documents retrieved from a collection to estimate semantic similarities between queries.","pages":"283--292","doi":"10.18653\/v1\/N18-1026","url":"https:\/\/www.aclweb.org\/anthology\/N18-1026","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1027","title":"Zero-Shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens","authors":["Rei, Marek","S{\\o}gaard, Anders"],"emails":["marek.rei@cl.cam.ac.uk","soegaard@di.ku.dk"],"author_id":["marek-rei","anders-sogaard"],"abstract":"Can attention- or gradient-based visualization techniques be used to infer token-level labels for binary sequence tagging problems, using networks trained only on sentence-level labels? We construct a neural network architecture based on soft attention, train it as a binary sentence classifier and evaluate against token-level annotation on four different datasets. Inferring token labels from a network provides a method for quantitatively evaluating what the model is learning, along with generating useful feedback in assistance systems. Our results indicate that attention-based methods are able to predict token-level labels more accurately, compared to gradient-based methods, sometimes even rivaling the supervised oracle network.","pages":"293--302","doi":"10.18653\/v1\/N18-1027","url":"https:\/\/www.aclweb.org\/anthology\/N18-1027","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1028","title":"Variable Typing: Assigning Meaning to Variables in Mathematical Text","authors":["Stathopoulos, Yiannos","Baker, Simon","Rei, Marek","Teufel, Simone"],"emails":["yiannos.stathopoulos@cl.cam.ac.uk","simon.baker@cl.cam.ac.uk","marek.rei@cl.cam.ac.uk","simone.teufel@cl.cam.ac.uk"],"author_id":["yiannos-stathopoulos","simon-baker","marek-rei","simone-teufel"],"abstract":"Information about the meaning of mathematical variables in text is useful in NLP\/IR tasks such as symbol disambiguation, topic modeling and mathematical information retrieval (MIR). We introduce \\textit{variable typing}, the task of assigning one \\textit{mathematical type} (multi-word technical terms referring to mathematical concepts) to each variable in a sentence of mathematical text. As part of this work, we also introduce a new annotated data set composed of 33,524 data points extracted from scientific documents published on arXiv. Our intrinsic evaluation demonstrates that our data set is sufficient to successfully train and evaluate current classifiers from three different model architectures. The best performing model is evaluated on an extrinsic task: MIR, by producing a \\textit{typed formula index}. Our results show that the best performing MIR models make use of our typed index, compared to a formula index only containing raw symbols, thereby demonstrating the usefulness of variable typing.","pages":"303--312","doi":"10.18653\/v1\/N18-1028","url":"https:\/\/www.aclweb.org\/anthology\/N18-1028","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1029","title":"Learning beyond Datasets: Knowledge Graph Augmented Neural Networks for Natural Language Processing","authors":["K M, Annervaz","Basu Roy Chowdhury, Somnath","Dukkipati, Ambedkar"],"emails":["annervaz@iisc.ac.in","brcsomnath@ee.iitkgp.ernet.in","ambedkar@iisc.ac.in"],"author_id":["annervaz-k-m1","somnath-basu-roy-chowdhury","ambedkar-dukkipati"],"abstract":"Machine Learning has been the quintessential solution for many AI problems, but learning models are heavily dependent on specific training data. Some learning models can be incorporated with prior knowledge using a Bayesian setup, but these learning models do not have the ability to access any organized world knowledge on demand. In this work, we propose to enhance learning models with world knowledge in the form of Knowledge Graph (KG) fact triples for Natural Language Processing (NLP) tasks. Our aim is to develop a deep learning model that can extract relevant prior support facts from knowledge graphs depending on the task using attention mechanism. We introduce a convolution-based model for learning representations of knowledge graph entity and relation clusters in order to reduce the attention space. We show that the proposed method is highly scalable to the amount of prior information that has to be processed and can be applied to any generic NLP task. Using this method we show significant improvement in performance for text classification with 20Newsgroups (News20) {\\&} DBPedia datasets, and natural language inference with Stanford Natural Language Inference (SNLI) dataset. We also demonstrate that a deep learning model can be trained with substantially less amount of labeled training data, when it has access to organized world knowledge in the form of a knowledge base.","pages":"313--322","doi":"10.18653\/v1\/N18-1029","url":"https:\/\/www.aclweb.org\/anthology\/N18-1029","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1030","title":"Comparing Constraints for Taxonomic Organization","authors":["Cocos, Anne","Apidianaki, Marianna","Callison-Burch, Chris"],"emails":["acocos@seas.upenn.edu","marapi@seas.upenn.edu","ccb@seas.upenn.edu"],"author_id":["anne-cocos","marianna-apidianaki","chris-callison-burch"],"abstract":"Building a taxonomy from the ground up involves several sub-tasks: selecting terms to include, predicting semantic relations between terms, and selecting a subset of relational instances to keep, given constraints on the taxonomy graph. Methods for this final step {--} taxonomic organization {--} vary both in terms of the constraints they impose, and whether they enable discovery of synonymous terms. It is hard to isolate the impact of these factors on the quality of the resulting taxonomy because organization methods are rarely compared directly. In this paper, we present a head-to-head comparison of six taxonomic organization algorithms that vary with respect to their structural and transitivity constraints, and treatment of synonymy. We find that while transitive algorithms out-perform their non-transitive counterparts, the top-performing transitive algorithm is prohibitively slow for taxonomies with as few as 50 entities. We propose a simple modification to a non-transitive optimum branching algorithm to explicitly incorporate synonymy, resulting in a method that is substantially faster than the best transitive algorithm while giving complementary performance.","pages":"323--333","doi":"10.18653\/v1\/N18-1030","url":"https:\/\/www.aclweb.org\/anthology\/N18-1030","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1031","title":"Improving Lexical Choice in Neural Machine Translation","authors":["Nguyen, Toan","Chiang, David"],"emails":["tnguye28@nd.edu","dchiang@nd.edu"],"author_id":["toan-q-nguyen","david-chiang"],"abstract":"We explore two solutions to the problem of mistranslating rare words in neural machine translation. First, we argue that the standard output layer, which computes the inner product of a vector representing the context with all possible output word embeddings, rewards frequent words disproportionately, and we propose to fix the norms of both vectors to a constant value. Second, we integrate a simple lexical module which is jointly trained with the rest of the model. We evaluate our approaches on eight language pairs with data sizes ranging from 100k to 8M words, and achieve improvements of up to +4.3 BLEU, surpassing phrase-based translation in nearly all settings.","pages":"334--343","doi":"10.18653\/v1\/N18-1031","url":"https:\/\/www.aclweb.org\/anthology\/N18-1031","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1032","title":"Universal Neural Machine Translation for Extremely Low Resource Languages","authors":["Gu, Jiatao","Hassan, Hany","Devlin, Jacob","Li, Victor O.K."],"emails":["jiataogu@eee.hku.hk","hanyh@microsoft.com","vli@eee.hku.hk","jacobdevlin@google.com"],"author_id":["jiatao-gu","hany-hassan","jacob-devlin","victor-o-k-li"],"abstract":"In this paper, we propose a new universal machine translation approach focusing on languages with a limited amount of parallel data. Our proposed approach utilizes a transfer-learning approach to share lexical and sentence level representations across multiple source languages into one target language. The lexical part is shared through a Universal Lexical Representation to support multi-lingual word-level sharing. The sentence-level sharing is represented by a model of experts from all source languages that share the source encoders with all other languages. This enables the low-resource language to utilize the lexical and sentence representations of the higher resource languages. Our approach is able to achieve 23 BLEU on Romanian-English WMT2016 using a tiny parallel corpus of 6k sentences, compared to the 18 BLEU of strong baseline system which uses multi-lingual training and back-translation. Furthermore, we show that the proposed approach can achieve almost 20 BLEU on the same dataset through fine-tuning a pre-trained multi-lingual system in a zero-shot setting.","pages":"344--354","doi":"10.18653\/v1\/N18-1032","url":"https:\/\/www.aclweb.org\/anthology\/N18-1032","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1033","title":"Classical Structured Prediction Losses for Sequence to Sequence Learning","authors":["Edunov, Sergey","Ott, Myle","Auli, Michael","Grangier, David","Ranzato, Marc{'}Aurelio"],"emails":["","","","",""],"author_id":["sergey-edunov","myle-ott","michael-auli","david-grangier","marcaurelio-ranzato"],"abstract":"There has been much recent work on training neural attention models at the sequence-level using either reinforcement learning-style methods or by optimizing the beam. In this paper, we survey a range of classical objective functions that have been widely used to train linear models for structured prediction and apply them to neural sequence to sequence models. Our experiments show that these losses can perform surprisingly well by slightly outperforming beam search optimization in a like for like setup. We also report new state of the art results on both IWSLT{'}14 German-English translation as well as Gigaword abstractive summarization. On the large WMT{'}14 English-French task, sequence-level training achieves 41.5 BLEU which is on par with the state of the art.","pages":"355--364","doi":"10.18653\/v1\/N18-1033","url":"https:\/\/www.aclweb.org\/anthology\/N18-1033","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1034","title":"Deep {D}irichlet Multinomial Regression","authors":["Benton, Adrian","Dredze, Mark"],"emails":["adrian@cs.jhu.edu","mdredze@cs.jhu.edu"],"author_id":["adrian-benton","mark-dredze"],"abstract":"Dirichlet Multinomial Regression (DMR) and other supervised topic models can incorporate arbitrary document-level features to inform topic priors. However, their ability to model corpora are limited by the representation and selection of these features {--} a choice the topic modeler must make. Instead, we seek models that can learn the feature representations upon which to condition topic selection. We present deep Dirichlet Multinomial Regression (dDMR), a generative topic model that simultaneously learns document feature representations and topics. We evaluate dDMR on three datasets: New York Times articles with fine-grained tags, Amazon product reviews with product images, and Reddit posts with subreddit identity. dDMR learns representations that outperform DMR and LDA according to heldout perplexity and are more effective at downstream predictive tasks as the number of topics grows. Additionally, human subjects judge dDMR topics as being more representative of associated document features. Finally, we find that supervision leads to faster convergence as compared to an LDA baseline and that dDMR{'}s model fit is less sensitive to training parameters than DMR.","pages":"365--374","doi":"10.18653\/v1\/N18-1034","url":"https:\/\/www.aclweb.org\/anthology\/N18-1034","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1035","title":"Microblog Conversation Recommendation via Joint Modeling of Topics and Discourse","authors":["Zeng, Xingshan","Li, Jing","Wang, Lu","Beauchamp, Nicholas","Shugars, Sarah","Wong, Kam-Fai"],"emails":["xszeng@se.cuhk.edu.hk","2ameliajli@tencent.com","4luwang@ccs.neu.edu","5n.beauchamp@northeastern.edu","6shugars.s@husky.neu.edu","kfwong@se.cuhk.edu.hk"],"author_id":["xingshan-zeng","jing-li","lu-wang","nicholas-beauchamp","sarah-shugars","kam-fai-wong"],"abstract":"Millions of conversations are generated every day on social media platforms. With limited attention, it is challenging for users to select which discussions they would like to participate in. Here we propose a new method for microblog conversation recommendation. While much prior work has focused on post-level recommendation, we exploit both the conversational context, and user content and behavior preferences. We propose a statistical model that jointly captures: (1) topics for representing user interests and conversation content, and (2) discourse modes for describing user replying behavior and conversation dynamics. Experimental results on two Twitter datasets demonstrate that our system outperforms methods that only model content without considering discourse.","pages":"375--385","doi":"10.18653\/v1\/N18-1035","url":"https:\/\/www.aclweb.org\/anthology\/N18-1035","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1036","title":"Before Name-Calling: Dynamics and Triggers of Ad Hominem Fallacies in Web Argumentation","authors":["Habernal, Ivan","Wachsmuth, Henning","Gurevych, Iryna","Stein, Benno"],"emails":["","","",""],"author_id":["ivan-habernal","henning-wachsmuth","iryna-gurevych","benno-stein"],"abstract":"Arguing without committing a fallacy is one of the main requirements of an ideal debate. But even when debating rules are strictly enforced and fallacious arguments punished, arguers often lapse into attacking the opponent by an ad hominem argument. As existing research lacks solid empirical investigation of the typology of ad hominem arguments as well as their potential causes, this paper fills this gap by (1) performing several large-scale annotation studies, (2) experimenting with various neural architectures and validating our working hypotheses, such as controversy or reasonableness, and (3) providing linguistic insights into triggers of ad hominem using explainable neural network architectures.","pages":"386--396","doi":"10.18653\/v1\/N18-1036","url":"https:\/\/www.aclweb.org\/anthology\/N18-1036","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1037","title":"Scene Graph Parsing as Dependency Parsing","authors":["Wang, Yu-Siang","Liu, Chenxi","Zeng, Xiaohui","Yuille, Alan"],"emails":["b03202047@ntu.edu.tw","cxliu@jhu.edu","xzengaf@connect.ust.hk","alan.yuille@jhu.edu"],"author_id":["yu-siang-wang","chenxi-liu","xiaohui-zeng","alan-yuille"],"abstract":"In this paper, we study the problem of parsing structured knowledge graphs from textual descriptions. In particular, we consider the scene graph representation that considers objects together with their attributes and relations: this representation has been proved useful across a variety of vision and language applications. We begin by introducing an alternative but equivalent edge-centric view of scene graphs that connect to dependency parses. Together with a careful redesign of label and action space, we combine the two-stage pipeline used in prior work (generic dependency parsing followed by simple post-processing) into one, enabling end-to-end training. The scene graphs generated by our learned neural dependency parser achieve an F-score similarity of 49.67{\\%} to ground truth graphs on our evaluation set, surpassing best previous approaches by 5{\\%}. We further demonstrate the effectiveness of our learned parser on image retrieval applications.","pages":"397--407","doi":"10.18653\/v1\/N18-1037","url":"https:\/\/www.aclweb.org\/anthology\/N18-1037","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1038","title":"Learning Visually Grounded Sentence Representations","authors":["Kiela, Douwe","Conneau, Alexis","Jabri, Allan","Nickel, Maximilian"],"emails":["dkiela@fb.com","aconneau@fb.com","ajabri@berkeley.edu","maxn@fb.com"],"author_id":["douwe-kiela","alexis-conneau","allan-jabri","maximilian-nickel"],"abstract":"We investigate grounded sentence representations, where we train a sentence encoder to predict the image features of a given caption{---}i.e., we try to {``}imagine{''} how a sentence would be depicted visually{---}and use the resultant features as sentence representations. We examine the quality of the learned representations on a variety of standard sentence representation quality benchmarks, showing improved performance for grounded models over non-grounded ones. In addition, we thoroughly analyze the extent to which grounding contributes to improved performance, and show that the system also learns improved word embeddings.","pages":"408--418","doi":"10.18653\/v1\/N18-1038","url":"https:\/\/www.aclweb.org\/anthology\/N18-1038","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1039","title":"Comparatives, Quantifiers, Proportions: a Multi-Task Model for the Learning of Quantities from Vision","authors":["Pezzelle, Sandro","Sorodoc, Ionut-Teodor","Bernardi, Raffaella"],"emails":["sandro.pezzelle@unitn.it","ionutteodor.sorodoc@upf.edu","raffaella.bernardi@unitn.it"],"author_id":["sandro-pezzelle","ionut-sorodoc","raffaella-bernardi"],"abstract":"The present work investigates whether different quantification mechanisms (set comparison, vague quantification, and proportional estimation) can be jointly learned from visual scenes by a multi-task computational model. The motivation is that, in humans, these processes underlie the same cognitive, non-symbolic ability, which allows an automatic estimation and comparison of set magnitudes. We show that when information about lower-complexity tasks is available, the higher-level proportional task becomes more accurate than when performed in isolation. Moreover, the multi-task model is able to generalize to unseen combinations of target\/non-target objects. Consistently with behavioral evidence showing the interference of absolute number in the proportional task, the multi-task model no longer works when asked to provide the number of target objects in the scene.","pages":"419--430","doi":"10.18653\/v1\/N18-1039","url":"https:\/\/www.aclweb.org\/anthology\/N18-1039","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1040","title":"Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets","authors":["Chao, Wei-Lun","Hu, Hexiang","Sha, Fei"],"emails":["weilunchao760414@gmail.com","hexiang.frank.hu@gmail.com","feisha@usc.edu"],"author_id":["wei-lun-chao","hexiang-hu","fei-sha"],"abstract":"Visual question answering (Visual QA) has attracted a lot of attention lately, seen essentially as a form of (visual) Turing test that artificial intelligence should strive to achieve. In this paper, we study a crucial component of this task: how can we design good datasets for the task? We focus on the design of multiple-choice based datasets where the learner has to select the right answer from a set of candidate ones including the target (i.e., the correct one) and the decoys (i.e., the incorrect ones). Through careful analysis of the results attained by state-of-the-art learning models and human annotators on existing datasets, we show that the design of the decoy answers has a significant impact on how and what the learning models learn from the datasets. In particular, the resulting learner can ignore the visual information, the question, or both while still doing well on the task. Inspired by this, we propose automatic procedures to remedy such design deficiencies. We apply the procedures to re-construct decoy answers for two popular Visual QA datasets as well as to create a new Visual QA dataset from the Visual Genome project, resulting in the largest dataset for this task. Extensive empirical studies show that the design deficiencies have been alleviated in the remedied datasets and the performance on them is likely a more faithful indicator of the difference among learning models. The datasets are released and publicly available via \\url{http:\/\/www.teds.usc.edu\/website_vqa\/}.","pages":"431--441","doi":"10.18653\/v1\/N18-1040","url":"https:\/\/www.aclweb.org\/anthology\/N18-1040","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1041","title":"Abstract Meaning Representation for Paraphrase Detection","authors":["Issa, Fuad","Damonte, Marco","Cohen, Shay B.","Yan, Xiaohui","Chang, Yi"],"emails":["issa.fuad@gmail.com","m.damonte@sms.ed.ac.uk","scohen@inf.ed.ac.uk","yanxiaohui2@huawei.com","yi.chang@huawei.com"],"author_id":["fuad-issa","marco-damonte","shay-b-cohen","xiaohui-yan","yi-chang"],"abstract":"Abstract Meaning Representation (AMR) parsing aims at abstracting away from the syntactic realization of a sentence, and denote only its meaning in a canonical form. As such, it is ideal for paraphrase detection, a problem in which one is required to specify whether two sentences have the same meaning. We show that na{\\\"\\i}ve use of AMR in paraphrase detection is not necessarily useful, and turn to describe a technique based on latent semantic analysis in combination with AMR parsing that significantly advances state-of-the-art results in paraphrase detection for the Microsoft Research Paraphrase Corpus. Our best results in the transductive setting are 86.6{\\%} for accuracy and 90.0{\\%} for F$_1$ measure.","pages":"442--452","doi":"10.18653\/v1\/N18-1041","url":"https:\/\/www.aclweb.org\/anthology\/N18-1041","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1042","title":"attr2vec: Jointly Learning Word and Contextual Attribute Embeddings with Factorization Machines","authors":["Petroni, Fabio","Plachouras, Vassilis","Nugent, Timothy","Leidner, Jochen L."],"emails":["fabio.petroni@tr.com","vplachouras@fb.com","tim.nugent@tr.com","jochen.leidner@tr.com"],"author_id":["fabio-petroni","vassilis-plachouras","timothy-nugent","jochen-l-leidner"],"abstract":"The widespread use of word embeddings is associated with the recent successes of many natural language processing (NLP) systems. The key approach of popular models such as word2vec and GloVe is to learn dense vector representations from the context of words. More recently, other approaches have been proposed that incorporate different types of contextual information, including topics, dependency relations, n-grams, and sentiment. However, these models typically integrate only limited additional contextual information, and often in ad hoc ways. In this work, we introduce attr2vec, a novel framework for jointly learning embeddings for words and contextual attributes based on factorization machines. We perform experiments with different types of contextual information. Our experimental results on a text classification task demonstrate that using attr2vec to jointly learn embeddings for words and Part-of-Speech (POS) tags improves results compared to learning the embeddings independently. Moreover, we use attr2vec to train dependency-based embeddings and we show that they exhibit higher similarity between functionally related words compared to traditional approaches.","pages":"453--462","doi":"10.18653\/v1\/N18-1042","url":"https:\/\/www.aclweb.org\/anthology\/N18-1042","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1043","title":"Can Network Embedding of Distributional Thesaurus Be Combined with Word Vectors for Better Representation?","authors":["Jana, Abhik","Goyal, Pawan"],"emails":["abhik.jana@iitkgp.ac.in","pawang@cse.iitkgp.ac.in"],"author_id":["abhik-jana","pawan-goyal"],"abstract":"Distributed representations of words learned from text have proved to be successful in various natural language processing tasks in recent times. While some methods represent words as vectors computed from text using predictive model (Word2vec) or dense count based model (GloVe), others attempt to represent these in a distributional thesaurus network structure where the neighborhood of a word is a set of words having adequate context overlap. Being motivated by recent surge of research in network embedding techniques (DeepWalk, LINE, node2vec etc.), we turn a distributional thesaurus network into dense word vectors and investigate the usefulness of distributional thesaurus embedding in improving overall word representation. This is the first attempt where we show that combining the proposed word representation obtained by distributional thesaurus embedding with the state-of-the-art word representations helps in improving the performance by a significant margin when evaluated against NLP tasks like word similarity and relatedness, synonym detection, analogy detection. Additionally, we show that even without using any handcrafted lexical resources we can come up with representations having comparable performance in the word similarity and relatedness tasks compared to the representations where a lexical resource has been used.","pages":"463--473","doi":"10.18653\/v1\/N18-1043","url":"https:\/\/www.aclweb.org\/anthology\/N18-1043","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1044","title":"Deep Neural Models of Semantic Shift","authors":["Rosenfeld, Alex","Erk, Katrin"],"emails":["alexbrosenfeld@gmail.com","katrin.erk@mail.utexas.edu"],"author_id":["alex-rosenfeld","katrin-erk"],"abstract":"Diachronic distributional models track changes in word use over time. In this paper, we propose a deep neural network diachronic distributional model. Instead of modeling lexical change via a time series as is done in previous work, we represent time as a continuous variable and model a word{'}s usage as a function of time. Additionally, we have also created a novel synthetic task which measures a model{'}s ability to capture the semantic trajectory. This evaluation quantitatively measures how well a model captures the semantic trajectory of a word over time. Finally, we explore how well the derivatives of our model can be used to measure the speed of lexical change.","pages":"474--484","doi":"10.18653\/v1\/N18-1044","url":"https:\/\/www.aclweb.org\/anthology\/N18-1044","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1045","title":"Distributional Inclusion Vector Embedding for Unsupervised Hypernymy Detection","authors":["Chang, Haw-Shiuan","Wang, Ziyun","Vilnis, Luke","McCallum, Andrew"],"emails":["hschang@cs.umass.edu","wang-zy14@mails.tsinghua.edu.cn","luke@cs.umass.edu","mccallum@cs.umass.edu"],"author_id":["haw-shiuan-chang","ziyun-wang","luke-vilnis","andrew-mccallum"],"abstract":"Modeling hypernymy, such as poodle is-a dog, is an important generalization aid to many NLP tasks, such as entailment, relation extraction, and question answering. Supervised learning from labeled hypernym sources, such as WordNet, limits the coverage of these models, which can be addressed by learning hypernyms from unlabeled text. Existing unsupervised methods either do not scale to large vocabularies or yield unacceptably poor accuracy. This paper introduces distributional inclusion vector embedding (DIVE), a simple-to-implement unsupervised method of hypernym discovery via per-word non-negative vector embeddings which preserve the inclusion property of word contexts. In experimental evaluations more comprehensive than any previous literature of which we are aware{---}evaluating on 11 datasets using multiple existing as well as newly proposed scoring functions{---}we find that our method provides up to double the precision of previous unsupervised methods, and the highest average performance, using a much more compact word representation, and yielding many new state-of-the-art results.","pages":"485--495","doi":"10.18653\/v1\/N18-1045","url":"https:\/\/www.aclweb.org\/anthology\/N18-1045","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1046","title":"Mining Possessions: Existence, Type and Temporal Anchors","authors":["Chinnappa, Dhivya","Blanco, Eduardo"],"emails":["dhivyainfantchinnappa@my.unt.edu","eduardo.blanco@unt.edu"],"author_id":["dhivya-chinnappa","eduardo-blanco"],"abstract":"This paper presents a corpus and experiments to mine possession relations from text. Specifically, we target alienable and control possessions, and assign temporal anchors indicating when the possession holds between possessor and possessee. We present new annotations for this task, and experimental results using both traditional classifiers and neural networks. Results show that the three subtasks (predicting possession existence, possession type and temporal anchors) can be automated.","pages":"496--505","doi":"10.18653\/v1\/N18-1046","url":"https:\/\/www.aclweb.org\/anthology\/N18-1046","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1047","title":"Neural Tensor Networks with Diagonal Slice Matrices","authors":["Ishihara, Takahiro","Hayashi, Katsuhiko","Manabe, Hitoshi","Shimbo, Masashi","Nagata, Masaaki"],"emails":["ishihara.takahiro.in0@is.naist.jp","khayashi0201@gmail.com","manabe.hitoshi.me0@is.naist.jp","shimbo@is.naist.jp","nagata.masaaki@lab.ntt.co.jp"],"author_id":["takahiro-ishihara","katsuhiko-hayashi","hitoshi-manabe","masashi-shimbo","masaaki-nagata"],"abstract":"Although neural tensor networks (NTNs) have been successful in many NLP tasks, they require a large number of parameters to be estimated, which often leads to overfitting and a long training time. We address these issues by applying eigendecomposition to each slice matrix of a tensor to reduce its number of paramters. First, we evaluate our proposed NTN models on knowledge graph completion. Second, we extend the models to recursive NTNs (RNTNs) and evaluate them on logical reasoning tasks. These experiments show that our proposed models learn better and faster than the original (R)NTNs.","pages":"506--515","doi":"10.18653\/v1\/N18-1047","url":"https:\/\/www.aclweb.org\/anthology\/N18-1047","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1048","title":"Post-Specialisation: Retrofitting Vectors of Words Unseen in Lexical Resources","authors":["Vuli{\\'c}, Ivan","Glava{\\v{s}}, Goran","Mrk{\\v{s}}i{\\'c}, Nikola","Korhonen, Anna"],"emails":["iv250@cam.ac.uk","goran@informatik.uni-mannheim.de","nikola@poly-ai.com","alk23@cam.ac.uk"],"author_id":["ivan-vulic","goran-glavas","nikola-mrksic","anna-korhonen"],"abstract":"Word vector specialisation (also known as retrofitting) is a portable, light-weight approach to fine-tuning arbitrary distributional word vector spaces by injecting external knowledge from rich lexical resources such as WordNet. By design, these post-processing methods only update the vectors of words occurring in external lexicons, leaving the representations of all unseen words intact. In this paper, we show that constraint-driven vector space specialisation can be extended to unseen words. We propose a novel post-specialisation method that: a) preserves the useful linguistic knowledge for seen words; while b) propagating this external signal to unseen words in order to improve their vector representations as well. Our post-specialisation approach explicits a non-linear specialisation function in the form of a deep neural network by learning to predict specialised vectors from their original distributional counterparts. The learned function is then used to specialise vectors of unseen words. This approach, applicable to any post-processing model, yields considerable gains over the initial specialisation models both in intrinsic word similarity tasks, and in two downstream tasks: dialogue state tracking and lexical text simplification. The positive effects persist across three languages, demonstrating the importance of specialising the full vocabulary of distributional word vector spaces.","pages":"516--527","doi":"10.18653\/v1\/N18-1048","url":"https:\/\/www.aclweb.org\/anthology\/N18-1048","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1049","title":"Unsupervised Learning of Sentence Embeddings Using Compositional n-Gram Features","authors":["Pagliardini, Matteo","Gupta, Prakhar","Jaggi, Martin"],"emails":["mpagliardini@iprova.com","prakhar.gupta@epfl.ch","martin.jaggi@epfl.ch"],"author_id":["matteo-pagliardini","prakhar-gupta","martin-jaggi"],"abstract":"The recent tremendous success of unsupervised word embeddings in a multitude of applications raises the obvious question if similar methods could be derived to improve embeddings (i.e. semantic representations) of word sequences as well. We present a simple but efficient unsupervised objective to train distributed representations of sentences. Our method outperforms the state-of-the-art unsupervised models on most benchmark tasks, highlighting the robustness of the produced general-purpose sentence embeddings.","pages":"528--540","doi":"10.18653\/v1\/N18-1049","url":"https:\/\/www.aclweb.org\/anthology\/N18-1049","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1050","title":"Learning Domain Representation for Multi-Domain Sentiment Classification","authors":["Liu, Qi","Zhang, Yue","Liu, Jiangming"],"emails":["jiangming.liu@ed.ac.uk","zhang@sutd.edu.sg",""],"author_id":["qi-liu","yue-zhang","jiangming-liu"],"abstract":"Training data for sentiment analysis are abundant in multiple domains, yet scarce for other domains. It is useful to leveraging data available for all existing domains to enhance performance on different domains. We investigate this problem by learning domain-specific representations of input sentences using neural network. In particular, a descriptor vector is learned for representing each domain, which is used to map adversarially trained domain-general Bi-LSTM input representations into domain-specific representations. Based on this model, we further expand the input representation with exemplary domain knowledge, collected by attending over a memory network of domain training data. Results show that our model outperforms existing methods on multi-domain sentiment analysis significantly, giving the best accuracies on two different benchmarks.","pages":"541--550","doi":"10.18653\/v1\/N18-1050","url":"https:\/\/www.aclweb.org\/anthology\/N18-1050","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1051","title":"Learning Sentence Representations over Tree Structures for Target-Dependent Classification","authors":["Duan, Junwen","Ding, Xiao","Liu, Ting"],"emails":["jwduan@ir.hit.edu.cn","xding@ir.hit.edu.cn","tliu@ir.hit.edu.cn"],"author_id":["junwen-duan","xiao-ding","ting-liu"],"abstract":"Target-dependent classification tasks, such as aspect-level sentiment analysis, perform fine-grained classifications towards specific targets. Semantic compositions over tree structures are promising for such tasks, as they can potentially capture long-distance interactions between targets and their contexts. However, previous work that operates on tree structures resorts to syntactic parsers or Treebank annotations, which are either subject to noise in informal texts or highly expensive to obtain. To address above issues, we propose a reinforcement learning based approach, which automatically induces target-specific sentence representations over tree structures. The underlying model is a RNN encoder-decoder that explores possible binary tree structures and a reward mechanism that encourages structures that improve performances on downstream tasks. We evaluate our approach on two benchmark tasks: firm-specific cumulative abnormal return prediction (based on formal news texts) and aspect-level sentiment analysis (based on informal social media texts). Experimental results show that our model gives superior performances compared to previous work that operates on parsed trees. Moreover, our approach gives some intuitions on how target-specific sentence representations can be achieved from its word constituents.","pages":"551--560","doi":"10.18653\/v1\/N18-1051","url":"https:\/\/www.aclweb.org\/anthology\/N18-1051","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1052","title":"Relevant Emotion Ranking from Text Constrained with Emotion Relationships","authors":["Zhou, Deyu","Yang, Yang","He, Yulan"],"emails":["d.zhou@seu.edu.cn","yyang@seu.edu.cn","y.he@cantab.net"],"author_id":["deyu-zhou","yang-yang","yulan-he"],"abstract":"Text might contain or invoke multiple emotions with varying intensities. As such, emotion detection, to predict multiple emotions associated with a given text, can be cast into a multi-label classification problem. We would like to go one step further so that a ranked list of relevant emotions are generated where top ranked emotions are more intensely associated with text compared to lower ranked emotions, whereas the rankings of irrelevant emotions are not important. A novel framework of relevant emotion ranking is proposed to tackle the problem. In the framework, the objective loss function is designed elaborately so that both emotion prediction and rankings of only relevant emotions can be achieved. Moreover, we observe that some emotions co-occur more often while other emotions rarely co-exist. Such information is incorporated into the framework as constraints to improve the accuracy of emotion detection. Experimental results on two real-world corpora show that the proposed framework can effectively deal with emotion detection and performs remarkably better than the state-of-the-art emotion detection approaches and multi-label learning methods.","pages":"561--571","doi":"10.18653\/v1\/N18-1052","url":"https:\/\/www.aclweb.org\/anthology\/N18-1052","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1053","title":"Solving Data Sparsity for Aspect Based Sentiment Analysis Using Cross-Linguality and Multi-Linguality","authors":["Akhtar, Md Shad","Sawant, Palaash","Sen, Sukanta","Ekbal, Asif","Bhattacharyya, Pushpak"],"emails":["shad.pcs15@iitp.ac.in","palaash77@gmail.com","sukanta.pcs15@iitp.ac.in","asif@iitp.ac.in","pb@iitp.ac.in"],"author_id":["md-shad-akhtar","palaash-sawant","sukanta-sen","asif-ekbal","pushpak-bhattacharyya"],"abstract":"Efficient word representations play an important role in solving various problems related to Natural Language Processing (NLP), data mining, text mining etc. The issue of data sparsity poses a great challenge in creating efficient word representation model for solving the underlying problem. The problem is more intensified in resource-poor scenario due to the absence of sufficient amount of corpus. In this work we propose to minimize the effect of data sparsity by leveraging bilingual word embeddings learned through a parallel corpus. We train and evaluate Long Short Term Memory (LSTM) based architecture for aspect level sentiment classification. The neural network architecture is further assisted by the hand-crafted features for the prediction. We show the efficacy of the proposed model against state-of-the-art methods in two experimental setups i.e. multi-lingual and cross-lingual.","pages":"572--582","doi":"10.18653\/v1\/N18-1053","url":"https:\/\/www.aclweb.org\/anthology\/N18-1053","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1054","title":"{SRL}4{ORL}: Improving Opinion Role Labeling Using Multi-Task Learning with Semantic Role Labeling","authors":["Marasovi{\\'c}, Ana","Frank, Anette"],"emails":["marasovic@cl.uni-heidelberg.de","frank@cl.uni-heidelberg.de"],"author_id":["ana-marasovic","anette-frank"],"abstract":"For over a decade, machine learning has been used to extract opinion-holder-target structures from text to answer the question {``}Who expressed what kind of sentiment towards what?{''}. Recent neural approaches do not outperform the state-of-the-art feature-based models for Opinion Role Labeling (ORL). We suspect this is due to the scarcity of labeled training data and address this issue using different multi-task learning (MTL) techniques with a related task which has substantially more data, i.e. Semantic Role Labeling (SRL). We show that two MTL models improve significantly over the single-task model for labeling of both holders and targets, on the development and the test sets. We found that the vanilla MTL model, which makes predictions using only shared ORL and SRL features, performs the best. With deeper analysis we determine what works and what might be done to make further improvements for ORL.","pages":"583--594","doi":"10.18653\/v1\/N18-1054","url":"https:\/\/www.aclweb.org\/anthology\/N18-1054","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1055","title":"Approaching Neural Grammatical Error Correction as a Low-Resource Machine Translation Task","authors":["Junczys-Dowmunt, Marcin","Grundkiewicz, Roman","Guha, Shubha","Heafield, Kenneth"],"emails":["marcinjd@microsoft.com","rgrundki@inf.ed.ac.uk","sguha@ed-alumni.net","kheafiel@inf.ed.ac.uk"],"author_id":["marcin-junczys-dowmunt","roman-grundkiewicz","shubha-guha","kenneth-heafield"],"abstract":"Previously, neural methods in grammatical error correction (GEC) did not reach state-of-the-art results compared to phrase-based statistical machine translation (SMT) baselines. We demonstrate parallels between neural GEC and low-resource neural MT and successfully adapt several methods from low-resource MT to neural GEC. We further establish guidelines for trustable results in neural GEC and propose a set of model-independent methods for neural GEC that can be easily applied in most GEC settings. Proposed methods include adding source-side noise, domain-adaptation techniques, a GEC-specific training-objective, transfer learning with monolingual data, and ensembling of independently trained GEC models and language models. The combined effects of these methods result in better than state-of-the-art neural GEC models that outperform previously best neural GEC systems by more than 10{\\%} M{\\mbox{$^2$}} on the CoNLL-2014 benchmark and 5.9{\\%} on the JFLEG test set. Non-neural state-of-the-art systems are outperformed by more than 2{\\%} on the CoNLL-2014 benchmark and by 4{\\%} on JFLEG.","pages":"595--606","doi":"10.18653\/v1\/N18-1055","url":"https:\/\/www.aclweb.org\/anthology\/N18-1055","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1056","title":"Robust Cross-Lingual Hypernymy Detection Using Dependency Context","authors":["Upadhyay, Shyam","Vyas, Yogarshi","Carpuat, Marine","Roth, Dan"],"emails":["shyamupa@seas.upenn.edu","yogarshi@cs.umd.edu","marine@cs.umd.edu","danroth@seas.upenn.edu"],"author_id":["shyam-upadhyay","yogarshi-vyas","marine-carpuat","dan-roth"],"abstract":"Cross-lingual Hypernymy Detection involves determining if a word in one language ({``}fruit{''}) is a hypernym of a word in another language ({``}pomme{''} i.e. apple in French). The ability to detect hypernymy cross-lingually can aid in solving cross-lingual versions of tasks such as textual entailment and event coreference. We propose BiSparse-Dep, a family of unsupervised approaches for cross-lingual hypernymy detection, which learns sparse, bilingual word embeddings based on dependency contexts. We show that BiSparse-Dep can significantly improve performance on this task, compared to approaches based only on lexical context. Our approach is also robust, showing promise for low-resource settings: our dependency-based embeddings can be learned using a parser trained on related languages, with negligible loss in performance. We also crowd-source a challenging dataset for this task on four languages {--} Russian, French, Arabic, and Chinese. Our embeddings and datasets are publicly available.","pages":"607--618","doi":"10.18653\/v1\/N18-1056","url":"https:\/\/www.aclweb.org\/anthology\/N18-1056","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1057","title":"Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction","authors":["Xie, Ziang","Genthial, Guillaume","Xie, Stanley","Ng, Andrew","Jurafsky, Dan"],"emails":["zxie@cs.stanford.edu","genthial@cs.stanford.edu","stanxie@cs.stanford.edu","ang@cs.stanford.edu","jurafsky@stanford.edu"],"author_id":["ziang-xie","guillaume-genthial","stanley-xie","andrew-y-ng","dan-jurafsky"],"abstract":"Translation-based methods for grammar correction that directly map noisy, ungrammatical text to their clean counterparts are able to correct a broad range of errors; however, such techniques are bottlenecked by the need for a large parallel corpus of noisy and clean sentence pairs. In this paper, we consider synthesizing parallel data by noising a clean monolingual corpus. While most previous approaches introduce perturbations using features computed from local context windows, we instead develop error generation processes using a neural sequence transduction model trained to translate clean examples to their noisy counterparts. Given a corpus of clean examples, we propose beam search noising procedures to synthesize additional noisy examples that human evaluators were nearly unable to discriminate from nonsynthesized examples. Surprisingly, when trained on additional data synthesized using our best-performing noising scheme, our model approaches the same performance as when trained on additional nonsynthesized data.","pages":"619--628","doi":"10.18653\/v1\/N18-1057","url":"https:\/\/www.aclweb.org\/anthology\/N18-1057","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1058","title":"Self-Training for Jointly Learning to Ask and Answer Questions","authors":["Sachan, Mrinmaya","Xing, Eric"],"emails":["mrinmays@cs.cmu.edu","epxing@cs.cmu.edu"],"author_id":["mrinmaya-sachan","eric-xing"],"abstract":"Building curious machines that can answer as well as ask questions is an important challenge for AI. The two tasks of question answering and question generation are usually tackled separately in the NLP literature. At the same time, both require significant amounts of supervised data which is hard to obtain in many domains. To alleviate these issues, we propose a self-training method for jointly learning to ask as well as answer questions, leveraging unlabeled text along with labeled question answer pairs for learning. We evaluate our approach on four benchmark datasets: SQUAD, MS MARCO, WikiQA and TrecQA, and show significant improvements over a number of established baselines on both question answering and question generation tasks. We also achieved new state-of-the-art results on two competitive answer sentence selection tasks: WikiQA and TrecQA.","pages":"629--640","doi":"10.18653\/v1\/N18-1058","url":"https:\/\/www.aclweb.org\/anthology\/N18-1058","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1059","title":"The Web as a Knowledge-Base for Answering Complex Questions","authors":["Talmor, Alon","Berant, Jonathan"],"emails":["alontalmor@mail.tau.ac.il","joberant@cs.tau.ac.il"],"author_id":["alon-talmor","jonathan-berant"],"abstract":"Answering complex questions is a time-consuming activity for humans that requires reasoning and integration of information. Recent work on reading comprehension made headway in answering simple questions, but tackling complex questions is still an ongoing research challenge. Conversely, semantic parsers have been successful at handling compositionality, but only when the information resides in a target knowledge-base. In this paper, we present a novel framework for answering broad and complex questions, assuming answering simple questions is possible using a search engine and a reading comprehension model. We propose to decompose complex questions into a sequence of simple questions, and compute the final answer from the sequence of answers. To illustrate the viability of our approach, we create a new dataset of complex questions, ComplexWebQuestions, and present a model that decomposes questions and interacts with the web to compute an answer. We empirically demonstrate that question decomposition improves performance from 20.8 precision@1 to 27.5 precision@1 on this new dataset.","pages":"641--651","doi":"10.18653\/v1\/N18-1059","url":"https:\/\/www.aclweb.org\/anthology\/N18-1059","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1060","title":"A Meaning-Based Statistical {E}nglish Math Word Problem Solver","authors":["Liang, Chao-Chun","Wong, Yu-Shiang","Lin, Yi-Chung","Su, Keh-Yih"],"emails":["ccliang@iis.sinica.edu.tw","yushiangwtw@iis.sinica.edu.tw","lyc@iis.sinica.edu.tw","kysu@iis.sinica.edu.tw"],"author_id":["chao-chun-liang","yu-shiang-wong","yi-chung-lin","keh-yih-su"],"abstract":"We introduce MeSys, a meaning-based approach, for solving English math word problems (MWPs) via understanding and reasoning in this paper. It first analyzes the text, transforms both body and question parts into their corresponding logic forms, and then performs inference on them. The associated context of each quantity is represented with proposed role-tags (e.g., nsubj, verb, etc.), which provides the flexibility for annotating an extracted math quantity with its associated context information (i.e., the physical meaning of this quantity). Statistical models are proposed to select the operator and operands. A noisy dataset is designed to assess if a solver solves MWPs mainly via understanding or mechanical pattern matching. Experimental results show that our approach outperforms existing systems on both benchmark datasets and the noisy dataset, which demonstrates that the proposed approach understands the meaning of each quantity in the text more.","pages":"652--662","doi":"10.18653\/v1\/N18-1060","url":"https:\/\/www.aclweb.org\/anthology\/N18-1060","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1061","title":"Fine-Grained Temporal Orientation and its Relationship with Psycho-Demographic Correlates","authors":["Kamila, Sabyasachi","Hasanuzzaman, Mohammed","Ekbal, Asif","Bhattacharyya, Pushpak","Way, Andy"],"emails":["sabysachi.pcs16@iitp.ac.in","","","",""],"author_id":["sabyasachi-kamila","mohammed-hasanuzzaman","asif-ekbal","pushpak-bhattacharyya","andy-way"],"abstract":"Temporal orientation refers to an individual{'}s tendency to connect to the psychological concepts of past, present or future, and it affects personality, motivation, emotion, decision making and stress coping processes. The study of the social media users{'} psycho-demographic attributes from the perspective of human temporal orientation can be of utmost interest and importance to the business and administrative decision makers as it can provide an extra precious information for them to make informed decisions. In this paper, we propose a very first study to demonstrate the association between the sentiment view of the temporal orientation of the users and their different psycho-demographic attributes by analyzing their tweets. We first create a temporal orientation classifier in a minimally supervised way which classifies each tweet of the users in one of the three temporal categories, namely past, present, and future. A deep Bi-directional Long Short Term Memory (BLSTM) is used for the tweet classification task. Our tweet classifier achieves an accuracy of 78.27{\\%} when tested on a manually created test set. We then determine the users{'} overall temporal orientation based on their tweets on the social media. The sentiment is added to the tweets at the fine-grained level where each temporal tweet is given a sentiment with either of the positive, negative or neutral. Our experiment reveals that depending upon the sentiment view of temporal orientation, a user{'}s attributes vary. We finally measure the correlation between the users{'} sentiment view of temporal orientation and their different psycho-demographic factors using regression.","pages":"663--674","doi":"10.18653\/v1\/N18-1061","url":"https:\/\/www.aclweb.org\/anthology\/N18-1061","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1062","title":"Querying Word Embeddings for Similarity and Relatedness","authors":["Torabi Asr, Fatemeh","Zinkov, Robert","Jones, Michael"],"emails":["ftorabia@sfu.ca","zinkov@robots.ox.ac.uk","jonesmn@indiana.edu"],"author_id":["fatemeh-torabi-asr","robert-zinkov","michael-jones"],"abstract":"Word embeddings obtained from neural network models such as Word2Vec Skipgram have become popular representations of word meaning and have been evaluated on a variety of word similarity and relatedness norming data. Skipgram generates a set of word and context embeddings, the latter typically discarded after training. We demonstrate the usefulness of context embeddings in predicting asymmetric association between words from a recently published dataset of production norms (Jouravlev {\\&} McRae, 2016). Our findings suggest that humans respond with words closer to the cue within the context embedding space (rather than the word embedding space), when asked to generate thematically related words.","pages":"675--684","doi":"10.18653\/v1\/N18-1062","url":"https:\/\/www.aclweb.org\/anthology\/N18-1062","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1063","title":"Semantic Structural Evaluation for Text Simplification","authors":["Sulem, Elior","Abend, Omri","Rappoport, Ari"],"emails":["eliors@cs.huji.ac.il","oabend@cs.huji.ac.il","arir@cs.huji.ac.il"],"author_id":["elior-sulem","omri-abend","ari-rappoport"],"abstract":"Current measures for evaluating text simplification systems focus on evaluating lexical text aspects, neglecting its structural aspects. In this paper we propose the first measure to address structural aspects of text simplification, called SAMSA. It leverages recent advances in semantic parsing to assess simplification quality by decomposing the input based on its semantic structure and comparing it to the output. SAMSA provides a reference-less automatic evaluation procedure, avoiding the problems that reference-based methods face due to the vast space of valid simplifications for a given sentence. Our human evaluation experiments show both SAMSA{'}s substantial correlation with human judgments, as well as the deficiency of existing reference-based measures in evaluating structural simplification.","pages":"685--696","doi":"10.18653\/v1\/N18-1063","url":"https:\/\/www.aclweb.org\/anthology\/N18-1063","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1064","title":"Entity Commonsense Representation for Neural Abstractive Summarization","authors":["Amplayo, Reinald Kim","Lim, Seonjae","Hwang, Seung-won"],"emails":["rktamplayo@yonsei.ac.kr","sun.lim@yonsei.ac.kr","seungwonh@yonsei.ac.kr"],"author_id":["reinald-kim-amplayo","seonjae-lim","seung-won-hwang"],"abstract":"A major proportion of a text summary includes important entities found in the original text. These entities build up the topic of the summary. Moreover, they hold commonsense information once they are linked to a knowledge base. Based on these observations, this paper investigates the usage of linked entities to guide the decoder of a neural text summarizer to generate concise and better summaries. To this end, we leverage on an off-the-shelf entity linking system (ELS) to extract linked entities and propose Entity2Topic (E2T), a module easily attachable to a sequence-to-sequence model that transforms a list of entities into a vector representation of the topic of the summary. Current available ELS{'}s are still not sufficiently effective, possibly introducing unresolved ambiguities and irrelevant entities. We resolve the imperfections of the ELS by (a) encoding entities with selective disambiguation, and (b) pooling entity vectors using firm attention. By applying E2T to a simple sequenceto-sequence model with attention mechanism as base model, we see significant improvements of the performance in the Gigaword (sentence to title) and CNN (long document to multi-sentence highlights) summarization datasets by at least 2 ROUGE points.","pages":"697--707","doi":"10.18653\/v1\/N18-1064","url":"https:\/\/www.aclweb.org\/anthology\/N18-1064","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1065","title":"{N}ewsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies","authors":["Grusky, Max","Naaman, Mor","Artzi, Yoav"],"emails":["","",""],"author_id":["max-grusky","mor-naaman","yoav-artzi"],"abstract":"We present NEWSROOM, a summarization dataset of 1.3 million articles and summaries written by authors and editors in newsrooms of 38 major news publications. Extracted from search and social media metadata between 1998 and 2017, these high-quality summaries demonstrate high diversity of summarization styles. In particular, the summaries combine abstractive and extractive strategies, borrowing words and phrases from articles at varying rates. We analyze the extraction strategies used in NEWSROOM summaries against other datasets to quantify the diversity and difficulty of our new data, and train existing methods on the data to evaluate its utility and challenges. The dataset is available online at summari.es.","pages":"708--719","doi":"10.18653\/v1\/N18-1065","url":"https:\/\/www.aclweb.org\/anthology\/N18-1065","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1066","title":"Polyglot Semantic Parsing in {API}s","authors":["Richardson, Kyle","Berant, Jonathan","Kuhn, Jonas"],"emails":["kyle@ims.uni-stuttgart.de","joberant@cs.tau.ac.il","jonas@ims.uni-stuttgart.de"],"author_id":["kyle-richardson","jonathan-berant","jonas-kuhn"],"abstract":"Traditional approaches to semantic parsing (SP) work by training individual models for each available parallel dataset of text-meaning pairs. In this paper, we explore the idea of polyglot semantic translation, or learning semantic parsing models that are trained on multiple datasets and natural languages. In particular, we focus on translating text to code signature representations using the software component datasets of Richardson and Kuhn (2017b,a). The advantage of such models is that they can be used for parsing a wide variety of input natural languages and output programming languages, or mixed input languages, using a single unified model. To facilitate modeling of this type, we develop a novel graph-based decoding framework that achieves state-of-the-art performance on the above datasets, and apply this method to two other benchmark SP tasks.","pages":"720--730","doi":"10.18653\/v1\/N18-1066","url":"https:\/\/www.aclweb.org\/anthology\/N18-1066","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1067","title":"Neural Models of Factuality","authors":["Rudinger, Rachel","White, Aaron Steven","Van Durme, Benjamin"],"emails":["","",""],"author_id":["rachel-rudinger","aaron-steven-white","benjamin-van-durme"],"abstract":"We present two neural models for event factuality prediction, which yield significant performance gains over previous models on three event factuality datasets: FactBank, UW, and MEANTIME. We also present a substantial expansion of the It Happened portion of the Universal Decompositional Semantics dataset, yielding the largest event factuality dataset to date. We report model results on this extended factuality dataset as well.","pages":"731--744","doi":"10.18653\/v1\/N18-1067","url":"https:\/\/www.aclweb.org\/anthology\/N18-1067","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1068","title":"Accurate Text-Enhanced Knowledge Graph Representation Learning","authors":["An, Bo","Chen, Bo","Han, Xianpei","Sun, Le"],"emails":["anbo@iscas.ac.cn","chenbo@iscas.ac.cn","xianpei@iscas.ac.cn","sunle@iscas.ac.cn"],"author_id":["bo-an","bo-chen","xianpei-han","le-sun"],"abstract":"Previous representation learning techniques for knowledge graph representation usually represent the same entity or relation in different triples with the same representation, without considering the ambiguity of relations and entities. To appropriately handle the semantic variety of entities\/relations in distinct triples, we propose an accurate text-enhanced knowledge graph representation learning method, which can represent a relation\/entity with different representations in different triples by exploiting additional textual information. Specifically, our method enhances representations by exploiting the entity descriptions and triple-specific relation mention. And a mutual attention mechanism between relation mention and entity description is proposed to learn more accurate textual representations for further improving knowledge graph representation. Experimental results show that our method achieves the state-of-the-art performance on both link prediction and triple classification tasks, and significantly outperforms previous text-enhanced knowledge representation models.","pages":"745--755","doi":"10.18653\/v1\/N18-1068","url":"https:\/\/www.aclweb.org\/anthology\/N18-1068","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1069","title":"Acquisition of Phrase Correspondences Using Natural Deduction Proofs","authors":["Yanaka, Hitomi","Mineshima, Koji","Mart{\\'\\i}nez-G{\\'o}mez, Pascual","Bekki, Daisuke"],"emails":["hitomiyanaka@g.ecc.u-tokyo.ac.jp","mineshima.koji@ocha.ac.jp","pascual.mg@aist.go.jp","bekki@is.ocha.ac.jp"],"author_id":["hitomi-yanaka","koji-mineshima","pascual-martinez-gomez","daisuke-bekki"],"abstract":"How to identify, extract, and use phrasal knowledge is a crucial problem for the task of Recognizing Textual Entailment (RTE). To solve this problem, we propose a method for detecting paraphrases via natural deduction proofs of semantic relations between sentence pairs. Our solution relies on a graph reformulation of partial variable unifications and an algorithm that induces subgraph alignments between meaning representations. Experiments show that our method can automatically detect various paraphrases that are absent from existing paraphrase databases. In addition, the detection of paraphrases using proof information improves the accuracy of RTE tasks.","pages":"756--766","doi":"10.18653\/v1\/N18-1069","url":"https:\/\/www.aclweb.org\/anthology\/N18-1069","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1070","title":"Automatic Stance Detection Using End-to-End Memory Networks","authors":["Mohtarami, Mitra","Baly, Ramy","Glass, James","Nakov, Preslav","M{\\`a}rquez, Llu{\\'\\i}s","Moschitti, Alessandro"],"emails":["mitra@csail.mit.edu","baly@csail.mit.edu","glass@csail.mit.edu","pnakov@hbku.edu.qa","lluismv@amazon.com","amosch@amazon.com"],"author_id":["mitra-mohtarami","ramy-baly","james-glass","preslav-nakov","lluis-marquez","alessandro-moschitti"],"abstract":"We present an effective end-to-end memory network model that jointly (i) predicts whether a given document can be considered as relevant evidence for a given claim, and (ii) extracts snippets of evidence that can be used to reason about the factuality of the target claim. Our model combines the advantages of convolutional and recurrent neural networks as part of a memory network. We further introduce a similarity matrix at the inference level of the memory network in order to extract snippets of evidence for input claims more accurately. Our experiments on a public benchmark dataset, FakeNewsChallenge, demonstrate the effectiveness of our approach.","pages":"767--776","doi":"10.18653\/v1\/N18-1070","url":"https:\/\/www.aclweb.org\/anthology\/N18-1070","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1071","title":"Collective Entity Disambiguation with Structured Gradient Tree Boosting","authors":["Yang, Yi","Irsoy, Ozan","Rahman, Kazi Shefaet"],"emails":["","","krahman7@bloomberg.net"],"author_id":["yi-yang","ozan-irsoy","kazi-shefaet-rahman"],"abstract":"We present a gradient-tree-boosting-based structured learning model for jointly disambiguating named entities in a document. Gradient tree boosting is a widely used machine learning algorithm that underlies many top-performing natural language processing systems. Surprisingly, most works limit the use of gradient tree boosting as a tool for regular classification or regression problems, despite the structured nature of language. To the best of our knowledge, our work is the first one that employs the structured gradient tree boosting (SGTB) algorithm for collective entity disambiguation. By defining global features over previous disambiguation decisions and jointly modeling them with local features, our system is able to produce globally optimized entity assignments for mentions in a document. Exact inference is prohibitively expensive for our globally normalized model. To solve this problem, we propose Bidirectional Beam Search with Gold path (BiBSG), an approximate inference algorithm that is a variant of the standard beam search algorithm. BiBSG makes use of global information from both past and future to perform better local search. Experiments on standard benchmark datasets show that SGTB significantly improves upon published results. Specifically, SGTB outperforms the previous state-of-the-art neural system by near 1{\\%} absolute accuracy on the popular AIDA-CoNLL dataset.","pages":"777--786","doi":"10.18653\/v1\/N18-1071","url":"https:\/\/www.aclweb.org\/anthology\/N18-1071","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1072","title":"{D}eep{A}lignment: Unsupervised Ontology Matching with Refined Word Vectors","authors":["Kolyvakis, Prodromos","Kalousis, Alexandros","Kiritsis, Dimitris"],"emails":["prodromos.kolyvakis@epfl.ch","alexandros.kalousis@hesge.ch","dimitris.kiritsis@epfl.ch"],"author_id":["prodromos-kolyvakis","alexandros-kalousis","dimitris-kiritsis"],"abstract":"Ontologies compartmentalize types and relations in a target domain and provide the semantic backbone needed for a plethora of practical applications. Very often different ontologies are developed independently for the same domain. Such {``}parallel{''} ontologies raise the need for a process that will establish alignments between their entities in order to unify and extend the existing knowledge. In this work, we present a novel entity alignment method which we dub DeepAlignment. DeepAlignment refines pre-trained word vectors aiming at deriving ontological entity descriptions which are tailored to the ontology matching task. The absence of explicit information relevant to the ontology matching task during the refinement process makes DeepAlignment completely unsupervised. We empirically evaluate our method using standard ontology matching benchmarks. We present significant performance improvements over the current state-of-the-art, demonstrating the advantages that representation learning techniques bring to ontology matching.","pages":"787--798","doi":"10.18653\/v1\/N18-1072","url":"https:\/\/www.aclweb.org\/anthology\/N18-1072","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1073","title":"Efficient Sequence Learning with Group Recurrent Networks","authors":["Gao, Fei","Wu, Lijun","Zhao, Li","Qin, Tao","Cheng, Xueqi","Liu, Tie-Yan"],"emails":["feiga@microsoft.com","wulijun3@mail2.sysu.edu.cn","lizo@microsoft.com","taoqin@microsoft.com","cxq@ict.ac.cn","tie-yan.liu@microsoft.com"],"author_id":["fei-gao","lijun-wu","li-zhao","tao-qin","xueqi-cheng","tie-yan-liu"],"abstract":"Recurrent neural networks have achieved state-of-the-art results in many artificial intelligence tasks, such as language modeling, neural machine translation, speech recognition and so on. One of the key factors to these successes is big models. However, training such big models usually takes days or even weeks of time even if using tens of GPU cards. In this paper, we propose an efficient architecture to improve the efficiency of such RNN model training, which adopts the group strategy for recurrent layers, while exploiting the representation rearrangement strategy between layers as well as time steps. To demonstrate the advantages of our models, we conduct experiments on several datasets and tasks. The results show that our architecture achieves comparable or better accuracy comparing with baselines, with a much smaller number of parameters and at a much lower computational cost.","pages":"799--808","doi":"10.18653\/v1\/N18-1073","url":"https:\/\/www.aclweb.org\/anthology\/N18-1073","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1074","title":"{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification","authors":["Thorne, James","Vlachos, Andreas","Christodoulopoulos, Christos","Mittal, Arpit"],"emails":["j.thorne@sheffield.ac.uk","a.vlachos@sheffield.ac.uk","chrchrs@amazon.co.uk","mitarpit@amazon.co.uk"],"author_id":["james-thorne","andreas-vlachos","christos-christodoulopoulos","arpit-mittal"],"abstract":"In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87{\\%}, while if we ignore the evidence we achieve 50.91{\\%}. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.","pages":"809--819","doi":"10.18653\/v1\/N18-1074","url":"https:\/\/www.aclweb.org\/anthology\/N18-1074","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1075","title":"Global Relation Embedding for Relation Extraction","authors":["Su, Yu","Liu, Honglei","Yavuz, Semih","G{\\\"u}r, Izzeddin","Sun, Huan","Yan, Xifeng"],"emails":["ysu@cs.ucsb.edu","honglei@cs.ucsb.edu","syavuz@cs.ucsb.edu","izzeddingur@cs.ucsb.edu","sun.397@osu.edu","xyan@cs.ucsb.edu"],"author_id":["yu-su","honglei-liu","semih-yavuz","izzeddin-gur1","huan-sun","xifeng-yan"],"abstract":"We study the problem of textual relation embedding with distant supervision. To combat the wrong labeling problem of distant supervision, we propose to embed textual relations with global statistics of relations, i.e., the co-occurrence statistics of textual and knowledge base relations collected from the entire corpus. This approach turns out to be more robust to the training noise introduced by distant supervision. On a popular relation extraction dataset, we show that the learned textual relation embedding can be used to augment existing relation extraction models and significantly improve their performance. Most remarkably, for the top 1,000 relational facts discovered by the best existing model, the precision can be improved from 83.9{\\%} to 89.3{\\%}.","pages":"820--830","doi":"10.18653\/v1\/N18-1075","url":"https:\/\/www.aclweb.org\/anthology\/N18-1075","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1076","title":"Implicit Argument Prediction with Event Knowledge","authors":["Cheng, Pengxiang","Erk, Katrin"],"emails":["pxcheng@cs.utexas.edu","katrin.erk@mail.utexas.edu"],"author_id":["pengxiang-cheng","katrin-erk"],"abstract":"Implicit arguments are not syntactically connected to their predicates, and are therefore hard to extract. Previous work has used models with large numbers of features, evaluated on very small datasets. We propose to train models for implicit argument prediction on a simple cloze task, for which data can be generated automatically at scale. This allows us to use a neural model, which draws on narrative coherence and entity salience for predictions. We show that our model has superior performance on both synthetic and natural data.","pages":"831--840","doi":"10.18653\/v1\/N18-1076","url":"https:\/\/www.aclweb.org\/anthology\/N18-1076","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1077","title":"Improving Temporal Relation Extraction with a Globally Acquired Statistical Resource","authors":["Ning, Qiang","Wu, Hao","Peng, Haoruo","Roth, Dan"],"emails":["qning2@illinois.edu","haowu4@seas.upenn.edu","hpeng7@illinois.edu","danroth@seas.upenn.edu"],"author_id":["qiang-ning","hao-wu","haoruo-peng","dan-roth"],"abstract":"Extracting temporal relations (before, after, overlapping, etc.) is a key aspect of understanding events described in natural language. We argue that this task would gain from the availability of a resource that provides prior knowledge in the form of the temporal order that events usually follow. This paper develops such a resource {--} a probabilistic knowledge base acquired in the news domain {--} by extracting temporal relations between events from the New York Times (NYT) articles over a 20-year span (1987{--}2007). We show that existing temporal extraction systems can be improved via this resource. As a byproduct, we also show that interesting statistics can be retrieved from this resource, which can potentially benefit other time-aware tasks. The proposed system and resource are both publicly available.","pages":"841--851","doi":"10.18653\/v1\/N18-1077","url":"https:\/\/www.aclweb.org\/anthology\/N18-1077","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1078","title":"Multimodal Named Entity Recognition for Short Social Media Posts","authors":["Moon, Seungwhan","Neves, Leonardo","Carvalho, Vitor"],"emails":["seungwhm@cs.cmu.edu","lneves@snap.com","carvalho@intuit.com"],"author_id":["seungwhan-moon","leonardo-neves","vitor-carvalho"],"abstract":"We introduce a new task called Multimodal Named Entity Recognition (MNER) for noisy user-generated data such as tweets or Snapchat captions, which comprise short text with accompanying images. These social media posts often come in inconsistent or incomplete syntax and lexical notations with very limited surrounding textual contexts, bringing significant challenges for NER. To this end, we create a new dataset for MNER called SnapCaptions (Snapchat image-caption pairs submitted to public and crowd-sourced stories with fully annotated named entities). We then build upon the state-of-the-art Bi-LSTM word\/character based NER models with 1) a deep image network which incorporates relevant visual context to augment textual information, and 2) a generic \\textit{modality-attention} module which learns to attenuate irrelevant modalities while amplifying the most informative ones to extract contexts from, adaptive to each sample and token. The proposed MNER model with modality attention significantly outperforms the state-of-the-art text-only NER models by successfully leveraging provided visual contexts, opening up potential applications of MNER on myriads of social media platforms.","pages":"852--860","doi":"10.18653\/v1\/N18-1078","url":"https:\/\/www.aclweb.org\/anthology\/N18-1078","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1079","title":"Nested Named Entity Recognition Revisited","authors":["Katiyar, Arzoo","Cardie, Claire"],"emails":["arzoo@cs.cornell.edu","cardie@cs.cornell.edu"],"author_id":["arzoo-katiyar","claire-cardie"],"abstract":"We propose a novel recurrent neural network-based approach to simultaneously handle nested named entity recognition and nested entity mention detection. The model learns a hypergraph representation for nested entities using features extracted from a recurrent neural network. In evaluations on three standard data sets, we show that our approach significantly outperforms existing state-of-the-art methods, which are feature-based. The approach is also efficient: it operates linearly in the number of tokens and the number of possible output labels at any token. Finally, we present an extension of our model that jointly learns the head of each entity mention.","pages":"861--871","doi":"10.18653\/v1\/N18-1079","url":"https:\/\/www.aclweb.org\/anthology\/N18-1079","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1080","title":"Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction","authors":["Verga, Patrick","Strubell, Emma","McCallum, Andrew"],"emails":["pat@cs.umass.edu","strubell@cs.umass.edu","mccallum@cs.umass.edu"],"author_id":["patrick-verga","emma-strubell","andrew-mccallum"],"abstract":"Most work in relation extraction forms a prediction by looking at a short span of text within a single sentence containing a single entity pair mention. This approach often does not consider interactions across mentions, requires redundant computation for each mention pair, and ignores relationships expressed across sentence boundaries. These problems are exacerbated by the document- (rather than sentence-) level annotation common in biological text. In response, we propose a model which simultaneously predicts relationships between all mention pairs in a document. We form pairwise predictions over entire paper abstracts using an efficient self-attention encoder. All-pairs mention scores allow us to perform multi-instance learning by aggregating over mentions to form entity pair representations. We further adapt to settings without mention-level annotation by jointly training to predict named entities and adding a corpus of weakly labeled data. In experiments on two Biocreative benchmark datasets, we achieve state of the art performance on the Biocreative V Chemical Disease Relation dataset for models without external KB resources. We also introduce a new dataset an order of magnitude larger than existing human-annotated biological information extraction datasets and more accurate than distantly supervised alternatives.","pages":"872--884","doi":"10.18653\/v1\/N18-1080","url":"https:\/\/www.aclweb.org\/anthology\/N18-1080","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1081","title":"Supervised Open Information Extraction","authors":["Stanovsky, Gabriel","Michael, Julian","Zettlemoyer, Luke","Dagan, Ido"],"emails":["gabis@cs.washington.edu","julianjm@cs.washington.edu","lsz@cs.washington.edu","dagan@cs.biu.ac.il"],"author_id":["gabriel-stanovsky","julian-michael","luke-zettlemoyer","ido-dagan"],"abstract":"We present data and methods that enable a supervised learning approach to Open Information Extraction (Open IE). Central to the approach is a novel formulation of Open IE as a sequence tagging problem, addressing challenges such as encoding multiple extractions for a predicate. We also develop a bi-LSTM transducer, extending recent deep Semantic Role Labeling models to extract Open IE tuples and provide confidence scores for tuning their precision-recall tradeoff. Furthermore, we show that the recently released Question-Answer Meaning Representation dataset can be automatically converted into an Open IE corpus which significantly increases the amount of available training data. Our supervised model outperforms the existing state-of-the-art Open IE systems on benchmark datasets.","pages":"885--895","doi":"10.18653\/v1\/N18-1081","url":"https:\/\/www.aclweb.org\/anthology\/N18-1081","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1082","title":"Embedding Syntax and Semantics of Prepositions via Tensor Decomposition","authors":["Gong, Hongyu","Bhat, Suma","Viswanath, Pramod"],"emails":["hgong6@illinois.edu","spbhat2@illinois.edu","pramodv@illinois.edu"],"author_id":["hongyu-gong","suma-bhat","pramod-viswanath"],"abstract":"Prepositions are among the most frequent words in English and play complex roles in the syntax and semantics of sentences. Not surprisingly, they pose well-known difficulties in automatic processing of sentences (prepositional attachment ambiguities and idiosyncratic uses in phrases). Existing methods on preposition representation treat prepositions no different from content words (e.g., word2vec and GloVe). In addition, recent studies aiming at solving prepositional attachment and preposition selection problems depend heavily on external linguistic resources and use dataset-specific word representations. In this paper we use \\textit{word-triple} counts (one of the triples being a preposition) to capture a preposition{'}s interaction with its attachment and complement. We then derive preposition embeddings via tensor decomposition on a large unlabeled corpus. We reveal a new geometry involving Hadamard products and empirically demonstrate its utility in paraphrasing phrasal verbs. Furthermore, our preposition embeddings are used as simple features in two challenging downstream tasks: preposition selection and prepositional attachment disambiguation. We achieve results comparable to or better than the state-of-the-art on multiple standardized datasets.","pages":"896--906","doi":"10.18653\/v1\/N18-1082","url":"https:\/\/www.aclweb.org\/anthology\/N18-1082","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1083","title":"From Phonology to Syntax: Unsupervised Linguistic Typology at Different Levels with Language Embeddings","authors":["Bjerva, Johannes","Augenstein, Isabelle"],"emails":["bjerva@di.ku.dk","augenstein@di.ku.dk"],"author_id":["johannes-bjerva","isabelle-augenstein"],"abstract":"A core part of linguistic typology is the classification of languages according to linguistic properties, such as those detailed in the World Atlas of Language Structure (WALS). Doing this manually is prohibitively time-consuming, which is in part evidenced by the fact that only 100 out of over 7,000 languages spoken in the world are fully covered in WALS. We learn distributed language representations, which can be used to predict typological properties on a massively multilingual scale. Additionally, quantitative and qualitative analyses of these language embeddings can tell us how language similarities are encoded in NLP models for tasks at different typological levels. The representations are learned in an unsupervised manner alongside tasks at three typological levels: phonology (grapheme-to-phoneme prediction, and phoneme reconstruction), morphology (morphological inflection), and syntax (part-of-speech tagging). We consider more than 800 languages and find significant differences in the language representations encoded, depending on the target task. For instance, although Norwegian Bokm{\\aa}l and Danish are typologically close to one another, they are phonologically distant, which is reflected in their language embeddings growing relatively distant in a phonological task. We are also able to predict typological features in WALS with high accuracies, even for unseen language families.","pages":"907--916","doi":"10.18653\/v1\/N18-1083","url":"https:\/\/www.aclweb.org\/anthology\/N18-1083","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1084","title":"Monte {C}arlo Syntax Marginals for Exploring and Using Dependency Parses","authors":["Keith, Katherine","Blodgett, Su Lin","O{'}Connor, Brendan"],"emails":["kkeith@cs.umass.edu","blodgett@cs.umass.edu","brenocon@cs.umass.edu"],"author_id":["katherine-keith","su-lin-blodgett","brendan-oconnor"],"abstract":"Dependency parsing research, which has made significant gains in recent years, typically focuses on improving the accuracy of single-tree predictions. However, ambiguity is inherent to natural language syntax, and communicating such ambiguity is important for error analysis and better-informed downstream applications. In this work, we propose a transition sampling algorithm to sample from the full joint distribution of parse trees defined by a transition-based parsing model, and demonstrate the use of the samples in probabilistic dependency analysis. First, we define the new task of dependency path prediction, inferring syntactic substructures over part of a sentence, and provide the first analysis of performance on this task. Second, we demonstrate the usefulness of our Monte Carlo syntax marginal method for parser error analysis and calibration. Finally, we use this method to propagate parse uncertainty to two downstream information extraction applications: identifying persons killed by police and semantic role assignment.","pages":"917--928","doi":"10.18653\/v1\/N18-1084","url":"https:\/\/www.aclweb.org\/anthology\/N18-1084","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1085","title":"Neural Particle Smoothing for Sampling from Conditional Sequence Models","authors":["Lin, Chu-Cheng","Eisner, Jason"],"emails":["kitsing@cs.jhu.edu","jason@cs.jhu.edu"],"author_id":["chu-cheng-lin","jason-eisner"],"abstract":"We introduce neural particle smoothing, a sequential Monte Carlo method for sampling annotations of an input string from a given probability model. In contrast to conventional particle filtering algorithms, we train a proposal distribution that looks ahead to the end of the input string by means of a right-to-left LSTM. We demonstrate that this innovation can improve the quality of the sample. To motivate our formal choices, we explain how neural transduction models and our sampler can be viewed as low-dimensional but nonlinear approximations to working with HMMs over very large state spaces.","pages":"929--941","doi":"10.18653\/v1\/N18-1085","url":"https:\/\/www.aclweb.org\/anthology\/N18-1085","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1086","title":"Neural Syntactic Generative Models with Exact Marginalization","authors":["Buys, Jan","Blunsom, Phil"],"emails":["jbuys@cs.washington.edu","phil.blunsom@cs.ox.ac.uk"],"author_id":["jan-buys","phil-blunsom"],"abstract":"We present neural syntactic generative models with exact marginalization that support both dependency parsing and language modeling. Exact marginalization is made tractable through dynamic programming over shift-reduce parsing and minimal RNN-based feature sets. Our algorithms complement previous approaches by supporting batched training and enabling online computation of next word probabilities. For supervised dependency parsing, our model achieves a state-of-the-art result among generative approaches. We also report empirical results on unsupervised syntactic models and their role in language modeling. We find that our model formulation of latent dependencies with exact marginalization do not lead to better intrinsic language modeling performance than vanilla RNNs, and that parsing accuracy is not correlated with language modeling perplexity in stack-based models.","pages":"942--952","doi":"10.18653\/v1\/N18-1086","url":"https:\/\/www.aclweb.org\/anthology\/N18-1086","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1087","title":"Noise-Robust Morphological Disambiguation for Dialectal {A}rabic","authors":["Zalmout, Nasser","Erdmann, Alexander","Habash, Nizar"],"emails":["nasser.zalmout@nyu.edu","ae1541@nyu.edu","nizar.habash@nyu.edu"],"author_id":["nasser-zalmout","alexander-erdmann","nizar-habash"],"abstract":"User-generated text tends to be noisy with many lexical and orthographic inconsistencies, making natural language processing (NLP) tasks more challenging. The challenging nature of noisy text processing is exacerbated for dialectal content, where in addition to spelling and lexical differences, dialectal text is characterized with morpho-syntactic and phonetic variations. These issues increase sparsity in NLP models and reduce accuracy. We present a neural morphological tagging and disambiguation model for Egyptian Arabic, with various extensions to handle noisy and inconsistent content. Our models achieve about 5{\\%} relative error reduction (1.1{\\%} absolute improvement) for full morphological analysis, and around 22{\\%} relative error reduction (1.8{\\%} absolute improvement) for part-of-speech tagging, over a state-of-the-art baseline.","pages":"953--964","doi":"10.18653\/v1\/N18-1087","url":"https:\/\/www.aclweb.org\/anthology\/N18-1087","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1088","title":"Parsing Tweets into Universal Dependencies","authors":["Liu, Yijia","Zhu, Yi","Che, Wanxiang","Qin, Bing","Schneider, Nathan","Smith, Noah A."],"emails":["yjliu@ir.hit.edu.cn","yz568@cam.ac.uk","","","",""],"author_id":["yijia-liu","yi-zhu","wanxiang-che","bing-qin","nathan-schneider","noah-a-smith"],"abstract":"We study the problem of analyzing tweets with universal dependencies (UD). We extend the UD guidelines to cover special constructions in tweets that affect tokenization, part-of-speech tagging, and labeled dependencies. Using the extended guidelines, we create a new tweet treebank for English (Tweebank v2) that is four times larger than the (unlabeled) Tweebank v1 introduced by Kong et al. (2014). We characterize the disagreements between our annotators and show that it is challenging to deliver consistent annotation due to ambiguity in understanding and explaining tweets. Nonetheless, using the new treebank, we build a pipeline system to parse raw tweets into UD. To overcome the annotation noise without sacrificing computational efficiency, we propose a new method to distill an ensemble of 20 transition-based parsers into a single one. Our parser achieves an improvement of 2.2 in LAS over the un-ensembled baseline and outperforms parsers that are state-of-the-art on other treebanks in both accuracy and speed.","pages":"965--975","doi":"10.18653\/v1\/N18-1088","url":"https:\/\/www.aclweb.org\/anthology\/N18-1088","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1089","title":"Robust Multilingual Part-of-Speech Tagging via Adversarial Training","authors":["Yasunaga, Michihiro","Kasai, Jungo","Radev, Dragomir"],"emails":["michihiro.yasunaga@yale.edu","jungo.kasai@yale.edu","dragomir.radev@yale.edu"],"author_id":["michihiro-yasunaga","jungo-kasai","dragomir-radev"],"abstract":"Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare \/ unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks.","pages":"976--986","doi":"10.18653\/v1\/N18-1089","url":"https:\/\/www.aclweb.org\/anthology\/N18-1089","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1090","title":"Universal Dependency Parsing for {H}indi-{E}nglish Code-Switching","authors":["Bhat, Irshad","Bhat, Riyaz A.","Shrivastava, Manish","Sharma, Dipti"],"emails":["irshad.bhat@iiit.ac.in","rbhat@interactions.com","m.shrivastava@iiit.ac.in","dipti@iiit.ac.in"],"author_id":["irshad-bhat","riyaz-ahmad-bhat","manish-shrivastava","dipti-misra-sharma"],"abstract":"Code-switching is a phenomenon of mixing grammatical structures of two or more languages under varied social constraints. The code-switching data differ so radically from the benchmark corpora used in NLP community that the application of standard technologies to these data degrades their performance sharply. Unlike standard corpora, these data often need to go through additional processes such as language identification, normalization and\/or back-transliteration for their efficient processing. In this paper, we investigate these indispensable processes and other problems associated with syntactic parsing of code-switching data and propose methods to mitigate their effects. In particular, we study dependency parsing of code-switching data of Hindi and English multilingual speakers from Twitter. We present a treebank of Hindi-English code-switching tweets under Universal Dependencies scheme and propose a neural stacking model for parsing that efficiently leverages the part-of-speech tag and syntactic tree annotations in the code-switching treebank and the preexisting Hindi and English treebanks. We also present normalization and back-transliteration models with a decoding process tailored for code-switching data. Results show that our neural stacking parser is 1.5{\\%} LAS points better than the augmented parsing model and 3.8{\\%} LAS points better than the one which uses first-best normalization and\/or back-transliteration.","pages":"987--998","doi":"10.18653\/v1\/N18-1090","url":"https:\/\/www.aclweb.org\/anthology\/N18-1090","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1091","title":"What{'}s Going On in Neural Constituency Parsers? An Analysis","authors":["Gaddy, David","Stern, Mitchell","Klein, Dan"],"emails":["dgaddy@berkeley.edu","mitchell@berkeley.edu","klein@berkeley.edu"],"author_id":["david-gaddy","mitchell-stern","dan-klein"],"abstract":"A number of differences have emerged between modern and classic approaches to constituency parsing in recent years, with structural components like grammars and feature-rich lexicons becoming less central while recurrent neural network representations rise in popularity. The goal of this work is to analyze the extent to which information provided directly by the model structure in classical systems is still being captured by neural methods. To this end, we propose a high-performance neural model (92.08 F1 on PTB) that is representative of recent work and perform a series of investigative experiments. We find that our model implicitly learns to encode much of the same information that was explicitly provided by grammars and lexicons in the past, indicating that this scaffolding can largely be subsumed by powerful general-purpose neural machinery.","pages":"999--1010","doi":"10.18653\/v1\/N18-1091","url":"https:\/\/www.aclweb.org\/anthology\/N18-1091","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1092","title":"Deep Generative Model for Joint Alignment and Word Representation","authors":["Rios, Miguel","Aziz, Wilker","Sima{'}an, Khalil"],"emails":["m.riosgaona@uva.nl","w.aziz@uva.nl","k.simaan@uva.nl"],"author_id":["miguel-rios","wilker-aziz","khalil-simaan"],"abstract":"This work exploits translation data as a source of semantically relevant learning signal for models of word representation. In particular, we exploit equivalence through translation as a form of distributional context and jointly learn how to embed and align with a deep generative model. Our EmbedAlign model embeds words in their complete observed context and learns by marginalisation of latent lexical alignments. Besides, it embeds words as posterior probability densities, rather than point estimates, which allows us to compare words in context using a measure of overlap between distributions (e.g. KL divergence). We investigate our model{'}s performance on a range of lexical semantics tasks achieving competitive results on several standard benchmarks including natural language inference, paraphrasing, and text similarity.","pages":"1011--1023","doi":"10.18653\/v1\/N18-1092","url":"https:\/\/www.aclweb.org\/anthology\/N18-1092","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1093","title":"Learning Word Embeddings for Low-Resource Languages by {PU} Learning","authors":["Jiang, Chao","Yu, Hsiang-Fu","Hsieh, Cho-Jui","Chang, Kai-Wei"],"emails":["cj7an@virginia.edu","rofuyu@cs.utexas.edu","chohsieh@ucdavis.edu","kwchang@cs.ucla.edu"],"author_id":["chao-jiang","hsiang-fu-yu","cho-jui-hsieh","kai-wei-chang"],"abstract":"Word embedding is a key component in many downstream applications in processing natural languages. Existing approaches often assume the existence of a large collection of text for learning effective word embedding. However, such a corpus may not be available for some low-resource languages. In this paper, we study how to effectively learn a word embedding model on a corpus with only a few million tokens. In such a situation, the co-occurrence matrix is sparse as the co-occurrences of many word pairs are unobserved. In contrast to existing approaches often only sample a few unobserved word pairs as negative samples, we argue that the zero entries in the co-occurrence matrix also provide valuable information. We then design a Positive-Unlabeled Learning (PU-Learning) approach to factorize the co-occurrence matrix and validate the proposed approaches in four different languages.","pages":"1024--1034","doi":"10.18653\/v1\/N18-1093","url":"https:\/\/www.aclweb.org\/anthology\/N18-1093","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1094","title":"Exploring the Role of Prior Beliefs for Argument Persuasion","authors":["Durmus, Esin","Cardie, Claire"],"emails":["ed459@cornell.edu","cardie@cs.cornell.edu"],"author_id":["esin-durmus","claire-cardie"],"abstract":"Public debate forums provide a common platform for exchanging opinions on a topic of interest. While recent studies in natural language processing (NLP) have provided empirical evidence that the language of the debaters and their patterns of interaction play a key role in changing the mind of a reader, research in psychology has shown that prior beliefs can affect our interpretation of an argument and could therefore constitute a competing alternative explanation for resistance to changing one{'}s stance. To study the actual effect of language use vs. prior beliefs on persuasion, we provide a new dataset and propose a controlled setting that takes into consideration two reader-level factors: political and religious ideology. We find that prior beliefs affected by these reader-level factors play a more important role than language use effects and argue that it is important to account for them in NLP studies of persuasion.","pages":"1035--1045","doi":"10.18653\/v1\/N18-1094","url":"https:\/\/www.aclweb.org\/anthology\/N18-1094","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1095","title":"Inducing a Lexicon of Abusive Words {--} a Feature-Based Approach","authors":["Wiegand, Michael","Ruppenhofer, Josef","Schmidt, Anna","Greenberg, Clayton"],"emails":["michael.wiegand@lsv.uni-saarland.de","ruppenhofer@ids-mannheim.de","anna.schmidt@lsv.uni-saarland.de","clayton.greenberg@lsv.uni-saarland.de"],"author_id":["michael-wiegand","josef-ruppenhofer","anna-schmidt","clayton-greenberg"],"abstract":"We address the detection of abusive words. The task is to identify such words among a set of negative polar expressions. We propose novel features employing information from both corpora and lexical resources. These features are calibrated on a small manually annotated base lexicon which we use to produce a large lexicon. We show that the word-level information we learn cannot be equally derived from a large dataset of annotated microposts. We demonstrate the effectiveness of our (domain-independent) lexicon in the cross-domain detection of abusive microposts.","pages":"1046--1056","doi":"10.18653\/v1\/N18-1095","url":"https:\/\/www.aclweb.org\/anthology\/N18-1095","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1096","title":"Author Commitment and Social Power: Automatic Belief Tagging to Infer the Social Context of Interactions","authors":["Prabhakaran, Vinodkumar","Ganeshkumar, Premkumar","Rambow, Owen"],"emails":["vinod@cs.stanford.edu","prem@agolo.com","owenr@elementalcognition.com"],"author_id":["vinodkumar-prabhakaran","premkumar-ganeshkumar","owen-rambow"],"abstract":"Understanding how social power structures affect the way we interact with one another is of great interest to social scientists who want to answer fundamental questions about human behavior, as well as to computer scientists who want to build automatic methods to infer the social contexts of interactions. In this paper, we employ advancements in extra-propositional semantics extraction within NLP to study how author commitment reflects the social context of an interactions. Specifically, we investigate whether the level of commitment expressed by individuals in an organizational interaction reflects the hierarchical power structures they are part of. We find that subordinates use significantly more instances of non-commitment than superiors. More importantly, we also find that subordinates attribute propositions to other agents more often than superiors do {---} an aspect that has not been studied before. Finally, we show that enriching lexical features with commitment labels captures important distinctions in social meanings.","pages":"1057--1068","doi":"10.18653\/v1\/N18-1096","url":"https:\/\/www.aclweb.org\/anthology\/N18-1096","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1097","title":"Comparing Automatic and Human Evaluation of Local Explanations for Text Classification","authors":["Nguyen, Dong"],"emails":["dnguyen@turing.ac.uk"],"author_id":["dong-nguyen"],"abstract":"Text classification models are becoming increasingly complex and opaque, however for many applications it is essential that the models are interpretable. Recently, a variety of approaches have been proposed for generating local explanations. While robust evaluations are needed to drive further progress, so far it is unclear which evaluation approaches are suitable. This paper is a first step towards more robust evaluations of local explanations. We evaluate a variety of local explanation approaches using automatic measures based on word deletion. Furthermore, we show that an evaluation using a crowdsourcing experiment correlates moderately with these automatic measures and that a variety of other factors also impact the human judgements.","pages":"1069--1078","doi":"10.18653\/v1\/N18-1097","url":"https:\/\/www.aclweb.org\/anthology\/N18-1097","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1098","title":"Deep Temporal-Recurrent-Replicated-Softmax for Topical Trends over Time","authors":["Gupta, Pankaj","Rajaram, Subburam","Sch{\\\"u}tze, Hinrich","Andrassy, Bernt"],"emails":["pankaj.gupta@siemens.com","subburam.rajaram@siemens.com","inquiries@cislmu.org","bernt.andrassy@siemens.com"],"author_id":["pankaj-gupta","subburam-rajaram","hinrich-schutze","bernt-andrassy"],"abstract":"Dynamic topic modeling facilitates the identification of topical trends over time in temporal collections of unstructured documents. We introduce a novel unsupervised neural dynamic topic model named as Recurrent Neural Network-Replicated Softmax Model (RNNRSM), where the discovered topics at each time influence the topic discovery in the subsequent time steps. We account for the temporal ordering of documents by explicitly modeling a joint distribution of latent topical dependencies over time, using distributional estimators with temporal recurrent connections. Applying RNN-RSM to 19 years of articles on NLP research, we demonstrate that compared to state-of-the art topic models, RNNRSM shows better generalization, topic interpretation, evolution and trends. We also introduce a metric (named as SPAN) to quantify the capability of dynamic topic model to capture word evolution in topics over time.","pages":"1079--1089","doi":"10.18653\/v1\/N18-1098","url":"https:\/\/www.aclweb.org\/anthology\/N18-1098","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1099","title":"Lessons from the {B}ible on Modern Topics: Low-Resource Multilingual Topic Model Evaluation","authors":["Hao, Shudong","Boyd-Graber, Jordan","Paul, Michael J."],"emails":["shudong@colorado.edu","jbg@umiacs.umd.edu","mpaul@colorado.edu"],"author_id":["shudong-hao","jordan-boyd-graber","michael-paul"],"abstract":"Multilingual topic models enable document analysis across languages through coherent multilingual summaries of the data. However, there is no standard and effective metric to evaluate the quality of multilingual topics. We introduce a new intrinsic evaluation of multilingual topic models that correlates well with human judgments of multilingual topic coherence as well as performance in downstream applications. Importantly, we also study evaluation for low-resource languages. Because standard metrics fail to accurately measure topic quality when robust external resources are unavailable, we propose an adaptation model that improves the accuracy and reliability of these metrics in low-resource settings.","pages":"1090--1100","doi":"10.18653\/v1\/N18-1099","url":"https:\/\/www.aclweb.org\/anthology\/N18-1099","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1100","title":"Explainable Prediction of Medical Codes from Clinical Text","authors":["Mullenbach, James","Wiegreffe, Sarah","Duke, Jon","Sun, Jimeng","Eisenstein, Jacob"],"emails":["jmullenbach3@gatech.edu","swiegreffe6@gatech.edu","jon.duke@gatech.edu","jsun@cc.gatech.edu","jacobe@gatech.edu"],"author_id":["james-mullenbach","sarah-wiegreffe","jon-duke","jimeng-sun","jacob-eisenstein"],"abstract":"Clinical notes are text documents that are created by clinicians for each patient encounter. They are typically accompanied by medical codes, which describe the diagnosis and treatment. Annotating these codes is labor intensive and error prone; furthermore, the connection between the codes and the text is not annotated, obscuring the reasons and details behind specific diagnoses and treatments. We present an attentional convolutional network that predicts medical codes from clinical text. Our method aggregates information across the document using a convolutional neural network, and uses an attention mechanism to select the most relevant segments for each of the thousands of possible codes. The method is accurate, achieving precision@8 of 0.71 and a Micro-F1 of 0.54, which are both better than the prior state of the art. Furthermore, through an interpretability evaluation by a physician, we show that the attention mechanism identifies meaningful explanations for each code assignment.","pages":"1101--1111","doi":"10.18653\/v1\/N18-1100","url":"https:\/\/www.aclweb.org\/anthology\/N18-1100","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1101","title":"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference","authors":["Williams, Adina","Nangia, Nikita","Bowman, Samuel"],"emails":["adinawilliams@nyu.edu","nikitanangia@nyu.edu","bowman@nyu.edu"],"author_id":["adina-williams","nikita-nangia","samuel-bowman"],"abstract":"This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.","pages":"1112--1122","doi":"10.18653\/v1\/N18-1101","url":"https:\/\/www.aclweb.org\/anthology\/N18-1101","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1102","title":"Filling Missing Paths: Modeling Co-occurrences of Word Pairs and Dependency Paths for Recognizing Lexical Semantic Relations","authors":["Washio, Koki","Kato, Tsuneaki"],"emails":["kokiwashio@g.ecc",""],"author_id":["koki-washio","tsuneaki-kato"],"abstract":"Recognizing lexical semantic relations between word pairs is an important task for many applications of natural language processing. One of the mainstream approaches to this task is to exploit the lexico-syntactic paths connecting two target words, which reflect the semantic relations of word pairs. However, this method requires that the considered words co-occur in a sentence. This requirement is hardly satisfied because of Zipf{'}s law, which states that most content words occur very rarely. In this paper, we propose novel methods with a neural model of P(path|w1,w2) to solve this problem. Our proposed model of P (path|w1, w2 ) can be learned in an unsupervised manner and can generalize the co-occurrences of word pairs and dependency paths. This model can be used to augment the path data of word pairs that do not co-occur in the corpus, and extract features capturing relational information from word pairs. Our experimental results demonstrate that our methods improve on previous neural approaches based on dependency paths and successfully solve the focused problem.","pages":"1123--1133","doi":"10.18653\/v1\/N18-1102","url":"https:\/\/www.aclweb.org\/anthology\/N18-1102","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1103","title":"Specialising Word Vectors for Lexical Entailment","authors":["Vuli{\\'c}, Ivan","Mrk{\\v{s}}i{\\'c}, Nikola"],"emails":["iv250@cam.ac.uk","nikola@poly-ai.com"],"author_id":["ivan-vulic","nikola-mrksic"],"abstract":"We present LEAR (Lexical Entailment Attract-Repel), a novel post-processing method that transforms any input word vector space to emphasise the asymmetric relation of lexical entailment (LE), also known as the IS-A or hyponymy-hypernymy relation. By injecting external linguistic constraints (e.g., WordNet links) into the initial vector space, the LE specialisation procedure brings true hyponymy-hypernymy pairs closer together in the transformed Euclidean space. The proposed asymmetric distance measure adjusts the norms of word vectors to reflect the actual WordNet-style hierarchy of concepts. Simultaneously, a joint objective enforces semantic similarity using the symmetric cosine distance, yielding a vector space specialised for both lexical relations at once. LEAR specialisation achieves state-of-the-art performance in the tasks of hypernymy directionality, hypernymy detection, and graded lexical entailment, demonstrating the effectiveness and robustness of the proposed asymmetric specialisation model.","pages":"1134--1145","doi":"10.18653\/v1\/N18-1103","url":"https:\/\/www.aclweb.org\/anthology\/N18-1103","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1104","title":"Cross-Lingual Abstract Meaning Representation Parsing","authors":["Damonte, Marco","Cohen, Shay B."],"emails":["m.damonte@sms.ed.ac.uk","scohen@inf.ed.ac.uk"],"author_id":["marco-damonte","shay-b-cohen"],"abstract":"Abstract Meaning Representation (AMR) research has mostly focused on English. We show that it is possible to use AMR annotations for English as a semantic representation for sentences written in other languages. We exploit an AMR parser for English and parallel corpora to learn AMR parsers for Italian, Spanish, German and Chinese. Qualitative analysis show that the new parsers overcome structural differences between the languages. We further propose a method to evaluate the parsers that does not require gold standard data in the target languages. This method highly correlates with the gold standard evaluation, obtaining a Pearson correlation coefficient of 0.95.","pages":"1146--1155","doi":"10.18653\/v1\/N18-1104","url":"https:\/\/www.aclweb.org\/anthology\/N18-1104","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1105","title":"Sentences with Gapping: Parsing and Reconstructing Elided Predicates","authors":["Schuster, Sebastian","Nivre, Joakim","Manning, Christopher D."],"emails":["sebschu@stanford.edu","joakim.nivre@lingfil.uu.se","manning@stanford.edu"],"author_id":["sebastian-schuster","joakim-nivre","christopher-d-manning"],"abstract":"Sentences with gapping, such as Paul likes coffee and Mary tea, lack an overt predicate to indicate the relation between two or more arguments. Surface syntax representations of such sentences are often produced poorly by parsers, and even if correct, not well suited to downstream natural language understanding tasks such as relation extraction that are typically designed to extract information from sentences with canonical clause structure. In this paper, we present two methods for parsing to a Universal Dependencies graph representation that explicitly encodes the elided material with additional nodes and edges. We find that both methods can reconstruct elided material from dependency trees with high accuracy when the parser correctly predicts the existence of a gap. We further demonstrate that one of our methods can be applied to other languages based on a case study on Swedish.","pages":"1156--1168","doi":"10.18653\/v1\/N18-1105","url":"https:\/\/www.aclweb.org\/anthology\/N18-1105","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1106","title":"A Structured Syntax-Semantics Interface for {E}nglish-{AMR} Alignment","authors":["Szubert, Ida","Lopez, Adam","Schneider, Nathan"],"emails":["","","nathan.schneider@georgetown.edu"],"author_id":["ida-szubert","adam-lopez","nathan-schneider"],"abstract":"Abstract Meaning Representation (AMR) annotations are often assumed to closely mirror dependency syntax, but AMR explicitly does not require this, and the assumption has never been tested. To test it, we devise an expressive framework to align AMR graphs to dependency graphs, which we use to annotate 200 AMRs. Our annotation explains how 97{\\%} of AMR edges are evoked by words or syntax. Previously existing AMR alignment frameworks did not allow for mapping AMR onto syntax, and as a consequence they explained at most 23{\\%}. While we find that there are indeed many cases where AMR annotations closely mirror syntax, there are also pervasive differences. We use our annotations to test a baseline AMR-to-syntax aligner, finding that this task is more difficult than AMR-to-string alignment; and to pinpoint errors in an AMR parser. We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax.","pages":"1169--1180","doi":"10.18653\/v1\/N18-1106","url":"https:\/\/www.aclweb.org\/anthology\/N18-1106","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1107","title":"End-to-End Graph-Based {TAG} Parsing with Neural Networks","authors":["Kasai, Jungo","Frank, Robert","Xu, Pauli","Merrill, William","Rambow, Owen"],"emails":["jungo.kasai@yale.edu","bob.frank@yale.edu","pauli.xu@yale.edu","william.merrill@yale.edu","owenr@elementalcognition.com"],"author_id":["jungo-kasai","robert-frank","pauli-xu","william-merrill","owen-rambow"],"abstract":"We present a graph-based Tree Adjoining Grammar (TAG) parser that uses BiLSTMs, highway connections, and character-level CNNs. Our best end-to-end parser, which jointly performs supertagging, POS tagging, and parsing, outperforms the previously reported best results by more than 2.2 LAS and UAS points. The graph-based parsing architecture allows for global inference and rich feature representations for TAG parsing, alleviating the fundamental trade-off between transition-based and graph-based parsing systems. We also demonstrate that the proposed parser achieves state-of-the-art performance in the downstream tasks of Parsing Evaluation using Textual Entailments (PETE) and Unbounded Dependency Recovery. This provides further support for the claim that TAG is a viable formalism for problems that require rich structural analysis of sentences.","pages":"1181--1194","doi":"10.18653\/v1\/N18-1107","url":"https:\/\/www.aclweb.org\/anthology\/N18-1107","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1108","title":"Colorless Green Recurrent Networks Dream Hierarchically","authors":["Gulordava, Kristina","Bojanowski, Piotr","Grave, Edouard","Linzen, Tal","Baroni, Marco"],"emails":["kristina.gulordava@unige.ch","bojanowski@fb.com","egrave@fb.com","tal.linzen@jhu.edu","mbaroni@fb.com"],"author_id":["kristina-gulordava","piotr-bojanowski","edouard-grave","tal-linzen","marco-baroni"],"abstract":"Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues ({``}The colorless green ideas I ate with the chair sleep furiously{''}), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.","pages":"1195--1205","doi":"10.18653\/v1\/N18-1108","url":"https:\/\/www.aclweb.org\/anthology\/N18-1108","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1109","title":"Diverse Few-Shot Text Classification with Multiple Metrics","authors":["Yu, Mo","Guo, Xiaoxiao","Yi, Jinfeng","Chang, Shiyu","Potdar, Saloni","Cheng, Yu","Tesauro, Gerald","Wang, Haoyu","Zhou, Bowen"],"emails":["","","","","","","","",""],"author_id":["mo-yu","xiaoxiao-guo","jinfeng-yi","shiyu-chang","saloni-potdar","yu-cheng","gerald-tesauro","haoyu-wang","bowen-zhou"],"abstract":"We study few-shot learning in natural language domains. Compared to many existing works that apply either metric-based or optimization-based meta-learning to image domain with low inter-task variance, we consider a more realistic setting, where tasks are diverse. However, it imposes tremendous difficulties to existing state-of-the-art metric-based algorithms since a single metric is insufficient to capture complex task variations in natural language domain. To alleviate the problem, we propose an adaptive metric learning approach that automatically determines the best weighted combination from a set of metrics obtained from meta-training tasks for a newly seen few-shot task. Extensive quantitative evaluations on real-world sentiment analysis and dialog intent classification datasets demonstrate that the proposed method performs favorably against state-of-the-art few shot learning algorithms in terms of predictive accuracy. We make our code and data available for further study.","pages":"1206--1215","doi":"10.18653\/v1\/N18-1109","url":"https:\/\/www.aclweb.org\/anthology\/N18-1109","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1110","title":"Early Text Classification Using Multi-Resolution Concept Representations","authors":["L{\\'o}pez-Monroy, Adrian Pastor","Gonz{\\'a}lez, Fabio A.","Montes, Manuel","Escalante, Hugo Jair","Solorio, Thamar"],"emails":["alopezmonroy@uh.edu","fagonzalezo@unal.edu.co","mmontesg@ccc.inaoep.mx","hugojair@ccc.inaoep.mx","solorio@cs.uh.edu"],"author_id":["adrian-pastor-lopez-monroy","fabio-a-gonzalez","manuel-montes","hugo-jair-escalante","thamar-solorio"],"abstract":"The intensive use of e-communications in everyday life has given rise to new threats and risks. When the vulnerable asset is the user, detecting these potential attacks before they cause serious damages is extremely important. This paper proposes a novel document representation to improve the early detection of risks in social media sources. The goal is to effectively identify the potential risk using as few text as possible and with as much anticipation as possible. Accordingly, we devise a Multi-Resolution Representation (MulR), which allows us to generate multiple {``}views{''} of the analyzed text. These views capture different semantic meanings for words and documents at different levels of detail, which is very useful in early scenarios to model the variable amounts of evidence. Intuitively, the representation captures better the content of short documents (very early stages) in low resolutions, whereas large documents (medium\/large stages) are better modeled with higher resolutions. We evaluate the proposed ideas in two different tasks where anticipation is critical: sexual predator detection and depression detection. The experimental evaluation for these early tasks revealed that the proposed approach outperforms previous methodologies by a considerable margin.","pages":"1216--1225","doi":"10.18653\/v1\/N18-1110","url":"https:\/\/www.aclweb.org\/anthology\/N18-1110","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1111","title":"Multinomial Adversarial Networks for Multi-Domain Text Classification","authors":["Chen, Xilun","Cardie, Claire"],"emails":["xlchen@cs.cornell.edu","cardie@cs.cornell.edu"],"author_id":["xilun-chen","claire-cardie"],"abstract":"Many text classification tasks are known to be highly domain-dependent. Unfortunately, the availability of training data can vary drastically across domains. Worse still, for some domains there may not be any annotated data at all. In this work, we propose a multinomial adversarial network (MAN) to tackle this real-world problem of multi-domain text classification (MDTC) in which labeled data may exist for multiple domains, but in insufficient amounts to train effective classifiers for one or more of the domains. We provide theoretical justifications for the MAN framework, proving that different instances of MANs are essentially minimizers of various f-divergence metrics (Ali and Silvey, 1966) among multiple probability distributions. MANs are thus a theoretically sound generalization of traditional adversarial networks that discriminate over two distributions. More specifically, for the MDTC task, MAN learns features that are invariant across multiple domains by resorting to its ability to reduce the divergence among the feature distributions of each domain. We present experimental results showing that MANs significantly outperform the prior art on the MDTC task. We also show that MANs achieve state-of-the-art performance for domains with no labeled data.","pages":"1226--1240","doi":"10.18653\/v1\/N18-1111","url":"https:\/\/www.aclweb.org\/anthology\/N18-1111","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1112","title":"Pivot Based Language Modeling for Improved Neural Domain Adaptation","authors":["Ziser, Yftah","Reichart, Roi"],"emails":["syftah@campus.technion.ac.il","roiri@ie.technion.ac.il"],"author_id":["yftah-ziser","roi-reichart"],"abstract":"Representation learning with pivot-based methods and with Neural Networks (NNs) have lead to significant progress in domain adaptation for Natural Language Processing. However, most previous work that follows these approaches does not explicitly exploit the structure of the input text, and its output is most often a single representation vector for the entire text. In this paper we present the Pivot Based Language Model (PBLM), a representation learning model that marries together pivot-based and NN modeling in a structure aware manner. Particularly, our model processes the information in the text with a sequential NN (LSTM) and its output consists of a representation vector for every input word. Unlike most previous representation learning models in domain adaptation, PBLM can naturally feed structure aware text classifiers such as LSTM and CNN. We experiment with the task of cross-domain sentiment classification on 20 domain pairs and show substantial improvements over strong baselines.","pages":"1241--1251","doi":"10.18653\/v1\/N18-1112","url":"https:\/\/www.aclweb.org\/anthology\/N18-1112","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1113","title":"Reinforced Co-Training","authors":["Wu, Jiawei","Li, Lei","Wang, William Yang"],"emails":["wu@cs.ucsb.edu","lileicc@gmail.com","william@cs.ucsb.edu"],"author_id":["jiawei-wu","lei-li","william-yang-wang"],"abstract":"Co-training is a popular semi-supervised learning framework to utilize a large amount of unlabeled data in addition to a small labeled set. Co-training methods exploit predicted labels on the unlabeled data and select samples based on prediction confidence to augment the training. However, the selection of samples in existing co-training methods is based on a predetermined policy, which ignores the sampling bias between the unlabeled and the labeled subsets, and fails to explore the data space. In this paper, we propose a novel method, Reinforced Co-Training, to select high-quality unlabeled samples to better co-train on. More specifically, our approach uses Q-learning to learn a data selection policy with a small labeled dataset, and then exploits this policy to train the co-training classifiers automatically. Experimental results on clickbait detection and generic text classification tasks demonstrate that our proposed method can obtain more accurate text classification results.","pages":"1252--1262","doi":"10.18653\/v1\/N18-1113","url":"https:\/\/www.aclweb.org\/anthology\/N18-1113","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1114","title":"Tensor Product Generation Networks for Deep {NLP} Modeling","authors":["Huang, Qiuyuan","Smolensky, Paul","He, Xiaodong","Deng, Li","Wu, Dapeng"],"emails":["qihua@microsoft.com","psmo@microsoft.com","xiaodong.he@jd.com","l.deng@ieee.org","dpwu@ufl.edu"],"author_id":["qiuyuan-huang","paul-smolensky","xiaodong-he","li-deng","dapeng-wu"],"abstract":"We present a new approach to the design of deep networks for natural language processing (NLP), based on the general technique of Tensor Product Representations (TPRs) for encoding and processing symbol structures in distributed neural networks. A network architecture {---} the Tensor Product Generation Network (TPGN) {---} is proposed which is capable in principle of carrying out TPR computation, but which uses unconstrained deep learning to design its internal representations. Instantiated in a model for image-caption generation, TPGN outperforms LSTM baselines when evaluated on the COCO dataset. The TPR-capable structure enables interpretation of internal representations and operations, which prove to contain considerable grammatical content. Our caption-generation model can be interpreted as generating sequences of grammatical categories and retrieving words by their categories from a plan encoded as a distributed representation.","pages":"1263--1273","doi":"10.18653\/v1\/N18-1114","url":"https:\/\/www.aclweb.org\/anthology\/N18-1114","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1115","title":"The Context-Dependent Additive Recurrent Neural Net","authors":["Tran, Quan Hung","Lai, Tuan","Haffari, Gholamreza","Zukerman, Ingrid","Bui, Trung","Bui, Hung"],"emails":["","","","","",""],"author_id":["quan-hung-tran","tuan-lai","gholamreza-haffari","ingrid-zukerman","trung-bui","hung-bui"],"abstract":"Contextual sequence mapping is one of the fundamental problems in Natural Language Processing (NLP). Here, instead of relying solely on the information presented in the text, the learning agents have access to a strong external signal given to assist the learning process. In this paper, we propose a novel family of Recurrent Neural Network unit: the Context-dependent Additive Recurrent Neural Network (CARNN) that is designed specifically to address this type of problem. The experimental results on public datasets in the dialog problem (Babi dialog Task 6 and Frame), contextual language model (Switchboard and Penn Tree Bank) and question answering (Trec QA) show that our novel CARNN-based architectures outperform previous methods.","pages":"1274--1283","doi":"10.18653\/v1\/N18-1115","url":"https:\/\/www.aclweb.org\/anthology\/N18-1115","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1116","title":"Combining Character and Word Information in Neural Machine Translation Using a Multi-Level Attention","authors":["Chen, Huadong","Huang, Shujian","Chiang, David","Dai, Xinyu","Chen, Jiajun"],"emails":["chenhd@nlp.nju.edu.cn","huangsj@nlp.nju.edu.cn","dchiang@nd.edu","daixinyu@nlp.nju.edu.cn","chenjj@nlp.nju.edu.cn"],"author_id":["huadong-chen","shujian-huang","david-chiang","xinyu-dai","jiajun-chen"],"abstract":"Natural language sentences, being hierarchical, can be represented at different levels of granularity, like words, subwords, or characters. But most neural machine translation systems require the sentence to be represented as a sequence at a single level of granularity. It can be difficult to determine which granularity is better for a particular translation task. In this paper, we improve the model by incorporating multiple levels of granularity. Specifically, we propose (1) an encoder with character attention which augments the (sub)word-level representation with character-level information; (2) a decoder with multiple attentions that enable the representations from different levels of granularity to control the translation cooperatively. Experiments on three translation tasks demonstrate that our proposed models outperform the standard word-based model, the subword-based model, and a strong character-based model.","pages":"1284--1293","doi":"10.18653\/v1\/N18-1116","url":"https:\/\/www.aclweb.org\/anthology\/N18-1116","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1117","title":"Dense Information Flow for Neural Machine Translation","authors":["Shen, Yanyao","Tan, Xu","He, Di","Qin, Tao","Liu, Tie-Yan"],"emails":["shenyanyao@utexas.edu","xuta@microsoft.com","he@pku.edu.cn","taoqin@microsoft.com","tie-yan.liu@microsoft.com"],"author_id":["yanyao-shen","xu-tan","di-he","tao-qin","tie-yan-liu"],"abstract":"Recently, neural machine translation has achieved remarkable progress by introducing well-designed deep neural networks into its encoder-decoder framework. From the optimization perspective, residual connections are adopted to improve learning performance for both encoder and decoder in most of these deep architectures, and advanced attention connections are applied as well. Inspired by the success of the DenseNet model in computer vision problems, in this paper, we propose a densely connected NMT architecture (DenseNMT) that is able to train more efficiently for NMT. The proposed DenseNMT not only allows dense connection in creating new features for both encoder and decoder, but also uses the dense attention structure to improve attention quality. Our experiments on multiple datasets show that DenseNMT structure is more competitive and efficient.","pages":"1294--1303","doi":"10.18653\/v1\/N18-1117","url":"https:\/\/www.aclweb.org\/anthology\/N18-1117","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1118","title":"Evaluating Discourse Phenomena in Neural Machine Translation","authors":["Bawden, Rachel","Sennrich, Rico","Birch, Alexandra","Haddow, Barry"],"emails":["rachel.bawden@limsi.fr","rico.sennrich@ed.ac.uk","a.birch@ed.ac.uk","bhaddow@inf.ed.ac.uk"],"author_id":["rachel-bawden","rico-sennrich","alexandra-birch","barry-haddow"],"abstract":"For machine translation to tackle discourse phenomena, models must have access to extra-sentential linguistic context. There has been recent interest in modelling context in neural machine translation (NMT), but models have been principally evaluated with standard automatic metrics, poorly adapted to evaluating discourse phenomena. In this article, we present hand-crafted, discourse test sets, designed to test the models{'} ability to exploit previous source and target sentences. We investigate the performance of recently proposed multi-encoder NMT models trained on subtitles for English to French. We also explore a novel way of exploiting context from the previous sentence. Despite gains using BLEU, multi-encoder models give limited improvement in the handling of discourse phenomena: 50{\\%} accuracy on our coreference test set and 53.5{\\%} for coherence\/cohesion (compared to a non-contextual baseline of 50{\\%}). A simple strategy of decoding the concatenation of the previous and current sentence leads to good performance, and our novel strategy of multi-encoding and decoding of two sentences leads to the best performance (72.5{\\%} for coreference and 57{\\%} for coherence\/cohesion), highlighting the importance of target-side context.","pages":"1304--1313","doi":"10.18653\/v1\/N18-1118","url":"https:\/\/www.aclweb.org\/anthology\/N18-1118","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1119","title":"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation","authors":["Post, Matt","Vilar, David"],"emails":["",""],"author_id":["matt-post","david-vilar"],"abstract":"The end-to-end nature of neural machine translation (NMT) removes many ways of manually guiding the translation process that were available in older paradigms. Recent work, however, has introduced a new capability: lexically constrained or guided decoding, a modification to beam search that forces the inclusion of pre-specified words and phrases in the output. However, while theoretically sound, existing approaches have computational complexities that are either linear (Hokamp and Liu, 2017) or exponential (Anderson et al., 2017) in the number of constraints. We present a algorithm for lexically constrained decoding with a complexity of O(1) in the number of constraints. We demonstrate the algorithm{'}s remarkable ability to properly place these constraints, and use it to explore the shaky relationship between model and BLEU scores. Our implementation is available as part of Sockeye.","pages":"1314--1324","doi":"10.18653\/v1\/N18-1119","url":"https:\/\/www.aclweb.org\/anthology\/N18-1119","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1120","title":"Guiding Neural Machine Translation with Retrieved Translation Pieces","authors":["Zhang, Jingyi","Utiyama, Masao","Sumita, Eiichro","Neubig, Graham","Nakamura, Satoshi"],"emails":["","","eiichiro.sumita@nict.go.jp","gneubig@cs.cmu.edu","s-nakamura@is.naist.jp"],"author_id":["jingyi-zhang","masao-utiyama","eiichiro-sumita","graham-neubig","satoshi-nakamura"],"abstract":"One of the difficulties of neural machine translation (NMT) is the recall and appropriate translation of low-frequency words or phrases. In this paper, we propose a simple, fast, and effective method for recalling previously seen translation examples and incorporating them into the NMT decoding process. Specifically, for an input sentence, we use a search engine to retrieve sentence pairs whose source sides are similar with the input sentence, and then collect n-grams that are both in the retrieved target sentences and aligned with words that match in the source sentences, which we call {``}translation pieces{''}. We compute pseudo-probabilities for each retrieved sentence based on similarities between the input sentence and the retrieved source sentences, and use these to weight the retrieved translation pieces. Finally, an existing NMT model is used to translate the input sentence, with an additional bonus given to outputs that contain the collected translation pieces. We show our method improves NMT translation results up to 6 BLEU points on three narrow domain translation tasks where repetitiveness of the target sentences is particularly salient. It also causes little increase in the translation time, and compares favorably to another alternative retrieval-based method with respect to accuracy, speed, and simplicity of implementation.","pages":"1325--1335","doi":"10.18653\/v1\/N18-1120","url":"https:\/\/www.aclweb.org\/anthology\/N18-1120","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1121","title":"Handling Homographs in Neural Machine Translation","authors":["Liu, Frederick","Lu, Han","Neubig, Graham"],"emails":["fliu1@cs.cmu.edu","hlu2@cs.cmu.edu","gneubig@cs.cmu.edu"],"author_id":["frederick-liu","han-lu","graham-neubig"],"abstract":"Homographs, words with different meanings but the same surface form, have long caused difficulty for machine translation systems, as it is difficult to select the correct translation based on the context. However, with the advent of neural machine translation (NMT) systems, which can theoretically take into account global sentential context, one may hypothesize that this problem has been alleviated. In this paper, we first provide empirical evidence that existing NMT systems in fact still have significant problems in properly translating ambiguous words. We then proceed to describe methods, inspired by the word sense disambiguation literature, that model the context of the input word with context-aware word embeddings that help to differentiate the word sense before feeding it into the encoder. Experiments on three language pairs demonstrate that such models improve the performance of NMT systems both in terms of BLEU score and in the accuracy of translating homographs.","pages":"1336--1345","doi":"10.18653\/v1\/N18-1121","url":"https:\/\/www.aclweb.org\/anthology\/N18-1121","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1122","title":"Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets","authors":["Yang, Zhen","Chen, Wei","Wang, Feng","Xu, Bo"],"emails":["yangzhen2014@ia.ac.cn","wei.chen.media@ia.ac.cn","feng.wang@ia.ac.cn","xubo@ia.ac.cn"],"author_id":["zhen-yang","wei-chen","feng-wang","bo-xu"],"abstract":"This paper proposes an approach for applying GANs to NMT. We build a conditional sequence generative adversarial net which comprises of two adversarial sub models, a generator and a discriminator. The generator aims to generate sentences which are hard to be discriminated from human-translated sentences ( i.e., the golden target sentences); And the discriminator makes efforts to discriminate the machine-generated sentences from human-translated ones. The two sub models play a mini-max game and achieve the win-win situation when they reach a Nash Equilibrium. Additionally, the static sentence-level BLEU is utilized as the reinforced objective for the generator, which biases the generation towards high BLEU points. During training, both the dynamic discriminator and the static BLEU objective are employed to evaluate the generated sentences and feedback the evaluations to guide the learning of the generator. Experimental results show that the proposed model consistently outperforms the traditional RNNSearch and the newly emerged state-of-the-art Transformer on English-German and Chinese-English translation tasks.","pages":"1346--1355","doi":"10.18653\/v1\/N18-1122","url":"https:\/\/www.aclweb.org\/anthology\/N18-1122","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1123","title":"Neural Machine Translation for Bilingually Scarce Scenarios: a Deep Multi-Task Learning Approach","authors":["Zaremoodi, Poorya","Haffari, Gholamreza"],"emails":["poorya.zaremoodi@monash.edu","gholamreza.haffari@monash.edu"],"author_id":["poorya-zaremoodi","gholamreza-haffari"],"abstract":"Neural machine translation requires large amount of parallel training text to learn a reasonable quality translation model. This is particularly inconvenient for language pairs for which enough parallel text is not available. In this paper, we use monolingual linguistic resources in the source side to address this challenging problem based on a multi-task learning approach. More specifically, we scaffold the machine translation task on auxiliary tasks including semantic parsing, syntactic parsing, and named-entity recognition. This effectively injects semantic and\/or syntactic knowledge into the translation model, which would otherwise require a large amount of training bitext to learn from. We empirically analyze and show the effectiveness of our multitask learning approach on three translation tasks: English-to-French, English-to-Farsi, and English-to-Vietnamese.","pages":"1356--1365","doi":"10.18653\/v1\/N18-1123","url":"https:\/\/www.aclweb.org\/anthology\/N18-1123","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1124","title":"Self-Attentive Residual Decoder for Neural Machine Translation","authors":["Miculicich Werlen, Lesly","Pappas, Nikolaos","Ram, Dhananjay","Popescu-Belis, Andrei"],"emails":["lmiculicich@idiap.ch","npappas@idiap.ch","dram@idiap.ch","andrei.popescu-belis@heig-vd.ch"],"author_id":["lesly-miculicich-werlen","nikolaos-pappas","dhananjay-ram","andrei-popescu-belis"],"abstract":"Neural sequence-to-sequence networks with attention have achieved remarkable performance for machine translation. One of the reasons for their effectiveness is their ability to capture relevant source-side contextual information at each time-step prediction through an attention mechanism. However, the target-side context is solely based on the sequence model which, in practice, is prone to a recency bias and lacks the ability to capture effectively non-sequential dependencies among words. To address this limitation, we propose a target-side-attentive residual recurrent network for decoding, where attention over previous words contributes directly to the prediction of the next word. The residual learning facilitates the flow of information from the distant past and is able to emphasize any of the previously translated words, hence it gains access to a wider context. The proposed model outperforms a neural MT baseline as well as a memory and self-attention network on three language pairs. The analysis of the attention learned by the decoder confirms that it emphasizes a wider context, and that it captures syntactic-like structures.","pages":"1366--1379","doi":"10.18653\/v1\/N18-1124","url":"https:\/\/www.aclweb.org\/anthology\/N18-1124","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1125","title":"Target Foresight Based Attention for Neural Machine Translation","authors":["Li, Xintong","Liu, Lemao","Tu, Zhaopeng","Shi, Shuming","Meng, Max"],"emails":["xtli@ee.cuhk.edu.hk","redmondliu@tencent.com","zptu@tencent.com","shumingshi@tencent.com","qhmeng@ee.cuhk.edu.hk"],"author_id":["xintong-li","lemao-liu","zhaopeng-tu","shuming-shi","max-meng"],"abstract":"In neural machine translation, an attention model is used to identify the aligned source words for a target word (target foresight word) in order to select translation context, but it does not make use of any information of this target foresight word at all. Previous work proposed an approach to improve the attention model by explicitly accessing this target foresight word and demonstrated the substantial gains in alignment task. However, this approach is useless in machine translation task on which the target foresight word is unavailable. In this paper, we propose a new attention model enhanced by the implicit information of target foresight word oriented to both alignment and translation tasks. Empirical experiments on Chinese-to-English and Japanese-to-English datasets show that the proposed attention model delivers significant improvements in terms of both alignment error rate and BLEU.","pages":"1380--1390","doi":"10.18653\/v1\/N18-1125","url":"https:\/\/www.aclweb.org\/anthology\/N18-1125","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1126","title":"Context Sensitive Neural Lemmatization with {L}ematus","authors":["Bergmanis, Toms","Goldwater, Sharon"],"emails":["ergmanis@sms.ed.ac.uk","sgwater@inf.ed.ac.uk"],"author_id":["toms-bergmanis","sharon-goldwater"],"abstract":"The main motivation for developing contextsensitive lemmatizers is to improve performance on unseen and ambiguous words. Yet previous systems have not carefully evaluated whether the use of context actually helps in these cases. We introduce Lematus, a lemmatizer based on a standard encoder-decoder architecture, which incorporates character-level sentence context. We evaluate its lemmatization accuracy across 20 languages in both a full data setting and a lower-resource setting with 10k training examples in each language. In both settings, we show that including context significantly improves results against a context-free version of the model. Context helps more for ambiguous words than for unseen words, though the latter has a greater effect on overall performance differences between languages. We also compare to three previous context-sensitive lemmatization systems, which all use pre-extracted edit trees as well as hand-selected features and\/or additional sources of information such as tagged training data. Without using any of these, our context-sensitive model outperforms the best competitor system (Lemming) in the fulldata setting, and performs on par in the lowerresource setting.","pages":"1391--1400","doi":"10.18653\/v1\/N18-1126","url":"https:\/\/www.aclweb.org\/anthology\/N18-1126","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1127","title":"Modeling Noisiness to Recognize Named Entities using Multitask Neural Networks on Social Media","authors":["Aguilar, Gustavo","L{\\'o}pez-Monroy, Adrian Pastor","Gonz{\\'a}lez, Fabio","Solorio, Thamar"],"emails":["gaguilaralas@uh.edu","alopezmonroy@uh.edu","fagonzalezo@unal.edu.co","solorio@cs.uh.edu"],"author_id":["gustavo-aguilar","adrian-pastor-lopez-monroy","fabio-a-gonzalez","thamar-solorio"],"abstract":"Recognizing named entities in a document is a key task in many NLP applications. Although current state-of-the-art approaches to this task reach a high performance on clean text (e.g. newswire genres), those algorithms dramatically degrade when they are moved to noisy environments such as social media domains. We present two systems that address the challenges of processing social media data using character-level phonetics and phonology, word embeddings, and Part-of-Speech tags as features. The first model is a multitask end-to-end Bidirectional Long Short-Term Memory (BLSTM)-Conditional Random Field (CRF) network whose output layer contains two CRF classifiers. The second model uses a multitask BLSTM network as feature extractor that transfers the learning to a CRF classifier for the final prediction. Our systems outperform the current F1 scores of the state of the art on the Workshop on Noisy User-generated Text 2017 dataset by 2.45{\\%} and 3.69{\\%}, establishing a more suitable approach for social media environments.","pages":"1401--1412","doi":"10.18653\/v1\/N18-1127","url":"https:\/\/www.aclweb.org\/anthology\/N18-1127","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1128","title":"Reusing Weights in Subword-Aware Neural Language Models","authors":["Assylbekov, Zhenisbek","Takhanov, Rustem"],"emails":["zhassylbekov@nu.edu.kz","rustem.takhanov@nu.edu.kz"],"author_id":["zhenisbek-assylbekov","rustem-takhanov"],"abstract":"We propose several ways of reusing subword embeddings and other weights in subword-aware neural language models. The proposed techniques do not benefit a competitive character-aware model, but some of them improve the performance of syllable- and morpheme-aware models while showing significant reductions in model sizes. We discover a simple hands-on principle: in a multi-layer input embedding model, layers should be tied consecutively bottom-up if reused at output. Our best morpheme-aware model with properly reused weights beats the competitive word-level model by a large margin across multiple languages and has 20{\\%}-87{\\%} fewer parameters.","pages":"1413--1423","doi":"10.18653\/v1\/N18-1128","url":"https:\/\/www.aclweb.org\/anthology\/N18-1128","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1129","title":"Simple Models for Word Formation in Slang","authors":["Kulkarni, Vivek","Wang, William Yang"],"emails":["vvkulkarni@cs.ucsb.edu","william@cs.ucsb.edu"],"author_id":["vivek-kulkarni","william-yang-wang"],"abstract":"We propose the first generative models for three types of extra-grammatical word formation phenomena abounding in slang: Blends, Clippings, and Reduplicatives. Adopting a data-driven approach coupled with linguistic knowledge, we propose simple models with state of the art performance on human annotated gold standard datasets. Overall, our models reveal insights into the generative processes of word formation in slang {--} insights which are increasingly relevant in the context of the rising prevalence of slang and non-standard varieties on the Internet","pages":"1424--1434","doi":"10.18653\/v1\/N18-1129","url":"https:\/\/www.aclweb.org\/anthology\/N18-1129","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1130","title":"Using Morphological Knowledge in Open-Vocabulary Neural Language Models","authors":["Matthews, Austin","Neubig, Graham","Dyer, Chris"],"emails":["austinma@cs.cmu.edu","gneubig@cs.cmu.edu","cdyer@google.com"],"author_id":["austin-matthews","graham-neubig","chris-dyer"],"abstract":"Languages with productive morphology pose problems for language models that generate words from a fixed vocabulary. Although character-based models allow any possible word type to be generated, they are linguistically na{\\\"\\i}ve: they must discover that words exist and are delimited by spaces{---}basic linguistic facts that are built in to the structure of word-based models. We introduce an open-vocabulary language model that incorporates more sophisticated linguistic knowledge by predicting words using a mixture of three generative processes: (1) by generating words as a sequence of characters, (2) by directly generating full word forms, and (3) by generating words as a sequence of morphemes that are combined using a hand-written morphological analyzer. Experiments on Finnish, Turkish, and Russian show that our model outperforms character sequence models and other strong baselines on intrinsic and extrinsic measures. Furthermore, we show that our model learns to exploit morphological knowledge encoded in the analyzer, and, as a byproduct, it can perform effective unsupervised morphological disambiguation.","pages":"1435--1445","doi":"10.18653\/v1\/N18-1130","url":"https:\/\/www.aclweb.org\/anthology\/N18-1130","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1131","title":"A Neural Layered Model for Nested Named Entity Recognition","authors":["Ju, Meizhi","Miwa, Makoto","Ananiadou, Sophia"],"emails":["meizhi.ju@manchester.ac.uk","makoto-miwa@toyota-ti.ac.jp","sophia.ananiadou@manchester.ac.uk"],"author_id":["meizhi-ju","makoto-miwa","sophia-ananiadou"],"abstract":"Entity mentions embedded in longer entity mentions are referred to as nested entities. Most named entity recognition (NER) systems deal only with the flat entities and ignore the inner nested ones, which fails to capture finer-grained semantic information in underlying texts. To address this issue, we propose a novel neural model to identify nested entities by dynamically stacking flat NER layers. Each flat NER layer is based on the state-of-the-art flat NER model that captures sequential context representation with bidirectional Long Short-Term Memory (LSTM) layer and feeds it to the cascaded CRF layer. Our model merges the output of the LSTM layer in the current flat NER layer to build new representation for detected entities and subsequently feeds them into the next flat NER layer. This allows our model to extract outer entities by taking full advantage of information encoded in their corresponding inner entities, in an inside-to-outside way. Our model dynamically stacks the flat NER layers until no outer entities are extracted. Extensive evaluation shows that our dynamic model outperforms state-of-the-art feature-based systems on nested NER, achieving 74.7{\\%} and 72.2{\\%} on GENIA and ACE2005 datasets, respectively, in terms of F-score.","pages":"1446--1459","doi":"10.18653\/v1\/N18-1131","url":"https:\/\/www.aclweb.org\/anthology\/N18-1131","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1132","title":"{DR}-{B}i{LSTM}: Dependent Reading Bidirectional {LSTM} for Natural Language Inference","authors":["Ghaeini, Reza","Hasan, Sadid A.","Datla, Vivek","Liu, Joey","Lee, Kathy","Qadir, Ashequl","Ling, Yuan","Prakash, Aaditya","Fern, Xiaoli","Farri, Oladimeji"],"emails":["ghaeinim@eecs.oregonstate.edu","1@philips.com","","","","ashequl.qadir@philips.com","yuan.ling@philips.com","aaditya.prakash@philips.com","xfern@eecs.oregonstate.edu","dimeji.farri@philips.com"],"author_id":["reza-ghaeini","sadid-a-hasan","vivek-datla","joey-liu","kathy-lee","ashequl-qadir","yuan-ling","aaditya-prakash","xiaoli-fern","oladimeji-farri"],"abstract":"We present a novel deep learning architecture to address the natural language inference (NLI) task. Existing approaches mostly rely on simple reading mechanisms for independent encoding of the premise and hypothesis. Instead, we propose a novel dependent reading bidirectional LSTM network (DR-BiLSTM) to efficiently model the relationship between a premise and a hypothesis during encoding and inference. We also introduce a sophisticated ensemble strategy to combine our proposed models, which noticeably improves final predictions. Finally, we demonstrate how the results can be improved further with an additional preprocessing step. Our evaluation shows that DR-BiLSTM obtains the best single model and ensemble model results achieving the new state-of-the-art scores on the Stanford NLI dataset.","pages":"1460--1469","doi":"10.18653\/v1\/N18-1132","url":"https:\/\/www.aclweb.org\/anthology\/N18-1132","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1133","title":"{KBGAN}: Adversarial Learning for Knowledge Graph Embeddings","authors":["Cai, Liwei","Wang, William Yang"],"emails":["cai.lw123@gmail.com","william@cs.ucsb.edu"],"author_id":["liwei-cai","william-yang-wang"],"abstract":"We introduce KBGAN, an adversarial learning framework to improve the performances of a wide range of existing knowledge graph embedding models. Because knowledge graphs typically only contain positive facts, sampling useful negative training examples is a nontrivial task. Replacing the head or tail entity of a fact with a uniformly randomly selected entity is a conventional method for generating negative facts, but the majority of the generated negative facts can be easily discriminated from positive facts, and will contribute little towards the training. Inspired by generative adversarial networks (GANs), we use one knowledge graph embedding model as a negative sample generator to assist the training of our desired model, which acts as the discriminator in GANs. This framework is independent of the concrete form of generator and discriminator, and therefore can utilize a wide variety of knowledge graph embedding models as its building blocks. In experiments, we adversarially train two translation-based models, TRANSE and TRANSD, each with assistance from one of the two probability-based models, DISTMULT and COMPLEX. We evaluate the performances of KBGAN on the link prediction task, using three knowledge base completion datasets: FB15k-237, WN18 and WN18RR. Experimental results show that adversarial training substantially improves the performances of target embedding models under various settings.","pages":"1470--1480","doi":"10.18653\/v1\/N18-1133","url":"https:\/\/www.aclweb.org\/anthology\/N18-1133","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1134","title":"Multimodal Frame Identification with Multilingual Evaluation","authors":["Botschen, Teresa","Gurevych, Iryna","Klie, Jan-Christoph","Mousselly-Sergieh, Hatem","Roth, Stefan"],"emails":["","","","",""],"author_id":["teresa-botschen","iryna-gurevych","jan-christoph-klie","hatem-mousselly-sergieh","stefan-roth"],"abstract":"An essential step in FrameNet Semantic Role Labeling is the Frame Identification (FrameId) task, which aims at disambiguating a situation around a predicate. Whilst current FrameId methods rely on textual representations only, we hypothesize that FrameId can profit from a richer understanding of the situational context. Such contextual information can be obtained from common sense knowledge, which is more present in images than in text. In this paper, we extend a state-of-the-art FrameId system in order to effectively leverage multimodal representations. We conduct a comprehensive evaluation on the English FrameNet and its German counterpart SALSA. Our analysis shows that for the German data, textual representations are still competitive with multimodal ones. However on the English data, our multimodal FrameId approach outperforms its unimodal counterpart, setting a new state of the art. Its benefits are particularly apparent in dealing with ambiguous and rare instances, the main source of errors of current systems. For research purposes, we release (a) the implementation of our system, (b) our evaluation splits for SALSA 2.0, and (c) the embeddings for synsets and IMAGINED words.","pages":"1481--1491","doi":"10.18653\/v1\/N18-1134","url":"https:\/\/www.aclweb.org\/anthology\/N18-1134","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1135","title":"Learning Joint Semantic Parsers from Disjoint Data","authors":["Peng, Hao","Thomson, Sam","Swayamdipta, Swabha","Smith, Noah A."],"emails":["hapeng@cs.washington.edu","sthomson@cs.cmu.edu","swabha@cs.cmu.edu","nasmith@cs.washington.edu"],"author_id":["hao-peng","sam-thomson","swabha-swayamdipta","noah-a-smith"],"abstract":"We present a new approach to learning a semantic parser from multiple datasets, even when the target semantic formalisms are drastically different and the underlying corpora do not overlap. We handle such {``}disjoint{''} data by treating annotations for unobserved formalisms as latent structured variables. Building on state-of-the-art baselines, we show improvements both in frame-semantic parsing and semantic dependency parsing by modeling them jointly.","pages":"1492--1502","doi":"10.18653\/v1\/N18-1135","url":"https:\/\/www.aclweb.org\/anthology\/N18-1135","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1136","title":"Identifying Semantic Divergences in Parallel Text without Annotations","authors":["Vyas, Yogarshi","Niu, Xing","Carpuat, Marine"],"emails":["yogarshi@cs.umd.edu","xingniu@cs.umd.edu","marine@cs.umd.edu"],"author_id":["yogarshi-vyas","xing-niu","marine-carpuat"],"abstract":"Recognizing that even correct translations are not always semantically equivalent, we automatically detect meaning divergences in parallel sentence pairs with a deep neural model of bilingual semantic similarity which can be trained for any parallel corpus without any manual annotation. We show that our semantic model detects divergences more accurately than models based on surface features derived from word alignments, and that these divergences matter for neural machine translation.","pages":"1503--1515","doi":"10.18653\/v1\/N18-1136","url":"https:\/\/www.aclweb.org\/anthology\/N18-1136","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1137","title":"Bootstrapping Generators from Noisy Data","authors":["Perez-Beltrachini, Laura","Lapata, Mirella"],"emails":["lperez@inf.ed.ac.uk","mlap@inf.ed.ac.uk"],"author_id":["laura-perez-beltrachini","mirella-lapata"],"abstract":"A core step in statistical data-to-text generation concerns learning correspondences between structured data representations (e.g., facts in a database) and associated texts. In this paper we aim to bootstrap generators from large scale datasets where the data (e.g., DBPedia facts) and related texts (e.g., Wikipedia abstracts) are loosely aligned. We tackle this challenging task by introducing a special-purpose content selection mechanism. We use multi-instance learning to automatically discover correspondences between data and text pairs and show how these can be used to enhance the content signal while training an encoder-decoder architecture. Experimental results demonstrate that models trained with content-specific objectives improve upon a vanilla encoder-decoder which solely relies on soft attention.","pages":"1516--1527","doi":"10.18653\/v1\/N18-1137","url":"https:\/\/www.aclweb.org\/anthology\/N18-1137","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1138","title":"{SHAPED}: Shared-Private Encoder-Decoder for Text Style Adaptation","authors":["Zhang, Ye","Ding, Nan","Soricut, Radu"],"emails":["yezhang@utexas.edu","dingnan@google.com","rsoricut@google.com"],"author_id":["ye-zhang","nan-ding","radu-soricut"],"abstract":"Supervised training of abstractive language generation models results in learning conditional probabilities over language sequences based on the supervised training signal. When the training signal contains a variety of writing styles, such models may end up learning an {`}average{'} style that is directly influenced by the training data make-up and cannot be controlled by the needs of an application. We describe a family of model architectures capable of capturing both generic language characteristics via shared model parameters, as well as particular style characteristics via private model parameters. Such models are able to generate language according to a specific learned style, while still taking advantage of their power to model generic language phenomena. Furthermore, we describe an extension that uses a mixture of output distributions from all learned styles to perform on-the-fly style adaptation based on the textual input alone. Experimentally, we find that the proposed models consistently outperform models that encapsulate single-style or average-style language generation capabilities.","pages":"1528--1538","doi":"10.18653\/v1\/N18-1138","url":"https:\/\/www.aclweb.org\/anthology\/N18-1138","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1139","title":"Generating Descriptions from Structured Data Using a Bifocal Attention Mechanism and Gated Orthogonalization","authors":["Nema, Preksha","Shetty, Shreyas","Jain, Parag","Laha, Anirban","Sankaranarayanan, Karthik","Khapra, Mitesh M."],"emails":["preksha@cse.iitm.ac.in","shshett@cse.iitm.ac.in","pajain34@in.ibm.com","anirlaha@in.ibm.com","kartsank@in.ibm.com","miteshk@cse.iitm.ac.in"],"author_id":["preksha-nema","shreyas-shetty","parag-jain","anirban-laha","karthik-sankaranarayanan","mitesh-m-khapra"],"abstract":"In this work, we focus on the task of generating natural language descriptions from a structured table of facts containing fields (such as nationality, occupation, etc) and values (such as Indian, actor, director, etc). One simple choice is to treat the table as a sequence of fields and values and then use a standard seq2seq model for this task. However, such a model is too generic and does not exploit task specific characteristics. For example, while generating descriptions from a table, a human would attend to information at two levels: (i) the fields (macro level) and (ii) the values within the field (micro level). Further, a human would continue attending to a field for a few timesteps till all the information from that field has been rendered and then never return back to this field (because there is nothing left to say about it). To capture this behavior we use (i) a fused bifocal attention mechanism which exploits and combines this micro and macro level information and (ii) a gated orthogonalization mechanism which tries to ensure that a field is remembered for a few time steps and then forgotten. We experiment with a recently released dataset which contains fact tables about people and their corresponding one line biographical descriptions in English. In addition, we also introduce two similar datasets for French and German. Our experiments show that the proposed model gives 21{\\%} relative improvement over a recently proposed state of the art method and 10{\\%} relative improvement over basic seq2seq models. The code and the datasets developed as a part of this work are publicly available on \\url{https:\/\/github.com\/PrekshaNema25\/StructuredData_To_Descriptions}","pages":"1539--1550","doi":"10.18653\/v1\/N18-1139","url":"https:\/\/www.aclweb.org\/anthology\/N18-1139","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1140","title":"{C}li{CR}: a Dataset of Clinical Case Reports for Machine Reading Comprehension","authors":["{\\v{S}}uster, Simon","Daelemans, Walter"],"emails":["simon.suster@uantwerpen.be","walter.daelemans@uantwerpen.be"],"author_id":["simon-suster","walter-daelemans"],"abstract":"We present a new dataset for machine comprehension in the medical domain. Our dataset uses clinical case reports with around 100,000 gap-filling queries about these cases. We apply several baselines and state-of-the-art neural readers to the dataset, and observe a considerable gap in performance (20{\\%} F1) between the best human and machine readers. We analyze the skills required for successful answering and show how reader performance varies depending on the applicable skills. We find that inferences using domain knowledge and object tracking are the most frequently required skills, and that recognizing omitted information and spatio-temporal reasoning are the most difficult for the machines.","pages":"1551--1563","doi":"10.18653\/v1\/N18-1140","url":"https:\/\/www.aclweb.org\/anthology\/N18-1140","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1141","title":"Learning to Collaborate for Question Answering and Asking","authors":["Tang, Duyu","Duan, Nan","Yan, Zhao","Zhang, Zhirui","Sun, Yibo","Liu, Shujie","Lv, Yuanhua","Zhou, Ming"],"emails":["dutang@microsoft.com","nanduan@microsoft.com","v-zhaoya@microsoft.com","v-zhirzhi@microsoft.com","v-yibsu@microsoft.com","shujliu@microsoft.com","yuanhual@microsoft.com","mingzhou@microsoft.com"],"author_id":["duyu-tang","nan-duan","zhao-yan","zhirui-zhang","yibo-sun","shujie-liu","yuanhua-lv","ming-zhou"],"abstract":"Question answering (QA) and question generation (QG) are closely related tasks that could improve each other; however, the connection of these two tasks is not well explored in literature. In this paper, we give a systematic study that seeks to leverage the connection to improve both QA and QG. We present a training algorithm that generalizes both Generative Adversarial Network (GAN) and Generative Domain-Adaptive Nets (GDAN) under the question answering scenario. The two key ideas are improving the QG model with QA through incorporating additional QA-specific signal as the loss function, and improving the QA model with QG through adding artificially generated training instances. We conduct experiments on both document based and knowledge based question answering tasks. We have two main findings. Firstly, the performance of a QG model (e.g in terms of BLEU score) could be easily improved by a QA model via policy gradient. Secondly, directly applying GAN that regards all the generated questions as negative instances could not improve the accuracy of the QA model. Learning when to regard generated questions as positive instances could bring performance boost.","pages":"1564--1574","doi":"10.18653\/v1\/N18-1141","url":"https:\/\/www.aclweb.org\/anthology\/N18-1141","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1142","title":"Learning to Rank Question-Answer Pairs Using Hierarchical Recurrent Encoder with Latent Topic Clustering","authors":["Yoon, Seunghyun","Shin, Joongbo","Jung, Kyomin"],"emails":["mysmilesh@snu.ac.kr","jbshin@snu.ac.kr","kjung@snu.ac.kr"],"author_id":["seunghyun-yoon","joongbo-shin","kyomin-jung"],"abstract":"In this paper, we propose a novel end-to-end neural architecture for ranking candidate answers, that adapts a hierarchical recurrent neural network and a latent topic clustering module. With our proposed model, a text is encoded to a vector representation from an word-level to a chunk-level to effectively capture the entire meaning. In particular, by adapting the hierarchical structure, our model shows very small performance degradations in longer text comprehension while other state-of-the-art recurrent neural network models suffer from it. Additionally, the latent topic clustering module extracts semantic information from target samples. This clustering module is useful for any text related tasks by allowing each data sample to find its nearest topic cluster, thus helping the neural network model analyze the entire data. We evaluate our models on the Ubuntu Dialogue Corpus and consumer electronic domain question answering dataset, which is related to Samsung products. The proposed model shows state-of-the-art results for ranking question-answer pairs.","pages":"1575--1584","doi":"10.18653\/v1\/N18-1142","url":"https:\/\/www.aclweb.org\/anthology\/N18-1142","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1143","title":"Supervised and Unsupervised Transfer Learning for Question Answering","authors":["Chung, Yu-An","Lee, Hung-Yi","Glass, James"],"emails":["andyyuan@mit.edu","hungyilee@ntu.edu.tw","glass@mit.edu"],"author_id":["yu-an-chung","hung-yi-lee","james-glass"],"abstract":"Although transfer learning has been shown to be successful for tasks like object and speech recognition, its applicability to question answering (QA) has yet to be well-studied. In this paper, we conduct extensive experiments to investigate the transferability of knowledge learned from a source QA dataset to a target dataset using two QA models. The performance of both models on a TOEFL listening comprehension test (Tseng et al., 2016) and MCTest (Richardson et al., 2013) is significantly improved via a simple transfer learning technique from MovieQA (Tapaswi et al., 2016). In particular, one of the models achieves the state-of-the-art on all target datasets; for the TOEFL listening comprehension test, it outperforms the previous best model by 7{\\%}. Finally, we show that transfer learning is helpful even in unsupervised scenarios when correct answers for target QA dataset examples are not available.","pages":"1585--1594","doi":"10.18653\/v1\/N18-1143","url":"https:\/\/www.aclweb.org\/anthology\/N18-1143","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1144","title":"Tracking State Changes in Procedural Text: a Challenge Dataset and Models for Process Paragraph Comprehension","authors":["Dalvi, Bhavana","Huang, Lifu","Tandon, Niket","Yih, Wen-tau","Clark, Peter"],"emails":["bhavanad@allenai.org","warrior.fu@gmail.com","nikett@allenai.org","scottyih@allenai.org","peterc@allenai.org"],"author_id":["bhavana-dalvi","lifu-huang","niket-tandon","wen-tau-yih","peter-clark"],"abstract":"We present a new dataset and models for comprehending paragraphs about processes (e.g., photosynthesis), an important genre of text describing a dynamic world. The new dataset, ProPara, is the first to contain natural (rather than machine-generated) text about a changing world along with a full annotation of entity states (location and existence) during those changes (81k datapoints). The end-task, tracking the location and existence of entities through the text, is challenging because the causal effects of actions are often implicit and need to be inferred. We find that previous models that have worked well on synthetic data achieve only mediocre performance on ProPara, and introduce two new neural models that exploit alternative mechanisms for state prediction, in particular using LSTM input encoding and span prediction. The new models improve accuracy by up to 19{\\%}. We are releasing the ProPara dataset and our models to the community.","pages":"1595--1604","doi":"10.18653\/v1\/N18-1144","url":"https:\/\/www.aclweb.org\/anthology\/N18-1144","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1145","title":"Combining Deep Learning and Topic Modeling for Review Understanding in Context-Aware Recommendation","authors":["Jin, Mingmin","Luo, Xin","Zhu, Huiling","Zhuo, Hankz Hankui"],"emails":["","","",""],"author_id":["mingmin-jin","xin-luo","huiling-zhu","hankz-hankui-zhuo"],"abstract":"With the rise of e-commerce, people are accustomed to writing their reviews after receiving the goods. These comments are so important that a bad review can have a direct impact on others buying. Besides, the abundant information within user reviews is very useful for extracting user preferences and item properties. In this paper, we investigate the approach to effectively utilize review information for recommender systems. The proposed model is named LSTM-Topic matrix factorization (LTMF) which integrates both LSTM and Topic Modeling for review understanding. In the experiments on popular review dataset Amazon , our LTMF model outperforms previous proposed HFT model and ConvMF model in rating prediction. Furthermore, LTMF shows the better ability on making topic clustering than traditional topic model based method, which implies integrating the information from deep learning and topic modeling is a meaningful approach to make a better understanding of reviews.","pages":"1605--1614","doi":"10.18653\/v1\/N18-1145","url":"https:\/\/www.aclweb.org\/anthology\/N18-1145","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1146","title":"Deconfounded Lexicon Induction for Interpretable Social Science","authors":["Pryzant, Reid","Shen, Kelly","Jurafsky, Dan","Wagner, Stefan"],"emails":["rpryzant@stanford.edu","kshen21@stanford.edu","jurafsky@stanford.edu","swager@stanford.edu"],"author_id":["reid-pryzant","kelly-shen","dan-jurafsky","stefan-wagner"],"abstract":"NLP algorithms are increasingly used in computational social science to take linguistic observations and predict outcomes like human preferences or actions. Making these social models transparent and interpretable often requires identifying features in the input that predict outcomes while also controlling for potential confounds. We formalize this need as a new task: inducing a lexicon that is predictive of a set of target variables yet uncorrelated to a set of confounding variables. We introduce two deep learning algorithms for the task. The first uses a bifurcated architecture to separate the explanatory power of the text and confounds. The second uses an adversarial discriminator to force confound-invariant text encodings. Both elicit lexicons from learned weights and attentional scores. We use them to induce lexicons that are predictive of timely responses to consumer complaints (controlling for product), enrollment from course descriptions (controlling for subject), and sales from product descriptions (controlling for seller). In each domain our algorithms pick words that are associated with \\textit{narrative persuasion}; more predictive and less confound-related than those of standard feature weighting and lexicon induction techniques like regression and log odds.","pages":"1615--1625","doi":"10.18653\/v1\/N18-1146","url":"https:\/\/www.aclweb.org\/anthology\/N18-1146","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1147","title":"Detecting Denial-of-Service Attacks from Social Media Text: Applying {NLP} to Computer Security","authors":["Chambers, Nathanael","Fry, Ben","McMasters, James"],"emails":["nchamber@usna.edu","",""],"author_id":["nathanael-chambers","ben-fry","james-mcmasters"],"abstract":"This paper describes a novel application of NLP models to detect denial of service attacks using only social media as evidence. Individual networks are often slow in reporting attacks, so a detection system from public data could better assist a response to a broad attack across multiple services. We explore NLP methods to use social media as an indirect measure of network service status. We describe two learning frameworks for this task: a feed-forward neural network and a partially labeled LDA model. Both models outperform previous work by significant margins (20{\\%} F1 score). We further show that the topic-based model enables the first fine-grained analysis of how the public reacts to ongoing network attacks, discovering multiple {``}stages{''} of observation. This is the first model that both detects network attacks (with best performance) and provides an analysis of when and how the public interprets service outages. We describe the models, present experiments on the largest twitter DDoS corpus to date, and conclude with an analysis of public reactions based on the learned model{'}s output.","pages":"1626--1635","doi":"10.18653\/v1\/N18-1147","url":"https:\/\/www.aclweb.org\/anthology\/N18-1147","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1148","title":"The Importance of Calibration for Estimating Proportions from Annotations","authors":["Card, Dallas","Smith, Noah A."],"emails":["dcard@cmu.edu","nasmith@cs.washington.edu"],"author_id":["dallas-card","noah-a-smith"],"abstract":"Estimating label proportions in a target corpus is a type of measurement that is useful for answering certain types of social-scientific questions. While past work has described a number of relevant approaches, nearly all are based on an assumption which we argue is invalid for many problems, particularly when dealing with human annotations. In this paper, we identify and differentiate between two relevant data generating scenarios (intrinsic vs. extrinsic labels), introduce a simple but novel method which emphasizes the importance of calibration, and then analyze and experimentally validate the appropriateness of various methods for each of the two scenarios.","pages":"1636--1646","doi":"10.18653\/v1\/N18-1148","url":"https:\/\/www.aclweb.org\/anthology\/N18-1148","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1149","title":"A Dataset of Peer Reviews ({P}eer{R}ead): Collection, Insights and {NLP} Applications","authors":["Kang, Dongyeop","Ammar, Waleed","Dalvi, Bhavana","van Zuylen, Madeleine","Kohlmeier, Sebastian","Hovy, Eduard","Schwartz, Roy"],"emails":["dongyeok@cs.cmu.edu","waleeda@allenai.org","bhavanad@allenai.org","madeleinev@allenai.org","sebastiank@allenai.org","hovy@cs.cmu.edu","roys@allenai.org"],"author_id":["dongyeop-kang","waleed-ammar","bhavana-dalvi","madeleine-van-zuylen","sebastian-kohlmeier","eduard-hovy","roy-schwartz"],"abstract":"Peer reviewing is a central component in the scientific publishing process. We present the first public dataset of scientific peer reviews available for research purposes (PeerRead v1),1 providing an opportunity to study this important artifact. The dataset consists of 14.7K paper drafts and the corresponding accept\/reject decisions in top-tier venues including ACL, NIPS and ICLR. The dataset also includes 10.7K textual peer reviews written by experts for a subset of the papers. We describe the data collection process and report interesting observed phenomena in the peer reviews. We also propose two novel NLP tasks based on this dataset and provide simple baseline models. In the first task, we show that simple models can predict whether a paper is accepted with up to 21{\\%} error reduction compared to the majority baseline. In the second task, we predict the numerical scores of review aspects and show that simple models can outperform the mean baseline for aspects with high variance such as {`}originality{'} and {`}impact{'}.","pages":"1647--1661","doi":"10.18653\/v1\/N18-1149","url":"https:\/\/www.aclweb.org\/anthology\/N18-1149","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1150","title":"Deep Communicating Agents for Abstractive Summarization","authors":["Celikyilmaz, Asli","Bosselut, Antoine","He, Xiaodong","Choi, Yejin"],"emails":["aslicel@microsoft.com","antoineb@cs.washington.edu","xiaodong.he@jd.com","yejin@cs.washington.edu"],"author_id":["asli-celikyilmaz","antoine-bosselut","xiaodong-he","yejin-choi"],"abstract":"We present deep communicating agents in an encoder-decoder architecture to address the challenges of representing a long document for abstractive summarization. With deep communicating agents, the task of encoding a long text is divided across multiple collaborating agents, each in charge of a subsection of the input text. These encoders are connected to a single decoder, trained end-to-end using reinforcement learning to generate a focused and coherent summary. Empirical results demonstrate that multiple communicating encoders lead to a higher quality summary compared to several strong baselines, including those based on a single encoder or multiple non-communicating encoders.","pages":"1662--1675","doi":"10.18653\/v1\/N18-1150","url":"https:\/\/www.aclweb.org\/anthology\/N18-1150","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1151","title":"Encoding Conversation Context for Neural Keyphrase Extraction from Microblog Posts","authors":["Zhang, Yingyi","Li, Jing","Song, Yan","Zhang, Chengzhi"],"emails":["yingyizhang@njust.edu.cn","ameliajli@tencent.com","clksong@tencent.com","zhangcz@njust.edu.cn"],"author_id":["yingyi-zhang","jing-li","yan-song","chengzhi-zhang"],"abstract":"Existing keyphrase extraction methods suffer from data sparsity problem when they are conducted on short and informal texts, especially microblog messages. Enriching context is one way to alleviate this problem. Considering that conversations are formed by reposting and replying messages, they provide useful clues for recognizing essential content in target posts and are therefore helpful for keyphrase identification. In this paper, we present a neural keyphrase extraction framework for microblog posts that takes their conversation context into account, where four types of neural encoders, namely, averaged embedding, RNN, attention, and memory networks, are proposed to represent the conversation context. Experimental results on Twitter and Weibo datasets show that our framework with such encoders outperforms state-of-the-art approaches.","pages":"1676--1686","doi":"10.18653\/v1\/N18-1151","url":"https:\/\/www.aclweb.org\/anthology\/N18-1151","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1152","title":"Estimating Summary Quality with Pairwise Preferences","authors":["Zopf, Markus"],"emails":["zopf@aiphes.tu-darmstadt.de"],"author_id":["markus-zopf"],"abstract":"Automatic evaluation systems in the field of automatic summarization have been relying on the availability of gold standard summaries for over ten years. Gold standard summaries are expensive to obtain and often require the availability of domain experts to achieve high quality. In this paper, we propose an alternative evaluation approach based on pairwise preferences of sentences. In comparison to gold standard summaries, they are simpler and cheaper to obtain. In our experiments, we show that humans are able to provide useful feedback in the form of pairwise preferences. The new framework performs better than the three most popular versions of ROUGE with less expensive human input. We also show that our framework can reuse already available evaluation data and achieve even better results.","pages":"1687--1696","doi":"10.18653\/v1\/N18-1152","url":"https:\/\/www.aclweb.org\/anthology\/N18-1152","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1153","title":"Generating Topic-Oriented Summaries Using Neural Attention","authors":["Krishna, Kundan","Srinivasan, Balaji Vasan"],"emails":["kunkrish@adobe.com","balsrini@adobe.com"],"author_id":["kundan-krishna","balaji-vasan-srinivasan"],"abstract":"Summarizing a document requires identifying the important parts of the document with an objective of providing a quick overview to a reader. However, a long article can span several topics and a single summary cannot do justice to all the topics. Further, the interests of readers can vary and the notion of importance can change across them. Existing summarization algorithms generate a single summary and are not capable of generating multiple summaries tuned to the interests of the readers. In this paper, we propose an attention based RNN framework to generate multiple summaries of a single document tuned to different topics of interest. Our method outperforms existing baselines and our results suggest that the attention of generative networks can be successfully biased to look at sentences relevant to a topic and effectively used to generate topic-tuned summaries.","pages":"1697--1705","doi":"10.18653\/v1\/N18-1153","url":"https:\/\/www.aclweb.org\/anthology\/N18-1153","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1154","title":"Generative Bridging Network for Neural Sequence Prediction","authors":["Chen, Wenhu","Li, Guanlin","Ren, Shuo","Liu, Shujie","Zhang, Zhirui","Li, Mu","Zhou, Ming"],"emails":["wenhuchen@cs.ucsb.edu","shujliu@microsoft.com","v-shure@microsoft.com","epsilonlee.green@gmail.com","v-zhirzh@microsoft.com","muli@microsoft.com","mingzhou@microsoft.com"],"author_id":["wenhu-chen","guanlin-li","shuo-ren","shujie-liu","zhirui-zhang","mu-li","ming-zhou"],"abstract":"In order to alleviate data sparsity and overfitting problems in maximum likelihood estimation (MLE) for sequence prediction tasks, we propose the Generative Bridging Network (GBN), in which a novel bridge module is introduced to assist the training of the sequence prediction model (the generator network). Unlike MLE directly maximizing the conditional likelihood, the bridge extends the point-wise ground truth to a bridge distribution conditioned on it, and the generator is optimized to minimize their KL-divergence. Three different GBNs, namely uniform GBN, language-model GBN and coaching GBN, are proposed to penalize confidence, enhance language smoothness and relieve learning burden. Experiments conducted on two recognized sequence prediction tasks (machine translation and abstractive text summarization) show that our proposed GBNs can yield significant improvements over strong baselines. Furthermore, by analyzing samples drawn from different bridges, expected influences on the generator are verified.","pages":"1706--1715","doi":"10.18653\/v1\/N18-1154","url":"https:\/\/www.aclweb.org\/anthology\/N18-1154","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1155","title":"Higher-Order Syntactic Attention Network for Longer Sentence Compression","authors":["Kamigaito, Hidetaka","Hayashi, Katsuhiko","Hirao, Tsutomu","Nagata, Masaaki"],"emails":["kamigaito@lr.pi.titech.ac.jp","katsuhiko-h@sanken.osaka-u.ac.jp","hirao.tsutomu@lab.ntt.co.jp","nagata.masaaki@lab.ntt.co.jp"],"author_id":["hidetaka-kamigaito","katsuhiko-hayashi","tsutomu-hirao","masaaki-nagata"],"abstract":"A sentence compression method using LSTM can generate fluent compressed sentences. However, the performance of this method is significantly degraded when compressing longer sentences since it does not explicitly handle syntactic features. To solve this problem, we propose a higher-order syntactic attention network (HiSAN) that can handle higher-order dependency features as an attention distribution on LSTM hidden states. Furthermore, to avoid the influence of incorrect parse results, we trained HiSAN by maximizing jointly the probability of a correct output with the attention distribution. Experimental results on Google sentence compression dataset showed that our method achieved the best performance on F1 as well as ROUGE-1,2 and L scores, 83.2, 82.9, 75.8 and 82.7, respectively. In human evaluation, our methods also outperformed baseline methods in both readability and informativeness.","pages":"1716--1726","doi":"10.18653\/v1\/N18-1155","url":"https:\/\/www.aclweb.org\/anthology\/N18-1155","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1156","title":"Neural Storyline Extraction Model for Storyline Generation from News Articles","authors":["Zhou, Deyu","Guo, Linsen","He, Yulan"],"emails":["d.zhou@seu.edu.cn","guolinsen@seu.edu.cn","y.he@cantab.net"],"author_id":["deyu-zhou","linsen-guo","yulan-he"],"abstract":"Storyline generation aims to extract events described on news articles under a certain news topic and reveal how those events evolve over time. Most approaches to storyline generation first train supervised models to extract events from news articles published in different time periods and then link relevant extracted events into coherent stories. They are domain dependent and cannot deal with unseen event types. To tackle this problem, approaches based on probabilistic graphic models jointly model the generations of events and storylines without the use of annotated data. However, the parameter inference procedure is too complex and models often require long time to converge. In this paper, we propose a novel neural network based approach to extract structured representations and evolution patterns of storylines without using annotated data. In this model, title and main body of a news article are assumed to share the similar storyline distribution. Moreover, similar documents described in neighboring time periods are assumed to share similar storyline distributions. Based on these assumptions, structured representations and evolution patterns of storylines can be extracted. The proposed model has been evaluated on three news corpora and the experimental results show that it outperforms state-of-the-art approaches for storyline generation on both accuracy and efficiency.","pages":"1727--1736","doi":"10.18653\/v1\/N18-1156","url":"https:\/\/www.aclweb.org\/anthology\/N18-1156","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1157","title":"Provable Fast Greedy Compressive Summarization with Any Monotone Submodular Function","authors":["Sakaue, Shinsaku","Hirao, Tsutomu","Nishino, Masaaki","Nagata, Masaaki"],"emails":["sakaue.shinsaku@lab.ntt.co.jp","hirao.tsutomu@lab.ntt.co.jp","nishino.masaaki@lab.ntt.co.jp","nagata.masaaki@lab.ntt.co.jp"],"author_id":["shinsaku-sakaue","tsutomu-hirao","masaaki-nishino","masaaki-nagata"],"abstract":"Submodular maximization with the greedy algorithm has been studied as an effective approach to extractive summarization. This approach is known to have three advantages: its applicability to many useful submodular objective functions, the efficiency of the greedy algorithm, and the provable performance guarantee. However, when it comes to compressive summarization, we are currently missing a counterpart of the extractive method based on submodularity. In this paper, we propose a fast greedy method for compressive summarization. Our method is applicable to any monotone submodular objective function, including many functions well-suited for document summarization. We provide an approximation guarantee of our greedy algorithm. Experiments show that our method is about 100 to 400 times faster than an existing method based on integer-linear-programming (ILP) formulations and that our method empirically achieves more than 95{\\%}-approximation.","pages":"1737--1746","doi":"10.18653\/v1\/N18-1157","url":"https:\/\/www.aclweb.org\/anthology\/N18-1157","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1158","title":"Ranking Sentences for Extractive Summarization with Reinforcement Learning","authors":["Narayan, Shashi","Cohen, Shay B.","Lapata, Mirella"],"emails":["shashi.narayan@ed.ac.uk","scohen@inf.ed.ac.uk","mlap@inf.ed.ac.uk"],"author_id":["shashi-narayan","shay-b-cohen","mirella-lapata"],"abstract":"Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective. We use our algorithm to train a neural summarization model on the CNN and DailyMail datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and abstractive systems when evaluated automatically and by humans.","pages":"1747--1759","doi":"10.18653\/v1\/N18-1158","url":"https:\/\/www.aclweb.org\/anthology\/N18-1158","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1159","title":"Relational Summarization for Corpus Analysis","authors":["Handler, Abram","O{'}Connor, Brendan"],"emails":["ahandler@cs.umass.edu","brenocon@cs.umass.edu"],"author_id":["abram-handler","brendan-oconnor"],"abstract":"This work introduces a new problem, relational summarization, in which the goal is to generate a natural language summary of the relationship between two lexical items in a corpus, without reference to a knowledge base. Motivated by the needs of novel user interfaces, we define the task and give examples of its application. We also present a new query-focused method for finding natural language sentences which express relationships. Our method allows for summarization of more than two times more query pairs than baseline relation extractors, while returning measurably more readable output. Finally, to help guide future work, we analyze the challenges of relational summarization using both a news and a social media corpus.","pages":"1760--1769","doi":"10.18653\/v1\/N18-1159","url":"https:\/\/www.aclweb.org\/anthology\/N18-1159","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1160","title":"What{'}s This Movie About? A Joint Neural Network Architecture for Movie Content Analysis","authors":["Gorinski, Philip John","Lapata, Mirella"],"emails":["orinski@sms.ed.ac.uk","mlap@inf.ed.ac.uk"],"author_id":["philip-gorinski","mirella-lapata"],"abstract":"This work takes a first step toward movie content analysis by tackling the novel task of movie overview generation. Overviews are natural language texts that give a first impression of a movie, describing aspects such as its genre, plot, mood, or artistic style. We create a dataset that consists of movie scripts, attribute-value pairs for the movies{'} aspects, as well as overviews, which we extract from an online database. We present a novel end-to-end model for overview generation, consisting of a multi-label encoder for identifying screenplay attributes, and an LSTM decoder to generate natural language sentences conditioned on the identified attributes. Automatic and human evaluation show that the encoder is able to reliably assign good labels for the movie{'}s attributes, and the overviews provide descriptions of the movie{'}s content which are informative and faithful.","pages":"1770--1781","doi":"10.18653\/v1\/N18-1160","url":"https:\/\/www.aclweb.org\/anthology\/N18-1160","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1161","title":"Which Scores to Predict in Sentence Regression for Text Summarization?","authors":["Zopf, Markus","Loza Menc{\\'\\i}a, Eneldo","F{\\\"u}rnkranz, Johannes"],"emails":["","",""],"author_id":["markus-zopf","eneldo-loza-mencia","johannes-furnkranz"],"abstract":"The task of automatic text summarization is to generate a short text that summarizes the most important information in a given set of documents. Sentence regression is an emerging branch in automatic text summarizations. Its key idea is to estimate the importance of information via learned utility scores for individual sentences. These scores are then used for selecting sentences from the source documents, typically according to a greedy selection strategy. Recently proposed state-of-the-art models learn to predict ROUGE recall scores of individual sentences, which seems reasonable since the final summaries are evaluated according to ROUGE recall. In this paper, we show in extensive experiments that following this intuition leads to suboptimal results and that learning to predict ROUGE precision scores leads to better results. The crucial difference is to aim not at covering as much information as possible but at wasting as little space as possible in every greedy step.","pages":"1782--1791","doi":"10.18653\/v1\/N18-1161","url":"https:\/\/www.aclweb.org\/anthology\/N18-1161","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1162","title":"A Hierarchical Latent Structure for Variational Conversation Modeling","authors":["Park, Yookoon","Cho, Jaemin","Kim, Gunhee"],"emails":["yookoonpark@vision.snu.ac.kr","jaemin895@snu.ac.kr","gunhee@snu.ac.kr"],"author_id":["yookoon-park","jaemin-cho","gunhee-kim"],"abstract":"Variational autoencoders (VAE) combined with hierarchical RNNs have emerged as a powerful framework for conversation modeling. However, they suffer from the notorious degeneration problem, where the decoders learn to ignore latent variables and reduce to vanilla RNNs. We empirically show that this degeneracy occurs mostly due to two reasons. First, the expressive power of hierarchical RNN decoders is often high enough to model the data using only its decoding distributions without relying on the latent variables. Second, the conditional VAE structure whose generation process is conditioned on a context, makes the range of training targets very sparse; that is, the RNN decoders can easily overfit to the training data ignoring the latent variables. To solve the degeneration problem, we propose a novel model named Variational Hierarchical Conversation RNNs (VHCR), involving two key ideas of (1) using a hierarchical structure of latent variables, and (2) exploiting an utterance drop regularization. With evaluations on two datasets of Cornell Movie Dialog and Ubuntu Dialog Corpus, we show that our VHCR successfully utilizes latent variables and outperforms state-of-the-art models for conversation generation. Moreover, it can perform several new utterance control tasks, thanks to its hierarchical latent structure.","pages":"1792--1801","doi":"10.18653\/v1\/N18-1162","url":"https:\/\/www.aclweb.org\/anthology\/N18-1162","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1163","title":"Detecting Egregious Conversations between Customers and Virtual Agents","authors":["Sandbank, Tommy","Shmueli-Scheuer, Michal","Herzig, Jonathan","Konopnicki, David","Richards, John","Piorkowski, David"],"emails":["tommy@il.ibm.com","shmueli@il.ibm.com","ajtr@us.ibm.com","davidko@il.ibm.com","hjon@il.ibm.com","david.piorkowski@ibm.com"],"author_id":["tommy-sandbank","michal-shmueli-scheuer","jonathan-herzig","david-konopnicki","john-richards","david-piorkowski"],"abstract":"Virtual agents are becoming a prominent channel of interaction in customer service. Not all customer interactions are smooth, however, and some can become almost comically bad. In such instances, a human agent might need to step in and salvage the conversation. Detecting bad conversations is important since disappointing customer service may threaten customer loyalty and impact revenue. In this paper, we outline an approach to detecting such egregious conversations, using behavioral cues from the user, patterns in agent responses, and user-agent interaction. Using logs of two commercial systems, we show that using these features improves the detection F1-score by around 20{\\%} over using textual features alone. In addition, we show that those features are common across two quite different domains and, arguably, universal.","pages":"1802--1811","doi":"10.18653\/v1\/N18-1163","url":"https:\/\/www.aclweb.org\/anthology\/N18-1163","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1164","title":"Learning to Disentangle Interleaved Conversational Threads with a {S}iamese Hierarchical Network and Similarity Ranking","authors":["Jiang, Jyun-Yu","Chen, Francine","Chen, Yan-Ying","Wang, Wei"],"emails":["jyunyu@cs.ucla.edu","chen@fxpal.com","yanying@fxpal.com","weiwang@cs.ucla.edu"],"author_id":["jyun-yu-jiang","francine-chen","yan-ying-chen","wei-wang"],"abstract":"An enormous amount of conversation occurs online every day, such as on chat platforms where multiple conversations may take place concurrently. Interleaved conversations lead to difficulties in not only following discussions but also retrieving relevant information from simultaneous messages. Conversation disentanglement aims to separate intermingled messages into detached conversations. In this paper, we propose to leverage representation learning for conversation disentanglement. A Siamese hierarchical convolutional neural network (SHCNN), which integrates local and more global representations of a message, is first presented to estimate the conversation-level similarity between closely posted messages. With the estimated similarity scores, our algorithm for conversation identification by similarity ranking (CISIR) then derives conversations based on high-confidence message pairs and pairwise redundancy. Experiments were conducted with four publicly available datasets of conversations from Reddit and IRC channels. The experimental results show that our approach significantly outperforms comparative baselines in both pairwise similarity estimation and conversation disentanglement.","pages":"1812--1822","doi":"10.18653\/v1\/N18-1164","url":"https:\/\/www.aclweb.org\/anthology\/N18-1164","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1165","title":"Variational Knowledge Graph Reasoning","authors":["Chen, Wenhu","Xiong, Wenhan","Yan, Xifeng","Wang, William Yang"],"emails":["wenhuchen@cs.ucsb.edu","xwhan@cs.ucsb.edu","xyan@cs.ucsb.edu","william@cs.ucsb.edu"],"author_id":["wenhu-chen","wenhan-xiong","xifeng-yan","william-yang-wang"],"abstract":"Inferring missing links in knowledge graphs (KG) has attracted a lot of attention from the research community. In this paper, we tackle a practical query answering task involving predicting the relation of a given entity pair. We frame this prediction problem as an inference problem in a probabilistic graphical model and aim at resolving it from a variational inference perspective. In order to model the relation between the query entity pair, we assume that there exists an underlying latent variable (paths connecting two nodes) in the KG, which carries the equivalent semantics of their relations. However, due to the intractability of connections in large KGs, we propose to use variation inference to maximize the evidence lower bound. More specifically, our framework (Diva) is composed of three modules, i.e. a posterior approximator, a prior (path finder), and a likelihood (path reasoner). By using variational inference, we are able to incorporate them closely into a unified architecture and jointly optimize them to perform KG reasoning. With active interactions among these sub-modules, Diva is better at handling noise and coping with more complex reasoning scenarios. In order to evaluate our method, we conduct the experiment of the link prediction task on multiple datasets and achieve state-of-the-art performances on both datasets.","pages":"1823--1832","doi":"10.18653\/v1\/N18-1165","url":"https:\/\/www.aclweb.org\/anthology\/N18-1165","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1166","title":"Inducing Temporal Relations from Time Anchor Annotation","authors":["Cheng, Fei","Miyao, Yusuke"],"emails":["fei-cheng@nii.ac.jp","yusuke@nii.ac.jp"],"author_id":["fei-cheng","yusuke-miyao"],"abstract":"Recognizing temporal relations among events and time expressions has been an essential but challenging task in natural language processing. Conventional annotation of judging temporal relations puts a heavy load on annotators. In reality, the existing annotated corpora include annotations on only {``}salient{''} event pairs, or on pairs in a fixed window of sentences. In this paper, we propose a new approach to obtain temporal relations from absolute time value (a.k.a. time anchors), which is suitable for texts containing rich temporal information such as news articles. We start from time anchors for events and time expressions, and temporal relation annotations are induced automatically by computing relative order of two time anchors. This proposal shows several advantages over the current methods for temporal relation annotation: it requires less annotation effort, can induce inter-sentence relations easily, and increases informativeness of temporal relations. We compare the empirical statistics and automatic recognition results with our data against a previous temporal relation corpus. We also reveal that our data contributes to a significant improvement of the downstream time anchor prediction task, demonstrating 14.1 point increase in overall accuracy.","pages":"1833--1843","doi":"10.18653\/v1\/N18-1166","url":"https:\/\/www.aclweb.org\/anthology\/N18-1166","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1167","title":"{ELDEN}: Improved Entity Linking Using Densified Knowledge Graphs","authors":["Radhakrishnan, Priya","Talukdar, Partha","Varma, Vasudeva"],"emails":["priya.r@research.iiit.ac.in","ppt@iisc.ac.in","vv@iiit.ac.in"],"author_id":["priya-radhakrishnan","partha-talukdar","vasudeva-varma"],"abstract":"Entity Linking (EL) systems aim to automatically map mentions of an entity in text to the corresponding entity in a Knowledge Graph (KG). Degree of connectivity of an entity in the KG directly affects an EL system{'}s ability to correctly link mentions in text to the entity in KG. This causes many EL systems to perform well for entities well connected to other entities in KG, bringing into focus the role of KG density in EL. In this paper, we propose Entity Linking using Densified Knowledge Graphs (ELDEN). ELDEN is an EL system which first densifies the KG with co-occurrence statistics from a large text corpus, and then uses the densified KG to train entity embeddings. Entity similarity measured using these trained entity embeddings result in improved EL. ELDEN outperforms state-of-the-art EL system on benchmark datasets. Due to such densification, ELDEN performs well for sparsely connected entities in the KG too. ELDEN{'}s approach is simple, yet effective. We have made ELDEN{'}s code and data publicly available.","pages":"1844--1853","doi":"10.18653\/v1\/N18-1167","url":"https:\/\/www.aclweb.org\/anthology\/N18-1167","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1168","title":"Interpretable Charge Predictions for Criminal Cases: Learning to Generate Court Views from Fact Descriptions","authors":["Ye, Hai","Jiang, Xin","Luo, Zhunchen","Chao, Wenhan"],"emails":["yehai@buaa.edu.cn","xinjiang@buaa.edu.cn","zhunchenluo@gmail.com","chaowenhan@buaa.edu.cn"],"author_id":["hai-ye","xin-jiang","zhunchen-luo","wenhan-chao"],"abstract":"In this paper, we propose to study the problem of court view generation from the fact description in a criminal case. The task aims to improve the interpretability of charge prediction systems and help automatic legal document generation. We formulate this task as a text-to-text natural language generation (NLG) problem. Sequence-to-sequence model has achieved cutting-edge performances in many NLG tasks. However, due to the non-distinctions of fact descriptions, it is hard for Seq2Seq model to generate charge-discriminative court views. In this work, we explore charge labels to tackle this issue. We propose a label-conditioned Seq2Seq model with attention for this problem, to decode court views conditioned on encoded charge labels. Experimental results show the effectiveness of our method.","pages":"1854--1864","doi":"10.18653\/v1\/N18-1168","url":"https:\/\/www.aclweb.org\/anthology\/N18-1168","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1169","title":"Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer","authors":["Li, Juncen","Jia, Robin","He, He","Liang, Percy"],"emails":["juncenli@tencent.com","robinjia@cs.stanford.edu","hehe@cs.stanford.edu","pliang@cs.stanford.edu"],"author_id":["juncen-li","robin-jia","he-he","percy-liang"],"abstract":"We consider the task of text attribute transfer: transforming a sentence to alter a specific attribute (e.g., sentiment) while preserving its attribute-independent content (e.g., {``}screen is just the right size{''} to {``}screen is too small{''}). Our training data includes only sentences labeled with their attribute (e.g., positive and negative), but not pairs of sentences that only differ in the attributes, so we must learn to disentangle attributes from attribute-independent content in an unsupervised way. Previous work using adversarial methods has struggled to produce high-quality outputs. In this paper, we propose simpler methods motivated by the observation that text attributes are often marked by distinctive phrases (e.g., {``}too small{''}). Our strongest method extracts content words by deleting phrases associated with the sentence{'}s original attribute value, retrieves new phrases associated with the target attribute, and uses a neural model to fluently combine these into a final output. Based on human evaluation, our best method generates grammatical and appropriate responses on 22{\\%} more inputs than the best previous system, averaged over three attribute transfer datasets: altering sentiment of reviews on Yelp, altering sentiment of reviews on Amazon, and altering image captions to be more romantic or humorous.","pages":"1865--1874","doi":"10.18653\/v1\/N18-1169","url":"https:\/\/www.aclweb.org\/anthology\/N18-1169","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1170","title":"Adversarial Example Generation with Syntactically Controlled Paraphrase Networks","authors":["Iyyer, Mohit","Wieting, John","Gimpel, Kevin","Zettlemoyer, Luke"],"emails":["miyyer@cs.umass.edu","jwieting@cs.cmu.edu","kgimpel@ttic.edu","lsz@cs.washington.edu"],"author_id":["mohit-iyyer","john-wieting","kevin-gimpel","luke-zettlemoyer"],"abstract":"We propose syntactically controlled paraphrase networks (SCPNs) and use them to generate adversarial examples. Given a sentence and a target syntactic form (e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the sentence with the desired syntax. We show it is possible to create training data for this task by first doing backtranslation at a very large scale, and then using a parser to label the syntactic transformations that naturally occur during this process. Such data allows us to train a neural encoder-decoder model with extra inputs to specify the target syntax. A combination of automated and human evaluations show that SCPNs generate paraphrases that follow their target specifications without decreasing paraphrase quality when compared to baseline (uncontrolled) paraphrase systems. Furthermore, they are more capable of generating syntactically adversarial examples that both (1) {``}fool{''} pretrained models and (2) improve the robustness of these models to syntactic variation when used to augment their training data.","pages":"1875--1885","doi":"10.18653\/v1\/N18-1170","url":"https:\/\/www.aclweb.org\/anthology\/N18-1170","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1171","title":"Sentiment Analysis: It{'}s Complicated!","authors":["Kenyon-Dean, Kian","Ahmed, Eisha","Fujimoto, Scott","Georges-Filteau, Jeremy","Glasz, Christopher","Kaur, Barleen","Lalande, Auguste","Bhanderi, Shruti","Belfer, Robert","Kanagasabai, Nirmal","Sarrazingendron, Roman","Verma, Rohit","Ruths, Derek"],"emails":["kian.kenyon-dean@mail.mcgill.ca","eisha.ahmed@mail.mcgill.ca","scott.fujimoto@mail.mcgill.ca","jeremy.georges-filteau@mail.mcgill.ca","christopher.glasz@mail.mcgill.ca","barleen.kaur@mail.mcgill.ca","auguste.lalande@mail.mcgill.ca","shruti.bhanderi@mail.mcgill.ca","robert.belfer@mail.mcgill.ca","nirmal.kanagasabai@mail.mcgill.ca","roman.sarrazingendron@mail.mcgill.ca","rohit.verma@mail.mcgill.ca","derek.ruths@mcgill.ca"],"author_id":["kian-kenyon-dean","eisha-ahmed","scott-fujimoto","jeremy-georges-filteau","christopher-glasz","barleen-kaur","auguste-lalande","shruti-bhanderi","robert-belfer","nirmal-kanagasabai","roman-sarrazingendron","rohit-verma","derek-ruths"],"abstract":"Sentiment analysis is used as a proxy to measure human emotion, where the objective is to categorize text according to some predefined notion of sentiment. Sentiment analysis datasets are typically constructed with gold-standard sentiment labels, assigned based on the results of manual annotations. When working with such annotations, it is common for dataset constructors to discard {``}noisy{''} or {``}controversial{''} data where there is significant disagreement on the proper label. In datasets constructed for the purpose of Twitter sentiment analysis (TSA), these controversial examples can compose over 30{\\%} of the originally annotated data. We argue that the removal of such data is a problematic trend because, when performing real-time sentiment classification of short-text, an automated system cannot know a priori which samples would fall into this category of disputed sentiment. We therefore propose the notion of a {``}complicated{''} class of sentiment to categorize such text, and argue that its inclusion in the short-text sentiment analysis framework will improve the quality of automated sentiment analysis systems as they are implemented in real-world settings. We motivate this argument by building and analyzing a new publicly available TSA dataset of over 7,000 tweets annotated with 5x coverage, named MTSA. Our analysis of classifier performance over our dataset offers insights into sentiment analysis dataset and model design, how current techniques would perform in the real world, and how researchers should handle difficult data.","pages":"1886--1895","doi":"10.18653\/v1\/N18-1171","url":"https:\/\/www.aclweb.org\/anthology\/N18-1171","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1172","title":"Multi-Task Learning of Pairwise Sequence Classification Tasks over Disparate Label Spaces","authors":["Augenstein, Isabelle","Ruder, Sebastian","S{\\o}gaard, Anders"],"emails":["augenstein@di.ku.dk","sebastian@ruder.io","soegaard@di.ku.dk"],"author_id":["isabelle-augenstein","sebastian-ruder","anders-sogaard"],"abstract":"We combine multi-task learning and semi-supervised learning by inducing a joint embedding space between disparate label spaces and learning transfer functions between label embeddings, enabling us to jointly leverage unlabelled data and auxiliary, annotated datasets. We evaluate our approach on a variety of tasks with disparate label spaces. We outperform strong single and multi-task baselines and achieve a new state of the art for aspect-based and topic-based sentiment analysis.","pages":"1896--1906","doi":"10.18653\/v1\/N18-1172","url":"https:\/\/www.aclweb.org\/anthology\/N18-1172","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1173","title":"Word Emotion Induction for Multiple Languages as a Deep Multi-Task Learning Problem","authors":["Buechel, Sven","Hahn, Udo"],"emails":["sven.buechel@uni-jena.de","udo.hahn@uni-jena.de"],"author_id":["sven-buechel","udo-hahn"],"abstract":"Predicting the emotional value of lexical items is a well-known problem in sentiment analysis. While research has focused on polarity for quite a long time, meanwhile this early focus has been shifted to more expressive emotion representation models (such as Basic Emotions or Valence-Arousal-Dominance). This change resulted in a proliferation of heterogeneous formats and, in parallel, often small-sized, non-interoperable resources (lexicons and corpus annotations). In particular, the limitations in size hampered the application of deep learning methods in this area because they typically require large amounts of input data. We here present a solution to get around this language data bottleneck by rephrasing word emotion induction as a multi-task learning problem. In this approach, the prediction of each independent emotion dimension is considered as an individual task and hidden layers are shared between these dimensions. We investigate whether multi-task learning is more advantageous than single-task learning for emotion prediction by comparing our model against a wide range of alternative emotion and polarity induction methods featuring 9 typologically diverse languages and a total of 15 conditions. Our model turns out to outperform each one of them. Against all odds, the proposed deep learning approach yields the largest gain on the smallest data sets, merely composed of one thousand samples.","pages":"1907--1918","doi":"10.18653\/v1\/N18-1173","url":"https:\/\/www.aclweb.org\/anthology\/N18-1173","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1174","title":"Human Needs Categorization of Affective Events Using Labeled and Unlabeled Data","authors":["Ding, Haibo","Riloff, Ellen"],"emails":["hbding@cs.utah.edu","riloff@cs.utah.edu"],"author_id":["haibo-ding","ellen-riloff"],"abstract":"We often talk about events that impact us positively or negatively. For example {``}I got a job{''} is good news, but {``}I lost my job{''} is bad news. When we discuss an event, we not only understand its affective polarity but also the reason why the event is beneficial or detrimental. For example, getting or losing a job has affective polarity primarily because it impacts us financially. Our work aims to categorize affective events based upon human need categories that often explain people{'}s motivations and desires: PHYSIOLOGICAL, HEALTH, LEISURE, SOCIAL, FINANCIAL, COGNITION, and FREEDOM. We create classification models based on event expressions as well as models that use contexts surrounding event mentions. We also design a co-training model that learns from unlabeled data by simultaneously training event expression and event context classifiers in an iterative learning process. Our results show that co-training performs well, producing substantially better results than the individual classifiers.","pages":"1919--1929","doi":"10.18653\/v1\/N18-1174","url":"https:\/\/www.aclweb.org\/anthology\/N18-1174","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1175","title":"The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants","authors":["Habernal, Ivan","Wachsmuth, Henning","Gurevych, Iryna","Stein, Benno"],"emails":["","","",""],"author_id":["ivan-habernal","henning-wachsmuth","iryna-gurevych","benno-stein"],"abstract":"Reasoning is a crucial part of natural language argumentation. To comprehend an argument, one must analyze its warrant, which explains why its claim follows from its premises. As arguments are highly contextualized, warrants are usually presupposed and left implicit. Thus, the comprehension does not only require language understanding and logic skills, but also depends on common sense. In this paper we develop a methodology for reconstructing warrants systematically. We operationalize it in a scalable crowdsourcing process, resulting in a freely licensed dataset with warrants for 2k authentic arguments from news comments. On this basis, we present a new challenging task, the argument reasoning comprehension task. Given an argument with a claim and a premise, the goal is to choose the correct implicit warrant from two options. Both warrants are plausible and lexically close, but lead to contradicting claims. A solution to this task will define a substantial step towards automatic warrant reconstruction. However, experiments with several neural attention and language models reveal that current approaches do not suffice.","pages":"1930--1940","doi":"10.18653\/v1\/N18-1175","url":"https:\/\/www.aclweb.org\/anthology\/N18-1175","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1176","title":"Linguistic Cues to Deception and Perceived Deception in Interview Dialogues","authors":["Levitan, Sarah Ita","Maredia, Angel","Hirschberg, Julia"],"emails":["","",""],"author_id":["sarah-ita-levitan","angel-maredia","julia-hirschberg"],"abstract":"We explore deception detection in interview dialogues. We analyze a set of linguistic features in both truthful and deceptive responses to interview questions. We also study the perception of deception, identifying characteristics of statements that are perceived as truthful or deceptive by interviewers. Our analysis show significant differences between truthful and deceptive question responses, as well as variations in deception patterns across gender and native language. This analysis motivated our selection of features for machine learning experiments aimed at classifying globally deceptive speech. Our best classification performance is 72.74{\\%} F1-Score (about 17{\\%} better than human performance), which is achieved using a combination of linguistic features and individual traits.","pages":"1941--1950","doi":"10.18653\/v1\/N18-1176","url":"https:\/\/www.aclweb.org\/anthology\/N18-1176","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1177","title":"Unified Pragmatic Models for Generating and Following Instructions","authors":["Fried, Daniel","Andreas, Jacob","Klein, Dan"],"emails":["dfried@cs.berkeley.edu","jda@cs.berkeley.edu","klein@cs.berkeley.edu"],"author_id":["daniel-fried","jacob-andreas","dan-klein"],"abstract":"We show that explicit pragmatic inference aids in correctly generating and following natural language instructions for complex, sequential tasks. Our pragmatics-enabled models reason about why speakers produce certain instructions, and about how listeners will react upon hearing them. Like previous pragmatic models, we use learned base listener and speaker models to build a pragmatic speaker that uses the base listener to simulate the interpretation of candidate descriptions, and a pragmatic listener that reasons counterfactually about alternative descriptions. We extend these models to tasks with sequential structure. Evaluation of language generation and interpretation shows that pragmatic inference improves state-of-the-art listener models (at correctly interpreting human instructions) and speaker models (at producing instructions correctly interpreted by humans) in diverse settings.","pages":"1951--1963","doi":"10.18653\/v1\/N18-1177","url":"https:\/\/www.aclweb.org\/anthology\/N18-1177","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1178","title":"Hierarchical Structured Model for Fine-to-Coarse Manifesto Text Analysis","authors":["Subramanian, Shivashankar","Cohn, Trevor","Baldwin, Timothy"],"emails":["shivashankar@student.unimelb.edu.au","t.cohn@unimelb.edu.au","tbaldwin@unimelb.edu.au"],"author_id":["shivashankar-subramanian","trevor-cohn","timothy-baldwin"],"abstract":"Election manifestos document the intentions, motives, and views of political parties. They are often used for analysing a party{'}s fine-grained position on a particular issue, as well as for coarse-grained positioning of a party on the left{--}right spectrum. In this paper we propose a two-stage model for automatically performing both levels of analysis over manifestos. In the first step we employ a hierarchical multi-task structured deep model to predict fine- and coarse-grained positions, and in the second step we perform post-hoc calibration of coarse-grained positions using probabilistic soft logic. We empirically show that the proposed model outperforms state-of-art approaches at both granularities using manifestos from twelve countries, written in ten different languages.","pages":"1964--1974","doi":"10.18653\/v1\/N18-1178","url":"https:\/\/www.aclweb.org\/anthology\/N18-1178","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1179","title":"Behavior Analysis of {NLI} Models: Uncovering the Influence of Three Factors on Robustness","authors":["Sanchez, Ivan","Mitchell, Jeff","Riedel, Sebastian"],"emails":["i.sanchezcarmona@cs.ucl.ac.uk","j.mitchell@cs.ucl.ac.uk","s.riedel@cs.ucl.ac.uk"],"author_id":["ivan-sanchez","jeff-mitchell","sebastian-riedel"],"abstract":"Natural Language Inference is a challenging task that has received substantial attention, and state-of-the-art models now achieve impressive test set performance in the form of accuracy scores. Here, we go beyond this single evaluation metric to examine robustness to semantically-valid alterations to the input data. We identify three factors - insensitivity, polarity and unseen pairs - and compare their impact on three SNLI models under a variety of conditions. Our results demonstrate a number of strengths and weaknesses in the models{'} ability to generalise to new in-domain instances. In particular, while strong performance is possible on unseen hypernyms, unseen antonyms are more challenging for all the models. More generally, the models suffer from an insensitivity to certain small but semantically significant alterations, and are also often influenced by simple statistical correlations between words and training labels. Overall, we show that evaluations of NLI models can benefit from studying the influence of factors intrinsic to the models or found in the dataset used.","pages":"1975--1985","doi":"10.18653\/v1\/N18-1179","url":"https:\/\/www.aclweb.org\/anthology\/N18-1179","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1180","title":"Assessing Language Proficiency from Eye Movements in Reading","authors":["Berzak, Yevgeni","Katz, Boris","Levy, Roger"],"emails":["berzak@mit.edu","boris@mit.edu","rplevy@mit.edu"],"author_id":["yevgeni-berzak","boris-katz","roger-levy"],"abstract":"We present a novel approach for determining learners{'} second language proficiency which utilizes behavioral traces of eye movements during reading. Our approach provides stand-alone eyetracking based English proficiency scores which reflect the extent to which the learner{'}s gaze patterns in reading are similar to those of native English speakers. We show that our scores correlate strongly with standardized English proficiency tests. We also demonstrate that gaze information can be used to accurately predict the outcomes of such tests. Our approach yields the strongest performance when the test taker is presented with a suite of sentences for which we have eyetracking data from other readers. However, it remains effective even using eyetracking with sentences for which eye movement data have not been previously collected. By deriving proficiency as an automatic byproduct of eye movements during ordinary reading, our approach offers a potentially valuable new tool for second language proficiency assessment. More broadly, our results open the door to future methods for inferring reader characteristics from the behavioral traces of reading.","pages":"1986--1996","doi":"10.18653\/v1\/N18-1180","url":"https:\/\/www.aclweb.org\/anthology\/N18-1180","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1181","title":"Comparing Theories of Speaker Choice Using a Model of Classifier Production in {M}andarin {C}hinese","authors":["Zhan, Meilin","Levy, Roger"],"emails":["meilinz@mit.edu","rplevy@mit.edu"],"author_id":["meilin-zhan","roger-levy"],"abstract":"Speakers often have more than one way to express the same meaning. What general principles govern speaker choice in the face of optionality when near semantically invariant alternation exists? Studies have shown that optional reduction in language is sensitive to contextual predictability, such that more predictable a linguistic unit is, the more likely it is to get reduced. Yet it is unclear whether these cases of speaker choice are driven by audience design versus toward facilitating production. Here we argue that for a different optionality phenomenon, namely classifier choice in Mandarin Chinese, Uniform Information Density and at least one plausible variant of availability-based production make opposite predictions regarding the relationship between the predictability of the upcoming material and speaker choices. In a corpus analysis of Mandarin Chinese, we show that the distribution of speaker choices supports the availability-based production account and not the Uniform Information Density.","pages":"1997--2005","doi":"10.18653\/v1\/N18-1181","url":"https:\/\/www.aclweb.org\/anthology\/N18-1181","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1182","title":"Spotting Spurious Data with Neural Networks","authors":["Amiri, Hadi","Miller, Timothy","Savova, Guergana"],"emails":["hadi.amiri@childrens.harvard.edu","timothy.miller@childrens.harvard.edu","guergana.savova@childrens.harvard.edu"],"author_id":["hadi-amiri","timothy-miller","guergana-savova"],"abstract":"Automatic identification of spurious instances (those with potentially wrong labels in datasets) can improve the quality of existing language resources, especially when annotations are obtained through crowdsourcing or automatically generated based on coded rankings. In this paper, we present effective approaches inspired by queueing theory and psychology of learning to automatically identify spurious instances in datasets. Our approaches discriminate instances based on their {``}difficulty to learn,{''} determined by a downstream learner. Our methods can be applied to any dataset assuming the existence of a neural network model for the target task of the dataset. Our best approach outperforms competing state-of-the-art baselines and has a MAP of 0.85 and 0.22 in identifying spurious instances in synthetic and carefully-crowdsourced real-world datasets respectively.","pages":"2006--2016","doi":"10.18653\/v1\/N18-1182","url":"https:\/\/www.aclweb.org\/anthology\/N18-1182","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1183","title":"The Timing of Lexical Memory Retrievals in Language Production","authors":["Cole, Jeremy","Reitter, David"],"emails":["jrcole@psu.edu","reitter@psu.edu"],"author_id":["jeremy-cole","david-reitter"],"abstract":"This paper explores the time course of lexical memory retrieval by modeling fluent language production. The duration of retrievals is predicted using the ACT-R cognitive architecture. In a large-scale observational study of a spoken corpus, we find that language production at a time point preceding a word is sped up or slowed down depending on activation of that word. This computational analysis has consequences for the theoretical model of language production. The results point to interference between lexical and phonological stages as well as a quantifiable buffer for lexical information that opens up the possibility of non-sequential retrievals.","pages":"2017--2027","doi":"10.18653\/v1\/N18-1183","url":"https:\/\/www.aclweb.org\/anthology\/N18-1183","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1184","title":"Unsupervised Induction of Linguistic Categories with Records of Reading, Speaking, and Writing","authors":["Barrett, Maria","Gonz{\\'a}lez-Gardu{\\~n}o, Ana Valeria","Frermann, Lea","S{\\o}gaard, Anders"],"emails":["barrett@hum.ku.dk","ana@di.ku.dk","lfrerman@amazon.com","soegaard@di.ku.dk"],"author_id":["maria-barrett","ana-valeria-gonzalez-garduno","lea-frermann","anders-sogaard"],"abstract":"When learning POS taggers and syntactic chunkers for low-resource languages, different resources may be available, and often all we have is a small tag dictionary, motivating type-constrained unsupervised induction. Even small dictionaries can improve the performance of unsupervised induction algorithms. This paper shows that performance can be further improved by including data that is readily available or can be easily obtained for most languages, i.e., eye-tracking, speech, or keystroke logs (or any combination thereof). We project information from all these data sources into shared spaces, in which the union of words is represented. For English unsupervised POS induction, the additional information, which is not required at test time, leads to an average error reduction on Ontonotes domains of 1.5{\\%} over systems augmented with state-of-the-art word embeddings. On Penn Treebank the best model achieves 5.4{\\%} error reduction over a word embeddings baseline. We also achieve significant improvements for syntactic chunk induction. Our analysis shows that improvements are even bigger when the available tag dictionaries are smaller.","pages":"2028--2038","doi":"10.18653\/v1\/N18-1184","url":"https:\/\/www.aclweb.org\/anthology\/N18-1184","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1185","title":"Challenging Reading Comprehension on Daily Conversation: Passage Completion on Multiparty Dialog","authors":["Ma, Kaixin","Jurczyk, Tomasz","Choi, Jinho D."],"emails":["kaixin.ma@emory.edu","tomasz.jurczyk@emory.edu","jinho.choi@emory.edu"],"author_id":["kaixin-ma","tomasz-jurczyk","jinho-d-choi"],"abstract":"This paper presents a new corpus and a robust deep learning architecture for a task in reading comprehension, passage completion, on multiparty dialog. Given a dialog in text and a passage containing factual descriptions about the dialog where mentions of the characters are replaced by blanks, the task is to fill the blanks with the most appropriate character names that reflect the contexts in the dialog. Since there is no dataset that challenges the task of passage completion in this genre, we create a corpus by selecting transcripts from a TV show that comprise 1,681 dialogs, generating passages for each dialog through crowdsourcing, and annotating mentions of characters in both the dialog and the passages. Given this dataset, we build a deep neural model that integrates rich feature extraction from convolutional neural networks into sequence modeling in recurrent neural networks, optimized by utterance and dialog level attentions. Our model outperforms the previous state-of-the-art model on this task in a different genre using bidirectional LSTM, showing a 13.0+{\\%} improvement for longer dialogs. Our analysis shows the effectiveness of the attention mechanisms and suggests a direction to machine comprehension on multiparty dialog.","pages":"2039--2048","doi":"10.18653\/v1\/N18-1185","url":"https:\/\/www.aclweb.org\/anthology\/N18-1185","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1186","title":"Dialog Generation Using Multi-Turn Reasoning Neural Networks","authors":["Wu, Xianchao","Mart{\\'\\i}nez, Ander","Klyen, Momo"],"emails":["xiancwu@microsoft.com","ander.martinez.zy4@is.naist.jp","momokl@microsoft.com"],"author_id":["xianchao-wu","ander-martinez1","momo-klyen"],"abstract":"In this paper, we propose a generalizable dialog generation approach that adapts multi-turn reasoning, one recent advancement in the field of document comprehension, to generate responses ({``}answers{''}) by taking current conversation session context as a {``}document{''} and current query as a {``}question{''}. The major idea is to represent a conversation session into memories upon which attention-based memory reading mechanism can be performed multiple times, so that (1) user{'}s query is properly extended by contextual clues and (2) optimal responses are step-by-step generated. Considering that the speakers of one conversation are not limited to be one, we separate the single memory used for document comprehension into different groups for speaker-specific topic and opinion embedding. Namely, we utilize the queries{'} memory, the responses{'} memory, and their unified memory, following the time sequence of the conversation session. Experiments on Japanese 10-sentence (5-round) conversation modeling show impressive results on how multi-turn reasoning can produce more diverse and acceptable responses than state-of-the-art single-turn and non-reasoning baselines.","pages":"2049--2059","doi":"10.18653\/v1\/N18-1186","url":"https:\/\/www.aclweb.org\/anthology\/N18-1186","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1187","title":"Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems","authors":["Liu, Bing","T{\\\"u}r, Gokhan","Hakkani-T{\\\"u}r, Dilek","Shah, Pararth","Heck, Larry"],"emails":["liubing@cmu.edu","gokhan.tur@ieee.org","dilekh@google.com","pararth@google.com","larry.heck@ieee.org"],"author_id":["bing-liu","gokhan-tur","dilek-hakkani-tur","pararth-shah","larry-heck"],"abstract":"In this work, we present a hybrid learning method for training task-oriented dialogue systems through online user interactions. Popular methods for learning task-oriented dialogues include applying reinforcement learning with user feedback on supervised pre-training models. Efficiency of such learning method may suffer from the mismatch of dialogue state distribution between offline training and online interactive learning stages. To address this challenge, we propose a hybrid imitation and reinforcement learning method, with which a dialogue agent can effectively learn from its interaction with users by learning from human teaching and feedback. We design a neural network based task-oriented dialogue agent that can be optimized end-to-end with the proposed learning method. Experimental results show that our end-to-end dialogue agent can learn effectively from the mistake it makes via imitation learning from user teaching. Applying reinforcement learning with user feedback after the imitation learning stage further improves the agent{'}s capability in successfully completing a task.","pages":"2060--2069","doi":"10.18653\/v1\/N18-1187","url":"https:\/\/www.aclweb.org\/anthology\/N18-1187","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1188","title":"{LSDSCC}: a Large Scale Domain-Specific Conversational Corpus for Response Generation with Diversity Oriented Evaluation Metrics","authors":["Xu, Zhen","Jiang, Nan","Liu, Bingquan","Rong, Wenge","Wu, Bowen","Wang, Baoxun","Wang, Zhuoran","Wang, Xiaolong"],"emails":["zxu@insun.hit.edu.cn","nanjiang@buaa.edu.cn","liubq@insun.hit.edu.cn","w.rong@buaa.edu.cn","wubowen@trio.ai","wangxl@insun.hit.edu.cn","wangbaoxun@trio.ai","wangzhuoran@trio.ai"],"author_id":["zhen-xu","nan-jiang","bingquan-liu","wenge-rong","bowen-wu","baoxun-wang","zhuoran-wang","xiaolong-wang"],"abstract":"It has been proven that automatic conversational agents can be built up using the Endto-End Neural Response Generation (NRG) framework, and such a data-driven methodology requires a large number of dialog pairs for model training and reasonable evaluation metrics for testing. This paper proposes a Large Scale Domain-Specific Conversational Corpus (LSDSCC) composed of high-quality queryresponse pairs extracted from the domainspecific online forum, with thorough preprocessing and cleansing procedures. Also, a testing set, including multiple diverse responses annotated for each query, is constructed, and on this basis, the metrics for measuring the diversity of generated results are further presented. We evaluate the performances of neural dialog models with the widely applied diversity boosting strategies on the proposed dataset. The experimental results have shown that our proposed corpus can be taken as a new benchmark dataset for the NRG task, and the presented metrics are promising to guide the optimization of NRG models by quantifying the diversity of the generated responses reasonably.","pages":"2070--2080","doi":"10.18653\/v1\/N18-1188","url":"https:\/\/www.aclweb.org\/anthology\/N18-1188","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1189","title":"{EMR} Coding with Semi-Parametric Multi-Head Matching Networks","authors":["Rios, Anthony","Kavuluru, Ramakanth"],"emails":["anthony.rios1@uky.edu","ramakanth.kavuluru@uky.edu"],"author_id":["anthony-rios","ramakanth-kavuluru"],"abstract":"Coding EMRs with diagnosis and procedure codes is an indispensable task for billing, secondary data analyses, and monitoring health trends. Both speed and accuracy of coding are critical. While coding errors could lead to more patient-side financial burden and misinterpretation of a patient{'}s well-being, timely coding is also needed to avoid backlogs and additional costs for the healthcare facility. In this paper, we present a new neural network architecture that combines ideas from few-shot learning matching networks, multi-label loss functions, and convolutional neural networks for text classification to significantly outperform other state-of-the-art models. Our evaluations are conducted using a well known de-identified EMR dataset (MIMIC) with a variety of multi-label performance measures.","pages":"2081--2091","doi":"10.18653\/v1\/N18-1189","url":"https:\/\/www.aclweb.org\/anthology\/N18-1189","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1190","title":"Factors Influencing the Surprising Instability of Word Embeddings","authors":["Wendlandt, Laura","Kummerfeld, Jonathan K.","Mihalcea, Rada"],"emails":["wenlaura@umich.edu","jkummerf@umich.edu","mihalcea@umich.edu"],"author_id":["laura-burdick","jonathan-k-kummerfeld","rada-mihalcea"],"abstract":"Despite the recent popularity of word embedding methods, there is only a small body of work exploring the limitations of these representations. In this paper, we consider one aspect of embedding spaces, namely their stability. We show that even relatively high frequency words (100-200 occurrences) are often unstable. We provide empirical evidence for how various factors contribute to the stability of word embeddings, and we analyze the effects of stability on downstream tasks.","pages":"2092--2102","doi":"10.18653\/v1\/N18-1190","url":"https:\/\/www.aclweb.org\/anthology\/N18-1190","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1191","title":"Mining Evidences for Concept Stock Recommendation","authors":["Liu, Qi","Zhang, Yue"],"emails":["","zhang@sutd.edu.sg"],"author_id":["qi-liu","yue-zhang"],"abstract":"We investigate the task of mining relevant stocks given a topic of concern on emerging capital markets, for which there is lack of structural understanding. Deep learning is leveraged to mine evidences from large scale textual data, which contain valuable market information. In particular, distributed word similarities trained over large scale raw texts are taken as a basis of relevance measuring, and deep reinforcement learning is leveraged to learn a strategy of topic expansion, given a small amount of manually labeled data from financial analysts. Results on two Chinese stock market datasets show that our method outperforms a strong baseline using information retrieval techniques.","pages":"2103--2112","doi":"10.18653\/v1\/N18-1191","url":"https:\/\/www.aclweb.org\/anthology\/N18-1191","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1192","title":"Binarized {LSTM} Language Model","authors":["Liu, Xuan","Cao, Di","Yu, Kai"],"emails":["liuxuan0526@gmail.com","caodi0207@gmail.com","ky219.cam@gmail.com"],"author_id":["xuan-liu","di-cao","kai-yu"],"abstract":"Long short-term memory (LSTM) language model (LM) has been widely investigated for automatic speech recognition (ASR) and natural language processing (NLP). Although excellent performance is obtained for large vocabulary tasks, tremendous memory consumption prohibits the use of LSTM LM in low-resource devices. The memory consumption mainly comes from the word embedding layer. In this paper, a novel binarized LSTM LM is proposed to address the problem. Words are encoded into binary vectors and other LSTM parameters are further binarized to achieve high memory compression. This is the first effort to investigate binary LSTM for large vocabulary LM. Experiments on both English and Chinese LM and ASR tasks showed that can achieve a compression ratio of 11.3 without any loss of LM and ASR performances and a compression ratio of 31.6 with acceptable minor performance degradation.","pages":"2113--2121","doi":"10.18653\/v1\/N18-1192","url":"https:\/\/www.aclweb.org\/anthology\/N18-1192","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1193","title":"Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos","authors":["Hazarika, Devamanyu","Poria, Soujanya","Zadeh, Amir","Cambria, Erik","Morency, Louis-Philippe","Zimmermann, Roger"],"emails":["hazarika@comp.nus.edu.sg","sporia@ihpc.a-star.edu.sg","rogerz@comp.nus.edu.sg","cambria@ntu.edu.sg","morency@cs.cmu.edu","abagherz@cs.cmu.edu"],"author_id":["devamanyu-hazarika","soujanya-poria","amir-zadeh","erik-cambria","louis-philippe-morency","roger-zimmermann"],"abstract":"Emotion recognition in conversations is crucial for the development of empathetic machines. Present methods mostly ignore the role of inter-speaker dependency relations while classifying emotions in conversations. In this paper, we address recognizing utterance-level emotions in dyadic conversational videos. We propose a deep neural framework, termed Conversational Memory Network (CMN), which leverages contextual information from the conversation history. In particular, CMN uses multimodal approach comprising audio, visual and textual features with gated recurrent units to model past utterances of each speaker into memories. These memories are then merged using attention-based hops to capture inter-speaker dependencies. Experiments show a significant improvement of 3 \u2212 4{\\%} in accuracy over the state of the art.","pages":"2122--2132","doi":"10.18653\/v1\/N18-1193","url":"https:\/\/www.aclweb.org\/anthology\/N18-1193","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1194","title":"How Time Matters: Learning Time-Decay Attention for Contextual Spoken Language Understanding in Dialogues","authors":["Su, Shang-Yu","Yuan, Pei-Chieh","Chen, Yun-Nung"],"emails":["r05921117@ntu.edu.tw","b03901134@ntu.edu.tw","y.v.chen@ieee.org"],"author_id":["shang-yu-su","pei-chieh-yuan","yun-nung-chen"],"abstract":"Spoken language understanding (SLU) is an essential component in conversational systems. Most SLU components treats each utterance independently, and then the following components aggregate the multi-turn information in the separate phases. In order to avoid error propagation and effectively utilize contexts, prior work leveraged history for contextual SLU. However, most previous models only paid attention to the related content in history utterances, ignoring their temporal information. In the dialogues, it is intuitive that the most recent utterances are more important than the least recent ones, in other words, time-aware attention should be in a decaying manner. Therefore, this paper designs and investigates various types of time-decay attention on the sentence-level and speaker-level, and further proposes a flexible universal time-decay attention mechanism. The experiments on the benchmark Dialogue State Tracking Challenge (DSTC4) dataset show that the proposed time-decay attention mechanisms significantly improve the state-of-the-art model for contextual understanding performance.","pages":"2133--2142","doi":"10.18653\/v1\/N18-1194","url":"https:\/\/www.aclweb.org\/anthology\/N18-1194","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1195","title":"Towards Understanding Text Factors in Oral Reading","authors":["Loukina, Anastassia","Liceralde, Van Rynald T.","Beigman Klebanov, Beata"],"emails":["aloukina@ets.org","vliceralde@ets.org","bbeigmanklebanov@ets.org"],"author_id":["anastassia-loukina","van-rynald-t-liceralde","beata-beigman-klebanov"],"abstract":"Using a case study, we show that variation in oral reading rate across passages for professional narrators is consistent across readers and much of it can be explained using features of the texts being read. While text complexity is a poor predictor of the reading rate, a substantial share of variability can be explained by timing and story-based factors with performance reaching r=0.75 for unseen passages and narrator.","pages":"2143--2154","doi":"10.18653\/v1\/N18-1195","url":"https:\/\/www.aclweb.org\/anthology\/N18-1195","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1196","title":"Generating Bilingual Pragmatic Color References","authors":["Monroe, Will","Hu, Jennifer","Jong, Andrew","Potts, Christopher"],"emails":["wmonroe4@cs.stanford.edu","jenniferhu@college.harvard.edu","andrewjong.cs@gmail.com","cgpotts@stanford.edu"],"author_id":["will-monroe","jennifer-hu","andrew-jong","christopher-potts"],"abstract":"Contextual influences on language often exhibit substantial cross-lingual regularities; for example, we are more verbose in situations that require finer distinctions. However, these regularities are sometimes obscured by semantic and syntactic differences. Using a newly-collected dataset of color reference games in Mandarin Chinese (which we release to the public), we confirm that a variety of constructions display the same sensitivity to contextual difficulty in Chinese and English. We then show that a neural speaker agent trained on bilingual data with a simple multitask learning approach displays more human-like patterns of context dependence and is more pragmatically informative than its monolingual Chinese counterpart. Moreover, this is not at the expense of language-specific semantic understanding: the resulting speaker model learns the different basic color term systems of English and Chinese (with noteworthy cross-lingual influences), and it can identify synonyms between the two languages using vector analogy operations on its output layer, despite having no exposure to parallel data.","pages":"2155--2165","doi":"10.18653\/v1\/N18-1196","url":"https:\/\/www.aclweb.org\/anthology\/N18-1196","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1197","title":"Learning with Latent Language","authors":["Andreas, Jacob","Klein, Dan","Levine, Sergey"],"emails":["jda@eecs.berkeley.edu","klein@eecs.berkeley.edu","svlevine@eecs.berkeley.edu"],"author_id":["jacob-andreas","dan-klein","sergey-levine"],"abstract":"The named concepts and compositional operators present in natural language provide a rich source of information about the abstractions humans use to navigate the world. Can this linguistic background knowledge improve the generality and efficiency of learned classifiers and control policies? This paper aims to show that using the space of natural language strings as a parameter space is an effective way to capture natural task structure. In a pretraining phase, we learn a language interpretation model that transforms inputs (e.g. images) into outputs (e.g. labels) given natural language descriptions. To learn a new concept (e.g. a classifier), we search directly in the space of descriptions to minimize the interpreter{'}s loss on training examples. Crucially, our models do not require language data to learn these concepts: language is used only in pretraining to impose structure on subsequent learning. Results on image classification, text editing, and reinforcement learning show that, in all settings, models with a linguistic parameterization outperform those without.","pages":"2166--2179","doi":"10.18653\/v1\/N18-1197","url":"https:\/\/www.aclweb.org\/anthology\/N18-1197","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1198","title":"Object Counts! Bringing Explicit Detections Back into Image Captioning","authors":["Wang, Josiah","Madhyastha, Pranava Swaroop","Specia, Lucia"],"emails":["j.k.wang@sheffield.ac.uk","p.madhyastha@sheffield.ac.uk","l.specia@sheffield.ac.uk"],"author_id":["josiah-wang","pranava-swaroop-madhyastha","lucia-specia"],"abstract":"The use of explicit object detectors as an intermediate step to image captioning {--} which used to constitute an essential stage in early work {--} is often bypassed in the currently dominant end-to-end approaches, where the language model is conditioned directly on a mid-level image embedding. We argue that explicit detections provide rich semantic information, and can thus be used as an interpretable representation to better understand why end-to-end image captioning systems work well. We provide an in-depth analysis of end-to-end image captioning by exploring a variety of cues that can be derived from such object detections. Our study reveals that end-to-end image captioning systems rely on matching image representations to generate captions, and that encoding the frequency, size and position of objects are complementary and all play a role in forming a good image representation. It also reveals that different object categories contribute in different ways towards image captioning.","pages":"2180--2193","doi":"10.18653\/v1\/N18-1198","url":"https:\/\/www.aclweb.org\/anthology\/N18-1198","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1199","title":"Quantifying the Visual Concreteness of Words and Topics in Multimodal Datasets","authors":["Hessel, Jack","Mimno, David","Lee, Lillian"],"emails":["jhessel@cs.cornell.edu","mimno@cornell.edu","llee@cs.cornell.edu"],"author_id":["jack-hessel","david-mimno","lillian-lee"],"abstract":"Multimodal machine learning algorithms aim to learn visual-textual correspondences. Previous work suggests that concepts with concrete visual manifestations may be easier to learn than concepts with abstract ones. We give an algorithm for automatically computing the visual concreteness of words and topics within multimodal datasets. We apply the approach in four settings, ranging from image captions to images\/text scraped from historical books. In addition to enabling explorations of concepts in multimodal datasets, our concreteness scores predict the capacity of machine learning algorithms to learn textual\/visual relationships. We find that 1) concrete concepts are indeed easier to learn; 2) the large number of algorithms we consider have similar failure cases; 3) the precise positive relationship between concreteness and performance varies between datasets. We conclude with recommendations for using concreteness scores to facilitate future multimodal research.","pages":"2194--2205","doi":"10.18653\/v1\/N18-1199","url":"https:\/\/www.aclweb.org\/anthology\/N18-1199","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1200","title":"Speaker Naming in Movies","authors":["Azab, Mahmoud","Wang, Mingzhe","Smith, Max","Kojima, Noriyuki","Deng, Jia","Mihalcea, Rada"],"emails":["mazab@umich.edu","mzwang@umich.edu","mxsmith@umich.edu","kojimano@umich.edu","jiadeng@umich.edu","mihalcea@umich.edu"],"author_id":["mahmoud-azab","mingzhe-wang","max-smith","noriyuki-kojima","jia-deng","rada-mihalcea"],"abstract":"We propose a new model for speaker naming in movies that leverages visual, textual, and acoustic modalities in an unified optimization framework. To evaluate the performance of our model, we introduce a new dataset consisting of six episodes of the Big Bang Theory TV show and eighteen full movies covering different genres. Our experiments show that our multimodal model significantly outperforms several competitive baselines on the average weighted F-score metric. To demonstrate the effectiveness of our framework, we design an end-to-end memory network model that leverages our speaker naming model and achieves state-of-the-art results on the subtitles task of the MovieQA 2017 Challenge.","pages":"2206--2216","doi":"10.18653\/v1\/N18-1200","url":"https:\/\/www.aclweb.org\/anthology\/N18-1200","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1201","title":"Stacking with Auxiliary Features for Visual Question Answering","authors":["Rajani, Nazneen Fatema","Mooney, Raymond"],"emails":["nrajani@cs.utexas.edu","mooney@cs.utexas.edu"],"author_id":["nazneen-fatema-rajani","raymond-mooney"],"abstract":"Visual Question Answering (VQA) is a well-known and challenging task that requires systems to jointly reason about natural language and vision. Deep learning models in various forms have been the standard for solving VQA. However, some of these VQA models are better at certain types of image-question pairs than other models. Ensembling VQA models intelligently to leverage their diverse expertise is, therefore, advantageous. Stacking With Auxiliary Features (SWAF) is an intelligent ensembling technique which learns to combine the results of multiple models using features of the current problem as context. We propose four categories of auxiliary features for ensembling for VQA. Three out of the four categories of features can be inferred from an image-question pair and do not require querying the component models. The fourth category of auxiliary features uses model-specific explanations. In this paper, we describe how we use these various categories of auxiliary features to improve performance for VQA. Using SWAF to effectively ensemble three recent systems, we obtain a new state-of-the-art. Our work also highlights the advantages of explainable AI models.","pages":"2217--2226","doi":"10.18653\/v1\/N18-1201","url":"https:\/\/www.aclweb.org\/anthology\/N18-1201","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1202","title":"Deep Contextualized Word Representations","authors":["Peters, Matthew","Neumann, Mark","Iyyer, Mohit","Gardner, Matt","Clark, Christopher","Lee, Kenton","Zettlemoyer, Luke"],"emails":["matthewp@allenai.org","markn@allenai.org","mohiti@allenai.org","mattg@allenai.org","csquared@cs.washington.edu","kentonl@cs.washington.edu","lsz@cs.washington.edu"],"author_id":["matthew-peters","mark-neumann","mohit-iyyer","matt-gardner","christopher-clark","kenton-lee","luke-zettlemoyer"],"abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.","pages":"2227--2237","doi":"10.18653\/v1\/N18-1202","url":"https:\/\/www.aclweb.org\/anthology\/N18-1202","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1203","title":"Learning to Map Context-Dependent Sentences to Executable Formal Queries","authors":["Suhr, Alane","Iyer, Srinivasan","Artzi, Yoav"],"emails":["suhr@cs.cornell.edu","sviyer@cs.washington.edu","yoav@cs.cornell.edu"],"author_id":["alane-suhr","srinivasan-iyer","yoav-artzi"],"abstract":"We propose a context-dependent model to map utterances within an interaction to executable formal queries. To incorporate interaction history, the model maintains an interaction-level encoder that updates after each turn, and can copy sub-sequences of previously predicted queries during generation. Our approach combines implicit and explicit modeling of references between utterances. We evaluate our model on the ATIS flight planning interactions, and demonstrate the benefits of modeling context and explicit references.","pages":"2238--2249","doi":"10.18653\/v1\/N18-1203","url":"https:\/\/www.aclweb.org\/anthology\/N18-1203","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1204","title":"Neural Text Generation in Stories Using Entity Representations as Context","authors":["Clark, Elizabeth","Ji, Yangfeng","Smith, Noah A."],"emails":["eaclark7@cs.washington.edu","yangfeng@cs.washington.edu","nasmith@cs.washington.edu"],"author_id":["elizabeth-clark","yangfeng-ji","noah-a-smith"],"abstract":"We introduce an approach to neural text generation that explicitly represents entities mentioned in the text. Entity representations are vectors that are updated as the text proceeds; they are designed specifically for narrative text like fiction or news stories. Our experiments demonstrate that modeling entities offers a benefit in two automatic evaluations: mention generation (in which a model chooses which entity to mention next and which words to use in the mention) and selection between a correct next sentence and a distractor from later in the same story. We also conduct a human evaluation on automatically generated text in story contexts; this study supports our emphasis on entities and suggests directions for further research.","pages":"2250--2260","doi":"10.18653\/v1\/N18-1204","url":"https:\/\/www.aclweb.org\/anthology\/N18-1204","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"},{"id":"N18-1205","title":"Recurrent Neural Networks as Weighted Language Recognizers","authors":["Chen, Yining","Gilroy, Sorcha","Maletti, Andreas","May, Jonathan","Knight, Kevin"],"emails":["yining.chen.18@dartmouth.edu","s.gilroy@sms.ed.ac.uk","andreas.maletti@uni-leipzig.de","jonmay@isi.edu","knight@isi.edu"],"author_id":["yining-chen","sorcha-gilroy","andreas-maletti","jonathan-may","kevin-knight"],"abstract":"We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.","pages":"2261--2271","doi":"10.18653\/v1\/N18-1205","url":"https:\/\/www.aclweb.org\/anthology\/N18-1205","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"}]