[{"id":"W19-4801","title":"Transcoding Compositionally: Using Attention to Find More Generalizable Solutions","authors":["Korrel, Kris","Hupkes, Dieuwke","Dankers, Verna","Bruni, Elia"],"emails":["kris.korrel@gmail.com","d.hupkes@uva.nl","verna.dankers@gmail.com","elia.bruni@gmail.com"],"author_id":["kris-korrel","dieuwke-hupkes","verna-dankers","elia-bruni"],"abstract":"While sequence-to-sequence models have shown remarkable generalization power across several natural language tasks, their construct of solutions are argued to be less compositional than human-like generalization. In this paper, we present seq2attn, a new architecture that is specifically designed to exploit attention to find compositional patterns in the input. In seq2attn, the two standard components of an encoder-decoder model are connected via a transcoder, that modulates the information flow between them. We show that seq2attn can successfully generalize, without requiring any additional supervision, on two tasks which are specifically constructed to challenge the compositional skills of neural networks. The solutions found by the model are highly interpretable, allowing easy analysis of both the types of solutions that are found and potential causes for mistakes. We exploit this opportunity to introduce a new paradigm to test compositionality that studies the extent to which a model overgeneralizes when confronted with exceptions. We show that seq2attn exhibits such overgeneralization to a larger degree than a standard sequence-to-sequence model.","pages":"1--11","doi":"10.18653\/v1\/W19-4801","url":"https:\/\/www.aclweb.org\/anthology\/W19-4801","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4802","title":"Sentiment Analysis Is Not Solved! Assessing and Probing Sentiment Classification","authors":["Barnes, Jeremy","{\\O}vrelid, Lilja","Velldal, Erik"],"emails":["jeremycb@ifi.uio.no","liljao@ifi.uio.no","erikve@ifi.uio.no"],"author_id":["jeremy-barnes","lilja-ovrelid","erik-velldal"],"abstract":"Neural methods for sentiment analysis have led to quantitative improvements over previous approaches, but these advances are not always accompanied with a thorough analysis of the qualitative differences. Therefore, it is not clear what outstanding conceptual challenges for sentiment analysis remain. In this work, we attempt to discover what challenges still prove a problem for sentiment classifiers for English and to provide a challenging dataset. We collect the subset of sentences that an (oracle) ensemble of state-of-the-art sentiment classifiers misclassify and then annotate them for 18 linguistic and paralinguistic phenomena, such as negation, sarcasm, modality, etc. Finally, we provide a case study that demonstrates the usefulness of the dataset to probe the performance of a given sentiment classifier with respect to linguistic phenomena.","pages":"12--23","doi":"10.18653\/v1\/W19-4802","url":"https:\/\/www.aclweb.org\/anthology\/W19-4802","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4803","title":"Second-order Co-occurrence Sensitivity of Skip-Gram with Negative Sampling","authors":["Schlechtweg, Dominik","Oguz, Cennet","Schulte im Walde, Sabine"],"emails":["schlecdk@ims.uni-stuttgart.de","oguzct@ims.uni-stuttgart.de","schulte@ims.uni-stuttgart.de"],"author_id":["dominik-schlechtweg","cennet-oguz","sabine-schulte-im-walde"],"abstract":"We simulate first- and second-order context overlap and show that Skip-Gram with Negative Sampling is similar to Singular Value Decomposition in capturing second-order co-occurrence information, while Pointwise Mutual Information is agnostic to it. We support the results with an empirical study finding that the models react differently when provided with additional second-order information. Our findings reveal a basic property of Skip-Gram with Negative Sampling and point towards an explanation of its success on a variety of tasks.","pages":"24--30","doi":"10.18653\/v1\/W19-4803","url":"https:\/\/www.aclweb.org\/anthology\/W19-4803","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4804","title":"Can Neural Networks Understand Monotonicity Reasoning?","authors":["Yanaka, Hitomi","Mineshima, Koji","Bekki, Daisuke","Inui, Kentaro","Sekine, Satoshi","Abzianidze, Lasha","Bos, Johan"],"emails":["hitomi.yanaka@riken.jp","mineshima.koji@ocha.ac.jp","bekki@is.ocha.ac.jp","inui@ecei.tohoku.ac.jp","satoshi.sekine@riken.jp","l.abzianidze@rug.nl","johan.bos@rug.nl"],"author_id":["hitomi-yanaka","koji-mineshima","daisuke-bekki","kentaro-inui","satoshi-sekine","lasha-abzianidze","johan-bos"],"abstract":"Monotonicity reasoning is one of the important reasoning skills for any intelligent natural language inference (NLI) model in that it requires the ability to capture the interaction between lexical and syntactic structures. Since no test set has been developed for monotonicity reasoning with wide coverage, it is still unclear whether neural models can perform monotonicity reasoning in a proper way. To investigate this issue, we introduce the Monotonicity Entailment Dataset (MED). Performance by state-of-the-art NLI models on the new test set is substantially worse, under 55{\\%}, especially on downward reasoning. In addition, analysis using a monotonicity-driven data augmentation method showed that these models might be limited in their generalization ability in upward and downward reasoning.","pages":"31--40","doi":"10.18653\/v1\/W19-4804","url":"https:\/\/www.aclweb.org\/anthology\/W19-4804","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4805","title":"Multi-Granular Text Encoding for Self-Explaining Categorization","authors":["Wang, Zhiguo","Zhang, Yue","Yu, Mo","Zhang, Wei","Pan, Lin","Song, Linfeng","Xu, Kun","El-Kurdi, Yousef"],"emails":["","","zgw.tomorrow@gmail.com","","","","",""],"author_id":["zhiguo-wang","yue-zhang","mo-yu","wei-zhang","lin-pan","linfeng-song","kun-xu","yousef-el-kurdi"],"abstract":"Self-explaining text categorization requires a classifier to make a prediction along with supporting evidence. A popular type of evidence is sub-sequences extracted from the input text which are sufficient for the classifier to make the prediction. In this work, we define multi-granular ngrams as basic units for explanation, and organize all ngrams into a hierarchical structure, so that shorter ngrams can be reused while computing longer ngrams. We leverage the tree-structured LSTM to learn a context-independent representation for each unit via parameter sharing. Experiments on medical disease classification show that our model is more accurate, efficient and compact than the BiLSTM and CNN baselines. More importantly, our model can extract intuitive multi-granular evidence to support its predictions.","pages":"41--45","doi":"10.18653\/v1\/W19-4805","url":"https:\/\/www.aclweb.org\/anthology\/W19-4805","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4806","title":"The Meaning of {``}Most{''} for Visual Question Answering Models","authors":["Kuhnle, Alexander","Copestake, Ann"],"emails":["aok25@cam.ac.uk","aac10@cam.ac.uk"],"author_id":["alexander-kuhnle","ann-copestake"],"abstract":"The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of {``}most{''}, we discuss two strategies which rely on fundamentally different cognitive concepts. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from psycholinguistics where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber{'}s law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system.","pages":"46--55","doi":"10.18653\/v1\/W19-4806","url":"https:\/\/www.aclweb.org\/anthology\/W19-4806","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4807","title":"Do Human Rationales Improve Machine Explanations?","authors":["Strout, Julia","Zhang, Ye","Mooney, Raymond"],"emails":["jstrout@cs.utexas.edu","yezhang@cs.utexas.edu","mooney@cs.utexas.edu"],"author_id":["julia-strout","ye-zhang","raymond-mooney"],"abstract":"Work on {``}learning with rationales{''} shows that humans providing explanations to a machine learning system can improve the system{'}s predictive accuracy. However, this work has not been connected to work in {``}explainable AI{''} which concerns machines explaining their reasoning to humans. In this work, we show that learning with rationales can also improve the quality of the machine{'}s explanations as evaluated by human judges. Specifically, we present experiments showing that, for CNN-based text classification, explanations generated using {``}supervised attention{''} are judged superior to explanations generated using normal unsupervised attention.","pages":"56--62","doi":"10.18653\/v1\/W19-4807","url":"https:\/\/www.aclweb.org\/anthology\/W19-4807","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4808","title":"Analyzing the Structure of Attention in a Transformer Language Model","authors":["Vig, Jesse","Belinkov, Yonatan"],"emails":["jesse.vig@parc.com","belinkov@seas.harvard.edu"],"author_id":["jesse-vig","yonatan-belinkov"],"abstract":"The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.","pages":"63--76","doi":"10.18653\/v1\/W19-4808","url":"https:\/\/www.aclweb.org\/anthology\/W19-4808","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4809","title":"Detecting Political Bias in News Articles Using Headline Attention","authors":["Gangula, Rama Rohit Reddy","Duggenpudi, Suma Reddy","Mamidi, Radhika"],"emails":["ramarohitreddy.g@research.iiit.ac.in","sumareddy.duggenpudi@research.iiit.ac.in","radhika.mamidi@iiit.ac.in"],"author_id":["rama-rohit-reddy-gangula","suma-reddy-duggenpudi","radhika-mamidi"],"abstract":"Language is a powerful tool which can be used to state the facts as well as express our views and perceptions. Most of the times, we find a subtle bias towards or against someone or something. When it comes to politics, media houses and journalists are known to create bias by shrewd means such as misinterpreting reality and distorting viewpoints towards some parties. This misinterpretation on a large scale can lead to the production of biased news and conspiracy theories. Automating bias detection in newspaper articles could be a good challenge for research in NLP. We proposed a headline attention network for this bias detection. Our model has two distinctive characteristics: (i) it has a structure that mirrors a person{'}s way of reading a news article (ii) it has attention mechanism applied on the article based on its headline, enabling it to attend to more critical content to predict bias. As the required datasets were not available, we created a dataset comprising of 1329 news articles collected from various Telugu newspapers and marked them for bias towards a particular political party. The experiments conducted on it demonstrated that our model outperforms various baseline methods by a substantial margin.","pages":"77--84","doi":"10.18653\/v1\/W19-4809","url":"https:\/\/www.aclweb.org\/anthology\/W19-4809","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4810","title":"Testing the Generalization Power of Neural Network Models across {NLI} Benchmarks","authors":["Talman, Aarne","Chatzikyriakidis, Stergios"],"emails":["aarne.talman@helsinki.fi","stergios.chatzikyriakidis@gu.se"],"author_id":["aarne-talman","stergios-chatzikyriakidis"],"abstract":"Neural network models have been very successful in natural language inference, with the best models reaching 90{\\%} accuracy in some benchmarks. However, the success of these models turns out to be largely benchmark specific. We show that models trained on a natural language inference dataset drawn from one benchmark fail to perform well in others, even if the notion of inference assumed in these benchmarks is the same or similar. We train six high performing neural network models on different datasets and show that each one of these has problems of generalizing when we replace the original test set with a test set taken from another corpus designed for the same task. In light of these results, we argue that most of the current neural network models are not able to generalize well in the task of natural language inference. We find that using large pre-trained language models helps with transfer learning when the datasets are similar enough. Our results also highlight that the current NLI datasets do not cover the different nuances of inference extensively enough.","pages":"85--94","doi":"10.18653\/v1\/W19-4810","url":"https:\/\/www.aclweb.org\/anthology\/W19-4810","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4811","title":"Character Eyes: Seeing Language through Character-Level Taggers","authors":["Pinter, Yuval","Marone, Marc","Eisenstein, Jacob"],"emails":["uvp@gatech.edu","mmarone6@gatech.edu","jacobeisenstein@fb.com"],"author_id":["yuval-pinter","marc-marone","jacob-eisenstein"],"abstract":"Character-level models have been used extensively in recent years in NLP tasks as both supplements and replacements for closed-vocabulary token-level word representations. In one popular architecture, character-level LSTMs are used to feed token representations into a sequence tagger predicting token-level annotations such as part-of-speech (POS) tags. In this work, we examine the behavior of POS taggers across languages from the perspective of individual hidden units within the character LSTM. We aggregate the behavior of these units into language-level metrics which quantify the challenges that taggers face on languages with different morphological properties, and identify links between synthesis and affixation preference and emergent behavior of the hidden tagger layer. In a comparative experiment, we show how modifying the balance between forward and backward hidden units affects model arrangement and performance in these types of languages.","pages":"95--102","doi":"10.18653\/v1\/W19-4811","url":"https:\/\/www.aclweb.org\/anthology\/W19-4811","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4812","title":"Faithful Multimodal Explanation for Visual Question Answering","authors":["Wu, Jialin","Mooney, Raymond"],"emails":["jialinwu@cs.utexas.edu","mooney@cs.utexas.edu"],"author_id":["jialin-wu","raymond-mooney"],"abstract":"AI systems{'} ability to explain their reasoning is critical to their utility and trustworthiness. Deep neural networks have enabled significant progress on many challenging problems such as visual question answering (VQA). However, most of them are opaque black boxes with limited explanatory capability. This paper presents a novel approach to developing a high-performing VQA system that can elucidate its answers with integrated textual and visual explanations that faithfully reflect important aspects of its underlying reasoning while capturing the style of comprehensible human explanations. Extensive experimental evaluation demonstrates the advantages of this approach compared to competing methods using both automated metrics and human evaluation.","pages":"103--112","doi":"10.18653\/v1\/W19-4812","url":"https:\/\/www.aclweb.org\/anthology\/W19-4812","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4813","title":"Evaluating Recurrent Neural Network Explanations","authors":["Arras, Leila","Osman, Ahmed","M{\\\"u}ller, Klaus-Robert","Samek, Wojciech"],"emails":["leila.arras@hhi.fraunhofer.de","","","wojciech.samek@hhi.fraunhofer.de"],"author_id":["leila-arras","ahmed-osman","klaus-robert-muller","wojciech-samek"],"abstract":"Recently, several methods have been proposed to explain the predictions of recurrent neural networks (RNNs), in particular of LSTMs. The goal of these methods is to understand the network{'}s decisions by assigning to each input variable, e.g., a word, a relevance indicating to which extent it contributed to a particular prediction. In previous works, some of these methods were not yet compared to one another, or were evaluated only qualitatively. We close this gap by systematically and quantitatively comparing these methods in different settings, namely (1) a toy arithmetic task which we use as a sanity check, (2) a five-class sentiment prediction of movie reviews, and besides (3) we explore the usefulness of word relevances to build sentence-level representations. Lastly, using the method that performed best in our experiments, we show how specific linguistic phenomena such as the negation in sentiment analysis reflect in terms of relevance patterns, and how the relevance visualization can help to understand the misclassification of individual samples.","pages":"113--126","doi":"10.18653\/v1\/W19-4813","url":"https:\/\/www.aclweb.org\/anthology\/W19-4813","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4814","title":"On the Realization of Compositionality in Neural Networks","authors":["Baan, Joris","Leible, Jana","Nikolaus, Mitja","Rau, David","Ulmer, Dennis","Baumg{\\\"a}rtner, Tim","Hupkes, Dieuwke","Bruni, Elia"],"emails":["joris.baan@student.uva.nl","jana.leible@student.uva.nl","mitja.nikolaus@posteo.de","david.rau@student.uva.nl","dennis.ulmer@student.uva.nl","baumgaertner.t@gmail.com","d.hupkes@uva.nl","elia.bruni@gmail.com"],"author_id":["joris-baan","jana-leible","mitja-nikolaus","david-rau","dennis-ulmer","tim-baumgartner","dieuwke-hupkes","elia-bruni"],"abstract":"We present a detailed comparison of two types of sequence to sequence models trained to conduct a compositional task. The models are architecturally identical at inference time, but differ in the way that they are trained: our baseline model is trained with a task-success signal only, while the other model receives additional supervision on its attention mechanism (Attentive Guidance), which has shown to be an effective method for encouraging more compositional solutions. We first confirm that the models with attentive guidance indeed infer more compositional solutions than the baseline, by training them on the lookup table task presented by Liska et al. (2019). We then do an in-depth analysis of the structural differences between the two model types, focusing in particular on the organisation of the parameter space and the hidden layer activations and find noticeable differences in both these aspects. Guided networks focus more on the components of the input rather than the sequence as a whole and develop small functional groups of neurons with specific purposes that use their gates more selectively. Results from parameter heat maps, component swapping and graph analysis also indicate that guided networks exhibit a more modular structure with a small number of specialized, strongly connected neurons.","pages":"127--137","doi":"10.18653\/v1\/W19-4814","url":"https:\/\/www.aclweb.org\/anthology\/W19-4814","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4815","title":"Learning the {D}yck Language with Attention-based {S}eq2{S}eq Models","authors":["Yu, Xiang","Vu, Ngoc Thang","Kuhn, Jonas"],"emails":["xiangyu@ims.uni-stuttgart.de","thangvu@ims.uni-stuttgart.de","jonas@ims.uni-stuttgart.de"],"author_id":["xiang-yu","ngoc-thang-vu","jonas-kuhn"],"abstract":"The generalized Dyck language has been used to analyze the ability of Recurrent Neural Networks (RNNs) to learn context-free grammars (CFGs). Recent studies draw conflicting conclusions on their performance, especially regarding the generalizability of the models with respect to the depth of recursion. In this paper, we revisit several common models and experimental settings, discuss the potential problems of the tasks and analyses. Furthermore, we explore the use of attention mechanisms within the seq2seq framework to learn the Dyck language, which could compensate for the limited encoding ability of RNNs. Our findings reveal that attention mechanisms still cannot truly generalize over the recursion depth, although they perform much better than other models on the closing bracket tagging task. Moreover, this also suggests that this commonly used task is not sufficient to test a model{'}s understanding of CFGs.","pages":"138--146","doi":"10.18653\/v1\/W19-4815","url":"https:\/\/www.aclweb.org\/anthology\/W19-4815","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4816","title":"Modeling Paths for Explainable Knowledge Base Completion","authors":["Stadelmaier, Josua","Pad{\\'o}, Sebastian"],"emails":["josua.stadelmaier@ims.uni-stuttgart.de","sebastian.pado@ims.uni-stuttgart.de"],"author_id":["josua-stadelmaier","sebastian-pado"],"abstract":"A common approach in knowledge base completion (KBC) is to learn representations for entities and relations in order to infer missing facts by generalizing existing ones. A shortcoming of standard models is that they do not explain their predictions to make them verifiable easily to human inspection. In this paper, we propose the Context Path Model (CPM) which generates explanations for new facts in KBC by providing sets of \\textit{context paths} as supporting evidence for these triples. For example, a new triple (Theresa May, nationality, Britain) may be explained by the path (Theresa May, born in, Eastbourne, contained in, Britain). The CPM is formulated as a wrapper that can be applied on top of various existing KBC models. We evaluate it for the well-established TransE model. We observe that its performance remains very close despite the added complexity, and that most of the paths proposed as explanations provide meaningful evidence to assess the correctness.","pages":"147--157","doi":"10.18653\/v1\/W19-4816","url":"https:\/\/www.aclweb.org\/anthology\/W19-4816","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4817","title":"Probing Word and Sentence Embeddings for Long-distance Dependencies Effects in {F}rench and {E}nglish","authors":["Merlo, Paola"],"emails":["erlo@unige.ch"],"author_id":["paola-merlo"],"abstract":"The recent wide-spread and strong interest in RNNs has spurred detailed investigations of the distributed representations they generate and specifically if they exhibit properties similar to those characterising human languages. Results are at present inconclusive. In this paper, we extend previous work on long-distance dependencies in three ways. We manipulate word embeddings to translate them in a space that is attuned to the linguistic properties under study. We extend the work to sentence embeddings and to new languages. We confirm previous negative results: word embeddings and sentence embeddings do not unequivocally encode fine-grained linguistic properties of long-distance dependencies.","pages":"158--172","doi":"10.18653\/v1\/W19-4817","url":"https:\/\/www.aclweb.org\/anthology\/W19-4817","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4818","title":"Derivational Morphological Relations in Word Embeddings","authors":["Musil, Tom{\\'a}{\\v{s}}","Vidra, Jon{\\'a}{\\v{s}}","Mare{\\v{c}}ek, David"],"emails":["musil@ufal.mff.cuni.cz","vidra@ufal.mff.cuni.cz","marecek@ufal.mff.cuni.cz"],"author_id":["tomas-musil","jonas-vidra","david-marecek"],"abstract":"Derivation is a type of a word-formation process which creates new words from existing ones by adding, changing or deleting affixes. In this paper, we explore the potential of word embeddings to identify properties of word derivations in the morphologically rich Czech language. We extract derivational relations between pairs of words from DeriNet, a Czech lexical network, which organizes almost one million Czech lemmas into derivational trees. For each such pair, we compute the difference of the embeddings of the two words, and perform unsupervised clustering of the resulting vectors. Our results show that these clusters largely match manually annotated semantic categories of the derivational relations (e.g. the relation {`}bake{--}baker{'} belongs to category {`}actor{'}, and a correct clustering puts it into the same cluster as {`}govern{--}governor{'}).","pages":"173--180","doi":"10.18653\/v1\/W19-4818","url":"https:\/\/www.aclweb.org\/anthology\/W19-4818","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4819","title":"Hierarchical Representation in Neural Language Models: Suppression and Recovery of Expectations","authors":["Wilcox, Ethan","Levy, Roger","Futrell, Richard"],"emails":["wilcoxeg@g.harvard.edu","rplevy@mit.edu","rfutrell@uci.edu"],"author_id":["ethan-wilcox","roger-levy","richard-futrell"],"abstract":"Work using artificial languages as training input has shown that LSTMs are capable of inducing the stack-like data structures required to represent context-free and certain mildly context-sensitive languages {---} formal language classes which correspond in theory to the hierarchical structures of natural language. Here we present a suite of experiments probing whether neural language models trained on linguistic data induce these stack-like data structures and deploy them while incrementally predicting words. We study two natural language phenomena: center embedding sentences and syntactic island constraints on the filler{--}gap dependency. In order to properly predict words in these structures, a model must be able to temporarily suppress certain expectations and then recover those expectations later, essentially pushing and popping these expectations on a stack. Our results provide evidence that models can successfully suppress and recover expectations in many cases, but do not fully recover their previous grammatical state.","pages":"181--190","doi":"10.18653\/v1\/W19-4819","url":"https:\/\/www.aclweb.org\/anthology\/W19-4819","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4820","title":"Blackbox Meets Blackbox: Representational Similarity {\\&} Stability Analysis of Neural Language Models and Brains","authors":["Abnar, Samira","Beinborn, Lisa","Choenni, Rochelle","Zuidema, Willem"],"emails":["abnar@uva.nl","l.beinborn@uva.nl","rochelle.choenni@student.uva.nl","zuidema@uva.nl"],"author_id":["samira-abnar","lisa-beinborn","rochelle-choenni","willem-zuidema"],"abstract":"In this paper, we define and apply representational stability analysis (ReStA), an intuitive way of analyzing neural language models. ReStA is a variant of the popular representational similarity analysis (RSA) in cognitive neuroscience. While RSA can be used to compare representations in models, model components, and human brains, ReStA compares instances of the same model, while systematically varying single model parameter. Using ReStA, we study four recent and successful neural language models, and evaluate how sensitive their internal representations are to the amount of prior context. Using RSA, we perform a systematic study of how similar the representational spaces in the first and second (or higher) layers of these models are to each other and to patterns of activation in the human brain. Our results reveal surprisingly strong differences between language models, and give insights into where the deep linguistic processing, that integrates information over multiple sentences, is happening in these models. The combination of ReStA and RSA on models and brains allows us to start addressing the important question of what kind of linguistic processes we can hope to observe in fMRI brain imaging data. In particular, our results suggest that the data on story reading from Wehbe et al.\/ (2014) contains a signal of shallow linguistic processing, but show no evidence on the more interesting deep linguistic processing.","pages":"191--203","doi":"10.18653\/v1\/W19-4820","url":"https:\/\/www.aclweb.org\/anthology\/W19-4820","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4821","title":"An {LSTM} Adaptation Study of (Un)grammaticality","authors":["Chowdhury, Shammur Absar","Zamparelli, Roberto"],"emails":["shammur.chowdhury@unitn.it","roberto.zamparelli@unitn.it"],"author_id":["shammur-absar-chowdhury","roberto-zamparelli"],"abstract":"We propose a novel approach to the study of how artificial neural network perceive the distinction between grammatical and ungrammatical sentences, a crucial task in the growing field of synthetic linguistics. The method is based on performance measures of language models trained on corpora and fine-tuned with either grammatical or ungrammatical sentences, then applied to (different types of) grammatical or ungrammatical sentences. The results show that both in the difficult and highly symmetrical task of detecting subject islands and in the more open CoLA dataset, grammatical sentences give rise to better scores than ungrammatical ones, possibly because they can be better integrated within the body of linguistic structural knowledge that the language model has accumulated.","pages":"204--212","doi":"10.18653\/v1\/W19-4821","url":"https:\/\/www.aclweb.org\/anthology\/W19-4821","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4822","title":"An Analysis of Source-Side Grammatical Errors in {NMT}","authors":["Anastasopoulos, Antonios"],"emails":["aanastas@cs.cmu.edu"],"author_id":["antonios-anastasopoulos"],"abstract":"The quality of Neural Machine Translation (NMT) has been shown to significantly degrade when confronted with source-side noise. We present the first large-scale study of state-of-the-art English-to-German NMT on real grammatical noise, by evaluating on several Grammar Correction corpora. We present methods for evaluating NMT robustness without true references, and we use them for extensive analysis of the effects that different grammatical errors have on the NMT output. We also introduce a technique for visualizing the divergence distribution caused by a source-side error, which allows for additional insights.","pages":"213--223","doi":"10.18653\/v1\/W19-4822","url":"https:\/\/www.aclweb.org\/anthology\/W19-4822","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4823","title":"Finding Hierarchical Structure in Neural Stacks Using Unsupervised Parsing","authors":["Merrill, William","Khazan, Lenny","Amsel, Noah","Hao, Yiding","Mendelsohn, Simon","Frank, Robert"],"emails":["william.merrill@yale.edu","lenny.khazan@yale.edu","noah.amsel@yale.edu","yiding.hao@yale.edu","simon.mendelsohn@yale.edu","robert.frank@yale.edu"],"author_id":["william-merrill","lenny-khazan","noah-amsel","yiding-hao","simon-mendelsohn","robert-frank"],"abstract":"Neural network architectures have been augmented with differentiable stacks in order to introduce a bias toward learning hierarchy-sensitive regularities. It has, however, proven difficult to assess the degree to which such a bias is effective, as the operation of the differentiable stack is not always interpretable. In this paper, we attempt to detect the presence of latent representations of hierarchical structure through an exploration of the unsupervised learning of constituency structure. Using a technique due to Shen et al. (2018a,b), we extract syntactic trees from the pushing behavior of stack RNNs trained on language modeling and classification objectives. We find that our models produce parses that reflect natural language syntactic constituencies, demonstrating that stack RNNs do indeed infer linguistically relevant hierarchical structure.","pages":"224--232","doi":"10.18653\/v1\/W19-4823","url":"https:\/\/www.aclweb.org\/anthology\/W19-4823","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4824","title":"Adversarial Attack on Sentiment Classification","authors":["Tsai, Yi-Ting","Yang, Min-Chu","Chen, Han-Yu"],"emails":["aliciatsai@berkeley.edu","minchu.yang@berkeley.edu","chen@berkeley.edu"],"author_id":["yi-ting-tsai","min-chu-yang","han-yu-chen"],"abstract":"In this paper, we propose a white-box attack algorithm called {``}Global Search{''} method and compare it with a simple misspelling noise and a more sophisticated and common white-box attack approach called {``}Greedy Search{''}. The attack methods are evaluated on the Convolutional Neural Network (CNN) sentiment classifier trained on the IMDB movie review dataset. The attack success rate is used to evaluate the effectiveness of the attack methods and the perplexity of the sentences is used to measure the degree of distortion of the generated adversarial examples. The experiment results show that the proposed {``}Global Search{''} method generates more powerful adversarial examples with less distortion or less modification to the source text.","pages":"233--240","doi":"10.18653\/v1\/W19-4824","url":"https:\/\/www.aclweb.org\/anthology\/W19-4824","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4825","title":"Open Sesame: Getting inside {BERT}{'}s Linguistic Knowledge","authors":["Lin, Yongjie","Tan, Yi Chern","Frank, Robert"],"emails":["yongjie.lin@yale.edu","yichern.tan@yale.edu","robert.frank@yale.edu"],"author_id":["yongjie-lin","yi-chern-tan","robert-frank"],"abstract":"How and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like BERT perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of BERT{'}s representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT{'}s representation of subject-verb agreement and anaphor-antecedent dependencies through a quantitative assessment of self-attention vectors. In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that BERT{'}s representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found in human processing of reflexive anaphora.","pages":"241--253","doi":"10.18653\/v1\/W19-4825","url":"https:\/\/www.aclweb.org\/anthology\/W19-4825","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4826","title":"{GE}val: Tool for Debugging {NLP} Datasets and Models","authors":["Grali{\\'n}ski, Filip","Wr{\\'o}blewska, Anna","Stanis{\\l}awek, Tomasz","Grabowski, Kamil","G{\\'o}recki, Tomasz"],"emails":["filip.grali{\\'n}ski@applica.ai","anna.wr{\\'o}blewska@applica.ai","tomasz.stanis{\\l}awek@applica.ai","kamil.grabowski@applica.ai",""],"author_id":["filip-gralinski","anna-wroblewska","tomasz-stanislawek","kamil-grabowski","tomasz-gorecki"],"abstract":"This paper presents a simple but general and effective method to debug the output of machine learning (ML) supervised models, including neural networks. The algorithm looks for features that lower the evaluation metric in such a way that it cannot be ascribed to chance (as measured by their p-values). Using this method {--} implemented as MLEval tool {--} you can find: (1) anomalies in test sets, (2) issues in preprocessing, (3) problems in the ML model itself. It can give you an insight into what can be improved in the datasets and\/or the model. The same method can be used to compare ML models or different versions of the same model. We present the tool, the theory behind it and use cases for text-based models of various types.","pages":"254--262","doi":"10.18653\/v1\/W19-4826","url":"https:\/\/www.aclweb.org\/anthology\/W19-4826","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4827","title":"From Balustrades to Pierre Vinken: Looking for Syntax in Transformer Self-Attentions","authors":["Mare{\\v{c}}ek, David","Rosa, Rudolf"],"emails":["marecek@ufal.mff.cuni.cz","rosa@ufal.mff.cuni.cz"],"author_id":["david-marecek","rudolf-rosa"],"abstract":"We inspect the multi-head self-attention in Transformer NMT encoders for three source languages, looking for patterns that could have a syntactic interpretation. In many of the attention heads, we frequently find sequences of consecutive states attending to the same position, which resemble syntactic phrases. We propose a transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions, based on automatically building and evaluating phrase-structure trees from the phrase-like sequences. We compare the resulting trees to existing constituency treebanks, both manually and by computing precision and recall.","pages":"263--275","doi":"10.18653\/v1\/W19-4827","url":"https:\/\/www.aclweb.org\/anthology\/W19-4827","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"},{"id":"W19-4828","title":"What Does {BERT} Look at? An Analysis of {BERT}{'}s Attention","authors":["Clark, Kevin","Khandelwal, Urvashi","Levy, Omer","Manning, Christopher D."],"emails":["kevclark@cs.stanford.edu","urvashik@cs.stanford.edu","omerlevy@fb.com","manning@cs.stanford.edu"],"author_id":["kevin-clark","urvashi-khandelwal","omer-levy","christopher-d-manning"],"abstract":"Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT{'}s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT{'}s attention.","pages":"276--286","doi":"10.18653\/v1\/W19-4828","url":"https:\/\/www.aclweb.org\/anthology\/W19-4828","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"}]