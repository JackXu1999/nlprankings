[{"id":"W17-5301","title":"The {R}ep{E}val 2017 Shared Task: Multi-Genre Natural Language Inference with Sentence Representations","authors":["Nangia, Nikita","Williams, Adina","Lazaridou, Angeliki","Bowman, Samuel"],"emails":["nikitanangia@nyu.edu","adinawilliams@nyu.edu","angeliki@deepmind.com","bowman@nyu.edu"],"author_id":["nikita-nangia","adina-williams","angeliki-lazaridou","samuel-bowman"],"abstract":"This paper presents the results of the RepEval 2017 Shared Task, which evaluated neural network sentence representation learning models on the Multi-Genre Natural Language Inference corpus (MultiNLI) recently introduced by Williams et al. (2017). All of the five participating teams beat the bidirectional LSTM (BiLSTM) and continuous bag of words baselines reported in Williams et al. The best single model used stacked BiLSTMs with residual connections to extract sentence features and reached 74.5{\\%} accuracy on the genre-matched test set. Surprisingly, the results of the competition were fairly consistent across the genre-matched and genre-mismatched test sets, and across subsets of the test data representing a variety of linguistic phenomena, suggesting that all of the submitted systems learned reasonably domain-independent representations for sentence meaning.","pages":"1--10","doi":"10.18653\/v1\/W17-5301","url":"https:\/\/www.aclweb.org\/anthology\/W17-5301","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for {NLP}"},{"id":"W17-5302","title":"Traversal-Free Word Vector Evaluation in Analogy Space","authors":["Che, Xiaoyin","Ring, Nico","Raschkowski, Willi","Yang, Haojin","Meinel, Christoph"],"emails":["xiaoyin.che@hpi.de","nico.ring@student.hpi.uni-potsdam.de","willi.raschkowskil@student.hpi.uni-potsdam.de","haojin.yang@hpi.de","christoph.meinel@hpi.de"],"author_id":["xiaoyin-che","nico-ring","willi-raschkowski","haojin-yang","christoph-meinel"],"abstract":"In this paper, we propose an alternative evaluating metric for word analogy questions (A to B is as C to D) in word vector evaluation. Different from the traditional method which predicts the fourth word by the given three, we measure the similarity directly on the {``}relations{''} of two pairs of given words, just as shifting the relation vectors into a new analogy space. Cosine and Euclidean distances are then calculated as measurements. Observation and experiments shows the proposed analogy space evaluation could offer a more comprehensive evaluating result on word vectors with word analogy questions. Meanwhile, computational complexity are remarkably reduced by avoiding traversing the vocabulary.","pages":"11--15","doi":"10.18653\/v1\/W17-5302","url":"https:\/\/www.aclweb.org\/anthology\/W17-5302","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for {NLP}"},{"id":"W17-5303","title":"Hypothesis Testing based Intrinsic Evaluation of Word Embeddings","authors":["Gurnani, Nishant"],"emails":["ndgurnan@ucsd.edu"],"author_id":["nishant-gurnani"],"abstract":"We introduce the cross-match test - an exact, distribution free, high-dimensional hypothesis test as an intrinsic evaluation metric for word embeddings. We show that cross-match is an effective means of measuring the distributional similarity between different vector representations and of evaluating the statistical significance of different vector embedding models. Additionally, we find that cross-match can be used to provide a quantitative measure of linguistic similarity for selecting bridge languages for machine translation. We demonstrate that the results of the hypothesis test align with our expectations and note that the framework of two sample hypothesis testing is not limited to word embeddings and can be extended to all vector representations.","pages":"16--20","doi":"10.18653\/v1\/W17-5303","url":"https:\/\/www.aclweb.org\/anthology\/W17-5303","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for {NLP}"},{"id":"W17-5304","title":"Evaluation of word embeddings against cognitive processes: primed reaction times in lexical decision and naming tasks","authors":["Auguste, Jeremy","Rey, Arnaud","Favre, Benoit"],"emails":["jeremy.auguste@univ-amu.fr","arnaud.rey@univ-amu.fr","benoit.favre@univ-amu.fr"],"author_id":["jeremy-auguste","arnaud-rey","benoit-favre"],"abstract":"This work presents a framework for word similarity evaluation grounded on cognitive sciences experimental data. Word pair similarities are compared to reaction times of subjects in large scale lexical decision and naming tasks under semantic priming. Results show that GloVe embeddings lead to significantly higher correlation with experimental measurements than other controlled and off-the-shelf embeddings, and that the choice of a training corpus is less important than that of the algorithm. Comparison of rankings with other datasets shows that the cognitive phenomenon covers more aspects than simply word relatedness or similarity.","pages":"21--26","doi":"10.18653\/v1\/W17-5304","url":"https:\/\/www.aclweb.org\/anthology\/W17-5304","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for {NLP}"},{"id":"W17-5305","title":"Playing with Embeddings : Evaluating embeddings for Robot Language Learning through {MUD} Games","authors":["Gulati, Anmol","Agrawal, Kumar Krishna"],"emails":["anmol01gulati@gmail.com","kumarkagrawal@gmail.com"],"author_id":["anmol-gulati","kumar-krishna-agrawal"],"abstract":"Acquiring language provides a ubiquitous mode of communication, across humans and robots. To this effect, distributional representations of words based on co-occurrence statistics, have provided significant advancements ranging across machine translation to comprehension. In this paper, we study the suitability of using general purpose word-embeddings for language learning in robots. We propose using text-based games as a proxy to evaluating word embedding on real robots. Based in a risk-reward setting, we review the effectiveness of the embeddings in navigating tasks in fantasy games, as an approximation to their performance on more complex scenarios, like language assisted robot navigation.","pages":"27--30","doi":"10.18653\/v1\/W17-5305","url":"https:\/\/www.aclweb.org\/anthology\/W17-5305","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for {NLP}"},{"id":"W17-5306","title":"Recognizing Textual Entailment in Twitter Using Word Embeddings","authors":["{\\c{S}}ulea, Octavia-Maria"],"emails":["mary.octavia@gmail.com"],"author_id":["octavia-maria-sulea"],"abstract":"In this paper, we investigate the application of machine learning techniques and word embeddings to the task of Recognizing Textual Entailment (RTE) in Social Media. We look at a manually labeled dataset consisting of user generated short texts posted on Twitter (tweets) and related to four recent media events (the Charlie Hebdo shooting, the Ottawa shooting, the Sydney Siege, and the German Wings crash) and test to what extent neural techniques and embeddings are able to distinguish between tweets that entail or contradict each other or that claim unrelated things. We obtain comparable results to the state of the art in a train-test setting, but we show that, due to the noisy aspect of the data, results plummet in an evaluation strategy crafted to better simulate a real-life train-test scenario.","pages":"31--35","doi":"10.18653\/v1\/W17-5306","url":"https:\/\/www.aclweb.org\/anthology\/W17-5306","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for {NLP}"},{"id":"W17-5307","title":"Recurrent Neural Network-Based Sentence Encoder with Gated Attention for Natural Language Inference","authors":["Chen, Qian","Zhu, Xiaodan","Ling, Zhen-Hua","Wei, Si","Jiang, Hui","Inkpen, Diana"],"emails":["cq1231@mail.ustc.edu.cn","xiaodan.zhu@queensu.ca","zhling@ustc.edu.cn","siwei@iflytek.com","hj@cse.yorku.ca","diana@site.uottawa.ca"],"author_id":["qian-chen","xiaodan-zhu","zhen-hua-ling","si-wei","hui-jiang","diana-inkpen"],"abstract":"The RepEval 2017 Shared Task aims to evaluate natural language understanding models for sentence representation, in which a sentence is represented as a fixed-length vector with neural networks and the quality of the representation is tested with a natural language inference task. This paper describes our system (alpha) that is ranked among the top in the Shared Task, on both the in-domain test set (obtaining a 74.9{\\%} accuracy) and on the cross-domain test set (also attaining a 74.9{\\%} accuracy), demonstrating that the model generalizes well to the cross-domain data. Our model is equipped with intra-sentence gated-attention composition which helps achieve a better performance. In addition to submitting our model to the Shared Task, we have also tested it on the Stanford Natural Language Inference (SNLI) dataset. We obtain an accuracy of 85.5{\\%}, which is the best reported result on SNLI when cross-sentence attention is not allowed, the same condition enforced in RepEval 2017.","pages":"36--40","doi":"10.18653\/v1\/W17-5307","url":"https:\/\/www.aclweb.org\/anthology\/W17-5307","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for {NLP}"},{"id":"W17-5308","title":"Shortcut-Stacked Sentence Encoders for Multi-Domain Inference","authors":["Nie, Yixin","Bansal, Mohit"],"emails":["yixin1@cs.unc.edu","mbansal@cs.unc.edu"],"author_id":["yixin-nie","mohit-bansal"],"abstract":"We present a simple sequential sentence encoder for multi-domain natural language inference. Our encoder is based on stacked bidirectional LSTM-RNNs with shortcut connections and fine-tuning of word embeddings. The overall supervised model uses the above encoder to encode two input sentences into two vectors, and then uses a classifier over the vector combination to label the relationship between these two sentences as that of entailment, contradiction, or neural. Our Shortcut-Stacked sentence encoders achieve strong improvements over existing encoders on matched and mismatched multi-domain natural language inference (top single-model result in the EMNLP RepEval 2017 Shared Task (Nangia et al., 2017)). Moreover, they achieve the new state-of-the-art encoding result on the original SNLI dataset (Bowman et al., 2015).","pages":"41--45","doi":"10.18653\/v1\/W17-5308","url":"https:\/\/www.aclweb.org\/anthology\/W17-5308","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for {NLP}"},{"id":"W17-5309","title":"Character-level Intra Attention Network for Natural Language Inference","authors":["Yang, Han","Costa-juss{\\`a}, Marta R.","Fonollosa, Jos{\\'e} A. R."],"emails":["han.yang@est.fib.upc.edu","marta.ruiz@upc.edu","jose.fonollosa@upc.edu"],"author_id":["han-yang","marta-r-costa-jussa","jose-a-r-fonollosa"],"abstract":"Natural language inference (NLI) is a central problem in language understanding. End-to-end artificial neural networks have reached state-of-the-art performance in NLI field recently. In this paper, we propose Character-level Intra Attention Network (CIAN) for the NLI task. In our model, we use the character-level convolutional network to replace the standard word embedding layer, and we use the intra attention to capture the intra-sentence semantics. The proposed CIAN model provides improved results based on a newly published MNLI corpus.","pages":"46--50","doi":"10.18653\/v1\/W17-5309","url":"https:\/\/www.aclweb.org\/anthology\/W17-5309","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for {NLP}"},{"id":"W17-5310","title":"Refining Raw Sentence Representations for Textual Entailment Recognition via Attention","authors":["Balazs, Jorge","Marrese-Taylor, Edison","Loyola, Pablo","Matsuo, Yutaka"],"emails":["jorge@weblab.t.u-tokyo.ac.jp","emarrese@weblab.t.u-tokyo.ac.jp","pablo@weblab.t.u-tokyo.ac.jp","matsuo@weblab.t.u-tokyo.ac.jp"],"author_id":["jorge-balazs","edison-marrese-taylor","pablo-loyola","yutaka-matsuo"],"abstract":"In this paper we present the model used by the team Rivercorners for the 2017 RepEval shared task. First, our model separately encodes a pair of sentences into variable-length representations by using a bidirectional LSTM. Later, it creates fixed-length raw representations by means of simple aggregation functions, which are then refined using an attention mechanism. Finally it combines the refined representations of both sentences into a single vector to be used for classification. With this model we obtained test accuracies of 72.057{\\%} and 72.055{\\%} in the matched and mismatched evaluation tracks respectively, outperforming the LSTM baseline, and obtaining performances similar to a model that relies on shared information between sentences (ESIM). When using an ensemble both accuracies increased to 72.247{\\%} and 72.827{\\%} respectively.","pages":"51--55","doi":"10.18653\/v1\/W17-5310","url":"https:\/\/www.aclweb.org\/anthology\/W17-5310","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for {NLP}"},{"id":"W17-5311","title":"{LCT}-{MALTA}{'}s Submission to {R}ep{E}val 2017 Shared Task","authors":["Vu, Hoa Trong","Pham, Thuong-Hai","Bai, Xiaoyu","Tanti, Marc","van der Plas, Lonneke","Gatt, Albert"],"emails":["hoa.vutrong.16@um.edu.mt","thuong-hai.pham.16@um.edu.mt","xiaoyu.bai.16@um.edu.mt","marctanti@gmail.com","lonneke.vanderplas@um.edu.mt","albert.gatt@um.edu.mt"],"author_id":["hoa-trong-vu","thuong-hai-pham","xiaoyu-bai","marc-tanti","lonneke-van-der-plas","albert-gatt"],"abstract":"System using BiLSTM and max pooling. Embedding is enhanced by POS, character and dependency info.","pages":"56--60","doi":"10.18653\/v1\/W17-5311","url":"https:\/\/www.aclweb.org\/anthology\/W17-5311","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for {NLP}"}]