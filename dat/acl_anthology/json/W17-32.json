[{"id":"W17-3201","title":"An Empirical Study of Adequate Vision Span for Attention-Based Neural Machine Translation","authors":["Shu, Raphael","Nakayama, Hideki"],"emails":["shu@nlab.ci.i.u-tokyo.ac.jp","nakayama@ci.i.u-tokyo.ac.jp"],"author_id":["raphael-shu","hideki-nakayama"],"abstract":"Recently, the attention mechanism plays a key role to achieve high performance for Neural Machine Translation models. However, as it computes a score function for the encoder states in all positions at each decoding step, the attention model greatly increases the computational complexity. In this paper, we investigate the adequate vision span of attention models in the context of machine translation, by proposing a novel attention framework that is capable of reducing redundant score computation dynamically. The term {``}vision span{''}{'} means a window of the encoder states considered by the attention model in one step. In our experiments, we found that the average window size of vision span can be reduced by over 50{\\%} with modest loss in accuracy on English-Japanese and German-English translation tasks.","pages":"1--10","doi":"10.18653\/v1\/W17-3201","url":"https:\/\/www.aclweb.org\/anthology\/W17-3201","publisher":"Association for Computational Linguistics","address":"Vancouver","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Neural Machine Translation"},{"id":"W17-3202","title":"Analyzing Neural {MT} Search and Model Performance","authors":["Niehues, Jan","Cho, Eunah","Ha, Thanh-Le","Waibel, Alex"],"emails":["jan.niehues@kit.edu","eunah.cho@kit.edu","thanh-le.ha@kit.edu","alex.waibel@kit.edu"],"author_id":["jan-niehues","eunah-cho","thanh-le-ha","alex-waibel"],"abstract":"In this paper, we offer an in-depth analysis about the modeling and search performance. We address the question if a more complex search algorithm is necessary. Furthermore, we investigate the question if more complex models which might only be applicable during rescoring are promising. By separating the search space and the modeling using n-best list reranking, we analyze the influence of both parts of an NMT system independently. By comparing differently performing NMT systems, we show that the better translation is already in the search space of the translation systems with less performance. This results indicate that the current search algorithms are sufficient for the NMT systems. Furthermore, we could show that even a relatively small $n$-best list of 50 hypotheses already contain notably better translations.","pages":"11--17","doi":"10.18653\/v1\/W17-3202","url":"https:\/\/www.aclweb.org\/anthology\/W17-3202","publisher":"Association for Computational Linguistics","address":"Vancouver","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Neural Machine Translation"},{"id":"W17-3203","title":"Stronger Baselines for Trustable Results in Neural Machine Translation","authors":["Denkowski, Michael","Neubig, Graham"],"emails":["mdenkows@amazon.com","gneubig@cs.cmu.edu"],"author_id":["michael-denkowski","graham-neubig"],"abstract":"Interest in neural machine translation has grown rapidly as its effectiveness has been demonstrated across language and data scenarios. New research regularly introduces architectural and algorithmic improvements that lead to significant gains over {``}vanilla{''} NMT implementations. However, these new techniques are rarely evaluated in the context of previously published techniques, specifically those that are widely used in state-of-the-art production and shared-task systems. As a result, it is often difficult to determine whether improvements from research will carry over to systems deployed for real-world use. In this work, we recommend three specific methods that are relatively easy to implement and result in much stronger experimental systems. Beyond reporting significantly higher BLEU scores, we conduct an in-depth analysis of where improvements originate and what inherent weaknesses of basic NMT models are being addressed. We then compare the relative gains afforded by several other techniques proposed in the literature when starting with vanilla systems versus our stronger baselines, showing that experimental conclusions may change depending on the baseline chosen. This indicates that choosing a strong baseline is crucial for reporting reliable experimental results.","pages":"18--27","doi":"10.18653\/v1\/W17-3203","url":"https:\/\/www.aclweb.org\/anthology\/W17-3203","publisher":"Association for Computational Linguistics","address":"Vancouver","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Neural Machine Translation"},{"id":"W17-3204","title":"Six Challenges for Neural Machine Translation","authors":["Koehn, Philipp","Knowles, Rebecca"],"emails":["phi@jhu.edu","rknowles@jhu.edu"],"author_id":["philipp-koehn","rebecca-knowles"],"abstract":"We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.","pages":"28--39","doi":"10.18653\/v1\/W17-3204","url":"https:\/\/www.aclweb.org\/anthology\/W17-3204","publisher":"Association for Computational Linguistics","address":"Vancouver","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Neural Machine Translation"},{"id":"W17-3205","title":"Cost Weighting for Neural Machine Translation Domain Adaptation","authors":["Chen, Boxing","Cherry, Colin","Foster, George","Larkin, Samuel"],"emails":["","","ast@nrc-cnrc.gc.ca",""],"author_id":["boxing-chen","colin-cherry","george-foster","samuel-larkin"],"abstract":"In this paper, we propose a new domain adaptation technique for neural machine translation called cost weighting, which is appropriate for adaptation scenarios in which a small in-domain data set and a large general-domain data set are available. Cost weighting incorporates a domain classifier into the neural machine translation training algorithm, using features derived from the encoder representation in order to distinguish in-domain from out-of-domain data. Classifier probabilities are used to weight sentences according to their domain similarity when updating the parameters of the neural translation model. We compare cost weighting to two traditional domain adaptation techniques developed for statistical machine translation: data selection and sub-corpus weighting. Experiments on two large-data tasks show that both the traditional techniques and our novel proposal lead to significant gains, with cost weighting outperforming the traditional methods.","pages":"40--46","doi":"10.18653\/v1\/W17-3205","url":"https:\/\/www.aclweb.org\/anthology\/W17-3205","publisher":"Association for Computational Linguistics","address":"Vancouver","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Neural Machine Translation"},{"id":"W17-3206","title":"Detecting Untranslated Content for Neural Machine Translation","authors":["Goto, Isao","Tanaka, Hideki"],"emails":["goto.i-es@nhk.or.jp","tanaka.h-ja@nhk.or.jp"],"author_id":["isao-goto","hideki-tanaka"],"abstract":"Despite its promise, neural machine translation (NMT) has a serious problem in that source content may be mistakenly left untranslated. The ability to detect untranslated content is important for the practical use of NMT. We evaluate two types of probability with which to detect untranslated content: the cumulative attention (ATN) probability and back translation (BT) probability from the target sentence to the source sentence. Experiments on detecting untranslated content in Japanese-English patent translations show that ATN and BT are each more effective than random choice, BT is more effective than ATN, and the combination of the two provides further improvements. We also confirmed the effectiveness of using ATN and BT to rerank the n-best NMT outputs.","pages":"47--55","doi":"10.18653\/v1\/W17-3206","url":"https:\/\/www.aclweb.org\/anthology\/W17-3206","publisher":"Association for Computational Linguistics","address":"Vancouver","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Neural Machine Translation"},{"id":"W17-3207","title":"Beam Search Strategies for Neural Machine Translation","authors":["Freitag, Markus","Al-Onaizan, Yaser"],"emails":["freitagm@us.ibm.com","onaizan@us.ibm.com"],"author_id":["markus-freitag","yaser-al-onaizan"],"abstract":"The basic concept in Neural Machine Translation (NMT) is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is then using a simple left-to-right beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-to-right while keeping a fixed amount of active candidates at each time step. First, this simple search is less adaptive as it also expands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increasing the beam size until no performance improvement can be observed. While you can reach better performance, this has the drawback of a slower decoding speed. In this paper, we concentrate on speeding up the decoder by applying a more flexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores. We speed up the original decoder by up to 43{\\%} for the two language pairs German to English and Chinese to English without losing any translation quality.","pages":"56--60","doi":"10.18653\/v1\/W17-3207","url":"https:\/\/www.aclweb.org\/anthology\/W17-3207","publisher":"Association for Computational Linguistics","address":"Vancouver","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Neural Machine Translation"},{"id":"W17-3208","title":"An Empirical Study of Mini-Batch Creation Strategies for Neural Machine Translation","authors":["Morishita, Makoto","Oda, Yusuke","Neubig, Graham","Yoshino, Koichiro","Sudoh, Katsuhito","Nakamura, Satoshi"],"emails":["morishita.makoto@lab.ntt.co.jp","oda.yusuke.on9@is.naist.jp","gneubig@cs.cmu.edu","koichiro@is.naist.jp","sudoh@is.naist.jp","s-nakamura@is.naist.jp"],"author_id":["makoto-morishita","yusuke-oda","graham-neubig","koichiro-yoshino","katsuhito-sudoh","satoshi-nakamura"],"abstract":"Training of neural machine translation (NMT) models usually uses mini-batches for efficiency purposes. During the mini-batched training process, it is necessary to pad shorter sentences in a mini-batch to be equal in length to the longest sentence therein for efficient computation. Previous work has noted that sorting the corpus based on the sentence length before making mini-batches reduces the amount of padding and increases the processing speed. However, despite the fact that mini-batch creation is an essential step in NMT training, widely used NMT toolkits implement disparate strategies for doing so, which have not been empirically validated or compared. This work investigates mini-batch creation strategies with experiments over two different datasets. Our results suggest that the choice of a mini-batch creation strategy has a large effect on NMT training and some length-based sorting strategies do not always work well compared with simple shuffling.","pages":"61--68","doi":"10.18653\/v1\/W17-3208","url":"https:\/\/www.aclweb.org\/anthology\/W17-3208","publisher":"Association for Computational Linguistics","address":"Vancouver","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Neural Machine Translation"},{"id":"W17-3209","title":"Detecting Cross-Lingual Semantic Divergence for Neural Machine Translation","authors":["Carpuat, Marine","Vyas, Yogarshi","Niu, Xing"],"emails":["marine@cs.umd.edu","yogarshi@cs.umd.edu","xingniu@cs.umd.edu"],"author_id":["marine-carpuat","yogarshi-vyas","xing-niu"],"abstract":"Parallel corpora are often not as parallel as one might assume: non-literal translations and noisy translations abound, even in curated corpora routinely used for training and evaluation. We use a cross-lingual textual entailment system to distinguish sentence pairs that are parallel in meaning from those that are not, and show that filtering out divergent examples from training improves translation quality.","pages":"69--79","doi":"10.18653\/v1\/W17-3209","url":"https:\/\/www.aclweb.org\/anthology\/W17-3209","publisher":"Association for Computational Linguistics","address":"Vancouver","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Neural Machine Translation"}]