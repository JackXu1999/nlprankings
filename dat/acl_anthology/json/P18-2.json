[{"id":"P18-2001","title":"Continuous Learning in a Hierarchical Multiscale Neural Network","authors":["Wolf, Thomas","Chaumond, Julien","Delangue, Clement"],"emails":["thomas@huggingface.co","julien@huggingface.co","clement@huggingface.co"],"author_id":["thomas-wolf","julien-chaumond","clement-delangue"],"abstract":"We reformulate the problem of encoding a multi-scale representation of a sequence in a language model by casting it in a continuous learning framework. We propose a hierarchical multi-scale language model in which short time-scale dependencies are encoded in the hidden state of a lower-level recurrent neural network while longer time-scale dependencies are encoded in the dynamic of the lower-level network by having a meta-learner update the weights of the lower-level neural network in an online meta-learning fashion. We use elastic weights consolidation as a higher-level to prevent catastrophic forgetting in our continuous learning framework.","pages":"1--7","doi":"10.18653\/v1\/P18-2001","url":"https:\/\/www.aclweb.org\/anthology\/P18-2001","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2002","title":"Restricted Recurrent Neural Tensor Networks: Exploiting Word Frequency and Compositionality","authors":["Salle, Alexandre","Villavicencio, Aline"],"emails":["alex@alexsalle.com","avillavicencio@inf.ufrgs.br"],"author_id":["alexandre-salle","aline-villavicencio"],"abstract":"Increasing the capacity of recurrent neural networks (RNN) usually involves augmenting the size of the hidden layer, with significant increase of computational cost. Recurrent neural tensor networks (RNTN) increase capacity using distinct hidden layer weights for each word, but with greater costs in memory usage. In this paper, we introduce restricted recurrent neural tensor networks (r-RNTN) which reserve distinct hidden layer weights for frequent vocabulary words while sharing a single set of weights for infrequent words. Perplexity evaluations show that for fixed hidden layer sizes, r-RNTNs improve language model performance over RNNs using only a small fraction of the parameters of unrestricted RNTNs. These results hold for r-RNTNs using Gated Recurrent Units and Long Short-Term Memory.","pages":"8--13","doi":"10.18653\/v1\/P18-2002","url":"https:\/\/www.aclweb.org\/anthology\/P18-2002","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2003","title":"Deep {RNN}s Encode Soft Hierarchical Syntax","authors":["Blevins, Terra","Levy, Omer","Zettlemoyer, Luke"],"emails":["blvns@cs.washington.edu","omerlevy@cs.washington.edu","lsz@cs.washington.edu"],"author_id":["terra-blevins","omer-levy","luke-zettlemoyer"],"abstract":"We present a set of experiments to demonstrate that deep recurrent neural networks (RNNs) learn internal representations that capture soft hierarchical notions of syntax from highly varied supervision. We consider four syntax tasks at different depths of the parse tree; for each word, we predict its part of speech as well as the first (parent), second (grandparent) and third level (great-grandparent) constituent labels that appear above it. These predictions are made from representations produced at different depths in networks that are pretrained with one of four objectives: dependency parsing, semantic role labeling, machine translation, or language modeling. In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision.","pages":"14--19","doi":"10.18653\/v1\/P18-2003","url":"https:\/\/www.aclweb.org\/anthology\/P18-2003","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2004","title":"Word Error Rate Estimation for Speech Recognition: e-{WER}","authors":["Ali, Ahmed","Renals, Steve"],"emails":["amali@qf.org.qa","s.renals@ed.ac.uk"],"author_id":["ahmed-ali","steve-renals"],"abstract":"Measuring the performance of automatic speech recognition (ASR) systems requires manually transcribed data in order to compute the word error rate (WER), which is often time-consuming and expensive. In this paper, we propose a novel approach to estimate WER, or e-WER, which does not require a gold-standard transcription of the test set. Our e-WER framework uses a comprehensive set of features: ASR recognised text, character recognition results to complement recognition output, and internal decoder features. We report results for the two features; black-box and glass-box using unseen 24 Arabic broadcast programs. Our system achieves 16.9{\\%} WER root mean squared error (RMSE) across 1,400 sentences. The estimated overall WER e-WER was 25.3{\\%} for the three hours test set, while the actual WER was 28.5{\\%}.","pages":"20--24","doi":"10.18653\/v1\/P18-2004","url":"https:\/\/www.aclweb.org\/anthology\/P18-2004","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2005","title":"Towards Robust and Privacy-preserving Text Representations","authors":["Li, Yitong","Baldwin, Timothy","Cohn, Trevor"],"emails":["yitongl4@student.unimelb.edu.au","tbaldwin@unimelb.edu.au","tcohn@unimelb.edu.au"],"author_id":["yitong-li","timothy-baldwin","trevor-cohn"],"abstract":"Written text often provides sufficient clues to identify the author, their gender, age, and other important attributes. Consequently, the authorship of training and evaluation corpora can have unforeseen impacts, including differing model performance for different user groups, as well as privacy implications. In this paper, we propose an approach to explicitly obscure important author characteristics at training time, such that representations learned are invariant to these attributes. Evaluating on two tasks, we show that this leads to increased privacy in the learned representations, as well as more robust models to varying evaluation conditions, including out-of-domain corpora.","pages":"25--30","doi":"10.18653\/v1\/P18-2005","url":"https:\/\/www.aclweb.org\/anthology\/P18-2005","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2006","title":"{H}ot{F}lip: White-Box Adversarial Examples for Text Classification","authors":["Ebrahimi, Javid","Rao, Anyi","Lowd, Daniel","Dou, Dejing"],"emails":["javid@cs.uoregon.edu","anyirao@smail.nju.edu.cn","lowd@cs.uoregon.edu","dou@cs.uoregon.edu"],"author_id":["javid-ebrahimi","anyi-rao","daniel-lowd","dejing-dou"],"abstract":"We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.","pages":"31--36","doi":"10.18653\/v1\/P18-2006","url":"https:\/\/www.aclweb.org\/anthology\/P18-2006","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2007","title":"Domain Adapted Word Embeddings for Improved Sentiment Classification","authors":["K Sarma, Prathusha","Liang, Yingyu","Sethares, Bill"],"emails":["kameswarasar@wisc.edu","yliang@cs.wisc.edu","sethares@wisc.edu"],"author_id":["prathusha-kameswara-sarma","yingyu-liang","bill-sethares"],"abstract":"Generic word embeddings are trained on large-scale generic corpora; Domain Specific (DS) word embeddings are trained only on data from a domain of interest. This paper proposes a method to combine the breadth of generic embeddings with the specificity of domain specific embeddings. The resulting embeddings, called Domain Adapted (DA) word embeddings, are formed by aligning corresponding word vectors using Canonical Correlation Analysis (CCA) or the related nonlinear Kernel CCA. Evaluation results on sentiment classification tasks show that the DA embeddings substantially outperform both generic, DS embeddings when used as input features to standard or state-of-the-art sentence encoding algorithms for classification.","pages":"37--42","doi":"10.18653\/v1\/P18-2007","url":"https:\/\/www.aclweb.org\/anthology\/P18-2007","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2008","title":"Active learning for deep semantic parsing","authors":["Duong, Long","Afshar, Hadi","Estival, Dominique","Pink, Glen","Cohen, Philip","Johnson, Mark"],"emails":["longd@voicebox.com","hadia@voicebox.com","dominiquee@voicebox.com","glenp@voicebox.com","philipc@voicebox.com","markj@voicebox.com"],"author_id":["long-duong","hadi-afshar","dominique-estival","glen-pink","philip-r-cohen","mark-johnson"],"abstract":"Semantic parsing requires training data that is expensive and slow to collect. We apply active learning to both traditional and {``}overnight{''} data collection approaches. We show that it is possible to obtain good training hyperparameters from seed data which is only a small fraction of the full dataset. We show that uncertainty sampling based on least confidence score is competitive in traditional data collection but not applicable for overnight collection. We propose several active learning strategies for overnight data collection and show that different example selection strategies per domain perform best.","pages":"43--48","doi":"10.18653\/v1\/P18-2008","url":"https:\/\/www.aclweb.org\/anthology\/P18-2008","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2009","title":"Learning Thematic Similarity Metric from Article Sections Using Triplet Networks","authors":["Ein Dor, Liat","Mass, Yosi","Halfon, Alon","Venezian, Elad","Shnayderman, Ilya","Aharonov, Ranit","Slonim, Noam"],"emails":["liate@il.ibm.com","yosimass@il.ibm.com","alonhal@il.ibm.com","eladv@il.ibm.com","ilyashn@il.ibm.com","ranita@il.ibm.com","noams@il.ibm.com"],"author_id":["liat-ein-dor","yosi-mass","alon-halfon","elad-venezian","ilya-shnayderman","ranit-aharonov","noam-slonim"],"abstract":"In this paper we suggest to leverage the partition of articles into sections, in order to learn thematic similarity metric between sentences. We assume that a sentence is thematically closer to sentences within its section than to sentences from other sections. Based on this assumption, we use Wikipedia articles to automatically create a large dataset of weakly labeled sentence triplets, composed of a pivot sentence, one sentence from the same section and one from another section. We train a triplet network to embed sentences from the same section closer. To test the performance of the learned embeddings, we create and release a sentence clustering benchmark. We show that the triplet network learns useful thematic metrics, that significantly outperform state-of-the-art semantic similarity methods and multipurpose embeddings on the task of thematic clustering of sentences. We also show that the learned embeddings perform well on the task of sentence semantic similarity prediction.","pages":"49--54","doi":"10.18653\/v1\/P18-2009","url":"https:\/\/www.aclweb.org\/anthology\/P18-2009","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2010","title":"Unsupervised Semantic Frame Induction using Triclustering","authors":["Ustalov, Dmitry","Panchenko, Alexander","Kutuzov, Andrey","Biemann, Chris","Ponzetto, Simone Paolo"],"emails":["dmitry@informatik.uni-mannheim.de","panchenko@informatik.uni-hamburg.de","andreku@ifi.uio.no","biemann@informatik.uni-hamburg.de","simone@informatik.uni-mannheim.de"],"author_id":["dmitry-ustalov","alexander-panchenko","andrey-kutuzov","chris-biemann","simone-paolo-ponzetto"],"abstract":"We use dependency triples automatically extracted from a Web-scale corpus to perform unsupervised semantic frame induction. We cast the frame induction problem as a triclustering problem that is a generalization of clustering for triadic data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task.","pages":"55--62","doi":"10.18653\/v1\/P18-2010","url":"https:\/\/www.aclweb.org\/anthology\/P18-2010","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2011","title":"Identification of Alias Links among Participants in Narratives","authors":["Patil, Sangameshwar","Pawar, Sachin","Hingmire, Swapnil","Palshikar, Girish","Varma, Vasudeva","Bhattacharyya, Pushpak"],"emails":["sangameshwar.patil@tcs.com","sachin7.p@tcs.com","swapnil.hingmire@tcs.com","gk.palshikar@tcs.com","vv@iiit.ac.in","pb@cse.iitb.ac.in"],"author_id":["sangameshwar-patil","sachin-pawar","swapnil-hingmire","girish-palshikar","vasudeva-varma","pushpak-bhattacharyya"],"abstract":"Identification of distinct and independent participants (entities of interest) in a narrative is an important task for many NLP applications. This task becomes challenging because these participants are often referred to using multiple aliases. In this paper, we propose an approach based on linguistic knowledge for identification of aliases mentioned using proper nouns, pronouns or noun phrases with common noun headword. We use Markov Logic Network (MLN) to encode the linguistic knowledge for identification of aliases. We evaluate on four diverse history narratives of varying complexity. Our approach performs better than the state-of-the-art approach as well as a combination of standard named entity recognition and coreference resolution techniques.","pages":"63--68","doi":"10.18653\/v1\/P18-2011","url":"https:\/\/www.aclweb.org\/anthology\/P18-2011","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2012","title":"Named Entity Recognition With Parallel Recurrent Neural Networks","authors":["{\\v{Z}}ukov-Gregori{\\v{c}}, Andrej","Bachrach, Yoram","Coope, Sam"],"emails":["andrej.zukovgregoric.2010@live.rhul.ac.uk","yorambac@gmail.com","sam@digitalgenius.com"],"author_id":["andrej-zukov-gregoric","yoram-bachrach","sam-coope"],"abstract":"We present a new architecture for named entity recognition. Our model employs multiple independent bidirectional LSTM units across the same input and promotes diversity among them by employing an inter-model regularization term. By distributing computation across multiple smaller LSTMs we find a significant reduction in the total number of parameters. We find our architecture achieves state-of-the-art performance on the CoNLL 2003 NER dataset.","pages":"69--74","doi":"10.18653\/v1\/P18-2012","url":"https:\/\/www.aclweb.org\/anthology\/P18-2012","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2013","title":"Type-Sensitive Knowledge Base Inference Without Explicit Type Supervision","authors":["Jain, Prachi","Kumar, Pankaj","{Mausam}","Chakrabarti, Soumen"],"emails":["p6.jain@gmail.com","k97.pankaj@gmail.com","mausam@cse.iitd.ac.in","soumen@cse.iitb.ac.in"],"author_id":["prachi-jain","pankaj-kumar","mausam","soumen-chakrabarti"],"abstract":"State-of-the-art knowledge base completion (KBC) models predict a score for every known or unknown fact via a latent factorization over entity and relation embeddings. We observe that when they fail, they often make entity predictions that are incompatible with the type required by the relation. In response, we enhance each base factorization with two type-compatibility terms between entity-relation pairs, and combine the signals in a novel manner. Without explicit supervision from a type catalog, our proposed modification obtains up to 7{\\%} MRR gains over base models, and new state-of-the-art results on several datasets. Further analysis reveals that our models better represent the latent types of entities and their embeddings also predict supervised types better than the embeddings fitted by baseline models.","pages":"75--80","doi":"10.18653\/v1\/P18-2013","url":"https:\/\/www.aclweb.org\/anthology\/P18-2013","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2014","title":"A Walk-based Model on Entity Graphs for Relation Extraction","authors":["Christopoulou, Fenia","Miwa, Makoto","Ananiadou, Sophia"],"emails":["efstathia.christopoulou@manchester.ac.uk","makoto-miwa@toyota-ti.ac.jp","sophia.ananiadou@manchester.ac.uk"],"author_id":["fenia-christopoulou","makoto-miwa","sophia-ananiadou"],"abstract":"We present a novel graph-based neural network model for relation extraction. Our model treats multiple pairs in a sentence simultaneously and considers interactions among them. All the entities in a sentence are placed as nodes in a fully-connected graph structure. The edges are represented with position-aware contexts around the entity pairs. In order to consider different relation paths between two entities, we construct up to $l$-length walks between each pair. The resulting walks are merged and iteratively used to update the edge representations into longer walks representations. We show that the model achieves performance comparable to the state-of-the-art systems on the ACE 2005 dataset without using any external tools.","pages":"81--88","doi":"10.18653\/v1\/P18-2014","url":"https:\/\/www.aclweb.org\/anthology\/P18-2014","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2015","title":"Ranking-Based Automatic Seed Selection and Noise Reduction for Weakly Supervised Relation Extraction","authors":["Phi, Van-Thuy","Santoso, Joan","Shimbo, Masashi","Matsumoto, Yuji"],"emails":["phi.thuy.ph8@is.naist.jp","2joan@stts.edu","shimbo@is.naist.jp","matsu@is.naist.jp"],"author_id":["van-thuy-phi","joan-santoso","masashi-shimbo","yuji-matsumoto"],"abstract":"This paper addresses the tasks of automatic seed selection for bootstrapping relation extraction, and noise reduction for distantly supervised relation extraction. We first point out that these tasks are related. Then, inspired by ranking relation instances and patterns computed by the HITS algorithm, and selecting cluster centroids using the K-means, LSA, or NMF method, we propose methods for selecting the initial seeds from an existing resource, or reducing the level of noise in the distantly labeled data. Experiments show that our proposed methods achieve a better performance than the baseline systems in both tasks.","pages":"89--95","doi":"10.18653\/v1\/P18-2015","url":"https:\/\/www.aclweb.org\/anthology\/P18-2015","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2016","title":"Automatic Extraction of Commonsense {L}ocated{N}ear Knowledge","authors":["Xu, Frank F.","Lin, Bill Yuchen","Zhu, Kenny"],"emails":["frankxu@sjtu.edu.cn","yuchenlin@sjtu.edu.cn","kzhu@cs.sjtu.edu.cn"],"author_id":["frank-f-xu","bill-yuchen-lin","kenny-zhu"],"abstract":"LocatedNear relation is a kind of commonsense knowledge describing two physical objects that are typically found near each other in real life. In this paper, we study how to automatically extract such relationship through a sentence-level relation classifier and aggregating the scores of entity pairs from a large corpus. Also, we release two benchmark datasets for evaluation and future research.","pages":"96--101","doi":"10.18653\/v1\/P18-2016","url":"https:\/\/www.aclweb.org\/anthology\/P18-2016","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2017","title":"Neural Coreference Resolution with Deep Biaffine Attention by Joint Mention Detection and Mention Clustering","authors":["Zhang, Rui","Nogueira dos Santos, C{\\'\\i}cero","Yasunaga, Michihiro","Xiang, Bing","Radev, Dragomir"],"emails":["r.zhang@yale.edu","cicerons@us.ibm.com","michihiro.yasunaga@yale.edu","bingxia@us.ibm.com","dragomir.radev@yale.edu"],"author_id":["rui-zhang","cicero-dos-santos","michihiro-yasunaga","bing-xiang","dragomir-radev"],"abstract":"Coreference resolution aims to identify in a text all mentions that refer to the same real world entity. The state-of-the-art end-to-end neural coreference model considers all text spans in a document as potential mentions and learns to link an antecedent for each possible mention. In this paper, we propose to improve the end-to-end coreference resolution system by (1) using a biaffine attention model to get antecedent scores for each possible mention, and (2) jointly optimizing the mention detection accuracy and mention clustering accuracy given the mention cluster labels. Our model achieves the state-of-the-art performance on the CoNLL-2012 shared task English test set.","pages":"102--107","doi":"10.18653\/v1\/P18-2017","url":"https:\/\/www.aclweb.org\/anthology\/P18-2017","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2018","title":"Fully Statistical Neural Belief Tracking","authors":["Mrk{\\v{s}}i{\\'c}, Nikola","Vuli{\\'c}, Ivan"],"emails":["nikola@poly-ai.com","ivan@poly-ai.com"],"author_id":["nikola-mrksic","ivan-vulic"],"abstract":"This paper proposes an improvement to the existing data-driven Neural Belief Tracking (NBT) framework for Dialogue State Tracking (DST). The existing NBT model uses a hand-crafted belief state update mechanism which involves an expensive manual retuning step whenever the model is deployed to a new dialogue domain. We show that this update mechanism can be learned jointly with the semantic decoding and context modelling parts of the NBT model, eliminating the last rule-based module from this DST framework. We propose two different statistical update mechanisms and show that dialogue dynamics can be modelled with a very small number of additional model parameters. In our DST evaluation over three languages, we show that this model achieves competitive performance and provides a robust framework for building resource-light DST models.","pages":"108--113","doi":"10.18653\/v1\/P18-2018","url":"https:\/\/www.aclweb.org\/anthology\/P18-2018","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2019","title":"Some of Them Can be Guessed! Exploring the Effect of Linguistic Context in Predicting Quantifiers","authors":["Pezzelle, Sandro","Steinert-Threlkeld, Shane","Bernardi, Raffaella","Szymanik, Jakub"],"emails":["sandro.pezzelle@unitn.it","s.n.m.steinert-threlkeld@uva.nl","raffaella.bernardi@unitn.it","j.k.szymanik@uva.nl"],"author_id":["sandro-pezzelle","shane-steinert-threlkeld","raffaella-bernardi","jakub-szymanik"],"abstract":"We study the role of linguistic context in predicting quantifiers ({`}few{'}, {`}all{'}). We collect crowdsourced data from human participants and test various models in a local (single-sentence) and a global context (multi-sentence) condition. Models significantly out-perform humans in the former setting and are only slightly better in the latter. While human performance improves with more linguistic context (especially on proportional quantifiers), model performance suffers. Models are very effective in exploiting lexical and morpho-syntactic patterns; humans are better at genuinely understanding the meaning of the (global) context.","pages":"114--119","doi":"10.18653\/v1\/P18-2019","url":"https:\/\/www.aclweb.org\/anthology\/P18-2019","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2020","title":"A Named Entity Recognition Shootout for {G}erman","authors":["Riedl, Martin","Pad{\\'o}, Sebastian"],"emails":["riedlmn@ims.uni-stuttgart.de","pado@ims.uni-stuttgart.de"],"author_id":["martin-riedl","sebastian-pado"],"abstract":"We ask how to practically build a model for German named entity recognition (NER) that performs at the state of the art for both contemporary and historical texts, i.e., a big-data and a small-data scenario. The two best-performing model families are pitted against each other (linear-chain CRFs and BiLSTM) to observe the trade-off between expressiveness and data requirements. BiLSTM outperforms the CRF when large datasets are available and performs inferior for the smallest dataset. BiLSTMs profit substantially from transfer learning, which enables them to be trained on multiple corpora, resulting in a new state-of-the-art model for German NER on two contemporary German corpora (CoNLL 2003 and GermEval 2014) and two historic corpora.","pages":"120--125","doi":"10.18653\/v1\/P18-2020","url":"https:\/\/www.aclweb.org\/anthology\/P18-2020","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2021","title":"A dataset for identifying actionable feedback in collaborative software development","authors":["Meyers, Benjamin S.","Munaiah, Nuthan","Prud{'}hommeaux, Emily","Meneely, Andrew","Wolff, Josephine","Ovesdotter Alm, Cecilia","Murukannaiah, Pradeep"],"emails":["bsm9339@rit.edu","nm6061@rit.edu","emilypx@rit.edu","axmvse@rit.edu","jcwgpt@rit.edu","coagla@rit.edu","pkmvse@rit.edu"],"author_id":["benjamin-s-meyers","nuthan-munaiah","emily-prudhommeaux","andrew-meneely","josephine-wolff","cecilia-ovesdotter-alm","pradeep-murukannaiah"],"abstract":"Software developers and testers have long struggled with how to elicit proactive responses from their coworkers when reviewing code for security vulnerabilities and errors. For a code review to be successful, it must not only identify potential problems but also elicit an active response from the colleague responsible for modifying the code. To understand the factors that contribute to this outcome, we analyze a novel dataset of more than one million code reviews for the Google Chromium project, from which we extract linguistic features of feedback that elicited responsive actions from coworkers. Using a manually-labeled subset of reviewer comments, we trained a highly accurate classifier to identify acted-upon comments (AUC = 0.85). Our results demonstrate the utility of our dataset, the feasibility of using NLP for this new task, and the potential of NLP to improve our understanding of how communications between colleagues can be authored to elicit positive, proactive responses.","pages":"126--131","doi":"10.18653\/v1\/P18-2021","url":"https:\/\/www.aclweb.org\/anthology\/P18-2021","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2022","title":"{SNAG}: Spoken Narratives and Gaze Dataset","authors":["Vaidyanathan, Preethi","Prud{'}hommeaux, Emily T.","Pelz, Jeff B.","Alm, Cecilia O."],"emails":["pxv1621@rit.edu","emilypx@rit.edu","pelz@cis.rit.edu","coagla@rit.edu"],"author_id":["preethi-vaidyanathan","emily-prudhommeaux","jeff-b-pelz","cecilia-ovesdotter-alm"],"abstract":"Humans rely on multiple sensory modalities when examining and reasoning over images. In this paper, we describe a new multimodal dataset that consists of gaze measurements and spoken descriptions collected in parallel during an image inspection task. The task was performed by multiple participants on 100 general-domain images showing everyday objects and activities. We demonstrate the usefulness of the dataset by applying an existing visual-linguistic data fusion framework in order to label important image regions with appropriate linguistic labels.","pages":"132--137","doi":"10.18653\/v1\/P18-2022","url":"https:\/\/www.aclweb.org\/anthology\/P18-2022","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2023","title":"Analogical Reasoning on {C}hinese Morphological and Semantic Relations","authors":["Li, Shen","Zhao, Zhe","Hu, Renfen","Li, Wensi","Liu, Tao","Du, Xiaoyong"],"emails":["shen@mail.bnu.edu.cn","irishere@mail.bnu.edu.cn","helloworld@ruc.edu.cn","tliu@ruc.edu.cn","zjklws@163.com","duyong@ruc.edu.cn"],"author_id":["shen-li","zhe-zhao","renfen-hu","wensi-li","tao-liu","xiaoyong-du"],"abstract":"Analogical reasoning is effective in capturing linguistic regularities. This paper proposes an analogical reasoning task on Chinese. After delving into Chinese lexical knowledge, we sketch 68 implicit morphological relations and 28 explicit semantic relations. A big and balanced dataset CA8 is then built for this task, including 17813 questions. Furthermore, we systematically explore the influences of vector representations, context features, and corpora on analogical reasoning. With the experiments, CA8 is proved to be a reliable benchmark for evaluating Chinese word embeddings.","pages":"138--143","doi":"10.18653\/v1\/P18-2023","url":"https:\/\/www.aclweb.org\/anthology\/P18-2023","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2024","title":"Construction of a {C}hinese Corpus for the Analysis of the Emotionality of Metaphorical Expressions","authors":["Zhang, Dongyu","Lin, Hongfei","Yang, Liang","Zhang, Shaowu","Xu, Bo"],"emails":["zhangdongyu@dlut.edu.cn","hflin@dlut.edu.cn","liang@dlut.edu.cn","zhangsw@dlut.edu.cn","xubo2011@mail.dlut.edu.cn"],"author_id":["dongyu-zhang","hongfei-lin","liang-yang","shaowu-zhang","bo-xu"],"abstract":"Metaphors are frequently used to convey emotions. However, there is little research on the construction of metaphor corpora annotated with emotion for the analysis of emotionality of metaphorical expressions. Furthermore, most studies focus on English, and few in other languages, particularly Sino-Tibetan languages such as Chinese, for emotion analysis from metaphorical texts, although there are likely to be many differences in emotional expressions of metaphorical usages across different languages. We therefore construct a significant new corpus on metaphor, with 5,605 manually annotated sentences in Chinese. We present an annotation scheme that contains annotations of linguistic metaphors, emotional categories (joy, anger, sadness, fear, love, disgust and surprise), and intensity. The annotation agreement analyses for multiple annotators are described. We also use the corpus to explore and analyze the emotionality of metaphors. To the best of our knowledge, this is the first relatively large metaphor corpus with an annotation of emotions in Chinese.","pages":"144--150","doi":"10.18653\/v1\/P18-2024","url":"https:\/\/www.aclweb.org\/anthology\/P18-2024","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2025","title":"Automatic Article Commenting: the Task and Dataset","authors":["Qin, Lianhui","Liu, Lemao","Bi, Wei","Wang, Yan","Liu, Xiaojiang","Hu, Zhiting","Zhao, Hai","Shi, Shuming"],"emails":["lianhuiqin9@gmail.com","lmliu@tencent.com","victoriabi@tencent.com","brandenwang@tencent.com","kieranliu@tencent.com","zhitinghu@gmail.com","zhaohai@cs.sjtu.edu.cn","shumingshi@tencent.com"],"author_id":["lianhui-qin","lemao-liu","wei-bi","yan-wang","xiaojiang-liu","zhiting-hu","hai-zhao","shuming-shi"],"abstract":"Comments of online articles provide extended views and improve user engagement. Automatically making comments thus become a valuable functionality for online forums, intelligent chatbots, etc. This paper proposes the new task of automatic article commenting, and introduces a large-scale Chinese dataset with millions of real comments and a human-annotated subset characterizing the comments{'} varying quality. Incorporating the human bias of comment quality, we further develop automatic metrics that generalize a broad set of popular reference-based metrics and exhibit greatly improved correlations with human evaluations.","pages":"151--156","doi":"10.18653\/v1\/P18-2025","url":"https:\/\/www.aclweb.org\/anthology\/P18-2025","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2026","title":"Improved Evaluation Framework for Complex Plagiarism Detection","authors":["Belyy, Anton","Dubova, Marina","Nekrasov, Dmitry"],"emails":["anton.belyy@gmail.com","marina.dubova.97@gmail.com","dpokrasko@gmail.com"],"author_id":["anton-belyy","marina-dubova","dmitry-nekrasov"],"abstract":"Plagiarism is a major issue in science and education. Complex plagiarism, such as plagiarism of ideas, is hard to detect, and therefore it is especially important to track improvement of methods correctly. In this paper, we study the performance of plagdet, the main measure for plagiarim detection, on manually paraphrased datasets (such as PAN Summary). We reveal its fallibility under certain conditions and propose an evaluation framework with normalization of inner terms, which is resilient to the dataset imbalance. We conclude with the experimental justification of the proposed measure. The implementation of the new framework is made publicly available as a Github repository.","pages":"157--162","doi":"10.18653\/v1\/P18-2026","url":"https:\/\/www.aclweb.org\/anthology\/P18-2026","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2027","title":"Global Encoding for Abstractive Summarization","authors":["Lin, Junyang","Sun, Xu","Ma, Shuming","Su, Qi"],"emails":["linjunyang@pku.edu.cn","xusun@pku.edu.cn","shumingma@pku.edu.cn","sukia@pku.edu.cn"],"author_id":["junyang-lin","xu-sun","shuming-ma","qi-su"],"abstract":"In neural abstractive summarization, the conventional sequence-to-sequence (seq2seq) model often suffers from repetition and semantic irrelevance. To tackle the problem, we propose a global encoding framework, which controls the information flow from the encoder to the decoder based on the global information of the source context. It consists of a convolutional gated unit to perform global encoding to improve the representations of the source-side information. Evaluations on the LCSTS and the English Gigaword both demonstrate that our model outperforms the baseline models, and the analysis shows that our model is capable of generating summary of higher quality and reducing repetition.","pages":"163--169","doi":"10.18653\/v1\/P18-2027","url":"https:\/\/www.aclweb.org\/anthology\/P18-2027","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2028","title":"A Language Model based Evaluator for Sentence Compression","authors":["Zhao, Yang","Luo, Zhiyuan","Aizawa, Akiko"],"emails":["zhao@is.s.u-tokyo.ac.jp","zyluo24@is.s.u-tokyo.ac.jp","aizawa@nii.ac.jp"],"author_id":["yang-zhao","zhiyuan-luo","akiko-aizawa"],"abstract":"We herein present a language-model-based evaluator for deletion-based sentence compression and view this task as a series of deletion-and-evaluation operations using the evaluator. More specifically, the evaluator is a syntactic neural language model that is first built by learning the syntactic and structural collocation among words. Subsequently, a series of trial-and-error deletion operations are conducted on the source sentences via a reinforcement learning framework to obtain the best target compression. An empirical study shows that the proposed model can effectively generate more readable compression, comparable or superior to several strong baselines. Furthermore, we introduce a 200-sentence test set for a large-scale dataset, setting a new baseline for the future research.","pages":"170--175","doi":"10.18653\/v1\/P18-2028","url":"https:\/\/www.aclweb.org\/anthology\/P18-2028","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2029","title":"Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources","authors":["Glenski, Maria","Weninger, Tim","Volkova, Svitlana"],"emails":["mglenski@nd.edu","tweninge@nd.edu","svitlana.volkova@pnnl.gov"],"author_id":["maria-glenski","tim-weninger","svitlana-volkova"],"abstract":"In the age of social news, it is important to understand the types of reactions that are evoked from news sources with various levels of credibility. In the present work we seek to better understand how users react to trusted and deceptive news sources across two popular, and very different, social media platforms. To that end, (1) we develop a model to classify user reactions into one of nine types, such as answer, elaboration, and question, etc, and (2) we measure the speed and the type of reaction for trusted and deceptive news sources for 10.8M Twitter posts and 6.2M Reddit comments. We show that there are significant differences in the speed and the type of reactions between trusted and deceptive news sources on Twitter, but far smaller differences on Reddit.","pages":"176--181","doi":"10.18653\/v1\/P18-2029","url":"https:\/\/www.aclweb.org\/anthology\/P18-2029","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2030","title":"Content-based Popularity Prediction of Online Petitions Using a Deep Regression Model","authors":["Subramanian, Shivashankar","Baldwin, Timothy","Cohn, Trevor"],"emails":["shivashankar@student.unimelb.edu.au","tbaldwin@unimelb.edu.au","t.cohn@unimelb.edu.au"],"author_id":["shivashankar-subramanian","timothy-baldwin","trevor-cohn"],"abstract":"Online petitions are a cost-effective way for citizens to collectively engage with policy-makers in a democracy. Predicting the popularity of a petition {---} commonly measured by its signature count {---} based on its textual content has utility for policymakers as well as those posting the petition. In this work, we model this task using CNN regression with an auxiliary ordinal regression objective. We demonstrate the effectiveness of our proposed approach using UK and US government petition datasets.","pages":"182--188","doi":"10.18653\/v1\/P18-2030","url":"https:\/\/www.aclweb.org\/anthology\/P18-2030","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2031","title":"Fighting Offensive Language on Social Media with Unsupervised Text Style Transfer","authors":["Nogueira dos Santos, Cicero","Melnyk, Igor","Padhi, Inkit"],"emails":["cicerons@us.ibm.bom","igor.melnyk@ibm.com","inkit.padhi@ibm.com"],"author_id":["cicero-dos-santos","igor-melnyk","inkit-padhi"],"abstract":"We introduce a new approach to tackle the problem of offensive language in online social media. Our approach uses unsupervised text style transfer to translate offensive sentences into non-offensive ones. We propose a new method for training encoder-decoders using non-parallel data that combines a collaborative classifier, attention and the cycle consistency loss. Experimental results on data from Twitter and Reddit show that our method outperforms a state-of-the-art text style transfer system in two out of three quantitative metrics and produces reliable non-offensive transferred sentences.","pages":"189--194","doi":"10.18653\/v1\/P18-2031","url":"https:\/\/www.aclweb.org\/anthology\/P18-2031","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2032","title":"Diachronic degradation of language models: Insights from social media","authors":["Jaidka, Kokil","Chhaya, Niyati","Ungar, Lyle"],"emails":["jaidka@sas.upenn.edu","nchhaya@adobe.com","ungar@cis.upenn.edu"],"author_id":["kokil-jaidka","niyati-chhaya","lyle-ungar"],"abstract":"Natural languages change over time because they evolve to the needs of their users and the socio-technological environment. This study investigates the diachronic accuracy of pre-trained language models for downstream tasks in machine learning and user profiling. It asks the question: given that the social media platform and its users remain the same, how is language changing over time? How can these differences be used to track the changes in the affect around a particular topic? To our knowledge, this is the first study to show that it is possible to measure diachronic semantic drifts within social media and within the span of a few years.","pages":"195--200","doi":"10.18653\/v1\/P18-2032","url":"https:\/\/www.aclweb.org\/anthology\/P18-2032","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2033","title":"Task-oriented Dialogue System for Automatic Diagnosis","authors":["Wei, Zhongyu","Liu, Qianlong","Peng, Baolin","Tou, Huaixiao","Chen, Ting","Huang, Xuanjing","Wong, Kam-fai","Dai, Xiangying"],"emails":["zywei@fudan.edu.cn","17210980040@fudan.edu.cn","blpeng@se.cuhk.edu.hk","17210980013@fudan.edu.cn","17210980029@fudan.edu.cn","xjhuang@fudan.edu.cn","kfwong@se.cuhk.edu.hk","daixiangying@baidu.com"],"author_id":["zhongyu-wei","qianlong-liu","baolin-peng","huaixiao-tou","ting-chen","xuan-jing-huang","kam-fai-wong","xiang-dai"],"abstract":"In this paper, we make a move to build a dialogue system for automatic diagnosis. We first build a dataset collected from an online medical forum by extracting symptoms from both patients{'} self-reports and conversational data between patients and doctors. Then we propose a task-oriented dialogue system framework to make diagnosis for patients automatically, which can converse with patients to collect additional symptoms beyond their self-reports. Experimental results on our dataset show that additional symptoms extracted from conversation can greatly improve the accuracy for disease identification and our dialogue system is able to collect these symptoms automatically and make a better diagnosis.","pages":"201--207","doi":"10.18653\/v1\/P18-2033","url":"https:\/\/www.aclweb.org\/anthology\/P18-2033","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2034","title":"Transfer Learning for Context-Aware Question Matching in Information-seeking Conversations in E-commerce","authors":["Qiu, Minghui","Yang, Liu","Ji, Feng","Zhou, Wei","Huang, Jun","Chen, Haiqing","Croft, Bruce","Lin, Wei"],"emails":["minghui.qmh@alibaba-inc.com","lyang@cs.umass.edu","","zhongxiu.jf@alibaba-inc.com","","","croft@cs.umass.edu",""],"author_id":["minghui-qiu","liu-yang","feng-ji","wei-zhou","jun-huang","haiqing-chen","w-bruce-croft","wei-lin"],"abstract":"Building multi-turn information-seeking conversation systems is an important and challenging research topic. Although several advanced neural text matching models have been proposed for this task, they are generally not efficient for industrial applications. Furthermore, they rely on a large amount of labeled data, which may not be available in real-world applications. To alleviate these problems, we study transfer learning for multi-turn information seeking conversations in this paper. We first propose an efficient and effective multi-turn conversation model based on convolutional neural networks. After that, we extend our model to adapt the knowledge learned from a resource-rich domain to enhance the performance. Finally, we deployed our model in an industrial chatbot called AliMe Assist and observed a significant improvement over the existing online model.","pages":"208--213","doi":"10.18653\/v1\/P18-2034","url":"https:\/\/www.aclweb.org\/anthology\/P18-2034","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2035","title":"A Multi-task Approach to Learning Multilingual Representations","authors":["Singla, Karan","Can, Dogan","Narayanan, Shrikanth"],"emails":["singlak@usc.edu","dogancan@usc.edu","shri@sipi.usc.edu"],"author_id":["karan-singla","dogan-can","shrikanth-narayanan"],"abstract":"We present a novel multi-task modeling approach to learning multilingual distributed representations of text. Our system learns word and sentence embeddings jointly by training a multilingual skip-gram model together with a cross-lingual sentence similarity model. Our architecture can transparently use both monolingual and sentence aligned bilingual corpora to learn multilingual embeddings, thus covering a vocabulary significantly larger than the vocabulary of the bilingual corpora alone. Our model shows competitive performance in a standard cross-lingual document classification task. We also show the effectiveness of our method in a limited resource scenario.","pages":"214--220","doi":"10.18653\/v1\/P18-2035","url":"https:\/\/www.aclweb.org\/anthology\/P18-2035","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2036","title":"Characterizing Departures from Linearity in Word Translation","authors":["Nakashole, Ndapa","Flauger, Raphael"],"emails":["nnakashole@eng.ucsd.edu",""],"author_id":["ndapandula-nakashole","raphael-flauger"],"abstract":"We investigate the behavior of maps learned by machine translation methods. The maps translate words by projecting between word embedding spaces of different languages. We locally approximate these maps using linear maps, and find that they vary across the word embedding space. This demonstrates that the underlying maps are non-linear. Importantly, we show that the locally linear maps vary by an amount that is tightly correlated with the distance between the neighborhoods on which they are trained. Our results can be used to test non-linear methods, and to drive the design of more accurate maps for word translation.","pages":"221--227","doi":"10.18653\/v1\/P18-2036","url":"https:\/\/www.aclweb.org\/anthology\/P18-2036","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2037","title":"Filtering and Mining Parallel Data in a Joint Multilingual Space","authors":["Schwenk, Holger"],"emails":["schwenk@dfb.com"],"author_id":["holger-schwenk"],"abstract":"We learn a joint multilingual sentence embedding and use the distance between sentences in different languages to filter noisy parallel data and to mine for parallel data in large news collections. We are able to improve a competitive baseline on the WMT{'}14 English to German task by 0.3 BLEU by filtering out 25{\\%} of the training data. The same approach is used to mine additional bitexts for the WMT{'}14 system and to obtain competitive results on the BUCC shared task to identify parallel sentences in comparable corpora. The approach is generic, it can be applied to many language pairs and it is independent of the architecture of the machine translation system.","pages":"228--234","doi":"10.18653\/v1\/P18-2037","url":"https:\/\/www.aclweb.org\/anthology\/P18-2037","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2038","title":"Hybrid semi-{M}arkov {CRF} for Neural Sequence Labeling","authors":["Ye, Zhixiu","Ling, Zhen-Hua"],"emails":["zxye@mail.ustc.edu.cn","zhling@ustc.edu.cn"],"author_id":["zhixiu-ye","zhen-hua-ling"],"abstract":"This paper proposes hybrid semi-Markov conditional random fields (SCRFs) for neural sequence labeling in natural language processing. Based on conventional conditional random fields (CRFs), SCRFs have been designed for the tasks of assigning labels to segments by extracting features from and describing transitions between segments instead of words. In this paper, we improve the existing SCRF methods by employing word-level and segment-level information simultaneously. First, word-level labels are utilized to derive the segment scores in SCRFs. Second, a CRF output layer and an SCRF output layer are integrated into a unified neural network and trained jointly. Experimental results on CoNLL 2003 named entity recognition (NER) shared task show that our model achieves state-of-the-art performance when no external knowledge is used.","pages":"235--240","doi":"10.18653\/v1\/P18-2038","url":"https:\/\/www.aclweb.org\/anthology\/P18-2038","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2039","title":"A Study of the Importance of External Knowledge in the Named Entity Recognition Task","authors":["Seyler, Dominic","Dembelova, Tatiana","Del Corro, Luciano","Hoffart, Johannes","Weikum, Gerhard"],"emails":["dseyler2@illinois.edu","tdembelo@mpi-inf.mpg.de","corrogg@mpi-inf.mpg.de","jhoffart@mpi-inf.mpg.de","weikum@mpi-inf.mpg.de"],"author_id":["dominic-seyler","tatiana-dembelova","luciano-del-corro","johannes-hoffart","gerhard-weikum"],"abstract":"In this work, we discuss the importance of external knowledge for performing Named Entity Recognition (NER). We present a novel modular framework that divides the knowledge into four categories according to the depth of knowledge they convey. Each category consists of a set of features automatically generated from different information sources, such as a knowledge-base, a list of names, or document-specific semantic annotations. Further, we show the effects on performance when incrementally adding deeper knowledge and discuss effectiveness\/efficiency trade-offs.","pages":"241--246","doi":"10.18653\/v1\/P18-2039","url":"https:\/\/www.aclweb.org\/anthology\/P18-2039","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2040","title":"Improving Topic Quality by Promoting Named Entities in Topic Modeling","authors":["Krasnashchok, Katsiaryna","Jouili, Salim"],"emails":["katherine.krasnoschok@euranova.eu","salim.jouili@euranova.eu"],"author_id":["katsiaryna-krasnashchok","salim-jouili"],"abstract":"News related content has been extensively studied in both topic modeling research and named entity recognition. However, expressive power of named entities and their potential for improving the quality of discovered topics has not received much attention. In this paper we use named entities as domain-specific terms for news-centric content and present a new weighting model for Latent Dirichlet Allocation. Our experimental results indicate that involving more named entities in topic descriptors positively influences the overall quality of topics, improving their interpretability, specificity and diversity.","pages":"247--253","doi":"10.18653\/v1\/P18-2040","url":"https:\/\/www.aclweb.org\/anthology\/P18-2040","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2041","title":"Obligation and Prohibition Extraction Using Hierarchical {RNN}s","authors":["Chalkidis, Ilias","Androutsopoulos, Ion","Michos, Achilleas"],"emails":["","",""],"author_id":["ilias-chalkidis","ion-androutsopoulos","achilleas-michos"],"abstract":"We consider the task of detecting contractual obligations and prohibitions. We show that a self-attention mechanism improves the performance of a BILSTM classifier, the previous state of the art for this task, by allowing it to focus on indicative tokens. We also introduce a hierarchical BILSTM, which converts each sentence to an embedding, and processes the sentence embeddings to classify each sentence. Apart from being faster to train, the hierarchical BILSTM outperforms the flat one, even when the latter considers surrounding sentences, because the hierarchical model has a broader discourse view.","pages":"254--259","doi":"10.18653\/v1\/P18-2041","url":"https:\/\/www.aclweb.org\/anthology\/P18-2041","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2042","title":"Paper Abstract Writing through Editing Mechanism","authors":["Wang, Qingyun","Zhou, Zhihao","Huang, Lifu","Whitehead, Spencer","Zhang, Boliang","Ji, Heng","Knight, Kevin"],"emails":["","","","","","jih@rpi.edu","knight@isi.edu"],"author_id":["qingyun-wang","zhihao-zhou","lifu-huang","spencer-whitehead","boliang-zhang","heng-ji","kevin-knight"],"abstract":"We present a paper abstract writing system based on an attentive neural sequence-to-sequence model that can take a title as input and automatically generate an abstract. We design a novel Writing-editing Network that can attend to both the title and the previously generated abstract drafts and then iteratively revise and polish the abstract. With two series of Turing tests, where the human judges are asked to distinguish the system-generated abstracts from human-written ones, our system passes Turing tests by junior domain experts at a rate up to 30{\\%} and by non-expert at a rate up to 80{\\%}.","pages":"260--265","doi":"10.18653\/v1\/P18-2042","url":"https:\/\/www.aclweb.org\/anthology\/P18-2042","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2043","title":"Conditional Generators of Words Definitions","authors":["Gadetsky, Artyom","Yakubovskiy, Ilya","Vetrov, Dmitry"],"emails":["artygadetsky@yandex.ru","yakubovskiy@joom.com","vetrovd@yandex.ru"],"author_id":["artyom-gadetsky","ilya-yakubovskiy","dmitry-vetrov"],"abstract":"We explore recently introduced definition modeling technique that provided the tool for evaluation of different distributed vector representations of words through modeling dictionary definitions of words. In this work, we study the problem of word ambiguities in definition modeling and propose a possible solution by employing latent variable modeling and soft attention mechanisms. Our quantitative and qualitative evaluation and analysis of the model shows that taking into account words{'} ambiguity and polysemy leads to performance improvement.","pages":"266--271","doi":"10.18653\/v1\/P18-2043","url":"https:\/\/www.aclweb.org\/anthology\/P18-2043","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2044","title":"{CNN} for Text-Based Multiple Choice Question Answering","authors":["Chaturvedi, Akshay","Pandit, Onkar","Garain, Utpal"],"emails":["akshay91.isi@gmail.com","oapandit@gmail.com","utpal@isical.ac.in"],"author_id":["akshay-chaturvedi","onkar-arun-pandit","utpal-garain"],"abstract":"The task of Question Answering is at the very core of machine comprehension. In this paper, we propose a Convolutional Neural Network (CNN) model for text-based multiple choice question answering where questions are based on a particular article. Given an article and a multiple choice question, our model assigns a score to each question-option tuple and chooses the final option accordingly. We test our model on Textbook Question Answering (TQA) and SciQ dataset. Our model outperforms several LSTM-based baseline models on the two datasets.","pages":"272--277","doi":"10.18653\/v1\/P18-2044","url":"https:\/\/www.aclweb.org\/anthology\/P18-2044","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2045","title":"Narrative Modeling with Memory Chains and Semantic Supervision","authors":["Liu, Fei","Cohn, Trevor","Baldwin, Timothy"],"emails":["fliu3@student.unimelb.edu.au","t.cohn@unimelb.edu.au","tb@ldwin.net"],"author_id":["fei-liu","trevor-cohn","timothy-baldwin"],"abstract":"Story comprehension requires a deep semantic understanding of the narrative, making it a challenging task. Inspired by previous studies on ROC Story Cloze Test, we propose a novel method, tracking various semantic aspects with external neural memory chains while encouraging each to focus on a particular semantic aspect. Evaluated on the task of story ending prediction, our model demonstrates superior performance to a collection of competitive baselines, setting a new state of the art.","pages":"278--284","doi":"10.18653\/v1\/P18-2045","url":"https:\/\/www.aclweb.org\/anthology\/P18-2045","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2046","title":"Injecting Relational Structural Representation in Neural Networks for Question Similarity","authors":["Uva, Antonio","Bonadiman, Daniele","Moschitti, Alessandro"],"emails":["antonio.uva@unitn.it","d.bonadiman@unitn.it","amosch@amazon.com"],"author_id":["antonio-uva","daniele-bonadiman","alessandro-moschitti"],"abstract":"Effectively using full syntactic parsing information in Neural Networks (NNs) for solving relational tasks, e.g., question similarity, is still an open problem. In this paper, we propose to inject structural representations in NNs by (i) learning a model with Tree Kernels (TKs) on relatively few pairs of questions (few thousands) as gold standard (GS) training data is typically scarce, (ii) predicting labels on a very large corpus of question pairs, and (iii) pre-training NNs on such large corpus. The results on Quora and SemEval question similarity datasets show that NNs using our approach can learn more accurate models, especially after fine tuning on GS.","pages":"285--291","doi":"10.18653\/v1\/P18-2046","url":"https:\/\/www.aclweb.org\/anthology\/P18-2046","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2047","title":"A Simple and Effective Approach to Coverage-Aware Neural Machine Translation","authors":["Li, Yanyang","Xiao, Tong","Li, Yinqiao","Wang, Qiang","Xu, Changming","Zhu, Jingbo"],"emails":["li.yin.qiao.2012@hotmail.com","xiaotong@mail.neu.edu.cn","blamedrlee@outlook.com","wangqiangneu@gmail.com","changmingxu@neuq.edu.cn","lxq@bistu.edu.cn"],"author_id":["yanyang-li","tong-xiao","yinqiao-li","qiang-wang","changming-xu","jingbo-zhu"],"abstract":"We offer a simple and effective method to seek a better balance between model confidence and length preference for Neural Machine Translation (NMT). Unlike the popular length normalization and coverage models, our model does not require training nor reranking the limited n-best outputs. Moreover, it is robust to large beam sizes, which is not well studied in previous work. On the Chinese-English and English-German translation tasks, our approach yields +0.4 1.5 BLEU improvements over the state-of-the-art baselines.","pages":"292--297","doi":"10.18653\/v1\/P18-2047","url":"https:\/\/www.aclweb.org\/anthology\/P18-2047","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2048","title":"Dynamic Sentence Sampling for Efficient Training of Neural Machine Translation","authors":["Wang, Rui","Utiyama, Masao","Sumita, Eiichiro"],"emails":["wangrui@nict.go.jp","mutiyama@nict.go.jp","eiichiro.sumita@nict.go.jp"],"author_id":["rui-wang","masao-utiyama","eiichiro-sumita"],"abstract":"Traditional Neural machine translation (NMT) involves a fixed training procedure where each sentence is sampled once during each epoch. In reality, some sentences are well-learned during the initial few epochs; however, using this approach, the well-learned sentences would continue to be trained along with those sentences that were not well learned for 10-30 epochs, which results in a wastage of time. Here, we propose an efficient method to dynamically sample the sentences in order to accelerate the NMT training. In this approach, a weight is assigned to each sentence based on the measured difference between the training costs of two iterations. Further, in each epoch, a certain percentage of sentences are dynamically sampled according to their weights. Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks show that the proposed method can significantly accelerate the NMT training and improve the NMT performance.","pages":"298--304","doi":"10.18653\/v1\/P18-2048","url":"https:\/\/www.aclweb.org\/anthology\/P18-2048","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2049","title":"Compositional Representation of Morphologically-Rich Input for Neural Machine Translation","authors":["Ataman, Duygu","Federico, Marcello"],"emails":["ataman@fbk.eu","federico@fbk.eu"],"author_id":["duygu-ataman","marcello-federico"],"abstract":"Neural machine translation (NMT) models are typically trained with fixed-size input and output vocabularies, which creates an important bottleneck on their accuracy and generalization capability. As a solution, various studies proposed segmenting words into sub-word units and performing translation at the sub-lexical level. However, statistical word segmentation methods have recently shown to be prone to morphological errors, which can lead to inaccurate translations. In this paper, we propose to overcome this problem by replacing the source-language embedding layer of NMT with a bi-directional recurrent neural network that generates compositional representations of the input at any desired level of granularity. We test our approach in a low-resource setting with five languages from different morphological typologies, and under different composition assumptions. By training NMT to compose word representations from character n-grams, our approach consistently outperforms (from 1.71 to 2.48 BLEU points) NMT learning embeddings of statistically generated sub-word units.","pages":"305--311","doi":"10.18653\/v1\/P18-2049","url":"https:\/\/www.aclweb.org\/anthology\/P18-2049","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2050","title":"Extreme Adaptation for Personalized Neural Machine Translation","authors":["Michel, Paul","Neubig, Graham"],"emails":["pmichel1@cs.cmu.edu","gneubig@cs.cmu.edu"],"author_id":["paul-michel","graham-neubig"],"abstract":"Every person speaks or writes their own flavor of their native language, influenced by a number of factors: the content they tend to talk about, their gender, their social status, or their geographical origin. When attempting to perform Machine Translation (MT), these variations have a significant effect on how the system should perform translation, but this is not captured well by standard one-size-fits-all models. In this paper, we propose a simple and parameter-efficient adaptation technique that only requires adapting the bias of the output softmax to each particular user of the MT system, either directly or through a factored approximation. Experiments on TED talks in three languages demonstrate improvements in translation accuracy, and better reflection of speaker traits in the target text.","pages":"312--318","doi":"10.18653\/v1\/P18-2050","url":"https:\/\/www.aclweb.org\/anthology\/P18-2050","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2051","title":"Multi-representation ensembles and delayed {SGD} updates improve syntax-based {NMT}","authors":["Saunders, Danielle","Stahlberg, Felix","de Gispert, Adri{\\`a}","Byrne, Bill"],"emails":["","","",""],"author_id":["danielle-saunders","felix-stahlberg","adria-de-gispert","bill-byrne"],"abstract":"We explore strategies for incorporating target syntax into Neural Machine Translation. We specifically focus on syntax in ensembles containing multiple sentence representations. We formulate beam search over such ensembles using WFSTs, and describe a delayed SGD update training procedure that is especially effective for long representations like linearized syntax. Our approach gives state-of-the-art performance on a difficult Japanese-English task.","pages":"319--325","doi":"10.18653\/v1\/P18-2051","url":"https:\/\/www.aclweb.org\/anthology\/P18-2051","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2052","title":"Learning from Chunk-based Feedback in Neural Machine Translation","authors":["Petrushkov, Pavel","Khadivi, Shahram","Matusov, Evgeny"],"emails":["ppetrushkov@ebay.com","skhadivi@ebay.com","ematusov@ebay.com"],"author_id":["pavel-petrushkov","shahram-khadivi","evgeny-matusov"],"abstract":"We empirically investigate learning from partial feedback in neural machine translation (NMT), when partial feedback is collected by asking users to highlight a correct chunk of a translation. We propose a simple and effective way of utilizing such feedback in NMT training. We demonstrate how the common machine translation problem of domain mismatch between training and deployment can be reduced solely based on chunk-level user feedback. We conduct a series of simulation experiments to test the effectiveness of the proposed method. Our results show that chunk-level feedback outperforms sentence based feedback by up to 2.61{\\%} BLEU absolute.","pages":"326--331","doi":"10.18653\/v1\/P18-2052","url":"https:\/\/www.aclweb.org\/anthology\/P18-2052","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2053","title":"Bag-of-Words as Target for Neural Machine Translation","authors":["Ma, Shuming","Sun, Xu","Wang, Yizhong","Lin, Junyang"],"emails":["shumingma@pku.edu.cn","xusun@pku.edu.cn","yizhong@pku.edu.cn","linjunyang@pku.edu.cn"],"author_id":["shuming-ma","xu-sun","yizhong-wang","junyang-lin"],"abstract":"A sentence can be translated into more than one correct sentences. However, most of the existing neural machine translation models only use one of the correct translations as the targets, and the other correct sentences are punished as the incorrect sentences in the training stage. Since most of the correct translations for one sentence share the similar bag-of-words, it is possible to distinguish the correct translations from the incorrect ones by the bag-of-words. In this paper, we propose an approach that uses both the sentences and the bag-of-words as targets in the training stage, in order to encourage the model to generate the potentially correct sentences that are not appeared in the training set. We evaluate our model on a Chinese-English translation dataset, and experiments show our model outperforms the strong baselines by the BLEU score of 4.55.","pages":"332--338","doi":"10.18653\/v1\/P18-2053","url":"https:\/\/www.aclweb.org\/anthology\/P18-2053","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2054","title":"Improving Beam Search by Removing Monotonic Constraint for Neural Machine Translation","authors":["Shu, Raphael","Nakayama, Hideki"],"emails":["shu@nlab.ci.i.u-tokyo.ac.jp","nakayama@ci.i.u-tokyo.ac.jp"],"author_id":["raphael-shu","hideki-nakayama"],"abstract":"To achieve high translation performance, neural machine translation models usually rely on the beam search algorithm for decoding sentences. The beam search finds good candidate translations by considering multiple hypotheses of translations simultaneously. However, as the algorithm produces hypotheses in a monotonic left-to-right order, a hypothesis can not be revisited once it is discarded. We found such monotonicity forces the algorithm to sacrifice some good decoding paths. To mitigate this problem, we relax the monotonic constraint of the beam search by maintaining all found hypotheses in a single priority queue and using a universal score function for hypothesis selection. The proposed algorithm allows discarded hypotheses to be recovered in a later step. Despite its simplicity, we show that the proposed decoding algorithm enhances the quality of selected hypotheses and improve the translations even for high-performance models in English-Japanese translation task.","pages":"339--344","doi":"10.18653\/v1\/P18-2054","url":"https:\/\/www.aclweb.org\/anthology\/P18-2054","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2055","title":"Leveraging distributed representations and lexico-syntactic fixedness for token-level prediction of the idiomaticity of {E}nglish verb-noun combinations","authors":["King, Milton","Cook, Paul"],"emails":["milton.king@unb.ca","paul.cook@unb.ca"],"author_id":["milton-king","paul-cook"],"abstract":"Verb-noun combinations (VNCs) - e.g., blow the whistle, hit the roof, and see stars - are a common type of English idiom that are ambiguous with literal usages. In this paper we propose and evaluate models for classifying VNC usages as idiomatic or literal, based on a variety of approaches to forming distributed representations. Our results show that a model based on averaging word embeddings performs on par with, or better than, a previously-proposed approach based on skip-thoughts. Idiomatic usages of VNCs are known to exhibit lexico-syntactic fixedness. We further incorporate this information into our models, demonstrating that this rich linguistic knowledge is complementary to the information carried by distributed representations.","pages":"345--350","doi":"10.18653\/v1\/P18-2055","url":"https:\/\/www.aclweb.org\/anthology\/P18-2055","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2056","title":"Using pseudo-senses for improving the extraction of synonyms from word embeddings","authors":["Ferret, Olivier"],"emails":["olivier.ferret@cea.fr"],"author_id":["olivier-ferret"],"abstract":"The methods proposed recently for specializing word embeddings according to a particular perspective generally rely on external knowledge. In this article, we propose Pseudofit, a new method for specializing word embeddings according to semantic similarity without any external knowledge. Pseudofit exploits the notion of pseudo-sense for building several representations for each word and uses these representations for making the initial embeddings more generic. We illustrate the interest of Pseudofit for acquiring synonyms and study several variants of Pseudofit according to this perspective.","pages":"351--357","doi":"10.18653\/v1\/P18-2056","url":"https:\/\/www.aclweb.org\/anthology\/P18-2056","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2057","title":"Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora","authors":["Roller, Stephen","Kiela, Douwe","Nickel, Maximilian"],"emails":["roller@fb.com","dkiela@fb.com","maxn@fb.com"],"author_id":["stephen-roller","douwe-kiela","maximilian-nickel"],"abstract":"Methods for unsupervised hypernym detection may broadly be categorized according to two paradigms: pattern-based and distributional methods. In this paper, we study the performance of both approaches on several hypernymy tasks and find that simple pattern-based methods consistently outperform distributional methods on common benchmark datasets. Our results show that pattern-based models provide important contextual constraints which are not yet captured in distributional methods.","pages":"358--363","doi":"10.18653\/v1\/P18-2057","url":"https:\/\/www.aclweb.org\/anthology\/P18-2057","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2058","title":"Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling","authors":["He, Luheng","Lee, Kenton","Levy, Omer","Zettlemoyer, Luke"],"emails":["luheng@cs.washington.edu","kentonl@cs.washington.edu","omerlevy@cs.washington.edu","lsz@cs.washington.edu"],"author_id":["luheng-he","kenton-lee","omer-levy","luke-zettlemoyer"],"abstract":"Recent BIO-tagging-based neural semantic role labeling models are very high performing, but assume gold predicates as part of the input and cannot incorporate span-level features. We propose an end-to-end approach for jointly predicting all predicates, arguments spans, and the relations between them. The model makes independent decisions about what relationship, if any, holds between every possible word-span pair, and learns contextualized span representations that provide rich, shared input features for each decision. Experiments demonstrate that this approach sets a new state of the art on PropBank SRL without gold predicates.","pages":"364--369","doi":"10.18653\/v1\/P18-2058","url":"https:\/\/www.aclweb.org\/anthology\/P18-2058","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2059","title":"Sparse and Constrained Attention for Neural Machine Translation","authors":["Malaviya, Chaitanya","Ferreira, Pedro","Martins, Andr{\\'e} F. T."],"emails":["cmalaviy@cs.cmu.edu","pedro.miguel.ferreira@tecnico.ulisboa.pt","andre.martins@unbabel.com"],"author_id":["chaitanya-malaviya","pedro-ferreira","andre-f-t-martins"],"abstract":"In neural machine translation, words are sometimes dropped from the source or generated repeatedly in the translation. We explore novel strategies to address the coverage problem that change only the attention transformation. Our approach allocates fertilities to source words, used to bound the attention each word can receive. We experiment with various sparse and constrained attention transformations and propose a new one, constrained sparsemax, shown to be differentiable and sparse. Empirical evaluation is provided in three languages pairs.","pages":"370--376","doi":"10.18653\/v1\/P18-2059","url":"https:\/\/www.aclweb.org\/anthology\/P18-2059","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2060","title":"Neural Hidden {M}arkov Model for Machine Translation","authors":["Wang, Weiyue","Zhu, Derui","Alkhouli, Tamer","Gan, Zixuan","Ney, Hermann"],"emails":["","","","",""],"author_id":["weiyue-wang","derui-zhu","tamer-alkhouli","zixuan-gan","hermann-ney"],"abstract":"Attention-based neural machine translation (NMT) models selectively focus on specific source positions to produce a translation, which brings significant improvements over pure encoder-decoder sequence-to-sequence models. This work investigates NMT while replacing the attention component. We study a neural hidden Markov model (HMM) consisting of neural network-based alignment and lexicon models, which are trained jointly using the forward-backward algorithm. We show that the attention component can be effectively replaced by the neural network alignment model and the neural HMM approach is able to provide comparable performance with the state-of-the-art attention-based models on the WMT 2017 German\u2194English and Chinese\u2192English translation tasks.","pages":"377--382","doi":"10.18653\/v1\/P18-2060","url":"https:\/\/www.aclweb.org\/anthology\/P18-2060","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2061","title":"Bleaching Text: Abstract Features for Cross-lingual Gender Prediction","authors":["van der Goot, Rob","Ljube{\\v{s}}i{\\'c}, Nikola","Matroos, Ian","Nissim, Malvina","Plank, Barbara"],"emails":["r.van.der.goot@rug.nl","nljubesi@gmail.com","i.matroos@rug.nl","m.nissim@rug.nl","bplank@itu.dk"],"author_id":["rob-van-der-goot","nikola-ljubesic","ian-matroos","malvina-nissim","barbara-plank"],"abstract":"Gender prediction has typically focused on lexical and social network features, yielding good performance, but making systems highly language-, topic-, and platform dependent. Cross-lingual embeddings circumvent some of these limitations, but capture gender-specific style less. We propose an alternative: bleaching text, i.e., transforming lexical strings into more abstract features. This study provides evidence that such features allow for better transfer across languages. Moreover, we present a first study on the ability of humans to perform cross-lingual gender prediction. We find that human predictive power proves similar to that of our bleached models, and both perform better than lexical models.","pages":"383--389","doi":"10.18653\/v1\/P18-2061","url":"https:\/\/www.aclweb.org\/anthology\/P18-2061","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2062","title":"Orthographic Features for Bilingual Lexicon Induction","authors":["Riley, Parker","Gildea, Daniel"],"emails":["",""],"author_id":["parker-riley","daniel-gildea"],"abstract":"Recent embedding-based methods in bilingual lexicon induction show good results, but do not take advantage of orthographic features, such as edit distance, which can be helpful for pairs of related languages. This work extends embedding-based methods to incorporate these features, resulting in significant accuracy gains for related languages.","pages":"390--394","doi":"10.18653\/v1\/P18-2062","url":"https:\/\/www.aclweb.org\/anthology\/P18-2062","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2063","title":"Neural Cross-Lingual Coreference Resolution And Its Application To Entity Linking","authors":["Kundu, Gourab","Sil, Avi","Florian, Radu","Hamza, Wael"],"emails":["gkundu@us.ibm.com","avi@us.ibm.com","raduf@us.ibm.com","whamza@us.ibm.com"],"author_id":["gourab-kundu","avirup-sil","radu-florian","wael-hamza"],"abstract":"We propose an entity-centric neural crosslingual coreference model that builds on multi-lingual embeddings and language independent features. We perform both intrinsic and extrinsic evaluations of our model. In the intrinsic evaluation, we show that our model, when trained on English and tested on Chinese and Spanish, achieves competitive results to the models trained directly on Chinese and Spanish respectively. In the extrinsic evaluation, we show that our English model helps achieve superior entity linking accuracy on Chinese and Spanish test sets than the top 2015 TAC system without using any annotated data from Chinese or Spanish.","pages":"395--400","doi":"10.18653\/v1\/P18-2063","url":"https:\/\/www.aclweb.org\/anthology\/P18-2063","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2064","title":"Judicious Selection of Training Data in Assisting Language for Multilingual Neural {NER}","authors":["Murthy, Rudra","Kunchukuttan, Anoop","Bhattacharyya, Pushpak"],"emails":["rudra@cse.iitb.ac.in","ankunchu@microsoft.com","pb@cse.iitb.ac.in"],"author_id":["rudra-murthy","anoop-kunchukuttan","pushpak-bhattacharyya"],"abstract":"Multilingual learning for Neural Named Entity Recognition (NNER) involves jointly training a neural network for multiple languages. Typically, the goal is improving the NER performance of one of the languages (the primary language) using the other assisting languages. We show that the divergence in the tag distributions of the common named entities between the primary and assisting languages can reduce the effectiveness of multilingual learning. To alleviate this problem, we propose a metric based on symmetric KL divergence to filter out the highly divergent training instances in the assisting language. We empirically show that our data selection strategy improves NER performance in many languages, including those with very limited training data.","pages":"401--406","doi":"10.18653\/v1\/P18-2064","url":"https:\/\/www.aclweb.org\/anthology\/P18-2064","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2065","title":"Neural Open Information Extraction","authors":["Cui, Lei","Wei, Furu","Zhou, Ming"],"emails":["lecu@microsoft.com","fuwei@microsoft.com","mingzhou@microsoft.com"],"author_id":["lei-cui","furu-wei","ming-zhou"],"abstract":"Conventional Open Information Extraction (Open IE) systems are usually built on hand-crafted patterns from other NLP tools such as syntactic parsing, yet they face problems of error propagation. In this paper, we propose a neural Open IE approach with an encoder-decoder framework. Distinct from existing methods, the neural Open IE approach learns highly confident arguments and relation tuples bootstrapped from a state-of-the-art Open IE system. An empirical study on a large benchmark dataset shows that the neural Open IE system significantly outperforms several baselines, while maintaining comparable computational efficiency.","pages":"407--413","doi":"10.18653\/v1\/P18-2065","url":"https:\/\/www.aclweb.org\/anthology\/P18-2065","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2066","title":"Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention","authors":["Zhao, Yue","Jin, Xiaolong","Wang, Yuanzhuo","Cheng, Xueqi"],"emails":["zhaoyue@software.ict.ac.cn","jinxiaolong@ict.ac.cn","wangyuanzhuo@ict.ac.cn","cxq@ict.ac.cn"],"author_id":["yue-zhao","xiaolong-jin","yuanzhuo-wang","xueqi-cheng"],"abstract":"Document-level information is very important for event detection even at sentence level. In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, to detect events in sentences. This model first learns event detection oriented embeddings of documents through a hierarchical and supervised attention based RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events. It then uses the learned document embedding to enhance another bidirectional RNN model to identify event triggers and their types in sentences. Through experiments on the ACE-2005 dataset, we demonstrate the effectiveness and merits of the proposed DEEB-RNN model via comparison with state-of-the-art methods.","pages":"414--419","doi":"10.18653\/v1\/P18-2066","url":"https:\/\/www.aclweb.org\/anthology\/P18-2066","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2067","title":"Learning Matching Models with Weak Supervision for Response Selection in Retrieval-based Chatbots","authors":["Wu, Yu","Wu, Wei","Li, Zhoujun","Zhou, Ming"],"emails":["wuyu@buaa.edu.cn","wuwei@microsoft.com","lizj@buaa.edu.cn","mingzhou@microsoft.com"],"author_id":["yu-wu","wei-wu","zhoujun-li","ming-zhou"],"abstract":"We propose a method that can leverage unlabeled data to learn a matching model for response selection in retrieval-based chatbots. The method employs a sequence-to-sequence architecture (Seq2Seq) model as a weak annotator to judge the matching degree of unlabeled pairs, and then performs learning with both the weak signals and the unlabeled data. Experimental results on two public data sets indicate that matching models get significant improvements when they are learned with the proposed method.","pages":"420--425","doi":"10.18653\/v1\/P18-2067","url":"https:\/\/www.aclweb.org\/anthology\/P18-2067","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2068","title":"Improving Slot Filling in Spoken Language Understanding with Joint Pointer and Attention","authors":["Zhao, Lin","Feng, Zhe"],"emails":["lin.zhao@us.bosch.com","zhe.feng2@us.bosch.com"],"author_id":["lin-zhao","zhe-feng"],"abstract":"We present a generative neural network model for slot filling based on a sequence-to-sequence (Seq2Seq) model together with a pointer network, in the situation where only sentence-level slot annotations are available in the spoken dialogue data. This model predicts slot values by jointly learning to copy a word which may be out-of-vocabulary (OOV) from an input utterance through a pointer network, or generate a word within the vocabulary through an attentional Seq2Seq model. Experimental results show the effectiveness of our slot filling model, especially at addressing the OOV problem. Additionally, we integrate the proposed model into a spoken language understanding system and achieve the state-of-the-art performance on the benchmark data.","pages":"426--431","doi":"10.18653\/v1\/P18-2068","url":"https:\/\/www.aclweb.org\/anthology\/P18-2068","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2069","title":"Large-Scale Multi-Domain Belief Tracking with Knowledge Sharing","authors":["Ramadan, Osman","Budzianowski, Pawe{\\l}","Ga{\\v{s}}i{\\'c}, Milica"],"emails":["oior2@cam.ac.uk","pfb30@cam.ac.uk","mg436@cam.ac.uk"],"author_id":["osman-ramadan","pawel-budzianowski","milica-gasic"],"abstract":"Robust dialogue belief tracking is a key component in maintaining good quality dialogue systems. The tasks that dialogue systems are trying to solve are becoming increasingly complex, requiring scalability to multi-domain, semantically rich dialogues. However, most current approaches have difficulty scaling up with domains because of the dependency of the model parameters on the dialogue ontology. In this paper, a novel approach is introduced that fully utilizes semantic similarity between dialogue utterances and the ontology terms, allowing the information to be shared across domains. The evaluation is performed on a recently collected multi-domain dialogues dataset, one order of magnitude larger than currently available corpora. Our model demonstrates great capability in handling multi-domain dialogues, simultaneously outperforming existing state-of-the-art models in single-domain dialogue tracking tasks.","pages":"432--437","doi":"10.18653\/v1\/P18-2069","url":"https:\/\/www.aclweb.org\/anthology\/P18-2069","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2070","title":"Modeling discourse cohesion for discourse parsing via memory network","authors":["Jia, Yanyan","Ye, Yuan","Feng, Yansong","Lai, Yuxuan","Yan, Rui","Zhao, Dongyan"],"emails":["jiayanyan@pku.edu.cn","pkuyeyuan@pku.edu.cn","fengyansong@pku.edu.cn","erutan@pku.edu.cn","ruiyan@pku.edu.cn","zhaody@pku.edu.cn"],"author_id":["yanyan-jia","yuan-ye","yansong-feng","yuxuan-lai","rui-yan","dongyan-zhao"],"abstract":"Identifying long-span dependencies between discourse units is crucial to improve discourse parsing performance. Most existing approaches design sophisticated features or exploit various off-the-shelf tools, but achieve little success. In this paper, we propose a new transition-based discourse parser that makes use of memory networks to take discourse cohesion into account. The automatically captured discourse cohesion benefits discourse parsing, especially for long span scenarios. Experiments on the RST discourse treebank show that our method outperforms traditional featured based methods, and the memory based discourse cohesion can improve the overall parsing performance significantly.","pages":"438--443","doi":"10.18653\/v1\/P18-2070","url":"https:\/\/www.aclweb.org\/anthology\/P18-2070","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2071","title":"{S}ci{DTB}: Discourse Dependency {T}ree{B}ank for Scientific Abstracts","authors":["Yang, An","Li, Sujian"],"emails":["yangan@pku.edu.cn","lisujian@pku.edu.cn"],"author_id":["an-yang","sujian-li"],"abstract":"Annotation corpus for discourse relations benefits NLP tasks such as machine translation and question answering. In this paper, we present SciDTB, a domain-specific discourse treebank annotated on scientific articles. Different from widely-used RST-DT and PDTB, SciDTB uses dependency trees to represent discourse structure, which is flexible and simplified to some extent but do not sacrifice structural integrity. We discuss the labeling framework, annotation workflow and some statistics about SciDTB. Furthermore, our treebank is made as a benchmark for evaluating discourse dependency parsers, on which we provide several baselines as fundamental work.","pages":"444--449","doi":"10.18653\/v1\/P18-2071","url":"https:\/\/www.aclweb.org\/anthology\/P18-2071","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2072","title":"Predicting accuracy on large datasets from smaller pilot data","authors":["Johnson, Mark","Anderson, Peter","Dras, Mark","Steedman, Mark"],"emails":["","","","steedman@inf.ed.ac.uk"],"author_id":["mark-johnson","peter-anderson","mark-dras","mark-steedman"],"abstract":"Because obtaining training data is often the most difficult part of an NLP or ML project, we develop methods for predicting how much data is required to achieve a desired test accuracy by extrapolating results from models trained on a small pilot training dataset. We model how accuracy varies as a function of training size on subsets of the pilot data, and use that model to predict how much training data would be required to achieve the desired accuracy. We introduce a new performance extrapolation task to evaluate how well different extrapolations predict accuracy on larger training sets. We show that details of hyperparameter optimisation and the extrapolation models can have dramatic effects in a document classification task. We believe this is an important first step in developing methods for estimating the resources required to meet specific engineering performance targets.","pages":"450--455","doi":"10.18653\/v1\/P18-2072","url":"https:\/\/www.aclweb.org\/anthology\/P18-2072","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2073","title":"The Influence of Context on Sentence Acceptability Judgements","authors":["Bernardy, Jean-Philippe","Lappin, Shalom","Lau, Jey Han"],"emails":["jean-philippe.bernardy@gu.se","shalom.lappin@gu.se","jeyhan.lau@gmail.com"],"author_id":["jean-philippe-bernardy","shalom-lappin","jey-han-lau"],"abstract":"We investigate the influence that document context exerts on human acceptability judgements for English sentences, via two sets of experiments. The first compares ratings for sentences presented on their own with ratings for the same set of sentences given in their document contexts. The second assesses the accuracy with which two types of neural models {---} one that incorporates context during training and one that does not {---} predict these judgements. Our results indicate that: (1) context improves acceptability ratings for ill-formed sentences, but also reduces them for well-formed sentences; and (2) context helps unsupervised systems to model acceptability.","pages":"456--461","doi":"10.18653\/v1\/P18-2073","url":"https:\/\/www.aclweb.org\/anthology\/P18-2073","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2074","title":"Do Neural Network Cross-Modal Mappings Really Bridge Modalities?","authors":["Collell, Guillem","Moens, Marie-Francine"],"emails":["gcollell@kuleuven.be","sien.moens@cs.kuleuven.be"],"author_id":["guillem-collell","marie-francine-moens"],"abstract":"Feed-forward networks are widely used in cross-modal applications to bridge modalities by mapping distributed vectors of one modality to the other, or to a shared space. The predicted vectors are then used to perform e.g., retrieval or labeling. Thus, the success of the whole system relies on the ability of the mapping to make the neighborhood structure (i.e., the pairwise similarities) of the predicted vectors akin to that of the target vectors. However, whether this is achieved has not been investigated yet. Here, we propose a new similarity measure and two ad hoc experiments to shed light on this issue. In three cross-modal benchmarks we learn a large number of language-to-vision and vision-to-language neural network mappings (up to five layers) using a rich diversity of image and text features and loss functions. Our results reveal that, surprisingly, the neighborhood structure of the predicted vectors consistently resembles more that of the input vectors than that of the target vectors. In a second experiment, we further show that untrained nets do not significantly disrupt the neighborhood (i.e., semantic) structure of the input vectors.","pages":"462--468","doi":"10.18653\/v1\/P18-2074","url":"https:\/\/www.aclweb.org\/anthology\/P18-2074","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2075","title":"Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing","authors":["Fried, Daniel","Klein, Dan"],"emails":["dfried@cs.berkeley.edu","klein@cs.berkeley.edu"],"author_id":["daniel-fried","dan-klein"],"abstract":"Dynamic oracles provide strong supervision for training constituency parsers with exploration, but must be custom defined for a given parser{'}s transition system. We explore using a policy gradient method as a parser-agnostic alternative. In addition to directly optimizing for a tree-level metric such as F1, policy gradient has the potential to reduce exposure bias by allowing exploration during training; moreover, it does not require a dynamic oracle for supervision. On four constituency parsers in three languages, the method substantially outperforms static oracle likelihood training in almost all settings. For parsers where a dynamic oracle is available (including a novel oracle which we define for the transition system of Dyer et al., 2016), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle.","pages":"469--476","doi":"10.18653\/v1\/P18-2075","url":"https:\/\/www.aclweb.org\/anthology\/P18-2075","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2076","title":"Linear-time Constituency Parsing with {RNN}s and Dynamic Programming","authors":["Hong, Juneki","Huang, Liang"],"emails":["juneki.hong@gmail.com","liang.huang.sh@gmail.com"],"author_id":["juneki-hong","liang-huang"],"abstract":"Recently, span-based constituency parsing has achieved competitive accuracies with extremely simple models by using bidirectional RNNs to model {``}spans{''}. However, the minimal span parser of Stern et al. (2017a) which holds the current state of the art accuracy is a chart parser running in cubic time, $O(n^3)$, which is too slow for longer sentences and for applications beyond sentence boundaries such as end-to-end discourse parsing and joint sentence boundary detection and parsing. We propose a linear-time constituency parser with RNNs and dynamic programming using graph-structured stack and beam search, which runs in time $O(n b^2)$ where $b$ is the beam size. We further speed this up to $O(n b log b)$ by integrating cube pruning. Compared with chart parsing baselines, this linear-time parser is substantially faster for long sentences on the Penn Treebank and orders of magnitude faster for discourse parsing, and achieves the highest F1 accuracy on the Penn Treebank among single model end-to-end systems.","pages":"477--483","doi":"10.18653\/v1\/P18-2076","url":"https:\/\/www.aclweb.org\/anthology\/P18-2076","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2077","title":"Simpler but More Accurate Semantic Dependency Parsing","authors":["Dozat, Timothy","Manning, Christopher D."],"emails":["tdozat@stanford.edu","manning@stanford.edu"],"author_id":["timothy-dozat","christopher-d-manning"],"abstract":"While syntactic dependency annotations concentrate on the surface or functional structure of a sentence, semantic dependency annotations aim to capture between-word relationships that are more closely related to the meaning of a sentence, using graph-structured representations. We extend the LSTM-based syntactic parser of Dozat and Manning (2017) to train on and generate these graph structures. The resulting system on its own achieves state-of-the-art performance, beating the previous, substantially more complex state-of-the-art system by 0.6{\\%} labeled F1. Adding linguistically richer input representations pushes the margin even higher, allowing us to beat it by 1.9{\\%} labeled F1.","pages":"484--490","doi":"10.18653\/v1\/P18-2077","url":"https:\/\/www.aclweb.org\/anthology\/P18-2077","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2078","title":"Simplified Abugidas","authors":["Ding, Chenchen","Utiyama, Masao","Sumita, Eiichiro"],"emails":["chenchen.ding@nict.go.jp","mutiyama@nict.go.jp","eiichiro.sumita@nict.go.jp"],"author_id":["chenchen-ding","masao-utiyama","eiichiro-sumita"],"abstract":"An abugida is a writing system where the consonant letters represent syllables with a default vowel and other vowels are denoted by diacritics. We investigate the feasibility of recovering the original text written in an abugida after omitting subordinate diacritics and merging consonant letters with similar phonetic values. This is crucial for developing more efficient input methods by reducing the complexity in abugidas. Four abugidas in the southern Brahmic family, i.e., Thai, Burmese, Khmer, and Lao, were studied using a newswire 20,000-sentence dataset. We compared the recovery performance of a support vector machine and an LSTM-based recurrent neural network, finding that the abugida graphemes could be recovered with 94{\\%} - 97{\\%} accuracy at the top-1 level and 98{\\%} - 99{\\%} at the top-4 level, even after omitting most diacritics (10 - 30 types) and merging the remaining 30 - 50 characters into 21 graphemes.","pages":"491--495","doi":"10.18653\/v1\/P18-2078","url":"https:\/\/www.aclweb.org\/anthology\/P18-2078","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2079","title":"Automatic Academic Paper Rating Based on Modularized Hierarchical Convolutional Neural Network","authors":["Yang, Pengcheng","Sun, Xu","Li, Wei","Ma, Shuming"],"emails":["pc@pku.edu.cn","xusun@pku.edu.cn","liweitj47@pku.edu.cn","shumingma@pku.edu.cn"],"author_id":["pengcheng-yang","xu-sun","wei-li","shuming-ma"],"abstract":"As more and more academic papers are being submitted to conferences and journals, evaluating all these papers by professionals is time-consuming and can cause inequality due to the personal factors of the reviewers. In this paper, in order to assist professionals in evaluating academic papers, we propose a novel task: automatic academic paper rating (AAPR), which automatically determine whether to accept academic papers. We build a new dataset for this task and propose a novel modularized hierarchical convolutional neural network to achieve automatic academic paper rating. Evaluation results show that the proposed model outperforms the baselines by a large margin. The dataset and code are available at \\url{https:\/\/github.com\/lancopku\/AAPR}","pages":"496--502","doi":"10.18653\/v1\/P18-2079","url":"https:\/\/www.aclweb.org\/anthology\/P18-2079","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2080","title":"Automated essay scoring with string kernels and word embeddings","authors":["Cozma, M{\\u{a}}d{\\u{a}}lina","Butnaru, Andrei","Ionescu, Radu Tudor"],"emails":["","butnaruandreimadalin@gmail.com","raducu.ionescu@gmail.com"],"author_id":["madalina-cozma","andrei-butnaru","radu-tudor-ionescu"],"abstract":"In this work, we present an approach based on combining string kernels and word embeddings for automatic essay scoring. String kernels capture the similarity among strings based on counting common character n-grams, which are a low-level yet powerful type of feature, demonstrating state-of-the-art results in various text classification tasks such as Arabic dialect identification or native language identification. To our best knowledge, we are the first to apply string kernels to automatically score essays. We are also the first to combine them with a high-level semantic feature representation, namely the bag-of-super-word-embeddings. We report the best performance on the Automated Student Assessment Prize data set, in both in-domain and cross-domain settings, surpassing recent state-of-the-art deep learning approaches.","pages":"503--509","doi":"10.18653\/v1\/P18-2080","url":"https:\/\/www.aclweb.org\/anthology\/P18-2080","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2081","title":"Party Matters: Enhancing Legislative Embeddings with Author Attributes for Vote Prediction","authors":["Kornilova, Anastassia","Argyle, Daniel","Eidelman, Vladimir"],"emails":["anastassia.kornilova@fiscalnote.com","daniel@fiscalnote.com","vlad@fiscalnote.com"],"author_id":["anastassia-kornilova","daniel-argyle","vladimir-eidelman"],"abstract":"Predicting how Congressional legislators will vote is important for understanding their past and future behavior. However, previous work on roll-call prediction has been limited to single session settings, thus not allowing for generalization across sessions. In this paper, we show that text alone is insufficient for modeling voting outcomes in new contexts, as session changes lead to changes in the underlying data generation process. We propose a novel neural method for encoding documents alongside additional metadata, achieving an average of a 4{\\%} boost in accuracy over the previous state-of-the-art.","pages":"510--515","doi":"10.18653\/v1\/P18-2081","url":"https:\/\/www.aclweb.org\/anthology\/P18-2081","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2082","title":"Dynamic and Static Topic Model for Analyzing Time-Series Document Collections","authors":["Hida, Rem","Takeishi, Naoya","Yairi, Takehisa","Hori, Koichi"],"emails":["hida@ailab.t.u-tokyo.ac.jp","naoya.takeishi@riken.jp","yairi@ailab.t.u-tokyo.ac.jp","hori@ailab.t.u-tokyo.ac.jp"],"author_id":["rem-hida","naoya-takeishi","takehisa-yairi","koichi-hori"],"abstract":"For extracting meaningful topics from texts, their structures should be considered properly. In this paper, we aim to analyze structured time-series documents such as a collection of news articles and a series of scientific papers, wherein topics evolve along time depending on multiple topics in the past and are also related to each other at each time. To this end, we propose a dynamic and static topic model, which simultaneously considers the dynamic structures of the temporal topic evolution and the static structures of the topic hierarchy at each time. We show the results of experiments on collections of scientific papers, in which the proposed method outperformed conventional models. Moreover, we show an example of extracted topic structures, which we found helpful for analyzing research activities.","pages":"516--520","doi":"10.18653\/v1\/P18-2082","url":"https:\/\/www.aclweb.org\/anthology\/P18-2082","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2083","title":"{P}hrase{CTM}: Correlated Topic Modeling on Phrases within {M}arkov Random Fields","authors":["Huang, Weijing"],"emails":["huangweijing@pku.edu.cn"],"author_id":["weijing-huang"],"abstract":"Recent emerged phrase-level topic models are able to provide topics of phrases, which are easy to read for humans. But these models are lack of the ability to capture the correlation structure among the discovered numerous topics. We propose a novel topic model PhraseCTM and a two-stage method to find out the correlated topics at phrase level. In the first stage, we train PhraseCTM, which models the generation of words and phrases simultaneously by linking the phrases and component words within Markov Random Fields when they are semantically coherent. In the second stage, we generate the correlation of topics from PhraseCTM. We evaluate our method by a quantitative experiment and a human study, showing the correlated topic modeling on phrases is a good and practical way to interpret the underlying themes of a corpus.","pages":"521--526","doi":"10.18653\/v1\/P18-2083","url":"https:\/\/www.aclweb.org\/anthology\/P18-2083","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2084","title":"A Document Descriptor using Covariance of Word Vectors","authors":["Torki, Marwan"],"emails":["mtorki@alexu.edu.eg"],"author_id":["marwan-torki"],"abstract":"In this paper, we address the problem of finding a novel document descriptor based on the covariance matrix of the word vectors of a document. Our descriptor has a fixed length, which makes it easy to use in many supervised and unsupervised applications. We tested our novel descriptor in different tasks including supervised and unsupervised settings. Our evaluation shows that our document covariance descriptor fits different tasks with competitive performance against state-of-the-art methods.","pages":"527--532","doi":"10.18653\/v1\/P18-2084","url":"https:\/\/www.aclweb.org\/anthology\/P18-2084","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2085","title":"Learning with Structured Representations for Negation Scope Extraction","authors":["Li, Hao","Lu, Wei"],"emails":["li@mymail.sutd.edu.sg","luwei@sutd.edu.sg"],"author_id":["hao-li","wei-lu"],"abstract":"We report an empirical study on the task of negation scope extraction given the negation cue. Our key observation is that certain useful information such as features related to negation cue, long-distance dependencies as well as some latent structural information can be exploited for such a task. We design approaches based on conditional random fields (CRF), semi-Markov CRF, as well as latent-variable CRF models to capture such information. Extensive experiments on several standard datasets demonstrate that our approaches are able to achieve better results than existing approaches reported in the literature.","pages":"533--539","doi":"10.18653\/v1\/P18-2085","url":"https:\/\/www.aclweb.org\/anthology\/P18-2085","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2086","title":"End-Task Oriented Textual Entailment via Deep Explorations of Inter-Sentence Interactions","authors":["Yin, Wenpeng","Roth, Dan","Sch{\\\"u}tze, Hinrich"],"emails":["wenpeng@seas.upenn.edu","danroth@seas.upenn.edu",""],"author_id":["wenpeng-yin","dan-roth","hinrich-schutze"],"abstract":"This work deals with SciTail, a natural entailment challenge derived from a multi-choice question answering problem. The premises and hypotheses in SciTail were generated with no awareness of each other, and did not specifically aim at the entailment task. This makes it more challenging than other entailment data sets and more directly useful to the end-task {--} question answering. We propose DEISTE (deep explorations of inter-sentence interactions for textual entailment) for this entailment task. Given word-to-word interactions between the premise-hypothesis pair (P, H), DEISTE consists of: (i) a parameter-dynamic convolution to make important words in P and H play a dominant role in learnt representations; and (ii) a position-aware attentive convolution to encode the representation and position information of the aligned word pairs. Experiments show that DEISTE gets {\\mbox{$\\approx$}}5{\\%} improvement over prior state of the art and that the pretrained DEISTE on SciTail generalizes well on RTE-5.","pages":"540--545","doi":"10.18653\/v1\/P18-2086","url":"https:\/\/www.aclweb.org\/anthology\/P18-2086","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2087","title":"Sense-Aware Neural Models for Pun Location in Texts","authors":["Cai, Yitao","Li, Yin","Wan, Xiaojun"],"emails":["caiyitao@pku.edu.cn","xyz1305121@pku.edu.cn","wanxiaojun@pku.edu.cn"],"author_id":["yitao-cai","yin-li","xiaojun-wan"],"abstract":"A homographic pun is a form of wordplay in which one signifier (usually a word) suggests two or more meanings by exploiting polysemy for an intended humorous or rhetorical effect. In this paper, we focus on the task of pun location, which aims to identify the pun word in a given short text. We propose a sense-aware neural model to address this challenging task. Our model first obtains several WSD results for the text, and then leverages a bidirectional LSTM network to model each sequence of word senses. The outputs at each time step for different LSTM networks are then concatenated for prediction. Evaluation results on the benchmark SemEval 2017 dataset demonstrate the efficacy of our proposed model.","pages":"546--551","doi":"10.18653\/v1\/P18-2087","url":"https:\/\/www.aclweb.org\/anthology\/P18-2087","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2088","title":"A Rank-Based Similarity Metric for Word Embeddings","authors":["Santus, Enrico","Wang, Hongmin","Chersoni, Emmanuele","Zhang, Yue"],"emails":["esantus@mit.edu","wang@cs.ucsb.edu","emmanuelechersoni@gmail.com","zhang@sutd.edu.sg"],"author_id":["enrico-santus","hongmin-wang","emmanuele-chersoni","yue-zhang"],"abstract":"Word Embeddings have recently imposed themselves as a standard for representing word meaning in NLP. Semantic similarity between word pairs has become the most common evaluation benchmark for these representations, with vector cosine being typically used as the only similarity metric. In this paper, we report experiments with a rank-based metric for WE, which performs comparably to vector cosine in similarity estimation and outperforms it in the recently-introduced and challenging task of outlier detection, thus suggesting that rank-based measures can improve clustering quality.","pages":"552--557","doi":"10.18653\/v1\/P18-2088","url":"https:\/\/www.aclweb.org\/anthology\/P18-2088","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2089","title":"Addressing Noise in Multidialectal Word Embeddings","authors":["Erdmann, Alexander","Zalmout, Nasser","Habash, Nizar"],"emails":["ae1541@nyu.edu","nasser.zalmout@nyu.edu","nizar.habash@nyu.edu"],"author_id":["alexander-erdmann","nasser-zalmout","nizar-habash"],"abstract":"Word embeddings are crucial to many natural language processing tasks. The quality of embeddings relies on large non-noisy corpora. Arabic dialects lack large corpora and are noisy, being linguistically disparate with no standardized spelling. We make three contributions to address this noise. First, we describe simple but effective adaptations to word embedding tools to maximize the informative content leveraged in each training sentence. Second, we analyze methods for representing disparate dialects in one embedding space, either by mapping individual dialects into a shared space or learning a joint model of all dialects. Finally, we evaluate via dictionary induction, showing that two metrics not typically reported in the task enable us to analyze our contributions{'} effects on low and high frequency words. In addition to boosting performance between 2-53{\\%}, we specifically improve on noisy, low frequency forms without compromising accuracy on high frequency forms.","pages":"558--565","doi":"10.18653\/v1\/P18-2089","url":"https:\/\/www.aclweb.org\/anthology\/P18-2089","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2090","title":"{GNEG}: Graph-Based Negative Sampling for word2vec","authors":["Zhang, Zheng","Zweigenbaum, Pierre"],"emails":["zheng.zhang@limsi.fr","pz@limsi.fr"],"author_id":["zheng-zhang","pierre-zweigenbaum"],"abstract":"Negative sampling is an important component in word2vec for distributed word representation learning. We hypothesize that taking into account global, corpus-level information and generating a different noise distribution for each target word better satisfies the requirements of negative examples for each training word than the original frequency-based distribution. In this purpose we pre-compute word co-occurrence statistics from the corpus and apply to it network algorithms such as random walk. We test this hypothesis through a set of experiments whose results show that our approach boosts the word analogy task by about 5{\\%} and improves the performance on word similarity tasks by about 1{\\%} compared to the skip-gram negative sampling baseline.","pages":"566--571","doi":"10.18653\/v1\/P18-2090","url":"https:\/\/www.aclweb.org\/anthology\/P18-2090","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2091","title":"Unsupervised Learning of Style-sensitive Word Vectors","authors":["Akama, Reina","Watanabe, Kento","Yokoi, Sho","Kobayashi, Sosuke","Inui, Kentaro"],"emails":["1reina.a@ecei.tohoku.ac.jp","2kento.watanabe@aist.go.jp","3yokoi@ecei.tohoku.ac.jp","4sosk@preferred.jp","5inui@ecei.tohoku.ac.jp"],"author_id":["reina-akama","kento-watanabe","sho-yokoi","sosuke-kobayashi","kentaro-inui"],"abstract":"This paper presents the first study aimed at capturing stylistic similarity between words in an unsupervised manner. We propose extending the continuous bag of words (CBOW) embedding model (Mikolov et al., 2013b) to learn style-sensitive word vectors using a wider context window under the assumption that the style of all the words in an utterance is consistent. In addition, we introduce a novel task to predict lexical stylistic similarity and to create a benchmark dataset for this task. Our experiment with this dataset supports our assumption and demonstrates that the proposed extensions contribute to the acquisition of style-sensitive word embeddings.","pages":"572--578","doi":"10.18653\/v1\/P18-2091","url":"https:\/\/www.aclweb.org\/anthology\/P18-2091","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2092","title":"Exploiting Document Knowledge for Aspect-level Sentiment Classification","authors":["He, Ruidan","Lee, Wee Sun","Ng, Hwee Tou","Dahlmeier, Daniel"],"emails":["ruidanhe@comp.nus.edu.sg","leews@comp.nus.edu.sg","nght@comp.nus.edu.sg","d.dahlmeier@sap.com"],"author_id":["ruidan-he","wee-sun-lee","hwee-tou-ng","daniel-dahlmeier"],"abstract":"Attention-based long short-term memory (LSTM) networks have proven to be useful in aspect-level sentiment classification. However, due to the difficulties in annotating aspect-level data, existing public datasets for this task are all relatively small, which largely limits the effectiveness of those neural models. In this paper, we explore two approaches that transfer knowledge from document-level data, which is much less expensive to obtain, to improve the performance of aspect-level sentiment classification. We demonstrate the effectiveness of our approaches on 4 public datasets from SemEval 2014, 2015, and 2016, and we show that attention-based LSTM benefits from document-level knowledge in multiple ways.","pages":"579--585","doi":"10.18653\/v1\/P18-2092","url":"https:\/\/www.aclweb.org\/anthology\/P18-2092","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2093","title":"Modeling Sentiment Association in Discourse for Humor Recognition","authors":["Liu, Lizhen","Zhang, Donghai","Song, Wei"],"emails":["liu7480@cnu.edu.cn","dhzhang@cnu.edu.cn","wsong@cnu.edu.cn"],"author_id":["lizhen-liu","donghai-zhang","wei-song"],"abstract":"Humor is one of the most attractive parts in human communication. However, automatically recognizing humor in text is challenging due to the complex characteristics of humor. This paper proposes to model sentiment association between discourse units to indicate how the punchline breaks the expectation of the setup. We found that discourse relation, sentiment conflict and sentiment transition are effective indicators for humor recognition. On the perspective of using sentiment related features, sentiment association in discourse is more useful than counting the number of emotional words.","pages":"586--591","doi":"10.18653\/v1\/P18-2093","url":"https:\/\/www.aclweb.org\/anthology\/P18-2093","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2094","title":"Double Embeddings and {CNN}-based Sequence Labeling for Aspect Extraction","authors":["Xu, Hu","Liu, Bing","Shu, Lei","Yu, Philip S."],"emails":["hxu48@uic.edu","liub@uic.edu","lshu3@uic.edu","psyu@uic.edu"],"author_id":["hu-xu","bing-liu","lei-shu","philip-s-yu"],"abstract":"One key task of fine-grained sentiment analysis of product reviews is to extract product aspects or features that users have expressed opinions on. This paper focuses on supervised aspect extraction using deep learning. Unlike other highly sophisticated supervised deep learning models, this paper proposes a novel and yet simple CNN model employing two types of pre-trained embeddings for aspect extraction: general-purpose embeddings and domain-specific embeddings. Without using any additional supervision, this model achieves surprisingly good results, outperforming state-of-the-art sophisticated existing methods. To our knowledge, this paper is the first to report such double embeddings based CNN model for aspect extraction and achieve very good results.","pages":"592--598","doi":"10.18653\/v1\/P18-2094","url":"https:\/\/www.aclweb.org\/anthology\/P18-2094","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2095","title":"Will it Blend? Blending Weak and Strong Labeled Data in a Neural Network for Argumentation Mining","authors":["Shnarch, Eyal","Alzate, Carlos","Dankin, Lena","Gleize, Martin","Hou, Yufang","Choshen, Leshem","Aharonov, Ranit","Slonim, Noam"],"emails":["eyals@il.ibm.com","carlos.alzate@ie.ibm.com","lenad@il.ibm.com","martin.gleize@ie.ibm.com","yhou@ie.ibm.com","leshem.choshen@il.ibm.com","ranita@il.ibm.com","noams@il.ibm.com"],"author_id":["eyal-shnarch","carlos-alzate","lena-dankin","martin-gleize","yufang-hou","leshem-choshen","ranit-aharonov","noam-slonim"],"abstract":"The process of obtaining high quality labeled data for natural language understanding tasks is often slow, error-prone, complicated and expensive. With the vast usage of neural networks, this issue becomes more notorious since these networks require a large amount of labeled data to produce satisfactory results. We propose a methodology to blend high quality but scarce strong labeled data with noisy but abundant weak labeled data during the training of neural networks. Experiments in the context of topic-dependent evidence detection with two forms of weak labeled data show the advantages of the blending scheme. In addition, we provide a manually annotated data set for the task of topic-dependent evidence detection. We believe that blending weak and strong labeled data is a general notion that may be applicable to many language understanding tasks, and can especially assist researchers who wish to train a network but have a small amount of high quality labeled data for their task of interest.","pages":"599--605","doi":"10.18653\/v1\/P18-2095","url":"https:\/\/www.aclweb.org\/anthology\/P18-2095","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2096","title":"Investigating Audio, Video, and Text Fusion Methods for End-to-End Automatic Personality Prediction","authors":["Kampman, Onno","J. Barezi, Elham","Bertero, Dario","Fung, Pascale"],"emails":["","ejs@connect.ust.hk","dbertero@connect.ust.hk","pascale@ece.ust.hk"],"author_id":["onno-kampman","elham-j-barezi1","dario-bertero","pascale-fung"],"abstract":"We propose a tri-modal architecture to predict Big Five personality trait scores from video clips with different channels for audio, text, and video data. For each channel, stacked Convolutional Neural Networks are employed. The channels are fused both on decision-level and by concatenating their respective fully connected layers. It is shown that a multimodal fusion approach outperforms each single modality channel, with an improvement of 9.4{\\%} over the best individual modality (video). Full backpropagation is also shown to be better than a linear combination of modalities, meaning complex interactions between modalities can be leveraged to build better models. Furthermore, we can see the prediction relevance of each modality for each trait. The described model can be used to increase the emotional intelligence of virtual agents.","pages":"606--611","doi":"10.18653\/v1\/P18-2096","url":"https:\/\/www.aclweb.org\/anthology\/P18-2096","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2097","title":"An Empirical Study of Building a Strong Baseline for Constituency Parsing","authors":["Suzuki, Jun","Takase, Sho","Kamigaito, Hidetaka","Morishita, Makoto","Nagata, Masaaki"],"emails":["suzuki.jun@lab.ntt.co.jp","takase.sho@lab.ntt.co.jp","kamigaito.hidetaka@lab.ntt.co.jp","morishita.makoto@lab.ntt.co.jp","nagata.masaaki@lab.ntt.co.jp"],"author_id":["jun-suzuki","sho-takase","hidetaka-kamigaito","makoto-morishita","masaaki-nagata"],"abstract":"This paper investigates the construction of a strong baseline based on general purpose sequence-to-sequence models for constituency parsing. We incorporate several techniques that were mainly developed in natural language generation tasks, e.g., machine translation and summarization, and demonstrate that the sequence-to-sequence model achieves the current top-notch parsers{'} performance (almost) without requiring any explicit task-specific knowledge or architecture of constituent parsing.","pages":"612--618","doi":"10.18653\/v1\/P18-2097","url":"https:\/\/www.aclweb.org\/anthology\/P18-2097","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2098","title":"Parser Training with Heterogeneous Treebanks","authors":["Stymne, Sara","de Lhoneux, Miryam","Smith, Aaron","Nivre, Joakim"],"emails":["","ame@lingfil.uu.se","",""],"author_id":["sara-stymne","miryam-de-lhoneux","aaron-smith","joakim-nivre"],"abstract":"How to make the most of multiple heterogeneous treebanks when training a monolingual dependency parser is an open question. We start by investigating previously suggested, but little evaluated, strategies for exploiting multiple treebanks based on concatenating training sets, with or without fine-tuning. We go on to propose a new method based on treebank embeddings. We perform experiments for several languages and show that in many cases fine-tuning and treebank embeddings lead to substantial improvements over single treebanks or concatenation, with average gains of 2.0{--}3.5 LAS points. We argue that treebank embeddings should be preferred due to their conceptual simplicity, flexibility and extensibility.","pages":"619--625","doi":"10.18653\/v1\/P18-2098","url":"https:\/\/www.aclweb.org\/anthology\/P18-2098","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2099","title":"Generalized chart constraints for efficient {PCFG} and {TAG} parsing","authors":["Gr{\\\"u}newald, Stefan","Henning, Sophie","Koller, Alexander"],"emails":["stefang@coli.uni-saarland.de","shenning@coli.uni-saarland.de","koller@coli.uni-saarland.de"],"author_id":["stefan-grunewald","sophie-henning","alexander-koller"],"abstract":"Chart constraints, which specify at which string positions a constituent may begin or end, have been shown to speed up chart parsers for PCFGs. We generalize chart constraints to more expressive grammar formalisms and describe a neural tagger which predicts chart constraints at very high precision. Our constraints accelerate both PCFG and TAG parsing, and combine effectively with other pruning techniques (coarse-to-fine and supertagging) for an overall speedup of two orders of magnitude, while improving accuracy.","pages":"626--631","doi":"10.18653\/v1\/P18-2099","url":"https:\/\/www.aclweb.org\/anthology\/P18-2099","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2100","title":"Exploring Semantic Properties of Sentence Embeddings","authors":["Zhu, Xunjie","Li, Tingfeng","de Melo, Gerard"],"emails":["xunjie.zhu@rutgers.edu","ltf@mail.nwpu.edu.cn","gdm@demelo.org"],"author_id":["xunjie-zhu","tingfeng-li","gerard-de-melo"],"abstract":"Neural vector representations are ubiquitous throughout all subfields of NLP. While word vectors have been studied in much detail, thus far only little light has been shed on the properties of sentence embeddings. In this paper, we assess to what extent prominent sentence embedding methods exhibit select semantic properties. We propose a framework that generate triplets of sentences to explore how changes in the syntactic structure or semantics of a given sentence affect the similarities obtained between their sentence embeddings.","pages":"632--637","doi":"10.18653\/v1\/P18-2100","url":"https:\/\/www.aclweb.org\/anthology\/P18-2100","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2101","title":"Scoring Lexical Entailment with a Supervised Directional Similarity Network","authors":["Rei, Marek","Gerz, Daniela","Vuli{\\'c}, Ivan"],"emails":["marek.rei@cl.cam.ac.uk","dsg40@cam.ac.uk","iv250@cam.ac.uk"],"author_id":["marek-rei","daniela-gerz","ivan-vulic"],"abstract":"We present the Supervised Directional Similarity Network, a novel neural architecture for learning task-specific transformation functions on top of general-purpose word embeddings. Relying on only a limited amount of supervision from task-specific scores on a subset of the vocabulary, our architecture is able to generalise and transform a general-purpose distributional vector space to model the relation of lexical entailment. Experiments show excellent performance on scoring graded lexical entailment, raising the state-of-the-art on the HyperLex dataset by approximately 25{\\%}.","pages":"638--643","doi":"10.18653\/v1\/P18-2101","url":"https:\/\/www.aclweb.org\/anthology\/P18-2101","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2102","title":"Extracting Commonsense Properties from Embeddings with Limited Human Guidance","authors":["Yang, Yiben","Birnbaum, Larry","Wang, Ji-Ping","Downey, Doug"],"emails":["yiben.yang@northwestern.edu","l-birnbaum@northwestern.edu","jzwang@northwestern.edu","d-downey@northwestern.edu"],"author_id":["yiben-yang","larry-birnbaum","ji-ping-wang","doug-downey"],"abstract":"Intelligent systems require common sense, but automatically extracting this knowledge from text can be difficult. We propose and assess methods for extracting one type of commonsense knowledge, object-property comparisons, from pre-trained embeddings. In experiments, we show that our approach exceeds the accuracy of previous work but requires substantially less hand-annotated knowledge. Further, we show that an active learning approach that synthesizes common-sense queries can boost accuracy.","pages":"644--649","doi":"10.18653\/v1\/P18-2102","url":"https:\/\/www.aclweb.org\/anthology\/P18-2102","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2103","title":"Breaking {NLI} Systems with Sentences that Require Simple Lexical Inferences","authors":["Glockner, Max","Shwartz, Vered","Goldberg, Yoav"],"emails":["maxg216@gmail.com","vered1986@gmail.com","yoav.goldberg@gmail.com"],"author_id":["max-glockner","vered-shwartz","yoav-goldberg"],"abstract":"We create a new NLI test set that shows the deficiency of state-of-the-art models in inferences that require lexical and world knowledge. The new examples are simpler than the SNLI test set, containing sentences that differ by at most one word from sentences in the training set. Yet, the performance on the new test set is substantially worse across systems trained on SNLI, demonstrating that these systems are limited in their generalization ability, failing to capture many simple inferences.","pages":"650--655","doi":"10.18653\/v1\/P18-2103","url":"https:\/\/www.aclweb.org\/anthology\/P18-2103","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2104","title":"Adaptive Knowledge Sharing in Multi-Task Learning: Improving Low-Resource Neural Machine Translation","authors":["Zaremoodi, Poorya","Buntine, Wray","Haffari, Gholamreza"],"emails":["poorya.zaremoodi@monash.edu","wray.buntine@monash.edu","gholamreza.haffari@monash.edu"],"author_id":["poorya-zaremoodi","wray-buntine","gholamreza-haffari"],"abstract":"Neural Machine Translation (NMT) is notorious for its need for large amounts of bilingual data. An effective approach to compensate for this requirement is Multi-Task Learning (MTL) to leverage different linguistic resources as a source of inductive bias. Current MTL architectures are based on the Seq2Seq transduction, and (partially) share different components of the models among the tasks. However, this MTL approach often suffers from task interference and is not able to fully capture commonalities among subsets of tasks. We address this issue by extending the recurrent units with multiple {``}blocks{''} along with a trainable {``}routing network{''}. The routing network enables adaptive collaboration by dynamic sharing of blocks conditioned on the task at hand, input, and model state. Empirical evaluation of two low-resource translation tasks, English to Vietnamese and Farsi, show +1 BLEU score improvements compared to strong baselines.","pages":"656--661","doi":"10.18653\/v1\/P18-2104","url":"https:\/\/www.aclweb.org\/anthology\/P18-2104","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2105","title":"Automatic Estimation of Simultaneous Interpreter Performance","authors":["Stewart, Craig","Vogler, Nikolai","Hu, Junjie","Boyd-Graber, Jordan","Neubig, Graham"],"emails":["","","","",""],"author_id":["craig-stewart","nikolai-vogler","junjie-hu","jordan-boyd-graber","graham-neubig"],"abstract":"Simultaneous interpretation, translation of the spoken word in real-time, is both highly challenging and physically demanding. Methods to predict interpreter confidence and the adequacy of the interpreted message have a number of potential applications, such as in computer-assisted interpretation interfaces or pedagogical tools. We propose the task of predicting simultaneous interpreter performance by building on existing methodology for quality estimation (QE) of machine translation output. In experiments over five settings in three language pairs, we extend a QE pipeline to estimate interpreter performance (as approximated by the METEOR evaluation metric) and propose novel features reflecting interpretation strategy and evaluation measures that further improve prediction accuracy.","pages":"662--666","doi":"10.18653\/v1\/P18-2105","url":"https:\/\/www.aclweb.org\/anthology\/P18-2105","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2106","title":"Polyglot Semantic Role Labeling","authors":["Mulcaire, Phoebe","Swayamdipta, Swabha","Smith, Noah A."],"emails":["pmulc@cs.washington.edu","swabha@cs.cmu.edu","nasmith@cs.washington.edu"],"author_id":["phoebe-mulcaire","swabha-swayamdipta","noah-a-smith"],"abstract":"Previous approaches to multilingual semantic dependency parsing treat languages independently, without exploiting the similarities between semantic structures across languages. We experiment with a new approach where we combine resources from different languages in the CoNLL 2009 shared task to build a single polyglot semantic dependency parser. Notwithstanding the absence of parallel data, and the dissimilarity in annotations between languages, our approach results in improvement in parsing performance on several languages over a monolingual baseline. Analysis of the polyglot models{'} performance provides a new understanding of the similarities and differences between languages in the shared task.","pages":"667--672","doi":"10.18653\/v1\/P18-2106","url":"https:\/\/www.aclweb.org\/anthology\/P18-2106","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2107","title":"Learning Cross-lingual Distributed Logical Representations for Semantic Parsing","authors":["Zou, Yanyan","Lu, Wei"],"emails":["zou@mymail.sutd.edu.sg","luwei@sutd.edu.sg"],"author_id":["yanyan-zou","wei-lu"],"abstract":"With the development of several multilingual datasets used for semantic parsing, recent research efforts have looked into the problem of learning semantic parsers in a multilingual setup. However, how to improve the performance of a monolingual semantic parser for a specific language by leveraging data annotated in different languages remains a research question that is under-explored. In this work, we present a study to show how learning distributed representations of the logical forms from data annotated in different languages can be used for improving the performance of a monolingual semantic parser. We extend two existing monolingual semantic parsers to incorporate such cross-lingual distributed logical representations as features. Experiments show that our proposed approach is able to yield improved semantic parsing results on the standard multilingual GeoQuery dataset.","pages":"673--679","doi":"10.18653\/v1\/P18-2107","url":"https:\/\/www.aclweb.org\/anthology\/P18-2107","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2108","title":"Enhancing Drug-Drug Interaction Extraction from Texts by Molecular Structure Information","authors":["Asada, Masaki","Miwa, Makoto","Sasaki, Yutaka"],"emails":["sd17402@toyota-ti.ac.jp","makoto-miwa@toyota-ti.ac.jp","yutaka.sasaki@toyota-ti.ac.jp"],"author_id":["masaki-asada","makoto-miwa","yutaka-sasaki"],"abstract":"We propose a novel neural method to extract drug-drug interactions (DDIs) from texts using external drug molecular structure information. We encode textual drug pairs with convolutional neural networks and their molecular pairs with graph convolutional networks (GCNs), and then we concatenate the outputs of these two networks. In the experiments, we show that GCNs can predict DDIs from the molecular structures of drugs in high accuracy and the molecular information can enhance text-based DDI extraction by 2.39 percent points in the F-score on the DDIExtraction 2013 shared task data set.","pages":"680--685","doi":"10.18653\/v1\/P18-2108","url":"https:\/\/www.aclweb.org\/anthology\/P18-2108","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2109","title":"dia{NED}: Time-Aware Named Entity Disambiguation for Diachronic Corpora","authors":["Agarwal, Prabal","Str{\\\"o}tgen, Jannik","del Corro, Luciano","Hoffart, Johannes","Weikum, Gerhard"],"emails":["pagarwal@mpi-inf.mpg.de","jannik.stroetgen@de.bosch.com","luciano@ambiverse.com","johannes@ambiverse.com","weikum@mpi-inf.mpg.de"],"author_id":["prabal-agarwal","jannik-strotgen","luciano-del-corro1","johannes-hoffart","gerhard-weikum"],"abstract":"Named Entity Disambiguation (NED) systems perform well on news articles and other texts covering a specific time interval. However, NED quality drops when inputs span long time periods like in archives or historic corpora. This paper presents the first time-aware method for NED that resolves ambiguities even when mention contexts give only few cues. The method is based on computing temporal signatures for entities and comparing these to the temporal contexts of input mentions. Our experiments show superior quality on a newly created diachronic corpus.","pages":"686--693","doi":"10.18653\/v1\/P18-2109","url":"https:\/\/www.aclweb.org\/anthology\/P18-2109","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2110","title":"Examining Temporality in Document Classification","authors":["Huang, Xiaolei","Paul, Michael J."],"emails":["xiaolei.huang@colorado.edu","mpaul@colorado.edu"],"author_id":["xiaolei-huang","michael-paul"],"abstract":"Many corpora span broad periods of time. Language processing models trained during one time period may not work well in future time periods, and the best model may depend on specific times of year (e.g., people might describe hotels differently in reviews during the winter versus the summer). This study investigates how document classifiers trained on documents from certain time intervals perform on documents from other time intervals, considering both seasonal intervals (intervals that repeat across years, e.g., winter) and non-seasonal intervals (e.g., specific years). We show experimentally that classification performance varies over time, and that performance can be improved by using a standard domain adaptation approach to adjust for changes in time.","pages":"694--699","doi":"10.18653\/v1\/P18-2110","url":"https:\/\/www.aclweb.org\/anthology\/P18-2110","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2111","title":"Personalized Language Model for Query Auto-Completion","authors":["Jaech, Aaron","Ostendorf, Mari"],"emails":["ajaech@uw.edu","ostendor@uw.edu"],"author_id":["aaron-jaech","mari-ostendorf"],"abstract":"Query auto-completion is a search engine feature whereby the system suggests completed queries as the user types. Recently, the use of a recurrent neural network language model was suggested as a method of generating query completions. We show how an adaptable language model can be used to generate personalized completions and how the model can use online updating to make predictions for users not seen during training. The personalized predictions are significantly better than a baseline that uses no user information.","pages":"700--705","doi":"10.18653\/v1\/P18-2111","url":"https:\/\/www.aclweb.org\/anthology\/P18-2111","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2112","title":"Personalized Review Generation By Expanding Phrases and Attending on Aspect-Aware Representations","authors":["Ni, Jianmo","McAuley, Julian"],"emails":["jin018@ucsd.edu","jmcauley@ucsd.edu"],"author_id":["jianmo-ni","julian-mcauley"],"abstract":"In this paper, we focus on the problem of building assistive systems that can help users to write reviews. We cast this problem using an encoder-decoder framework that generates personalized reviews by expanding short phrases (e.g. review summaries, product titles) provided as input to the system. We incorporate aspect-level information via an aspect encoder that learns aspect-aware user and item representations. An attention fusion layer is applied to control generation by attending on the outputs of multiple encoders. Experimental results show that our model successfully learns representations capable of generating coherent and diverse reviews. In addition, the learned aspect-aware representations discover those aspects that users are more inclined to discuss and bias the generated text toward their personalized aspect preferences.","pages":"706--711","doi":"10.18653\/v1\/P18-2112","url":"https:\/\/www.aclweb.org\/anthology\/P18-2112","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2113","title":"Learning Simplifications for Specific Target Audiences","authors":["Scarton, Carolina","Specia, Lucia"],"emails":["c.scarton@sheffield.ac.uk","l.specia@sheffield.ac.uk"],"author_id":["carolina-scarton","lucia-specia"],"abstract":"Text simplification (TS) is a monolingual text-to-text transformation task where an original (complex) text is transformed into a target (simpler) text. Most recent work is based on sequence-to-sequence neural models similar to those used for machine translation (MT). Different from MT, TS data comprises more elaborate transformations, such as sentence splitting. It can also contain multiple simplifications of the same original text targeting different audiences, such as school grade levels. We explore these two features of TS to build models tailored for specific grade levels. Our approach uses a standard sequence-to-sequence architecture where the original sequence is annotated with information about the target audience and\/or the (predicted) type of simplification operation. We show that it outperforms state-of-the-art TS approaches (up to 3 and 12 BLEU and SARI points, respectively), including when training data for the specific complex-simple combination of grade levels is not available, i.e. zero-shot learning.","pages":"712--718","doi":"10.18653\/v1\/P18-2113","url":"https:\/\/www.aclweb.org\/anthology\/P18-2113","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2114","title":"Split and Rephrase: Better Evaluation and Stronger Baselines","authors":["Aharoni, Roee","Goldberg, Yoav"],"emails":["roee.aharoni@gmail.com","yoav.goldberg@gmail.com"],"author_id":["roee-aharoni","yoav-goldberg"],"abstract":"Splitting and rephrasing a complex sentence into several shorter sentences that convey the same meaning is a challenging problem in NLP. We show that while vanilla seq2seq models can reach high scores on the proposed benchmark (Narayan et al., 2017), they suffer from memorization of the training set which contains more than 89{\\%} of the unique simple sentences from the validation and test sets. To aid this, we present a new train-development-test data split and neural models augmented with a copy-mechanism, outperforming the best reported baseline by 8.68 BLEU and fostering further progress on the task.","pages":"719--724","doi":"10.18653\/v1\/P18-2114","url":"https:\/\/www.aclweb.org\/anthology\/P18-2114","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2115","title":"Autoencoder as Assistant Supervisor: Improving Text Representation for {C}hinese Social Media Text Summarization","authors":["Ma, Shuming","Sun, Xu","Lin, Junyang","Wang, Houfeng"],"emails":["shumingma@pku.edu.cn","xusun@pku.edu.cn","linjunyang@pku.edu.cn","wanghf@pku.edu.cn"],"author_id":["shuming-ma","xu-sun","junyang-lin","houfeng-wang"],"abstract":"Most of the current abstractive text summarization models are based on the sequence-to-sequence model (Seq2Seq). The source content of social media is long and noisy, so it is difficult for Seq2Seq to learn an accurate semantic representation. Compared with the source content, the annotated summary is short and well written. Moreover, it shares the same meaning as the source content. In this work, we supervise the learning of the representation of the source content with that of the summary. In implementation, we regard a summary autoencoder as an assistant supervisor of Seq2Seq. Following previous work, we evaluate our model on a popular Chinese social media dataset. Experimental results show that our model achieves the state-of-the-art performances on the benchmark dataset.","pages":"725--731","doi":"10.18653\/v1\/P18-2115","url":"https:\/\/www.aclweb.org\/anthology\/P18-2115","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2116","title":"Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum","authors":["Levy, Omer","Lee, Kenton","FitzGerald, Nicholas","Zettlemoyer, Luke"],"emails":["omerlevy@cs.washington.edu","kentonl@cs.washington.edu","nfitz@cs.washington.edu","lsz@cs.washington.edu"],"author_id":["omer-levy","kenton-lee","nicholas-fitzgerald","luke-zettlemoyer"],"abstract":"LSTMs were introduced to combat vanishing gradients in simple RNNs by augmenting them with gated additive recurrent connections. We present an alternative view to explain the success of LSTMs: the gates themselves are versatile recurrent models that provide more representational power than previously appreciated. We do this by decoupling the LSTM{'}s gates from the embedded simple RNN, producing a new class of RNNs where the recurrence computes an element-wise weighted sum of context-independent functions of the input. Ablations on a range of problems demonstrate that the gating mechanism alone performs as well as an LSTM in most settings, strongly suggesting that the gates are doing much more in practice than just alleviating vanishing gradients.","pages":"732--739","doi":"10.18653\/v1\/P18-2116","url":"https:\/\/www.aclweb.org\/anthology\/P18-2116","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2117","title":"On the Practical Computational Power of Finite Precision {RNN}s for Language Recognition","authors":["Weiss, Gail","Goldberg, Yoav","Yahav, Eran"],"emails":["sgailw@cs.technion.ac.il","yogo@cs.biu.ac.il","yahave@cs.technion.ac.il"],"author_id":["gail-weiss","yoav-goldberg","eran-yahav"],"abstract":"While Recurrent Neural Networks (RNNs) are famously known to be Turing complete, this relies on infinite precision in the states and unbounded computation time. We consider the case of RNNs with finite precision whose computation time is linear in the input length. Under these limitations, we show that different RNN variants have different computational power. In particular, we show that the LSTM and the Elman-RNN with ReLU activation are strictly stronger than the RNN with a squashing activation and the GRU. This is achieved because LSTMs and ReLU-RNNs can easily implement counting behavior. We show empirically that the LSTM does indeed learn to effectively use the counting mechanism.","pages":"740--745","doi":"10.18653\/v1\/P18-2117","url":"https:\/\/www.aclweb.org\/anthology\/P18-2117","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2118","title":"A Co-Matching Model for Multi-choice Reading Comprehension","authors":["Wang, Shuohang","Yu, Mo","Jiang, Jing","Chang, Shiyu"],"emails":["shwang.2014@smu.edu.sg","yum@us.ibm.com","jingjiang@smu.edu.sg","shiyu.chang@ibm.com"],"author_id":["shuohang-wang","mo-yu","jing-jiang","shiyu-chang"],"abstract":"Multi-choice reading comprehension is a challenging task, which involves the matching between a passage and a question-answer pair. This paper proposes a new co-matching approach to this problem, which jointly models whether a passage can match both a question and a candidate answer. Experimental results on the RACE dataset demonstrate that our approach achieves state-of-the-art performance.","pages":"746--751","doi":"10.18653\/v1\/P18-2118","url":"https:\/\/www.aclweb.org\/anthology\/P18-2118","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2119","title":"Tackling the Story Ending Biases in The Story Cloze Test","authors":["Sharma, Rishi","Allen, James","Bakhshandeh, Omid","Mostafazadeh, Nasrin"],"emails":["rishi.sharma@rochester.edu","","","nasrinm@cs.rochester.edu"],"author_id":["rishi-sharma","james-allen","omid-bakhshandeh","nasrin-mostafazadeh"],"abstract":"The Story Cloze Test (SCT) is a recent framework for evaluating story comprehension and script learning. There have been a variety of models tackling the SCT so far. Although the original goal behind the SCT was to require systems to perform deep language understanding and commonsense reasoning for successful narrative understanding, some recent models could perform significantly better than the initial baselines by leveraging human-authorship biases discovered in the SCT dataset. In order to shed some light on this issue, we have performed various data analysis and analyzed a variety of top performing models presented for this task. Given the statistics we have aggregated, we have designed a new crowdsourcing scheme that creates a new SCT dataset, which overcomes some of the biases. We benchmark a few models on the new dataset and show that the top-performing model on the original SCT dataset fails to keep up its performance. Our findings further signify the importance of benchmarking NLP systems on various evolving test sets.","pages":"752--757","doi":"10.18653\/v1\/P18-2119","url":"https:\/\/www.aclweb.org\/anthology\/P18-2119","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2120","title":"A Multi-sentiment-resource Enhanced Attention Network for Sentiment Classification","authors":["Lei, Zeyang","Yang, Yujiu","Yang, Min","Liu, Yi"],"emails":["leizy16@mails.tsinghua.edu.cn","yang.yujiu@sz.tsinghua.edu.cn","min.yang1129@gmail.com","eeyliu@gmail.com"],"author_id":["zeyang-lei","yujiu-yang","min-yang","yi-liu"],"abstract":"Deep learning approaches for sentiment classification do not fully exploit sentiment linguistic knowledge. In this paper, we propose a Multi-sentiment-resource Enhanced Attention Network (MEAN) to alleviate the problem by integrating three kinds of sentiment linguistic knowledge (e.g., sentiment lexicon, negation words, intensity words) into the deep neural network via attention mechanisms. By using various types of sentiment resources, MEAN utilizes sentiment-relevant information from different representation sub-spaces, which makes it more effective to capture the overall semantics of the sentiment, negation and intensity words for sentiment prediction. The experimental results demonstrate that MEAN has robust superiority over strong competitors.","pages":"758--763","doi":"10.18653\/v1\/P18-2120","url":"https:\/\/www.aclweb.org\/anthology\/P18-2120","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2121","title":"Pretraining Sentiment Classifiers with Unlabeled Dialog Data","authors":["Shimizu, Toru","Shimizu, Nobuyuki","Kobayashi, Hayato"],"emails":["ftoshimiz@yahoo-corp.jp","nobushimg@yahoo-corp.jp","hakobaya@yahoo-corp.jp"],"author_id":["tohru-shimizu","nobuyuki-shimizu","hayato-kobayashi"],"abstract":"The huge cost of creating labeled training data is a common problem for supervised learning tasks such as sentiment classification. Recent studies showed that pretraining with unlabeled data via a language model can improve the performance of classification models. In this paper, we take the concept a step further by using a conditional language model, instead of a language model. Specifically, we address a sentiment classification task for a tweet analysis service as a case study and propose a pretraining strategy with unlabeled dialog data (tweet-reply pairs) via an encoder-decoder model. Experimental results show that our strategy can improve the performance of sentiment classifiers and outperform several state-of-the-art strategies including language model pretraining.","pages":"764--770","doi":"10.18653\/v1\/P18-2121","url":"https:\/\/www.aclweb.org\/anthology\/P18-2121","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2122","title":"Disambiguating False-Alarm Hashtag Usages in Tweets for Irony Detection","authors":["Huang, Hen-Hsen","Chen, Chiao-Chen","Chen, Hsin-Hsi"],"emails":["hhhuang@nlg.csie.ntu.edu.tw","hhchen@ntu.edu.tw","b04902055@ntu.edu.tw"],"author_id":["hen-hsen-huang","chiao-chen-chen","hsin-hsi-chen"],"abstract":"The reliability of self-labeled data is an important issue when the data are regarded as ground-truth for training and testing learning-based models. This paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection. We analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three hashtags that are widely used to collect the training data for irony detection. Furthermore, we apply our model to prune the self-labeled training data. Experimental results show that the irony detection model trained on the less but cleaner training instances outperforms the models trained on all data.","pages":"771--777","doi":"10.18653\/v1\/P18-2122","url":"https:\/\/www.aclweb.org\/anthology\/P18-2122","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2123","title":"Cross-Target Stance Classification with Self-Attention Networks","authors":["Xu, Chang","Paris, C{\\'e}cile","Nepal, Surya","Sparks, Ross"],"emails":["","","","parks@data61.csiro.au"],"author_id":["chang-xu","cecile-paris","surya-nepal","ross-sparks"],"abstract":"In stance classification, the target on which the stance is made defines the boundary of the task, and a classifier is usually trained for prediction on the same target. In this work, we explore the potential for generalizing classifiers between different targets, and propose a neural model that can apply what has been learned from a source target to a destination target. We show that our model can find useful information shared between relevant targets which improves generalization in certain scenarios.","pages":"778--783","doi":"10.18653\/v1\/P18-2123","url":"https:\/\/www.aclweb.org\/anthology\/P18-2123","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2124","title":"Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}","authors":["Rajpurkar, Pranav","Jia, Robin","Liang, Percy"],"emails":["pranavsr@cs.stanford.edu","robinjia@cs.stanford.edu","pliang@cs.stanford.edu"],"author_id":["pranav-rajpurkar","robin-jia","percy-liang"],"abstract":"Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86{\\%} F1 on SQuAD achieves only 66{\\%} F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.","pages":"784--789","doi":"10.18653\/v1\/P18-2124","url":"https:\/\/www.aclweb.org\/anthology\/P18-2124","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"id":"P18-2125","title":"{`}Lighter{'} Can Still Be Dark: Modeling Comparative Color Descriptions","authors":["Winn, Olivia","Muresan, Smaranda"],"emails":["olivia@cs.columbia.edu","smara@columbia.edu"],"author_id":["olivia-winn","smaranda-muresan"],"abstract":"We propose a novel paradigm of grounding comparative adjectives within the realm of color descriptions. Given a reference RGB color and a comparative term (e.g., lighter, darker), our model learns to ground the comparative as a direction in the RGB space such that the colors along the vector, rooted at the reference color, satisfy the comparison. Our model generates grounded representations of comparative adjectives with an average accuracy of 0.65 cosine similarity to the desired direction of change. These vectors approach colors with Delta-E scores of under 7 compared to the target colors, indicating the differences are very small with respect to human perception. Our approach makes use of a newly created dataset for this task derived from existing labeled color data.","pages":"790--795","doi":"10.18653\/v1\/P18-2125","url":"https:\/\/www.aclweb.org\/anthology\/P18-2125","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"}]