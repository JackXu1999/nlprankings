[{"id":"S18-2001","title":"Resolving Event Coreference with Supervised Representation Learning and Clustering-Oriented Regularization","authors":["Kenyon-Dean, Kian","Cheung, Jackie Chi Kit","Precup, Doina"],"emails":["kian.kenyon-dean@mail.mcgill.ca","jcheung@cs.mcgill.ca","dprecup@cs.mcgill.ca"],"author_id":["kian-kenyon-dean","jackie-chi-kit-cheung","doina-precup"],"abstract":"We present an approach to event coreference resolution by developing a general framework for clustering that uses supervised representation learning. We propose a neural network architecture with novel Clustering-Oriented Regularization (CORE) terms in the objective function. These terms encourage the model to create embeddings of event mentions that are amenable to clustering. We then use agglomerative clustering on these embeddings to build event coreference chains. For both within- and cross-document coreference on the ECB+ corpus, our model obtains better results than models that require significantly more pre-annotated information. This work provides insight and motivating results for a new general approach to solving coreference and clustering problems with representation learning.","pages":"1--10","doi":"10.18653\/v1\/S18-2001","url":"https:\/\/www.aclweb.org\/anthology\/S18-2001","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2002","title":"Learning distributed event representations with a multi-task approach","authors":["Hong, Xudong","Sayeed, Asad","Demberg, Vera"],"emails":["xhong@coli.uni-saarland.de","asad.sayeed@gu.se","vera@coli.uni-saarland.de"],"author_id":["xudong-hong","asad-sayeed","vera-demberg"],"abstract":"Human world knowledge contains information about prototypical events and their participants and locations. In this paper, we train the first models using multi-task learning that can both predict missing event participants and also perform semantic role classification based on semantic plausibility. Our best-performing model is an improvement over the previous state-of-the-art on thematic fit modelling tasks. The event embeddings learned by the model can additionally be used effectively in an event similarity task, also outperforming the state-of-the-art.","pages":"11--21","doi":"10.18653\/v1\/S18-2002","url":"https:\/\/www.aclweb.org\/anthology\/S18-2002","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2003","title":"Assessing Meaning Components in {G}erman Complex Verbs: A Collection of Source-Target Domains and Directionality","authors":["Schulte im Walde, Sabine","K{\\\"o}per, Maximilian","Springorum, Sylvia"],"emails":["schulte@ims.uni-stuttgart.de","maximilian.koeper@ims.uni-stuttgart.de","sylvia.springorum@ims.uni-stuttgart.de"],"author_id":["sabine-schulte-im-walde","maximilian-koper","sylvia-springorum"],"abstract":"This paper presents a collection to assess meaning components in German complex verbs, which frequently undergo meaning shifts. We use a novel strategy to obtain source and target domain characterisations via sentence generation rather than sentence annotation. A selection of arrows adds spatial directional information to the generated contexts. We provide a broad qualitative description of the dataset, and a series of standard classification experiments verifies the quantitative reliability of the presented resource. The setup for collecting the meaning components is applicable also to other languages, regarding complex verbs as well as other language-specific targets that involve meaning shifts.","pages":"22--32","doi":"10.18653\/v1\/S18-2003","url":"https:\/\/www.aclweb.org\/anthology\/S18-2003","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2004","title":"Learning Neural Word Salience Scores","authors":["Samardzhiev, Krasen","Gargett, Andrew","Bollegala, Danushka"],"emails":["krasensam@gmail.com","andrew.gargett@stfc.ac.uk","danushka@liverpool.ac.uk"],"author_id":["krasen-samardzhiev","andrew-gargett","danushka-bollegala"],"abstract":"Measuring the salience of a word is an essential step in numerous NLP tasks. Heuristic approaches such as tfidf have been used so far to estimate the salience of words. We propose \\textit{Neural Word Salience} (NWS) scores, unlike heuristics, are learnt from a corpus. Specifically, we learn word salience scores such that, using pre-trained word embeddings as the input, can accurately predict the words that appear in a sentence, given the words that appear in the sentences preceding or succeeding that sentence. Experimental results on sentence similarity prediction show that the learnt word salience scores perform comparably or better than some of the state-of-the-art approaches for representing sentences on benchmark datasets for sentence similarity, while using only a fraction of the training and prediction times required by prior methods. Moreover, our NWS scores positively correlate with psycholinguistic measures such as concreteness, and imageability implying a close connection to the salience as perceived by humans.","pages":"33--42","doi":"10.18653\/v1\/S18-2004","url":"https:\/\/www.aclweb.org\/anthology\/S18-2004","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2005","title":"Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems","authors":["Kiritchenko, Svetlana","Mohammad, Saif"],"emails":["svetlana.kiritchenko@nrc-cnrc.gc.ca","saif.mohammad@nrc-cnrc.gc.ca"],"author_id":["svetlana-kiritchenko","saif-mohammad"],"abstract":"Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems. Further, there is no benchmark dataset for examining inappropriate biases in systems. Here for the first time, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders. We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 {`}Affect in Tweets{'}. We find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. We make the EEC freely available.","pages":"43--53","doi":"10.18653\/v1\/S18-2005","url":"https:\/\/www.aclweb.org\/anthology\/S18-2005","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2006","title":"Graph Algebraic Combinatory Categorial Grammar","authors":["Beschke, Sebastian","Menzel, Wolfgang"],"emails":["beschke@informatik.uni-hamburg.de","menzel@informatik.uni-hamburg.de"],"author_id":["sebastian-beschke","wolfgang-menzel"],"abstract":"This paper describes CCG\/AMR, a novel grammar for semantic parsing of Abstract Meaning Representations. CCG\/AMR equips Combinatory Categorial Grammar derivations with graph semantics by assigning each CCG combinator an interpretation in terms of a graph algebra. We provide an algorithm that induces a CCG\/AMR from a corpus and show that it creates a compact lexicon with low ambiguity and achieves a robust coverage of 78{\\%} of the examined sentences under ideal conditions. We also identify several phenomena that affect any approach relying either on CCG or graph algebraic approaches for AMR parsing. This includes differences of representation between CCG and AMR, as well as non-compositional constructions that are not expressible through a monotonous construction process. To our knowledge, this paper provides the first analysis of these corpus issues.","pages":"54--64","doi":"10.18653\/v1\/S18-2006","url":"https:\/\/www.aclweb.org\/anthology\/S18-2006","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2007","title":"Mixing Context Granularities for Improved Entity Linking on Question Answering Data across Entity Categories","authors":["Sorokin, Daniil","Gurevych, Iryna"],"emails":["sorokin@ukp.informatik.tu-darmstadt.de","gurevych@ukp.informatik.tu-darmstadt.de"],"author_id":["daniil-sorokin","iryna-gurevych"],"abstract":"The first stage of every knowledge base question answering approach is to link entities in the input question. We investigate entity linking in the context of question answering task and present a jointly optimized neural architecture for entity mention detection and entity disambiguation that models the surrounding context on different levels of granularity. We use the Wikidata knowledge base and available question answering datasets to create benchmarks for entity linking on question answering data. Our approach outperforms the previous state-of-the-art system on this data, resulting in an average 8{\\%} improvement of the final score. We further demonstrate that our model delivers a strong performance across different entity categories.","pages":"65--75","doi":"10.18653\/v1\/S18-2007","url":"https:\/\/www.aclweb.org\/anthology\/S18-2007","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2008","title":"Quantitative Semantic Variation in the Contexts of Concrete and Abstract Words","authors":["Naumann, Daniela","Frassinelli, Diego","Schulte im Walde, Sabine"],"emails":["naumanda@ims.uni-stuttgart.de","frassinelli@ims.uni-stuttgart.de","schulte@ims.uni-stuttgart.de"],"author_id":["daniela-naumann","diego-frassinelli","sabine-schulte-im-walde"],"abstract":"Across disciplines, researchers are eager to gain insight into empirical features of abstract vs. concrete concepts. In this work, we provide a detailed characterisation of the distributional nature of abstract and concrete words across 16,620 English nouns, verbs and adjectives. Specifically, we investigate the following questions: (1) What is the distribution of concreteness in the contexts of concrete and abstract target words? (2) What are the differences between concrete and abstract words in terms of contextual semantic diversity? (3) How does the entropy of concrete and abstract word contexts differ? Overall, our studies show consistent differences in the distributional representation of concrete and abstract words, thus challenging existing theories of cognition and providing a more fine-grained description of their nature.","pages":"76--85","doi":"10.18653\/v1\/S18-2008","url":"https:\/\/www.aclweb.org\/anthology\/S18-2008","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2009","title":"{E}mo{W}ord{N}et: Automatic Expansion of Emotion Lexicon Using {E}nglish {W}ord{N}et","authors":["Badaro, Gilbert","Jundi, Hussein","Hajj, Hazem","El-Hajj, Wassim"],"emails":["ggb05@aub.edu.lb","we07@aub.edu.lb","hh63@aub.edu.lb","haj14@aub.edu.lb"],"author_id":["gilbert-badaro","hussein-jundi","hazem-hajj","wassim-el-hajj"],"abstract":"Nowadays, social media have become a platform where people can easily express their opinions and emotions about any topic such as politics, movies, music, electronic products and many others. On the other hand, politicians, companies, and businesses are interested in analyzing automatically people{'}s opinions and emotions. In the last decade, a lot of efforts has been put into extracting sentiment polarity from texts. Recently, the focus has expanded to also cover emotion recognition from texts. In this work, we expand an existing emotion lexicon, DepecheMood, by leveraging semantic knowledge from English WordNet (EWN). We create an expanded lexicon, EmoWordNet, consisting of 67K terms aligned with EWN, almost 1.8 times the size of DepecheMood. We also evaluate EmoWordNet in an emotion recognition task using SemEval 2007 news headlines dataset and we achieve an improvement compared to the use of DepecheMood. EmoWordNet is publicly available to speed up research in the field on \\url{http:\/\/oma-project.com}.","pages":"86--93","doi":"10.18653\/v1\/S18-2009","url":"https:\/\/www.aclweb.org\/anthology\/S18-2009","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2010","title":"The Limitations of Cross-language Word Embeddings Evaluation","authors":["Bakarov, Amir","Suvorov, Roman","Sochenkov, Ilya"],"emails":["amirbakarov@gmail.com","rsuvorov@isa.ru","ivsochenkov@gmail.com"],"author_id":["amir-bakarov","roman-suvorov","ilya-sochenkov"],"abstract":"The aim of this work is to explore the possible limitations of existing methods of cross-language word embeddings evaluation, addressing the lack of correlation between intrinsic and extrinsic cross-language evaluation methods. To prove this hypothesis, we construct English-Russian datasets for extrinsic and intrinsic evaluation tasks and compare performances of 5 different cross-language models on them. The results say that the scores even on different intrinsic benchmarks do not correlate to each other. We can conclude that the use of human references as ground truth for cross-language word embeddings is not proper unless one does not understand how do native speakers process semantics in their cognition.","pages":"94--100","doi":"10.18653\/v1\/S18-2010","url":"https:\/\/www.aclweb.org\/anthology\/S18-2010","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2011","title":"How Gender and Skin Tone Modifiers Affect Emoji Semantics in Twitter","authors":["Barbieri, Francesco","Camacho-Collados, Jose"],"emails":["francesco.barbieri@cardiff.ac.uk","jose.camacho-collados@cardiff.ac.uk"],"author_id":["francesco-barbieri","jose-camacho-collados"],"abstract":"In this paper we analyze the use of emojis in social media with respect to gender and skin tone. By gathering a dataset of over twenty two million tweets from United States some findings are clearly highlighted after performing a simple frequency-based analysis. Moreover, we carry out a semantic analysis on the usage of emojis and their modifiers (e.g. gender and skin tone) by embedding all words, emojis and modifiers into the same vector space. Our analyses reveal that some stereotypes related to the skin color and gender seem to be reflected on the use of these modifiers. For example, emojis representing hand gestures are more widely utilized with lighter skin tones, and the usage across skin tones differs significantly. At the same time, the vector corresponding to the male modifier tends to be semantically close to emojis related to business or technology, whereas their female counterparts appear closer to emojis about love or makeup.","pages":"101--106","doi":"10.18653\/v1\/S18-2011","url":"https:\/\/www.aclweb.org\/anthology\/S18-2011","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2012","title":"Element-wise Bilinear Interaction for Sentence Matching","authors":["Choi, Jihun","Kim, Taeuk","Lee, Sang-goo"],"emails":["jhchoi@europa.snu.ac.kr","taeuk@europa.snu.ac.kr","sglee@europa.snu.ac.kr"],"author_id":["jihun-choi","taeuk-kim","sang-goo-lee"],"abstract":"When we build a neural network model predicting the relationship between two sentences, the most general and intuitive approach is to use a Siamese architecture, where the sentence vectors obtained from a shared encoder is given as input to a classifier. For the classifier to work effectively, it is important to extract appropriate features from the two vectors and feed them as input. There exist several previous works that suggest heuristic-based function for matching sentence vectors, however it cannot be said that the heuristics tailored for a specific task generalize to other tasks. In this work, we propose a new matching function, ElBiS, that learns to model element-wise interaction between two vectors. From experiments, we empirically demonstrate that the proposed ElBiS matching function outperforms the concatenation-based or heuristic-based matching functions on natural language inference and paraphrase identification, while maintaining the fused representation compact.","pages":"107--112","doi":"10.18653\/v1\/S18-2012","url":"https:\/\/www.aclweb.org\/anthology\/S18-2012","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2013","title":"Named Graphs for Semantic Representation","authors":["Crouch, Richard","Kalouli, Aikaterini-Lida"],"emails":["dick.crouch@gmail.com","aikaterini-lida.kalouli@uni-konstanz.de"],"author_id":["richard-crouch","aikaterini-lida-kalouli"],"abstract":"A position paper arguing that purely graphical representations for natural language semantics lack a fundamental degree of expressiveness, and cannot deal with even basic Boolean operations like negation or disjunction. Moving from graphs to named graphs leads to representations that stand some chance of having sufficient expressive power. Named $\\mathcal{FL}_0$ graphs are of particular interest.","pages":"113--118","doi":"10.18653\/v1\/S18-2013","url":"https:\/\/www.aclweb.org\/anthology\/S18-2013","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2014","title":"Learning Patient Representations from Text","authors":["Dligach, Dmitriy","Miller, Timothy"],"emails":["1ddligach@luc.edu","2timothy.miller@childrens.harvard.edu"],"author_id":["dmitriy-dligach","timothy-miller"],"abstract":"Mining electronic health records for patients who satisfy a set of predefined criteria is known in medical informatics as phenotyping. Phenotyping has numerous applications such as outcome prediction, clinical trial recruitment, and retrospective studies. Supervised machine learning for phenotyping typically relies on sparse patient representations such as bag-of-words. We consider an alternative that involves learning patient representations. We develop a neural network model for learning patient representations and show that the learned representations are general enough to obtain state-of-the-art performance on a standard comorbidity detection task.","pages":"119--123","doi":"10.18653\/v1\/S18-2014","url":"https:\/\/www.aclweb.org\/anthology\/S18-2014","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2015","title":"Polarity Computations in Flexible Categorial Grammar","authors":["Hu, Hai","Moss, Larry"],"emails":["huhai@indiana.edu","lsm@cs.indiana.edu"],"author_id":["hai-hu","larry-moss"],"abstract":"This paper shows how to take parse trees in CCG and algorithmically find the polarities of all the constituents. Our work uses the well-known polarization principle corresponding to function application, and we have extended this with principles for type raising and composition. We provide an algorithm, extending the polarity marking algorithm of van Benthem. We discuss how our system works in practice, taking input from the C{\\&}C parser.","pages":"124--129","doi":"10.18653\/v1\/S18-2015","url":"https:\/\/www.aclweb.org\/anthology\/S18-2015","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2016","title":"Coarse Lexical Frame Acquisition at the Syntax{--}Semantics Interface Using a Latent-Variable {PCFG} Model","authors":["Kallmeyer, Laura","QasemiZadeh, Behrang","Cheung, Jackie Chi Kit"],"emails":["kallmeyer@hhu.de","zadeh@phil.hhu.de","jcheung@cs.mcgill.ca"],"author_id":["laura-kallmeyer","behrang-qasemizadeh","jackie-chi-kit-cheung"],"abstract":"We present a method for unsupervised lexical frame acquisition at the syntax{--}semantics interface. Given a set of input strings derived from dependency parses, our method generates a set of clusters that resemble lexical frame structures. Our work is motivated not only by its practical applications (e.g., to build, or expand the coverage of lexical frame databases), but also to gain linguistic insight into frame structures with respect to lexical distributions in relation to grammatical structures. We model our task using a hierarchical Bayesian network and employ tools and methods from latent variable probabilistic context free grammars (L-PCFGs) for statistical inference and parameter fitting, for which we propose a new split and merge procedure. We show that our model outperforms several baselines on a portion of the Wall Street Journal sentences that we have newly annotated for evaluation purposes.","pages":"130--141","doi":"10.18653\/v1\/S18-2016","url":"https:\/\/www.aclweb.org\/anthology\/S18-2016","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2017","title":"{H}alo: Learning Semantics-Aware Representations for Cross-Lingual Information Extraction","authors":["Mei, Hongyuan","Zhang, Sheng","Duh, Kevin","Van Durme, Benjamin"],"emails":["hmei@cs.jhu.edu","s.zhang@cs.jhu.edu","kevinduh@cs.jhu.edu","vandurme@cs.jhu.edu"],"author_id":["hongyuan-mei","sheng-zhang","kevin-duh","benjamin-van-durme"],"abstract":"Cross-lingual information extraction (CLIE) is an important and challenging task, especially in low resource scenarios. To tackle this challenge, we propose a training method, called \\textit{Halo}, which enforces the local region of each hidden state of a neural model to only generate target tokens with the same semantic structure tag. This simple but powerful technique enables a neural model to learn semantics-aware representations that are robust to noise, without introducing any extra parameter, thus yielding better generalization in both high and low resource settings.","pages":"142--147","doi":"10.18653\/v1\/S18-2017","url":"https:\/\/www.aclweb.org\/anthology\/S18-2017","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2018","title":"Exploiting Partially Annotated Data in Temporal Relation Extraction","authors":["Ning, Qiang","Yu, Zhongzhi","Fan, Chuchu","Roth, Dan"],"emails":["qning2@illinois.edu","zyu19@illinois.edu","cfan10@illinois.edu","danroth@seas.upenn.edu"],"author_id":["qiang-ning","zhongzhi-yu","chuchu-fan","dan-roth"],"abstract":"Annotating temporal relations (TempRel) between events described in natural language is known to be labor intensive, partly because the total number of TempRels is quadratic in the number of events. As a result, only a small number of documents are typically annotated, limiting the coverage of various lexical\/semantic phenomena. In order to improve existing approaches, one possibility is to make use of the readily available, partially annotated data (P as in partial) that cover more documents. However, missing annotations in P are known to hurt, rather than help, existing systems. This work is a case study in exploring various usages of P for TempRel extraction. Results show that despite missing annotations, P is still a useful supervision signal for this task within a constrained bootstrapping learning framework. The system described in this system is publicly available.","pages":"148--153","doi":"10.18653\/v1\/S18-2018","url":"https:\/\/www.aclweb.org\/anthology\/S18-2018","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2019","title":"Predicting Word Embeddings Variability","authors":["Pierrejean, B{\\'e}n{\\'e}dicte","Tanguy, Ludovic"],"emails":["benedicte.pierrejean@univ-tlse2.fr","ludovic.tanguy@univ-tlse2.fr"],"author_id":["benedicte-pierrejean","ludovic-tanguy"],"abstract":"Neural word embeddings models (such as those built with word2vec) are known to have stability problems: when retraining a model with the exact same hyperparameters, words neighborhoods may change. We propose a method to estimate such variation, based on the overlap of neighbors of a given word in two models trained with identical hyperparameters. We show that this inherent variation is not negligible, and that it does not affect every word in the same way. We examine the influence of several features that are intrinsic to a word, corpus or embedding model and provide a methodology that can predict the variability (and as such, reliability) of a word representation in a semantic vector space.","pages":"154--159","doi":"10.18653\/v1\/S18-2019","url":"https:\/\/www.aclweb.org\/anthology\/S18-2019","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2020","title":"Integrating Multiplicative Features into Supervised Distributional Methods for Lexical Entailment","authors":["Vu, Tu","Shwartz, Vered"],"emails":["tuvu@cs.umass.edu","vered1986@gmail.com"],"author_id":["tu-vu","vered-shwartz"],"abstract":"Supervised distributional methods are applied successfully in lexical entailment, but recent work questioned whether these methods actually learn a relation between two words. Specifically, Levy et al. (2015) claimed that linear classifiers learn only separate properties of each word. We suggest a cheap and easy way to boost the performance of these methods by integrating multiplicative features into commonly used representations. We provide an extensive evaluation with different classifiers and evaluation setups, and suggest a suitable evaluation setup for the task, eliminating biases existing in previous ones.","pages":"160--166","doi":"10.18653\/v1\/S18-2020","url":"https:\/\/www.aclweb.org\/anthology\/S18-2020","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2021","title":"Deep Affix Features Improve Neural Named Entity Recognizers","authors":["Yadav, Vikas","Sharp, Rebecca","Bethard, Steven"],"emails":["vikasy@email.arizona.edu","bsharp@email.arizona.edu","bethard@email.arizona.edu"],"author_id":["vikas-yadav","rebecca-sharp","steven-bethard"],"abstract":"We propose a practical model for named entity recognition (NER) that combines word and character-level information with a specific learned representation of the prefixes and suffixes of the word. We apply this approach to multilingual and multi-domain NER and show that it achieves state of the art results on the CoNLL 2002 Spanish and Dutch and CoNLL 2003 German NER datasets, consistently achieving 1.5-2.3 percent over the state of the art without relying on any dictionary features. Additionally, we show improvement on SemEval 2013 task 9.1 DrugNER, achieving state of the art results on the MedLine dataset and the second best results overall (-1.3{\\%} from state of the art). We also establish a new benchmark on the I2B2 2010 Clinical NER dataset with 84.70 F-score.","pages":"167--172","doi":"10.18653\/v1\/S18-2021","url":"https:\/\/www.aclweb.org\/anthology\/S18-2021","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2022","title":"Fine-grained Entity Typing through Increased Discourse Context and Adaptive Classification Thresholds","authors":["Zhang, Sheng","Duh, Kevin","Van Durme, Benjamin"],"emails":["zsheng2@jhu.edu","kevinduh@cs.jhu.edu","vandurme@cs.jhu.edu"],"author_id":["sheng-zhang","kevin-duh","benjamin-van-durme"],"abstract":"Fine-grained entity typing is the task of assigning fine-grained semantic types to entity mentions. We propose a neural architecture which learns a distributional semantic representation that leverages a greater amount of semantic context {--} both document and sentence level information {--} than prior work. We find that additional context improves performance, with further improvements gained by utilizing adaptive classification thresholds. Experiments show that our approach without reliance on hand-crafted features achieves the state-of-the-art results on three benchmark datasets.","pages":"173--179","doi":"10.18653\/v1\/S18-2022","url":"https:\/\/www.aclweb.org\/anthology\/S18-2022","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2023","title":"Hypothesis Only Baselines in Natural Language Inference","authors":["Poliak, Adam","Naradowsky, Jason","Haldar, Aparajita","Rudinger, Rachel","Van Durme, Benjamin"],"emails":["azpoliak@cs.jhu.edu","narad@jhu.edu","ahaldar1@jhu.edu","rudinger@jhu.edu","vandurme@cs.jhu.edu"],"author_id":["adam-poliak","jason-naradowsky","aparajita-haldar","rachel-rudinger","benjamin-van-durme"],"abstract":"We propose a hypothesis only baseline for diagnosing Natural Language Inference (NLI). Especially when an NLI dataset assumes inference is occurring based purely on the relationship between a context and a hypothesis, it follows that assessing entailment relations while ignoring the provided context is a degenerate solution. Yet, through experiments on 10 distinct NLI datasets, we find that this approach, which we refer to as a hypothesis-only model, is able to significantly outperform a majority-class baseline across a number of NLI datasets. Our analysis suggests that statistical irregularities may allow a model to perform NLI in some datasets beyond what should be achievable without access to the context.","pages":"180--191","doi":"10.18653\/v1\/S18-2023","url":"https:\/\/www.aclweb.org\/anthology\/S18-2023","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2024","title":"Quality Signals in Generated Stories","authors":["Sagarkar, Manasvi","Wieting, John","Tu, Lifu","Gimpel, Kevin"],"emails":["manasvi@uchicago.edu","jwieting@cs.cmu.edu","lifu@ttic.edu","kgimpel@ttic.edu"],"author_id":["manasvi-sagarkar","john-wieting","lifu-tu","kevin-gimpel"],"abstract":"We study the problem of measuring the quality of automatically-generated stories. We focus on the setting in which a few sentences of a story are provided and the task is to generate the next sentence ({``}continuation{''}) in the story. We seek to identify what makes a story continuation interesting, relevant, and have high overall quality. We crowdsource annotations along these three criteria for the outputs of story continuation systems, design features, and train models to predict the annotations. Our trained scorer can be used as a rich feature function for story generation, a reward function for systems that use reinforcement learning to learn to generate stories, and as a partial evaluation metric for story generation.","pages":"192--202","doi":"10.18653\/v1\/S18-2024","url":"https:\/\/www.aclweb.org\/anthology\/S18-2024","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2025","title":"Term Definitions Help Hypernymy Detection","authors":["Yin, Wenpeng","Roth, Dan"],"emails":["wenpeng@seas.upenn.edu","danroth@seas.upenn.edu"],"author_id":["wenpeng-yin","dan-roth"],"abstract":"Existing methods of hypernymy detection mainly rely on statistics over a big corpus, either mining some co-occurring patterns like {``}animals such as cats{''} or embedding words of interest into context-aware vectors. These approaches are therefore limited by the availability of a large enough corpus that can cover all terms of interest and provide sufficient contextual information to represent their meaning. In this work, we propose a new paradigm, HyperDef, for hypernymy detection {--} expressing word meaning by encoding word definitions, along with context driven representation. This has two main benefits: (i) Definitional sentences express (sense-specific) corpus-independent meanings of words, hence definition-driven approaches enable strong generalization {--} once trained, the model is expected to work well in open-domain testbeds; (ii) Global context from a large corpus and definitions provide complementary information for words. Consequently, our model, HyperDef, once trained on task-agnostic data, gets state-of-the-art results in multiple benchmarks","pages":"203--213","doi":"10.18653\/v1\/S18-2025","url":"https:\/\/www.aclweb.org\/anthology\/S18-2025","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2026","title":"Agree or Disagree: Predicting Judgments on Nuanced Assertions","authors":["Wojatzki, Michael","Zesch, Torsten","Mohammad, Saif","Kiritchenko, Svetlana"],"emails":["michael.wojatzki@uni-due.de","torsten.zeschi@uni-due.de","saif.mohammad@nrc-cnrc.gc.ca","svetlana.kiritchenko@nrc-cnrc.gc.ca"],"author_id":["michael-wojatzki","torsten-zesch","saif-mohammad","svetlana-kiritchenko"],"abstract":"Being able to predict whether people agree or disagree with an assertion (i.e. an explicit, self-contained statement) has several applications ranging from predicting how many people will like or dislike a social media post to classifying posts based on whether they are in accordance with a particular point of view. We formalize this as two NLP tasks: predicting judgments of (i) individuals and (ii) groups based on the text of the assertion and previous judgments. We evaluate a wide range of approaches on a crowdsourced data set containing over 100,000 judgments on over 2,000 assertions. We find that predicting individual judgments is a hard task with our best results only slightly exceeding a majority baseline, but that judgments of groups can be more reliably predicted using a Siamese neural network, which outperforms all other approaches by a wide margin.","pages":"214--224","doi":"10.18653\/v1\/S18-2026","url":"https:\/\/www.aclweb.org\/anthology\/S18-2026","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2027","title":"A Multimodal Translation-Based Approach for Knowledge Graph Representation Learning","authors":["Mousselly-Sergieh, Hatem","Botschen, Teresa","Gurevych, Iryna","Roth, Stefan"],"emails":["h.m.sergieh@gmail.com","","gurevych@ukp.informatik","aiphes@ukp.informatik"],"author_id":["hatem-mousselly-sergieh","teresa-botschen","iryna-gurevych","stefan-roth"],"abstract":"Current methods for knowledge graph (KG) representation learning focus solely on the structure of the KG and do not exploit any kind of external information, such as visual and linguistic information corresponding to the KG entities. In this paper, we propose a multimodal translation-based approach that defines the energy of a KG triple as the sum of sub-energy functions that leverage both multimodal (visual and linguistic) and structural KG representations. Next, a ranking-based loss is minimized using a simple neural network architecture. Moreover, we introduce a new large-scale dataset for multimodal KG representation learning. We compared the performance of our approach to other baselines on two standard tasks, namely knowledge graph completion and triple classification, using our as well as the WN9-IMG dataset. The results demonstrate that our approach outperforms all baselines on both tasks and datasets.","pages":"225--234","doi":"10.18653\/v1\/S18-2027","url":"https:\/\/www.aclweb.org\/anthology\/S18-2027","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2028","title":"Putting Semantics into Semantic Roles","authors":["Allen, James","Teng, Choh Man"],"emails":["jallen@ihmc.us","cmteng@ihmc.us"],"author_id":["james-allen","choh-man-teng"],"abstract":"While there have been many proposals for theories of semantic roles over the years, these models are mostly justified by intuition and the only evaluation methods have been inter-annotator agreement. We explore three different ideas for providing more rigorous theories of semantic roles. These ideas give rise to more objective criteria for designing role sets, and lend themselves to some experimental evaluation. We illustrate the discussion by examining the semantic roles in TRIPS.","pages":"235--244","doi":"10.18653\/v1\/S18-2028","url":"https:\/\/www.aclweb.org\/anthology\/S18-2028","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2029","title":"Measuring Frame Instance Relatedness","authors":["Basile, Valerio","Lopez Condori, Roque","Cabrio, Elena"],"emails":["basile@di.unito.it","roque.lopez-condori@inria.fr","elena.cabrio@unice.fr"],"author_id":["valerio-basile","roque-lopez-condori","elena-cabrio"],"abstract":"Frame semantics is a well-established framework to represent the meaning of natural language in computational terms. In this work, we aim to propose a quantitative measure of relatedness between pairs of frame instances. We test our method on a dataset of sentence pairs, highlighting the correlation between our metric and human judgments of semantic similarity. Furthermore, we propose an application of our measure for clustering frame instances to extract prototypical knowledge from natural language.","pages":"245--254","doi":"10.18653\/v1\/S18-2029","url":"https:\/\/www.aclweb.org\/anthology\/S18-2029","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2030","title":"Solving Feature Sparseness in Text Classification using Core-Periphery Decomposition","authors":["Cui, Xia","Kojaku, Sadamori","Masuda, Naoki","Bollegala, Danushka"],"emails":["xia.cui@liverpool.ac.uk","sadamori.koujaku@bristol.ac.uk","naoki.masuda@bristol.ac.uk","danushka.bollegala@liverpool.ac.uk"],"author_id":["xia-cui","sadamori-kojaku","naoki-masuda","danushka-bollegala"],"abstract":"Feature sparseness is a problem common to cross-domain and short-text classification tasks. To overcome this feature sparseness problem, we propose a novel method based on graph decomposition to find candidate features for expanding feature vectors. Specifically, we first create a feature-relatedness graph, which is subsequently decomposed into core-periphery (CP) pairs and use the peripheries as the expansion candidates of the cores. We expand both training and test instances using the computed related features and use them to train a text classifier. We observe that prioritising features that are common to both training and test instances as cores during the CP decomposition to further improve the accuracy of text classification. We evaluate the proposed CP-decomposition-based feature expansion method on benchmark datasets for cross-domain sentiment classification and short-text classification. Our experimental results show that the proposed method consistently outperforms all baselines on short-text classification tasks, and perform competitively with pivot-based cross-domain sentiment classification methods.","pages":"255--264","doi":"10.18653\/v1\/S18-2030","url":"https:\/\/www.aclweb.org\/anthology\/S18-2030","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2031","title":"Robust Handling of Polysemy via Sparse Representations","authors":["Mahabal, Abhijit","Roth, Dan","Mittal, Sid"],"emails":["amahabal@google.com","danroth@seas.upenn.edu","sidmittal@google.com"],"author_id":["abhijit-mahabal","dan-roth","sid-mittal"],"abstract":"Words are polysemous and multi-faceted, with many shades of meanings. We suggest that sparse distributed representations are more suitable than other, commonly used, (dense) representations to express these multiple facets, and present Category Builder, a working system that, as we show, makes use of sparse representations to support multi-faceted lexical representations. We argue that the set expansion task is well suited to study these meaning distinctions since a word may belong to multiple sets with a different reason for membership in each. We therefore exhibit the performance of Category Builder on this task, while showing that our representation captures at the same time analogy problems such as {``}the Ganga of Egypt{''} or {``}the Voldemort of Tolkien{''}. Category Builder is shown to be a more expressive lexical representation and to outperform dense representations such as Word2Vec in some analogy classes despite being shown only two of the three input terms.","pages":"265--275","doi":"10.18653\/v1\/S18-2031","url":"https:\/\/www.aclweb.org\/anthology\/S18-2031","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"},{"id":"S18-2032","title":"Multiplicative Tree-Structured Long Short-Term Memory Networks for Semantic Representations","authors":["Tran, Nam Khanh","Cheng, Weiwei"],"emails":["ntran@l3s.de","weiweic@amazon.com"],"author_id":["nam-khanh-tran","weiwei-cheng"],"abstract":"Tree-structured LSTMs have shown advantages in learning semantic representations by exploiting syntactic information. Most existing methods model tree structures by bottom-up combinations of constituent nodes using the same shared compositional function and often making use of input word information only. The inability to capture the richness of compositionality makes these models lack expressive power. In this paper, we propose multiplicative tree-structured LSTMs to tackle this problem. Our model makes use of not only word information but also relation information between words. It is more expressive, as different combination functions can be used for each child node. In addition to syntactic trees, we also investigate the use of Abstract Meaning Representation in tree-structured models, in order to incorporate both syntactic and semantic information from the sentence. Experimental results on common NLP tasks show the proposed models lead to better sentence representation and AMR brings benefits in complex tasks.","pages":"276--286","doi":"10.18653\/v1\/S18-2032","url":"https:\/\/www.aclweb.org\/anthology\/S18-2032","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics"}]