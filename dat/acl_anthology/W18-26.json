[{"id":"W18-2601","title":"Ruminating Reader: Reasoning with Gated Multi-hop Attention","authors":["Gong, Yichen","Bowman, Samuel"],"emails":["yichen.gong@nyu.edu","bowman@nyu.edu"],"author_id":["yichen-gong","samuel-bowman"],"abstract":"To answer the question in machine comprehension (MC) task, the models need to establish the interaction between the question and the context. To tackle the problem that the single-pass model cannot reflect on and correct its answer, we present Ruminating Reader. Ruminating Reader adds a second pass of attention and a novel information fusion component to the Bi-Directional Attention Flow model (BiDAF). We propose novel layer structures that construct a query aware context vector representation and fuse encoding representation with intermediate representation on top of BiDAF model. We show that a multi-hop attention mechanism can be applied to a bi-directional attention structure. In experiments on SQuAD, we find that the Reader outperforms the BiDAF baseline by 2.1 F1 score and 2.7 EM score. Our analysis shows that different hops of the attention have different responsibilities in selecting answers.","pages":"1--11","doi":"10.18653\/v1\/W18-2601","url":"https:\/\/www.aclweb.org\/anthology\/W18-2601","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the Workshop on Machine Reading for Question Answering"},{"id":"W18-2602","title":"Systematic Error Analysis of the {S}tanford Question Answering Dataset","authors":["Rondeau, Marc-Antoine","Hazen, T. J."],"emails":["marondea@microsoft.com","tj.hazen@microsoft.com"],"author_id":["marc-antoine-rondeau","timothy-j-hazen"],"abstract":"We analyzed the outputs of multiple question answering (QA) models applied to the Stanford Question Answering Dataset (SQuAD) to identify the core challenges for QA systems on this data set. Through an iterative process, challenging aspects were hypothesized through qualitative analysis of the common error cases. A classifier was then constructed to predict whether SQuAD test examples were likely to be difficult for systems to answer based on features associated with the hypothesized aspects. The classifier{'}s performance was used to accept or reject each aspect as an indicator of difficulty. With this approach, we ensured that our hypotheses were systematically tested and not simply accepted based on our pre-existing biases. Our explanations are not accepted based on human evaluation of individual examples. This process also enabled us to identify the primary QA strategy learned by the models, i.e., systems determined the acceptable answer type for a question and then selected the acceptable answer span of that type containing the highest density of words present in the question within its local vicinity in the passage.","pages":"12--20","doi":"10.18653\/v1\/W18-2602","url":"https:\/\/www.aclweb.org\/anthology\/W18-2602","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the Workshop on Machine Reading for Question Answering"},{"id":"W18-2603","title":"A Multi-Stage Memory Augmented Neural Network for Machine Reading Comprehension","authors":["Yu, Seunghak","Indurthi, Sathish Reddy","Back, Seohyun","Lee, Haejun"],"emails":["seunghak.yu@samsung.com","s.indurthi@samsung.com","scv.back@samsung.com",""],"author_id":["seunghak-yu","sathish-reddy-indurthi","seohyun-back","haejun-lee"],"abstract":"Reading Comprehension (RC) of text is one of the fundamental tasks in natural language processing. In recent years, several end-to-end neural network models have been proposed to solve RC tasks. However, most of these models suffer in reasoning over long documents. In this work, we propose a novel Memory Augmented Machine Comprehension Network (MAMCN) to address long-range dependencies present in machine reading comprehension. We perform extensive experiments to evaluate proposed method with the renowned benchmark datasets such as SQuAD, QUASAR-T, and TriviaQA. We achieve the state of the art performance on both the document-level (QUASAR-T, TriviaQA) and paragraph-level (SQuAD) datasets compared to all the previously published approaches.","pages":"21--30","doi":"10.18653\/v1\/W18-2603","url":"https:\/\/www.aclweb.org\/anthology\/W18-2603","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the Workshop on Machine Reading for Question Answering"},{"id":"W18-2604","title":"Tackling Adversarial Examples in {QA} via Answer Sentence Selection","authors":["Ren, Yuanhang","Du, Ye","Wang, Di"],"emails":["ryuanhang@gmail.com","henry.duye@gmail.com","albertwang0921@gmail.com"],"author_id":["yuanhang-ren","ye-du","di-wang"],"abstract":"Question answering systems deteriorate dramatically in the presence of adversarial sentences in articles. According to Jia and Liang (2017), the single BiDAF system (Seo et al., 2016) only achieves an F1 score of 4.8 on the ADDANY adversarial dataset. In this paper, we present a method to tackle this problem via answer sentence selection. Given a paragraph of an article and a corresponding query, instead of directly feeding the whole paragraph to the single BiDAF system, a sentence that most likely contains the answer to the query is first selected, which is done via a deep neural network based on TreeLSTM (Tai et al., 2015). Experiments on ADDANY adversarial dataset validate the effectiveness of our method. The F1 score has been improved to 52.3.","pages":"31--36","doi":"10.18653\/v1\/W18-2604","url":"https:\/\/www.aclweb.org\/anthology\/W18-2604","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the Workshop on Machine Reading for Question Answering"},{"id":"W18-2605","title":"{D}u{R}eader: a {C}hinese Machine Reading Comprehension Dataset from Real-world Applications","authors":["He, Wei","Liu, Kai","Liu, Jing","Lyu, Yajuan","Zhao, Shiqi","Xiao, Xinyan","Liu, Yuan","Wang, Yizhong","Wu, Hua","She, Qiaoqiao","Liu, Xuan","Wu, Tian","Wang, Haifeng"],"emails":["sheqiaoqiao@baidu.com","liuxuan@baidu.com","","","","","","wanghaifeng@baidu.com","hua@baidu.com","","","wutian@baidu.com",""],"author_id":["wei-he","kai-liu","jing-liu","yajuan-lyu","shiqi-zhao","xinyan-xiao","yuan-liu","yizhong-wang","hua-wu","qiaoqiao-she","xuan-liu","tian-wu","haifeng-wang"],"abstract":"This paper introduces DuReader, a new large-scale, open-domain Chinese machine reading comprehension (MRC) dataset, designed to address real-world MRC. DuReader has three advantages over previous MRC datasets: (1) data sources: questions and documents are based on Baidu Search and Baidu Zhidao; answers are manually generated. (2) question types: it provides rich annotations for more question types, especially yes-no and opinion questions, that leaves more opportunity for the research community. (3) scale: it contains 200K questions, 420K answers and 1M documents; it is the largest Chinese MRC dataset so far. Experiments show that human performance is well above current state-of-the-art baseline systems, leaving plenty of room for the community to make improvements. To help the community make these improvements, both DuReader and baseline systems have been posted online. We also organize a shared competition to encourage the exploration of more models. Since the release of the task, there are significant improvements over the baselines.","pages":"37--46","doi":"10.18653\/v1\/W18-2605","url":"https:\/\/www.aclweb.org\/anthology\/W18-2605","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the Workshop on Machine Reading for Question Answering"},{"id":"W18-2606","title":"Robust and Scalable Differentiable Neural Computer for Question Answering","authors":["Franke, J{\\\"o}rg","Niehues, Jan","Waibel, Alex"],"emails":["joerg.franke@student.kit.edu","jan.niehues@kit.edu","alex.waibel@kit.edu"],"author_id":["jorg-franke","jan-niehues","alex-waibel"],"abstract":"Deep learning models are often not easily adaptable to new tasks and require task-specific adjustments. The differentiable neural computer (DNC), a memory-augmented neural network, is designed as a general problem solver which can be used in a wide range of tasks. But in reality, it is hard to apply this model to new tasks. We analyze the DNC and identify possible improvements within the application of question answering. This motivates a more robust and scalable DNC (rsDNC). The objective precondition is to keep the general character of this model intact while making its application more reliable and speeding up its required training time. The rsDNC is distinguished by a more robust training, a slim memory unit and a bidirectional architecture. We not only achieve new state-of-the-art performance on the bAbI task, but also minimize the performance variance between different initializations. Furthermore, we demonstrate the simplified applicability of the rsDNC to new tasks with passable results on the CNN RC task without adaptions.","pages":"47--59","doi":"10.18653\/v1\/W18-2606","url":"https:\/\/www.aclweb.org\/anthology\/W18-2606","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the Workshop on Machine Reading for Question Answering"},{"id":"W18-2607","title":"A Systematic Classification of Knowledge, Reasoning, and Context within the {ARC} Dataset","authors":["Boratko, Michael","Padigela, Harshit","Mikkilineni, Divyendra","Yuvraj, Pritish","Das, Rajarshi","McCallum, Andrew","Chang, Maria","Fokoue-Nkoutche, Achille","Kapanipathi, Pavan","Mattei, Nicholas","Musa, Ryan","Talamadupula, Kartik","Witbrock, Michael"],"emails":["","","","","","","","","","","","",""],"author_id":["michael-boratko","harshit-padigela","divyendra-mikkilineni","pritish-yuvraj","rajarshi-das","andrew-mccallum","maria-chang","achille-fokoue-nkoutche","pavan-kapanipathi","nicholas-mattei","ryan-musa","kartik-talamadupula","michael-j-witbrock"],"abstract":"The recent work of Clark et al. (2018) introduces the AI2 Reasoning Challenge (ARC) and the associated ARC dataset that partitions open domain, complex science questions into easy and challenge sets. That paper includes an analysis of 100 questions with respect to the types of knowledge and reasoning required to answer them; however, it does not include clear definitions of these types, nor does it offer information about the quality of the labels. We propose a comprehensive set of definitions of knowledge and reasoning types necessary for answering the questions in the ARC dataset. Using ten annotators and a sophisticated annotation interface, we analyze the distribution of labels across the challenge set and statistics related to them. Additionally, we demonstrate that although naive information retrieval methods return sentences that are irrelevant to answering the query, sufficient supporting text is often present in the (ARC) corpus. Evaluating with human-selected relevant sentences improves the performance of a neural machine comprehension model by 42 points.","pages":"60--70","doi":"10.18653\/v1\/W18-2607","url":"https:\/\/www.aclweb.org\/anthology\/W18-2607","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the Workshop on Machine Reading for Question Answering"},{"id":"W18-2608","title":"{RECIPE}: Applying Open Domain Question Answering to Privacy Policies","authors":["Shvartzshanider, Yan","Balashankar, Ananth","Wies, Thomas","Subramanian, Lakshminarayanan"],"emails":["yansh@nyu.edu","ananth@nyu.edu","wies@cs.nyu.edu","lakshmi@cs.nyu.edu"],"author_id":["yan-shvartzshanider","ananth-balashankar","thomas-wies","lakshminarayanan-subramanian"],"abstract":"We describe our experiences in using an open domain question answering model (Chen et al., 2017) to evaluate an out-of-domain QA task of assisting in analyzing privacy policies of companies. Specifically, Relevant CI Parameters Extractor (RECIPE) seeks to answer questions posed by the theory of contextual integrity (CI) regarding the information flows described in the privacy statements. These questions have a simple syntactic structure and the answers are factoids or descriptive in nature. The model achieved an F1 score of 72.33, but we noticed that combining the results of this model with a neural dependency parser based approach yields a significantly higher F1 score of 92.35 compared to manual annotations. This indicates that future work which in-corporates signals from parsing like NLP tasks more explicitly can generalize better on out-of-domain tasks.","pages":"71--77","doi":"10.18653\/v1\/W18-2608","url":"https:\/\/www.aclweb.org\/anthology\/W18-2608","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the Workshop on Machine Reading for Question Answering"},{"id":"W18-2609","title":"Neural Models for Key Phrase Extraction and Question Generation","authors":["Subramanian, Sandeep","Wang, Tong","Yuan, Xingdi","Zhang, Saizheng","Trischler, Adam","Bengio, Yoshua"],"emails":["sandeep.subramanian.1@umontreal.ca","","","","",""],"author_id":["sandeep-subramanian","tong-wang","xingdi-yuan","saizheng-zhang","adam-trischler","yoshua-bengio"],"abstract":"We propose a two-stage neural model to tackle question generation from documents. First, our model estimates the probability that word sequences in a document are ones that a human would pick when selecting candidate answers by training a neural key-phrase extractor on the answers in a question-answering corpus. Predicted key phrases then act as target answers and condition a sequence-to-sequence question-generation model with a copy mechanism. Empirically, our key-phrase extraction model significantly outperforms an entity-tagging baseline and existing rule-based approaches. We further demonstrate that our question generation system formulates fluent, answerable questions from key phrases. This two-stage system could be used to augment or generate reading comprehension datasets, which may be leveraged to improve machine reading systems or in educational settings.","pages":"78--88","doi":"10.18653\/v1\/W18-2609","url":"https:\/\/www.aclweb.org\/anthology\/W18-2609","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the Workshop on Machine Reading for Question Answering"},{"id":"W18-2610","title":"Comparative Analysis of Neural {QA} models on {SQ}u{AD}","authors":["Wadhwa, Soumya","Chandu, Khyathi","Nyberg, Eric"],"emails":["soumyaw@andrew.cmu.edu","kchandu@andrew.cmu.edu","en09@andrew.cmu.edu"],"author_id":["soumya-wadhwa","khyathi-chandu","eric-nyberg"],"abstract":"The task of Question Answering has gained prominence in the past few decades for testing the ability of machines to understand natural language. Large datasets for Machine Reading have led to the development of neural models that cater to deeper language understanding compared to information retrieval tasks. Different components in these neural architectures are intended to tackle different challenges. As a first step towards achieving generalization across multiple domains, we attempt to understand and compare the peculiarities of existing end-to-end neural models on the Stanford Question Answering Dataset (SQuAD) by performing quantitative as well as qualitative analysis of the results attained by each of them. We observed that prediction errors reflect certain model-specific biases, which we further discuss in this paper.","pages":"89--97","doi":"10.18653\/v1\/W18-2610","url":"https:\/\/www.aclweb.org\/anthology\/W18-2610","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the Workshop on Machine Reading for Question Answering"},{"id":"W18-2611","title":"Adaptations of {ROUGE} and {BLEU} to Better Evaluate Machine Reading Comprehension Task","authors":["Yang, An","Liu, Kai","Liu, Jing","Lyu, Yajuan","Li, Sujian"],"emails":["yangan@pku.edu.cn","liukai20@baidu.com","liujing46@baidu.com","lvyajuan@baidu.com","lisujian@pku.edu.cn"],"author_id":["an-yang","kai-liu","jing-liu","yajuan-lyu","sujian-li"],"abstract":"Current evaluation metrics to question answering based machine reading comprehension (MRC) systems generally focus on the lexical overlap between candidate and reference answers, such as ROUGE and BLEU. However, bias may appear when these metrics are used for specific question types, especially questions inquiring yes-no opinions and entity lists. In this paper, we make adaptations on the metrics to better correlate $n$-gram overlap with the human judgment for answers to these two question types. Statistical analysis proves the effectiveness of our approach. Our adaptations may provide positive guidance for the development of real-scene MRC systems.","pages":"98--104","doi":"10.18653\/v1\/W18-2611","url":"https:\/\/www.aclweb.org\/anthology\/W18-2611","publisher":"Association for Computational Linguistics","address":"Melbourne, Australia","year":"2018","month":"July","booktitle":"Proceedings of the Workshop on Machine Reading for Question Answering"}]