@proceedings{ws-2018-graph,
    title = "Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing ({T}ext{G}raphs-12)",
    author = "Glava{\v{s}}, Goran  and
      Somasundaran, Swapna  and
      Riedl, Martin  and
      Hovy, Eduard",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-1700",
    doi = "10.18653/v1/W18-17",
}
@inproceedings{luo-etal-2018-scientific,
    title = "Scientific Discovery as Link Prediction in Influence and Citation Graphs",
    author = "Luo, Fan  and
      Valenzuela-Esc{\'a}rcega, Marco A.  and
      Hahn-Powell, Gus  and
      Surdeanu, Mihai",
    booktitle = "Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing ({T}ext{G}raphs-12)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-1701",
    doi = "10.18653/v1/W18-1701",
    pages = "1--6",
    abstract = "We introduce a machine learning approach for the identification of {``}white spaces{''} in scientific knowledge. Our approach addresses this task as link prediction over a graph that contains over 2M influence statements such as {``}CTCF activates FOXA1{''}, which were automatically extracted using open-domain machine reading. We model this prediction task using graph-based features extracted from the above influence graph, as well as from a citation graph that captures scientific communities. We evaluated the proposed approach through backtesting. Although the data is heavily unbalanced (50 times more negative examples than positives), our approach predicts which influence links will be discovered in the {``}near future{''} with a F1 score of 27 points, and a mean average precision of 68{\%}.",
}
@inproceedings{zhang-etal-2018-efficient,
    title = "Efficient Generation and Processing of Word Co-occurrence Networks Using corpus2graph",
    author = "Zhang, Zheng  and
      Zweigenbaum, Pierre  and
      Yin, Ruiqing",
    booktitle = "Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing ({T}ext{G}raphs-12)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-1702",
    doi = "10.18653/v1/W18-1702",
    pages = "7--11",
    abstract = "Corpus2graph is an open-source NLP-application-oriented tool that generates a word co-occurrence network from a large corpus. It not only contains different built-in methods to preprocess words, analyze sentences, extract word pairs and define edge weights, but also supports user-customized functions. By using parallelization techniques, it can generate a large word co-occurrence network of the whole English Wikipedia data within hours. And thanks to its nodes-edges-weight three-level progressive calculation design, rebuilding networks with different configurations is even faster as it does not need to start all over again. This tool also works with other graph libraries such as igraph, NetworkX and graph-tool as a front end providing data to boost network generation speed.",
}
@inproceedings{jansen-2018-multi,
    title = "Multi-hop Inference for Sentence-level {T}ext{G}raphs: How Challenging is Meaningfully Combining Information for Science Question Answering?",
    author = "Jansen, Peter",
    booktitle = "Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing ({T}ext{G}raphs-12)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-1703",
    doi = "10.18653/v1/W18-1703",
    pages = "12--17",
    abstract = "Question Answering for complex questions is often modelled as a graph construction or traversal task, where a solver must build or traverse a graph of facts that answer and explain a given question. This {``}multi-hop{''} inference has been shown to be extremely challenging, with few models able to aggregate more than two facts before being overwhelmed by {``}semantic drift{''}, or the tendency for long chains of facts to quickly drift off topic. This is a major barrier to current inference models, as even elementary science questions require an average of 4 to 6 facts to answer and explain. In this work we empirically characterize the difficulty of building or traversing a graph of sentences connected by lexical overlap, by evaluating chance sentence aggregation quality through 9,784 manually-annotated judgements across knowledge graphs built from three free-text corpora (including study guides and Simple Wikipedia). We demonstrate semantic drift tends to be high and aggregation quality low, at between 0.04 and 3, and highlight scenarios that maximize the likelihood of meaningfully combining information.",
}
@inproceedings{linhares-pontes-etal-2018-multi,
    title = "Multi-Sentence Compression with Word Vertex-Labeled Graphs and Integer Linear Programming",
    author = "Linhares Pontes, Elvys  and
      Huet, St{\'e}phane  and
      Gouveia da Silva, Thiago  and
      Linhares, Andr{\'e}a carneiro  and
      Torres-Moreno, Juan-Manuel",
    booktitle = "Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing ({T}ext{G}raphs-12)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-1704",
    doi = "10.18653/v1/W18-1704",
    pages = "18--27",
    abstract = "Multi-Sentence Compression (MSC) aims to generate a short sentence with key information from a cluster of closely related sentences. MSC enables summarization and question-answering systems to generate outputs combining fully formed sentences from one or several documents. This paper describes a new Integer Linear Programming method for MSC using a vertex-labeled graph to select different keywords, and novel 3-gram scores to generate more informative sentences while maintaining their grammaticality. Our system is of good quality and outperforms the state-of-the-art for evaluations led on news dataset. We led both automatic and manual evaluations to determine the informativeness and the grammaticality of compressions for each dataset. Additional tests, which take advantage of the fact that the length of compressions can be modulated, still improve ROUGE scores with shorter output sentences.",
}
@inproceedings{pham-chen-2018-large,
    title = "Large-scale spectral clustering using diffusion coordinates on landmark-based bipartite graphs",
    author = "Pham, Khiem  and
      Chen, Guangliang",
    booktitle = "Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing ({T}ext{G}raphs-12)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-1705",
    doi = "10.18653/v1/W18-1705",
    pages = "28--37",
    abstract = "Spectral clustering has received a lot of attention due to its ability to separate nonconvex, non-intersecting manifolds, but its high computational complexity has significantly limited its applicability. Motivated by the document-term co-clustering framework by Dhillon (2001), we propose a landmark-based scalable spectral clustering approach in which we first use the selected landmark set and the given data to form a bipartite graph and then run a diffusion process on it to obtain a family of diffusion coordinates for clustering. We show that our proposed algorithm can be implemented based on very efficient operations on the affinity matrix between the given data and selected landmarks, thus capable of handling large data. Finally, we demonstrate the excellent performance of our method by comparing with the state-of-the-art scalable algorithms on several benchmark data sets.",
}
@inproceedings{chang-etal-2018-efficient,
    title = "Efficient Graph-based Word Sense Induction by Distributional Inclusion Vector Embeddings",
    author = "Chang, Haw-Shiuan  and
      Agrawal, Amol  and
      Ganesh, Ananya  and
      Desai, Anirudha  and
      Mathur, Vinayak  and
      Hough, Alfred  and
      McCallum, Andrew",
    booktitle = "Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing ({T}ext{G}raphs-12)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-1706",
    doi = "10.18653/v1/W18-1706",
    pages = "38--48",
    abstract = "Word sense induction (WSI), which addresses polysemy by unsupervised discovery of multiple word senses, resolves ambiguities for downstream NLP tasks and also makes word representations more interpretable. This paper proposes an accurate and efficient graph-based method for WSI that builds a global non-negative vector embedding basis (which are interpretable like topics) and clusters the basis indexes in the ego network of each polysemous word. By adopting distributional inclusion vector embeddings as our basis formation model, we avoid the expensive step of nearest neighbor search that plagues other graph-based methods without sacrificing the quality of sense clusters. Experiments on three datasets show that our proposed method produces similar or better sense clusters and embeddings compared with previous state-of-the-art methods while being significantly more efficient.",
}
@inproceedings{skianis-etal-2018-fusing,
    title = "Fusing Document, Collection and Label Graph-based Representations with Word Embeddings for Text Classification",
    author = "Skianis, Konstantinos  and
      Malliaros, Fragkiskos  and
      Vazirgiannis, Michalis",
    booktitle = "Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing ({T}ext{G}raphs-12)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-1707",
    doi = "10.18653/v1/W18-1707",
    pages = "49--58",
    abstract = "Contrary to the traditional Bag-of-Words approach, we consider the Graph-of-Words(GoW) model in which each document is represented by a graph that encodes relationships between the different terms. Based on this formulation, the importance of a term is determined by weighting the corresponding node in the document, collection and label graphs, using node centrality criteria. We also introduce novel graph-based weighting schemes by enriching graphs with word-embedding similarities, in order to reward or penalize semantic relationships. Our methods produce more discriminative feature weights for text categorization, outperforming existing frequency-based criteria.",
}
@inproceedings{dhingra-etal-2018-embedding,
    title = "Embedding Text in Hyperbolic Spaces",
    author = "Dhingra, Bhuwan  and
      Shallue, Christopher  and
      Norouzi, Mohammad  and
      Dai, Andrew  and
      Dahl, George",
    booktitle = "Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing ({T}ext{G}raphs-12)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-1708",
    doi = "10.18653/v1/W18-1708",
    pages = "59--69",
    abstract = "Natural language text exhibits hierarchical structure in a variety of respects. Ideally, we could incorporate our prior knowledge of this hierarchical structure into unsupervised learning algorithms that work on text data. Recent work by Nickel and Kiela (2017) proposed using hyperbolic instead of Euclidean embedding spaces to represent hierarchical data and demonstrated encouraging results when embedding graphs. In this work, we extend their method with a re-parameterization technique that allows us to learn hyperbolic embeddings of arbitrarily parameterized objects. We apply this framework to learn word and sentence embeddings in hyperbolic space in an unsupervised manner from text corpora. The resulting embeddings seem to encode certain intuitive notions of hierarchy, such as word-context frequency and phrase constituency. However, the implicit continuous hierarchy in the learned hyperbolic space makes interrogating the model{'}s learned hierarchies more difficult than for models that learn explicit edges between items. The learned hyperbolic embeddings show improvements over Euclidean embeddings in some {--} but not all {--} downstream tasks, suggesting that hierarchical organization is more useful for some tasks than others.",
}
