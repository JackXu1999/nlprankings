[{"id":"W17-3001","title":"Dimensions of Abusive Language on Twitter","authors":["Clarke, Isobelle","Grieve, Jack"],"emails":["clarkei@aston.ac.uk","grievejw@gmail.com"],"author_id":["isobelle-clarke","jack-grieve"],"abstract":"In this paper, we use a new categorical form of multidimensional register analysis to identify the main dimensions of functional linguistic variation in a corpus of abusive language, consisting of racist and sexist Tweets. By analysing the use of a wide variety of parts-of-speech and grammatical constructions, as well as various features related to Twitter and computer-mediated communication, we discover three dimensions of linguistic variation in this corpus, which we interpret as being related to the degree of interactive, antagonistic and attitudinal language exhibited by individual Tweets. We then demonstrate that there is a significant functional difference between racist and sexist Tweets, with sexists Tweets tending to be more interactive and attitudinal than racist Tweets.","pages":"1--10","doi":"10.18653\/v1\/W17-3001","url":"https:\/\/www.aclweb.org\/anthology\/W17-3001","publisher":"Association for Computational Linguistics","address":"Vancouver, BC, Canada","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Abusive Language Online"},{"id":"W17-3002","title":"Constructive Language in News Comments","authors":["Kolhatkar, Varada","Taboada, Maite"],"emails":["vkolhatk@sfu.ca","mtaboada@sfu.ca"],"author_id":["varada-kolhatkar","maite-taboada"],"abstract":"We discuss the characteristics of constructive news comments, and present methods to identify them. First, we define the notion of constructiveness. Second, we annotate a corpus for constructiveness. Third, we explore whether available argumentation corpora can be useful to identify constructiveness in news comments. Our model trained on argumentation corpora achieves a top accuracy of 72.59{\\%} (baseline=49.44{\\%}) on our crowd-annotated test data. Finally, we examine the relation between constructiveness and toxicity. In our crowd-annotated data, 21.42{\\%} of the non-constructive comments and 17.89{\\%} of the constructive comments are toxic, suggesting that non-constructive comments are not much more toxic than constructive comments.","pages":"11--17","doi":"10.18653\/v1\/W17-3002","url":"https:\/\/www.aclweb.org\/anthology\/W17-3002","publisher":"Association for Computational Linguistics","address":"Vancouver, BC, Canada","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Abusive Language Online"},{"id":"W17-3003","title":"Rephrasing Profanity in {C}hinese Text","authors":["Su, Hui-Po","Huang, Zhen-Jie","Chang, Hao-Tsung","Lin, Chuan-Jie"],"emails":["","","1htchang.cse@mail.ntou.edu.tw","2cjlin@mail.ntou.edu.tw"],"author_id":["hui-po-su","zhen-jie-huang","hao-tsung-chang","chuan-jie-lin"],"abstract":"This paper proposes a system that can detect and rephrase profanity in Chinese text. Rather than just masking detected profanity, we want to revise the input sentence by using inoffensive words while keeping their original meanings. 29 of such rephrasing rules were invented after observing sentences on real-word social websites. The overall accuracy of the proposed system is 85.56{\\%}","pages":"18--24","doi":"10.18653\/v1\/W17-3003","url":"https:\/\/www.aclweb.org\/anthology\/W17-3003","publisher":"Association for Computational Linguistics","address":"Vancouver, BC, Canada","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Abusive Language Online"},{"id":"W17-3004","title":"Deep Learning for User Comment Moderation","authors":["Pavlopoulos, John","Malakasiotis, Prodromos","Androutsopoulos, Ion"],"emails":["ip@straintek.com","mm@straintek.com","ion@aueb.gr"],"author_id":["john-pavlopoulos","prodromos-malakasiotis","ion-androutsopoulos"],"abstract":"Experimenting with a new dataset of 1.6M user comments from a Greek news portal and existing datasets of EnglishWikipedia comments, we show that an RNN outperforms the previous state of the art in moderation. A deep, classification-specific attention mechanism improves further the overall performance of the RNN. We also compare against a CNN and a word-list baseline, considering both fully automatic and semi-automatic moderation.","pages":"25--35","doi":"10.18653\/v1\/W17-3004","url":"https:\/\/www.aclweb.org\/anthology\/W17-3004","publisher":"Association for Computational Linguistics","address":"Vancouver, BC, Canada","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Abusive Language Online"},{"id":"W17-3005","title":"Class-based Prediction Errors to Detect Hate Speech with Out-of-vocabulary Words","authors":["Serr{\\`a}, Joan","Leontiadis, Ilias","Spathis, Dimitris","Stringhini, Gianluca","Blackburn, Jeremy","Vakali, Athena"],"emails":["","","","","",""],"author_id":["joan-serra","ilias-leontiadis","dimitris-spathis","gianluca-stringhini","jeremy-blackburn","athena-vakali"],"abstract":"Common approaches to text categorization essentially rely either on n-gram counts or on word embeddings. This presents important difficulties in highly dynamic or quickly-interacting environments, where the appearance of new words and\/or varied misspellings is the norm. A paradigmatic example of this situation is abusive online behavior, with social networks and media platforms struggling to effectively combat uncommon or non-blacklisted hate words. To better deal with these issues in those fast-paced environments, we propose using the error signal of class-based language models as input to text classification algorithms. In particular, we train a next-character prediction model for any given class and then exploit the error of such class-based models to inform a neural network classifier. This way, we shift from the {`}ability to describe{'} seen documents to the {`}ability to predict{'} unseen content. Preliminary studies using out-of-vocabulary splits from abusive tweet data show promising results, outperforming competitive text categorization strategies by 4-11{\\%}.","pages":"36--40","doi":"10.18653\/v1\/W17-3005","url":"https:\/\/www.aclweb.org\/anthology\/W17-3005","publisher":"Association for Computational Linguistics","address":"Vancouver, BC, Canada","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Abusive Language Online"},{"id":"W17-3006","title":"One-step and Two-step Classification for Abusive Language Detection on Twitter","authors":["Park, Ji Ho","Fung, Pascale"],"emails":["jhpark@connect.ust.hk","pascale@ece.ust.hk"],"author_id":["ji-ho-park","pascale-fung"],"abstract":"Automatic abusive language detection is a difficult but important task for online social media. Our research explores a two-step approach of performing classification on abusive language and then classifying into specific types and compares it with one-step approach of doing one multi-class classification for detecting sexist and racist languages. With a public English Twitter corpus of 20 thousand tweets in the type of sexism and racism, our approach shows a promising performance of 0.827 F-measure by using HybridCNN in one-step and 0.824 F-measure by using logistic regression in two-steps.","pages":"41--45","doi":"10.18653\/v1\/W17-3006","url":"https:\/\/www.aclweb.org\/anthology\/W17-3006","publisher":"Association for Computational Linguistics","address":"Vancouver, BC, Canada","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Abusive Language Online"},{"id":"W17-3007","title":"Legal Framework, Dataset and Annotation Schema for Socially Unacceptable Online Discourse Practices in {S}lovene","authors":["Fi{\\v{s}}er, Darja","Erjavec, Toma{\\v{z}}","Ljube{\\v{s}}i{\\'c}, Nikola"],"emails":["darja.fiser@ff.uni-lj.si","tomaz.erjavec@ijs.si","nikola.ljubesic@ijs.si"],"author_id":["darja-fiser","tomaz-erjavec","nikola-ljubesic"],"abstract":"In this paper we present the legal framework, dataset and annotation schema of socially unacceptable discourse practices on social networking platforms in Slovenia. On this basis we aim to train an automatic identification and classification system with which we wish contribute towards an improved methodology, understanding and treatment of such practices in the contemporary, increasingly multicultural information society.","pages":"46--51","doi":"10.18653\/v1\/W17-3007","url":"https:\/\/www.aclweb.org\/anthology\/W17-3007","publisher":"Association for Computational Linguistics","address":"Vancouver, BC, Canada","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Abusive Language Online"},{"id":"W17-3008","title":"Abusive Language Detection on {A}rabic Social Media","authors":["Mubarak, Hamdy","Darwish, Kareem","Magdy, Walid"],"emails":["hmubarak@qf.org.qa","kdarwish@qf.org.qa","wmagdy@inf.ed.ac.uk"],"author_id":["hamdy-mubarak","kareem-darwish","walid-magdy"],"abstract":"In this paper, we present our work on detecting abusive language on Arabic social media. We extract a list of obscene words and hashtags using common patterns used in offensive and rude communications. We also classify Twitter users according to whether they use any of these words or not in their tweets. We expand the list of obscene words using this classification, and we report results on a newly created dataset of classified Arabic tweets (obscene, offensive, and clean). We make this dataset freely available for research, in addition to the list of obscene words and hashtags. We are also publicly releasing a large corpus of classified user comments that were deleted from a popular Arabic news site due to violations the site{'}s rules and guidelines.","pages":"52--56","doi":"10.18653\/v1\/W17-3008","url":"https:\/\/www.aclweb.org\/anthology\/W17-3008","publisher":"Association for Computational Linguistics","address":"Vancouver, BC, Canada","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Abusive Language Online"},{"id":"W17-3009","title":"Vectors for Counterspeech on Twitter","authors":["Wright, Lucas","Ruths, Derek","Dillon, Kelly P","Saleem, Haji Mohammad","Benesch, Susan"],"emails":["","","","","sbenesch@cyber.law.harvard.edu"],"author_id":["lucas-wright","derek-ruths","kelly-p-dillon","haji-mohammad-saleem","susan-benesch"],"abstract":"A study of conversations on Twitter found that some arguments between strangers led to favorable change in discourse and even in attitudes. The authors propose that such exchanges can be usefully distinguished according to whether individuals or groups take part on each side, since the opportunity for a constructive exchange of views seems to vary accordingly.","pages":"57--62","doi":"10.18653\/v1\/W17-3009","url":"https:\/\/www.aclweb.org\/anthology\/W17-3009","publisher":"Association for Computational Linguistics","address":"Vancouver, BC, Canada","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Abusive Language Online"},{"id":"W17-3010","title":"Detecting Nastiness in Social Media","authors":["Safi Samghabadi, Niloofar","Maharjan, Suraj","Sprague, Alan","Diaz-Sprague, Raquel","Solorio, Thamar"],"emails":["nsafisamghabadi@uh.edu","smaharjan2@uh.edu","sprague@cis.uab.edu","diazspra@uab.edu","tsolorio@uh.edu"],"author_id":["niloofar-safi-samghabadi","suraj-maharjan","alan-sprague","raquel-diaz-sprague","thamar-solorio"],"abstract":"Although social media has made it easy for people to connect on a virtually unlimited basis, it has also opened doors to people who misuse it to undermine, harass, humiliate, threaten and bully others. There is a lack of adequate resources to detect and hinder its occurrence. In this paper, we present our initial NLP approach to detect invective posts as a first step to eventually detect and deter cyberbullying. We crawl data containing profanities and then determine whether or not it contains invective. Annotations on this data are improved iteratively by in-lab annotations and crowdsourcing. We pursue different NLP approaches containing various typical and some newer techniques to distinguish the use of swear words in a neutral way from those instances in which they are used in an insulting way. We also show that this model not only works for our data set, but also can be successfully applied to different data sets.","pages":"63--72","doi":"10.18653\/v1\/W17-3010","url":"https:\/\/www.aclweb.org\/anthology\/W17-3010","publisher":"Association for Computational Linguistics","address":"Vancouver, BC, Canada","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Abusive Language Online"},{"id":"W17-3011","title":"Technology Solutions to Combat Online Harassment","authors":["Kennedy, George","McCollough, Andrew","Dixon, Edward","Bastidas, Alexei","Ryan, John","Loo, Chris","Sahay, Saurav"],"emails":["george.w.kennedy@intel.com","","edward.dixon@intel.com","","","",""],"author_id":["george-kennedy","andrew-mccollough","edward-dixon","alexei-bastidas","john-ryan","chris-loo","saurav-sahay"],"abstract":"This work is part of a new initiative to use machine learning to identify online harassment in social media and comment streams. Online harassment goes under-reported due to the reliance on humans to identify and report harassment, reporting that is further slowed by requirements to fill out forms providing context. In addition, the time for moderators to respond and apply human judgment can take days, but response times in terms of minutes are needed in the online context. Though some of the major social media companies have been doing proprietary work in automating the detection of harassment, there are few tools available for use by the public. In addition, the amount of labeled online harassment data and availability of cross-platform online harassment datasets is limited. We present the methodology used to create a harassment dataset and classifier and the dataset used to help the system learn what harassment looks like.","pages":"73--77","doi":"10.18653\/v1\/W17-3011","url":"https:\/\/www.aclweb.org\/anthology\/W17-3011","publisher":"Association for Computational Linguistics","address":"Vancouver, BC, Canada","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Abusive Language Online"},{"id":"W17-3012","title":"Understanding Abuse: A Typology of Abusive Language Detection Subtasks","authors":["Waseem, Zeerak","Davidson, Thomas","Warmsley, Dana","Weber, Ingmar"],"emails":["z.w.butt@sheffield.ac.uk","trd54@cornell.edu","dw457@cornell.edu","iweber@hbku.edu.qa"],"author_id":["zeerak-waseem","thomas-davidson","dana-warmsley","ingmar-weber"],"abstract":"As the body of research on abusive language detection and analysis grows, there is a need for critical consideration of the relationships between different subtasks that have been grouped under this label. Based on work on hate speech, cyberbullying, and online abuse we propose a typology that captures central similarities and differences between subtasks and discuss the implications of this for data annotation and feature construction. We emphasize the practical actions that can be taken by researchers to best approach their abusive language detection subtask of interest.","pages":"78--84","doi":"10.18653\/v1\/W17-3012","url":"https:\/\/www.aclweb.org\/anthology\/W17-3012","publisher":"Association for Computational Linguistics","address":"Vancouver, BC, Canada","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Abusive Language Online"},{"id":"W17-3013","title":"Using Convolutional Neural Networks to Classify Hate-Speech","authors":["Gamb{\\\"a}ck, Bj{\\\"o}rn","Sikdar, Utpal Kumar"],"emails":["gamback@ntnu.no","utpal.sikdar@gmail.com"],"author_id":["bjorn-gamback","utpal-kumar-sikdar"],"abstract":"The paper introduces a deep learning-based Twitter hate-speech text classification system. The classifier assigns each tweet to one of four predefined categories: racism, sexism, both (racism and sexism) and non-hate-speech. Four Convolutional Neural Network models were trained on resp. character 4-grams, word vectors based on semantic information built using word2vec, randomly generated word vectors, and word vectors combined with character n-grams. The feature set was down-sized in the networks by max-pooling, and a softmax function used to classify tweets. Tested by 10-fold cross-validation, the model based on word2vec embeddings performed best, with higher precision than recall, and a 78.3{\\%} F-score.","pages":"85--90","doi":"10.18653\/v1\/W17-3013","url":"https:\/\/www.aclweb.org\/anthology\/W17-3013","publisher":"Association for Computational Linguistics","address":"Vancouver, BC, Canada","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Abusive Language Online"},{"id":"W17-3014","title":"Illegal is not a Noun: Linguistic Form for Detection of Pejorative Nominalizations","authors":["Palmer, Alexis","Robinson, Melissa","Phillips, Kristy K."],"emails":["alexis.palmer@unt.edu","melissa.robinson@my.unt.edu","kristy.phillips@unt.edu"],"author_id":["alexis-palmer","melissa-robinson","kristy-k-phillips"],"abstract":"This paper focuses on a particular type of abusive language, targeting expressions in which typically neutral adjectives take on pejorative meaning when used as nouns - compare {`}gay people{'} to {`}the gays{'}. We first collect and analyze a corpus of hand-curated, expert-annotated pejorative nominalizations for four target adjectives: female, gay, illegal, and poor. We then collect a second corpus of automatically-extracted and POS-tagged, crowd-annotated tweets. For both corpora, we find support for the hypothesis that some adjectives, when nominalized, take on negative meaning. The targeted constructions are non-standard yet widely-used, and part-of-speech taggers mistag some nominal forms as adjectives. We implement a tool called NomCatcher to correct these mistaggings, and find that the same tool is effective for identifying new adjectives subject to transformation via nominalization into abusive language.","pages":"91--100","doi":"10.18653\/v1\/W17-3014","url":"https:\/\/www.aclweb.org\/anthology\/W17-3014","publisher":"Association for Computational Linguistics","address":"Vancouver, BC, Canada","year":"2017","month":"August","booktitle":"Proceedings of the First Workshop on Abusive Language Online"}]