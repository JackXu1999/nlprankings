@proceedings{ws-2017-natural,
    title = "Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA} 2017)",
    author = "Tseng, Yuen-Hsien  and
      Chen, Hsin-Hsi  and
      Lee, Lung-Hao  and
      Yu, Liang-Chih",
    month = dec,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/W17-5900",
}
@inproceedings{winder-etal-2017-ntucle,
    title = "{NTUCLE}: Developing a Corpus of Learner {E}nglish to Provide Writing Support for Engineering Students",
    author = "Winder, Roger Vivek Placidus  and
      MacKinnon, Joseph  and
      Li, Shu Yun  and
      Lin, Benedict Christopher Tzer Liang  and
      Heah, Carmel Lee Hah  and
      Morgado da Costa, Lu{\'\i}s  and
      Kuribayashi, Takayuki  and
      Bond, Francis",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA} 2017)",
    month = dec,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/W17-5901",
    pages = "1--11",
    abstract = "This paper describes the creation of a new annotated learner corpus. The aim is to use this corpus to develop an automated system for corrective feedback on students{'} writing. With this system, students will be able to receive timely feedback on language errors before they submit their assignments for grading. A corpus of assignments submitted by first year engineering students was compiled, and a new error tag set for the NTU Corpus of Learner English (NTUCLE) was developed based on that of the NUS Corpus of Learner English (NUCLE), as well as marking rubrics used at NTU. After a description of the corpus, error tag set and annotation process, the paper presents the results of the annotation exercise as well as follow up actions. The final error tag set, which is significantly larger than that for the NUCLE error categories, is then presented before a brief conclusion summarising our experience and future plans.",
}
@inproceedings{hana-hladka-2017-understanding,
    title = "Understanding Non-Native Writings: Can a Parser Help?",
    author = "Hana, Jirka  and
      Hladk{\'a}, Barbora",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA} 2017)",
    month = dec,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/W17-5902",
    pages = "12--16",
    abstract = "We present a pilot study on parsing non-native texts written by learners of Czech. We performed experiments that have shown that at least high-level syntactic functions, like subject, predicate, and object, can be assigned based on a parser trained on standard native language.",
}
@inproceedings{jiang-lee-2017-carrier,
    title = "Carrier Sentence Selection for Fill-in-the-blank Items",
    author = "Jiang, Shu  and
      Lee, John",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA} 2017)",
    month = dec,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/W17-5903",
    pages = "17--22",
    abstract = "Fill-in-the-blank items are a common form of exercise in computer-assisted language learning systems. To automatically generate an effective item, the system must be able to select a high-quality carrier sentence that illustrates the usage of the target word. Previous approaches for carrier sentence selection have considered sentence length, vocabulary difficulty, the position of the target word and the presence of finite verbs. This paper investigates the utility of word co-occurrence statistics and lexical similarity as selection criteria. In an evaluation on generating fill-in-the-blank items for learning Chinese as a foreign language, we show that these two criteria can improve carrier sentence quality.",
}
@inproceedings{redkar-etal-2017-hindi,
    title = "{H}indi Shabdamitra: A {W}ordnet based E-Learning Tool for Language Learning and Teaching",
    author = "Redkar, Hanumant  and
      Singh, Sandhya  and
      Somasundaram, Meenakshi  and
      Gorasia, Dhara  and
      Kulkarni, Malhar  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA} 2017)",
    month = dec,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/W17-5904",
    pages = "23--28",
    abstract = "In today{'}s technology driven digital era, education domain is undergoing a transformation from traditional approaches to more learner controlled and flexible methods of learning. This transformation has opened the new avenues for interdisciplinary research in the field of educational technology and natural language processing in developing quality digital aids for learning and teaching. The tool presented here - Hindi Shabhadamitra, developed using Hindi Wordnet for Hindi language learning, is one such e-learning tool. It has been developed as a teaching and learning aid suitable for formal school based curriculum and informal setup for self learning users. Besides vocabulary, it also provides word based grammar along with images and pronunciation for better learning and retention. This aid demonstrates that how a rich lexical resource like wordnet can be systematically remodeled for practical usage in the educational domain.",
}
@inproceedings{fung-etal-2017-nlptea,
    title = "{NLPTEA} 2017 Shared Task {--} {C}hinese Spelling Check",
    author = "Fung, Gabriel  and
      Debosschere, Maxime  and
      Wang, Dingmin  and
      Li, Bo  and
      Zhu, Jia  and
      Wong, Kam-Fai",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA} 2017)",
    month = dec,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/W17-5905",
    pages = "29--34",
    abstract = "This paper provides an overview along with our findings of the Chinese Spelling Check shared task at NLPTEA 2017. The goal of this task is to develop a computer-assisted system to automatically diagnose typing errors in traditional Chinese sentences written by students. We defined six types of errors which belong to two categories. Given a sentence, the system should detect where the errors are, and for each detected error determine its type and provide correction suggestions. We designed, constructed, and released a benchmark dataset for this task.",
}
@inproceedings{yeh-etal-2017-chinese,
    title = "{C}hinese Spelling Check based on N-gram and String Matching Algorithm",
    author = "Yeh, Jui-Feng  and
      Chang, Li-Ting  and
      Liu, Chan-Yi  and
      Hsu, Tsung-Wei",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA} 2017)",
    month = dec,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/W17-5906",
    pages = "35--38",
    abstract = "This paper presents a Chinese spelling check approach based on language models combined with string match algorithm to treat the problems resulted from the influence caused by Cantonese mother tone. N-grams first used to detecting the probability of sentence constructed by the writers, a string matching algorithm called Knuth-Morris-Pratt (KMP) Algorithm is used to detect and correct the error. According to the experimental results, the proposed approach can detect the error and provide the corresponding correction.",
}
@inproceedings{zhao-etal-2017-n,
    title = "N-gram Model for {C}hinese Grammatical Error Diagnosis",
    author = "Zhao, Jianbo  and
      Liu, Hao  and
      Bao, Zuyi  and
      Bai, Xiaopeng  and
      Li, Si  and
      Lin, Zhiqing",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA} 2017)",
    month = dec,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/W17-5907",
    pages = "39--44",
    abstract = "Detection and correction of Chinese grammatical errors have been two of major challenges for Chinese automatic grammatical error diagnosis.This paper presents an N-gram model for automatic detection and correction of Chinese grammatical errors in NLPTEA 2017 task. The experiment results show that the proposed method is good at correction of Chinese grammatical errors.",
}
@inproceedings{horbach-etal-2017-influence,
    title = "The Influence of Spelling Errors on Content Scoring Performance",
    author = "Horbach, Andrea  and
      Ding, Yuning  and
      Zesch, Torsten",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA} 2017)",
    month = dec,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/W17-5908",
    pages = "45--53",
    abstract = "Spelling errors occur frequently in educational settings, but their influence on automatic scoring is largely unknown. We therefore investigate the influence of spelling errors on content scoring performance using the example of the ASAP corpus. We conduct an annotation study on the nature of spelling errors in the ASAP dataset and utilize these finding in machine learning experiments that measure the influence of spelling errors on automatic content scoring. Our main finding is that scoring methods using both token and character n-gram features are robust against spelling errors up to the error frequency in ASAP.",
}
@inproceedings{mizumoto-nagata-2017-analyzing,
    title = "Analyzing the Impact of Spelling Errors on {POS}-Tagging and Chunking in Learner {E}nglish",
    author = "Mizumoto, Tomoya  and
      Nagata, Ryo",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA} 2017)",
    month = dec,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/W17-5909",
    pages = "54--58",
    abstract = "Part-of-speech (POS) tagging and chunking have been used in tasks targeting learner English; however, to the best our knowledge, few studies have evaluated their performance and no studies have revealed the causes of POS-tagging/chunking errors in detail. Therefore, we investigate performance and analyze the causes of failure. We focus on spelling errors that occur frequently in learner English. We demonstrate that spelling errors reduced POS-tagging performance by 0.23{\%} owing to spelling errors, and that a spell checker is not necessary for POS-tagging/chunking of learner English.",
}
@inproceedings{zampieri-etal-2017-complex,
    title = "Complex Word Identification: Challenges in Data Annotation and System Performance",
    author = "Zampieri, Marcos  and
      Malmasi, Shervin  and
      Paetzold, Gustavo  and
      Specia, Lucia",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA} 2017)",
    month = dec,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/W17-5910",
    pages = "59--63",
    abstract = "This paper revisits the problem of complex word identification (CWI) following up the SemEval CWI shared task. We use ensemble classifiers to investigate how well computational methods can discriminate between complex and non-complex words. Furthermore, we analyze the classification performance to understand what makes lexical complexity challenging. Our findings show that most systems performed poorly on the SemEval CWI dataset, and one of the reasons for that is the way in which human annotation was performed.",
}
@inproceedings{shioda-etal-2017-suggesting,
    title = "Suggesting Sentences for {ESL} using Kernel Embeddings",
    author = "Shioda, Kent  and
      Komachi, Mamoru  and
      Ikeya, Rue  and
      Mochihashi, Daichi",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA} 2017)",
    month = dec,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/W17-5911",
    pages = "64--68",
    abstract = "Sentence retrieval is an important NLP application for English as a Second Language (ESL) learners. ESL learners are familiar with web search engines, but generic web search results may not be adequate for composing documents in a specific domain. However, if we build our own search system specialized to a domain, it may be subject to the data sparseness problem. Recently proposed word2vec partially addresses the data sparseness problem, but fails to extract sentences relevant to queries owing to the modeling of the latent intent of the query. Thus, we propose a method of retrieving example sentences using kernel embeddings and N-gram windows. This method implicitly models latent intent of query and sentences, and alleviates the problem of noisy alignment. Our results show that our method achieved higher precision in sentence retrieval for ESL in the domain of a university press release corpus, as compared to a previous unsupervised method used for a semantic textual similarity task.",
}
@inproceedings{bedi-etal-2017-event,
    title = "Event Timeline Generation from History Textbooks",
    author = "Bedi, Harsimran  and
      Patil, Sangameshwar  and
      Hingmire, Swapnil  and
      Palshikar, Girish",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA} 2017)",
    month = dec,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/W17-5912",
    pages = "69--77",
    abstract = "Event timeline serves as the basic structure of history, and it is used as a disposition of key phenomena in studying history as a subject in secondary school. In order to enable a student to understand a historical phenomenon as a series of connected events, we present a system for automatic event timeline generation from history textbooks. Additionally, we propose Message Sequence Chart (MSC) and time-map based visualization techniques to visualize an event timeline. We also identify key computational challenges in developing natural language processing based applications for history textbooks.",
}
