[{"id":"W19-4401","title":"The many dimensions of algorithmic fairness in educational applications","authors":["Loukina, Anastassia","Madnani, Nitin","Zechner, Klaus"],"emails":["aloukina@ets.org","nmadnani@ets.org","kzechner@ets.org"],"author_id":["anastassia-loukina","nitin-madnani","klaus-zechner"],"abstract":"The issues of algorithmic fairness and bias have recently featured prominently in many publications highlighting the fact that training the algorithms for maximum performance may often result in predictions that are biased against various groups. Educational applications based on NLP and speech processing technologies often combine multiple complex machine learning algorithms and are thus vulnerable to the same sources of bias as other machine learning systems. Yet such systems can have high impact on people{'}s lives especially when deployed as part of high-stakes tests. In this paper we discuss different definitions of fairness and possible ways to apply them to educational applications. We then use simulated and real data to consider how test-takers{'} native language backgrounds can affect their automated scores on an English language proficiency assessment. We illustrate that total fairness may not be achievable and that different definitions of fairness may require different solutions.","pages":"1--10","doi":"10.18653\/v1\/W19-4401","url":"https:\/\/www.aclweb.org\/anthology\/W19-4401","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4402","title":"Predicting the Difficulty of Multiple Choice Questions in a High-stakes Medical Exam","authors":["Ha, Le An","Yaneva, Victoria","Baldwin, Peter","Mee, Janet"],"emails":["ha.l.a@wlv.ac.uk","vyaneva@nbme.org","pbaldwin@nbme.org","jmee@nbme.org"],"author_id":["le-an-ha","victoria-yaneva","peter-baldwin","janet-mee"],"abstract":"Predicting the construct-relevant difficulty of Multiple-Choice Questions (MCQs) has the potential to reduce cost while maintaining the quality of high-stakes exams. In this paper, we propose a method for estimating the difficulty of MCQs from a high-stakes medical exam, where all questions were deliberately written to a common reading level. To accomplish this, we extract a large number of linguistic features and embedding types, as well as features quantifying the difficulty of the items for an automatic question-answering system. The results show that the proposed approach outperforms various baselines with a statistically significant difference. Best results were achieved when using the full feature set, where embeddings had the highest predictive power, followed by linguistic features. An ablation study of the various types of linguistic features suggested that information from all levels of linguistic processing contributes to predicting item difficulty, with features related to semantic ambiguity and the psycholinguistic properties of words having a slightly higher importance. Owing to its generic nature, the presented approach has the potential to generalize over other exams containing MCQs.","pages":"11--20","doi":"10.18653\/v1\/W19-4402","url":"https:\/\/www.aclweb.org\/anthology\/W19-4402","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4403","title":"An Intelligent Testing Strategy for Vocabulary Assessment of {C}hinese Second Language Learners","authors":["Zhou, Wei","Hu, Renfen","Sun, Feipeng","Huang, Ronghuai"],"emails":["zhouwei@bnu.edu.cn","irishu@bnu.edu.cn","sunfeipeng@blcu.edu.cn","huangrh@bnu.edu.cn"],"author_id":["wei-zhou","renfen-hu","feipeng-sun","ronghuai-huang"],"abstract":"Vocabulary is one of the most important parts of language competence. Testing of vocabulary knowledge is central to research on reading and language. However, it usually costs a large amount of time and human labor to build an item bank and to test large number of students. In this paper, we propose a novel testing strategy by combining automatic item generation (AIG) and computerized adaptive testing (CAT) in vocabulary assessment for Chinese L2 learners. Firstly, we generate three types of vocabulary questions by modeling both the vocabulary knowledge and learners{'} writing error data. After evaluation and calibration, we construct a balanced item pool with automatically generated items, and implement a three-parameter computerized adaptive test. We conduct manual item evaluation and online student tests in the experiments. The results show that the combination of AIG and CAT can construct test items efficiently and reduce test cost significantly. Also, the test result of CAT can provide valuable feedback to AIG algorithms.","pages":"21--29","doi":"10.18653\/v1\/W19-4403","url":"https:\/\/www.aclweb.org\/anthology\/W19-4403","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4404","title":"Computationally Modeling the Impact of Task-Appropriate Language Complexity and Accuracy on Human Grading of {G}erman Essays","authors":["Weiss, Zarah","Riemenschneider, Anja","Schr{\\\"o}ter, Pauline","Meurers, Detmar"],"emails":["zweiss@sfs.uni-tuebingen.de","riemenan@iqb.hu-berlin.de","schrotpa@iqb.hu-berlin.de","dm@sfs.uni-tuebingen.de"],"author_id":["zarah-weiss","anja-riemenschneider","pauline-schroter","detmar-meurers"],"abstract":"Computational linguistic research on the language complexity of student writing typically involves human ratings as a gold standard. However, educational science shows that teachers find it difficult to identify and cleanly separate accuracy, different aspects of complexity, contents, and structure. In this paper, we therefore explore the use of computational linguistic methods to investigate how task-appropriate complexity and accuracy relate to the grading of overall performance, content performance, and language performance as assigned by teachers. Based on texts written by students for the official school-leaving state examination (Abitur), we show that teachers successfully assign higher language performance grades to essays with higher task-appropriate language complexity and properly separate this from content scores. Yet, accuracy impacts teacher assessment for all grading rubrics, also the content score, overemphasizing the role of accuracy. Our analysis is based on broad computational linguistic modeling of German language complexity and an innovative theory- and data-driven feature aggregation method inferring task-appropriate language complexity.","pages":"30--45","doi":"10.18653\/v1\/W19-4404","url":"https:\/\/www.aclweb.org\/anthology\/W19-4404","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4405","title":"Analysing Rhetorical Structure as a Key Feature of Summary Coherence","authors":["{\\v{S}}najder, Jan","Sladoljev-Agejev, Tamara","Koli{\\'c} Vehovec, Svjetlana"],"emails":["jan.snajder@fer.hr","tagejev@efzg.hr","skolic@ffri.hr"],"author_id":["jan-snajder","tamara-sladoljev-agejev","svjetlana-kolic-vehovec"],"abstract":"We present a model for automatic scoring of coherence based on comparing the rhetorical structure (RS) of college student summaries in L2 (English) against expert summaries. Coherence is conceptualised as a construct consisting of the rhetorical relation and its arguments. Comparison with expert-assigned scores shows that RS scores correlate with both cohesion and coherence. Furthermore, RS scores improve the accuracy of a regression model for cohesion score prediction.","pages":"46--51","doi":"10.18653\/v1\/W19-4405","url":"https:\/\/www.aclweb.org\/anthology\/W19-4405","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4406","title":"The {BEA}-2019 Shared Task on Grammatical Error Correction","authors":["Bryant, Christopher","Felice, Mariano","Andersen, {\\O}istein E.","Briscoe, Ted"],"emails":["cjb255@cam.ac.uk","mf501@cam.ac.uk","oa223@cam.ac.uk","ejb@cam.ac.uk"],"author_id":["christopher-bryant","mariano-felice","oistein-e-andersen","ted-briscoe"],"abstract":"This paper reports on the BEA-2019 Shared Task on Grammatical Error Correction (GEC). As with the CoNLL-2014 shared task, participants are required to correct all types of errors in test data. One of the main contributions of the BEA-2019 shared task is the introduction of a new dataset, the Write{\\&}Improve+LOCNESS corpus, which represents a wider range of native and learner English levels and abilities. Another contribution is the introduction of tracks, which control the amount of annotated data available to participants. Systems are evaluated in terms of ERRANT F{\\_}0.5, which allows us to report a much wider range of performance statistics. The competition was hosted on Codalab and remains open for further submissions on the blind test set.","pages":"52--75","doi":"10.18653\/v1\/W19-4406","url":"https:\/\/www.aclweb.org\/anthology\/W19-4406","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4407","title":"A Benchmark Corpus of {E}nglish Misspellings and a Minimally-supervised Model for Spelling Correction","authors":["Flor, Michael","Fried, Michael","Rozovskaya, Alla"],"emails":["mflor@ets.org","mjf314@gmail.com","arozovskaya@qc.cuny.edu"],"author_id":["michael-flor","michael-fried","alla-rozovskaya"],"abstract":"Spelling correction has attracted a lot of attention in the NLP community. However, models have been usually evaluated on artificiallycreated or proprietary corpora. A publiclyavailable corpus of authentic misspellings, annotated in context, is still lacking. To address this, we present and release an annotated data set of 6,121 spelling errors in context, based on a corpus of essays written by English language learners. We also develop a minimallysupervised context-aware approach to spelling correction. It achieves strong results on our data: 88.12{\\%} accuracy. This approach can also train with a minimal amount of annotated data (performance reduced by less than 1{\\%}). Furthermore, this approach allows easy portability to new domains. We evaluate our model on data from a medical domain and demonstrate that it rivals the performance of a model trained and tuned on in-domain data.","pages":"76--86","doi":"10.18653\/v1\/W19-4407","url":"https:\/\/www.aclweb.org\/anthology\/W19-4407","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4408","title":"Artificial Error Generation with Fluency Filtering","authors":["Qiu, Mengyang","Park, Jungyeul"],"emails":["mengyang@buffalo.edu","jungyeul@buffalo.edu"],"author_id":["mengyang-qiu","jungyeul-park"],"abstract":"The quantity and quality of training data plays a crucial role in grammatical error correction (GEC). However, due to the fact that obtaining human-annotated GEC data is both time-consuming and expensive, several studies have focused on generating artificial error sentences to boost training data for grammatical error correction, and shown significantly better performance. The present study explores how fluency filtering can affect the quality of artificial errors. By comparing artificial data filtered by different levels of fluency, we find that artificial error sentences with low fluency can greatly facilitate error correction, while high fluency errors introduce more noise.","pages":"87--91","doi":"10.18653\/v1\/W19-4408","url":"https:\/\/www.aclweb.org\/anthology\/W19-4408","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4409","title":"Regression or classification? Automated Essay Scoring for {N}orwegian","authors":["Johan Berggren, Stig","Rama, Taraka","{\\O}vrelid, Lilja"],"emails":["stigjb@gmail.com","taraka.kasi@gmail.com","liljao@ifi.uio.no"],"author_id":["stig-johan-berggren","taraka-rama","lilja-ovrelid"],"abstract":"In this paper we present first results for the task of Automated Essay Scoring for Norwegian learner language. We analyze a number of properties of this task experimentally and assess (i) the formulation of the task as either regression or classification, (ii) the use of various non-neural and neural machine learning architectures with various types of input representations, and (iii) applying multi-task learning for joint prediction of essay scoring and native language identification. We find that a GRU-based attention model trained in a single-task setting performs best at the AES task.","pages":"92--102","doi":"10.18653\/v1\/W19-4409","url":"https:\/\/www.aclweb.org\/anthology\/W19-4409","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4410","title":"Context is Key: Grammatical Error Detection with Contextual Word Representations","authors":["Bell, Samuel","Yannakoudakis, Helen","Rei, Marek"],"emails":["sjb326@cam.ac.uk","helen.yannakoudakis@cl.cam.ac.uk","marek.rei@cl.cam.ac.uk"],"author_id":["samuel-bell","helen-yannakoudakis","marek-rei"],"abstract":"Grammatical error detection (GED) in non- native writing requires systems to identify a wide range of errors in text written by lan- guage learners. Error detection as a purely supervised task can be challenging, as GED datasets are limited in size and the label dis- tributions are highly imbalanced. Contextual- ized word representations offer a possible so- lution, as they can efficiently capture composi- tional information in language and can be opti- mized on large amounts of unsupervised data. In this paper, we perform a systematic com- parison of ELMo, BERT and Flair embeddings (Peters et al., 2017; Devlin et al., 2018; Akbik et al., 2018) on a range of public GED datasets, and propose an approach to effectively inte- grate such representations in current methods, achieving a new state of the art on GED. We further analyze the strengths and weaknesses of different contextual embeddings for the task at hand, and present detailed analyses of their impact on different types of errors.","pages":"103--115","doi":"10.18653\/v1\/W19-4410","url":"https:\/\/www.aclweb.org\/anthology\/W19-4410","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4411","title":"How to account for mispellings: Quantifying the benefit of character representations in neural content scoring models","authors":["Riordan, Brian","Flor, Michael","Pugh, Robert"],"emails":["briordan@ets.org","mflor@ets.org","robert.pugh@coursehero.com"],"author_id":["brian-riordan","michael-flor","robert-pugh"],"abstract":"Character-based representations in neural models have been claimed to be a tool to overcome spelling variation in in word token-based input. We examine this claim in neural models for content scoring. We formulate precise hypotheses about the possible effects of adding character representations to word-based models and test these hypotheses on large-scale real world content scoring datasets. We find that, while character representations may provide small performance gains in general, their effectiveness in accounting for spelling variation may be limited. We show that spelling correction can provide larger gains than character representations, and that spelling correction improves the performance of models with character representations. With these insights, we report a new state of the art on the ASAP-SAS content scoring dataset.","pages":"116--126","doi":"10.18653\/v1\/W19-4411","url":"https:\/\/www.aclweb.org\/anthology\/W19-4411","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4412","title":"The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction","authors":["Alikaniotis, Dimitris","Raheja, Vipul","Tetreault, Joel"],"emails":["dimitris.alikaniotis@grammarly.com","vipul.raheja@grammarly.com","joel.tetreault@grammarly.com"],"author_id":["dimitris-alikaniotis","vipul-raheja","joel-tetreault"],"abstract":"Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.","pages":"127--133","doi":"10.18653\/v1\/W19-4412","url":"https:\/\/www.aclweb.org\/anthology\/W19-4412","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4413","title":"(Almost) Unsupervised Grammatical Error Correction using Synthetic Comparable Corpus","authors":["Katsumata, Satoru","Komachi, Mamoru"],"emails":["katsumata-satoru@ed.tmu.ac.jp","komachi@tmu.ac.jp"],"author_id":["satoru-katsumata","mamoru-komachi"],"abstract":"We introduce unsupervised techniques based on phrase-based statistical machine translation for grammatical error correction (GEC) trained on a pseudo learner corpus created by Google Translation. We verified our GEC system through experiments on a low resource track of the shared task at BEA2019. As a result, we achieved an F0.5 score of 28.31 points with the test data.","pages":"134--138","doi":"10.18653\/v1\/W19-4413","url":"https:\/\/www.aclweb.org\/anthology\/W19-4413","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4414","title":"Learning to combine Grammatical Error Corrections","authors":["Kantor, Yoav","Katz, Yoav","Choshen, Leshem","Cohen-Karlik, Edo","Liberman, Naftali","Toledo, Assaf","Menczel, Amir","Slonim, Noam"],"emails":["yoavka@il.ibm.com","katz@il.ibm.com","leshem.choshen@il.ibm.com","edo.cohen@ibm.com","naftali.liberman@ibm.com","assaf.toledo@ibm.com","amir.menczel@il.ibm.com","noams@il.ibm.com"],"author_id":["yoav-kantor","yoav-katz","leshem-choshen","edo-cohen-karlik","naftali-liberman","assaf-toledo","amir-menczel","noam-slonim"],"abstract":"The field of Grammatical Error Correction (GEC) has produced various systems to deal with focused phenomena or general text editing. We propose an automatic way to combine black-box systems. Our method automatically detects the strength of a system or the combination of several systems per error type, improving precision and recall while optimizing F-score directly. We show consistent improvement over the best standalone system in all the configurations tested. This approach also outperforms average ensembling of different RNN models with random initializations. In addition, we analyze the use of BERT for GEC - reporting promising results on this end. We also present a spellchecker created for this task which outperforms standard spellcheckers tested on the task of spellchecking. This paper describes a system submission to Building Educational Applications 2019 Shared Task: Grammatical Error Correction. Combining the output of top BEA 2019 shared task systems using our approach, currently holds the highest reported score in the open phase of the BEA 2019 shared task, improving F-0.5 score by 3.7 points over the best result reported.","pages":"139--148","doi":"10.18653\/v1\/W19-4414","url":"https:\/\/www.aclweb.org\/anthology\/W19-4414","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4415","title":"Erroneous data generation for Grammatical Error Correction","authors":["Xu, Shuyao","Zhang, Jiehao","Chen, Jin","Qin, Long"],"emails":["xushuy@singsound.com","zhangjiehao@singsound.com","chenjin@singsound.com","qinlong@singsound.com"],"author_id":["shuyao-xu","jiehao-zhang","jin-chen","long-qin"],"abstract":"It has been demonstrated that the utilization of a monolingual corpus in neural Grammatical Error Correction (GEC) systems can significantly improve the system performance. The previous state-of-the-art neural GEC system is an ensemble of four Transformer models pretrained on a large amount of Wikipedia Edits. The Singsound GEC system follows a similar approach but is equipped with a sophisticated erroneous data generating component. Our system achieved an F0:5 of 66.61 in the BEA 2019 Shared Task: Grammatical Error Correction. With our novel erroneous data generating component, the Singsound neural GEC system yielded an M2 of 63.2 on the CoNLL-2014 benchmark (8.4{\\%} relative improvement over the previous state-of-the-art system).","pages":"149--158","doi":"10.18653\/v1\/W19-4415","url":"https:\/\/www.aclweb.org\/anthology\/W19-4415","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4416","title":"The {LAIX} Systems in the {BEA}-2019 {GEC} Shared Task","authors":["Li, Ruobing","Wang, Chuan","Zha, Yefei","Yu, Yonghong","Guo, Shiman","Wang, Qiang","Liu, Yang","Lin, Hui"],"emails":["ruobing.li@liulishuo.com","","","","","","yang.liu@liulishuo.com","hui.lin@liulishuo.com"],"author_id":["ruobing-li","chuan-wang","yefei-zha","yonghong-yu","shiman-guo","qiang-wang","yang-liu-icsi","hui-lin"],"abstract":"In this paper, we describe two systems we developed for the three tracks we have participated in the BEA-2019 GEC Shared Task. We investigate competitive classification models with bi-directional recurrent neural networks (Bi-RNN) and neural machine translation (NMT) models. For different tracks, we use ensemble systems to selectively combine the NMT models, the classification models, and some rules, and demonstrate that an ensemble solution can effectively improve GEC performance over single systems. Our GEC systems ranked the first in the Unrestricted Track, and the third in both the Restricted Track and the Low Resource Track.","pages":"159--167","doi":"10.18653\/v1\/W19-4416","url":"https:\/\/www.aclweb.org\/anthology\/W19-4416","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4417","title":"The {CUED}{'}s Grammatical Error Correction Systems for {BEA}-2019","authors":["Stahlberg, Felix","Byrne, Bill"],"emails":["fs439@cam.ac.uk","wjb31@cam.ac.uk"],"author_id":["felix-stahlberg","bill-byrne"],"abstract":"We describe two entries from the Cambridge University Engineering Department to the BEA 2019 Shared Task on grammatical error correction. Our submission to the low-resource track is based on prior work on using finite state transducers together with strong neural language models. Our system for the restricted track is a purely neural system consisting of neural language models and neural machine translation models trained with back-translation and a combination of checkpoint averaging and fine-tuning {--} without the help of any additional tools like spell checkers. The latter system has been used inside a separate system combination entry in cooperation with the Cambridge University Computer Lab.","pages":"168--175","doi":"10.18653\/v1\/W19-4417","url":"https:\/\/www.aclweb.org\/anthology\/W19-4417","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4418","title":"The {AIP}-Tohoku System at the {BEA}-2019 Shared Task","authors":["Asano, Hiroki","Mita, Masato","Mizumoto, Tomoya","Suzuki, Jun"],"emails":["asano@ecei.tohoku.ac.jp","masato.mita@riken.jp","tomoya.mizumoto@riken.jp","jun.suzuki@ecei.tohoku.ac.jp"],"author_id":["hiroki-asano","masato-mita","tomoya-mizumoto","jun-suzuki"],"abstract":"We introduce the AIP-Tohoku grammatical error correction (GEC) system for the BEA-2019 shared task in Track 1 (Restricted Track) and Track 2 (Unrestricted Track) using the same system architecture. Our system comprises two key components: error generation and sentence-level error detection. In particular, GEC with sentence-level grammatical error detection is a novel and versatile approach, and we experimentally demonstrate that it significantly improves the precision of the base model. Our system is ranked 9th in Track 1 and 2nd in Track 2.","pages":"176--182","doi":"10.18653\/v1\/W19-4418","url":"https:\/\/www.aclweb.org\/anthology\/W19-4418","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4419","title":"{CUNI} System for the Building Educational Applications 2019 Shared Task: Grammatical Error Correction","authors":["N{\\'a}plava, Jakub","Straka, Milan"],"emails":["naplava@ufal.mff.cuni.cz","straka@ufal.mff.cuni.cz"],"author_id":["jakub-naplava","milan-straka"],"abstract":"Our submitted models are NMT systems based on the Transformer model, which we improve by incorporating several enhancements: applying dropout to whole source and target words, weighting target subwords, averaging model checkpoints, and using the trained model iteratively for correcting the intermediate translations. The system in the Restricted Track is trained on the provided corpora with oversampled {``}cleaner{''} sentences and reaches 59.39 F0.5 score on the test set. The system in the Low-Resource Track is trained from Wikipedia revision histories and reaches 44.13 F0.5 score. Finally, we finetune the system from the Low-Resource Track on restricted data and achieve 64.55 F0.5 score.","pages":"183--190","doi":"10.18653\/v1\/W19-4419","url":"https:\/\/www.aclweb.org\/anthology\/W19-4419","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4420","title":"Noisy Channel for Low Resource Grammatical Error Correction","authors":["Lacroix, Oph{\\'e}lie","Flachs, Simon","S{\\o}gaard, Anders"],"emails":["ola@siteimprove.com","sfl@siteimprove.com","soegaard@di.ku.dk"],"author_id":["ophelie-lacroix","simon-flachs","anders-sogaard"],"abstract":"This paper describes our contribution to the low-resource track of the BEA 2019 shared task on Grammatical Error Correction (GEC). Our approach to GEC builds on the theory of the noisy channel by combining a channel model and language model. We generate confusion sets from the Wikipedia edit history and use the frequencies of edits to estimate the channel model. Additionally, we use two pre-trained language models: 1) Google{'}s BERT model, which we fine-tune for specific error types and 2) OpenAI{'}s GPT-2 model, utilizing that it can operate with previous sentences as context. Furthermore, we search for the optimal combinations of corrections using beam search.","pages":"191--196","doi":"10.18653\/v1\/W19-4420","url":"https:\/\/www.aclweb.org\/anthology\/W19-4420","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4421","title":"The {BLCU} System in the {BEA} 2019 Shared Task","authors":["Yang, Liner","Wang, Chencheng"],"emails":["lineryang@gmail.com","hsamswang@gmail.com"],"author_id":["liner-yang","chencheng-wang"],"abstract":"This paper describes the BLCU Group submissions to the Building Educational Applications (BEA) 2019 Shared Task on Grammatical Error Correction (GEC). The task is to detect and correct grammatical errors that occurred in essays. We participate in 2 tracks including the Restricted Track and the Unrestricted Track. Our system is based on a Transformer model architecture. We integrate many effective methods proposed in recent years. Such as, Byte Pair Encoding, model ensemble, checkpoints average and spell checker. We also corrupt the public monolingual data to further improve the performance of the model. On the test data of the BEA 2019 Shared Task, our system yields F0.5 = 58.62 and 59.50, ranking twelfth and fourth respectively.","pages":"197--206","doi":"10.18653\/v1\/W19-4421","url":"https:\/\/www.aclweb.org\/anthology\/W19-4421","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4422","title":"{TMU} Transformer System Using {BERT} for Re-ranking at {BEA} 2019 Grammatical Error Correction on Restricted Track","authors":["Kaneko, Masahiro","Hotate, Kengo","Katsumata, Satoru","Komachi, Mamoru"],"emails":["","","",""],"author_id":["masahiro-kaneko","kengo-hotate","satoru-katsumata","mamoru-komachi"],"abstract":"We introduce our system that is submitted to the restricted track of the BEA 2019 shared task on grammatical error correction1 (GEC). It is essential to select an appropriate hypothesis sentence from the candidates list generated by the GEC model. A re-ranker can evaluate the naturalness of a corrected sentence us- ing language models trained on large corpora. On the other hand, these language models and language representations do not explicitly take into account the grammatical errors written by learners. Thus, it is not straightforward to utilize language representations trained from a large corpus, such as Bidirectional Encoder Representations from Transformers (BERT), in a form suitable for the learner{'}s grammatical errors. Therefore, we propose to fine- tune BERT on learner corpora with grammatical errors for re-ranking. The experimental results of the W{\\&}I+LOCNESS development dataset demonstrate that re-ranking using BERT can effectively improve the correction performance.","pages":"207--212","doi":"10.18653\/v1\/W19-4422","url":"https:\/\/www.aclweb.org\/anthology\/W19-4422","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4423","title":"A Neural Grammatical Error Correction System Built On Better Pre-training and Sequential Transfer Learning","authors":["Choe, Yo Joong","Ham, Jiyeon","Park, Kyubyong","Yoon, Yeoil"],"emails":["yj.c@kakaocorp.com","jiyeon.ham@kakaobrain.com","kyubyong.park@kakaobrain.com","yeoil.yoon@kakaobrain.com"],"author_id":["yo-joong-choe","jiyeon-ham","kyubyong-park","yeoil-yoon"],"abstract":"Grammatical error correction can be viewed as a low-resource sequence-to-sequence task, because publicly available parallel corpora are limited.To tackle this challenge, we first generate erroneous versions of large unannotated corpora using a realistic noising function. The resulting parallel corpora are sub-sequently used to pre-train Transformer models. Then, by sequentially applying transfer learning, we adapt these models to the domain and style of the test set. Combined with a context-aware neural spellchecker, our system achieves competitive results in both restricted and low resource tracks in ACL 2019 BEAShared Task. We release all of our code and materials for reproducibility.","pages":"213--227","doi":"10.18653\/v1\/W19-4423","url":"https:\/\/www.aclweb.org\/anthology\/W19-4423","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4424","title":"Neural and {FST}-based approaches to grammatical error correction","authors":["Yuan, Zheng","Stahlberg, Felix","Rei, Marek","Byrne, Bill","Yannakoudakis, Helen"],"emails":["zheng.yuan@cl.cam.ac.uk","fs439@eng.cam.ac.uk","marek.rei@cl.cam.ac.uk","bill.byrne@eng.cam.ac.uk","helen.yannakoudakis@cl.cam.ac.uk"],"author_id":["zheng-yuan","felix-stahlberg","marek-rei","bill-byrne","helen-yannakoudakis"],"abstract":"In this paper, we describe our submission to the BEA 2019 shared task on grammat- ical error correction. We present a system pipeline that utilises both error detection and correction models. The input text is first corrected by two complementary neural ma- chine translation systems: one using convo- lutional networks and multi-task learning, and another using a neural Transformer-based sys- tem. Training is performed on publicly avail- able data, along with artificial examples gener- ated through back-translation. The n-best lists of these two machine translation systems are then combined and scored using a finite state transducer (FST). Finally, an unsupervised re- ranking system is applied to the n-best output of the FST. The re-ranker uses a number of error detection features to re-rank the FST n- best list and identify the final 1-best correction hypothesis. Our system achieves 66.75{\\%} F 0.5 on error correction (ranking 4th), and 82.52{\\%} F 0.5 on token-level error detection (ranking 2nd) in the restricted track of the shared task.","pages":"228--239","doi":"10.18653\/v1\/W19-4424","url":"https:\/\/www.aclweb.org\/anthology\/W19-4424","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4425","title":"Improving Precision of Grammatical Error Correction with a Cheat Sheet","authors":["Qiu, Mengyang","Chen, Xuejiao","Liu, Maggie","Parvathala, Krishna","Patil, Apurva","Park, Jungyeul"],"emails":["mengyang@buffalo.edu","xuejiaoc@buffalo.edu","mliu22@buffalo.edu","leelasai@buffalo.edu","acpatil@buffalo.edu","jungyeul@buffalo.edu"],"author_id":["mengyang-qiu","xuejiao-chen","maggie-liu","krishna-parvathala","apurva-patil","jungyeul-park"],"abstract":"In this paper, we explore two approaches of generating error-focused phrases and examine whether these phrases can lead to better performance in grammatical error correction for the restricted track of BEA 2019 Shared Task on GEC. Our results show that phrases directly extracted from GEC corpora outperform phrases from statistical machine translation phrase table by a large margin. Appending error+context phrases to the original GEC corpora yields comparably high precision. We also explore the generation of artificial syntactic error sentences using error+context phrases for the unrestricted track. The additional training data greatly facilitates syntactic error correction (e.g., verb form) and contributes to better overall performance.","pages":"240--245","doi":"10.18653\/v1\/W19-4425","url":"https:\/\/www.aclweb.org\/anthology\/W19-4425","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4426","title":"Multi-headed Architecture Based on {BERT} for Grammatical Errors Correction","authors":["Didenko, Bohdan","Shaptala, Julia"],"emails":["bogdan@webspellchecker.net","julia@webspellchecker.net"],"author_id":["bohdan-didenko","julia-shaptala"],"abstract":"In this paper, we describe our approach to GEC using the BERT model for creation of encoded representation and some of our enhancements, namely, {``}Heads{''} are fully-connected networks which are used for finding the errors and later receive recommendation from the networks on dealing with a highlighted part of the sentence only. Among the main advantages of our solution is increasing the system productivity and lowering the time of processing while keeping the high accuracy of GEC results.","pages":"246--251","doi":"10.18653\/v1\/W19-4426","url":"https:\/\/www.aclweb.org\/anthology\/W19-4426","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4427","title":"Neural Grammatical Error Correction Systems with Unsupervised Pre-training on Synthetic Data","authors":["Grundkiewicz, Roman","Junczys-Dowmunt, Marcin","Heafield, Kenneth"],"emails":["rgrundki@inf.ed.ac.uk","marcinjd@microsoft.com","kheafiel@inf.ed.ac.uk"],"author_id":["roman-grundkiewicz","marcin-junczys-dowmunt","kenneth-heafield"],"abstract":"Considerable effort has been made to address the data sparsity problem in neural grammatical error correction. In this work, we propose a simple and surprisingly effective unsupervised synthetic error generation method based on confusion sets extracted from a spellchecker to increase the amount of training data. Synthetic data is used to pre-train a Transformer sequence-to-sequence model, which not only improves over a strong baseline trained on authentic error-annotated data, but also enables the development of a practical GEC system in a scenario where little genuine error-annotated data is available. The developed systems placed first in the BEA19 shared task, achieving 69.47 and 64.24 F$_{0.5}$ in the restricted and low-resource tracks respectively, both on the W{\\&}I+LOCNESS test set. On the popular CoNLL 2014 test set, we report state-of-the-art results of 64.16 M$^2$ for the submitted system, and 61.30 M$^2$ for the constrained system trained on the NUCLE and Lang-8 data.","pages":"252--263","doi":"10.18653\/v1\/W19-4427","url":"https:\/\/www.aclweb.org\/anthology\/W19-4427","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4428","title":"Evaluation of automatic collocation extraction methods for language learning","authors":["Bhalla, Vishal","Klimcikova, Klara"],"emails":["vishal.bhalla@tum.de","k.klimcikova@lmu.de"],"author_id":["vishal-bhalla","klara-klimcikova"],"abstract":"A number of methods have been proposed to automatically extract collocations, i.e., conventionalized lexical combinations, from text corpora. However, the attempts to evaluate and compare them with a specific application in mind lag behind. This paper compares three end-to-end resources for collocation learning, all of which used the same corpus but different methods. Adopting a gold-standard evaluation method, the results show that the method of dependency parsing outperforms regex-over-pos in collocation identification. The lexical association measures (AMs) used for collocation ranking perform about the same overall but differently for individual collocation types. Further analysis has also revealed that there are considerable differences between other commonly used AMs.","pages":"264--274","doi":"10.18653\/v1\/W19-4428","url":"https:\/\/www.aclweb.org\/anthology\/W19-4428","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4429","title":"Anglicized Words and Misspelled Cognates in Native Language Identification","authors":["Markov, Ilia","Nastase, Vivi","Strapparava, Carlo"],"emails":["ilia.markov@uantwerpen.be","nastase@cl.uni-heidelberg.de","strappa@fbk.eu"],"author_id":["ilia-markov","vivi-nastase","carlo-strapparava"],"abstract":"In this paper, we present experiments that estimate the impact of specific lexical choices of people writing in a second language (L2). In particular, we look at misspelled words that indicate lexical uncertainty on the part of the author, and separate them into three categories: misspelled cognates, {``}L2-ed{''} (in our case, anglicized) words, and all other spelling errors. We test the assumption that such errors contain clues about the native language of an essay{'}s author through the task of native language identification. The results of the experiments show that the information brought by each of these categories is complementary. We also note that while the distribution of such features changes with the proficiency level of the writer, their contribution towards native language identification remains significant at all levels.","pages":"275--284","doi":"10.18653\/v1\/W19-4429","url":"https:\/\/www.aclweb.org\/anthology\/W19-4429","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4430","title":"Linguistically-Driven Strategy for Concept Prerequisites Learning on {I}talian","authors":["Miaschi, Alessio","Alzetta, Chiara","Cardillo, Franco Alberto","Dell{'}Orletta, Felice"],"emails":["alessio.miaschi@phd.unipi.it","chiara.alzetta@edu.unige.it","francoalberto.cardillo@ilc.cnr.it","felice.dellorletta@ilc.cnr.it"],"author_id":["alessio-miaschi","chiara-alzetta","franco-alberto-cardillo","felice-dellorletta"],"abstract":"We present a new concept prerequisite learning method for Learning Object (LO) ordering that exploits only linguistic features extracted from textual educational resources. The method was tested in a cross- and in- domain scenario both for Italian and English. Additionally, we performed experiments based on a incremental training strategy to study the impact of the training set size on the classifier performances. The paper also introduces ITA-PREREQ, to the best of our knowledge the first Italian dataset annotated with prerequisite relations between pairs of educational concepts, and describe the automatic strategy devised to build it.","pages":"285--295","doi":"10.18653\/v1\/W19-4430","url":"https:\/\/www.aclweb.org\/anthology\/W19-4430","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4431","title":"Grammatical-Error-Aware Incorrect Example Retrieval System for Learners of {J}apanese as a Second Language","authors":["Arai, Mio","Kaneko, Masahiro","Komachi, Mamoru"],"emails":["","",""],"author_id":["mio-arai","masahiro-kaneko","mamoru-komachi"],"abstract":"Existing example retrieval systems do not include grammatically incorrect examples or present only a few examples, if any. Even if a retrieval system has a wide coverage of incorrect examples along with the correct counterpart, learners need to know whether their query includes errors or not. Considering the usability of retrieving incorrect examples, our proposed method uses a large-scale corpus and presents correct expressions along with incorrect expressions using a grammatical error detection system so that the learner do not need to be aware of how to search for the examples. Intrinsic and extrinsic evaluations indicate that our method improves accuracy of example sentence retrieval and quality of learner{'}s writing.","pages":"296--305","doi":"10.18653\/v1\/W19-4431","url":"https:\/\/www.aclweb.org\/anthology\/W19-4431","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4432","title":"Toward Automated Content Feedback Generation for Non-native Spontaneous Speech","authors":["Yoon, Su-Youn","Hsieh, Ching-Ni","Zechner, Klaus","Mulholland, Matthew","Wang, Yuan","Madnani, Nitin"],"emails":["syoon@ets.org","chsieh@ets.org","kzechner@ets.org","mmulholland@ets.org","ywang@ets.org","nmadnani@ets.org"],"author_id":["su-youn-yoon","ching-ni-hsieh","klaus-zechner","matthew-mulholland","yuan-wang","nitin-madnani"],"abstract":"In this study, we developed an automated algorithm to provide feedback about the specific content of non-native English speakers{'} spoken responses. The responses were spontaneous speech, elicited using integrated tasks where the language learners listened to and\/or read passages and integrated the core content in their spoken responses. Our models detected the absence of key points considered to be important in a spoken response to a particular test question, based on two different models: (a) a model using word-embedding based content features and (b) a state-of-the art short response scoring engine using traditional n-gram based features. Both models achieved a substantially improved performance over the majority baseline, and the combination of the two models achieved a significant further improvement. In particular, the models were robust to automated speech recognition (ASR) errors, and performance based on the ASR word hypotheses was comparable to that based on manual transcriptions. The accuracy and F-score of the best model for the questions included in the train set were 0.80 and 0.68, respectively. Finally, we discussed possible approaches to generating targeted feedback about the content of a language learner{'}s response, based on automatically detected missing key points.","pages":"306--315","doi":"10.18653\/v1\/W19-4432","url":"https:\/\/www.aclweb.org\/anthology\/W19-4432","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4433","title":"Analytic Score Prediction and Justification Identification in Automated Short Answer Scoring","authors":["Mizumoto, Tomoya","Ouchi, Hiroki","Isobe, Yoriko","Reisert, Paul","Nagata, Ryo","Sekine, Satoshi","Inui, Kentaro"],"emails":["tomoya.mizumoto@riken.jp","hiroki.ouchi@riken.jp","yoriko.isobe@riken.jp","paul.reisert@riken.jp","nagata-bea2019@ml.hyogo-u.ac.jp","satoshi.sekine@riken.jp","inui@ecei.tohoku.ac.jp"],"author_id":["tomoya-mizumoto","hiroki-ouchi","yoriko-isobe","paul-reisert","ryo-nagata","satoshi-sekine","kentaro-inui"],"abstract":"This paper provides an analytical assessment of student short answer responses with a view to potential benefits in pedagogical contexts. We first propose and formalize two novel analytical assessment tasks: analytic score prediction and justification identification, and then provide the first dataset created for analytic short answer scoring research. Subsequently, we present a neural baseline model and report our extensive empirical results to demonstrate how our dataset can be used to explore new and intriguing technical challenges in short answer scoring. The dataset is publicly available for research purposes.","pages":"316--325","doi":"10.18653\/v1\/W19-4433","url":"https:\/\/www.aclweb.org\/anthology\/W19-4433","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4434","title":"Content Customization for Micro Learning using Human Augmented {AI} Techniques","authors":["Shah, Ayush","Abuelsaad, Tamer","Ahn, Jae-Wook","Dey, Prasenjit","Kokku, Ravi","Sharma Mittal, Ruhi","Vempaty, Aditya","Sharma, Mourvi"],"emails":["ayush13027@iiitd.ac.in","tamera@us.ibm.com","jaewook.ahn@us.ibm.com","prasenjit.dey@in.ibm.com","ravi.kokku@gmail.com","mourvi@gmail.com","aditya.vempaty@ieee.org","ruhi.sharma@in.ibm.com"],"author_id":["ayush-shah","tamer-abuelsaad","jae-wook-ahn","prasenjit-dey","ravi-kokku","ruhi-sharma-mittal","aditya-vempaty","mourvi-sharma"],"abstract":"Visual content has been proven to be effective for micro-learning compared to other media. In this paper, we discuss leveraging this observation in our efforts to build audio-visual content for young learners{'} vocabulary learning. We attempt to tackle two major issues in the process of traditional visual curation tasks. Generic learning videos do not necessarily satisfy the unique context of a learner and\/or an educator, and hence may not result in maximal learning outcomes. Also, manual video curation by educators is a highly labor-intensive process. To this end, we present a customizable micro-learning audio-visual content curation tool that is designed to reduce the human (educator) effort in creating just-in-time learning videos from a textual description (learning script). This provides educators with control of the content while preparing the learning scripts, and in turn can also be customized to capture the desired learning objectives and outcomes. As a use case, we automatically generate learning videos with British National Corpus{'} (BNC) frequently spoken vocabulary words and evaluate them with experts. They positively recommended the generated learning videos with an average rating of 4.25 on a Likert scale of 5 points. The inter-annotator agreement between the experts for the video quality was substantial (Fleiss Kappa=0.62) with an overall agreement of 81{\\%}.","pages":"326--335","doi":"10.18653\/v1\/W19-4434","url":"https:\/\/www.aclweb.org\/anthology\/W19-4434","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4435","title":"Curio {S}mart{C}hat : A system for Natural Language Question Answering for Self-Paced K-12 Learning","authors":["Raamadhurai, Srikrishna","Baker, Ryan","Poduval, Vikraman"],"emails":["","",""],"author_id":["srikrishna-raamadhurai","ryan-baker","vikraman-poduval"],"abstract":"During learning, students often have questions which they would benefit from responses to in real time. In class, a student can ask a question to a teacher. During homework, or even in class if the student is shy, it can be more difficult to receive a rapid response. In this work, we introduce Curio SmartChat, an automated question answering system for middle school Science topics. Our system has now been used by around 20,000 students who have so far asked over 100,000 questions. We present data on the challenge created by students{'} grammatical errors and spelling mistakes, and discuss our system{'}s approach and degree of effectiveness at disambiguating questions that the system is initially unsure about. We also discuss the prevalence of student {``}small talk{''} not related to science topics, the pluses and minuses of this behavior, and how a system should respond to these conversational acts. We conclude with discussions and point to directions for potential future work.","pages":"336--342","doi":"10.18653\/v1\/W19-4435","url":"https:\/\/www.aclweb.org\/anthology\/W19-4435","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4436","title":"Supporting content evaluation of student summaries by Idea Unit embedding","authors":["Gecchele, Marcello","Yamada, Hiroaki","Tokunaga, Takenobu","Sawaki, Yasuyo"],"emails":["marcello.gecchele@pm.me","yamada.h.ax@m.titech.ac.jp","take@c.titech.ac.jp","ysawaki@waseda.jp"],"author_id":["marcello-gecchele","hiroaki-yamada","takenobu-tokunaga","yasuyo-sawaki"],"abstract":"This paper discusses the computer-assisted content evaluation of summaries. We propose a method to make a correspondence between the segments of the source text and its summary. As a unit of the segment, we adopt {``}Idea Unit (IU){''} which is proposed in Applied Linguistics. Introducing IUs enables us to make a correspondence even for the sentences that contain multiple ideas. The IU correspondence is made based on the similarity between vector representations of IU. An evaluation experiment with two source texts and 20 summaries showed that the proposed method is more robust against rephrased expressions than the conventional ROUGE-based baselines. Also, the proposed method outperformed the baselines in recall. We im-plemented the proposed method in a GUI tool{``}Segment Matcher{''} that aids teachers to estab-lish a link between corresponding IUs acrossthe summary and source text.","pages":"343--348","doi":"10.18653\/v1\/W19-4436","url":"https:\/\/www.aclweb.org\/anthology\/W19-4436","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4437","title":"On Understanding the Relation between Expert Annotations of Text Readability and Target Reader Comprehension","authors":["Vajjala, Sowmya","Lucic, Ivana"],"emails":["sowmya.vajjala@nrc-cnrc.gc.ca","ilucic@iastate.edu"],"author_id":["sowmya-vajjala","ivana-lucic1"],"abstract":"Automatic readability assessment aims to ensure that readers read texts that they can comprehend. However, computational models are typically trained on texts created from the perspective of the text writer, not the target reader. There is little experimental research on the relationship between expert annotations of readability, reader{'}s language proficiency, and different levels of reading comprehension. To address this gap, we conducted a user study in which over a 100 participants read texts of different reading levels and answered questions created to test three forms of comprehension. Our results indicate that more than readability annotation or reader proficiency, it is the type of comprehension question asked that shows differences between reader responses - inferential questions were difficult for users of all levels of proficiency across reading levels. The data collected from this study will be released with this paper, which will, for the first time, provide a collection of 45 reader bench marked texts to evaluate readability assessment systems developed for adult learners of English. It can also potentially be useful for the development of question generation approaches in intelligent tutoring systems research.","pages":"349--359","doi":"10.18653\/v1\/W19-4437","url":"https:\/\/www.aclweb.org\/anthology\/W19-4437","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4438","title":"Measuring Text Complexity for {I}talian as a Second Language Learning Purposes","authors":["Forti, Luciana","Milani, Alfredo","Piersanti, Luisa","Santarelli, Filippo","Santucci, Valentino","Spina, Stefania"],"emails":["luciana.forti@unistrapg.it","alfredo.milani@unipg.it","luisa.piersanti@studenti.unipg.it","filippo.santarelli@unicam.it","valentino.santucci@unistrapg.it","stefania.spina@unistrapg.it"],"author_id":["luciana-forti","alfredo-milani","luisa-piersanti","filippo-santarelli","valentino-santucci","stefania-spina"],"abstract":"The selection of texts for second language learning purposes typically relies on teachers{'} and test developers{'} individual judgment of the observable qualitative properties of a text. Little or no consideration is generally given to the quantitative dimension within an evidence-based framework of reproducibility. This study aims to fill the gap by evaluating the effectiveness of an automatic tool trained to assess text complexity in the context of Italian as a second language learning. A dataset of texts labeled by expert test developers was used to evaluate the performance of three classifier models (decision tree, random forest, and support vector machine), which were trained using linguistic features measured quantitatively and extracted from the texts. The experimental analysis provided satisfactory results, also in relation to which kind of linguistic trait contributed the most to the final outcome.","pages":"360--368","doi":"10.18653\/v1\/W19-4438","url":"https:\/\/www.aclweb.org\/anthology\/W19-4438","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4439","title":"Simple Construction of Mixed-Language Texts for Vocabulary Learning","authors":["Renduchintala, Adithya","Koehn, Philipp","Eisner, Jason"],"emails":["adi.r@jhu.edu","phi@jhu.edu","jason@cs.jhu.edu"],"author_id":["adithya-renduchintala","philipp-koehn","jason-eisner"],"abstract":"We present a machine foreign-language teacher that takes documents written in a student{'}s native language and detects situations where it can replace words with their foreign glosses such that new foreign vocabulary can be learned simply through reading the resulting mixed-language text. We show that it is possible to design such a machine teacher without any supervised data from (human) students. We accomplish this by modifying a cloze language model to incrementally learn new vocabulary items, and use this language model as a proxy for the word guessing and learning ability of real students. Our machine foreign-language teacher decides which subset of words to replace by consulting this language model. We evaluate three variants of our student proxy language models through a study on Amazon Mechanical Turk (MTurk). We find that MTurk {``}students{''} were able to guess the meanings of foreign words introduced by the machine teacher with high accuracy for both function words as well as content words in two out of the three models. In addition, we show that students are able to retain their knowledge about the foreign words after they finish reading the document.","pages":"369--379","doi":"10.18653\/v1\/W19-4439","url":"https:\/\/www.aclweb.org\/anthology\/W19-4439","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4440","title":"Analyzing Linguistic Complexity and Accuracy in Academic Language Development of {G}erman across Elementary and Secondary School","authors":["Weiss, Zarah","Meurers, Detmar"],"emails":["zweiss@sfs.uni-tuebingen.de","dm@sfs.uni-tuebingen.de"],"author_id":["zarah-weiss","detmar-meurers"],"abstract":"We track the development of writing complexity and accuracy in German students{'} early academic language development from first to eighth grade. Combining an empirically broad approach to linguistic complexity with the high-quality error annotation included in the Karlsruhe Children{'}s Text corpus (Lavalley et al. 2015) used, we construct models of German academic language development that successfully identify the student{'}s grade level. We show that classifiers for the early years rely more on accuracy development, whereas development in secondary school is better characterized by increasingly complex language in all domains: linguistic system, language use, and human sentence processing characteristics. We demonstrate the generalizability and robustness of models using such a broad complexity feature set across writing topics.","pages":"380--393","doi":"10.18653\/v1\/W19-4440","url":"https:\/\/www.aclweb.org\/anthology\/W19-4440","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4441","title":"Content Modeling for Automated Oral Proficiency Scoring System","authors":["Yoon, Su-Youn","Lee, Chong Min"],"emails":["syoon@ets.org","clee001@ets.org"],"author_id":["su-youn-yoon","chungmin-lee"],"abstract":"We developed an automated oral proficiency scoring system for non-native English speakers{'} spontaneous speech. Automated systems that score holistic proficiency are expected to assess a wide range of performance categories, and the content is one of the core performance categories. In order to assess the quality of the content, we trained a Siamese convolutional neural network (CNN) to model the semantic relationship between key points generated by experts and a test response. The correlation between human scores and Siamese CNN scores was comparable to human-human agreement (r=0.63), and it was higher than the baseline content features. The inclusion of Siamese CNN-based feature to the existing state-of-the-art automated scoring model achieved a small but statistically significant improvement. However, the new model suffered from score inflation for long atypical responses with serious content issues. We investigated the reasons of this score inflation by analyzing the associations with linguistic features and identifying areas strongly associated with the score errors.","pages":"394--401","doi":"10.18653\/v1\/W19-4441","url":"https:\/\/www.aclweb.org\/anthology\/W19-4441","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4442","title":"Learning Outcomes and Their Relatedness in a Medical Curriculum","authors":["Mondal, Sneha","Dhamecha, Tejas","Godbole, Shantanu","Pathak, Smriti","Mendoza, Red","Wijayarathna, K Gayathri","Zary, Nabil","Saha, Swarnadeep","Chetlur, Malolan"],"emails":["snmondal@in.ibm.com","tidhamecha@in.ibm.com","shantanugodbole@in.ibm.com","smriti.pathak@kcl.ac.uk","mendozaredante@ntu.edu.sg","gwkumari@ntu.edu.sg","nabil.zary@ntu.edu.sg","swarnads@in.ibm.com","mchetlur@in.ibm.com"],"author_id":["sneha-mondal","tejas-dhamecha","shantanu-godbole","smriti-pathak","red-mendoza","k-gayathri-wijayarathna","nabil-zary","swarnadeep-saha","malolan-chetlur"],"abstract":"A typical medical curriculum is organized in a hierarchy of instructional objectives called Learning Outcomes (LOs); a few thousand LOs span five years of study. Gaining a thorough understanding of the curriculum requires learners to recognize and apply related LOs across years, and across different parts of the curriculum. However, given the large scope of the curriculum, manually labeling related LOs is tedious, and almost impossible to scale. In this paper, we build a system that learns relationships between LOs, and we achieve up to human-level performance in the LO relationship extraction task. We then present an application where the proposed system is employed to build a map of related LOs and Learning Resources (LRs) pertaining to a virtual patient case. We believe that our system can help medical students grasp the curriculum better, within classroom as well as in Intelligent Tutoring Systems (ITS) settings.","pages":"402--411","doi":"10.18653\/v1\/W19-4442","url":"https:\/\/www.aclweb.org\/anthology\/W19-4442","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4443","title":"Measuring text readability with machine comprehension: a pilot study","authors":["Benzahra, Marc","Yvon, Fran{\\c{c}}ois"],"emails":["marc.benzahra@limsi.fr","francois.yvon@limsi.fr"],"author_id":["marc-benzahra","francois-yvon"],"abstract":"This article studies the relationship between text readability indice and automatic machine understanding systems. Our hypothesis is that the simpler a text is, the better it should be understood by a machine. We thus expect to a strong correlation between readability levels on the one hand, and performance of automatic reading systems on the other hand. We test this hypothesis with several understanding systems based on language models of varying strengths, measuring this correlation on two corpora of journalistic texts. Our results suggest that this correlation is rather small that existing comprehension systems are far to reproduce the gradual improvement of their performance on texts of decreasing complexity.","pages":"412--422","doi":"10.18653\/v1\/W19-4443","url":"https:\/\/www.aclweb.org\/anthology\/W19-4443","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4444","title":"Metaphors in Text Simplification: To change or not to change, that is the question","authors":["Clausen, Yulia","Nastase, Vivi"],"emails":["yulia.clausen@uni-potsdam.de","nastase@cl.uni-heidelberg.de"],"author_id":["yulia-clausen","vivi-nastase"],"abstract":"We present an analysis of metaphors in news text simplification. Using features that capture general and metaphor specific characteristics, we test whether we can automatically identify which metaphors will be changed or preserved, and whether there are features that have different predictive power for metaphors or literal words. The experiments show that the Age of Acquisition is the most distinctive feature for both metaphors and literal words. Features that capture Imageability and Concreteness are useful when used alone, but within the full set of features they lose their impact. Frequency of use seems to be the best feature to differentiate metaphors that should be changed and those to be preserved.","pages":"423--434","doi":"10.18653\/v1\/W19-4444","url":"https:\/\/www.aclweb.org\/anthology\/W19-4444","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4445","title":"Application of an Automatic Plagiarism Detection System in a Large-scale Assessment of {E}nglish Speaking Proficiency","authors":["Wang, Xinhao","Evanini, Keelan","Mulholland, Matthew","Qian, Yao","Bruno, James V."],"emails":["xwang002@ets.org","kevanini@ets.org","mmulholland@ets.org","yqian@ets.org","jbruno@ets.org"],"author_id":["xinhao-wang","keelan-evanini","matthew-mulholland","yao-qian","james-v-bruno"],"abstract":"This study aims to build an automatic system for the detection of plagiarized spoken responses in the context of an assessment of English speaking proficiency for non-native speakers. Classification models were trained to distinguish between plagiarized and non-plagiarized responses with two different types of features: text-to-text content similarity measures, which are commonly used in the task of plagiarism detection for written documents, and speaking proficiency measures, which were specifically designed for spontaneous speech and extracted using an automated speech scoring system. The experiments were first conducted on a large data set drawn from an operational English proficiency assessment across multiple years, and the best classifier on this heavily imbalanced data set resulted in an F1-score of 0.761 on the plagiarized class. This system was then validated on operational responses collected from a single administration of the assessment and achieved a recall of 0.897. The results indicate that the proposed system can potentially be used to improve the validity of both human and automated assessment of non-native spoken English.","pages":"435--443","doi":"10.18653\/v1\/W19-4445","url":"https:\/\/www.aclweb.org\/anthology\/W19-4445","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4446","title":"Equity Beyond Bias in Language Technologies for Education","authors":["Mayfield, Elijah","Madaio, Michael","Prabhumoye, Shrimai","Gerritsen, David","McLaughlin, Brittany","Dixon-Rom{\\'a}n, Ezekiel","Black, Alan W"],"emails":["elijah@cmu.edu","","","","","",""],"author_id":["elijah-mayfield","michael-madaio","shrimai-prabhumoye","david-gerritsen","brittany-mclaughlin","ezekiel-dixon-roman","alan-w-black"],"abstract":"There is a long record of research on equity in schools. As machine learning researchers begin to study fairness and bias in earnest, language technologies in education have an unusually strong theoretical and applied foundation to build on. Here, we introduce concepts from culturally relevant pedagogy and other frameworks for teaching and learning, identifying future work on equity in NLP. We present case studies in a range of topics like intelligent tutoring systems, computer-assisted language learning, automated essay scoring, and sentiment analysis in classrooms, and provide an actionable agenda for research.","pages":"444--460","doi":"10.18653\/v1\/W19-4446","url":"https:\/\/www.aclweb.org\/anthology\/W19-4446","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4447","title":"From Receptive to Productive: Learning to Use Confusing Words through Automatically Selected Example Sentences","authors":["Huang, Chieh-Yang","Huang, Yi-Ting","Chen, MeiHua","Ku, Lun-Wei"],"emails":["ythuang@iis.sinica.edu.tw","chiehyang@psu.edu","mhchen@thu.edu.tw","lwku@iis.sinica.edu.tw"],"author_id":["chieh-yang-huang","yi-ting-huang","meihua-chen","lun-wei-ku"],"abstract":"Knowing how to use words appropriately has been a key to improving language proficiency. Previous studies typically discuss how students learn receptively to select the correct candidate from a set of confusing words in the fill-in-the-blank task where specific context is given. In this paper, we go one step further, assisting students to learn to use confusing words appropriately in a productive task: sentence translation. We leverage the GiveMe-Example system, which suggests example sentences for each confusing word, to achieve this goal. In this study, students learn to differentiate the confusing words by reading the example sentences, and then choose the appropriate word(s) to complete the sentence translation task. Results show students made substantial progress in terms of sentence structure. In addition, highly proficient students better managed to learn confusing words. In view of the influence of the first language on learners, we further propose an effective approach to improve the quality of the suggested sentences.","pages":"461--471","doi":"10.18653\/v1\/W19-4447","url":"https:\/\/www.aclweb.org\/anthology\/W19-4447","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4448","title":"Equipping Educational Applications with Domain Knowledge","authors":["Sakakini, Tarek","Gong, Hongyu","Lee, Jong Yoon","Schloss, Robert","Xiong, JinJun","Bhat, Suma"],"emails":["sakakini@illinois.edu","hgong6@illinois.edu","jlee642@illinois.edu","rschloss@us.ibm.com","jinjun@us.ibm.com","spbhat2@illinois.edu"],"author_id":["tarek-sakakini","hongyu-gong","jong-yoon-lee","robert-schloss","jinjun-xiong","suma-bhat"],"abstract":"One of the challenges of building natural language processing (NLP) applications for education is finding a large domain-specific corpus for the subject of interest (e.g., history or science). To address this challenge, we propose a tool, Dexter, that extracts a subject-specific corpus from a heterogeneous corpus, such as Wikipedia, by relying on a small seed corpus and distributed document representations. We empirically show the impact of the generated corpus on language modeling, estimating word embeddings, and consequently, distractor generation, resulting in better performances than while using a general domain corpus, a heuristically constructed domain-specific corpus, and a corpus generated by a popular system: BootCaT.","pages":"472--477","doi":"10.18653\/v1\/W19-4448","url":"https:\/\/www.aclweb.org\/anthology\/W19-4448","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4449","title":"The Unbearable Weight of Generating Artificial Errors for Grammatical Error Correction","authors":["Htut, Phu Mon","Tetreault, Joel"],"emails":["pmh330@nyu.edu","joel.tetreault@grammarly.com"],"author_id":["phu-mon-htut","joel-tetreault"],"abstract":"In this paper, we investigate the impact of using 4 recent neural models for generating artificial errors to help train the neural grammatical error correction models. We conduct a battery of experiments on the effect of data size, models, and comparison with a rule-based approach.","pages":"478--483","doi":"10.18653\/v1\/W19-4449","url":"https:\/\/www.aclweb.org\/anthology\/W19-4449","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4450","title":"Automated Essay Scoring with Discourse-Aware Neural Models","authors":["Nadeem, Farah","Nguyen, Huy","Liu, Yang","Ostendorf, Mari"],"emails":["farahn@uw.edu","huy.nguyen@liulishuo.com","yang.liu@liulishuo.com","ostendor@uw.edu"],"author_id":["farah-nadeem","huy-nguyen","yang-liu-icsi","mari-ostendorf"],"abstract":"Automated essay scoring systems typically rely on hand-crafted features to predict essay quality, but such systems are limited by the cost of feature engineering. Neural networks offer an alternative to feature engineering, but they typically require more annotated data. This paper explores network structures, contextualized embeddings and pre-training strategies aimed at capturing discourse characteristics of essays. Experiments on three essay scoring tasks show benefits from all three strategies in different combinations, with simpler architectures being more effective when less training data is available.","pages":"484--493","doi":"10.18653\/v1\/W19-4450","url":"https:\/\/www.aclweb.org\/anthology\/W19-4450","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4451","title":"Modeling language learning using specialized Elo rating","authors":["Hou, Jue","Maximilian, Koppatz","Hoya Quecedo, Jos{\\'e} Mar{\\'\\i}a","Stoyanova, Nataliya","Yangarber, Roman"],"emails":["jue.hou@helsinki.fi","koppatz.maximilian@helsinki.fi","jos{\\'e}.hoya quecedo@helsinki.fi","nataliya.stoyanova@helsinki.fi","roman.yangarber@helsinki.fi"],"author_id":["jue-hou","koppatz-maximilian","jose-maria-hoya-quecedo","nataliya-stoyanova","roman-yangarber"],"abstract":"Automatic assessment of the proficiency levels of the learner is a critical part of Intelligent Tutoring Systems. We present methods for assessment in the context of language learning. We use a specialized Elo formula used in conjunction with educational data mining. We simultaneously obtain ratings for the proficiency of the learners and for the difficulty of the linguistic concepts that the learners are trying to master. From the same data we also learn a graph structure representing a domain model capturing the relations among the concepts. This application of Elo provides ratings for learners and concepts which correlate well with subjective proficiency levels of the learners and difficulty levels of the concepts.","pages":"494--506","doi":"10.18653\/v1\/W19-4451","url":"https:\/\/www.aclweb.org\/anthology\/W19-4451","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"},{"id":"W19-4452","title":"Rubric Reliability and Annotation of Content and Argument in Source-Based Argument Essays","authors":["Gao, Yanjun","Driban, Alex","Xavier McManus, Brennan","Musi, Elena","Davies, Patricia","Muresan, Smaranda","Passonneau, Rebecca J."],"emails":["yug125@cse.psu.edu","akd524@cse.psu.edu","bm2530@columbia.edu","usi@liverpool.ac.uk","pdavies@pmu.edu.sa","smara@columbia.edu","rjp49@cse.psu.edu"],"author_id":["yanjun-gao","alex-driban","brennan-xavier-mcmanus","elena-musi","patricia-davies","smaranda-muresan","rebecca-j-passonneau"],"abstract":"We present a unique dataset of student source-based argument essays to facilitate research on the relations between content, argumentation skills, and assessment. Two classroom writing assignments were given to college students in a STEM major, accompanied by a carefully designed rubric. The paper presents a reliability study of the rubric, showing it to be highly reliable, and initial annotation on content and argumentation annotation of the essays.","pages":"507--518","doi":"10.18653\/v1\/W19-4452","url":"https:\/\/www.aclweb.org\/anthology\/W19-4452","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"}]