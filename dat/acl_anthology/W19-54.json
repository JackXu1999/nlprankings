[{"id":"W19-5401","title":"Findings of the {WMT} 2019 Shared Tasks on Quality Estimation","authors":["Fonseca, Erick","Yankovskaya, Lisa","Martins, Andr{\\'e} F. T.","Fishel, Mark","Federmann, Christian"],"emails":["erick.fonseca@lx.it.pt","lisa.yankovskaya@ut.ee","andre.martins@unbabel.com","fishel@ut.ee","chrife@microsoft.com"],"author_id":["erick-fonseca","lisa-yankovskaya","andre-f-t-martins","mark-fishel","christian-federmann"],"abstract":"We report the results of the WMT19 shared task on Quality Estimation, i.e. the task of predicting the quality of the output of machine translation systems given just the source text and the hypothesis translations. The task includes estimation at three granularity levels: word, sentence and document. A novel addition is evaluating sentence-level QE against human judgments: in other words, designing MT metrics that do not need a reference translation. This year we include three language pairs, produced solely by neural machine translation systems. Participating teams from eleven institutions submitted a variety of systems to different task variants and language pairs.","pages":"1--10","doi":"10.18653\/v1\/W19-5401","url":"https:\/\/www.aclweb.org\/anthology\/W19-5401","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5402","title":"Findings of the {WMT} 2019 Shared Task on Automatic Post-Editing","authors":["Chatterjee, Rajen","Federmann, Christian","Negri, Matteo","Turchi, Marco"],"emails":["","","",""],"author_id":["rajen-chatterjee","christian-federmann","matteo-negri","marco-turchi"],"abstract":"We present the results from the 5th round of the WMT task on MT Automatic Post-Editing. The task consists in automatically correcting the output of a {``}black-box{''} machine translation system by learning from human corrections. Keeping the same general evaluation setting of the previous four rounds, this year we focused on two language pairs (English-German and English-Russian) and on domain-specific data (In-formation Technology). For both the language directions, MT outputs were produced by neural systems unknown to par-ticipants. Seven teams participated in the English-German task, with a total of 18 submitted runs. The evaluation, which was performed on the same test set used for the 2018 round, shows a slight progress in APE technology: 4 teams achieved better results than last year{'}s winning system, with improvements up to -0.78 TER and +1.23 BLEU points over the baseline. Two teams participated in theEnglish-Russian task submitting 2 runs each. On this new language direction, characterized by a higher quality of the original translations, the task proved to be particularly challenging. None of the submitted runs improved the very high results of the strong system used to produce the initial translations(16.16 TER, 76.20 BLEU).","pages":"11--28","doi":"10.18653\/v1\/W19-5402","url":"https:\/\/www.aclweb.org\/anthology\/W19-5402","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5403","title":"Findings of the {WMT} 2019 Biomedical Translation Shared Task: Evaluation for {MEDLINE} Abstracts and Biomedical Terminologies","authors":["Bawden, Rachel","Bretonnel Cohen, Kevin","Grozea, Cristian","Jimeno Yepes, Antonio","Kittner, Madeleine","Krallinger, Martin","Mah, Nancy","Neveol, Aurelie","Neves, Mariana","Soares, Felipe","Siu, Amy","Verspoor, Karin","Vicente Navarro, Maika"],"emails":["","","","","","","","","","","","",""],"author_id":["rachel-bawden","kevin-bretonnel-cohen","cristian-grozea","antonio-jimeno-yepes","madeleine-kittner","martin-krallinger","nancy-mah","aurelie-neveol","mariana-neves","felipe-soares","amy-siu","karin-verspoor","maika-vicente-navarro"],"abstract":"In the fourth edition of the WMT Biomedical Translation task, we considered a total of six languages, namely Chinese (zh), English (en), French (fr), German (de), Portuguese (pt), and Spanish (es). We performed an evaluation of automatic translations for a total of 10 language directions, namely, zh\/en, en\/zh, fr\/en, en\/fr, de\/en, en\/de, pt\/en, en\/pt, es\/en, and en\/es. We provided training data based on MEDLINE abstracts for eight of the 10 language pairs and test sets for all of them. In addition to that, we offered a new sub-task for the translation of terms in biomedical terminologies for the en\/es language direction. Higher BLEU scores (close to 0.5) were obtained for the es\/en, en\/es and en\/pt test sets, as well as for the terminology sub-task. After manual validation of the primary runs, some submissions were judged to be better than the reference translations, for instance, for de\/en, en\/es and es\/en.","pages":"29--53","doi":"10.18653\/v1\/W19-5403","url":"https:\/\/www.aclweb.org\/anthology\/W19-5403","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5404","title":"Findings of the {WMT} 2019 Shared Task on Parallel Corpus Filtering for Low-Resource Conditions","authors":["Koehn, Philipp","Guzm{\\'a}n, Francisco","Chaudhary, Vishrav","Pino, Juan"],"emails":["phi@jhu.edu","fguzman@fb.com","vishrav@fb.com","juancarabina@fb.com"],"author_id":["philipp-koehn","francisco-guzman","vishrav-chaudhary","juan-pino"],"abstract":"Following the WMT 2018 Shared Task on Parallel Corpus Filtering, we posed the challenge of assigning sentence-level quality scores for very noisy corpora of sentence pairs crawled from the web, with the goal of sub-selecting 2{\\%} and 10{\\%} of the highest-quality data to be used to train machine translation systems. This year, the task tackled the low resource condition of Nepali-English and Sinhala-English. Eleven participants from companies, national research labs, and universities participated in this task.","pages":"54--72","doi":"10.18653\/v1\/W19-5404","url":"https:\/\/www.aclweb.org\/anthology\/W19-5404","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5405","title":"{RTM} Stacking Results for Machine Translation Performance Prediction","authors":["Bi{\\c{c}}ici, Ergun"],"emails":["ergun.bicici@boun.edu.tr"],"author_id":["ergun-bicici"],"abstract":"We obtain new results using referential translation machines with increased number of learning models in the set of results that are stacked to obtain a better mixture of experts prediction. We combine features extracted from the word-level predictions with the sentence- or document-level features, which significantly improve the results on the training sets but decrease the test set results.","pages":"73--77","doi":"10.18653\/v1\/W19-5405","url":"https:\/\/www.aclweb.org\/anthology\/W19-5405","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5406","title":"Unbabel{'}s Participation in the {WMT}19 Translation Quality Estimation Shared Task","authors":["Kepler, Fabio","Tr{\\'e}nous, Jonay","Treviso, Marcos","Vera, Miguel","G{\\'o}is, Ant{\\'o}nio","Farajian, M. Amin","Lopes, Ant{\\'o}nio V.","Martins, Andr{\\'e} F. T."],"emails":["kepler@unbabel.com","sony@unbabel.com","marcosvtreviso@gmail.com","miguel.vera@unbabel.com","antonio.gois@unbabel.com","amin@unbabel.com","antonio.lopes@unbabel.com","andre.martins@unbabel.com"],"author_id":["fabio-kepler","jonay-trenous","marcos-treviso","miguel-vera","antonio-gois","m-amin-farajian","antonio-v-lopes","andre-f-t-martins"],"abstract":"We present the contribution of the Unbabel team to the WMT 2019 Shared Task on Quality Estimation. We participated on the word, sentence, and document-level tracks, encompassing 3 language pairs: English-German, English-Russian, and English-French. Our submissions build upon the recent OpenKiwi framework: We combine linear, neural, and predictor-estimator systems with new transfer learning approaches using BERT and XLM pre-trained models. We compare systems individually and propose new ensemble techniques for word and sentence-level predictions. We also propose a simple technique for converting word labels into document-level predictions. Overall, our submitted systems achieve the best results on all tracks and language pairs by a considerable margin.","pages":"78--84","doi":"10.18653\/v1\/W19-5406","url":"https:\/\/www.aclweb.org\/anthology\/W19-5406","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5407","title":"{QE} {BERT}: Bilingual {BERT} Using Multi-task Learning for Neural Quality Estimation","authors":["Kim, Hyun","Lim, Joon-Ho","Kim, Hyun-Ki","Na, Seung-Hoon"],"emails":["h.kim@etri.re.kr","joonho.lim@etri.re.kr","hkk@etri.re.kr","nash@jbnu.ac.kr"],"author_id":["hyun-kim","joon-ho-lim","hyun-ki-kim","seung-hoon-na"],"abstract":"For translation quality estimation at word and sentence levels, this paper presents a novel approach based on BERT that recently has achieved impressive results on various natural language processing tasks. Our proposed model is re-purposed BERT for the translation quality estimation and uses multi-task learning for the sentence-level task and word-level subtasks (i.e., source word, target word, and target gap). Experimental results on Quality Estimation shared task of WMT19 show that our systems show competitive results and provide significant improvements over the baseline.","pages":"85--89","doi":"10.18653\/v1\/W19-5407","url":"https:\/\/www.aclweb.org\/anthology\/W19-5407","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5408","title":"{MIPT} System for World-Level Quality Estimation","authors":["Mosyagin, Mikhail","Logacheva, Varvara"],"emails":["mosyagin.md@phystech.edu","logacheva.vk@mipt.ru"],"author_id":["mikhail-mosyagin","varvara-logacheva"],"abstract":"We explore different model architectures for the WMT 19 shared task on word-level quality estimation of automatic translation. We start with a model similar to Shef-bRNN, which we modify by using conditional random fields for sequence labelling. Additionally, we use a different approach for labelling gaps and source words. We further develop this model by including features from different sources such as BERT, baseline features for the task and transformer encoders. We evaluate the performance of our models on the English-German dataset for the corresponding shared task.","pages":"90--94","doi":"10.18653\/v1\/W19-5408","url":"https:\/\/www.aclweb.org\/anthology\/W19-5408","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5409","title":"{NJU} Submissions for the {WMT}19 Quality Estimation Shared Task","authors":["Qi, Hou"],"emails":["houq@nlp.nju.edu.cn"],"author_id":["hou-qi"],"abstract":"In this paper, we describe the submissions of the team from Nanjing University for the WMT19 sentence-level Quality Estimation (QE) shared task on English-German language pair. We develop two approaches based on a two-stage neural QE model consisting of a feature extractor and a quality estimator. More specifically, one of the proposed approaches employs the translation knowledge between the two languages from two different translation directions; while the other one employs extra monolingual knowledge from both source and target sides, obtained by pre-training deep self-attention networks. To efficiently train these two-stage models, a joint learning training method is applied. Experiments show that the ensemble model of the above two models achieves the best results on the benchmark dataset of the WMT17 sentence-level QE shared task and obtains competitive results in WMT19, ranking 3rd out of 10 submissions.","pages":"95--100","doi":"10.18653\/v1\/W19-5409","url":"https:\/\/www.aclweb.org\/anthology\/W19-5409","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5410","title":"Quality Estimation and Translation Metrics via Pre-trained Word and Sentence Embeddings","authors":["Yankovskaya, Elizaveta","T{\\\"a}ttar, Andre","Fishel, Mark"],"emails":["lisa.yankovskaya@ut.ee","andre.tattar@ut.ee","fishel@ut.ee"],"author_id":["elizaveta-yankovskaya","andre-tattar","mark-fishel"],"abstract":"We propose the use of pre-trained embeddings as features of a regression model for sentence-level quality estimation of machine translation. In our work we combine freely available BERT and LASER multilingual embeddings to train a neural-based regression model. In the second proposed method we use as an input features not only pre-trained embeddings, but also log probability of any machine translation (MT) system. Both methods are applied to several language pairs and are evaluated both as a classical quality estimation system (predicting the HTER score) as well as an MT metric (predicting human judgements of translation quality).","pages":"101--105","doi":"10.18653\/v1\/W19-5410","url":"https:\/\/www.aclweb.org\/anthology\/W19-5410","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5411","title":"{SOURCE}: {SOUR}ce-Conditional Elmo-style Model for Machine Translation Quality Estimation","authors":["Zhou, Junpei","Zhang, Zhisong","Hu, Zecong"],"emails":["junpeiz@andrew.cmu.edu","zhisongz@andrew.cmu.edu","zeconghu@andrew.cmu.edu"],"author_id":["junpei-zhou","zhisong-zhang","zecong-hu"],"abstract":"Quality estimation (QE) of machine translation (MT) systems is a task of growing importance. It reduces the cost of post-editing, allowing machine-translated text to be used in formal occasions. In this work, we describe our submission system in WMT 2019 sentence-level QE task. We mainly explore the utilization of pre-trained translation models in QE and adopt a bi-directional translation-like strategy. The strategy is similar to ELMo, but additionally conditions on source sentences. Experiments on WMT QE dataset show that our strategy, which makes the pre-training slightly harder, can bring improvements for QE. In WMT-2019 QE task, our system ranked in the second place on En-De NMT dataset and the third place on En-Ru NMT dataset.","pages":"106--111","doi":"10.18653\/v1\/W19-5411","url":"https:\/\/www.aclweb.org\/anthology\/W19-5411","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5412","title":"Transformer-based Automatic Post-Editing Model with Joint Encoder and Multi-source Attention of Decoder","authors":["Lee, WonKee","Shin, Jaehun","Lee, Jong-Hyeok"],"emails":["","",""],"author_id":["wonkee-lee","jaehun-shin","jong-hyeok-lee"],"abstract":"This paper describes POSTECH{'}s submission to the WMT 2019 shared task on Automatic Post-Editing (APE). In this paper, we propose a new multi-source APE model by extending Transformer. The main contributions of our study are that we 1) reconstruct the encoder to generate a joint representation of translation (mt) and its src context, in addition to the conventional src encoding and 2) suggest two types of multi-source attention layers to compute attention between two outputs of the encoder and the decoder state in the decoder. Furthermore, we train our model by applying various teacher-forcing ratios to alleviate exposure bias. Finally, we adopt the ensemble technique across variations of our model. Experiments on the WMT19 English-German APE data set show improvements in terms of both TER and BLEU scores over the baseline. Our primary submission achieves -0.73 in TER and +1.49 in BLEU compare to the baseline.","pages":"112--117","doi":"10.18653\/v1\/W19-5412","url":"https:\/\/www.aclweb.org\/anthology\/W19-5412","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5413","title":"Unbabel{'}s Submission to the {WMT}2019 {APE} Shared Task: {BERT}-Based Encoder-Decoder for Automatic Post-Editing","authors":["Lopes, Ant{\\'o}nio V.","Farajian, M. Amin","Correia, Gon{\\c{c}}alo M.","Tr{\\'e}nous, Jonay","Martins, Andr{\\'e} F. T."],"emails":["antonio.lopes@unbabel.com","amin@unbabel.com","goncalo.correia@lx.it.pt","sony@unbabel.com","andre.martins@unbabel.com"],"author_id":["antonio-v-lopes","m-amin-farajian","goncalo-m-correia","jonay-trenous","andre-f-t-martins"],"abstract":"This paper describes Unbabel{'}s submission to the WMT2019 APE Shared Task for the English-German language pair. Following the recent rise of large, powerful, pre-trained models, we adapt the BERT pretrained model to perform Automatic Post-Editing in an encoder-decoder framework. Analogously to dual-encoder architectures we develop a BERT-based encoder-decoder (BED) model in which a single pretrained BERT encoder receives both the source src and machine translation mt strings. Furthermore, we explore a conservativeness factor to constrain the APE system to perform fewer edits. As the official results show, when trained on a weighted combination of in-domain and artificial training data, our BED system with the conservativeness penalty improves significantly the translations of a strong NMT system by -0.78 and +1.23 in terms of TER and BLEU, respectively. Finally, our submission achieves a new state-of-the-art, ex-aequo, in English-German APE of NMT.","pages":"118--123","doi":"10.18653\/v1\/W19-5413","url":"https:\/\/www.aclweb.org\/anthology\/W19-5413","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5414","title":"{USAAR}-{DFKI} {--} The Transference Architecture for {E}nglish{--}{G}erman Automatic Post-Editing","authors":["Pal, Santanu","Xu, Hongfei","Herbig, Nico","Kr{\\\"u}ger, Antonio","van Genabith, Josef"],"emails":["santanu.pal@uni-saarland.de","","genabith@dfki.de","","josef.vangenabith@uni-saarland.de"],"author_id":["santanu-pal","hongfei-xu","nico-herbig","antonio-kruger","josef-van-genabith"],"abstract":"In this paper we present an English{--}German Automatic Post-Editing (APE) system called transference, submitted to the APE Task organized at WMT 2019. Our transference model is based on a multi-encoder transformer architecture. Unlike previous approaches, it (i) uses a transformer encoder block for src, (ii) followed by a transformer decoder block, but without masking, for self-attention on mt, which effectively acts as second encoder combining src {--}{\\textgreater} mt, and (iii) feeds this representation into a final decoder block generating pe. Our model improves over the raw black-box neural machine translation system by 0.9 and 1.0 absolute BLEU points on the WMT 2019 APE development and test set. Our submission ranked 3rd, however compared to the two top systems, performance differences are not statistically significant.","pages":"124--131","doi":"10.18653\/v1\/W19-5414","url":"https:\/\/www.aclweb.org\/anthology\/W19-5414","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5415","title":"{APE} through Neural and Statistical {MT} with Augmented Data. {ADAPT}\/{DCU} Submission to the {WMT} 2019 {APE} Shared Task","authors":["Shterionov, Dimitar","Wagner, Joachim","do Carmo, F{\\'e}lix"],"emails":["lastname@adaptcentre.ie","",""],"author_id":["dimitar-shterionov","joachim-wagner","felix-do-carmo"],"abstract":"Automatic post-editing (APE) can be reduced to a machine translation (MT) task, where the source is the output of a specific MT system and the target is its post-edited variant. However, this approach does not consider context information that can be found in the original source of the MT system. Thus a better approach is to employ multi-source MT, where two input sequences are considered {--} the one being the original source and the other being the MT output. Extra context information can be introduced in the form of extra tokens that identify certain global property of a group of segments, added as a prefix or a suffix to each segment. Successfully applied in domain adaptation of MT as well as on APE, this technique deserves further attention. In this work we investigate multi-source neural APE (or NPE) systems with training data which has been augmented with two types of extra context tokens. We experiment with authentic and synthetic data provided by WMT 2019 and submit our results to the APE shared task. We also experiment with using statistical machine translation (SMT) methods for APE. While our systems score bellow the baseline, we consider this work a step towards understanding the added value of extra context in the case of APE.","pages":"132--138","doi":"10.18653\/v1\/W19-5415","url":"https:\/\/www.aclweb.org\/anthology\/W19-5415","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5416","title":"Effort-Aware Neural Automatic Post-Editing","authors":["Tebbifakhr, Amirhossein","Negri, Matteo","Turchi, Marco"],"emails":["atebbifakhr@fbk.eu","negri@fbk.eu","turchi@fbk.eu"],"author_id":["amirhossein-tebbifakhr","matteo-negri","marco-turchi"],"abstract":"For this round of the WMT 2019 APE shared task, our submission focuses on addressing the {``}over-correction{''} problem in APE. Over-correction occurs when the APE system tends to rephrase an already correct MT output, and the resulting sentence is penalized by a reference-based evaluation against human post-edits. Our intuition is that this problem can be prevented by informing the system about the predicted quality of the MT output or, in other terms, the expected amount of needed corrections. For this purpose, following the common approach in multilingual NMT, we prepend a special token to the beginning of both the source text and the MT output indicating the required amount of post-editing. Following the best submissions to the WMT 2018 APE shared task, our backbone architecture is based on multi-source Transformer to encode both the MT output and the corresponding source text. We participated both in the English-German and English-Russian subtasks. In the first subtask, our best submission improved the original MT output quality up to +0.98 BLEU and -0.47 TER. In the second subtask, where the higher quality of the MT output increases the risk of over-correction, none of our submitted runs was able to improve the MT output.","pages":"139--144","doi":"10.18653\/v1\/W19-5416","url":"https:\/\/www.aclweb.org\/anthology\/W19-5416","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5417","title":"{U}d{S} Submission for the {WMT} 19 Automatic Post-Editing Task","authors":["Xu, Hongfei","Liu, Qiuhui","van Genabith, Josef"],"emails":["hfxunlp@foxmail.com","liuqiuhui@cmos.chinamobile.com","genabith@dfki.de"],"author_id":["hongfei-xu","qiuhui-liu","josef-van-genabith"],"abstract":"In this paper, we describe our submission to the English-German APE shared task at WMT 2019. We utilize and adapt an NMT architecture originally developed for exploiting context information to APE, implement this in our own transformer model and explore joint training of the APE task with a de-noising encoder.","pages":"145--150","doi":"10.18653\/v1\/W19-5417","url":"https:\/\/www.aclweb.org\/anthology\/W19-5417","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5418","title":"Terminology-Aware Segmentation and Domain Feature for the {WMT}19 Biomedical Translation Task","authors":["Carrino, Casimiro Pio","Rafieian, Bardia","Costa-juss{\\`a}, Marta R.","Fonollosa, Jos{\\'e} A. R."],"emails":["casimiro.pio.carrino@upc.edu","bardia.rafieian@upc.edu","marta.ruiz@upc.edu","jose.fonollosa@upc.edu"],"author_id":["casimiro-pio-carrino","bardia-rafieian","marta-r-costa-jussa","jose-a-r-fonollosa"],"abstract":"In this work, we give a description of the TALP-UPC systems submitted for the WMT19 Biomedical Translation Task. Our proposed strategy is NMT model-independent and relies only on one ingredient, a biomedical terminology list. We first extracted such a terminology list by labelling biomedical words in our training dataset using the BabelNet API. Then, we designed a data preparation strategy to insert the terms information at a token level. Finally, we trained the Transformer model with this terms-informed data. Our best-submitted system ranked 2nd and 3rd for Spanish-English and English-Spanish translation directions, respectively.","pages":"151--155","doi":"10.18653\/v1\/W19-5418","url":"https:\/\/www.aclweb.org\/anthology\/W19-5418","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5419","title":"Exploring Transfer Learning and Domain Data Selection for the Biomedical Translation","authors":["Hira, Noor-e-","Abdul Rauf, Sadaf","Kiani, Kiran","Zafar, Ammara","Nawaz, Raheel"],"emails":["noorehira94@gmail.com","sadaf.abdulrauf@limsi.fr","kianithe1@gmail.com","ammarazafar11@gmail.com",""],"author_id":["noor-e-hira","sadaf-abdul-rauf1","kiran-kiani","ammara-zafar","raheel-nawaz"],"abstract":"Transfer Learning and Selective data training are two of the many approaches being extensively investigated to improve the quality of Neural Machine Translation systems. This paper presents a series of experiments by applying transfer learning and selective data training for participation in the Bio-medical shared task of WMT19. We have used Information Retrieval to selectively choose related sentences from out-of-domain data and used them as additional training data using transfer learning. We also report the effect of tokenization on translation model performance.","pages":"156--163","doi":"10.18653\/v1\/W19-5419","url":"https:\/\/www.aclweb.org\/anthology\/W19-5419","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5420","title":"Huawei{'}s {NMT} Systems for the {WMT} 2019 Biomedical Translation Task","authors":["Peng, Wei","Liu, Jianfeng","Li, Liangyou","Liu, Qun"],"emails":["peng.wei1@huawei.com","liujianfeng@huawei.com","liliangyou@huawei.com","qun.liu@huawei.com"],"author_id":["wei-peng","jianfeng-liu","liangyou-li","qun-liu"],"abstract":"This paper describes Huawei{'}s neural machine translation systems for the WMT 2019 biomedical translation shared task. We trained and fine-tuned our systems on a combination of out-of-domain and in-domain parallel corpora for six translation directions covering English{--}Chinese, English{--}French and English{--}German language pairs. Our submitted systems achieve the best BLEU scores on English{--}French and English{--}German language pairs according to the official evaluation results. In the English{--}Chinese translation task, our systems are in the second place. The enhanced performance is attributed to more in-domain training and more sophisticated models developed. Development of translation models and transfer learning (or domain adaptation) methods has significantly contributed to the progress of the task.","pages":"164--168","doi":"10.18653\/v1\/W19-5420","url":"https:\/\/www.aclweb.org\/anthology\/W19-5420","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5421","title":"{UCAM} Biomedical Translation at {WMT}19: Transfer Learning Multi-domain Ensembles","authors":["Saunders, Danielle","Stahlberg, Felix","Byrne, Bill"],"emails":["","",""],"author_id":["danielle-saunders","felix-stahlberg","bill-byrne"],"abstract":"The 2019 WMT Biomedical translation task involved translating Medline abstracts. We approached this using transfer learning to obtain a series of strong neural models on distinct domains, and combining them into multi-domain ensembles. We further experimented with an adaptive language-model ensemble weighting scheme. Our submission achieved the best submitted results on both directions of English-Spanish.","pages":"169--174","doi":"10.18653\/v1\/W19-5421","url":"https:\/\/www.aclweb.org\/anthology\/W19-5421","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5422","title":"{BSC} Participation in the {WMT} Translation of Biomedical Abstracts","authors":["Soares, Felipe","Krallinger, Martin"],"emails":["felipe.soares@bsc.es","martin.krallinger@bsc.es"],"author_id":["felipe-soares","martin-krallinger"],"abstract":"This paper describes the machine translation systems developed by the Barcelona Supercomputing (BSC) team for the biomedical translation shared task of WMT19. Our system is based on Neural Machine Translation unsing the OpenNMT-py toolkit and Transformer architecture. We participated in four translation directions for the English\/Spanish and English\/Portuguese language pairs. To create our training data, we concatenated several parallel corpora, both from in-domain and out-of-domain sources, as well as terminological resources from UMLS.","pages":"175--178","doi":"10.18653\/v1\/W19-5422","url":"https:\/\/www.aclweb.org\/anthology\/W19-5422","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5423","title":"The {MLLP}-{UPV} {S}panish-{P}ortuguese and {P}ortuguese-{S}panish Machine Translation Systems for {WMT}19 Similar Language Translation Task","authors":["Baquero-Arnal, Pau","Iranzo-S{\\'a}nchez, Javier","Civera, Jorge","Juan, Alfons"],"emails":["pabaar@vrain.upv.es","jairsan@vrain.upv.es","jcivera@vrain.upv.es","ajuan@vrain.upv.es"],"author_id":["pau-baquero-arnal","javier-iranzo-sanchez","jorge-civera","alfons-juan"],"abstract":"This paper describes the participation of the MLLP research group of the Universitat Polit{\\`e}cnica de Val{\\`e}ncia in the WMT 2019 Similar Language Translation Shared Task. We have submitted systems for the Portuguese \u2194 Spanish language pair, in both directions. We have submitted systems based on the Transformer architecture as well as an in development novel architecture which we have called 2D alternating RNN. We have carried out domain adaptation through fine-tuning.","pages":"179--184","doi":"10.18653\/v1\/W19-5423","url":"https:\/\/www.aclweb.org\/anthology\/W19-5423","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5424","title":"The {TALP}-{UPC} System for the {WMT} Similar Language Task: Statistical vs Neural Machine Translation","authors":["Biesialska, Magdalena","Guardia, Lluis","Costa-juss{\\`a}, Marta R."],"emails":["magdalena.biesialska@upc.edu","lluis.guardia@alu-etsetb.upc.edu","marta.ruiz@upc.edu"],"author_id":["magdalena-biesialska","lluis-guardia","marta-r-costa-jussa"],"abstract":"Although the problem of similar language translation has been an area of research interest for many years, yet it is still far from being solved. In this paper, we study the performance of two popular approaches: statistical and neural. We conclude that both methods yield similar results; however, the performance varies depending on the language pair. While the statistical approach outperforms the neural one by a difference of 6 BLEU points for the Spanish-Portuguese language pair, the proposed neural model surpasses the statistical one by a difference of 2 BLEU points for Czech-Polish. In the former case, the language similarity (based on perplexity) is much higher than in the latter case. Additionally, we report negative results for the system combination with back-translation. Our TALP-UPC system submission won 1st place for Czech-{\\textgreater}Polish and 2nd place for Spanish-{\\textgreater}Portuguese in the official evaluation of the 1st WMT Similar Language Translation task.","pages":"185--191","doi":"10.18653\/v1\/W19-5424","url":"https:\/\/www.aclweb.org\/anthology\/W19-5424","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5425","title":"Machine Translation from an Intercomprehension Perspective","authors":["Chen, Yu","Avgustinova, Tania"],"emails":["yuchen@coli.uni-sb.de","tania@coli.uni-sb.de"],"author_id":["yu-chen","tania-avgustinova"],"abstract":"Within the first shared task on machine translation between similar languages, we present our first attempts on Czech to Polish machine translation from an intercomprehension perspective. We propose methods based on the mutual intelligibility of the two languages, taking advantage of their orthographic and phonological similarity, in the hope to improve over our baselines. The translation results are evaluated using BLEU. On this metric, none of our proposals could outperform the baselines on the final test set. The current setups are rather preliminary, and there are several potential improvements we can try in the future.","pages":"192--196","doi":"10.18653\/v1\/W19-5425","url":"https:\/\/www.aclweb.org\/anthology\/W19-5425","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5426","title":"Utilizing Monolingual Data in {NMT} for Similar Languages: Submission to Similar Language Translation Task","authors":["Khatri, Jyotsana","Bhattacharyya, Pushpak"],"emails":["jyotsanak@cse.iitb.ac.in","pb@cse.iitb.ac.in"],"author_id":["jyotsana-khatri","pushpak-bhattacharyya"],"abstract":"This paper describes our submission to Shared Task on Similar Language Translation in Fourth Conference on Machine Translation (WMT 2019). We submitted three systems for Hindi -{\\textgreater} Nepali direction in which we have examined the performance of a RNN based NMT system, a semi-supervised NMT system where monolingual data of both languages is utilized using the architecture by and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data.","pages":"197--201","doi":"10.18653\/v1\/W19-5426","url":"https:\/\/www.aclweb.org\/anthology\/W19-5426","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5427","title":"Neural Machine Translation: {H}indi-{N}epali","authors":["Laskar, Sahinur Rahman","Pakray, Partha","Bandyopadhyay, Sivaji"],"emails":["sahinurlaskar.nits@gmail.com","parthapakray@gmail.com","sivaji.cse.ju@gmail.com"],"author_id":["sahinur-rahman-laskar","partha-pakray","sivaji-bandyopadhyay"],"abstract":"With the extensive use of Machine Translation (MT) technology, there is progressively interest in directly translating between pairs of similar languages. Because the main challenge is to overcome the limitation of available parallel data to produce a precise MT output. Current work relies on the Neural Machine Translation (NMT) with attention mechanism for the similar language translation of WMT19 shared task in the context of Hindi-Nepali pair. The NMT systems trained the Hindi-Nepali parallel corpus and tested, analyzed in Hindi \u21d4 Nepali translation. The official result declared at WMT19 shared task, which shows that our NMT system obtained Bilingual Evaluation Understudy (BLEU) score 24.6 for primary configuration in Nepali to Hindi translation. Also, we have achieved BLEU score 53.7 (Hindi to Nepali) and 49.1 (Nepali to Hindi) in contrastive system type.","pages":"202--207","doi":"10.18653\/v1\/W19-5427","url":"https:\/\/www.aclweb.org\/anthology\/W19-5427","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5428","title":"{NICT}{'}s Machine Translation Systems for the {WMT}19 Similar Language Translation Task","authors":["Marie, Benjamin","Dabre, Raj","Fujita, Atsushi"],"emails":["bmarie@nict.go.jp","raj.dabre@nict.go.jp","atsushi.fujita@nict.go.jp"],"author_id":["benjamin-marie","raj-dabre","atsushi-fujita"],"abstract":"This paper presents the NICT{'}s participation in the WMT19 shared Similar Language Translation Task. We participated in the Spanish-Portuguese task. For both translation directions, we prepared state-of-the-art statistical (SMT) and neural (NMT) machine translation systems. Our NMT systems with the Transformer architecture were trained on the provided parallel data enlarged with a large quantity of back-translated monolingual data. Our primary submission to the task is the result of a simple combination of our SMT and NMT systems. According to BLEU, our systems were ranked second and third respectively for the Portuguese-to-Spanish and Spanish-to-Portuguese translation directions. For contrastive experiments, we also submitted outputs generated with an unsupervised SMT system.","pages":"208--212","doi":"10.18653\/v1\/W19-5428","url":"https:\/\/www.aclweb.org\/anthology\/W19-5428","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5429","title":"Panlingua-{KMI} {MT} System for Similar Language Translation Task at {WMT} 2019","authors":["Ojha, Atul Kr.","Kumar, Ritesh","Bansal, Akanksha","Rani, Priya"],"emails":["shashwatup9k@gmail.com","llh@jnu.ac.in","akanksha.bansal15@gmail.com","pranijnu@gmail.com"],"author_id":["atul-kr-ojha","ritesh-kumar","akanksha-bansal","priya-rani"],"abstract":"The present paper enumerates the development of Panlingua-KMI Machine Translation (MT) systems for Hindi \u2194 Nepali language pair, designed as part of the Similar Language Translation Task at the WMT 2019 Shared Task. The Panlingua-KMI team conducted a series of experiments to explore both the phrase-based statistical (PBSMT) and neural methods (NMT). Among the 11 MT systems prepared under this task, 6 PBSMT systems were prepared for Nepali-Hindi, 1 PBSMT for Hindi-Nepali and 2 NMT systems were devel- oped for Nepali\u2194 Hindi. The results show that PBSMT could be an effective method for developing MT systems for closely-related languages. Our Hindi-Nepali PBSMT system was ranked 2nd among the 13 systems submit- ted for the pair and our Nepali-Hindi PBSMTsystem was ranked 4th among the 12 systems submitted for the task.","pages":"213--218","doi":"10.18653\/v1\/W19-5429","url":"https:\/\/www.aclweb.org\/anthology\/W19-5429","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5430","title":"{UDS}{--}{DFKI} Submission to the {WMT}2019 {C}zech{--}Polish Similar Language Translation Shared Task","authors":["Pal, Santanu","Zampieri, Marcos","van Genabith, Josef"],"emails":["santanu.pal@uni-saarland.de","",""],"author_id":["santanu-pal","marcos-zampieri","josef-van-genabith"],"abstract":"In this paper we present the UDS-DFKI system submitted to the Similar Language Translation shared task at WMT 2019. The first edition of this shared task featured data from three pairs of similar languages: Czech and Polish, Hindi and Nepali, and Portuguese and Spanish. Participants could choose to participate in any of these three tracks and submit system outputs in any translation direction. We report the results obtained by our system in translating from Czech to Polish and comment on the impact of out-of-domain test data in the performance of our system. UDS-DFKI achieved competitive performance ranking second among ten teams in Czech to Polish translation.","pages":"219--223","doi":"10.18653\/v1\/W19-5430","url":"https:\/\/www.aclweb.org\/anthology\/W19-5430","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5431","title":"Neural Machine Translation of Low-Resource and Similar Languages with Backtranslation","authors":["Przystupa, Michael","Abdul-Mageed, Muhammad"],"emails":["","muhammad.mageed@ubc.ca"],"author_id":["michael-przystupa","muhammad-abdul-mageed"],"abstract":"We present our contribution to the WMT19 Similar Language Translation shared task. We investigate the utility of neural machine translation on three low-resource, similar language pairs: Spanish {--} Portuguese, Czech {--} Polish, and Hindi {--} Nepali. Since state-of-the-art neural machine translation systems still require large amounts of bitext, which we do not have for the pairs we consider, we focus primarily on incorporating monolingual data into our models with backtranslation. In our analysis, we found Transformer models to work best on Spanish {--} Portuguese and Czech {--} Polish translation, whereas LSTMs with global attention worked best on Hindi {--} Nepali translation.","pages":"224--235","doi":"10.18653\/v1\/W19-5431","url":"https:\/\/www.aclweb.org\/anthology\/W19-5431","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5432","title":"The University of {H}elsinki Submissions to the {WMT}19 Similar Language Translation Task","authors":["Scherrer, Yves","V{\\'a}zquez, Ra{\\'u}l","Virpioja, Sami"],"emails":["yves.scherrer@helsinki.fi","ra{\\'u}l.v{\\'a}zquez@helsinki.fi","sami.virpioja@helsinki.fi"],"author_id":["yves-scherrer","raul-vazquez","sami-virpioja"],"abstract":"This paper describes the University of Helsinki Language Technology group{'}s participation in the WMT 2019 similar language translation task. We trained neural machine translation models for the language pairs Czech {\\textless}-{\\textgreater} Polish and Spanish {\\textless}-{\\textgreater} Portuguese. Our experiments focused on different subword segmentation methods, and in particular on the comparison of a cognate-aware segmentation method, Cognate Morfessor, with character segmentation and unsupervised segmentation methods for which the data from different languages were simply concatenated. We did not observe major benefits from cognate-aware segmentation methods, but further research may be needed to explore larger parts of the parameter space. Character-level models proved to be competitive for translation between Spanish and Portuguese, but they are slower in training and decoding.","pages":"236--244","doi":"10.18653\/v1\/W19-5432","url":"https:\/\/www.aclweb.org\/anthology\/W19-5432","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5433","title":"Dual Monolingual Cross-Entropy Delta Filtering of Noisy Parallel Data","authors":["Axelrod, Amittai","Kumar, Anish","Sloto, Steve"],"emails":["amittai@didiglobal.com","",""],"author_id":["amittai-axelrod","anish-kumar","steve-sloto"],"abstract":"We introduce a purely monolingual approach to filtering for parallel data from a noisy corpus in a low-resource scenario. Our work is inspired by Junczysdowmunt:2018, but we relax the requirements to allow for cases where no parallel data is available. Our primary contribution is a dual monolingual cross-entropy delta criterion modified from Cynical data selection Axelrod:2017, and is competitive (within 1.8 BLEU) with the best bilingual filtering method when used to train SMT systems. Our approach is featherweight, and runs end-to-end on a standard laptop in three hours.","pages":"245--251","doi":"10.18653\/v1\/W19-5433","url":"https:\/\/www.aclweb.org\/anthology\/W19-5433","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5434","title":"{NRC} Parallel Corpus Filtering System for {WMT} 2019","authors":["Bernier-Colborne, Gabriel","Lo, Chi-kiu"],"emails":["olborne@nrc-cnrc.gc.ca","o@nrc-cnrc.gc.ca"],"author_id":["gabriel-bernier-colborne","chi-kiu-lo"],"abstract":"We describe the National Research Council Canada team{'}s submissions to the parallel cor- pus filtering task at the Fourth Conference on Machine Translation.","pages":"252--260","doi":"10.18653\/v1\/W19-5434","url":"https:\/\/www.aclweb.org\/anthology\/W19-5434","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5435","title":"Low-Resource Corpus Filtering Using Multilingual Sentence Embeddings","authors":["Chaudhary, Vishrav","Tang, Yuqing","Guzm{\\'a}n, Francisco","Schwenk, Holger","Koehn, Philipp"],"emails":["vishrav@fb.com","yuqtang@fb.com","fguzman@fb.com","schwenk@fb.com","phi@jhu.edu"],"author_id":["vishrav-chaudhary","yuqing-tang","francisco-guzman","holger-schwenk","philipp-koehn"],"abstract":"In this paper, we describe our submission to the WMT19 low-resource parallel corpus filtering shared task. Our main approach is based on the LASER toolkit (Language-Agnostic SEntence Representations), which uses an encoder-decoder architecture trained on a parallel corpus to obtain multilingual sentence representations. We then use the representations directly to score and filter the noisy parallel sentences without additionally training a scoring function. We contrast our approach to other promising methods and show that LASER yields strong results. Finally, we produce an ensemble of different scoring methods and obtain additional gains. Our submission achieved the best overall performance for both the Nepali-English and Sinhala-English 1M tasks by a margin of 1.3 and 1.4 BLEU respectively, as compared to the second best systems. Moreover, our experiments show that this technique is promising for low and even no-resource scenarios.","pages":"261--266","doi":"10.18653\/v1\/W19-5435","url":"https:\/\/www.aclweb.org\/anthology\/W19-5435","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5436","title":"Quality and Coverage: The {AFRL} Submission to the {WMT}19 Parallel Corpus Filtering for Low-Resource Conditions Task","authors":["Erdmann, Grant","Gwinnup, Jeremy"],"emails":["grant.erdmann@us.af.mil","jeremy.gwinnup.1@us.af.mil"],"author_id":["grant-erdmann","jeremy-gwinnup"],"abstract":"The WMT19 Parallel Corpus Filtering For Low-Resource Conditions Task aims to test various methods of filtering a noisy parallel corpora, to make them useful for training machine translation systems. This year the noisy corpora are the relatively low-resource language pairs of Nepali-English and Sinhala- English. This papers describes the Air Force Research Laboratory (AFRL) submissions, including preprocessing methods and scoring metrics. Numerical results indicate a benefit over baseline and the relative benefits of different options.","pages":"267--270","doi":"10.18653\/v1\/W19-5436","url":"https:\/\/www.aclweb.org\/anthology\/W19-5436","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5437","title":"Webinterpret Submission to the {WMT}2019 Shared Task on Parallel Corpus Filtering","authors":["Gonz{\\'a}lez-Rubio, Jes{\\'u}s"],"emails":["jesus.gonzalez-rubio@webinterpret.com"],"author_id":["jesus-gonzalez-rubio"],"abstract":"This document describes the participation of Webinterpret in the shared task on parallel corpus filtering at the Fourth Conference on Machine Translation (WMT 2019). Here, we describe the main characteristics of our approach and discuss the results obtained on the data sets published for the shared task.","pages":"271--276","doi":"10.18653\/v1\/W19-5437","url":"https:\/\/www.aclweb.org\/anthology\/W19-5437","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5438","title":"Noisy Parallel Corpus Filtering through Projected Word Embeddings","authors":["Kurfal{\\i}, Murathan","{\\\"O}stling, Robert"],"emails":["murathan.kurfali@ling.su.se","robert@ling.su.se"],"author_id":["murathan-kurfali","robert-ostling"],"abstract":"We present a very simple method for parallel text cleaning of low-resource languages, based on projection of word embeddings trained on large monolingual corpora in high-resource languages. In spite of its simplicity, we approach the strong baseline system in the downstream machine translation evaluation.","pages":"277--281","doi":"10.18653\/v1\/W19-5438","url":"https:\/\/www.aclweb.org\/anthology\/W19-5438","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5439","title":"Filtering of Noisy Parallel Corpora Based on Hypothesis Generation","authors":["Parcheta, Zuzanna","Sanchis-Trilles, Germ{\\'a}n","Casacuberta, Francisco"],"emails":["zparcheta@sciling.com","gsanchis@sciling.com","fcn@prhlt.upv.es"],"author_id":["zuzanna-parcheta","german-sanchis-trilles","francisco-casacuberta"],"abstract":"The filtering task of noisy parallel corpora in WMT2019 aims to challenge participants to create filtering methods to be useful for training machine translation systems. In this work, we introduce a noisy parallel corpora filtering system based on generating hypotheses by means of a translation model. We train translation models in both language pairs: Nepali{--}English and Sinhala{--}English using provided parallel corpora. We select the training subset for three language pairs (Nepali, Sinhala and Hindi to English) jointly using bilingual cross-entropy selection to create the best possible translation model for both language pairs. Once the translation models are trained, we translate the noisy corpora and generate a hypothesis for each sentence pair. We compute the smoothed BLEU score between the target sentence and generated hypothesis. In addition, we apply several rules to discard very noisy or inadequate sentences which can lower the translation score. These heuristics are based on sentence length, source and target similarity and source language detection. We compare our results with the baseline published on the shared task website, which uses the Zipporah model, over which we achieve significant improvements in one of the conditions in the shared task. The designed filtering system is domain independent and all experiments are conducted using neural machine translation.","pages":"282--288","doi":"10.18653\/v1\/W19-5439","url":"https:\/\/www.aclweb.org\/anthology\/W19-5439","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5440","title":"Parallel Corpus Filtering Based on Fuzzy String Matching","authors":["Sen, Sukanta","Ekbal, Asif","Bhattacharyya, Pushpak"],"emails":["sukanta.pcs15@iitp.ac.in","asif@iitp.ac.in","pb@iitp.ac.in"],"author_id":["sukanta-sen","asif-ekbal","pushpak-bhattacharyya"],"abstract":"In this paper, we describe the IIT Patna{'}s submission to WMT 2019 shared task on parallel corpus filtering. This shared task asks the participants to develop methods for scoring each parallel sentence from a given noisy parallel corpus. Quality of the scoring method is judged based on the quality of SMT and NMT systems trained on smaller set of high-quality parallel sentences sub-sampled from the original noisy corpus. This task has two language pairs. We submit for both the Nepali-English and Sinhala-English language pairs. We define fuzzy string matching score between English and the translated (into English) source based on Levenshtein distance. Based on the scores, we sub-sample two sets (having 1 million and 5 millions English tokens) of parallel sentences from each parallel corpus, and train SMT systems for development purpose only. The organizers publish the official evaluation using both SMT and NMT on the final official test set. Total 10 teams participated in the shared task and according the official evaluation, our scoring method obtains 2nd position in the team ranking for 1-million NepaliEnglish NMT and 5-million Sinhala-English NMT categories.","pages":"289--293","doi":"10.18653\/v1\/W19-5440","url":"https:\/\/www.aclweb.org\/anthology\/W19-5440","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"},{"id":"W19-5441","title":"The University of {H}elsinki Submission to the {WMT}19 Parallel Corpus Filtering Task","authors":["V{\\'a}zquez, Ra{\\'u}l","Sulubacak, Umut","Tiedemann, J{\\\"o}rg"],"emails":["ra{\\'u}l.v{\\'a}zquez@helsinki.fi","umut.sulubacak@helsinki.fi","j{\\\"o}rg.tiedemann@helsinki.fi"],"author_id":["raul-vazquez","umut-sulubacak","jorg-tiedemann"],"abstract":"This paper describes the University of Helsinki Language Technology group{'}s participation in the WMT 2019 parallel corpus filtering task. Our scores were produced using a two-step strategy. First, we individually applied a series of filters to remove the {`}bad{'} quality sentences. Then, we produced scores for each sentence by weighting these features with a classification model. This methodology allowed us to build a simple and reliable system that is easily adaptable to other language pairs.","pages":"294--300","doi":"10.18653\/v1\/W19-5441","url":"https:\/\/www.aclweb.org\/anthology\/W19-5441","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)"}]