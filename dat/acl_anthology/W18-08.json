[{"id":"W18-0801","title":"On the Utility of Lay Summaries and {AI} Safety Disclosures: Toward Robust, Open Research Oversight","authors":["Schmaltz, Allen"],"emails":["schmaltz@fas.harvard.edu"],"author_id":["allen-schmaltz"],"abstract":"In this position paper, we propose that the community consider encouraging researchers to include two riders, a {``}Lay Summary{''} and an {``}AI Safety Disclosure{''}, as part of future NLP papers published in ACL forums that present user-facing systems. The goal is to encourage researchers{--}via a relatively non-intrusive mechanism{--}to consider the societal implications of technologies carrying (un)known and\/or (un)knowable long-term risks, to highlight failure cases, and to provide a mechanism by which the general public (and scientists in other disciplines) can more readily engage in the discussion in an informed manner. This simple proposal requires minimal additional up-front costs for researchers; the lay summary, at least, has significant precedence in the medical literature and other areas of science; and the proposal is aimed to supplement, rather than replace, existing approaches for encouraging researchers to consider the ethical implications of their work, such as those of the Collaborative Institutional Training Initiative (CITI) Program and institutional review boards (IRBs).","pages":"1--6","doi":"10.18653\/v1\/W18-0801","url":"https:\/\/www.aclweb.org\/anthology\/W18-0801","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana, USA","year":"2018","month":"June","booktitle":"Proceedings of the Second {ACL} Workshop on Ethics in Natural Language Processing"},{"id":"W18-0802","title":"{\\#}{M}e{T}oo {A}lexa: How Conversational Systems Respond to Sexual Harassment","authors":["Cercas Curry, Amanda","Rieser, Verena"],"emails":["ac293@hw.ac.uk","v.t.rieser@hw.ac.uk"],"author_id":["amanda-cercas-curry","verena-rieser"],"abstract":"Conversational AI systems, such as Amazon{'}s Alexa, are rapidly developing from purely transactional systems to social chatbots, which can respond to a wide variety of user requests. In this article, we establish how current state-of-the-art conversational systems react to inappropriate requests, such as bullying and sexual harassment on the part of the user, by collecting and analysing the novel {\\#}MeTooAlexa corpus. Our results show that commercial systems mainly avoid answering, while rule-based chatbots show a variety of behaviours and often deflect. Data-driven systems, on the other hand, are often non-coherent, but also run the risk of being interpreted as flirtatious and sometimes react with counter-aggression. This includes our own system, trained on {``}clean{''} data, which suggests that inappropriate system behaviour is not caused by data bias.","pages":"7--14","doi":"10.18653\/v1\/W18-0802","url":"https:\/\/www.aclweb.org\/anthology\/W18-0802","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana, USA","year":"2018","month":"June","booktitle":"Proceedings of the Second {ACL} Workshop on Ethics in Natural Language Processing"}]