[{"id":"W19-0401","title":"Projecting Temporal Properties, Events and Actions","authors":["Fernando, Tim"],"emails":["ernando@tcd.ie"],"author_id":["tim-fernando"],"abstract":"Temporal notions based on a finite set $A$ of properties are represented in strings, on which projections are defined that vary the granularity $A$. The structure of properties in $A$ is elaborated to describe statives, events and actions, subject to a distinction in meaning (advocated by Levin and Rappaport Hovav) between what the lexicon prescribes and what a context of use supplies. The projections proposed are deployed as labels for records and record types amenable to finite-state methods.","pages":"1--12","url":"https:\/\/www.aclweb.org\/anthology\/W19-0401","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0402","title":"A Type-coherent, Expressive Representation as an Initial Step to Language Understanding","authors":["Kim, Gene Louis","Schubert, Lenhart"],"emails":["gkim21@cs.rochester.edu","schubert@cs.rochester.edu"],"author_id":["gene-louis-kim","lenhart-schubert"],"abstract":"A growing interest in tasks involving language understanding by the NLP community has led to the need for effective semantic parsing and inference. Modern NLP systems use semantic representations that do not quite fulfill the nuanced needs for language understanding: adequately modeling language semantics, enabling general inferences, and being accurately recoverable. This document describes underspecified logical forms{\\textasciitilde}(ULF) for Episodic Logic{\\textasciitilde}(EL), which is an initial form for a semantic representation that balances these needs. ULFs fully resolve the semantic type structure while leaving issues such as quantifier scope, word sense, and anaphora unresolved; they provide a starting point for further resolution into EL, and enable certain structural inferences without further resolution. This document also presents preliminary results of creating a hand-annotated corpus of ULFs for the purpose of training a precise ULF parser, showing a three-person pairwise interannotator agreement of 0.88 on confident annotations. We hypothesize that a divide-and-conquer approach to semantic parsing starting with derivation of ULFs will lead to semantic analyses that do justice to subtle aspects of linguistic meaning, and will enable construction of more accurate semantic parsers.","pages":"13--30","url":"https:\/\/www.aclweb.org\/anthology\/W19-0402","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0403","title":"A Semantic Annotation Scheme for Quantification","authors":["Bunt, Harry"],"emails":["harry.bunt@uvt.nl"],"author_id":["harry-bunt"],"abstract":"This paper describes in brief the proposal called {`}QuantML{'} which was accepted by the International Organisation for Standards (ISO) last February as a starting point for developing a standard for the interoperable annotation of quantification phenomena in natural language, as part of the ISO 24617 Semantic Annotation Framework. The proposal, firmly rooted in the theory of generalised quantifiers, neo-Davidsonian semantics, and DRT, covers a wide range of quantification phenomena. The QuantML scheme consists of (1) an abstract syntax which defines {`}annotation structures{'} as triples and other set-theoretic constructs; (b) a compositional semantics of annotation structures; (3) an XML representation of annotation structures.","pages":"31--42","url":"https:\/\/www.aclweb.org\/anthology\/W19-0403","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0404","title":"Re-Ranking Words to Improve Interpretability of Automatically Generated Topics","authors":["Alokaili, Areej","Aletras, Nikolaos","Stevenson, Mark"],"emails":["areej.okaili@sheffield.ac.uk","n.aletras@sheffield.ac.uk","mark.stevenson@sheffield.ac.uk"],"author_id":["areej-alokaili","nikolaos-aletras","mark-stevenson"],"abstract":"Topics models, such as LDA, are widely used in Natural Language Processing. Making their output interpretable is an important area of research with applications to areas such as the enhancement of exploratory search interfaces and the development of interpretable machine learning models. Conventionally, topics are represented by their n most probable words, however, these representations are often difficult for humans to interpret. This paper explores the re-ranking of topic words to generate more interpretable topic representations. A range of approaches are compared and evaluated in two experiments. The first uses crowdworkers to associate topics represented by different word rankings with related documents. The second experiment is an automatic approach based on a document retrieval task applied on multiple domains. Results in both experiments demonstrate that re-ranking words improves topic interpretability and that the most effective re-ranking schemes were those which combine information about the importance of words both within topics and their relative frequency in the entire corpus. In addition, close correlation between the results of the two evaluation approaches suggests that the automatic method proposed here could be used to evaluate re-ranking methods without the need for human judgements.","pages":"43--54","url":"https:\/\/www.aclweb.org\/anthology\/W19-0404","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0405","title":"An Improved Approach for Semantic Graph Composition with {CCG}","authors":["Blodgett, Austin","Schneider, Nathan"],"emails":["ajb341@georgetown.edu","nathan.schneider@georgetown.edu"],"author_id":["austin-blodgett","nathan-schneider"],"abstract":"This paper builds on previous work using Combinatory Categorial Grammar (CCG) to derive a transparent syntax-semantics interface for Abstract Meaning Representation (AMR) parsing. We define new semantics for the CCG combinators that is better suited to deriving AMR graphs. In particular, we define relation-wise alternatives for the application and composition combinators: these require that the two constituents being combined overlap in one AMR relation. We also provide a new semantics for type raising, which is necessary for certain constructions. Using these mechanisms, we suggest an analysis of eventive nouns, which present a challenge for deriving AMR graphs. Our theoretical analysis will facilitate future work on robust and transparent AMR parsing using CCG.","pages":"55--70","url":"https:\/\/www.aclweb.org\/anthology\/W19-0405","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0406","title":"A Semantic Ontology of {D}anish Adjectives","authors":["Bick, Eckhard"],"emails":["eckhard.bick@mail.dk"],"author_id":["eckhard-bick"],"abstract":"This paper presents a semantic annotation scheme for Danish adjectives, focusing both on prototypical semantic content and semantic collocational restrictions on an adjective{'}s head noun. The core type set comprises about 110 categories ordered in a shallow hierarchy with 14 primary and 25 secondary umbrella categories. In addition, domain information and binary sentiment tags are provided, as well as VerbNet-derived frames and semantic roles for those adjectives governing arguments. The scheme has been almost fully implemented on the lexicon of the Danish VISL parser, DanGram, containing 14,000 adjectives. We discuss the annotation scheme and its applicational perspectives, and present a statistical breakdown and coverage evaluation for three Danish reference corpora.","pages":"71--78","url":"https:\/\/www.aclweb.org\/anthology\/W19-0406","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0407","title":"Towards a Compositional Analysis of {G}erman Light Verb Constructions ({LVC}s) Combining Lexicalized Tree Adjoining Grammar ({LTAG}) with Frame Semantics","authors":["Fleischhauer, Jens","Gamerschlag, Thomas","Kallmeyer, Laura","Petitjean, Simon"],"emails":["fleischhauer@phil.hhu.de","gamer@phil.hhu.de","kallmeyer@phil.hhu.de","petitjean@phil.hhu.de"],"author_id":["jens-fleischhauer","thomas-gamerschlag","laura-kallmeyer","simon-petitjean"],"abstract":"Complex predicates formed of a semantically {`}light{'} verbal head and a noun or verb which contributes the major part of the meaning are frequently referred to as {`}light verb constructions{'} (LVCs). In the paper, we present a case study of LVCs with the German posture verb stehen {`}stand{'}. In our account, we model the syntactic as well as semantic composition of such LVCs by combining Lexicalized Tree Adjoining Grammar (LTAG) with frames. Starting from the analysis of the literal uses of posture verbs, we show how the meaning components of the literal uses are systematically exploited in the interpretation of stehen-LVCs. The paper constitutes an important step towards a compositional and computational analysis of LVCs. We show that LTAG allows us to separate constructional from lexical meaning components and that frames enable elegant generalizations over event types and related constraints.","pages":"79--90","url":"https:\/\/www.aclweb.org\/anthology\/W19-0407","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0408","title":"Words are Vectors, Dependencies are Matrices: Learning Word Embeddings from Dependency Graphs","authors":["Czarnowska, Paula","Emerson, Guy","Copestake, Ann"],"emails":["pjc211@cam.ac.uk","gete2@cam.ac.uk","aac10@cam.ac.uk"],"author_id":["paula-czarnowska","guy-emerson","ann-copestake"],"abstract":"Distributional Semantic Models (DSMs) construct vector representations of word meanings based on their contexts. Typically, the contexts of a word are defined as its closest neighbours, but they can also be retrieved from its syntactic dependency relations. In this work, we propose a new dependency-based DSM. The novelty of our model lies in associating an independent meaning representation, a matrix, with each dependency-label. This allows it to capture specifics of the relations between words and contexts, leading to good performance on both intrinsic and extrinsic evaluation tasks. In addition to that, our model has an inherent ability to represent dependency chains as products of matrices which provides a straightforward way of handling further contexts of a word.","pages":"91--102","url":"https:\/\/www.aclweb.org\/anthology\/W19-0408","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0409","title":"Temporal and Aspectual Entailment","authors":["Kober, Thomas","de Vroe, Sander Bijl","Steedman, Mark"],"emails":["tkober@inf.ed.ac.uk","sbdv@ed.ac.uk","steedman@inf.ed.ac.uk"],"author_id":["thomas-kober","sander-bijl-de-vroe","mark-steedman"],"abstract":"Inferences regarding {``}Jane{'}s arrival in London{''} from predications such as {``}Jane is going to London{''} or {``}Jane has gone to London{''} depend on tense and aspect of the predications. Tense determines the temporal location of the predication in the past, present or future of the time of utterance. The aspectual auxiliaries on the other hand specify the internal constituency of the event, i.e. whether the event of {``}going to London{''} is completed and whether its consequences hold at that time or not. While tense and aspect are among the most important factors for determining natural language inference, there has been very little work to show whether modern embedding models capture these semantic concepts. In this paper we propose a novel entailment dataset and analyse the ability of contextualised word representations to perform inference on predications across aspectual types and tenses. We show that they encode a substantial amount of information relating to tense and aspect, but fail to consistently model inferences that require reasoning with these semantic properties.","pages":"103--119","url":"https:\/\/www.aclweb.org\/anthology\/W19-0409","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0410","title":"Don{'}t Blame Distributional Semantics if it can{'}t do Entailment","authors":["Westera, Matthijs","Boleda, Gemma"],"emails":["matthijs.westera@upf.edu","gemma.boleda@upf.edu"],"author_id":["matthijs-westera","gemma-boleda"],"abstract":"Distributional semantics has had enormous empirical success in Computational Linguistics and Cognitive Science in modeling various semantic phenomena, such as semantic similarity, and distributional models are widely used in state-of-the-art Natural Language Processing systems. However, the theoretical status of distributional semantics within a broader theory of language and cognition is still unclear: What does distributional semantics model? Can it be, on its own, a fully adequate model of the meanings of linguistic expressions? The standard answer is that distributional semantics is not fully adequate in this regard, because it falls short on some of the central aspects of formal semantic approaches: truth conditions, entailment, reference, and certain aspects of compositionality. We argue that this standard answer rests on a misconception: These aspects do not belong in a theory of expression meaning, they are instead aspects of speaker meaning, i.e., communicative intentions in a particular context. In a slogan: words do not refer, speakers do. Clearing this up enables us to argue that distributional semantics on its own is an adequate model of expression meaning. Our proposal sheds light on the role of distributional semantics in a broader theory of language and cognition, its relationship to formal semantics, and its place in computational models.","pages":"120--133","url":"https:\/\/www.aclweb.org\/anthology\/W19-0410","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0411","title":"Ambiguity in Explicit Discourse Connectives","authors":["Webber, Bonnie","Prasad, Rashmi","Lee, Alan"],"emails":["bonnie.webber@ed.ac.uk","rprasad@interactions.com","aleewk@seas.upenn.edu"],"author_id":["bonnie-webber","rashmi-prasad","alan-lee"],"abstract":"Discourse connectives are known to be subject to both usage and sense ambiguity, as has already been discussed in the literature. But discourse connectives are no different from other linguistic expressions in being subject to other types of ambiguity as well. Four are illustrated and discussed here.","pages":"134--141","url":"https:\/\/www.aclweb.org\/anthology\/W19-0411","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0412","title":"Aligning Open {IE} Relations and {KB} Relations using a {S}iamese Network Based on Word Embedding","authors":["Putri, Rifki Afina","Hong, Giwon","Myaeng, Sung-Hyon"],"emails":["rifkiaputri@kaist.ac.kr","gch02518@kaist.ac.kr","myaeng@kaist.ac.kr"],"author_id":["rifki-afina-putri","giwon-hong","sung-hyon-myaeng"],"abstract":"Open Information Extraction (Open IE) aims at generating entity-relation-entity triples from a large amount of text, aiming at capturing key semantics of the text. Given a triple, the relation expresses the type of semantic relation between the entities. Although relations from an Open IE system are more extensible than those used in a traditional Information Extraction system and a Knowledge Base (KB) such as Knowledge Graphs, the former lacks in semantics; an Open IE relation is simply a sequence of words, whereas a KB relation has a predefined meaning. As a way to provide a meaning to an Open IE relation, we attempt to align it with one of the predefined set of relations used in a KB. Our approach is to use a Siamese network that compares two sequences of word embeddings representing an Open IE relation and a predefined KB relation. In order to make the approach practical, we automatically generate a training dataset using a distant supervision approach instead of relying on a hand-labeled dataset. Our experiment shows that the proposed method can capture the relational semantics better than the recent approaches.","pages":"142--153","url":"https:\/\/www.aclweb.org\/anthology\/W19-0412","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0413","title":"Language-Agnostic Model for Aspect-Based Sentiment Analysis","authors":["Akhtar, Md Shad","Kumar, Abhishek","Ekbal, Asif","Biemann, Chris","Bhattacharyya, Pushpak"],"emails":["shad.pcs15@iitp.ac.in","abhishek.ee14@iitp.ac.in","asif@iitp.ac.in","biemann@informatik.uni-hamburg.de","pb@iitp.ac.in"],"author_id":["md-shad-akhtar","abhishek-kumar","asif-ekbal","chris-biemann","pushpak-bhattacharyya"],"abstract":"In this paper, we propose a language-agnostic deep neural network architecture for aspect-based sentiment analysis. The proposed approach is based on Bidirectional Long Short-Term Memory (Bi-LSTM) network, which is further assisted with extra hand-crafted features. We define three different architectures for the successful combination of word embeddings and hand-crafted features. We evaluate the proposed approach for six languages (i.e. English, Spanish, French, Dutch, German and Hindi) and two problems (i.e. aspect term extraction and aspect sentiment classification). Experiments show that the proposed model attains state-of-the-art performance in most of the settings.","pages":"154--164","url":"https:\/\/www.aclweb.org\/anthology\/W19-0413","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0414","title":"The Effect of Context on Metaphor Paraphrase Aptness Judgments","authors":["Bizzoni, Yuri","Lappin, Shalom"],"emails":["yuri.bizzoni@gu.se","shalom.lappin@gu.se"],"author_id":["yuri-bizzoni","shalom-lappin"],"abstract":"We conduct two experiments to study the effect of context on metaphor paraphrase aptness judgments. The first is an AMT crowd source task in which speakers rank metaphor-paraphrase candidate sentence pairs in short document contexts for paraphrase aptness. In the second we train a composite DNN to predict these human judgments, first in binary classifier mode, and then as gradient ratings. We found that for both mean human judgments and our DNN{'}s predictions, adding document context compresses the aptness scores towards the center of the scale, raising low out-of-context ratings and decreasing high out-of-context scores. We offer a provisional explanation for this compression effect.","pages":"165--175","url":"https:\/\/www.aclweb.org\/anthology\/W19-0414","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0415","title":"Predicting Word Concreteness and Imagery","authors":["Charbonnier, Jean","Wartena, Christian"],"emails":["jean.charbonnier@hs-hannover.de","christian.wartena@hs-hannover.de"],"author_id":["jean-charbonnier","christian-wartena"],"abstract":"Concreteness of words has been studied extensively in psycholinguistic literature. A number of datasets have been created with average values for perceived concreteness of words. We show that we can train a regression model on these data, using word embeddings and morphological features, that can predict these concreteness values with high accuracy. We evaluate the model on 7 publicly available datasets. Only for a few small subsets of these datasets prediction of concreteness values are found in the literature. Our results clearly outperform the reported results for these datasets.","pages":"176--187","url":"https:\/\/www.aclweb.org\/anthology\/W19-0415","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0416","title":"Learning to Explicitate Connectives with {S}eq2{S}eq Network for Implicit Discourse Relation Classification","authors":["Shi, Wei","Demberg, Vera"],"emails":["w.shi@coli.uni-saarland.de","vera@coli.uni-saarland.de"],"author_id":["wei-shi","vera-demberg"],"abstract":"Implicit discourse relation classification is one of the most difficult steps in discourse parsing. The difficulty stems from the fact that the coherence relation must be inferred based on the content of the discourse relational arguments. Therefore, an effective encoding of the relational arguments is of crucial importance. We here propose a new model for implicit discourse relation classification, which consists of a classifier, and a sequence-to-sequence model which is trained to generate a representation of the discourse relational arguments by trying to predict the relational arguments including a suitable implicit connective. Training is possible because such implicit connectives have been annotated as part of the PDTB corpus. Along with a memory network, our model could generate more refined representations for the task. And on the now standard 11-way classification, our method outperforms the previous state of the art systems on the PDTB benchmark on multiple settings including cross validation.","pages":"188--199","url":"https:\/\/www.aclweb.org\/anthology\/W19-0416","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0417","title":"Cross-Lingual Transfer of Semantic Roles: From Raw Text to Semantic Roles","authors":["Aminian, Maryam","Rasooli, Mohammad Sadegh","Diab, Mona"],"emails":["aminian@gwu.edu","rasooli@fb.com","mtdiab@gwu.edu"],"author_id":["maryam-aminian","mohammad-sadegh-rasooli","mona-diab"],"abstract":"We describe a transfer method based on annotation projection to develop a dependency-based semantic role labeling system for languages for which no supervised linguistic information other than parallel data is available. Unlike previous work that presumes the availability of supervised features such as lemmas, part-of-speech tags, and dependency parse trees, we only make use of word and character features. Our deep model considers using character-based representations as well as unsupervised stem embeddings to alleviate the need for supervised features. Our experiments outperform a state-of-the-art method that uses supervised lexico-syntactic features on 6 out of 7 languages in the Universal Proposition Bank.","pages":"200--210","url":"https:\/\/www.aclweb.org\/anthology\/W19-0417","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0418","title":"Evaluating the Representational Hub of Language and Vision Models","authors":["Shekhar, Ravi","Takmaz, Ece","Fern{\\'a}ndez, Raquel","Bernardi, Raffaella"],"emails":["","","raquel.fernandez@uva.nl","raffaella.bernardi@unitn.it"],"author_id":["ravi-shekhar","ece-takmaz","raquel-fernandez","raffaella-bernardi"],"abstract":"The multimodal models used in the emerging field at the intersection of computational linguistics and computer vision implement the bottom-up processing of the {``}Hub and Spoke{''} architecture proposed in cognitive science to represent how the brain processes and combines multi-sensory inputs. In particular, the Hub is implemented as a neural network encoder. We investigate the effect on this encoder of various vision-and-language tasks proposed in the literature: visual question answering, visual reference resolution, and visually grounded dialogue. To measure the quality of the representations learned by the encoder, we use two kinds of analyses. First, we evaluate the encoder pre-trained on the different vision-and-language tasks on an existing {``}diagnostic task{''} designed to assess multimodal semantic understanding. Second, we carry out a battery of analyses aimed at studying how the encoder merges and exploits the two modalities.","pages":"211--222","url":"https:\/\/www.aclweb.org\/anthology\/W19-0418","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0419","title":"The Fast and the Flexible: Training Neural Networks to Learn to Follow Instructions from Small Data","authors":["Leonandya, Rezka","Hupkes, Dieuwke","Bruni, Elia","Kruszewski, Germ{\\'a}n"],"emails":["rezka.aufar@gmail.com","d.hupkes@uva.nl","elia.bruni@gmail.com","germank@gmail.com"],"author_id":["rezka-leonandya","dieuwke-hupkes","elia-bruni","german-kruszewski"],"abstract":"Learning to follow human instructions is a long-pursued goal in artificial intelligence. The task becomes particularly challenging if no prior knowledge of the employed language is assumed while relying only on a handful of examples to learn from. Work in the past has relied on hand-coded components or manually engineered features to provide strong inductive biases that make learning in such situations possible. In contrast, here we seek to establish whether this knowledge can be acquired automatically by a neural network system through a two phase training procedure: A (slow) offline learning stage where the network learns about the general structure of the task and a (fast) online adaptation phase where the network learns the language of a new given speaker. Controlled experiments show that when the network is exposed to familiar instructions but containing novel words, the model adapts very efficiently to the new vocabulary. Moreover, even for human speakers whose language usage can depart significantly from our artificial training language, our network can still make use of its automatically acquired inductive bias to learn to follow instructions more effectively.","pages":"223--234","url":"https:\/\/www.aclweb.org\/anthology\/W19-0419","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0420","title":"Fast and Discriminative Semantic Embedding","authors":["Koopman, Rob","Wang, Shenghui","Englebienne, Gwenn"],"emails":["rob.koopman@oclc.org","shenghui.wang@oclc.org","g.englebienne@utwente.nl"],"author_id":["rob-koopman","shenghui-wang","gwenn-englebienne"],"abstract":"The embedding of words and documents in compact, semantically meaningful vector spaces is a crucial part of modern information systems. Deep Learning models are powerful but their hyperparameter selection is often complex and they are expensive to train, and while pre-trained models are available, embeddings trained on general corpora are not necessarily well-suited to domain specific tasks. We propose a novel embedding method which extends random projection by weighting and projecting raw term embeddings orthogonally to an average language vector, thus improving the discriminating power of resulting term embeddings, and build more meaningful document embeddings by assigning appropriate weights to individual terms. We describe how updating the term embeddings online as we process the training data results in an extremely efficient method, in terms of both computational and memory requirements. Our experiments show highly competitive results with various state-of-the-art embedding methods on different tasks, including the standard STS benchmark and a subject prediction task, at a fraction of the computational cost.","pages":"235--246","url":"https:\/\/www.aclweb.org\/anthology\/W19-0420","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0421","title":"Using Multi-Sense Vector Embeddings for Reverse Dictionaries","authors":["Hedderich, Michael A.","Yates, Andrew","Klakow, Dietrich","de Melo, Gerard"],"emails":["mhedderich@lsv.uni-saarland.de","ayates@mpi-inf.mpg.de","dietrich.klakow@lsv.uni-saarland.de","gdm@demelo.org"],"author_id":["michael-a-hedderich","andrew-yates","dietrich-klakow","gerard-de-melo"],"abstract":"Popular word embedding methods such as word2vec and GloVe assign a single vector representation to each word, even if a word has multiple distinct meanings. Multi-sense embeddings instead provide different vectors for each sense of a word. However, they typically cannot serve as a drop-in replacement for conventional single-sense embeddings, because the correct sense vector needs to be selected for each word. In this work, we study the effect of multi-sense embeddings on the task of reverse dictionaries. We propose a technique to easily integrate them into an existing neural network architecture using an attention mechanism. Our experiments demonstrate that large improvements can be obtained when employing multi-sense embeddings both in the input sequence as well as for the target representation. An analysis of the sense distributions and of the learned attention is provided as well.","pages":"247--258","url":"https:\/\/www.aclweb.org\/anthology\/W19-0421","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0422","title":"Using {W}iktionary as a resource for {WSD} : the case of {F}rench verbs","authors":["Segonne, Vincent","Candito, Marie","Crabb{\\'e}, Beno{\\^\\i}t"],"emails":["vincent.segonne@linguist.univ-paris-diderot.fr","marie.candito@linguist.univ-paris-diderot.fr","benoit.crabbe@linguist.univ-paris-diderot.fr"],"author_id":["vincent-segonne","marie-candito","benoit-crabbe"],"abstract":"As opposed to word sense induction, word sense disambiguation (WSD) has the advantage of us-ing interpretable senses, but requires annotated data, which are quite rare for most languages except English (Miller et al. 1993; Fellbaum, 1998). In this paper, we investigate which strategy to adopt to achieve WSD for languages lacking data that was annotated specifically for the task, focusing on the particular case of verb disambiguation in French. We first study the usability of Eurosense (Bovi et al. 2017) , a multilingual corpus extracted from Europarl (Kohen, 2005) and automatically annotated with BabelNet (Navigli and Ponzetto, 2010) senses. Such a resource opened up the way to supervised and semi-supervised WSD for resourceless languages like French. While this perspective looked promising, our evaluation on French verbs was inconclusive and showed the annotated senses{'} quality was not sufficient for supervised WSD on French verbs. Instead, we propose to use Wiktionary, a collaboratively edited, multilingual online dictionary, as a resource for WSD. Wiktionary provides both sense inventory and manually sense tagged examples which can be used to train supervised and semi-supervised WSD systems. Yet, because senses{'} distribution differ in lexicographic examples found in Wiktionary with respect to natural text, we then focus on studying the impact on WSD of the training data size and senses{'} distribution. Using state-of-the art semi-supervised systems, we report experiments of Wiktionary-based WSD for French verbs, evaluated on FrenchSemEval (FSE), a new dataset of French verbs manually annotated with wiktionary senses.","pages":"259--270","url":"https:\/\/www.aclweb.org\/anthology\/W19-0422","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0423","title":"A Comparison of Context-sensitive Models for Lexical Substitution","authors":["Soler, Aina Gar{\\'\\i}","Cocos, Anne","Apidianaki, Marianna","Callison-Burch, Chris"],"emails":["aina.gari@limsi.fr","acocos@seas.upenn.edu","marianna@limsi.fr","ccb@seas.upenn.edu"],"author_id":["aina-gari-soler1","anne-cocos","marianna-apidianaki","chris-callison-burch"],"abstract":"Word embedding representations provide good estimates of word meaning and give state-of-the art performance in semantic tasks. Embedding approaches differ as to whether and how they account for the context surrounding a word. We present a comparison of different word and context representations on the task of proposing substitutes for a target word in context (lexical substitution). We also experiment with tuning contextualized word embeddings on a dataset of sense-specific instances for each target word. We show that powerful contextualized word representations, which give high performance in several semantics-related tasks, deal less well with the subtle in-context similarity relationships needed for substitution. This is better handled by models trained with this objective in mind, where the inter-dependence between word and context representations is explicitly modeled during training.","pages":"271--282","url":"https:\/\/www.aclweb.org\/anthology\/W19-0423","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0424","title":"Natural Language Semantics With Pictures: Some Language {\\&} Vision Datasets and Potential Uses for Computational Semantics","authors":["Schlangen, David"],"emails":["david.schlangen@uni-potsdam.de"],"author_id":["david-schlangen"],"abstract":"Propelling, and propelled by, the {``}deep learning revolution{''}, recent years have seen the introduction of ever larger corpora of images annotated with natural language expressions. We survey some of these corpora, taking a perspective that reverses the usual directionality, as it were, by viewing the images as semantic annotation of the natural language expressions. We discuss datasets that can be derived from the corpora, and tasks of potential interest for computational semanticists that can be defined on those. In this, we make use of relations provided by the corpora (namely, the link between expression and image, and that between two expressions linked to the same image) and relations that we can add (similarity relations between expressions, or between images). Specifically, we show that in this way we can create data that can be used to learn and evaluate lexical and compositional grounded semantics, and we show that the {``}linked to same image{''} relation tracks a semantic implication relation that is recognisable to annotators even in the absence of the linking image as evidence. Finally, as an example of possible benefits of this approach, we show that an exemplar-model-based approach to implication beats a (simple) distributional space-based one on some derived datasets, while lending itself to explainability.","pages":"283--294","url":"https:\/\/www.aclweb.org\/anthology\/W19-0424","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"},{"id":"W19-0425","title":"Frame Identification as Categorization: Exemplars vs Prototypes in Embeddingland","authors":["Sikos, Jennifer","Pad{\\'o}, Sebastian"],"emails":["jen.sikos@ims.uni-stuttgart.de","pado@ims.uni-stuttgart.de"],"author_id":["jennifer-sikos","sebastian-pado"],"abstract":"Categorization is a central capability of human cognition, and a number of theories have been developed to account for properties of categorization. Even though many tasks in semantics also involve categorization of some kind, theories of categorization do not play a major role in contemporary research in computational linguistics. This paper follows the idea that embedding-based models of semantics lend themselves well to being formulated in terms of classical categorization theories. The benefit is a space of model families that enables (a) the formulation of hypotheses about the impact of major design decisions, and (b) a transparent assessment of these decisions. We instantiate this idea on the task of frame-semantic frame identification. We define four models that cross two design variables: (a) the choice of prototype vs. exemplar categorization, corresponding to different degrees of generalization applied to the input; and (b) the presence vs. absence of a fine-tuning step, corresponding to generic vs. task-adaptive categorization. We find that for frame identification, generalization and task-adaptive categorization both yield substantial benefits. Our prototype-based, fine-tuned model, which combines the best choices for these variables, establishes a new state of the art in frame identification.","pages":"295--306","url":"https:\/\/www.aclweb.org\/anthology\/W19-0425","publisher":"Association for Computational Linguistics","address":"Gothenburg, Sweden","year":"2019","month":"23{--}27 May","booktitle":"Proceedings of the 13th International Conference on Computational Semantics - Long Papers"}]